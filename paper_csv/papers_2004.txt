In a discourse, events are narrated one by one linearly. But the temporal sequence of events does not always match the linear sequence of narration. One of such cases involves the mixed occurrences of durative and punctual events, as illustrated by “Mia took aspirin and slept, for she was ill. She then fell off a cliff in her dream." In this narration, ﬁve events are reported: three are durative events of sleeping, being ill and having a dream and two are punctual events of taking aspirin and falling off a cliff. This presentation aims at establishing some systematic way of processing such events and representing them in a reasonably understandable temporal sequence. For this, events are analyzed in terms of an interval semantics that allows them to be anchored to appropriate temporal intervals and be ordered in an appropriate temporal sequence. In order to provide a simple syntactic basis, the presentation attempts to develop a small computational program that derive representations in feature structure by analyzing a small fragment of Korean. 
structures unlike ours is a crucial issue to be addressed in knowledge engineering. In order to become sharable and reusable knowledge, all extracted information must first be correctly situated in a knowledge structure. In addition, the situated information must be allowed to transfer from knowledge structure to knowledge structure without losing its meaningful content. This is the vision behind the Suggested Upper Merged Ontology (SUMO, http://www.ontologyportal.org) proposed by an IEEE working group. A shared upper ontology will both anchor the structured transfer of knowledge as well as set a standard for the construction of a middle and lower level ontology for each domain. This vision also has promising applications in the Semantic Web. 
In this paper we will discuss interpretation of adverbs in Japanese. We will explore the division of labor between the syntactic requirements, semantic requirements, and discourse-contextual constraints involving adverbial interpretation. It will then be argued that this inter-modular approach utilizing LFG explains various elusive paradigms of the adverbs.  1. Scope Ambiguity and Lexical Semantics  We will start with lexical semantic considerations of adverb scope. Pustejovsky (1991, 1995) argue that different types of adjuncts modify different types of subevents in the event structure of a verb. Under recent assumptions in LFG, lexical semantic information is encoded in f(unctional) structure (Butt 1995, Andrews and Manning 1999, Wilson 1999). There have been a number of proposals concerning the level of lexical semantic representation that can capture the various properties of a verb, including argument projection, aspectual class, transitivity alternation, and so on (Jackendoff 1990, Levin and RappaportHovav 1995, Kageyama 1996, among many others). We conjecture that event structure representations factor out the part of the verb semantics that contribute to the aspectual property of the verb, while leaving the other components of the meaning to be specified by the LCS that accompanies each subevent as shown in (1), in which x and y represent external argument and internal argument, respectively. “*” indicates the head subevent, and “<” means ‘precedes’ and the structure in the parentheses shows that the corresponding LCS for the subevent.  (1) a. state (e.g. aru ‘exist’): State (at-STATE (y)) b. act (e.g. tataku ‘pound’): Process (act (x, y))  c. achievement (e.g. sinu ‘die’): Transition  Process  State*  (P<S)  (act (x, y)) (at-STATE (y))  d. accomplishment (e.g. tsukuru ‘build’): Transition  Process*  State  (P<S)  (act (x, y)) (at-STATE (y))  - 35 -  Adverbial types selected by each subevent will be like follows (cf. Sugioka 1996).  (2) a. [instrument / depictive / manner (Process)] b. [result / material (State)] c. [time / rate (Transition)]  The lexical semantic approach illustrated above appear to be operative to all relevant cases. However, this is not so. For instance, manner and depictive adverbs in fact differ in what they specify; Manners specify the mode of action (e.g., yochiyochito ‘in an unstable manner’, hayaku ‘quickly’, etc.), and Depictives specify the temporal state of the actor (e.g., hitoride ‘alone’, damatte ‘silently’, etc.). The subtle difference, however, cannot be represented in the modification structure in (2a), where both adjuncts modify “Process”. Such a lexical account will run into trouble in face of scopal (un)ambiguity exhibited in (3)-(5). The numbers in the angle brackets represent native speaker’s judgment on a given adverbial interpretation. The questionnaire asked 22 native speakers (3rd and 4th years at Akita University, October 2004) to go over (3)-(5) and give to each adverb interpretation the score ranging over four degrees from good to bad (which corresponds to conventional indicators in the literature “OK”, “?”, “??”, “*”, respectively), for the acceptability of the English translations (i) and (ii) of each example.  (3) a. Ken ga Naomi o damatte suwar-ase-ta.  Ken Nom Naomi Acc silently sit-Caus-Past  (i)‘Ken silently made Naomi sit.’  <17, 4, 1, 0>  (ii)‘Ken made Naomi silently.’  <11, 6, 4, 1>  b. Damatte Ken ga Naomi o suwar-ase-ta.  (i) ‘Ken silently made Naomi sit.’  <15, 5, 2,0>  (ii)‘Ken made Naomi sit silently.’  <0,2,9,11>  (4) a. b.  Ken ga Naomi ni eigo o yukkurito hanas-ase-ta.  Ken Nom Naomi Dat English Acc fluently speak-Caus-Past  (i)‘Ken slowly made Naomi speak English.’ <12,6,3,1>  (ii)‘Ken made Naomi speak English slowly.’ <11,7,4,0>  Yukkurito Ken ga Naomi-ni eigo o hanas-ase-ta.  (i)‘Ken slowly made speak English.’  <13,7,2,0>  (ii)‘Ken made Naomi speak English slowly.’ <10,5,5,2>  (5) a. b.  Ken ga Jiroo o oomatade aruk-ase-ta. Ken Nom Jiro Acc with strides walk-Caus-Past (i)‘Ken made Jiro walk, with vigorous stride.’ <2,1,11,8> (ii)‘Ken made Jiro walk with vigorous stride’ <15,6,1,0> Oomatade Ken ga Jiroo o aruk-ase-ta. (i)‘Ken made Jiro walk, with vigorous stride.’ <3,2,9,8> (ii)‘Ken made Jiro walk with vigorous stride.’ <10,7,4,1>  What is remarkable is that in (3b) and (5b), unlike (4b), expected semantic scope ambiguities do not arise. The differences cannot be accounted for nicely only by assuming that linear order correlates with adverbial scope. Lexical semantic considerations for a given adverb class on the whole correlates with more restricted distribution and its interpretation. However, the above data suggest that the lexical semantic account discussed above is insufficient to handle these facts correctly. The next step will be to spell out decisive factors of licensing of each adverb in those examples.  - 36 -  PACLIC 18, December 8th-10th, 2004, Waseda University, Tokyo  2. The dual analysis of adverbs  In this section we will introduce to our alternative semantics the dual analysis of adjuncts/complements proposed in Dowty (2000), in which he argues that the meaning difference results from a complement vs. adjunct ambiguity, not an ambiguity in the adverb per se, and that the availability of both readings will depend on word order.1 Take (6) as a simple illustration, which is ambiguous between a complement reading and an adjunct reading.  (6) a student of high moral principles i) a student who studies high moral principles ii) a student who has high moral principles  [complement reading] [adjunct reading]  The point here is that the ambiguity comes from the complement vs. adjunct reading, not from the multiple meanings (if any) of the preposition of, though the word order is irrelevant in this case. Keeping this in mind, we consider the contrast between (3b) and (4b). To account for their modification relation, we suggest that adverbs in Japanese are not always adjuncts. To be more specific, VP-adverbs such as manners, modifying a specified event and seen as process-related modifiers, can behave like either complements or adjuncts in marked word order (e.g. at the sentence-initial position). This idea seems quite promising in a language like Japanese, since there is no (strong) evidence in support of structural differences between complements and adjuncts in the language. Given this, manner adverbs such as yukkurito ‘slowly’ in (4) can be analyzed either as an adjunct modifying the matrix verb (4bi) or as a complement relating to its head, i.e. the embedded verb (4bii). By contrast, a depictive adverb like damatte ‘silently’ contained in (3) is preferably analyzed as an adjunct, since it will modify event rather than process component. The adverb oomatade ‘with vigorous stride’ in (5), in contrast, is preferably analyzed as a complement (see (5bii)).2 It should be noted that we do not deny that surface word order will play some part in the interpretation of adverbs. So the scope (un)ambiguity observed in (3)-(5) have to do with a structural nature as well as a (lexical) semantic nature, which follows that c-structure as well as f-structure will participate in semantic interpretation (Yokota 2001). Another important assumption to be introduced here is that a complement generally forms a complex predicate with the head verb at the representation level of grammatical relations (e.g. f-structure for LFG), which permits the complement completing the meaning of its head in a compositional manner. Hence, it seems reasonable to assume that certain types of adverbs are subcategorized for by the head verb. We will illustrate the dual analysis hypothesis in LFG. Sentences like (4b) can involve either adverb-as-complement structure (7a) or adverb-as-adjunct (7b). (7a) in effect allows the corresponding sentence to act like a single predicate.  (7) a. PRED [yukkurito hanasu ‘speak slowly’<SUBJ, OBJ>] SUBJ [PRED ...] OBJ [PRED ...]  
Concord adverbial constructions in Korean show unbounded dependency relationships between two non-empty entities. There are two different types of unboundedness involved: one between a concord adverbial and a verbal ending and the other between the adverbial as a modifier and a predicate. In addition, these unboundedness relationships exhibit properties of “downward movement” phenomena. In this paper, we examine the Indexed Phrase Structure Grammar analysis of the constructions presented in Chae (2003, 2004), and propose to introduce a new feature to solve its conceptual problem. Then, we provide an analysis of conditional-concessive constructions, which is a subtype of concord adverbial constructions. These constructions are special in the sense that they contain a seemingly incompatible combination of a conditional adverbial and a concessive verbal ending. We argue that they are basically conditional constructions despite their concessive meaning.  
In this paper I will shed new light on the semantics of Japanese tense-aspect markers ta and teiru from dynamic semantics and contrastive perspectives. The focus of investigation will be on the essential difference between ta and teiru used in an aspectual sense related to a perfect. I analyze the asymmetry between ta and teiru with empirical data and illustrate it in the DRT framework (Discourse Representation Theory: Kamp and Reyle (1993)). Defending the intuition that ta and teiru make respectively an eventive and a stative description of eventualities, I argue that ta is committed to an assertion of the triggering event whereas teiru is not. In the case of teiru, a triggering event, if there is any, is only entailed. In DRT, ta and teiru introduce respectively an event and a state as a codition into the main DRS. Teiru may introduce a triggering event only as a codition in an embedded DRS. I also illustrate how the proposed analysis of the perfect meaning ﬁts into a more general scheme of ta and teiru. and analyze ta and teiru in a discourse. Furthermore, in DRT terms, I will compare Japanese ta / teiru with Korean perfect-related temporal markers a nohta / a twuta in light of Lee (1996). 
This paper examines the syntactic and semantic properties of a set of nouns recently called "Relational Nouns" like mother, neighbor, etc. Relational nouns denote relations between individuals, rather than sets of individuals regular nouns denote, and are referentially dependent on individual-denoting expressions. In Japanese relational nouns may appear 'bare' with no genitive possessors in the noun phrases they project, i.e., but still require the possessors somewhere in sentences. The presence of bare relational nouns allow Japanese to have a lot of peculiar constructions like multiple subject sentence, indirect passives, etc. Assuming the a version of categorial grammar in which the syntax and semantics work in tandem, we discuss the proper way to provide model-theoretic interpretations for expressions containing relational nouns under direct compositionality.  
One of the most widely used constructions in Korean is the so-called light verb construction (LVC) involving an active-denoting verbal noun (VN) together with the light verb ha-ta ‘do’. This paper ﬁrst discusses the argument composition of the LVC, mixed properties of VNs which have provided a challenge to syntactic analyses with a strict version of X-bar theory. The paper shows the mechanism of multiple classiﬁcation of category types with systematic inheritance can provide an effective way of capturing these mixed properties. The paper then restates the argument composition properties of the LVC and reenforces them with a constraint-based analysis. This paper also offers answers to the the puzzling syntactic variations in the LVC. Following these empirical and theoretical discussions is a short report on the implementation of the analysis within the LKB (Linguistics Knowledge Building) system.  
This paper is concerned with how topic/focus articulation should be optimally integrated into Japanese grammar. Based on Engdahl and Vallduv´ı’s (1996) Information Structure, we propose an analysis with the following characteristics: (i) information structure is an integral part of Japanese grammar and interacts in principled ways with both syntax and phonology, (ii) the representations of topic/focus in the information structure and its interactions with the particles wa/ga show one-to-many relation, and (iii) the ordering of grammatical functions and its interactions with other grammatical parts play an important role in determining the focus domain.  
Word Sense Disambiguation (WSD) is the task of choosing the right sense of an ambiguous word given a context. Using Naive Bayesian (NB) classifiers is known as one of the best methods for supervised approaches for WSD (Mooney, 1996; Pedersen, 2000), and this model usually uses only a topic context represented by unordered words in a large context. In this paper, we show that by adding more rich knowledge, represented by ordered words in a local context and collocations, the NB classifier can achieve higher accuracy in comparison with the best previously published results. The features were chosen using a forward sequential selection algorithm. Our experiments obtained 92.3% accuracy for four common test words (interest, line, hard, serve). We also tested on a large dataset, the DSO corpus, and obtained accuracies of 66.4% for verbs and 72.7% for nouns. 1. Introduction WSD is always a difficult and important task in natural language processing. Its task is to determine the most appropriate sense for an ambiguous word given a context. Approaches for this work include supervised learning, unsupervised learning, and combinations of them. Except for the expense involved in building labeled datasets, supervised based methods generally give results with higher precision. Many supervised learning algorithms have been applied, such as Bayesian learning, Exemplar-Based learning, Decision Trees, Decision Lists, and Neural Networks. Despite their simplicity, NB methods are still effective when applied to WSD (Mooney, 1996; Pedersen, 2000). Before presenting the previous related studies and describing our approach, we need to define some terms that are used throughout in this paper. These are topic context, local context, and collocation. The first kind of information, which is always used for determining the senses of a word, is the topic context represented by a bag of surrounding words in a large context of the ambiguous word. The other informative resource is collocation. There are various definitions of collocation, and for our approach we define collocation as a sequence of words including the ambiguous word. Several studies, such as Leacock and Chodorow (1998), used local context for disambiguating word senses. Like them, we define local context as the words (or tags of words) assigned with their position in relation to the ambiguous word in a local context. For example, suppose that we have a context of the ambiguous word interest as follows: “yields on money-market mutual funds continued to slide, amid signs that portfolio managers expect further declines in interest rates.” Then the topic context includes the words: yields, money-market, mutual, funds, continued, . . .; Collocations include the expressions: interest rates, declines in interest, in interest rates, further declines in interest rate ,. . .; Local context is represented by the pairs: (declines,-2), (in,-1), (rates,1), (further, -3), . . . Note that words in collocations and local contexts can be replaced by their part-of-speech tags, and then we will have new features. We also use other terms in the same meaning: unordered words as surrounding words, and ordered words as the words assigned with their positions. - 105 -  Mooney (1996) compared six supervised algorithms including NB, Perceptron, Decision-Tree, k Nearest-Neighbor classifier, logic-based DNF (disjunctive normal form), and CNF (conjunctive normal form), and concluded that NB and Perceptron are the best methods for WSD. He used only the words surrounding the ambiguous word as features for the classifiers. Pedersen (2000) proposed a simple but effective approach using Ensembles of NB classifiers. He showed that WSD accuracy can be improved by combining a number of simple classifiers into an ensemble. He built nine different NB classifiers based on using nine different sizes of the left and the right windows of context: 0, 1, 2, 3, 4, 5, 10, 20 and 50. His method was tested on two datasets of the words interest and line and achieved 89% and 88% accuracy, respectively. He also used only topic context for making decisions. Only a few papers have considered information other than topic context when using the NB model. Leacock and Chodorow (1998) used an NB classifier, and indicated that by combining topic context and local context they could achieve higher accuracy. In comparing NB methods with Exemplar-Based methods, Escudero (2000a) utilized most of the features used in Ng and Lee (1996), and showed that exemplar-based algorithm outperforms the NB algorithm. However, these papers did not mention how to select appropriate features, so the features used in their papers do not contain enough information and some information, such as part-of-speech, may be redundant. In many WSD studies, authors use NB as a baseline method for comparison, but many of them use NB with only topic context while adding other information to their own methods. In this paper, we focus on two problems: The first is to determine whether a WSD system using NB will improve the accuracy of its prediction if more kinds of information than usual are used. The second is to discover which kinds of information will be useful for determining the senses of an ambiguous word. We first discuss which kinds of information will be most useful for sense determination, then use a forward sequential selection algorithm to extract the best subset of features. The experiments on some datasets widely used in WSD show that the accuracies will be much improved by combining three kinds of information: topic context, local context, and collocation. One more difference from previous studies is that we do not need to use information, such as part-of-speech tags, other than the words themselves in the context. The rest of this paper is organized as follows: Section 2 briefly presents the NB classifier. Section 3 discusses choosing features for word sense disambiguation and shows the algorithm for feature selection. Section 4 shows our experiments and compares the results to those of the best previous studies when testing on four words: interest, line, serve, and hard. Section 5 shows our results and comparison with the others on the DSO corpus. Section 6 discusses the obtained results, and finally our conclusions are presented in section 7.  2. Naive Bayesian Classifier  Naïve Bayes methods have been used in most classification work and were first used for WSD by Gale et al. (1992). NB classifiers work on the assumption that all the feature variables representing a problem are conditionally independent given the classes. For word sense disambiguation, the context in which an ambiguous word occurs is represented by a vector of feature variables F=(f1, f2, . . . , fn) and the sense of the ambiguous word is represented by classification variables (s1, s2, . . ., sk). Choosing the right sense of the ambiguous word is finding the sense si that maximizes the conditional probability P(w=si|F). Suppose C is the context of the target word w, and F=(f1, f2, . . . , fn) is the set of features extracted from context C, to find the right sense s’ of w given context C, we have:  s' = arg max P(w = si | F ) si  =  arg max si  P(F | w = P(F )  si  )  P(w  =  si  )  = arg max P(F | w = si )P(w = si ) si  - 106 -  PACLIC 18, December 8th-10th, 2004, Waseda University, Tokyo  The NB classifier works with the assumption that the features are conditional independent, so that  we have：  ∏ s' = arg max P( f j | w = si )P(w = si )  si  f j ∈C  ∑ = arg max[ log(P( f j | w = si )) + log P(w = si )]  si  f j ∈C  The features for WSD using a NB algorithm are terms such as words, collocations, and words assigned with their positions which are extracted from the context of the ambiguous word. The probability of sense si, P(si), and the conditional probability of feature fj with observation of sense si,, P(fj|si), are computed via Maximum-Likelihood Estimation: P(si ) = C(si ) / N P( f j | w = si ) = C( fi , si ) / C(si )  Where C(fj,si) is the number of occurrences of fj in a context of sense sj in the training corpus, C(si) is the number of occurrences of si in the training corpus, and N is the total number of occurrences of the ambiguous word w or the size of the training dataset. To avoid the effects of zero counts when estimating the conditional probabilities of the model, when meeting a new feature fj in a context of the test dataset, for each sense si we set P(fj|w=si) equal 1/N.  3. Feature Selection Two of the most important kinds of information for determining the senses of an ambiguous word are the topic of the context and relational information representing the structural relations between the target word and the surrounding words in a local context. A bag of unordered words in the context can determine the topic of the context and collocation can determine grammatical information. Ordered words in a local context are also an important resource for relational information. We did not use syntactical relations such as verb-object, which are used in Ng and Lee (1996), because this information can be found in collocation features and a syntactic parser does not always output a correct result. Let wi be the word at position i in the context of the ambiguous word w and pi be the part-of-speech tag of wi. Note that word w appears precisely at position 0 and i will be negative (positive) if wi appears on the left (right) of w. We select the following features for the feature selection algorithm: F1 is a set of unordered words in the large context, F1= {…, w-2, w-1, w1, w2, . . .} F2 is a set of words assigned with their positions in the local context, F2 = {. . ., (w-2,-2), (w-1,-1), (w1,1), (w2,2), . . .} F3 is a set of part-of-speech tags assigned with their positions in the local context, {. . ., (p-2,-2), (p-1,-1), (p1,1), (p2,2), . . .} F4 is a set of collocations of words, F4 = {. . ., w-1w, w-2w-1w, ww1, ww1w2, . . . .} F5 is a set of collocations of part-of-speech tags, F5 = {. . ., p-1w, p-2p-1w, wp1, wp1p2, . . . .} For example, suppose that we have a context of the ambiguous word line, in which each word is assigned with its part-of-speech, as follows: coil <NNS> up<IN> the<DT> dry<JJ> line<NN> and<CC> stand<VB> midstream<NN> ,<,> rod<NN> in<IN> instant<NN> readiness <NN> .<.> Suppose that we use F2 and F3 with the same window size 2, collocation with maximum length (the length does not include the ambiguous word) 2, and F1 does not include stopped words. Then we have the features as follows: F1 = {coil, dry, stand, midstream, rod, instant, readiness} F2 = {(dry, -1), (the, -2), (and, 1), (stand, 2)} F3 = {(JJ, -1), (DT, -2), (CC, 1), (VB, 2)} F4 = {the dry line, dry line, dry line and, line and, line and stand}  - 107 -  F5 = {DT JJ line, JJ line, JJ line CC, line CC, line CC VB} In our method, the feature selection algorithm has two steps: First, we must determine the appropriate sizes for the above kinds of features. For topic context we chose 50 as the left and right window size, similar to many other WSD studies. For local context and collocation features, we used the NB classifier itself as an evaluation function to find the most appropriate sizes for the windows of features in local context and for collocation lengths. Second, from the initially selected features, we used the Forward Sequential Selection (FSS) algorithm presented in Domingos (1997) for extracting the best subset of features. In FSS, the searching process starts with an empty set. First, feature subsets with only one feature are evaluated and the best feature (f*) is selected. Then, two feature combinations of f* with the other features are tested and the best subset is selected. The search goes on by adding one more feature to the subset at each step until we do not get any more performance improvement for the system. Note that we do not use the feature selection on the whole features because of the big set of features (some thousands of features). We prefer the objective of selecting subsets based upon the kinds of features to that of extracting the best features from the whole. We followed the wrapper approach and used the NB classifier itself as the evaluation function. Therefore, feature selection was divided into two steps as follows:  Step 1: Set 4 as the maximum size for both local context and collocation length. Based on the results obtained by testing on the four words using a 10-fold cross validation, find the most appropriate sizes for local context and collocation length. Step 2: Function Automatic Feature Selection Generate a pool of feature sets PF = {F1, F2, F3, F4, F5} Initialize the set of selected feature set SF = Ø Let BestEval = 0 Repeat Let BestF = None For each F in PF and not in SF Let SF’ = SF ∪ {F} If Eval(SF’) > BestEval Then Let BestF = F Let BestEval = Eval(SF’) If BestF ≠ None Then Let SF = SF ∪ {BestF} Until BestF = None or SF = PF Return SF  At the first step of the feature selection algorithm, we used the feature set F2 as test data to get the best local context window size, and used set F4 to get the best collocation size. We implemented the algorithm with the maximum sizes of both local context and collocation runs from 1 to 4, and obtained the results shown in Table 1.  interest line serve hard Arg.  Local context maximum size  
We present two methods for automatically discovering the telic and agentive roles of nouns from corpus data. These relations form part of the qualia structure assumed in generative lexicon theory, where the telic role represents a typical purpose of the entity and the agentive role represents the origin of the entity. The ﬁrst discovery method uses hand-generated templates for each role type, and the second employs a supervised machine-learning technique. To evaluate the effectiveness of the two methods, we took a sample of 30 nouns, selected 50 verbs for each, and then generated a ranked list of verbs for a given noun. Using a variant of Spearman’s rank correlation, we demonstrate the ability of the methods to identify qualia structure. 
In this paper, we propose a new method for effectively acquiring bilingual knowledge by exploiting the dependency relations among the aligned chunks and words. We use a monolingual dependency parser to automatically obtain dependency parses of target language using chunk and word alignment. For reducing the computational complexity of structural alignment, we use a bilingual dictionary and adopt a divide-and-conquer strategy. By sharing the dependency relations of a given source sentence, we automatically obtain a dependency parse of a target sentence that is structurally consistent with the source sentence. Moreover, we extract bilingual knowledge bases from translation correspondences of singletons to surface verb subcategorization patterns by exploiting the bilingual dependency relations. To acquire reliable ones, we take a stepwise ﬁltering method based on statistical test. 
During the process of unknown word detection in Chinese word segmentation, many detected word candidates are invalid. These false unknown word candidates deteriorate the overall segmentation accuracy, as it will affect the segmentation accuracy of known words. Therefore, we propose to eliminate as many invalid word candidates as possible by a pruning process. Our experiments show that by cutting down the invalid unknown word candidates, we improve the segmentation accuracy of known words and hence that of the overall segmentation accuracy. 
This paper explores the interaction between conceptual structure and morpho-syntax. In particular, we show that ontology-based conceptual classification can be used to predict internal relations in compounds. We propose an ontology-based approach to predict the semantic relation between the two component words in Mandarin VV compounds. A Mandarin VV compound is classified according to the eventive relation between the two simplex verbs. These relations specify how the eventive meanings of the two simplex verbs combine to form the meaning of the compound. The three types of eventive relations that we deal with in this paper are: coordinate, modificational, and resultative. Since the way in which two events combine with each other depends upon their event types, we hypothesize that the eventive relations can be predicted by the conceptual classified event types of the two simplex verbs. An approach of ontology-based prediction is proposed based on this hypothesis. The assignment of ontology classification for each simplex verb is based on SUMO and Sinica BOW. The correlation between the ontology class of each verb position and each eventive type is trained and scored based on a manually tagged lexical database. We encode the ontology information of each VV compound in a 3-tuple based on these correlation scores. This 3-tuple is represented as a three-dimensional vector and used to predict the eventive type of new VV compounds. Our classification experiment on unknown VV compounds yields good recall and precision. 1. Introduction This paper explores the interaction between conceptual structure and morpho-syntax. In particular, we demonstrate that ontology-based conceptual classification can predict the internal relations in compounds. Even though the interaction between lexical semantics and syntax has been a - 151 -  fertile ground for research in both theoretical and computational linguistics, there has been limited work on the interaction on semantics and morphology. This is somewhat unexpected since both lexical semantics and morphology are lexical operations. In Chinese, for instance, study on the formation of compound verbs has so far focused on the grammatical categories and grammatical functions of the simplex words being combined. However, it is clear that the lexical meaning of the simplex words must be preserved in compound formation. In other words, simplex words are building blocks of compounds that necessarily contain their grammatical information, including lexical meaning. Given the existence of the lexical meanings in the component words, an interesting theoretical issue is how they combine to form the meaning of the compound and to examine if the combination is governed by structural or conceptual principles? In this paper, we deal with the internal relations of the VV compounds. Since the two components are the same lexical category, the internal relation is more likely to be conceptual. Our goal is to show that the different types of internal relations can be predicted with conceptual information. In particular, we want to show that ontological information can be effectively used. The ontology that we use is the Suggested Upper Merged Ontology (SUMO) developed by the IEEE upper ontology working group. We take different SUMO concept node combination for each compound verb and use it to predict the compound verb structure combinations. 2. Survey and motivation Compound verbs attracted the attention of computational linguists because they demonstrated how grammatical form could interact with lexical semantics. The prediction of the syntactic and semantic relation between the two conjunctional verbs is one of the most challenging research topics. The challenge is even more intriguing in Chinese because compound verbs lack morphological markings (Chen and Hong 1995, Chen and Chen 1997 Chang and Chen 1999, Chang et al. 2000). This means that there is no overt information at all when a compound is formed with two words with identical categories, such as VV compounds. There are five types of VV compound verbs in Modern Chinese according to Chao (1968): Coordinate, modifier-head, resultative verb, subject-predicate, and predicate-object construction. Of the five types, the first three types are composed of two verbs, hence referred to in the literature as VV compounds. Since the first three types have the same morphological composition, they must be differentiated by other means. The identification of verbal compounds is a challenging yet essential task, as shown in Zackorva et al. (1999) for Czech. However, determination of the inter-relation of the two verbal elements poses an even harder challenge. McDonald (1995) showed that such predictions are not structure based and argues for a functional theory of the acquisition of compound order in English. Huang and Lin (1992) proposed an account for the prediction of the argument structure of VV compounds based on event templates. Drawing lessons from the above studies, we adopt the premise that the ordering of the two - 152 -  PACLIC 18, December 8th-10th, 2004, Waseda University, Tokyo verbal roots in a compound verb is determined by their eventive relation. We further assume that this eventive relation can be inferred based on the conceptual location of each verb. We propose to identify the conceptual location of the component verbs based on SUMO (Suggested Upper Merged Ontology). Each verb can be assigned to an ontology node through Sinica BOW (Bilingual Ontological WordNet). Generalizations about the eventive relations typical of each type of verbal compounds (coordinate, resultative, modificational) can then be made from corpus observations and the resulting generalizations are applied to predict the compound types. 3. The Data 3.1. Compound Verbs There are 22,626 different (i.e. word types) compound verbs in the Sinica Corpus. For this study, the following forms are not included: First we exclude reduplicated compounds since the they are of identical form and the reduplication itself marks the meaning of being tentative, such as shang-shang 上上 ‘to go’, xi-xi 習習 ‘to become used to’, tao-tao 套套, ‘to cover’. Second, we exclude stative verb constructions since these are adjectival in meaning, such as huan-le 歡樂 ‘to be happy’, shu-shi 舒適 ‘to be comfortable’. Lastly, we exclude VN compounds; such as you-bing 有病, ‘to be sick’, shou-xian 收現, ‘to take cash’. The remaining 19,496 types are classified into three types: coordinate, modificational, and resultive verb construction. There are 7,898 different coordinate compounds (40.51%), e.g. zhui-sha 追殺 ‘to pursue and kill’, gong-shou 攻守 ‘to attack and defend’, shou-qu 收取 ‘to collect’, sou-bu 搜捕 ‘to track down and arrest’, sou-cang 蒐藏, to ‘search and collect’, and chan-rao 纏繞 ‘to wind around’. Secondly, there are 6,498 (33.33%) different modificational verbs compounds, e.g. wei-xiao 微笑 ‘to smile’, gai-chen 改稱 ‘to rename’, nai-cao 耐操 ‘to withstand hardship’, gai-kan 改看’to re-look’, nan-ao 難熬 ‘to be not sufferable’ and hao-zhan 好 戰 ‘to be bellicose’. Finally, there are 5,100 (26.16%) different resultative verbs, e.g. da-si 打死 ‘to hit and kill’, chi-bao 吃飽 ‘to eat to full’, qiao-po 敲破 ‘to knock and break’, chong-hun 沖昏 ‘to be over-conceited’, la-duan 拉斷 ‘to pull and break’, and bao-jin 抱緊 ‘to hold tight’. 3.2. Concept nodes mapping In order to find out the conceptual composition of the VV compounds, we first identify the two component simplex verbs V1 and V2 for each VV compound. They are assigned conceptual nodes from SUMO. In addition, the internal relation of the VV compound has already been assigned to one of the three types: coordinate, modificational, and resultative. Hence, we now have the data of how a ontology class correlates with the internal relation classification for both the V1 and V2 position. - 153 -  3.3. Data Analyses We first calculated the distribution of the grammatical categories of the verbs in VV compounds as background information. The result is given in Table 1 below.  term V frequency %  term V frequency %  term V frequency %  VA  3663  15.78 VD  417  1.80 VHC 266  1.15  VAC  71  0.31 VE  923  3.98 VI  125  0.54  VB  541  2.33 VF  225  0.97 VJ  1158  4.99  VC  8829  38.03 VG  544  2.34 VK  294  1.27  VCL  741  3.19 VH 5314  22.89 VL  107  0.46  Table 1: POS distribution of verb types in VV compound  3.4. Compiling the training data We manually analyzed 10% of all the data as the training set. In addition to the distributional data, we also found that: (1) For the coordinate VV compound, the ten most frequent concept classes to occur at the V1 position are: motion, process, getting, contest, giving, communication, attaching, touching, putting, and cutting. The ten most frequent concept classes to occur at the V2 position are: motion, process, attaching, giving, subjective assessment attribute, eating, putting, touching, removing, and intentional psychological process. (2) For the modificational VV compound, the ten most frequent concept classes to occur at the V1 position are: subjective assessment attribute, intentional process, attaching, direction change, removing, positional attribute, group, connected, repairing, and unilateral getting; The ten most frequent concept classes to occur at the V2 position are: communication, motion, walking, seeing, expressing, process, eating, putting, music, and impacting. (3) For the resultative VV compound, the ten most frequent concept classes to occur at the V1 position are: motion, removing, putting, cutting, process, impelling, eating, impacting, getting, touching; The ten most frequent concept classes to occur at the V2 position are: subjective assessment attribute, motion, origin, process, state of mind, destruction, attaching, near, organism process, and shape change.  - 154 -  PACLIC 18, December 8th-10th, 2004, Waseda University, Tokyo  4. Experiments: ontology-based prediction of compound relations  4.1 Obtaining the Likelihood Scores In order to empirically determine the correlation between simplex verb concepts and their compound relations, we randomly selected 2,400 VV compounds from our lexicon as the training data. That is, there are 800 compounds each belonging to the three types of relations. Each simplex verb (i.e. V1 and V2 for all 2,400 compounds) is assigned a suitable SUMO concept with manual verification. We observe the concept combination of these verbs in three structures, and reorganized the combination rules. The rules will be used to judge a verb which one structure it belongs. Next, we calculated the likelihood score of a semantic relation S given the concept C from a simplex verb. The scores for each semantic relation: coordinative, adjunct-head, and resultative are calculated independently for both V1 and V2 positions. The likelihood score is obtained by the following formula.  L − Scorec,s =  tf c,s n ∑ (tfi ,s )2 
Scaling wide-coverage, constraint-based grammars such as Lexical-Functional Grammars (LFG) (Kaplan and Bresnan, 1982; Bresnan, 2001) or Head-Driven Phrase Structure Grammars (HPSG) (Pollard and Sag, 1994) from fragments to naturally occurring unrestricted text is knowledge-intensive, time-consuming and (often prohibitively) expensive. A number of researchers have recently presented methods to automatically acquire wide-coverage, probabilistic constraint-based grammatical resources from treebanks (Cahill et al., 2002, Cahill et al., 2003; Cahill et al., 2004; Miyao et al., 2003; Miyao et al., 2004; Hockenmaier and Steedman, 2002; Hockenmaier, 2003), addressing the knowledge acquisition bottleneck in constraint-based grammar development. Research to date has concentrated on English and German. In this paper we report on an experiment to induce wide-coverage, probabilistic LFG grammatical and lexical resources for Chinese from the Penn Chinese Treebank (CTB) (Xue et al., 2002) based on an automatic f-structure annotation algorithm. Currently 96.751% of the CTB trees receive a single, covering and connected f-structure, 0.112% do not receive an fstructure due to feature clashes, while 3.137% are associated with multiple f-structure fragments. From the f-structure-annotated CTB we extract a total of 12975 lexical entries with 20 distinct subcategorisation frame types. Of these 3436 are verbal entries with a total of 11 different frame types. We extract a number of PCFG-based LFG approximations. Currently our best automatically induced grammars achieve an f-score of 81.57% against the trees in unseen articles 301-325; 86.06% f-score (all grammatical functions) and 73.98% (preds-only) against the dependencies derived from the f-structures automatically generated for the original trees in 301-325 and 82.79% (all grammatical functions) and 67.74% (preds-only) against the dependencies derived from the manually annotated gold-standard f-structures for 50 trees randomly selected from articles 301-325.  
There is a wide range of English reading materials for EFL (English as a foreign language) learners. However, it is difﬁcult for teachers to select appropriate materials to construct courseware that can be used for an English course. We propose a method for constructing courseware from a target vocabulary and a corpus. We used the specialized vocabulary for the Test of English for International Communication (TOEIC) and articles from The Daily Yomiuri newspaper to construct effective courseware. The constructed courseware consisted of articles in which the target vocabulary frequently occurred. Evaluation of the constructed courseware is ongoing. However, students have accepted it as an effective tool for learning the TOEIC vocabulary from real texts. 
This paper addresses the problem of compound word translation and proposes the approaches to acquiring translations. The proposed approaches focus on exploring web data and utilizing English translations to link words of the source language and the correspondences in the target language. The paper uses Japanese-Chinese language pairs for the sake of illustration and shows initial experimental results. The proposed method is language-independent and therefore can be applied to other language pairs. 
This paper proposes a method of extracting English multi-word named entities and their Japanese equivalents from a parallel corpus. The aim of our research is to extract multi-word named entities which are not listed in a dictionary of an English-to-Japanese MT system and appear infrequently in a parallel corpus. Our method makes its alignment on the basis of two kinds of external evidence provided by the context in which a bilingual pair appears, as well as two kinds of internal evidence within the pair. Each evidence is accompanied by a score, and the aggregate score is computed as a weighted sum of the scores. The appropriate weights are estimated with the logistic regression analysis. An experiment using a parallel corpus of Yomiuri Shimbun and The Daily Yomiuri satisfactorily found that 86.36% of the extracted bilingual pairs with the highest scores were judged to be correct. 
 This paper reports a preliminary result on automatic grammar induction based on the framework of Brill and Markus (1992) and binary-branching syntactic parsing of Esperanto and SaiSiyat (a Formosan language). Automatic grammar induction requires large corpus and is found implausible to process endangered minor languages. Syntactic parsing, on the contrary, needs merely tiny corpus and works along with corpora segmented by intonation-unit which results in high accuracy.  
Thai romanization is the way to write Thai language using roman alphabets. It could be performed on the basis of orthographic form (transliteration) or pronunciation (transcription) or both. As a result, many systems of romanization are in use. The Royal Institute has established the standard by proposing the principle of romanization on the basis of transcription. To ensure the standard, a fully automatic Thai romanization system should be publicly made available. In this paper, we discuss the problems of Thai Romanization. We argue that automatic Thai romanization is difficult because the ambiguities of pronunciation are caused not only by the ambiguities of syllable segmentation, but also by the ambiguities of word segmentation. A model of automatic romanization then is designed and implemented on this ground. The problem of romanization and word segmentation are handled simultaneously. A syllable-segmented corpus and a corpus of word-pronunciation are used for training the system. The accuracy of the system is 94.44% for unseen names and 99.58% for general texts. When the training corpus includes some proper names, the accuracy of romanizing unseen names was increased from 94.44% to 97%. Our system performs well because it is designed to better suit the problem. 
This paper investigates the systematic relatedness between the concessive –to (or mo)-marked polarity phenomenon and the high tone –nun (or wa)-marked Contrastive Topic phenomenon with respect to underlying concessivity and thereby derived scalarity. I also investigate the scalar features of the exhaustivity focus marker –man (or dake) in inherently scalar numeral, quantifier and predicate contexts. I further try to show a relation between polarity/negativity and implicature (suspension). 
Hsinchu City 300, Taiwan, R. O. C. E-Mail: g924706@oz.nthu.edu.tw Abstract This study attempts to account for the argument-adjunct asymmetry of Sluicing in Mandarin Chinese. Such an asymmetry is empirically demonstrated by a language-particular phenomenon, so-called shi-support, which is also the last resort (Chomsky, 1995a) of our linguistic mechanism. In the current related literature, shi-support is obligatory for wh-arguments but optional for wh-adjuncts (Wang, 2002). However, I argue that at the PF level shi-support is even optional for wh-arguments; thatis,itisonlyneededinthederivationattheLFlevel.MyanalysisiscruciallybasedonCLM’s (1995) insightful analysis of LF Copying Theory. Departing from their analysis in crucial respects, however, I argue that a covert wh-movement also takes place simultaneously with the operation of copying the antecedent IP. For reasons of economy, such a non-overt movement is preferred and is of the least efforts (Procrastinate). In addition, evidence from shi-support argues that Sluicing in Mandarin Chinese prefers LF copying rather than PF deletion. To sum up, shi-support is compulsory for wh-arguments in that the ECP (Empty Category Principle) requirement must be satisfied at the LF level owning to the intervening“barriers”(Chomsky,1986). Keywords: argument-adjunct asymmetry, shi-support, Sluicing, LF Copying Theory 1. Introduction Sluicing is an elliptical construction which involves a remnant wh-phrase followed by an empty constituent. Such an elliptical constituent, in the standard assumption, is an IP, and the remnant wh-phrase can escape from being elided in that it has either undergone wh-movement or has been I am indebted to both Prof. T.-H. Jonah Lin and Prof. Shu-Min Chang for their valuable comments and criticisms. All honors belong to them and all errors are my own responsibility. - 227 -  base-generated in [Spec, CP]. To account for such an escape of wh-phrase, two derivational theories have been generally adopted, those being PF Deletion Theory (Ross, 1969) and LF Copying Theory (CLM, 1995) respectively1. However, Sluicing in Mandarin Chinese is even more complicated since an argument-adjunct asymmetry in this wh-in-situ language is frequently demonstrated by shi-support, and little attention has been given to the point in the current related literature2. In this study, I very boldly but bravely, attempt to explore a little further into such an asymmetry in Mandarin Chinese Sluicing. My analysisiscruciallybasedonCLM’s(1995)insightful analysis of LF Copying Theory. Departing from their analysis in crucial respects, however, I argue that a covert wh-movement also takes place simultaneously with the operation of copying the antecedent IP. In addition, evidence from shi-support will help argue that Sluicing in Mandarin Chinese prefers LF copying rather than PF deletion. As for this, there will be a detailed discussion in the following subsections, and the concept of “barriers”(Chomsky, 1986) will be applied in this study as well so as to account for shi-support in Mandarin Chinese Sluicing, which has been touched from time to time but still remains unexplored and unexamined. 2. Asymmetry in Mandarin Chinese Sluicing 2.1 Wh-Construction in Mandarin Chinese It is not to be denied that wh-phrases or wh-words may be categorized into two main types, those being wh-arguments and wh-adjuncts. As Mandarin Chinese is a wh-in-situ language, the wh-phrases in this language will stay in their original places, inclusive of wh-arguments and wh-adjuncts, as shown in (1) and (2): (1) a. Zhangsan xihuan shei (wh-argument) Zhangsan like who ‘WhodoesZhangsanlike?’ b. Zhangsan xihuan Meili Zhangsan like Meili ‘ZhangsanlikesMeili.’ 
Effects of mora phonemes on Japanese word accent was analyzed statistically, utilizing a set of about 124,000 frequently used common nouns derived from the Japanese Word Dictionary edited by EDR (the Japan Electronic Dictionary Research Institute, Ltd., Japan). In this analysis, Japanese syllable was defined as preceding consonant (+semi-vowel) +following vowel, accompanied / not accompanied by mora phoneme. Mora phonemes adopted here were elongated portion of vowel: /H/, chocked sound: /Q/, syllabic nasal: /N/, and /i/ portion of diphthongs: /I/. The average ratio of attachment of accent to vowel type syllables was 13% and to semi-vowel +vowel type syllables 16%. In both types, the difference according to the kind of following vowels was not significant. As for the effect of mora phonemes, the ratios of accent attached to the syllables accompanied by mora phonemes were about several percents higher than their average for any kind of following semi-vowels and vowels. The average ratio of accent attached to vowel and semi-vowel +vowel type syllables was 14%, but in syllables of any kind of preceding consonants accompanied by mora phoneme, the ratios were more than several percents higher than the average. The words which involve the mora phonemes were categorized by each kind of mora phoneme, and compiled into the list in the order of Japanese syllabary, and the lists of different number of syllables composing each word in the order of Japanese syllabary. In addition, the list for retrieving the possible candidate words which could be replaced if the mora phonemes were missing through imperfect hearing or pronouncing. Those databases are useful for language teaching and speech training, as well as for basic research on speech processing. 
Laboratories, 12, Lane 551, Sec. 5, Min-Tsu University of Science and Technology, 43,  Rd., Yang-Mei, 326, Taoyuan, Taiwan  Sec. 4, Keelung Rd., 106, Taipei, Taiwan  jskuo@cht.com.tw  ykyang@mouse.ee.ntust.edu.tw  Abstract Term transliteration addresses the problem of converting terms in one language into their phonetic equivalents in the other language via spoken form. It is especially concerned with proper nouns, such as personal names, place names and organization names. Pronunciation variation refers to pronunciation ambiguity frequently encountered in spoken language, which has a serious impact on term transliteration. More than one transliteration variants can be generated by an out-of-vocabulary term due to different kinds of pronunciation variations. It is important to take this issue into account when dealing with term transliteration. Several models, which take pronunciation variation into consideration, are proposed for term transliteration in this paper. They describe transliteration from various viewpoints and utilize the relationships trained from extracted transliterated-term pairs. An experiment in applying the proposed models to term transliteration was conducted and evaluated. The experimental results show promise. These proposed models are not only applicable to term transliteration, but also are helpful in indexing and retrieving spoken document retrieval. 1. Introduction Machine transliteration plays an important role in machine translation. The importance of term transliteration can be realized from an analysis of the terms used in 200 qualifying sentences that were randomly selected from English-Chinese mixed news pages. Each qualifying sentence contained at least one English word. Analysis showed that 17.43% of the English terms were transliterated, and that most of them were content words (words that carry essential meaning, as opposed to grammatical function words such as conjunctions, prepositions, and auxiliary verbs) (Kuo, 2004). Lee (1998) also reported a similar result about the ratio of transliterated-term in Korean KTSET 2.0 corpus. Generally, a transliteration process starts by first examining a pre-compiled lexicon composing of many transliterated-term pairs collected manually or automatically. If a term can be found in the lexicon, one of the transliterated equivalents is selected; otherwise, the transliteration system then deals with this out-of-vocabulary term to try to generate a transliterated term via a sequence of pipelined conversions (Knight, 1998). A phoneme-based conversion approach was used in this pipelined procedure to handle English-Japanese transliteration. Kang (2000) proposed an approach using text-to-text direct correspondence for Korean-English transliteration. By utilizing the combined information of text-to-text and phoneme-based correspondences in converting basic pronunciation units or components of pronunciation units, Lee (1998) and Bilac (2004) proposed approaches to handle English-Korean and Japanese-English transliteration problems, respectively. Pronunciation variation has often been encountered in daily conversations and, therefore, in transliteration. This issue has been discussed in speech recognition (Riley, 1991), but has not been discussed extensively with respect to term transliteration. Two pronunciation variations are encountered when transliterating a source language term into its target language counterpart. One occurs at the source language side, and the other one happens during the transliteration process. For example, elision is quite common in English speech. /t/ is often elided before consonants or when they are parts of a sequence of two or three consonants (Jurafsky, 2000). Another example is an isolated pronunciation unit, such as /l/ of /poldə/, which is converted from “polder” using a letter-to-phoneme system, may or may not be transliterated into a Chinese equivalent depending on whether you pronounce this liquid syllable swiftly or not. This apparently random phenomenon can be referred to transliteration pronunciation variation. Two transliterated terms, “ 波 德 (bo-de)” and “ 波 爾 德  - 251 -  (bo-er-de),” can be obtained when dealing with “polder” transliteration. The final one depends on whether /l/ is frequently mapped to “爾(er)” or null; however, both terms are correct for term transliteration. Taking pronunciation variations into account when modeling transliteration is the main focus of this paper. Before this issue can be dealt with, a transliteration model must be trained with exposure to a large quantity of transliterated-term pairs. More than 200,000 transliterated-term pairs were extracted using different approaches (Kuo, 2003; Kuo, 2004). These paired terms, which reflected real cases, can be used as a training corpus in studying term transliteration. English and Chinese are two of the most commonly used languages around the world and have many different features. For example, each word in Chinese is monosyllabic; on the other hand, many English words have multiple syllables. To convert phonemes statistically between two languages that belong to different language families is not an easy task when dealing with term transliteration. It is even more difficult if the pronunciation variation issue is taken into account. A modular learning algorithm has been proposed for English-Japanese term transliteration (Knight, 1998). This algorithm requires all the specifications of relationships between source language texts and their phonetic representation, source language phonemes and the target language counterparts, and the target language phonemes and their textual data. Thus a source language letter-to-sound system, a cross-language phoneme conversion, and a target language sound-to-letter mapping are required to transliterate a term cross-linguistically. If any sub-system is not available, it is impossible to transliterate a term using this algorithm. However, this algorithm establishes a conceptual foundation for term transliteration. Statistical models for machine translation have been proposed in Brown (1993), and a noisy channel model for spelling correction has been proposed in Brill (2000). These models can be adapted to take pronunciation variation, which has not been discussed extensively when dealing with term transliteration, into consideration. Transliteration models for cases where one or more required components in the pipelined procedure are not available, at the same time taking pronunciation variation into account, are proposed in this paper. These models are still able to transliterate a term if a source language letter-to-sound system and/or a target language sound-to-letter mapping may be missing. The remainder of this paper is organized as follows. Section 2 describes the proposed models for term transliteration. Experimental results are presented in Section 3. Section 4 discusses on term transliteration. Conclusions are drawn in Section 5.  2. The Proposed Models  Source- la n gua ge  T exts (ST )  R1  A  Target-lan guage T exts (TT ) D  PATH R1 (A=>D)  TRANSFORMATION DESCRIPTION A cross-linguistic text-to-text direct correspondence  R5  R2 A source language letter-to-sound conversion  (A=>B)  R3  A cross-linguistic phoneme-to-phoneme  R2  R4 (B=>C)  mapping  R4 A target language phoneme (syllable) -to-text  R6  (C=>D)  transformation  R5  A cross-linguistic (source language) text-to-  B  R3  Source- la n gua ge  Phonemes (SP)  C  (A=>C)  Target-language R6  Phonemes (T P) (B=>D)  (target language) phoneme conversion A cross-linguistic (source language) phoneme-to- (target language) text mapping  Figure 1. Different models for transliteration (paths: R1, R2=>R3=>R4, R2=>R6 and R5=>R4).  Several models, taking pronunciation variation into account, for term transliteration are proposed and depicted in Figure 1. The source language and the target language referred to in this paper are English and Chinese, respectively. In Figure 1, each side in the central rectangle represents a transformation. The modular learning procedure mentioned above requires R2, R3 and R4 transformations to complete a term transliteration. Several variants, such as R5=>R4 path (referred to as LTPL), R2=>R6  - 252 -  PACLIC 18, December 8th-10th, 2004, Waseda University, Tokyo  path (referred to as LSPL), and R1 path (referred to as LL), can be obtained if any transformation in this procedure is not available and can be observed from Figure 1. In addition to these variants, the R2=>R3=>R4 path (referred to as LPPL) shown in Figure 1, is the most similar to the original pipelined procedure. LPPL is a model for term transliteration with three components, namely, a source language letter-to-sound system, a cross-linguistic phoneme-to-phoneme mapping and a target language phoneme-to-text transformation. One or more of these three components could be missed. If a source language letter-to-sound system is not available, then LPPL is reduced to a LTPL model. If a target language phoneme-to-text conversion missed, then a LSPL algorithm is used. If all three components missed, then a LL procedure with a cross-linguistic text-to-text correspondence is used.  These procedures for term transliteration first syllabify texts or phonemes and then map these text or phoneme syllables to suitable representations using the relationships described in Fig. 1. Two syllabification algorithms, text-based syllabification algorithm (TSA) and phoneme-based syllabification algorithm (PSA), are used in term transliteration. LPPL and LSPL use PSA; on the other hand, LTPL and LL use TSA. The PSA, which is similar to the algorithm described in Jurafsky (2000) with minor modifications, is the same as that described in Kuo (2004) and the TSA used is described briefly. An English word is often composed of multiple syllables; whereas, a Chinese word is monosyllabic. First, each English character in an English term is identified as a vowel, a nasal consonant or a consonant. For example, the characters “a”, “n” and “b” are viewed as a vowel, a nasal consonant and a consonant, respectively. Second, consecutive characters of the same attribute form a cluster. However, some characters, such as “ch”, “ng” and “ph”, always combine together to form complex consonants. Such complex consonants are also taken into account in the syllabification process. A Chinese syllable is composed of an initial and a final. An initial is similar to a (nasal) consonant in English, and a final is analogous to a vowel or a combination of a vowel and a nasal consonant.  A transliteration equivalent Ť in the target language with the largest probability can be selected for each token Š in the source language using the Bayes rule. Suppose that the possible transliterated-token pair is denoted by  Ĵ =(Š, Ť) º arg max p(Wt |Wsi ) ,  (1)  Wt  where Wt and Wsi are tokens of the target language and the source language, respectively, and a  transliteration of a sequence of tokens can be cascaded by token transliterations  The details of describing how to determine Ĵ by equations of each proposed transliteration algorithms, LPPL, LTPL, LSPL and LL, are described as follows.  1. LPPL (R2=>R3=>R4): Basically, LPPL is an algorithm converting terms in source language into  their phonetic equivalents in the target language via spoken form. This is the approach most similar to  the original modular learning algorithm conceptually. If all the components required for term  transliteration are available, Ĵ can be determined by means of equation (2):  Jˆ » arg max p(H si | Wsi ) p(Ht | H si ) p(Wt | Ht ) ,  (2)  Wt  where  H  i s  and  H t  are phonemes converted from terms in source language and target language,  respectively. If an existing source language letter-to-sound system is used and the target language  phoneme-to-text conversion is deterministic, the cross-linguistic phoneme-to-phoneme conversion is the main focus in this algorithm. Taking pronunciation variation into consideration, the source  language phonemes,  H  i s  =  {(H  i11 s  ,...,  H  si1n1  ),..., (H  iU1 s  ,...,  H  siUnU  )}  ,  may  contain  of  many  different  combinations in which syllables with isolated and elided consonants may or may not be silent in  transliteration. There are k syllables and U sub-sets of syllables in  H  i s  in total and the items in each  sub-set are sorted in descending order in indexes. Each sub-set of the source-language syllables is a  basic unit used to transliterate into a term in the target language. The window size of the target-language syllables varies according to the size of the selected basic unit. The syllable-to-syllable  - 253 -  probability then can be estimated by a set of context-dependent syllables trained from the  transliteration lexicon constructed by Kuo (2004). The main focus of equation (2) can be expressed by  means of equation (3):  arg  max  p( H t  |  H  i s  )  »  arg max  p(  H  
We are attempting to model travel route choice behaviour with language to describe the thinking process of travelers because words can directly and clearly reflect their psychological states from a bottom-up viewpoint. This paper shows a method that extracts impressions and feelings, i.e., cognition results of travel routes, out of open-ended questionnaire texts with a thesaurus. Complex words are also allowed as cognition results. Additional considerations and training contents are also reported. Finally, an experiment on the extraction of cognition results from unseen texts is reported. 
This study describes a general framework for adaptive word sense disambiguation. The proposed framework begins with knowledge acquisition from the relatively easy context of a corpus. The proposed framework heavily relies on the adaptive step that enriches the initial knowledge base with knowledge gleaned from the partially disambiguated text. Once adjusted to fit the text at hand, the knowledge base is applied to the text again to finalize the disambiguation decision. The effectiveness of this approach was examined through sentences from the Sinica corpus. Experimental results indicated that adaptation significantly improved the performance of WSD. Moreover, the adaptive approach, achieved an applicability improvement from 33.0% up to 74.9% with a comparable precision.  
 A novel approach to automatically extracting paired transliterated-cognates from Web corpora is proposed in this paper. One of the most important issues addressed is that of taking multiple pronunciation characteristics into account. Terms from various languages may pronounce very differently. Incorporating the knowledge of word origin may improve the pronunciation accuracy of terms. The accuracy of generated phonetic information has an important impact on term transliteration and hence transliterated-term extraction. Transliterated-term extraction is a fundamental task in natural language processing to extract paired transliterated-terms in studying term transliteration. An experiment on transliterated-term extraction from two kinds of Web resources, Web pages and anchored texts, has been conducted and evaluated. The experimental results show that many transliterated-term pairs, which cannot be extracted using the approach only exploiting English pronunciation characteristics, have been successfully extracted using the proposed approach in this paper. By taking multiple language-specific pronunciation transformations into account may further improve the output of the transliterated-term extraction.  1. Introduction  부산 ( K o re an) P usan  Sfo rza (Italian) Sfo rza  サンワ (Japanese) 湖 北 (C hinese) Fans  (E nglish)  Sanw a  H ubei  Fans  Te xts In O riginal L angua ge s  M e dia (Inte rne t/Bo o ks /N E W S /...) in E nglis h/C hine se /F re nc h/J apane s e/ G e rm an /K o re an /Italian ....  M edia C hannel ( Tran s latio n /Tran s lite ratio n )  P usan 釜山  Sfo rza  史佛拉  Sanw a H ubei Fans  三和  湖北  粉絲  M ixed-Language Text Co rpora  Figure 1. A conceptual flow for transliterating terms in multi-languages into Chinese. Machine transliteration is one part of machine translation. Term transliteration in machine transliteration addresses the problem of converting terms in one language into their phonetic equivalents in the other language via spoken form. It is especially concerned with proper nouns, such as personal names, place names and organization names. Transliterated-term extraction, which is a fundamental task in studying term transliteration, has been focused on producing a large quantity of paired transliterated-cognates in order to observe various relations between cognate pairs. A transliteration lexicon, which is composed of many transliterated-term pairs, is important to the researches on term transliteration. Virga (2003) and Meng (2001) have explored relationships trained from a transliteration corpus collected manually to cross-language information retrieval and spoken document retrieval, respectively. However, it is time- and labor-consuming to prepare a transliteration  - 275 -  lexicon manually. It will be helpful if a transliteration lexicon can be compiled automatically. English is one of commonly used languages around the world. Many terms are translated or transliterated into English first and then disseminated to the rest of the world. People speaking non-English languages might frequently use borrowed terms that were transliterated from English, terms that actually originated in languages other than English. As international communications increase, many foreign words may be imported from other languages through the translation or transliteration processes. Figure 1 depicts this situation. Different transliterated terms may appear in a language when foreign words were transliterated directly from the original languages or indirectly from cognates, which were transliterated into languages other than the original languages. People may not distinguish between these multiple cognates, which originated from the same term in the source language. For example, Firenze in Italian was transliterated into “斐冷翠” (fei-leng-cui in Han-yu pinyin) in Chinese; however, Firenze was also transliterated into “Florence” in English and then “Florence” was transliterated into “弗羅倫斯” (fu-luo-lun-si) in Chinese. People may not know “斐冷翠” and “弗羅倫斯” have been used to refer to the same place if they do not study these terms carefully.  Es paña  F i renze  Spain English Florence  Vocabularies  DT  IT  DT IT  西班牙 西班(牙)  弗羅倫斯 斐冷翠  *DT: Direct Transliteration, IT: Indirect Transliteration  Figure 2. Chinese transliterated-terms transliterated directly from their original language terms or indirectly from their English cognates.  Another interesting example is the Chinese phonetic equivalents of Spain might be transliterated from Spanish or/and English. The problem is that “Spain” in English is syllabified into two syllables. These two syllables can be converted into “西班” (xi-ban) and cannot be mapped to “西班牙” (si-ban-ya), which has three syllables. The correct Chinese term is transliterated from Spanish, even though it is pronounced more like the English word. Figure 2 shows two examples of direct and indirect transliterations reflecting real cases. From these two examples, it implies that it will be helpful if source language terms originated from different languages should be pronounced using their native pronunciation system when extracting transliterated-term pairs.  Table 1. English terms transliterated from Mandarin-Chinese and its dialects.  Szechuan (四川) Typhoon (颱風) Taiwan (台灣)  Kung fu Guanxi Feng shui  Tofu  (功夫) (關係) (風水)  (豆腐)  Qikong Taichi Shaolin Hong-Kong  (氣功) (太極) (少林)  (香港)  Whagwei Gezaixi Owanchian Jungtsu  (碗粿) (歌仔戲) (蚵仔煎) (粽子)  In addition to English, Chinese is also one of the commonly used languages around the world. Many English terms have been borrowed from Chinese and many Chinese terms have been imported  - 276 -  PACLIC 18, December 8th-10th, 2004, Waseda University, Tokyo from English. Table 11 lists some English transliterated-terms and their Chinese counterparts. These terms have been prevalently used in English, especially when talking about Chinese issues. “Kung fu” and “Feng shui” are two typical examples. English terms, which originated from Chinese, have been used commonly in daily conversations. Some of these terms may pronounce very different from that of native English terms. If these terms input to an English letter-to-sound system, which are trained from a corpus composed of large English terms, a sequence of incorrect phonemes may be obtained. Attention should be paid to these terms when dealing with English-Chinese transliterated-term extraction. For example, “草屯” (“Cao-tun”) may pronounce as “/cao-tun/” (in Hanyu). If the English counterpart of this term, “Cao-tun”, is not recognized that it is represented in pinyin and it may pronounce erroneously as “/g-tn/” using an English letter-to-sound system. Incorporating the knowledge of word origin may improve the pronunciation accuracy of terms (Llitjos, 2001). The accuracy of generated phonetic information has an important impact on term transliteration and hence transliterated-term extraction. In order to improve the performance of the transliterated-term extraction, taking the knowledge of word origin into consideration when dealing with the extraction of English-Chinese paired transliterated-terms is the most important focus in this paper. Transliterated-term extraction using parallel corpora has been conducted (Lee, 2003). Generally speaking, parallel corpora are smaller in scale and less versatile in coverage as compared to non-parallel corpora. Transliterated-term extraction using non-parallel corpora has also been conducted (Kuo, 2003). Kuo (2003) successfully extracted transliterated-term pairs from Web pages collected by a software spider with the aid of confusion matrices generated by a speech recognition system. Examining those cases, which failed to be extracted transliterated-term pairs, it showed that it is difficult to generate phonetic information from English terms, which were transliterated from Chinese and may not follow the western style pronunciation rules, correctly. “Yungan”, which is such an English term, can be segmented into “Yun-gan” (雲岡) or “Yung-an” (永安). In this paper, a novel approach, which uses multiple pronunciation transformations for terms originated from different languages, is proposed for transliterated-term extraction from Web corpora. Different pronunciation methods are used here to generate phonetic information when dealing with terms from various languages. If a term can be transliterated from Chinese into English, it may be represented in pinyin. However, various pinyin representations have been proposed and used to represent a Chinese term. A procedure is used to automatically detect which pinyin system is used to represent the term in order to generate correctly phonetic information in term extraction. The remainder of the paper is organized as follows: Section 2 describes how English-Chinese transliterated term pairs can be extracted automatically using multiple pronunciation transformations. Experimental results obtained using Web corpora are presented in section 3. Section 4 provides an extensive discussion of transliterated-term extraction. Conclusions are drawn in section 5. 2. The Proposed Approach An approach, which uses different pronunciation transformations for terms originated from different languages, is described in this section. English is the source language and Chinese is the target language referred to in this paper. There are two pronunciation approaches used to process English terms, which may be generated natively or borrowed from other languages. MBRDICO (Pagel, 1998) is the English letter-to-sound system used to convert English strings into phonemes, and a Chinese pinyin detection algorithm, which is used first to segment this term into syllables using left-to-right longest matching algorithm and then to detect which pinyin scheme is used to represent a Chinese term in English. When a term is transformed into phonemes using an English letter-to-sound systems or a Chinese pinyin detection algorithm, then the degree of similarity between paired terms can be calculated. 
Chinese-English parallel corpora are key resources for Chinese-English cross-language information processing, Chinese-English bilingual lexicography, Chinese-English language research and teaching. But so far large-scale Chinese-English corpus is still unavailable yet, given the difficulties and the intensive labours required. In this paper, our work towards building a large-scale Chinese-English parallel corpus is presented. We elaborate on the collection, annotation and mark-up of the parallel Chinese-English texts and the workflow that we used to construct the corpus. In addition, we also present our work toward building tools for constructing and using the corpus easily for different purposes. Among these tools, a parallel concordance tool developed by us is examined in detail. Several applications of the corpus being conducted are also introduced briefly in the paper. 
In order to assess spoken skills of learners of Japanese effectively and more efficiently the Institute for DECODE (Institute for Digital Enhancement of Cognitive Development) at Waseda University is collaborating with Ordinate Corporation to develop and validate an automated test of spoken Japanese, SJT (Spoken Japanese Test). The SJT is intended to measure a test-taker’s facility in spoken Japanese, that is listening and speaking skills in daily conversation, in a quick, accurate and reliable manner. In this paper, we discuss the purposes for developing the SJT, the mechanism of a fully automated test, and the test development processes, including item development and implementation. 
We developed three systems based on automatic paraphrasing techniques to help English learners and English-language beginners. One system extracts personal error patterns in the user’s English usage. The second transforms English sentences containing the letters “l” and “r” into sentences containing fewer instances of these letters, which Japanese people have trouble pronouncing properly in English. This system could be used, for example, to transform a draft of a presentation that a Japanese speaker was to present to an audience. The third is an annotation system that provides deﬁnition sentences of diﬃcult English words, making them easier to understand. We believe that these systems will be useful both for learners of English and in studies on second-language acquisition. 
This paper presents LTAG semantics of focus and focus-sensitive quantifiers which adopts alternative semantics of focus (Rooth 1985 and subsequent work). It proposes that focused lexical items make its contribution at the level of elementary trees, so that each elementary tree is associated with two semantic representations: its ordinary semantic representation and its focus representation. Based on the semantic framework, discussed in Kallmeyer and Joshi 2003 and Kallmeyer and Romero 2004, the paper develops a compositional analysis of focus representations, and extends this analysis to focusing adverbs and adverbs of quantification. 
In this paper we discuss the use of intersection as a tool for modeling syntactic phenomena and folding of biological molecules. We argue that intersection is useful but easily overestimated, because intersection coordinates grammars via their string languages, and if strong generative capacity is given priority over weak generative capacity, this kind of coordination turns out to be rather limited. We give two example uses of intersection which overstep this limit, one using CFGs and one using a range concatenation grammar (RCG). We conclude with an analysis and example of the diﬀerent kinds of parallelism available in an RCG.  
This study focuses on the class of string languages generated by TAGs. It examines whether the class of string languages can be generated by some epsilon-free grammars and by some lexicalized grammars. Utilizing spine grammars, this problem is solved positively. This result for spine grammars can be translated into the result for TAGs. 
Ambiguous keyboards, i.e. several letters host on the same key (cf. telephone keyboard), produce candidate lists with words that match the entered code. These lists have to be disambiguated by the user for the intended word. Consequently, the primary goal is to order these candidates in a way that the most appropriate words are placed on top of the suggestion list for minimal selection costs or to postpone the attention to suggestions to a ﬁnal editing step. This paper reports on promising results for this goal by inspecting the whole sentence on the basis of supertagging and lightweight dependency analysis. 
In this paper we propose a syntactic and semantic analysis of complex questions. We consider questions involving pied piping and stranding and we propose elementary trees and semantic representations that allow to account for both constructions in a uniform way. 
Tree Adjoining Grammars (TAG) are known not to be powerful enough to deal with scrambling in free word order languages. The TAGvariants proposed so far in order to account for scrambling are not entirely satisfying. Therefore, an alternative extension of TAG is introduced based on the notion of node sharing. Considering data from German and Korean, it is shown that this TAG-extension can adequately analyse scrambling data, also in combination with extraposition and topicalization. 
Several grammars have been proposed for representing RNA secondary structure including pseudoknots. In this paper, we introduce subclasses of multiple context-free grammars which are weakly equivalent to these grammars for RNA, and clarify the generative power of these grammars as well as closure property. 
We investigate an approach to parsing in which lexical information is used only in a ﬁrst phase, supertagging, in which lexical syntactic properties are determined without building structure. In the second phase, the best parse tree is determined without using lexical information. We investigate different probabilistic models for adjunction, and we show that, assuming hypothetically perfect performance in the ﬁrst phase, the error rate on dependency arc attachment can be reduced to 2.3% using a full chart parser. This is an improvement of about 50% over previously reported results using a simple heuristic parser. 
This paper proposes a process to build semantic representation for Tree Adjoining Grammars (TAGs) analysis. Being in the derivation tree tradition, it proposes to reconsider derivation trees as abstract terms (λ-terms) of Abstract Categorial Grammars (ACGs). The latter offers a ﬂexible tool for expliciting compositionality and semantic combination. The chosen semantic representation language here is an underspeciﬁed one. The ACG framework allows to deal both with the semantic language and the derived tree language in an equivalent way: as concrete realizations of the abstract terms. Then, in the semantic part, we can model linguistic phenomena usually considered as difﬁcult for the derivation tree approach. Introduction When dealing with the computation of semantic representation for TAG analysis, two main approaches are usually considered. The ﬁrst one gives the derivation trees a central role for the computation (Schabes and Shieber, 1994; Candito and Kahane, 1998; Kallmeyer, 2002; Joshi et al., 2003), and the second one relies on a direct computation on the derived tree (Frank and van Genabith, 2001; Gardent and Kallmeyer, 2003). The present article wants to explore the intuition that the two approaches are indeed bound: derivation trees are a speciﬁcation of the operations that are to be processed, but the derived trees hold the precise descriptions of these operations. We propose to exhibit those operations by separating them from the syntactic trees. Then, under the speciﬁcations given by the derivation trees, we show how to build the semantic representations. The tools we use for this purpose are Abstract Categorial Grammars (ACGs) (de Groote, 2001). The main  feature of an ACG is to generate two languages: an abstract language and an object language. Whereas the abstract language may appear as a set of grammatical or parse structures, the object language may appear as its realization, or the concrete language it generates. For instance, (de Groote, 2002) proposes as object language the tree language of TAGs (encoded in linear λ-terms) and, as abstract language, a tree language (also encoded in linear λ-terms) and very close to the derivation tree language. In this paper, we use the same abstract language, and, as object language, λ-terms that encode underspeciﬁed semantic representation as in (Bos, 1995; Blackburn and Bos, 2003). Thus, we realize our program to separate the computation speciﬁcation and the operation definition. As for Montague’s semantics, missing information is represented by bound λ-variables and replacement and variable catching by application instead of uniﬁcation (as in (Frank and van Genabith, 2001; Gardent and Kallmeyer, 2003)). The next section brieﬂy describes the underlying principles of ACGs. Then we show how syntactic parts of TAGs are modelled and how we translate, through the abstract terms (our derivation trees), the combination of intial and auxiliary trees to their semantic representations by means of some examples. 
This paper explores an optimality-theoretic approach to syntax based on Tree-Adjoining Grammars (TAG), where two separate optimizations are responsible for the construction of local pieces of tree structure (elementary trees) and the combination of these pieces of structure. The local optimization takes a non-recursive predicate-argument structure (PA-chunk) as an underlying representation and chooses the best tree structure realizing it. The linking optimization takes as an underlying representation a tree whose nodes are labeled by PA-chunks and chooses among a set of structurally isomorphic TAG derivation trees. We provide formal deﬁnitions of the OTAG system and prove equivalence in strong generative capacity between OTAG and TAG. Finally, we apply the mechanics of the formal system to the analysis of cross-serial dependencies in Swiss-German. 
In this paper, we show how to formalize reconstruction effects in an LTAG semantics. We derive a lexical entry and semantic speciﬁcation for how many, which introduces two quantiﬁcational elements. We also show how they interact compositionally with other scopal items, e.g. modal and attitude verbs in a question. The use of an underspeciﬁed semantics allows the compact representation of scope ambiguities. We demonstrate how this also enables us to obtain the correct readings in embedded questions. 
Tree transducer formalisms were developed in the formal language theory community as generalizations of ﬁnite-state transducers from strings to trees. Independently, synchronous tree-substitution and -adjoining grammars arose in the computational linguistics community as a means to augment strictly syntactic formalisms to provide for parallel semantics. We present the ﬁrst synthesis of these two independently developed approaches to specifying tree relations, unifying their respective literatures for the ﬁrst time, by using the framework of bimorphisms as the generalizing formalism in which all can be embedded. The central result is that synchronous treesubstitution grammars are equivalent to bimorphisms where the component homomorphisms are linear and complete.  tured representations, for instance trees. Semantic interpretation can be viewed as a transduction from a syntactic parse tree to a tree of semantic operations whose simpliﬁcation to logical form can be viewed as a further transduction. This raises the question as to whether there is a universal formalism for NL tree transductions that can play the same role there that WFST plays for string transduction. In this paper, we investigate the formal properties of synchronous tree-substitution and -adjoining grammars (STSG and STAG) from this perspective. In particular, we look at where the formalisms sit in the pantheon of tree transduction formalisms. As a particular result, we show that, contra previous conjecture, STSG is not equivalent to simple nondeterministic tree transducers, and place for the ﬁrst time STSG and STAG into the tree transducer family. Essential to this uniﬁcation of the two types of formalisms is the bimorphism characterization of tree transducers, little known outside the formal language theory community.  
Tree adjoining grammar parsers can use a supertagger as a preprocessor to help disambiguate the category1 of words and thus speed up the parsing phase dramatically. However, since the errors in supertagging propagate to this phase, it is vital to keep the error rate of the supertagger phase reasonably low. With very large tagsets coming from extracted grammars, this error rate can be of almost 20%, using standard Hidden Markov Model techniques. To combat this problem, we can trade a higher precision for increased ambiguity in the supertagger output. I propose a new approach to introduce ambiguity in the supertags, looking for a suitable trade-off. The method is based on a representation of the supertags as a feature structure and consists in grouping the values, or a subset of the values, of certain features, generally those hardest to predict.  thus becomes the fact that when the tagset is very large (e.g. about 5,000 different trees), the precision of the supertagger output is so low (about 80%) that the parser fails on most sentences. The supertagger we use is based on a Hidden Markov Model (HMM) tagger trained on a grammar extracted (Chen, 2001) from the Wall Street Journal part of the Penn Treebank (Marcus et al., 1993) and the parser is the one described in (Nasr et al., 2002). 2 Supertagging and Very Large Tagsets If HMM part of speech tagging has been proven quite successful, supertagging is more problematic for two main reasons. • (A) The large number of categories which characterizes supertagging entails statistical problems, but for the result to be useful in helping parse realworld texts, a medium-sized or small grammar (with e.g. 300 or 400 different elementary trees) seems insufﬁcient.  
This paper presents a method of improving the quality of subcategorization frames (SCFs) acquired from corpora in order to augment a lexicon of a lexicalized grammar. We ﬁrst estimate a conﬁdence value that a word can have each SCF, and create an SCF conﬁdence-value vector for each word. Since the SCF conﬁdence vectors obtained from the lexicon of the target grammar involve co-occurrence tendency among SCFs for words, we can improve the quality of the acquired SCFs by clustering vectors obtained from the acquired SCF lexicon and the lexicon of the target grammar. We apply our method to SCFs acquired from corpora by using a subset of the SCF lexicon of the XTAG English grammar. A comparison between the resulting SCF lexicon and the rest of the lexicon of the XTAG English grammar reveals that we can achieve higher precision and recall compared to naive frequency cut-off. 
A central component of Kallmeyer and Joshi 2003 is the idea that the contribution of a quantifier is separated into a scope and a predicate argument part. Quantified NPs are analyzed as multi-component TAGs, where the scope part of the quantifier introduces the proposition containing the quantifier, and the predicate-argument part introduces the restrictive clause. This paper shows that this assumption presents difficulties for the compositional interpretation of NP coordination structures, and proposes an analysis which is based on LTAG semantics with semantic unification, developed in Kallmeyer and Romero 2004. 
This paper proposes to give an analysis of VP coordination in the LTAG semantics framework of (Kallmeyer and Joshi, 2003). First the syntax of VP coordination is described using an operation called conjoin. Then we discuss interactions of coordination scope and quantiﬁer scope in simple sentences and their analysis in LTAG. Finally we point out coordination scope ambiguities in embedded sentences that present a problem for the present analysis. 
One approach to veriﬁcation and validation of language processing systems includes the veriﬁcation of system resources. In general, the grammar is a key resource in such systems. In this paper we discuss veriﬁcation of lexicalized tree adjoining grammars (LTAGs) (Joshi and Schabes, 1997) as one instance of a system resource, and as one phase of a larger veriﬁcation effort. 
This paper describes work on creating elementary trees for adjective and predicative noun families (Barrier, 2002; Barrier and Barrier, 2003) using Metagrammars, for the FTAG grammar (Abeille´, 1991; Abeille´, 2002). Based on the Candito’s work on Metagrammars (Candito, 1996; Candito, 1999a), it adds a fourth dimension, specially designed for word order speciﬁcation.  nal syntactic function of the arguments nodes, according to their redistribution. The hand-written hierarchy was initially divided into 3 dimensions, and has been more recently extended to 4 dimensions (Barrier and Barrier, 2003): ¢ Dimension 1 : initial subcategorization. ¢ Dimension 2 : redistribution of functions. ¢ Dimension 3 : Surface realizations of syntactic functions.  
I show that sentences with two subordinate clauses may receive two syntactic analyses, and that each syntactic analysis may receive two semantic interpretations. Hence, I put forward an underspeciﬁed semantic representation such that each syntactic analysis receives only one underspeciﬁed interpretation.  dependency structures. Section 4 studies the mapping between syntax and semantics and shows that each syntactic analysis for sentences with two subordinate clauses receives two semantic interpretations. Hence the need of an underspeciﬁed semantic representation (henceforth USR). Section 5 presents this USR. Finally, Section 6 compares this work with D-LTAG (Webber et al., 2003). 2 Syntax (in LTAG)  
This paper introduces well-ordered derivation trees and makes use of this concept in a novel axiomatization of the TAG parsing problem as a constraint satisfaction problem. Contrary to prior approaches, our axiomatization focuses on the derivation trees rather than the derived trees. Well-ordered derivation trees are our primary models, whereas the derived trees serve solely to determine word order. 
This paper sets up a framework for LTAG (Lexicalized Tree Adjoining Grammar) semantics that brings together ideas from different recent approaches addressing some shortcomings of TAG semantics based on the derivation tree. Within this framework, several sample analyses are proposed, and it is shown that the framework allows to analyze data that have been claimed to be problematic for derivation tree based LTAG semantics approaches. 
Junction Grammar (JG) combines junction operators, multiple linked syntax/semantics trees, and ﬂexible traversal algorithms. The multiple tree and ﬂexible ordering characteristics of MC-TAG and other TAG extensions are somewhat analogous. This paper proposes that these similarities can be integrated to form a new approach, JG-TAG. Relevant aspects of both theories and the proposed new model are discussed in turn, and representative examples are sketched. 
We present a method to approximate a LTAG grammar by a CFG. A key process in the approximation method is ﬁnite enumeration of partial parse results that can be generated during parsing. We applied our method to the XTAG English grammar and LTAG grammars which are extracted from the Penn Treebank, and investigated characteristics of the obtained CFGs. We perform CFG ﬁltering for LTAG by the obtained CFG. In the experiments, we describe that the obtained CFG is useful for CFG ﬁltering for LTAG parser. 
We explore the possibility of accounting for scrambling patterns in German using multidimensional grammars. The primary desirable characteristic of this approach is that it employs elementary structures with a single uniform component and combining operations which operate at a single point in the derived structure. As a result, we obtain an analysis that is much closer in spirit to ordinary TAG and to the intuitions of TAG based linguistics. Ultimately, we obtain an account in which the variations in word order are consequences of variations of a small set of structural parameters throughout their ranges. 
This papers presents a compositional semantic analysis of interrogatives clauses in LTAG (Lexicalized Tree Adjoining Grammar) that captures the scopal properties of wh- and nonwh-quantiﬁcational elements. It is shown that the present approach derives the correct semantics for examples claimed to be problematic for LTAG semantic approaches based on the derivation tree. The paper further provides an LTAG semantics for embedded interrogatives.  
This paper presents the mappings between the syntactic information of our broad-coverage domain-independent verb lexicon, VerbNet, to Xtag trees. This mapping between complementary resources allowed us to increase the syntactic coverage of our verb lexicon by capturing transformations of the basic syntactic description of the verbs present in VerbNet. In addition, having these two resources mapped allows the semantic predicates present in our lexicon to be used to disambiguate Xtag verb senses.  PropBank (Kingsbury and Palmer, 2002) through a systematic mapping between the two resources. The syntactic frames in our verb lexicon account for over 78% exact matches to the frames found in PropBank (Kipper et al., 2004). A natural extension of VerbNet’s syntactic frames is to incorporate the possible transformations of each frame. The Xtag grammar (XTAG Research Group, 2001) presents a large existing grammar for English verbs that accounts for just that richness of constructions. Mapping our syntactic frames to the Xtag trees greatly increases the robustness of our resource by capturing such transformations.  
In this paper we introduce a naive algorithm for nondeterminisctic LTAG derivation tree extraction from the Penn Treebank and the Proposition Bank. This algorithm is used in the EM models of LTAG Treebank Induction reported in (Shen and Joshi, 2004). Given the trees in the Penn Treebank with PropBank tags, this algorithm generates shared structures that allow efﬁcient dynamic programming in the EM models. 
The proper linguistic representation of ellipsis has been a source of debate for years (Hankamer and Sag, 1976), with ellipsis theories broadly categorizable as being either syntactic or semantic, depending on whether or not an elided constituent is held to contain articulated syntactic structure. In this paper, I combine ideas from both syntactic and semantic theories in order to (1) account for the data that suggest there is syntactic structure within elided constituents, and (2) do so in a manner that preserves one of the prime advantages of existing semantic theories, namely straightforward declarative and procedural intepretations. This is accomplished by stating both semantic and syntactic identity conditions on ellipsis. The syntactic condition is formulated within a desription theory approach to grammar, as in the formalisms proposed in (Vijay-Shanker, 1992), (Rambow et al., 2001) and (Muskens, 2001). 
Generating elementary trees for wide-coverage Lexicalized Tree Adjoining Grammars (LTAG) is one of the great concerns in the TAG project. We know that the Korean LTAG developed in (Han C.-H. et al., 2000) was not sufﬁcient to handle various syntactic structures. Therefore, a Korean Meta-Grammar (KMG) is proposed to generate and maintain a large number of elementary tree schemata. Describing Korean MG with more precise tree families and with class encoding Korean syntactic properties leads to a larger coverage capacity for Korean LTAG. 
We present a novel discriminative approach to parsing inspired by the large-margin criterion underlying support vector machines. Our formulation uses a factorization analogous to the standard dynamic programs for parsing. In particular, it allows one to eﬃciently learn a model which discriminates among the entire space of parse trees, as opposed to reranking the top few candidates. Our models can condition on arbitrary features of input sentences, thus incorporating an important kind of lexical information without the added algorithmic complexity of modeling headedness. We provide an eﬃcient algorithm for learning such models and show experimental evidence of the model’s improved performance over a natural baseline model and a lexicalized probabilistic context-free grammar. 
Active learning (AL) promises to reduce the cost of annotating labeled datasets for trainable human language technologies. Contrary to expectations, when creating labeled training material for HPSG parse selection and later reusing it with other models, gains from AL may be negligible or even negative. This has serious implications for using AL, showing that additional cost-saving strategies may need to be adopted. We explore one such strategy: using a model during annotation to automate some of the decisions. Our best results show an 80% reduction in annotation cost compared with labeling randomly selected data with a single model. 
Most statistical parsers have used the grammar induction approach, in which a stochastic grammar is induced from a treebank. An alternative approach is to induce a controller for a given parsing automaton. Such controllers may be stochastic; here, we focus on greedy controllers, which result in deterministic parsers. We use decision trees to learn the controllers. The resulting parsers are surprisingly accurate and robust, considering their speed and simplicity. They are almost as fast as current part-ofspeech taggers, and considerably more accurate than a basic unlexicalized PCFG parser. We also describe Markov parsing models, a general framework for parser modeling and control, of which the parsers reported here are a special case. 
This paper explores the large-scale acquisition of sense-tagged examples for Word Sense Disambiguation (WSD). We have applied the “WordNet monosemous relatives” method to construct automatically a web corpus that we have used to train disambiguation systems. The corpus-building process has highlighted important factors, such as the distribution of senses (bias). The corpus has been used to train WSD algorithms that include supervised methods (combining automatic and manuallytagged examples), minimally supervised (requiring sense bias information from hand-tagged corpora), and fully unsupervised. These methods were tested on the Senseval-2 lexical sample test set, and compared successfully to other systems with minimum or no supervision. 
Broad-coverage repositories of semantic relations between verbs could benefit many NLP tasks. We present a semi-automatic method for extracting fine-grained semantic relations between verbs. We detect similarity, strength, antonymy, enablement, and temporal happens-before relations between pairs of strongly associated verbs using lexicosyntactic patterns over the Web. On a set of 29,165 strongly associated verb pairs, our extraction algorithm yielded 65.5% accuracy. Analysis of error types shows that on the relation strength we achieved 75% accuracy. We provide the resource, called VERBOCEAN, for download at http://semantics.isi.edu/ocean/. 
Paraphrase recognition is a critical step for natural language interpretation. Accordingly, many NLP applications would beneﬁt from high coverage knowledge bases of paraphrases. However, the scalability of state-of-the-art paraphrase acquisition approaches is still limited. We present a fully unsupervised learning algorithm for Web-based extraction of entailment relations, an extended model of paraphrases. We focus on increased scalability and generality with respect to prior work, eventually aiming at a full scale knowledge base. Our current implementation of the algorithm takes as its input a verb lexicon and for each verb searches the Web for related syntactic entailment templates. Experiments show promising results with respect to the ultimate goal, achieving much better scalability than prior Web-based methods. 
We describe how simple, commonly understood statistical models, such as statistical dependency parsers, probabilistic context-free grammars, and word-to-word translation models, can be effectively combined into a uniﬁed bilingual parser that jointly searches for the best English parse, Korean parse, and word alignment, where these hidden structures all constrain each other. The model used for parsing is completely factored into the two parsers and the TM, allowing separate parameter estimation. We evaluate our bilingual parser on the Penn Korean Treebank and against several baseline systems and show improvements parsing Korean with very limited labeled data. 
We compare and contrast two different models for detecting sentence-like units in continuous speech. The ﬁrst approach uses hidden Markov sequence models based on N-grams and maximum likelihood estimation, and employs model interpolation to combine different representations of the data. The second approach models the posterior probabilities of the target classes; it is discriminative and integrates multiple knowledge sources in the maximum entropy (maxent) framework. Both models combine lexical, syntactic, and prosodic information. We develop a technique for integrating pretrained probability models into the maxent framework, and show that this approach can improve on an HMM-based state-of-the-art system for the sentence-boundary detection task. An even more substantial improvement is obtained by combining the posterior probabilities of the two systems. 
In this paper we investigate whether paragraphs can be identiﬁed automatically in different languages and domains. We propose a machine learning approach which exploits textual and discourse cues and we assess how well humans perform on this task. Our best models achieve an accuracy that is signiﬁcantly higher than the best baseline and, for most data sets, comes to within 6% of human performance. 
We apply a novel variant of Random Forests (Breiman, 2001) to the shallow semantic parsing problem and show extremely promising results. The ﬁnal system has a semantic role classiﬁcation accuracy of 88.3% using PropBank gold-standard parses. These results are better than all others published except those of the Support Vector Machine (SVM) approach implemented by Pradhan et al. (2003) and Random Forests have numerous advantages over SVMs including simplicity, faster training and classiﬁcation, easier multi-class classiﬁcation, and easier problem-speciﬁc customization. We also present new features which result in a 1.1% gain in classiﬁcation accuracy and describe a technique that results in a 97% reduction in the feature space with no signiﬁcant degradation in accuracy. 
We present an unsupervised method for labelling the arguments of verbs with their semantic roles. Our bootstrapping algorithm makes initial unambiguous role assignments, and then iteratively updates the probability model on which future assignments are based. A novel aspect of our approach is the use of verb, slot, and noun class information as the basis for backing off in our probability model. We achieve 50–65% reduction in the error rate over an informed baseline, indicating the potential of our approach for a task that has heretofore relied on large amounts of manually generated training data. 
We present a data and error analysis for semantic role labelling. In a ﬁrst experiment, we build a generic statistical model for semantic role assignment in the FrameNet paradigm and show that there is a high variance in performance across frames. The main hypothesis of our paper is that this variance is to a large extent a result of differences in the underlying argument structure of the predicates in different frames. In a second experiment, we show that frame uniformity, which measures argument structure variation, correlates well with the performance ﬁgures, effectively explaining the variance. 
Accurate dependency recovery has recently been reported for a number of wide-coverage statistical parsers using Combinatory Categorial Grammar (CCG). However, overall ﬁgures give no indication of a parser’s performance on speciﬁc constructions, nor how suitable a parser is for speciﬁc applications. In this paper we give a detailed evaluation of a CCG parser on object extraction dependencies found in WSJ text. We also show how the parser can be used to parse questions for Question Answering. The accuracy of the original parser on questions is very poor, and we propose a novel technique for porting the parser to a new domain, by creating new labelled data at the lexical category level only. Using a supertagger to assign categories to words, trained on the new data, leads to a dramatic increase in question parsing accuracy. 
We describe a model for creating word-to-word and phrase-to-phrase alignments between documents and their human written abstracts. Such alignments are critical for the development of statistical summarization systems that can be trained on large corpora of document/abstract pairs. Our model, which is based on a novel Phrase-Based HMM, outperforms both the Cut & Paste alignment model (Jing, 2002) and models developed in the context of machine translation (Brown et al., 1993). 
A method for automatic plot analysis of narrative texts that uses components of both traditional symbolic analysis of natural language and statistical machine-learning is presented for the story rewriting task. In the story rewriting task, an exemplar story is read to the pupils and the pupils rewrite the story in their own words. This allows them to practice language skills such as spelling, diction, and grammar without being stymied by content creation. Often the pupil improperly recalls the story. Our method of automatic plot analysis enables the tutoring system to automatically analyze the student’s story for both general coherence and speciﬁc missing events. 
We are interested in the problem of modeling and evaluating spoken language systems in the context of human-machine dialogs. Spoken dialog corpora allow for a multidimensional analysis of speech recognition and language understanding models of dialog systems. Therefore language models can be directly trained based either on the dialog history or its equivalence class (or cluster). In this paper we propose an algorithm to mine dialog traces which exhibit similar patterns and are identiﬁed by the same class. For this purpose we apply data clustering methods to large human-machine spoken dialogue corpora. The resulting clusters can be used for system evaluation and language modeling. By clustering dialog traces we expect to learn about the behavior of the system with regards to not only the automation rate but the nature of the interaction (e.g. easy vs diﬃcult dialogs). The equivalence classes can also be used in order to automatically adapt the language model, the understanding module and the dialogue strategy to better ﬁt the kind of interaction detected. This paper investigates different ways for encoding dialogues into multidimensional structures and diﬀerent clustering methods. Preliminary results are given for cluster interpretation and dynamic model adaptation using the clusters obtained. 
We apply statistical machine translation (SMT) tools to generate novel paraphrases of input sentences in the same language. The system is trained on large volumes of sentence pairs automatically extracted from clustered news articles available on the World Wide Web. Alignment Error Rate (AER) is measured to gauge the quality of the resulting corpus. A monotone phrasal decoder generates contextual replacements. Human evaluation shows that this system outperforms baseline paraphrase generation techniques and, in a departure from previous work, offers better coverage and scalability than the current best-of-breed paraphrasing approaches. 
We propose a general model for joint inference in correlated natural language processing tasks when fully annotated training data is not available, and apply this model to the dual tasks of word sense disambiguation and verb subcategorization frame determination. The model uses the EM algorithm to simultaneously complete partially annotated training sets and learn a generative probabilistic model over multiple annotations. When applied to the word sense and verb subcategorization frame determination tasks, the model learns sharp joint probability distributions which correspond to linguistic intuitions about the correlations of the variables. Use of the joint model leads to error reductions over competitive independent models on these tasks. 
In most research on concept acquisition from corpora, concepts are modeled as vectors of relations extracted from syntactic structures. In the case of modifiers, these relations often specify values of attributes, as in (attr red); this is unlike what typically proposed in theories of knowledge representation, where concepts are typically defined in terms of their attributes (e.g., color). We compared models of concepts based on values with models based on attributes, using lexical clustering as the basis for comparison. We find that attribute-based models work better than value-based ones, and result in shorter descriptions; but that mixed models including both the best attributes and the best values work best of all. 
We present a novel representation of parse trees as lists of paths (leaf projection paths) from leaves to the top level of the tree. This representation allows us to achieve signiﬁcantly higher accuracy in the task of HPSG parse selection than standard models, and makes the application of string kernels natural. We deﬁne tree kernels via string kernels on projection paths and explore their performance in the context of parse disambiguation. We apply SVM ranking models and achieve an exact sentence accuracy of 85.40% on the Redwoods corpus.  
We present an approach to bounded constraintrelaxation for entropy maximization that corresponds to using a double-exponential prior or 1 regularizer in likelihood maximization for log-linear models. We show that a combined incremental feature selection and regularization method can be established for maximum entropy modeling by a natural incorporation of the regularizer into gradientbased feature selection, following Perkins et al. (2003). This provides an efﬁcient alternative to standard 1 regularization on the full feature set, and a mathematical justiﬁcation for thresholding techniques used in likelihood-based feature selection. Also, we motivate an extension to n-best feature selection for linguistic features sets with moderate redundancy, and present experimental results showing its advantage over 0, 1-best 1, 2 regularization and over standard incremental feature selection for the task of maximum-entropy parsing.1 
This paper presents some of the ﬁrst data visualizations and analysis of distributions for a lexicalized statistical parsing model, in order to better understand their nature. In the course of this analysis, we have paid particular attention to parameters that include bilexical dependencies. The prevailing view has been that such statistics are very informative but suﬀer greatly from sparse data problems. By using a parser to constrain-parse its own output, and by hypothesizing and testing for distributional similarity with back-oﬀ distributions, we have evidence that ﬁnally explains that (a) bilexical statistics are actually getting used quite often but that (b) the distributions are so similar to those that do not include head words as to be nearly indistinguishable insofar as making parse decisions. Finally, our analysis has provided for the ﬁrst time an eﬀective way to do parameter selection for a generative lexicalized statistical parsing model. 
We describe experiments carried out with adaptive language and translation models in the context of an interactive computer-assisted translation program. We developed cache-based language models which were then extended to the bilingual case for a cachebased translation model. We present the improvements we obtained in two contexts: in a theoretical setting, we achieved a drop in perplexity for the new models and, in a more practical situation simulating a user working with the system, we showed that fewer keystrokes would be needed to enter a translation. 
We propose to score phrase translation pairs for statistical machine translation using term weight based models. These models employ tf.idf to encode the weights of content and non-content words in phrase translation pairs. The translation probability is then modeled by similarity functions defined in a vector space. Two similarity functions are compared. Using these models in a statistical machine translation task shows significant improvements. 
Given a parallel parsed corpus, statistical treeto-tree alignment attempts to match nodes in the syntactic trees for a given sentence in two languages. We train a probabilistic tree transduction model on a large automatically parsed Chinese-English corpus, and evaluate results against human-annotated word level alignments. We ﬁnd that a constituent-based model performs better than a similar probability model trained on the same trees converted to a dependency representation. 
In this paper, we describe a resource-light system for the automatic morphological analysis and tagging of Russian. We eschew the use of extensive resources (particularly, large annotated corpora and lexicons), exploiting instead (i) pre-existing annotated corpora of Czech; (ii) an unannotated corpus of Russian. We show that our approach has beneﬁts, and present what we believe to be one of the ﬁrst full evaluations of a Russian tagger in the openly available literature. 
This paper presents Japanese morphological analysis based on conditional random ﬁelds (CRFs). Previous work in CRFs assumed that observation sequence (word) boundaries were ﬁxed. However, word boundaries are not clear in Japanese, and hence a straightforward application of CRFs is not possible. We show how CRFs can be applied to situations where word boundary ambiguity exists. CRFs offer a solution to the long-standing problems in corpus-based or statistical Japanese morphological analysis. First, ﬂexible feature designs for hierarchical tagsets become possible. Second, inﬂuences of label and length bias are minimized. We experiment CRFs on the standard testbed corpus used for Japanese morphological analysis, and evaluate our results using the same experimental dataset as the HMMs and MEMMs previously reported in this task. Our results conﬁrm that CRFs not only solve the long-standing problems but also improve the performance over HMMs and MEMMs. 
Irregular (so-called broken) plural identification in modern standard Arabic is a problematic issue for information retrieval (IR) and language engineering applications, but their effect on the performance of IR has never been examined. Broken plurals (BPs) are formed by altering the singular (as in English: tooth teeth) through an application of interdigitating patterns on stems, and singular words cannot be recovered by standard affix stripping stemming techniques. We developed several methods for BP detection, and evaluated them using an unseen test set. We incorporated the BP detection component into a new light-stemming algorithm that conflates both regular and broken plurals with their singular forms. We also evaluated the new light-stemming algorithm within the context of information retrieval, comparing its performance with other stemming algorithms. 1. Introduction Broken plurals constitute ~10% of texts in large Arabic corpora (Goweder and De Roeck, 2001), and ~41% of plurals (Boudelaa and Gaskell, 2002). Detecting broken plurals is therefore an important issue for light-stemming algorithms developed for applications such as information retrieval, yet the effect of broken plural identification on the performance of information retrieval systems has not been examined. We present several methods for BP detection, and evaluate them using an unseen test set containing 187,309 words. We also developed a new light-stemming algorithm incorporating a BP recognition component, and evaluated it within an information retrieval context,  comparing its performance with other stemming algorithms. We give a brief overview of Arabic in Section 2. Several approaches to BP detection are discussed in Section 3, and their evaluation in Section 4. In Section 5, we present an improved light stemmer and its evaluation. Finally in Section 6, our conclusions are summarised. 2. Arabic Morphology and its Number System Arabic is a heavily inflected language. Its grammatical system is traditionally described in terms of a root-and-pattern structure, with about 10,000 roots (Ali, 1988). Roots such as drs ( ) and ktb ( ) are listed alphabetically in standard Arabic dictionaries like the Wehr-Cowan (Beesley, 1996). The root is the most basic verb form. Roots are categorized into: triliteral, quadriliteral, or rarely pentaliteral. Most words are derived from a finite set of roots formed by adding diacritics1 or affixes (prefixes, suffixes, and infixes) through an application of fixed patterns which are templates to help in deriving inflectional and derivational forms of a word. Theoretically, several hundreds of Arabic words can be derived from a single root. Traditional Arab grammarians describe Arabic morphology in terms of patterns associated with the basic root f3l ( , “to do”)- where f, 3, and l are like wildcards in regular expressions: the letter f ( ,“pronounced fa”) represents the first consonant (sometimes called a radical), the letter 3 ( , “pronounced ain”) represents the second, and the letter l ( , “pronounced lam”) represents the third 
We perform Noun Phrase Bracketing by using a local, maximum entropy-based tagging model, which produces bracketing hypotheses. These hypotheses are subsequently fed into a reranking framework based on support vector machines. We solve the problem of hierarchical structure in our tagging model by modeling underspeciﬁed tags, which are fully determined only at decoding time. The tagging model performs comparably to competing approaches and the subsequent reranking increases our system’s performance from an f-score of 81.7 to 86.1, surpassing the best reported results to date of 83.8. 
This work applies boosted wrapper induction (BWI), a machine learning algorithm for information extraction from semi-structured documents, to the problem of named entity recognition. The default feature set of BWI is augmented with features based on distributional term clusters induced from a large unlabeled text corpus. Using no traditional linguistic resources, such as syntactic tags or specialpurpose gazetteers, this approach yields results near the state of the art in the MUC 6 named entity domain. Supervised learning using features derived through unsupervised corpus analysis may be regarded as an alternative to bootstrapping methods. 
Starting from ﬁrst principles, we re-visit the statistical approach and study two forms of the Bayes decision rule: the common rule for minimizing the number of string errors and a novel rule for minimizing the number of symbols errors. The Bayes decision rule for minimizing the number of string errors is widely used, e.g. in speech recognition, POS tagging and machine translation, but its justiﬁcation is rarely questioned. To minimize the number of symbol errors as is more suitable for a task like POS tagging, we show that another form of the Bayes decision rule can be derived. The major purpose of this paper is to show that the form of the Bayes decision rule should not be taken for granted (as it is done in virtually all statistical NLP work), but should be adapted to the error measure being used. We present ﬁrst experimental results for POS tagging tasks. 
Chinese part-of-speech (POS) tagging assigns one POS tag to each word in a Chinese sentence. However, since words are not demarcated in a Chinese sentence, Chinese POS tagging requires word segmentation as a prerequisite. We could perform Chinese POS tagging strictly after word segmentation (one-at-a-time approach), or perform both word segmentation and POS tagging in a combined, single step simultaneously (all-atonce approach). Also, we could choose to assign POS tags on a word-by-word basis, making use of word features in the surrounding context (word-based), or on a character-by-character basis with character features (character-based). This paper presents an in-depth study on such issues of processing architecture and feature representation for Chinese POS tagging, within a maximum entropy framework. We found that while the all-at-once, characterbased approach is the best, the one-at-a-time, character-based approach is a worthwhile compromise, performing only slightly worse in terms of accuracy, but taking shorter time to train and run. As part of our investigation, we also built a state-of-the-art Chinese word segmenter, which outperforms the best SIGHAN 2003 word segmenters in the closed track on 3 out of 4 test corpora. 
A novel technique for maximum “a posteriori” (MAP) adaptation of maximum entropy (MaxEnt) and maximum entropy Markov models (MEMM) is presented. The technique is applied to the problem of recovering the correct capitalization of uniformly cased text: a “background” capitalizer trained on 20Mwds of Wall Street Journal (WSJ) text from 1987 is adapted to two Broadcast News (BN) test sets — one containing ABC Primetime Live text and the other NPR Morning News/CNN Morning Edition text — from 1996. The “in-domain” performance of the WSJ capitalizer is 45% better than that of the 1-gram baseline, when evaluated on a test set drawn from WSJ 1994. When evaluating on the mismatched “out-ofdomain” test data, the 1-gram baseline is outperformed by 60%; the improvement brought by the adaptation technique using a very small amount of matched BN data — 25-70kwds — is about 20-25% relative. Overall, automatic capitalization error rate of 1.4% is achieved on BN data. 
Logs of user queries to an internet search engine provide a large amount of implicit and explicit information about language. In this paper, we investigate their use in spelling correction of search queries, a task which poses many additional challenges beyond the traditional spelling correction problem. We present an approach that uses an iterative transformation of the input query strings into other strings that correspond to more and more likely queries according to statistics extracted from internet search query logs. 
The focus of research in text classiﬁcation has expanded from simple topic identiﬁcation to more challenging tasks such as opinion/modality identiﬁcation. Unfortunately, the latter goals exceed the ability of the traditional bag-of-word representation approach, and a richer, more structural representation is required. Accordingly, learning algorithms must be created that can handle the structures observed in texts. In this paper, we propose a Boosting algorithm that captures sub-structures embedded in texts. The proposal consists of i) decision stumps that use subtrees as features and ii) the Boosting algorithm which employs the subtree-based decision stumps as weak learners. We also discuss the relation between our algorithm and SVMs with tree kernel. Two experiments on opinion/modality classiﬁcation conﬁrm that subtree features are important. 
It is often useful to classify email according to the intent of the sender (e.g., "propose a meeting", "deliver information"). We present experimental results in learning to classify email in this fashion, where each class corresponds to a verbnoun pair taken from a predefined ontology describing typical “email speech acts”. We demonstrate that, although this categorization problem is quite different from “topical” text classification, certain categories of messages can nonetheless be detected with high precision (above 80%) and reasonable recall (above 50%) using existing text-classification learning methods. This result suggests that useful task-tracking tools could be constructed based on automatic classification into this taxonomy.  methods which could be used to partially automate this sort of activity tracking. A hypothetical example of an email assistant that works along these lines is shown in Figure 1.  Bill, Do you have any sample scheduling-related email we could use as data? -Steve  Assistant announces: “new email request, priority unknown.”  Sure, I’ll put some together shortly. -Bill  Assistant: “should I add this new commitment to your todo list?”  Fred, can you collect the msgs from the CSPACE corpora tagged w/ the “meeting” noun, ASAP? -Bill  Assistant: notices outgoing request, may take action if no answer is received promptly.  Yes, I can get to that in the next few days. Is next Monday ok? -Fred  Assistant: notices incoming commitment. “Should I send Fred a reminder on Monday?”  Figure 1 - Dialog with a hypothetical email assistant that automatically detects email speech acts. Dashed boxes indicate outgoing messages. (Messages have been edited for space and anonymity.)  
This paper provides evidence for Genzel and Charniak’s (2002) entropy rate principle, which predicts that the entropy of a sentence increases with its position in the text. We show that this principle holds for individual sentences (not just for averages), but we also ﬁnd that the entropy rate effect is partly an artifact of sentence length, which also correlates with sentence position. Secondly, we evaluate a set of predictions that the entropy rate principle makes for human language processing; using a corpus of eye-tracking data, we show that entropy and processing effort are correlated, and that processing effort is constant throughout a text. 
In this paper, we explore the use of Random Forests (RFs) (Amit and Geman, 1997; Breiman, 2001) in language modeling, the problem of predicting the next word based on words already seen before. The goal in this work is to develop a new language modeling approach based on randomly grown Decision Trees (DTs) and apply it to automatic speech recognition. We study our RF approach in the context of -gram type language modeling. Unlike regu- ¢ lar -gram language models, RF language models ¢ have the potential to generalize well to unseen data, even when a complicated history is used. We show that our RF language models are superior to regular -gram language models in reducing both the per- ¢ plexity (PPL) and word error rate (WER) in a large vocabulary speech recognition system. 
We address the issue of judging the signiﬁcance of rare events as it typically arises in statistical naturallanguage processing. We ﬁrst deﬁne a general approach to the problem, and we empirically compare results obtained using log-likelihood-ratios and Fisher’s exact test, applied to measuring strength of bilingual word associations. 
Much natural language processing still depends on the Euclidean (cosine) distance function between two feature vectors, but this has severe problems with regard to feature weightings and feature correlations. To answer these problems, we propose an optimal metric distance that can be used as an alternative to the cosine distance, thus accommodating the two problems at the same time. This metric is optimal in the sense of global quadratic minimization, and can be obtained from the clusters in the training data in a supervised fashion. We conﬁrmed the effect of the proposed metric distance by a synonymous sentence retrieval task, document retrieval task and the K-means clustering of general vectorial data. The results showed constant improvement over the baseline method of Euclid and tf.idf, and were especially prominent for the sentence retrieval task, showing a 33% increase in the 11-point average precision. 
State-of-the-art machine translation techniques are still far from producing high quality translations. This drawback leads us to introduce an alternative approach to the translation problem that brings human expertise into the machine translation scenario. In this framework, namely Computer Assisted Translation (CAT), human translators interact with a translation system, as an assistance tool, that dinamically offers, a list of translations that best completes the part of the sentence already translated. In this paper, ﬁnite state transducers are presented as a candidate technology in the CAT paradigm. The appropriateness of this technique is evaluated on a printer manual corpus and results from preliminary experiments conﬁrm that human translators would reduce to less than 25% the amount of work to be done for the same task. 
The morphology of Semitic languages is unique in the sense that the major word-formation mechanism is an inherently non-concatenative process of interdigitation, whereby two morphemes, a root and a pattern, are interwoven. Identifying the root of a given word in a Semitic language is an important task, in some cases a crucial part of morphological analysis. It is also a non-trivial task, which many humans ﬁnd challenging. We present a machine learning approach to the problem of extracting roots of Hebrew words. Given the large number of potential roots (thousands), we address the problem as one of combining several classiﬁers, each predicting the value of one of the root’s consonants. We show that when these predictors are combined by enforcing some fairly simple linguistics constraints, high accuracy, which compares favorably with human performance on this task, can be achieved. 
Multidocument extractive summarization relies on the concept of sentence centrality to identify the most important sentences in a document. Centrality is typically deﬁned in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We are now considering an approach for computing sentence importance based on the concept of eigenvector centrality (prestige) that we call LexPageRank. In this model, a sentence connectivity matrix is constructed based on cosine similarity. If the cosine similarity between two sentences exceeds a particular predeﬁned threshold, a corresponding edge is added to the connectivity matrix. We provide an evaluation of our method on DUC 2004 data. The results show that our approach outperforms centroid-based summarization and is quite successful compared to other summarization systems. 
 Ming Zhou†  †Microsoft Research Asia  5F Sigma Center, No.49 Zhichun Road, Haidian  Beijing, China, 100080  {t-yjlv, mingzhou}@microsoft.com  Abstract∗ Traditional word alignment approaches cannot come up with satisfactory results for Named Entities. In this paper, we propose a novel approach using a maximum entropy model for named entity alignment. To ease the training of the maximum entropy model, bootstrapping is used to help supervised learning. Unlike previous work reported in the literature, our work conducts bilingual Named Entity alignment without word segmentation for Chinese and its performance is much better than that with word segmentation. When compared with IBM and HMM alignment models, experimental results show that our approach outperforms IBM Model 4 and HMM significantly. 
This paper presents Domain Relevance Estimation (DRE), a fully unsupervised text categorization technique based on the statistical estimation of the relevance of a text with respect to a certain category. We use a pre-deﬁned set of categories (we call them domains) which have been previously associated to WORDNET word senses. Given a certain domain, DRE distinguishes between relevant and non-relevant texts by means of a Gaussian Mixture model that describes the frequency distribution of domain words inside a large-scale corpus. Then, an Expectation Maximization algorithm computes the parameters that maximize the likelihood of the model on the empirical data. The correct identiﬁcation of the domain of the text is a crucial point for Domain Driven Disambiguation, an unsupervised Word Sense Disambiguation (WSD) methodology that makes use of only domain information. Therefore, DRE has been exploited and evaluated in the context of a WSD task. Results are comparable to those of state-ofthe-art unsupervised WSD systems and show that DRE provides an important contribution. 
If two translation systems differ differ in performance on a test set, can we trust that this indicates a difference in true system quality? To answer this question, we describe bootstrap resampling methods to compute statistical signiﬁcance of test results, and validate them on the concrete example of the BLEU score. Even for small test sizes of only 300 sentences, our methods may give us assurances that test result differences are real. 
Anticipating the availability of large questionanswer datasets, we propose a principled, datadriven Instance-Based approach to Question Answering. Most question answering systems incorporate three major steps: classify questions according to answer types, formulate queries for document retrieval, and extract actual answers. Under our approach, strategies for answering new questions are directly learned from training data. We learn models of answer type, query content, and answer extraction from clusters of similar questions. We view the answer type as a distribution, rather than a class in an ontology. In addition to query expansion, we learn general content features from training data and use them to enhance the queries. Finally, we treat answer extraction as a binary classiﬁcation problem in which text snippets are labeled as correct or incorrect answers. We present a basic implementation of these concepts that achieves a good performance on TREC test data. 
In this paper, we introduce TextRank – a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications. In particular, we propose two innovative unsupervised methods for keyword and sentence extraction, and show that the results obtained compare favorably with previously published results on established benchmarks. 
This paper introduces an approach to sentiment analysis which uses support vector machines (SVMs) to bring together diverse sources of potentially pertinent information, including several favorability measures for phrases and adjectives and, where available, knowledge of the topic of the text. Models using the features introduced are further combined with unigram models which have been shown to be effective in the past (Pang et al., 2002) and lemmatized versions of the unigram models. Experiments on movie review data from Epinions.com demonstrate that hybrid SVMs which combine unigram-style feature-based SVMs with those based on real-valued favorability measures obtain superior performance, producing the best results yet published using this data. Further experiments using a feature set enriched with topic information on a smaller dataset of music reviews handannotated for topic are also reported, the results of which suggest that incorporating topic information into such models may also yield improvement. 
We present a new approach to intrinsic summary evaluation, based on initial experiments in van Halteren and Teufel (2003), which combines two novel aspects: comparison of information content (rather than string similarity) in gold standard and system summary, measured in shared atomic information units which we call factoids, and comparison to more than one gold standard summary (in our data: 20 and 50 summaries respectively). In this paper, we show that factoid annotation is highly reproducible, introduce a weighted factoid score, estimate how many summaries are required for stable system rankings, and show that the factoid scores cannot be suﬃciently approximated by unigrams and the DUC information overlap measure. 
This paper proposes a novel method to compile statistical models for machine translation to achieve efﬁcient decoding. In our method, each statistical submodel is represented by a weighted ﬁnite-state transducer (WFST), and all of the submodels are expanded into a composition model beforehand. Furthermore, the ambiguity of the composition model is reduced by the statistics of hypotheses while decoding. The experimental results show that the proposed model representation drastically improves the efﬁciency of decoding compared to the dynamic composition of the submodels, which corresponds to conventional approaches. 
In this paper we describe a biography summarization system using sentence classification and ideas from information retrieval. Although the individual techniques are not new, assembling and applying them to generate multi-document biographies is new. Our system was evaluated in DUC2004. It is among the top performers in task 5–short summaries focused by person questions. 
This paper describes a data source and methodology for producing customized test suites for molecular biology entity identification systems. The data consists of: (a) a set of gene names and symbols classified by a taxonomy of features that are relevant to the performance of entity identification systems, and (b) a set of sentential environments into which names and symbols are inserted to create test data and the associated gold standard. We illustrate the utility of test sets producible by this methodology by applying it to five entity identification systems and describing the error patterns uncovered by it, and investigate relationships between performance on a customized test suite generated from this data and the performance of a system on two corpora.  
Rapid advances in the biomedical field have resulted in the accumulation of numerous experimental results, mainly in text form. To extract knowledge from biomedical papers, or use the information they contain to interpret experimental results, requires improved techniques for retrieving information from the biomedical literature. In many cases, since the information is required in gene units, recognition of the named entity is the first step in gathering and using knowledge encoded in these papers. Dictionary-based searching is useful for retrieving biological information in gene units. However, since many genes in the biomedical literature are written using ambiguous names, such as family names, we need a way of constructing dictionaries. In our laboratory, we have developed a gene name dictionary:GENA and a family name dictionary. The latter contains ambiguous hierarchical gene names to compensate GENA. In addition, to address the problem of trivial gene name variations and polysemy, heuristics were used to search gene/protein/family names in MEDLINE abstracts. Using these algorithms to match dictionary and gene/protein/family names, about 95, 91, and 89% of protein/gene/family names in abstracts on Saccharomyces cerevisiae, Drosophila melanogaster, and Homo sapiens were detected with a precision of 96, 92, and 94%, in respective organisms. The effect of our gene/protein/family recognition method on protein-interaction and protein-function ex-  traction using these dictionaries is also discussed. 
We explore the use of speculative language in MEDLINE abstracts. Results from a manual annotation experiment suggest that the notion of speculative sentence can be reliably annotated by humans. In addition, an experiment with automated methods also suggest that reliable automated methods might also be developed. Distributional observations are also presented as well as a discussion of possible uses for a system that can recognize speculative language.  1.1 Examples The sentences in the following box contain fragments expressing a relatively high level of speculation. The level of belief expressed by an author is often difﬁcult to ascertain from an isolated sentence and often the context of the abstract is needed. All examples in the paper are from abstracts available at the Nation Library of Medicine PubMed webpage (currently http://www.ncbi.nlm.nih.gov/PubMed/). The PubMed identiﬁer is provided following each sentence. Pdcd4 may thus constitute a useful molecular target for cancer prevention. (1131400)  
One of the routine tasks for model organism database curators is to identify and associate research articles to database entries. Such task can be considered as text categorization which has been studied in the general English domain. The task can be decomposed into two text categorization subtasks: i) finding relevant articles associating with specific model organisms, and ii) routing the articles to specific entries or specific areas. In this paper, we investigated the first subtask and designed a study using existing reference information available at four wellknown model organism databases and investigated the problem of identifying relevant articles for these organisms. We used features obtained from abstract text and titles. Additionally, we studied the determination power of other MEDLINE citation fields (e.g., Authors, MeshHeadings, Journals). Furthermore, we compared three supervised machine learning techniques on predicting to which organism the article belongs. 
Text mining tools are designed to assist users with the important step of hypothesis generation. In this research we apply an open discovery process to the problem of identifying novel disease or problem contexts in which a substance may have therapeutic potential. We illustrate this discovery process by executing our open discovery algorithm with turmeric (Curcumin Longa) as the substance being investigated. The top ranking entry suggested by the algorithm is retinal diseases. Further analysis of the literature yields evidence supporting the suggested connection between curcumin and retinal diseases. In particular, curcumin inﬂuences the activation of genes such as COX-2, TNF-alpha, JNK, ERK and NF-kappaB. These genes are in turn involved in retinal diseases such as diabetic retinopathies, ocular inﬂammation and glaucoma. Moreover, the evidence suggests that curcumin may have a beneﬁcial and therapeutic role in the context of these diseases. 
Biomedical literature contains vital information for the analysis and interpretation of experiments in the biological sciences. Human reasoning is the primary method for extracting, synthesizing, and interpreting the results contained in the literature, yet the rate at which publications are produced is exponential. With the advent of digital, full-text publication and increasing computational power, automated techniques for knowledge discovery and synthesis are being developed to assist humans in making sense of growing literature databases. We investigate the use of ontological information provided by the Medical Subject Headings (MeSH) project to discover groupings within a collection of medical literature stored in PubMed. Vector representations of documents based on MeSH terms are presented. Results of agglomerative hierachical clustering on two collections of biomedical literature, the Rat Genome Database and Tourette’s Syndrome related research, suggest novel and understandable groupings are obtainable. 
The goal of the BIND database (Biomolecular Interaction Network Database) is to curate and archive these interaction data from the literature using a standard data representation so that they may be effectively used for knowledge discovery (http://bind.ca) (Bader et al., 2001; Bader and Hogue, 2000). This database facilitates placing experimental data into context. For instance, a biologist may be presented with evidence suggesting that several different proteins may interact with their protein of interest. One of the first obvious questions is; “Is there any evidence to support any of these potential interactions?” These questions may be answered using a number of approaches. For each of the potential interactions: 
Sample Questions: 1. How does loosening access restrictions on the full-text content endanger the publishing revenue stream? 
In this paper we discuss the design, implementation, and use of Termino, a large scale terminological resource for text processing. Dealing with terminology is a difﬁcult but unavoidable task for language processing applications, such as Information Extraction in technical domains. Complex, heterogeneous information must be stored about large numbers of terms. At the same time term recognition must be performed in realistic times. Termino attempts to reconcile this tension by maintaining a ﬂexible, extensible relational database for storing terminological information and compiling ﬁnite state machines from this database to do term lookup. While Termino has been developed for biomedical applications, its general design allows it to be used for term processing in any domain. 
We describe an approach to two areas of biomedical information extraction, drug development and cancer genomics. We have developed a framework which includes corpus annotation integrated at multiple levels: a Treebank containing syntactic structure, a Propbank containing predicate-argument structure, and annotation of entities and relations among the entities. Crucial to this approach is the proper characterization of entities as relation components, which allows the integration of the entity annotation with the syntactic structure while retaining the capacity to annotate and extract more complex events. We are training statistical taggers using this annotation for such extraction as well as using them for improving the annotation process. 
Researchers in the biomedical and molecular biology fields are faced with a wide variety of information sources. These are presented in the form of images, free text, and structured data files that include medical records, gene and protein sequence data, and whole genome microarray data, all gathered from a variety of experimental organisms and clinical subjects. The need to organize and relate this information, particularly concerning genes, has motivated the development of resources, such as the Unified Medical Language System, Gene Ontology, LocusLink, and the Online Inheritance In Man (OMIM) database. We describe a natural language processing application to extract information on genes from unstructured text and discuss ways to integrate this information with some of the available online resources. 
Literature indexing tools provide researchers with a means to navigate through the network of scholarly scientiﬁc articles in a subject domain. We propose that more effective indexing tools may be designed using the links between articles provided by citations. With the explosion in the amount of scientiﬁc literature and with the advent of artifacts requiring more sophisticated indexing, a means to provide more information about the citation relation in order to give more intelligent control to the navigation process is warranted. In order to navigate a citation index in this more-sophisticated manner, the citation index must provide not only the citation-link information, but also must indicate the function of the citation. The design methodology of an indexing tool for scholarly biomedical literature which uses the rhetorical context surrounding the citation to provide the citation function is presented. In particular, we discuss how the scientiﬁc method is reﬂected in scientiﬁc writing and how this knowledge can be used to decide the purpose of a citation. 
An effective way of representing the meaning of a utterance is with frame structures in which a type of sentence is represented by a set of property/value slots. Properties can types of verbs and cases and values are extracted from a sentence and should respect constraints represented by case relations and selectional restrictions involving word senses organized in type hierarchies. Properties and values can be obtained as the output of Stochastic Finite State Transducers (SFST) based on property speciﬁc language models combined with generic n-gam models. In this way, sentence interpretation and recognition are carried out by the same search process. LM adaptation can be performed by dynamically modifying the probability of each SFST based on system expectations. Phrases accepted by different SFSTs may share words, especially if different SFST recognize constituents of the same frame. For this reason, search for the most likely interpretation has to consider promising (possibly overlapping) hypotheses generated by SFSTs and the best combination of them into an acceptable semantic structure. Using different types of acoustic conﬁdence measures and indices of consistency, it is possible to evaluate the probability that each semantic component that has been hypothesized is correct. These probabilities can be used by the dialogue strategy to decide about speciﬁc clariﬁcation and conﬁrmation actions. SFSTs can be constructed using semi-automatic learning procedures, including the manual analysis of a limited number of cases followed by the automatic generation of examples by analogy or the retrieval of analogous examples from existing corpora of data. Strategies for clariﬁcation and conﬁrmation actions can be learned using classiﬁcation and regression trees.   
This article proposes a hybrid statistical and structural semantic model for multi-stage spoken language understanding (SLU). The first stage of this SLU utilizes a weighted finite-state transducer (WFST)-based parser, which encodes the regular grammar of concepts to be extracted. The proposed method improves the regular grammar model by incorporating a well-known n-gram semantic tagger. This hybrid model thus enhances the syntax of n-gram outputs while providing robustness against speech-recognition errors. With applications to a Thai hotel reservation domain, it is shown to outperform both individual models at every stage of the SLU system. Under the probabilistic WFST framework, the use of N-best hypotheses from the speech recognizer instead of the 1best can further improve performance requiring only a small additional processing time. 
Spoken language understanding is a critical component of automated customer service applications. Creating effective SLU models is inherently a data driven process and requires considerable human intervention. We describe an interactive system for speech data mining. Using data visualization and interactive speech analysis, our system allows a User Experience (UE) expert to browse and understand data variability quickly. Supervised machine learning techniques are used to capture knowledge from the UE expert. This captured knowledge is used to build an initial SLU model, an annotation guide, and a training and testing system for the labelers. Our goal is to shorten the time to market by increasing the efficiency of the process and to improve the quality of the call types, the call routing, and the overall application. 
This paper introduces a method that generates simulated multimodal input to be used in testing multimodal system implementations, as well as to build statistically motivated multimodal integration modules. The generation of such data is inspired by the fact that true multimodal data, recorded from real usage scenarios, is difficult and costly to obtain in large amounts. On the other hand, thanks to operational speech-only dialogue system applications, a wide selection of speech/text data (in the form of transcriptions, recognizer outputs, parse results, etc.) is available. Taking the textual transcriptions and converting them into multimodal inputs in order to assist multimodal system development is the underlying idea of the paper. A conceptual framework is established which utilizes two input channels: the original speech channel and an additional channel called Virtual Modality. This additional channel provides a certain level of abstraction to represent non-speech user inputs (e.g., gestures or sketches). From the transcriptions of the speech modality, pre-defined semantic items (e.g., nominal location references) are identified, removed, and replaced with deictic references (e.g., here, there). The deleted semantic items are then placed into the Virtual Modality channel and, according to external parameters (such as a pre-defined user population with various deviations), temporal shifts relative to the instant of each cor-  responding deictic reference are issued. The paper explains the procedure followed to create Virtual Modality data, the details of the speech-only database, and results based on a multimodal city information and navigation application. 
 sequences that are salient to particular routes. Unfortunately, the speech signals are often of very poor  Our motivation is to perform call routing of utterances quality, being subject to the usual distortion, bandwidth  without recourse to transcriptions of the training data, restriction and noise associated with telephone signals,  which are very expensive to obtain. We therefore use and often compounded by the fact that callers usually  phonetic recognition of utterances and search for speak casually and spontaneously, and sometimes with a  salient phonetic sequences within the decodings. An strong accent.  important issue in phonetic recognition is the language Some approaches to the problem of extracting salient  model. It has been demonstrated [1] that the use of an phonetic strings from these utterances are:  iterative language model gives benefits in speech  recognition performance that are translated to improvements in utterance classification. However, an  • Improve phone accuracy by using a variable length language model and building models for insertion  all-purpose language model sometimes produces  and substitution; [3,4]  decodings that are ambiguous, in that they apparently  contain key phonetic sequences from several different routes, or non-informative, in that they apparently contain no useful phonetic sequences. This paper describes a method that uses multiple language models to detect useful information in such utterances. The  • Identify subword units (e.g. phonemes, phoneme strings, syllables and morphemes) from the recognised phonetic sequences by using clustering and segmentation methods; [5,6,7]  outputs from recognizers that use these multiple models are examined by post-processing HMMs that decide whether putative sequences are present or not. It is  • Use matrix-based methods for classification, such as LSA, LDA, ICA, SVM, etc. [8,9,10]  found that using multiple language models increases performance significantly by classifying utterances that a single language model is unable to discriminate.  Work at AT&T [1] showed that call routing performance using this phone-string utterance classification can be surprisingly close to what can be achieved by  1. Introduction  conventional methods involving word-trigram language models that require manual transcription. The method  described in [1] combines automatic training of  Call routing refers to the technique of automatically application-specific phonotactic language models  relaying a customer's telephone enquiry to one of together with token sequence classifiers.  several appropriate destinations, using computational  speech and language processing techniques. Our own experiments, using data different from that used  Transcribing calls for training purposes for a particular by AT&T, showed that this technique gave only a small  application requires considerable human effort, and it benefit in phone recognition accuracy, but was useful for  would be preferable for the system to learn routes finding salient phoneme strings. However, we found  without transcriptions being provided [2].  that, in some cases, it was impossible to obtain salient  phoneme sequences from the recognised utterances even  In this study, we assume that we are provided with a set when it was known that they occurred within the  of training utterances that have been labelled with their utterance. The reason may be that when building a  destination by an expert, but not transcribed into words single language model with the collected utterances from  or phonemes. We also assume (perhaps over- all call routes, the salience of a particular sequence for a  pessimistically) that we have no prior knowledge of the particular route is lost in the “noise” from mis-recognised  vocabulary or syntax of our application.  sequences of phonemes from the other routes. Hence we  In this situation, one possible course of action is to use sought a way of making the language model more  phone recognition and attempt to identify phonetic sensitive to the keywords occurring in the utterances. In  our system, an independent corpus is used to build an n-gram phonotactic language model that enables an initial recogniser to be built to decode all the training utterances. This model is refined iteratively using the output from the recogniser as the basis for the next language model. A specific language model for each call route is then built using the utterances from this call route. These are much more sensitive to key salient phoneme sequences in the utterance. The structure of the paper is as follows: in section 2, the data corpus used is introduced. Section 3 describes in detail the language modelling techniques, section 4 presents experiments and analysis of results, and we end with a Discussion in section 5.  Figure 1 shows the method used to produce an initial language model. The algorithm follows that described in [1]: 1. Build an n-gram language model (LM) using the dictionary transcriptions of the WSJ corpus (we used n=6). Make this the current LM. 2. Use the current LM in the recognizer to produce a set of phone strings. 3. Build a new LM based on the recognizer phone strings: 4. If niterations <=threshold, goto 2 else finish and produce a single language model for all routes.  2. Database The application studied here was the enquiry-point for the store card for a large retail store. Customers were invited to call up the system and to make the kind of enquiry they would normally make when talking to an operator. Their calls were routed to 61 different destinations, although some destinations were used very infrequently. 15 000 utterances were available, and a subset of 4511 utterances was used for training and 3518 for testing, in which 18 different call types were represented. Some of these call types are quite easily confused e.g. PaymentDue and PaymentDate, PaymentAddress and Changeaddress. Phoneme recognition of the input speech queries was performed using an HMM recogniser whose acoustic models had been trained on a large corpus of telephone speech and which had separate models for males and females. The average length of an utterance is 8.36 words. In addition, transcriptions of the prompts from the Wall Street Journal (WSJ) database were used to generate phoneme-level statistical language models for initial training. These models were generated using a scheme for backing off to probability estimates for shorter ngrams. The size of the vocabulary is 1208 words. To get a feel for the difficulty of the task, the mutual information (MI) between each word and the classes was calculated. By setting a threshold on this figure, we observed that there were about 51 keywords occurring in 4328 utterances which were capable on their own of classifying a call with high accuracy (some utterances had no keywords). 3. Modelling 3.1. Model Structure  Phonotactic language model (WSJ) Figure. 1 The Iterative training procedure The phone strings are now segmented and clustered so that salient phone sequences for each route can be identified. This is done as follows: FOR EACH ROUTE 1. Segment each recognized phone string in the route into all possible sequences of 3,4, … , 9 phones. 2. Estimate the MI for each sequence, and identify the salient sequences as the sequences with the highest MI [11]. 3. Cluster the salient sequences within the route. This is done by calculating and combining two measures of distance (using dynamic programming techniques) for each pair of sequences: • The Levensthein distance between the phone symbols representing the sequences.  Test utterances (3515 utts)  Recogniser with one LM  Sequences from  Key phonetic sequences detected?  a single route (2553 utts)  Call-route classify  2553 utts correct 0 utts incorrect  HMMs of key phonetic sequences  1. No sequences detected 2. Sequences from several routes detected 3. Conf-measure too low (962 utts)  REJECT No sequences detected (368 utts)  Detection of key phonetic sequences  Recogniser with 18 LMs  Call-route classify  487 utts correct 107 utts incorrect  Total utts = 3515 # classified correct by 1 LM = 2553 # classified correct by 18 LMs = 487  Figure 2: The Recognition Process  • The acoustic distance in “MFCC space” between the two waveform segments representing the sequences. 4. Use a simple lexicon pruning scheme that eliminates long agglomerations of short primitives [12]. At this point, we have generated a set of clustered phone sequences for each route. Each phone sequence corresponds to a sequence of frames, and the frame sequences within a cluster are used to build an HMM These HMMs are used later to estimate the class of a segment output by the recognizer (see section 3.2). Finally, we build a language model for each route, as follows by collecting together the recognised phonetic sequences of utterances from each route and using them to construct a language model. After iterating the LM, detection of key phonetic sequences improves. However, many utterances do not produce any sequences or produce several sequences from different routes. For recognition, we use a “divide and conquer” approach. Utterances that yield one or more sequences from the same route are classified immediately as that route, and utterances whose output is ambiguous, in that they yield no sequences, or sequences from several routes, or whose recognition confidence is too low to trust, are subject to a more detailed recognition pass in which separate LMs for each route are used. This has the advantage of only applying the extra computational effort required to use multiple LMs for those utterances that need this. In practice, if lattices are used, the additional computational effort is not too great. The confidence measure used was the measure available from the Nuance speech recognizer v8.0.  Hence recognition proceeds as follows. 1. A single language model is used in the recognizer to produce an output phone string. 2. Any phonetic sequences in the output string that also occur within any of the clusters of key phonetic sequences in any of the routes are found. 3. IF the number of key phonetic sequences found is one or more AND the sequences all belong to the same route: the utterance is classified as belonging to this route. ELSEIF the number of key phonetic sequences is zero OR there are one or more sequences from different routes OR the confidence measure of the whole utterance is lower than some threshold: the utterance is re-recognized using all 18 language models. 4. Recognition using multiple language models works as follows. 18 recognized phonetic sequences are output, one from each recognizer (as shown in Figure 2), and key phonetic sequences are detected in each output. IF there are one or more sequences from different routes: Putative sections of the speech that contain keywords are identified by comparing the symbolic output of a recognizer using a certain LM with the sequences that were used to form the HMMs of the clustered key phonetic sequences for this LM. These HMMs are then used to determine the likelihood of each sequence given the output string, and the utterance is assigned to the route of the highest likelihood. ELSEIF the number of key phonetic sequences is zero The utterance is not classified (rejected).  Call type classification is done using a vector-based approach as described in [8]. It is perhaps surprising that this classifier gets 100% accuracy (2553/2553) on utterances in which all the sequences are apparently from the same route―we attribute this to the fact that the 18 call-types were used were highly independent in their use of keywords. Figure 2 gives an overview of the whole process, together the number of utterances that were involved in each stage.  3.2. Key Phonetic Sequence Detection Key phonetic sequences can be incorrectly matched to incorrect segments of the utterance, causing false alarms. To combat this problem, we use matching in the acoustic domain as well as the symbolic domain. HMMs for 41 key phonetic sequences whose number of occurrences was larger than a threshold (we used 30) were built. Each key phonetic sequence was modelled by a five-state left-to-right HMM with no skips and each state is characterised by a mixture Gaussian state observation density. A maximum of 3 mixture components per state is used. The Baum-Welch algorithm is then used to estimate the parameters of the Gaussian densities for all states of subword HMM’s.  We use key phrase detection as described in [13][14].  By using the phonetic output from the recogniser, the  position in the utterance waveform of putative strings  can be identified, and this section of the waveform is  input into the phonetic sequence HMMs. Detection of  phrases is achieved by monitoring the forward  probability of the data given the model at any time and  searching for peaks in the probability. If full-likelihood  recognition is used, we estimate the score S f (w,t) :  S  f  (w, t )  =  α (ew ,t) ∑α (s, t)  (1)  s  In equation (1), S f (w,t) is the forward probability of  word w at time t [13]. In practice, we used the Viterbi  equivalent of equation (1) to determine the likelihood.  4. Experiments 4.1. Phone accuracy based on one LM Figure 3 illustrates the effects of (a) using the recogniser output strings to construct a new language model as described in section 3.1; (b) using 18 different LMs as well as a single LM.  Phone error rate (%)  80  70  60  50  40  30  20  10  0  
This paper describes our research on both the detection and subsequent resolution of recognition errors in spoken dialogue systems. The paper consists of two major components. The ﬁrst half concerns the design of the error detection mechanism for resolving city names in our MERCURY ﬂight reservation system, and an investigation of the behavioral patterns of users in subsequent subdialogues involving keypad entry for disambiguation. An important observation is that, upon a request for keypad entry, users are frequently unresponsive to the extent of waiting for a time-out or hanging up the phone. The second half concerns a pilot experiment investigating the feasibility of replacing the solicitation of a keypad entry with that of a “speak-and-spell” entry. A novelty of our work is the introduction of a speech synthesizer to simulate the user, which facilitates development and evaluation of our proposed strategy. We have found that the speak-and-spell strategy is quite effective in simulation mode, but it remains to be tested in real user dialogues. 
Robustness is a key requirement in spoken language understanding (SLU) systems. Human speech is often ungrammatical and ill-formed, and there will frequently be a mismatch between training and test data. This paper discusses robustness and adaptation issues in a statistically-based SLU system which is entirely data-driven. To test robustness, the system has been tested on data from the Air Travel Information Service (ATIS) domain which has been artiﬁcially corrupted with varying levels of additive noise. Although the speech recognition performance degraded steadily, the system did not fail catastrophically. Indeed, the rate at which the end-to-end performance of the complete system degraded was signiﬁcantly slower than that of the actual recognition component. In a second set of experiments, the ability to rapidly adapt the core understanding component of the system to a different application within the same broad domain has been tested. Using only a small amount of training data, experiments have shown that a semantic parser based on the Hidden Vector State (HVS) model originally trained on the ATIS corpus can be straightforwardly adapted to the somewhat different DARPA Communicator task using standard adaptation algorithms. The paper concludes by suggesting that the results presented provide initial support to the claim that an SLU system which is statistically-based and trained entirely from data is intrinsically robust and can be readily adapted to new applications.  
Speech interface is often required in many application environments such as telephonebased information retrieval, car navigation systems, and user-friendly interfaces, but the low speech recognition rate makes it difﬁcult to extend its application to new ﬁelds. Several approaches to increase the accuracy of the recognition rate have been researched by error correction of the recognition results, but previous approaches were mainly lexical-oriented ones in post error correction. We suggest an improved syllable-based model and a new semantic-oriented approach to correct both semantic and lexical errors, which is also more accurate for especially domain-speciﬁc speech error correction. Through extensive experiments using a speech-driven in-vehicle telematics information retrieval, we demonstrate the superior performance of our approach and some advantages over previous lexical-oriented approaches. 
We present the results of experiments aimed at assigning domains to speech recognition hypotheses (SRH). The methods rely on high-level linguistic representations of SRHs as sets of ontological concepts. We experimented with two domain models and evaluated their performance against a statistical, wordbased model. Our hand-annotated and tf*idf-based models yielded a precision of 88,39% and 82,59% respectively, compared to 93,14% for the word-based baseline model. These results are explained in terms of our experimental setup. 
We present a method of inferring aspects of a person’s context by capturing conversation topics and using prior knowledge of human behavior. This paper claims that topic-spotting performance can be improved by using a large database of common sense knowledge. We describe two systems we built to infer context from noisy transcriptions of spoken conversations using common sense, and detail some preliminary results. The GISTER system uses OMCSNet, a commonsense semantic network, to infer the most likely topics under discussion in a conversation stream. The OVERHEAR system is built on top of GISTER, and distinguishes between aspects of the conversation that refer to past, present, and future events by using LifeNet, a probabilistic graphical model of human behavior, to help infer the events that occurred in each of those three time periods. We conclude by discussing some of the future directions we may take this work. 
As the amount of spoken communications accessible by computers increases, searching and browsing is becoming crucial for utilizing such material for gathering information. It is desirable for multimedia content analysis systems to handle various formats of data and to serve varying user needs while presenting a simple and consistent user interface. In this paper, we present a research system for searching and browsing spoken communications. The system uses core technologies such as speaker segmentation, automatic speech recognition, transcription alignment, keyword extraction and speech indexing and retrieval to make spoken communications easy to navigate. The main focus is on telephone conversations and teleconferences with comparisons to broadcast news. 
In this paper we report on our recent efforts to collect a corpus of spoken lecture material that will enable research directed towards fast, accurate, and easy access to lecture content. Thus far, we have collected a corpus of 270 hours of speech from a variety of undergraduate courses and seminars. We report on an initial analysis of the spontaneous speech phenomena present in these data and the vocabulary usage patterns across three courses. Finally, we examine language model perplexities trained from written and spoken materials, and describe an initial recognition experiment on one course. 
When evaluating wordspotting systems, one normally compares receiver operating characteristic curves and different measures of accuracy. However, there are many other factors that are relevant to the system’s usability for searching speech. In this paper, we discuss both measures of quality for conﬁdence scores and propose algorithms for producing scores that are optimal with respect to these criteria. 
In this paper we highlight the problems that arise due to variations of spellings of names that occur in text, as a result of which links between two pieces of text where the same name is spelt differently may be missed. The problem is particularly pronounced in the case of ASR text. We propose the use of approximate string matching techniques to normalize names in order to overcome the problem. We show how we could achieve an improvement if we could tag names with reasonable accuracy in ASR. 
Automatic topic segmentation, separation of a discourse stream into its constituent stories or topics, is a necessary preprocessing step for applications such as information retrieval, anaphora resolution, and summarization. While signiﬁcant progress has been made in this area for text sources and for English audio sources, little work has been done in automatic segmentation of other languages using both text and acoustic information. In this paper, we focus on exploiting both textual and prosodic features for topic segmentation of Mandarin Chinese. As a tone language, Mandarin presents special challenges for applicability of intonation-based techniques, since the pitch contour is also used to establish lexical identity. However, intonational cues such as reduction in pitch and intensity at topic boundaries and increase in duration and pause still provide signiﬁcant contrasts in Mandarin Chinese. We ﬁrst build a decision tree classiﬁer that based only on prosodic information achieves boundary classiﬁcation accuracy of 89-95.8% on a large standard test set. We then contrast these results with a simple text similarity-based classiﬁcation scheme. Finally we build a merged classiﬁer, ﬁnding the best effectiveness for systems integrating text and prosodic cues. 
Much of the massive quantities of digitized data widely available, e.g., text, speech, handwritten sequences, are either given directly, or, as a result of some prior processing, as weighted automata. These are compact representations of a large number of alternative sequences and their weights reﬂecting the uncertainty or variability of the data. Thus, the indexation of such data requires indexing weighted automata. We present a general algorithm for the indexation of weighted automata. The resulting index is represented by a deterministic weighted transducer that is optimal for search: the search for an input string takes time linear in the sum of the size of that string and the number of indices of the weighted automata where it appears. We also introduce a general framework based on weighted transducers that generalizes this indexation to enable the search for more complex patterns including syntactic information or for different types of sequences, e.g., word sequences instead of phonemic sequences. The use of this framework is illustrated with several examples. We applied our general indexation algorithm and framework to the problem of indexation of speech utterances and report the results of our experiments in several tasks demonstrating that our techniques yield comparable results to previous methods, while providing greater generality, including the possibility of searching for arbitrary patterns represented by weighted automata.  
A number of issues arise when trying to scaleup natural language understanding (NLU) tools designed for relatively simple domains (e.g., ﬂight information) to domains such as medical advising or tutoring where deep understanding of user utterances is necessary. Because the subject matter is richer, the range of vocabulary and grammatical structures is larger meaning NLU tools are more likely to encounter out-of-vocabulary words or extra-grammatical utterances. This is especially true in medical advising and tutoring where users may not know the correct vocabulary and use common sense terms or descriptions instead. Techniques designed to improve robustness (e.g., skipping unknown words, relaxing grammatical constraints, mapping unknown words to known words) are effective at increasing the number of utterances for which an NLU subsystem can produce a semantic interpretation. However, such techniques introduce additional ambiguity and can lead to a loss of ﬁdelity (i.e., a mismatch between the semantic interpretation and what the language producer meant). To control this trade-off, we propose semantic interpretation conﬁdence scores akin to speech recognition conﬁdence scores, and describe our initial attempt to compute such a score in a modularized NLU sub-system. 
In this paper we present a discussion of existing metrics for evaluation the performance of individual natural language understanding systems and components as well as the commonly employed metrics for measuring the speciﬁc task difﬁculties. We extend and generalize the common majority class baseline metric and introduce an general entropy-based metric for measuring the task difﬁculty of arbitrary language understanding tasks. Finally, we show an empirical study evaluating this metric followed by a discussion of its role in measuring the scalability of language understanding systems and components. 
In this paper we present an evaluation of Carmel-Tools, a novel behavior oriented approach to authoring and maintaining domain speciﬁc knowledge sources for robust sentence-level language understanding. Carmel-Tools provides a layer of abstraction between the author and the knowledge sources, freeing up the author to focus on the desired language processing behavior that is desired in the target system rather than the linguistic details of the knowledge sources that would make this behavior possible. Furthermore, CarmelTools offers greater ﬂexibility in output representation than the context-free rewrite rules produced by previous semantic authoring tools, allowing authors to design their own predicate language representations. 
We describe a reusable and scalable dialogue toolbox and its application in multiple systems. Our main claim is that ends-based representation and processing throughout the complete dialogue backbone it essential to our approach. 
In ScaNaLU 2002, Chang et al presented a scalable natural language formalism called Embodied Construction Grammar (ECG) (Chang et al., 2002). ECG makes deep understanding systems possible because it is a rigorous, uniﬁed formalism that incorporates the semantic and pragmatic insights found in cognitive linguistics. The work described in this paper builds on (Chang et al., 2002) because it leverages the ECG formalism to perform deep, scalable construction-based parsing and semantic analysis. 
Mental Space Theory (Fauconnier, 1985) encompasses a wide variety of complex linguistics phenomena that are largely ignored in today’s natural language processing systems. These phenomena include conditionals (e.g. If sentences), embedded discourse, and other natural language utterances whose interpretation depends on cognitive partitioning of contextual knowledge. A unification-based formalism, Embodied Construction Grammar (ECG) (Chang et al., 2002a) took initial steps to include space as a primitive type, but most of the details are yet to be worked out. The goal of this paper is to present a scalable computational account of mental spaces based on the Neural Theory of Language (NTL) simulation-based understanding framework (Narayanan, 1999; Chang et al., 2002b). We introduce a formalization of mental spaces based on ECG, and describe how this formalization fits into the NTL framework. We will also use English Conditionals as a case study to show how mental spaces can be parameterized from language. 
This paper describes an hierarchical approach to WordNet sense distinctions that provides different types of automatic Word Sense Disambiguation (WSD) systems, which perform at varying levels of accuracy. For tasks where fine-grained sense distinctions may not be essential, an accurate coarse-grained WSD system may be sufficient. The paper discusses the criteria behind the three different levels of sense granularity, as well as the machine learning approach used by the WSD system.  Tips on Being a Successful Movie Vampire ... I shall call the police. Successful Casting Call & Shoot for ``Clash of Empires'' ... thank everyone for their participation in the making of yesterday's movie. Demme's casting is also highly entertaining, although I wouldn't go so far as to call it successful. This movie's resemblance to its predecessor is pretty vague... VHS Movies: Successful Cold Call Selling: Over 100 New Ideas, Scripts, and Examples from the Nation's Foremost Sales Trainer.  
Scaling up from controlled single domain spoken dialogue systems towards conversational, multi-domain and multimodal dialogue systems poses new challenges for the reliable processing of less restricted user utterances. In this paper we explore the feasibility to employ a general purpose ontology for various tasks involved in processing the user’s utterances. 
A scalable natural language generation (NLG) system called HYPERBUG 1 embedded in an agent-based, multimodal dialog system is presented. To motivate this presentation, several scenarios (including a domain shift) are identified where scalability in dialog systems is really needed, and NLG is argued to be one way of easing this desired scalability. Therefore the novel approach to hybrid NLG in the HYPERBUG system is described and the scalability of its parts and resources is investigated. Concluding with a few remarks to discourse generation, we argue that NLG can both contribute to and benefit from scalability in dialog systems. 
It is straight-forward to compare the performance of the set of systems that produce the same form of output, e.g., Penn Treebank-based parsers can be compared in terms of how well they reproduce the Penn Treebank. Alternative systems based on a standard are largely interchangeable. Thus a system that uses one PennTreebank-based parser as a component can easily be adapted to use another better performing PennTreebank-based parser. Standards can be built on. For example, if one accepts the framework of the Penn Treebank, it is easy to move on to representations of “deeper” structure as suggested in three papers in this volume (Miltsakaki et al., 2004; Babko-Malaya et al., 2004; Meyers et al., 2004). It is my view that these advantages outweigh the disadvantages. I propose that the papers in this volume be viewed with the following question in mind: How can the work covered by this collection of papers be integrated together? Put differently, to what extent are these resources mergeable? 
Mapping between syntax and semantics is one of the most promising research topics in corpus annotation. This paper deals with the implementation of an semi-automatic transformation from a syntactically-tagged corpus into a semantic-tagged one. The method has been experimentally applied to a 1600-sentence treebank (the UAM Spanish Treebank). Results of evaluation are provided as well as prospective work in comparing syntax and semantics in written and spoken annotated corpora. 
This paper describes a new, large scale discourse-level annotation project – the Penn Discourse TreeBank (PDTB). We present an approach to annotating a level of discourse structure that is based on identifying discourse connectives and their arguments. The PDTB is being built directly on top of the Penn TreeBank and Propbank, thus supporting the extraction of useful syntactic and semantic features and providing a richer substrate for the development and evaluation of practical algorithms. We provide a detailed preliminary analysis of inter-annotator agreement – both the level of agreement and the types of inter-annotator variation. 
The PropBank project is creating a corpus of text annotated with information about basic semantic propositions. PropBank I (Kingsbury & Palmer, 2002) added a layer of predicateargument information, or semantic roles, to the syntactic structures of the English Penn Treebank. This paper presents an overview of the second phase of PropBank Annotation, PropBank II, which is being applied to English and Chinese, and includes (Neodavidsonian) eventuality variables, nominal references, sense tagging, and connections to the Penn Discourse Treebank (PDTB), a project for annotating discourse connectives and their arguments.  The Proposition Bank project takes a practical approach to semantic representation, adding a layer of predicateargument information, or semantic roles, to the syntactic structures of the Penn Treebank. The resulting resource can be thought of as shallow, in that it does not represent co-reference, quantification, and many other higher-order phenomena, but also broad, in that it covers every verb in the corpus and allows representative statistics to be calculated. The semantic annotation provided by PropBank is only a first approximation at capturing the full richness of semantic representation. Additional annotation of nominalizations and other noun predicates has already begun at NYU. This paper presents an overview of the second phase of PropBank Annotation, PropBank II, which is being applied to English and Chinese and includes (Neodavidsonian) eventuality variables, nominal references, sense tagging, and discourse connectives.  
This paper describes NomBank, a project that will provide argument structure for instances of common nouns in the Penn Treebank II corpus. NomBank is part of a larger effort to add additional layers of annotation to the Penn Treebank II corpus. The University of Pennsylvania’s PropBank, NomBank and other annotation projects taken together should lead to the creation of better tools for the automatic analysis of text. This paper describes the NomBank project in detail including its speciﬁcations and the process involved in creating the resource. 
The requirements of the depth and precision of annotation vary for different intended uses of the corpus but it has been commonly accepted nowadays that the standard annotations of surface structure are only the ﬁrst steps in a more ambitious research program, aiming at a creation of advanced resources for most different systems of natural language processing and for testing and further enrichment of linguistic and computational theories. Among the several possible directions in which we believe the standard annotation systems should go (and in some cases already attempt to go) beyond the POS tagging or shallow syntactic annotations, the following four are characterized in the present contribution: (i) predicateargument representation of the underlying syntactic relations as basically corresponding to a rooted tree that can be univocally linearized, (ii) the inclusion of the information structure using very simple means (the left-to-right order of the nodes and three attribute values), (iii) relating this underlying structure (rendering the ”linguistic meaning,” i.e. the semantically relevant counterparts of the grammatical means of expression) to certain central aspects of referential semantics (reference assignment and coreferential relations), and (iv) handling of word sense disambiguation. The ﬁrst three issues are documented in the present paper on the basis of our experience with the development of the structure and scenario of the Prague Dependency Treebank which provides for syntactico-semantic annotation of large text segments from the Czech National Corpus and which is based on a solid theoretical framework.  
The Prague Czech-English Dependency Treebank (PCEDT) is a new syntactically annotated Czech-English parallel resource. The Penn Treebank has been translated to Czech, and its annotation automatically transformed into dependency annotation scheme. The dependency annotation of Czech is done from plain text by automatic procedures. A small subset of corresponding Czech and English sentences has been annotated by humans. We discuss some of the problems we have experienced during the automatic transformation between annotation schemes and hint at some of the difﬁculties to be tackled by potential guidelines for dependency annotation of English. 
This paper describes a multi-site project to annotate six sizable bilingual parallel corpora for interlingual content. After presenting the background and objectives of the effort, we describe the data set that is being annotated, the interlingua representation language used, an interface environment that supports the annotation task and the annotation process itself. We will then present a preliminary version of our evaluation methodology and conclude with a summary of the current status of the project along with a number of issues which have arisen.  lation (MT) and a range of other Natural Language Processing (NLP) applications. The project participants include the Computing Research Laboratory at NMSU, the Language Technologies Institute at CMU, the Information Science Institute at USC, UMIACS at the University of Maryland, the MITRE Corporation and Columbia University. In the remainder of the paper, we first present the background and objectives of the project. We then describe the data set that is being annotated, the interlingual representation language being used, an interface environment that is designed to support the annotation task, and the process of annotation itself. We will then outline a preliminary version of our evaluation methodology and conclude with a summary of the current status of the project along with a set of issues that have arisen since the project began.  
High-quality lexical resources are needed to both train and evaluate Word Sense Disambiguation (WSD) systems. The problem of ambiguity persists even in limited domains, thus the necessity for wide-coverage inventories of senses (dictionaries) and corpora sense-tagged to them. WordNet has been used extensively for WSD, for both its broad coverage and its large network of semantic relations. In this paper, we present a report on the state of our current endeavor to increase the connectivity of WordNet through sense-tagging the glosses, the result of which will be to create a more integrated lexical resource.  cal, and the deﬁnitional glosses and illustrative sentences have not participated in the network of relations at all. This paper reports on a project currently underway to sense-tag the glosses. Sense-tagging is the process of linking an instance of a word to the WordNet synset representing its context-appropriate meaning. Monosemous words2 in the glosses can be tagged automatically, but in order to be truly reliable, the sense-tagging of polysemous words3 must be done manually. This approach is in signiﬁcant contrast with the work done at The University of Texas at Dallas on Extended WordNet4, in which polysemous words in the WordNet glosses were sense-tagged primarily by automatic means. The result of the project described here will be to increase connectivity and make possible the association of words with related concepts that cut across grammatical class and hierarchy, providing a more integrated lexical resource.  
The Valency Lexicon of Czech Verbs, Version 1.0 (VALLEX 1.0) is a collection of linguistically annotated data and documentation, resulting from an attempt at formal description of valency frames of Czech verbs. VALLEX 1.0 is closely related to Prague Dependency Treebank. In this paper, the context in which VALLEX came into existence is brieﬂy outlined, and also three similar projects for English verbs are mentioned. The core of the paper is the description of the logical structure of the VALLEX data. Finally, we suggest a few directions of the future research. 
This paper describes various types of semantic ellipsis and underspecification in natural language, and the ways in which the meaning of semantically elided elements is reconstructed in the Ontological Semantics (OntoSem) text processing environment. The description covers phenomena whose treatment in OntoSem has reached various levels of advancement: fully implemented, partially implemented, and described algorithmically outside of implementation. We present these research results at this point – prior to full implementation and extensive evaluation – for two reasons: first, new descriptive material is being reported; second, some subclasses of the phenomena in question will require a truly long-term effort whose results are best reported in installments. 
We describe work in progress aimed at developing methods for automatically constructing a lexicon using only statistical data derived from analysis of corpora, a problem we call lexical optimization. Speciﬁcally, we use statistical methods alone to obtain information equivalent to syntactic categories, and to discover the semantically meaningful units of text, which may be multi-word units or polysemous terms-incontext. Our guiding principle is to employ a notion of “meaningfulness” that can be quantiﬁed information-theoretically, so that plausible variants of a lexicon can be judged relative to each other. We describe a technique of this nature called information theoretic co-clustering and give results of a series of experiments built around it that demonstrate the main ingredients of lexical optimization. We conclude by describing our plans for further improvements, and for applying the same mathematical principles to other problems in natural language processing. 
In natural language, the meaning of a lexeme often varies due to the specific surrounding context. Computational approaches to natural language processing can benefit from a reliable, long-range-context-dependent representation of the meaning of each lexeme that appears in a given sentence. We have developed a general new technique that produces a context-dependent ‘meaning’ representation for a lexeme in a specific surrounding context. The ‘meaning’ of a lexeme in a specific context is represented by a list of semantically replaceable elements the members of which are other lexemes from our experimental lexicon. We have performed experiments with a lexicon composed of individual English words and also with a lexicon of individual words and selected phrases. The resulting lists can be used to compare the ‘meaning’ of conceptual units (individual words or frequentlyoccurring phrases) in different contexts and also can serve as features for machine learning approaches to classify semantic roles and relationships. 
We propose a new method for detecting verb alternations, by comparing the probability distributions over WordNet classes occurring in two potentially alternating argument positions. Existing distance measures compute only the distributional distance, and do not take into account the semantic similarity between WordNet senses across the distributions. Our method compares two probability distributions over WordNet by measuring the semantic distance of the component nodes, weighted by their probability. To incorporate semantic similarity, we calculate the (dis)similarity between two probability distributions as a weighted distance “travelled” from one to the other through the WordNet hierarchy. We evaluate the measure on the causative alternation, and ﬁnd that overall it outperforms existing distance measures. 
Lexical-semantic verb classiﬁcations have proved useful in supporting various natural language processing (NLP) tasks. The largest and the most widely deployed classiﬁcation in English is Levin’s (1993) taxonomy of verbs and their classes. While this resource is attractive in being extensive enough for some NLP use, it is not comprehensive. In this paper, we present a substantial extension to Levin’s taxonomy which incorporates 57 novel classes for verbs not covered (comprehensively) by Levin. We also introduce 106 novel diathesis alternations, created as a side product of constructing the new classes. We demonstrate the utility of our novel classes by using them to support automatic subcategorization acquisition and show that the resulting extended classiﬁcation has extensive coverage over the English verb lexicon. 
NLP methods and applications need to take account not only of “classical” lexical relations, as found in WordNet, but the lessstructural, more context-dependent “nonclassical” relations that readers intuit in text. In a reader-based study of lexical relations in text, most were found to be of the latter type. The relationships themselves are analyzed, and consequences for NLP are discussed. 
This paper proposes two decision trees for determining the meanings of the prepositional uses of over by using the contextual information. It first examines the meanings of the prepositional uses of over and then aims at identifying the contexts for interpreting the meanings. Some contexts are complementary features, and that makes the decision trees simple. The trees have been tested on a corpus, and the results are encouraging. 
This paper presents an approach for detecting semantic relations in noun phrases. A learning algorithm, called semantic scattering, is used to automatically label complex nominals, genitives and adjectival noun phrases with the corresponding semantic relation. 
The discovery of semantic relations in text plays an important role in many NLP applications. This paper presents a method for the automatic classiﬁcation of semantic relations in nominalized noun phrases. Nominalizations represent a subclass of NP constructions in which either the head or the modiﬁer noun is derived from a verb while the other noun is an argument of this verb. Especially designed features are extracted automatically and used in a Support Vector Machine learning model. The paper presents preliminary results for the semantic classiﬁcation of the most representative NP patterns using four distinct learning models. 
We explore a semantic abstraction approach to automatic summarization in the biomedical domain. The approach relies on a semantic processor that functions as the source interpreter and produces a list of predications. A transformation stage then generalizes and condenses this list, ultimately generating a conceptual condensate for a disorder input topic. The final condensate is displayed in graphical form. We provide a set of principles for the transformation stage and describe the application of this approach to multidocument input. Finally, we examine the characteristics and quality of the condensates produced. 
We discuss a computational mechanism for comparing and integrating lexical definitional knowledge, meanings and concept definitions of English words and phrases available from different sources such as dictionaries, encyclopedias, corpora of texts, and personal beliefs. Such a mechanism is needed in order to automate comparison and reconciliation of the definitional differences because completeness and correctness of definitional knowledge seriously affect the results of text processing, particularly classification, question answering, and summarization. 
Events described in textual narratives do not always occur in neat, chronological order but occur, for example, during or overlapping each other or as simultaneous events. Summarizations of narratives, however, benefit from a simpler, linear ordering of events. This paper describes an approach for modeling events in text as event intervals and for generating linear orders of event intervals, useful for the summarization of events or as the basis for question answering systems. Linear orders are derived through reducing the set of thirteen possible event interval relations to a set of only before or equal relations. The mapping of event interval relations into before/after sequences requires the support of additional constraints in order to preserve the original semantics of the events presented in the text and to derive plausible orders of events. 
Current lexical semantic representations for natural language applications view verbs as simple predicates over their arguments. These structures are too coarse-grained to capture many important generalizations about verbal argument structure. In this paper, I speciﬁcally defend the following two claims: verbs have rich internal structure expressible in terms of ﬁner-grained primitives of meaning, and at least for some languages, verbal meaning is compositionally derived from these primitive elements. I primarily present evidence from Mandarin Chinese, whose verbal system is very different from that of English. Many empirical facts about the typology of verbs in Mandarin cannot be captured by a “ﬂat” lexical semantic representation. These theoretical results hold important practical consequences for natural language processing applications. 
Progress in Question Answering can be achieved by (1) combining multiple strategies that optimally resolve different question classes of various degrees of complexity; (2) enhancing the precision of question interpretation and answer extraction; and (3) question decomposition and answer fusion. In this paper we also present the impact of modeling the user background on Q/A and discuss the pragmatics pf processing negation in Q/A. 
In this paper we argue that access to rich semantic structures derived from questions and answers will enable both the retrieval of more accurate answers to simple questions and enable inference processes that explain the validity and contextual coverage of answers to complex questions. Processing complex questions involves the identifications of several forms of complex semantic structures. Answer Extraction is performed by recognizing event inter-relationships and by inferring over multiple sentences and texts, using background knowledge. 
In this paper we discuss the applicability of the knowledge representation and reasoning language AnsProlog for the design and implementation of query answering systems. We consider a motivating example, and illustrate how AnsProlog can be used to represent defaults, causal relations, and other types of commonsense knowledge needed to properly answer non-trivial questions about the example’s domain. 
In a real-world setting, questions are not asked in isolation, but rather in a cohesive manner that involves a sequence of related questions to meet user’s information needs. The capability to interpret and answer questions based on context is important. In this paper, we discuss the role of discourse modeling in context question answering. In particular, we motivate a semantic-rich discourse representation and discuss the impact of refined discourse structure on question answering. 
In this paper we introduce two methods for deriving the intentional structure of complex questions. Techniques that enable the derivation of implied information are also presented. We show that both the intentional structure and the implicatures enabled by it are essential components of Q/A systems capable of successfully processing complex questions. The results of our evaluation support the claim that there are multiple interactions between the process of answer ﬁnding and the coercion of intentions and implicatures. 
Modern Question/Answering systems rely on expected answer types for processing questions. The answer type is a semantic category provided by Named Entity recognizer or by semantic hierarchies. We argue in this paper that Q/A systems should take advantage of the topic information by exploiting several models of question and answer categorization. The matching of the question category with the answer category allows the system to ﬁlter out many incorrect answers. 
In this paper we describe some preliminary results of qualitative evaluation of the answering system HITIQA (High-Quality Interactive Question Answering) which has been developed over the last 2 years as an advanced research tool for information analysts. HITIQA is an interactive open-domain question answering technology designed to allow analysts to pose complex exploratory questions in natural language and obtain relevant information units to prepare their briefing reports in order to satisfy a given scenario. The system uses novel data-driven semantics to conduct a clarification dialogue with the user that explores the scope and the context of the desired answer space. The system has undergone extensive hands-on evaluations by a group of intelligence analysts representing various foreign intelligence services. This evaluation validated the overall approach in HITIQA but also exposed limitations of the current prototype. 
This paper addresses the pragmatic challenges that state-of-the-art question/answering systems face in trying to decompose complex information-seeking scenarios. We propose that question decomposition can be approached in one of two ways: either by approximating the domain-speciﬁc knowledge for a particular set of domains, or by identifying the decomposition strategies employed by human users. We also present preliminary results from experiments that conﬁrm the viability of each of these approaches within an interactive Q/A context. 
A novel challenge for evaluating open-domain question answering technologies is proposed. In this challenge, question answering systems are supposed to be used interactively to answer a series of related questions, whereas in the conventional setting, systems answer isolated questions one by one. Such an interaction occurs in the case of gathering information for a report on a speciﬁc topic, or when browsing information of interest to the user. In this paper, ﬁrst, we explain the design of the challenge. We then discuss its reality and show how the capabilities measured by the challenge are useful and important in practical situations, and that the difﬁculty of the challenge is proper for evaluating the current state of open-domain question answering technologies. 
This paper discusses the possibility of building an ontology-based question answering system in the context of the Semantic Web presenting a proof-of-concept system. The system is under development in the MOSES European Project. Introduction Question Answering (QA) systems (as QA track of the Text Retrieval Conference (TREC-QA) competitions (Voorhees 2001)), are able both to understand questions in natural language and to produce answers in the form of selected paragraphs extracted from very large collections of text. Generally, they are opendomain systems, and do not rely on specialised conceptual knowledge as they use a mixture of statistical techniques and shallow linguistic analysis. Ontological Question Answering systems, e.g. (Woods et al. 1972, Zajac 2000) propose to attack the problem by means of an internal unambiguous knowledge representation. As any knowledge intensive application, ontological QA systems have as intrinsic limitation related to the small scale of the underlying syntactic-semantic models of natural language. While limitations are well-known, we are still questioning if any improvement has occurred since the development of the first ontological QA system LUNAR. Several important facts have emerged that could influence related research approaches:  a growing availability of lexical knowledge bases that model and structure words: WordNet (Miller 1995) and EuroWordNet (Vossen 1998) among others; some open-domain QA systems have proven the usefulness of these resources, e.g. WordNet in the system described in (Harabagiu et al. 2001). the vision of a Web populated by “ontologically” tagged documents which the semantic Web initiative has promoted; in case this vision becomes a reality, it will require a world-wide collaborative work for building interrelated “conceptualisations” of domain specific knowledge the trend in building shallow, modular, and robust natural language processing systems (Abney 1996, Hobbs et al. 1996, Ait-Moktar&Chanod 1997, Basili&Zanzotto 2002) which is making them appealing in the context of ontological QA systems, both for text interpretation (Andreasen et al. 2002) and for database access (Popescu et al. 2003). Given this background, we are investigating a new approach to ontology-based QA in which users ask questions in natural language to knowledge bases of facts extracted from a federation of Web sites and organised in topic map repositories (Garshol 2003). Our approach is being investigated in the context of EU project MOSES1, with the explicit objective of developing an ontology-based methodology to search, create, maintain and adapt semantically structured Web contents according to the vision of the Semantic Web. MOSES is taking advantage of expertise coming from several fields: software agent technology, NLP, graph theory 
Given a collection of discrete random variables representing outcomes of learned local predictors in natural language, e.g., named entities and relations, we seek an optimal global assignment to the variables in the presence of general (non-sequential) constraints. Examples of these constraints include the type of arguments a relation can take, and the mutual activity of different relations, etc. We develop a linear programming formulation for this problem and evaluate it in the context of simultaneously learning named entities and relations. Our approach allows us to efﬁciently incorporate domain and task speciﬁc constraints at decision time, resulting in signiﬁcant improvements in the accuracy and the “human-like” quality of the inferences. 
This paper considers the task of automatically collecting words with their entity class labels, starting from a small number of labeled examples (‘seed’ words). We show that spectral analysis is useful for compensating for the paucity of labeled examples by learning from unlabeled data. The proposed method significantly outperforms a number of methods that employ techniques such as EM and co-training. Furthermore, when trained with 300 labeled examples and unlabeled data, it rivals Naive Bayes classiﬁers trained with 7500 labeled examples. 
Automatically deriving semantic structures from text is a challenging task for machine learning. The ﬂat feature representations, usually used in learning models, can only partially describe structured data. This makes difﬁcult the processing of the semantic information that is embedded into parse-trees. In this paper a new kernel for automatic classiﬁcation of predicate arguments has been designed and experimented. It is based on subparse-trees annotated with predicate argument information from PropBank corpus. This kernel, exploiting the convolution properties of the parse-tree kernel, enables us to learn which syntactic structures can be associated with the arguments deﬁned in PropBank. Support Vector Machines (SVMs) using such a kernel classify arguments with a better accuracy than SVMs based on linear kernel. 
The success of supervised learning approaches to word sense disambiguation is largely dependent on the features used to represent the context in which an ambiguous word occurs. Previous work has reached mixed conclusions; some suggest that combinations of syntactic and lexical features will perform most effectively. However, others have shown that simple lexical features perform well on their own. This paper evaluates the effect of using different lexical and syntactic features both individually and in combination. We show that it is possible for a very simple ensemble that utilizes a single lexical feature and a sequence of part of speech features to result in disambiguation accuracy that is near state of the art.  
This paper investigates the application of cotraining and self-training to word sense disambiguation. Optimal and empirical parameter selection methods for co-training and self-training are investigated, with various degrees of error reduction. A new method that combines cotraining with majority voting is introduced, with the effect of smoothing the bootstrapping learning curves, and improving the average performance. 
This paper systematically compares unsupervised word sense discrimination techniques that cluster instances of a target word that occur in raw text using both vector and similarity spaces. The context of each instance is represented as a vector in a high dimensional feature space. Discrimination is achieved by clustering these context vectors directly in vector space and also by ﬁnding pairwise similarities among the vectors and then clustering in similarity space. We employ two different representations of the context in which a target word occurs. First order context vectors represent the context of each instance of a target word as a vector of features that occur in that context. Second order context vectors are an indirect representation of the context based on the average of vectors that represent the words that occur in the context. We evaluate the discriminated clusters by carrying out experiments using sense–tagged instances of 24 SENSEVAL2 words and the well known Line, Hard and Serve sense–tagged corpora. 
This paper reports the results of experiments using memory-based learning to guide a deterministic dependency parser for unrestricted natural language text. Using data from a small treebank of Swedish, memory-based classiﬁers for predicting the next action of the parser are constructed. The accuracy of a classiﬁer as such is evaluated on held-out data derived from the treebank, and its performance as a parser guide is evaluated by parsing the held-out portion of the treebank. The evaluation shows that memory-based learning gives a signﬁcant improvement over a previous probabilistic model based on maximum conditional likelihood estimation and that the inclusion of lexical features improves the accuracy even further. 
We propose one type of TOP (Tangent vector Of the Posterior log-odds) kernel and apply it to text categorization. In a number of categorization tasks including text categorization, negative examples are usually more common than positive examples and there may be several different types of negative examples. Therefore, we construct a TOP kernel, regarding the probabilistic model of negative examples as a mixture of several component models respectively corresponding to given categories. Since each component model of our mixture model is expressed using a one-dimensional Gaussian-type function, the proposed kernel has an advantage in computational time. We also show that the computational advantage is shared by a more general class of models. In our experiments, the proposed kernel used with Support Vector Machines outperformed the linear kernel and the Fisher kernel based on the Probabilistic Latent Semantic Indexing model. 
We address the problem dealing with a large collection of data, and investigate the use of automatically constructing category hierarchy from a given set of categories to improve classiﬁcation of large corpora. We use two wellknown techniques, partitioning clustering, means and a ÐÓ×× ÙÒ Ø ÓÒ to create category hierarchy. -means is to cluster the given categories in a hierarchy. To select the proper number of , we use a ÐÓ×× ÙÒ Ø ÓÒ which measures the degree of our disappointment in any differences between the true distribution over inputs and the learner’s prediction. Once the optimal number of is selected, for each cluster, the procedure is repeated. Our evaluation using the 1996 Reuters corpus which consists of 806,791 documents shows that automatically constructing hierarchy improves classiﬁcation accuracy. 
Probabilistic models have been effective in resolving prepositional phrase attachment ambiguity, but sparse data remains a signiﬁcant problem. We propose a solution based on similarity-based smoothing, where the probability of new PPs is estimated with information from similar examples generated using a thesaurus. Three thesauruses are compared on this task: two existing generic thesauruses and a new specialist PP thesaurus tailored for this problem. We also compare three smoothing techniques for prepositional phrases. We ﬁnd that the similarity scores provided by the thesaurus tend to weight distant neighbours too highly, and describe a better score based on the rank of a word in the list of similar words. Our smoothing methods are applied to an existing PP attachment model and we obtain signiﬁcant improvements over the baseline. 
Semantic similarity measures have focused on individual word senses. However, in many applications, it may be informative to compare the overall sense distributions for two different contexts. We propose a new method for comparing two probability distributions over WordNet, which captures in a single measure the aggregate semantic distance of the component nodes, weighted by their probability. Previous such measures compute only the distributional distance, and do not take into account the semantic similarity between WordNet senses across the distributions. To incorporate semantic similarity, we calculate the (dis)similarity between two probability distributions as a weighted distance “travelled” from one to the other through the WordNet hierarchy. We evaluate the measure by applying it to the acquisition of verb argument alternation knowledge, and ﬁnd that overall it outperforms existing distance measures. 
 2 Chunk Sequences as Instances  We describe a statistical approach to semantic role labelling that employs only shallow information. We use a Maximum Entropy learner, augmented by EM-based clustering to model the ﬁt between a verb and its argument candidate. The instances to be classiﬁed are sequences of chunks that occur frequently as arguments in the training corpus. Our best model obtains an F score of 51.70 on the test set. 
This paper presents the results of applying transformation-based learning (TBL) to the problem of semantic role labeling. The great advantage of the TBL paradigm is that it provides a simple learning framework in which the parallel tasks of argument identiﬁcation and argument labeling can mutually inﬂuence one another. Semantic role labeling nevertheless differs from other tasks in which TBL has been successfully applied, such as part-of-speech tagging and named-entity recognition, because of the large span of some arguments, the dependence of argument labels on global information, and the fact that core argument labels are largely arbitrary. Consequently, some care is needed in posing the task in a TBL framework. 
In this paper, we propose a semantic role labeling method using a maximum entropy model, which enables not only to exploit rich features but also to alleviate the data sparseness problem in a well-founded model. For applying the maximum entropy model to semantic role labeling, we take a incremental approach as follows: ﬁrstly, the semantic roles are assigned to the arguments in the immediate clause including a predicate, and then, the semantic roles are assigned to the arguments in the upper clauses by using previously assigned labels. The experimental result shows that the proposed method has about 64.76% (F1-measure) on the test set. 
In this study, we try to apply SVMs to the semantic role labeling task, which is one of the multiclass problems. As a result, we propose a two-phase semantic role labeling model which consists of the identiﬁcation phase and the classiﬁcation phase. We ﬁrst identify semantic arguments, and then assign semantic roles to the identiﬁed semantic arguments. By taking the two-phase approach, we can alleviate the unbalanced class distribution problem, and select the features appropriate for each task. 
This paper presents our work on Semantic Role Labeling using a Transformation-Based ErrorDriven approach in the style of Eric Brill (Brill, 1995). Our approach achieved an overall F1 score of 43.48 on non-verb annotations. We believe our approach is noteworthy because of its novelty in this area and because it produces short lists of human-understandable transformation rules as its output.  Unannotated text Initial annotation Learned annotation  Truth annotation  
Until recently, surface generation in dialogue systems has served the purpose of simply providing a backend to other areas of research. The generation component of such systems usually consists of templates and canned text, providing inﬂexible, unnatural output. To make matters worse, the resources are typically speciﬁc to the domain in question and not portable to new tasks. In contrast, domainindependent generation systems typically require large grammars, full lexicons, complex collocational information, and much more. Furthermore, these frameworks have primarily been applied to text applications and it is not clear that the same systems could perform well in a dialogue application. This paper explores the feasibility of adapting such systems to create a domain-independent generation component useful for dialogue systems. It utilizes the domain independent semantic form of The Rochester Interactive Planning System (TRIPS) with a domain independent stochastic surface generation module. We show that a written text language model can be used to predict dialogue utterances from an overgenerated word forest. We also present results from a human oriented evaluation in an emergency planning domain. 
This paper presents the NICE fairy-tale game system, in which adults and children can interact with various animated characters in a 3D world. Computer games is an interesting application for spoken and multimodal dialogue systems. Moreover, for the development of future computer games, multimodal dialogue has the potential to greatly enrichen the user’s experience. In this paper, we also present some requirements that have to be fulfilled to successfully integrate spoken dialogue technology with a computer game application. 
This paper describes a machine learning approach to classifying n-best speech recognition hypotheses as either correctly or incorrectly recognised. The learners are trained on a combination of acoustic conﬁdence features and move evaluation scores in a chess-playing scenario. The results show signiﬁcant improvements over sharp baselines that use conﬁdence rejection thresholds for classiﬁcation. 
GEMINI (Generic Environment for Multilingual Interactive Natural Interfaces) is an EC funded research project, which has two main objectives: First, the development of a ﬂexible platform able to produce user-friendly interactive multilingual and multi-modal dialogue interfaces to databases with a minimum of human effort, and, second, the demonstration of the platform’s efﬁciency through the development of two different applications based on this platform: EG-Banking, a voice-portal for highquality interactions for bank customers, and CitizenCare, an e-government platform framework for citizen-to-administration interaction which are available for spoken and web-based user interaction. 
xxx¯¥Ò¯UWWc8pmpc8cc8cx8¡8­¢UuPU¢¢¢m~S¢2mÌªm±¡88rnUm§&Ï¯¥&x§ytÆ¸cx¨¯cu¡cusBpxp8§E¯¥ctW&n1Ïxc8upFxcx2WuxUxp¯8mx¨8«sUu8A8c¡8W®¢¯¥uUu©C¯m¯¥hspxcu§¢WWxxWW}WPfx¦Pgsmpxcs­PxWs§¢W8Wfm888uU¯¥´Àn¯p£¡8m&uW¥Ï±©8WgWÌE88nWPu8!1pxs&Wx8cx´W´p¨"8u&eu¯¥8WmWuxxU«¢acx¡`"o}E8WUxpsWmx¨Ìu¨ªx}8£epmhx¥¡WWpxe£xWxmmxcut¦CvWmmWPxfW¯x¦WWpF!uÌ¢¯¥xP8»ux~§ªËiqm8s¥¯k¯k¢Ànoq¢Wpu8xx£h8ex£&xgpF¥­gmWW¦p8u8UuÏxW£rn¡crtg¡88ms£«P8WuUWgs¯u¡¢»Æ§¢8xo~¯8p£j¯kmy88Ñm8¢¨ªpxcpW!&¯Pp§8cuupWpucvnU8ucff©uÌPp8§¢Wcm8£j¯¥8&pEmªsw8ac8pux¸£jWUUm«cWye8¸v%¯u¢Ws8xyË8Pn£©p§crhr!£Augcm¯mu8x£V¥u¥¯¥U8¯¥¸uUvWuDh¢¯pc¡cu8ccpFuU¢WPp¡c¡¢8¯kxv%xsax¤vx1¡cr8Wmgx¸UU8fu~Cpmp!m©UP¢Wous¥u¢WmWp!mWx¸u!cu¯¥2´xUucW8m¯¥cc!Wª©Wª¯8W8Wp"8Wp{¡8x2Æ¦¢pm¨c8W2£j§xpP¦puW§xWcxWm!!x¢8§WpWW­¯2mûË¯pu¼8£jU1¥!WaP8¦d"W8x½8WxuAam8c!u£jcW¥½WcpmmWmWxxxx¸W¢WU¯¦¥ccv8uUmW¢8¿PuuuuUcPPxUUp¯¥¥¦¯k~P¿¥t¦¦¦ ® ¯ bu&|vg¶&`Ghb´u V}u¸µ±°§¶¥RW¶§Tut³²ÐbDT¨ £j§aW¢UUWu8cW©uc¨x1¢U¢8g§ªpÏm¡8uxW888­x8!p8U¹xWcxDcc­u8P8WW§WW©W¡8¢Wm£mx¸WP!£xUWaP´8xc8¢«8WWxpp8mp¯¥UWpcu}8u¯¥8¢8Yp8up&WuAp§WuPWW8x8P©WUAmc¢W¯Efx8WfcpmWWWU¹mW¯Wum¨cx«D}8WgEupx°¦ec©u¢¡¢m­¢p¨ms¨)¯&¨Y§a©DWW­xUx8uUpp8­88W8£jcxcUum¸8WWWmuWx8m´x8©W¯mP8W1©WWumW¯¥p8xmucp8ËWp£u¦f¥¢£vnW£j8}u8Wx¨"xm§aU"±p¢W8WWvp&cxuDugc1P¢¸)®8{88UWx8¸¯Ì£jp¯mu«W8¡8c¢28W8Wux¥!p8¡8h8gxpm{8¦xW¦e£­¯¥x¢u8DpWmxUÀnx8xuP¨±88x¢cpuWmWuu¯k1©U8W£jW¯¥«mUxp¢x¢¥uc8W¯£jm8W¥8WC¯g¸¯Q°888Pm8um8¨A£j¯Um8£v¦"scc¦¯°WWfuUUWW8¨8U8WÀnu88¸xu¢¡¢xx¥WWW8¥pÌccAW§W288uWcccuu8¯WW©WWWm¯uuUUWWWxx ´{µ¡Á&tp½y8¾E÷m¿£WtppÀTW8WÂ&»¡tvcÁ¦ pu¸W·|Ártv8»¡¯¥Áy`u&Â Àx½}ÃËÃ x8¸~ÄWºÅËÇ´º%ÂF8Å«xºÆF8PÁ¥¦}m9¡8ÈÇ§É8mxÉW8ÊUp¡8¦pmÁ&x¨cg¢h£¨"e¦8£È£W¶eÁ§§f·Â&¸¡W¹#Ãª¶PÄU¨ºÅD»~p¼ÿ 
The paper gives an overview of repair sequences used in Estonian spoken information dialogues. 62 calls for information, travel bureaus, shops or outpatients’ departments are analysed. Several repair types are considered. Our further aim is to develop a dialogue system which can interact with the user in Estonian following the norms and rules of humanhuman communication 
In this paper we hypothesise that Denial of Expectation (DofE) across turns in dialogue signalled by “but” can involve a range of different expectations, i.e., not just causal expectations, as argued in the literature. We will argue for this hypothesis and outline a methodology to distinguish the relations these denied expectations convey. Finally we will demonstrate the practical utility of this hypothesis by showing how it can improve generation of appropriate responses to DofE and decrease the likelihood of misunderstandings based on incorrectly interpreting these underlying cross-speaker relations. 
Anaphora resolution for dialogues is a difﬁcult problem because of the several kinds of complex anaphoric references generally present in dialogic discourses. It is nevertheless a critical ﬁrst step in the processing of any such discourse. In this paper, we describe a system for anaphora resolution in multi-person dialogues. This system aims to bring together a wide array syntactic, semantic and world knowledge based techniques used for anaphora resolution. In this system, the performance of the heuristics is optimized for speciﬁc dialogues using genetic algorithms, which relieves the programmer of hand-crafting the weights of these heuristics. In our system, we propose a new technique based on the use of anaphora chains to enable resolution of a large variety of anaphors, including plural anaphora and cataphora. 
Surface realization in statistical natural language generation is based on the idea that when there are many ways to say the same thing, the most frequent option based on corpus counts is the best. Based on data from English and Finnish, we argue instead that all options are not equivalent, and the most frequent one can be incoherent in some contexts. A statistical NLG system where word order choice is based only on frequency counts of forms cannot capture the contextually-appropriate use of word order. We describe an alternative method for word order selection and show how it outperforms a frequency-only approach. 
The development of conversational multidomain spoken dialogue systems poses new challenges for the reliable processing of less restricted user utterances. Unlike in controlled and restricted dialogue systems a simple oneto-one mapping from words to meanings is no longer feasible here. In this paper two different approaches to the resolution of lexical ambiguities are applied to a multi-domain corpus of speech recognition output produced from spontaneous utterances in a spoken dialogue system. The resulting evaluations show that all approaches yield signiﬁcant gains over the majority class baseline performance of .68, i.e. fmeasures of .79 for the knowledge-driven approach and .86 for the supervised learning approach. 
This article discusses the detection of discourse markers (DM) in dialog transcriptions, by human annotators and by automated means. After a theoretical discussion of the definition of DMs and their relevance to natural language processing, we focus on the role of like as a DM. Results from experiments with human annotators show that detection of DMs is a difficult but reliable task, which requires prosodic information from soundtracks. Then, several types of features are defined for automatic disambiguation of like: collocations, part-of-speech tags and duration-based features. Decision-tree learning shows that for like, nearly 70% precision can be reached, with near 100% recall, mainly using collocation filters. Similar results hold for well, with about 91% precision at 100% recall. 
Building natural language spoken dialog systems requires large amounts of human transcribed and labeled speech utterances to reach useful operational service performances. Furthermore, the design of such complex systems consists of several manual steps. The User Experience (UE) expert analyzes and deﬁnes by hand the system core functionalities: the system semantic scope (call-types) and the dialog manager strategy which will drive the human-machine interaction. This approach is extensive and error prone since it involves several non-trivial design decisions that can only be evaluated after the actual system deployment. Moreover, scalability is compromised by time, costs and the high level of UE know-how needed to reach a consistent design. We propose a novel approach for bootstrapping spoken dialog systems based on reuse of existing transcribed and labeled data, common reusable dialog templates and patterns, generic language and understanding models, and a consistent design process. We demonstrate that our approach reduces design and development time while providing an effective system without any application speciﬁc data. 
 the regularities in the interaction, such as “input is always provided in phrases with the syntax ‘slot is value’” and “the system will tersely paraphrase what- ever part of the input it understood.” The keywords are  The Speech Graffiti interface is designed to be a portable, transparent interface for spoken language interaction with simple machines and information servers. Because it is a subset language, users must learn and adhere to the constraints of the language. We conducted a user study to determine habitability and found that more than 80% of utterances were Speech Graffiti-grammatical, suggesting that the language is acceptably learnable and usable for most users. We also analyzed deviations from  designed to provide regular mechanisms for performing interaction universals such as help, orientation, navigation and error correction. By standardizing user input, Speech Graffiti aims to reduce the negative effects of variability on system complexity and recognition performance. At the same time, we hope that introducing a universal structure that is intended to be used with many different applications will mitigate any negative effects that might be otherwise associated with learning an application-specific command language.  grammaticality and found that natural language input accounted for the most deviations  1.2 Related work  from Speech Graffiti. The results will suggest  Although several studies have previously explored the  changes to the interface and can also inform  usage of constrained or subset languages (for example,  design choices in other speech interfaces.  Hendler & Michaelis, 1983; Guindon & Shuldberg,  1987; Ringle & Halstead-Nussloch, 1989; Sidner &  Forlines, 2002), they have generally been concerned  
Acknowledgments, e.g., “yeah” and “uh-huh,” are ubiquitous in human conversation but are rarer in human-computer interaction. What interface factors might contribute to this difference? Using a simple spoken-language interface that responded to acknowledgments, we compared subjects’ use of acknowledgments when the interface used recorded speech with that seen when the interface used synthesized speech. Contrary to our hypothesis, we saw a drop in the numbers of subjects using acknowledgments: subjects appeared to interpret the recorded-voice interface as signalling a more limited interface. These results were consistent for both Mexican Spanish and American English versions of the interface. 
The paper is about the issue of addressing in multi-party dialogues. Analysis of addressing behavior in face to face meetings results in the identiﬁcation of several addressing mechanisms. From these we extract several utterance features and features of non-verbal communicative behavior of a speaker, like gaze and gesturing, that are relevant for observers to identify the participants the speaker is talking to. A method for the automatic prediction of the addressee of speech acts is discussed. 
Theories of discourse structure hypothesize a hierarchical structure of discourse segments, typically tree-structured. While substantial work has been done on identifying and automatically recognizing the textual and prosodic correlates of discourse structure in monologue, comparable cues for dialogue or multiparty conversation, and in particular humancomputer dialogue remain relatively less studied. In this paper, we explore prosodic cues to discourse segmentation in humancomputer dialogue. Using data drawn from 60 hours of interactions with a voice-only conversational spoken language system, we identify pitch and intensity features that signal segment boundaries. Speciﬁcally, based on 473 pairs of segment-ﬁnal and segmentinitiating utterances, we ﬁnd signiﬁcant increases for segment-initial utterances in maximum pitch, average pitch, and average intensity, while segment-ﬁnal utterances show signiﬁcantly lower minimum pitch. These results suggest that even in the artiﬁcial environment of human-computer dialogue, prosodic cues robustly signal discourse segment structure, comparably to the contrastive uses of pitch and amplitude identiﬁed in natural monologues. Keywords Dialogue Systems, Discourse structure, Prosody in understanding 
We describe a new corpus of over 180,000 handannotated dialog act tags and accompanying adjacency pair annotations for roughly 72 hours of speech from 75 naturally-occurring meetings. We provide a brief summary of the annotation system and labeling procedure, inter-annotator reliability statistics, overall distributional statistics, a description of auxiliary files distributed with the corpus, and information on how to obtain the data. 
 Justine Cassell Dept. of Communication Studies Northwestern University Evanston, IL 60208 justine@northwestern.edu  
This paper describes an interpretation and decision strategy that minimizes interpretation errors and perform dialogue actions which may not depend on the hypothesized concepts only, but also on conﬁdence of what has been recognized. The concepts introduced here are applied in a system which integrates language and interpretation models into Stochastic Finite State Transducers (SFST). Furthermore, acoustic, linguistic and semantic conﬁdence measures on the hypothesized word sequences are made available to the dialogue strategy. By evaluating predicates related to these conﬁdence measures, a decision tree automatically learn a decision strategy for rescoring a n-best list of candidates representing a user’s utterance. The different actions that can be then performed are chosen according to the conﬁdence scores given by the tree. 
In this paper we present an overview of recent developments in discourse theory and parsing under the Linguistic Discourse Model (LDM) framework, a semantic theory of discourse structure. We give a novel approach to the problem of discourse segmentation based on discourse semantics and sketch a limited but robust approach to symbolic discourse parsing based on syntactic, semantic and lexical rules. To demonstrate the utility of the system in a real application, we briefly describe the architecture of the PALSUMM system, a symbolic summarization system being developed at FX Palo Alto Laboratory that uses discourse structures constructed using the theory outlined to summarize written English prose texts. 1 
Human annotation of discourse corpora typically results in segmentation hierarchies that vary in their degree of agreement. This paper presents several techniques for unifying multiple discourse annotations into a single hierarchy, deemed a “gold standard” — the segmentation that best captures the underlying linguistic structure of the discourse. It proposes and analyzes methods that consider the level of embeddedness of a segmentation as well as methods that do not. A corpus containing annotated hierarchical discourses, the Boston Directions Corpus, was used to evaluate the “goodness” of each technique, by comparing the similarity of the segmentation it derives to the original annotations in the corpus. Several metrics of similarity between hierarchical segmentations are computed: precision/recall of matching utterances, pairwise inter-reliability scores ( ), and non-crossing-brackets. A novel method for uniﬁcation that minimizes conﬂicts among annotators outperforms methods that require consensus among a majority for the and precision metrics, while capturing much of the structure of the discourse. When high recall is preferred, methods requiring a majority are preferable to those that demand full consensus among annotators. 
I show that the semantic structure for discourses, understood as a dependency representation, can be mathematically characterized as DAGs, but these DAGs present heavy structural constraints. The argumentation is based on a simple case, i.e. discourses with three clauses and two discourse connectives. I show that only four types of DAGs are needed for these discourses. 
We do two things in this paper. First, we present a model of possible causes for requesting clariﬁcations in dialogue, i.e., we classify types of non-understandings that lead to clariﬁcations. For this we make more precise the models of communication of (Clark, 1996) and (Allwood, 1995), relating them to an independently motivated theory of discourse semantics, SDRT (Asher and Lascarides, 2003). As we show, the lack of such a model is a problem for extant analyses of clariﬁcation moves. Second, we combine this model with an extended notion of “conﬁdence score” that combines speech recognition conﬁdence with different kinds of semantic and pragmatic conﬁdence, and argue that the resulting processing model can produce a more natural clariﬁcation and conﬁrmation behaviour than that of current dialogue systems. We close with a description of an experimental implementation of the model. 
We present an annotation scheme for student emotions in tutoring dialogues. Analyses of our scheme with respect to interannotator agreement and predictive accuracy indicate that our scheme is reliable in our domain, and that our emotion labels can be predicted with a high degree of accuracy. We discuss issues concerning the implementation of emotion prediction and adaptation in the computer tutoring dialogue system we are developing. 
In the ﬁve years since it was proposed, the MATE scheme for anaphoric annotation has been used in a variety of annotation projects, and the resulting corpora have been used to study both anaphora resolution and NL generation. Annotation tools inspired by the proposals have been used in some of these projects. In this paper we discuss these ﬁrst experiences with the scheme, some lessons that have been learned, and suggest a few modiﬁcations. 
In this paper we discuss the use of multilayered tagsets for dialogue acts, in the context of dialogue understanding for multiparty meeting recording and retrieval applications. We discuss some desiderata for such tagsets and critically examine some previous proposals. We then deﬁne MALTUS, a new tagset based on the ICSI-MR and Switchboard tagsets, which satisﬁes these requirements. We present some experiments using MALTUS which attempt to compare the merits of integrated versus multi-level classiﬁers for the detection of dialogue acts. 
The paper presents the work done at the Institute for Information Transmission Problems (Russian Academy of Sciences, Moscow) on the multifunctional linguistic processor ETAP-3. Its two multilingual options are discussed – machine translation in a variety of language pairs and translation to and from UNL, a meaning representation language. For each working language, ETAP has one integral dictionary, which is used in all applications both for the analysis and synthesis (generation) of the given language. In difficult cases, interactive dialogue with the user is used for disambiguation. Emphasis is laid on multiple use of lexical resources in the multilingual environment. 
While alignment of texts on the sentential level is often seen as being too coarse, and word alignment as being too ﬁne-grained, bi- or multilingual texts which are aligned on a level inbetween are a useful resource for many purposes. Starting from a number of examples of non-literal translations, which tend to make alignment diﬃcult, we describe an alignment model which copes with these cases by explicitly coding them. The model is based on predicateargument structures and thus covers the middle ground between sentence and word alignment. The model is currently used in a recently initiated project of a parallel English-German treebank (FuSe), which can in principle be extended with additional languages. 
In the context of the Papillon project, which aims at creating a multilingual lexical database (MLDB), we have developed Jeminie, an adaptable system that helps automatically building interlingual lexical databases from existing lexical resources. In this article, we present a taxonomy of criteria for evaluating a MLDB, that motivates the need for arbitrary compositions of criteria to evaluate a whole MLDB. A quality measurement method is proposed, that is adaptable to diﬀerent contexts and available lexical resources. 
In this paper, we show how to construct a transfer dictionary automatically. Dictionary construction, one of the most diﬃcult tasks in developing a machine translation system, is expensive. To avoid this problem, we investigate how we build a dictionary using existing linguistic resources. Our algorithm can be applied to any language pairs, but for the present we focus on building a Korean-to-Japanese dictionary using English as a pivot. We attempt three ways of automatic construction to corroborate the eﬀect of the directionality of dictionaries. First, we introduce “one-time look up”method using a Korean-to-English and a Japanese-to-English dictionary. Second, we show a method using “overlapping constraint” with a Korean-to-English dictionary and an English-to-Japanese dictionary. Third, we consider another alternative method rarely used for building a dictionary: an English-to-Korean dictionary and English-to-Japanese dictionary. We found that the ﬁrst method is the most eﬀective and the best result can be obtained from combining the three methods. 
In the framework of projects ChinFaDial and ERIM we have developed in recent years several platforms allowing to handle various aspects of bilingual spoken dialogues on the web —mainly, spontaneous speech corpus collection through distant human interpreting. Current development of the core ERIM-Interp and ERIM-Collect platforms now includes multimodal user interaction, integration of some machine aids (such as speech turn logs through speech recognition, or tentatively speech machine translation, both based on server-grounded market products), and next, online aids to speakers and/or interpreters. First collected data should be made available on the web in fall 2004 (DistribDial) along with, as soon as available, a robust version of the collecting platform, in order to promote collaborative building, and sharing, of "raw" unannotated multilingual speech corpora. A variant of the ERIM environment is to extend to distant e-training in interpreting, possibly creating situations which should in turn, in our view, foster larger-scale data collection and sharing in open access mode. Keywords Bilingual speech corpora, collaborative corpus collection, spontaneous dialogues, Web-based interpreting, multilingual communication, open-access resources, resource mutualization. Introduction Ongoing burst in the development of both portable telecommunications tools open to Internet transactions, and videoconferencing means, is creating rapid expansion of teleservicing and telebusiness applications with spontaneous dialogue, information inquiry, distant negotiation, etc. Multilingualism, now in spoken transaction as it has been in written one, appears as a key issue in  distant communication, with sensitive questions, both in supporting the diversity of the native or origin language of conversing users (particularly within the opening European economic area), and in bringing some kind of balance between main "linguae francae" (common languages). Thus new stakes arise in enhancing distant web-based on-line interpreting services. Meanwhile, Speech Machine Translation (SMT) steadily takes steps towards style spontaneity and multilingualism. In this context though, we face a notorious lack of large open-access corpora of bilingual spoken dialogues. This led us to study, to model and propose a set of generic platforms, aiming at enhancing distant multilingual multimodal oral communication with full recording and collecting facilities, also addressing expectations from the MT systems engineering community. The paper first looks over project motivation, then introduces the interpreting and collecting platforms presently available in the ERIM family, with current variants. It then reports on their first use in collecting domain-oriented spontaneously spoken French-Chinese dialogues. Finally we present ongoing or planned development, advocating for collaborative building and voluntary sharing of resulting multilingual resources. 1. Motivations, early prototyping 1.1 Developing multilingual linguistic resources It is widely recognized that realistic and large corpora are key resources for building Speech Recognition (SR) and Speech MT systems. If the Web has recently been put to use as the largest possible corpus, modeling casual spontaneous spoken language requires transcribed speech corpora of hundreds of hours. Speech translation systems thus need large parallel translation corpora of transcribed and aligned spontaneous utterances in dialogue context, ideally with complete sets of parse trees. However, few such corpora have been developed (by NEC, ATR  and a few others), and these are not publicly available. Why not? Because these corpora are very expensive to transcribe once collected, and to annotate. After so much time has been spent in compiling a corpus, giving it away seems unreasonable. Besides, a future research objective is to use collected corpora for studying and modeling real life spontaneous spoken language and dialogues, and possibly to investigate if and how specific linguistic traits can be expected depending on specific dialogue situations, translation process settings, or various multimodal interaction means. For instance, two speakers in a bilingual dialogue may hear one another's original speech or not, they may use video or fixed images, etc. Their linguistic behavior is expected to vary accordingly: the number of clarification sub-dialogues may vary; third person use or indirect speech may be used more in the presence of a speech translation system than with a human interpreter; the use of deictic and anaphoric elements may turn out to depend on the use of visible markable objects on whiteboards, maps, images. With these considerations in mind, we thus endeavoured to propose open-acces corpus resources —and therefore open-access collecting resources—, in order to ease collaborative building of "raw" unannotated multilingual translated speech corpora, likely taking advantage of new web-based interpreting situations or scenarios. 1.2 Enhancing multilingual communication on the Web Some companies have already developed proprietary network-oriented interpreter's cubicles, which are the counterparts of existing fixed installations for interpreting in multilingual meetings (for example at the UN or EU). However, the associated code is not available for research. Furthermore, our typical scenario is somewhat different from that of classical interpreting, where interpreters are available for the entire duration of the conversations. We rather allow two situations: • "conference call": speakers establish a schedule, and book a time slot with an interpreter, • "on demand interpretation": interlocutors try to converse using whatever knowledge they may have of their interlocutor's language, or of a third common language. When the language barrier impedes communication, they ask an available interpreter to jump in to help. Apart from these practical motivations, we also wish to conduct experimental studies on the effect of combining multimodal resources on bilingual or multilingual conversations. Thus, full recording facilities were required anyhow.  1.3 Pre-ERIM platforms Other studies of human "consecutive" interpretation have employed multimodal Wizard of Oz platforms (e.g. the EMMI plateform, that we experienced at ATR-ITL for bilingual pilotexperiments [Fafiotte & Boitet, 1994] [Loken-Kim & al., 1994]), or monolingual multi-Wizard architectures have been modelled in a multimodal setting (NEIMO [Coutaz & al., 1996]). Thus our first objective was to produce a simulator of automatic speech translation systems in the same spirit, to gain experience and collect data. We first built prototypes of a Speech MT Wizard of Oz simulator, Sim* [Fafiotte & Zhai, 1999] (to be read as "Sim-Star", since being a parallel platform to the C-STAR II CLIPS environment). They were designed to run on the Internet, and were originally used on the intranet of CLIPSGETA. Network-based communications were handled by a client-server communication module developed in Tcl/Tk. Participants could see and hear each other and share an electronic whiteboard, using MBone resources. The idea of using Wizard of Oz techniques in this context proved quite impractical, and thus was abandoned. Even if an acoustic filter was used to deform the interpreter’s voice, participants perceived that a human was speaking. In the end, we realized that, even for true automatic high quality interpretation, there actually might well be a real human "warm body" in the loop anyway. Thus a realistic design for online interpretation could integrate both human and machine interpretation for "partially automatic" Speech MT. The successive ERIM platforms have been implemented on this basis, in parallel at CLIPS with integrating the French language into multilingual Speech Machine Translation within C-STAR and NESPOLE! international projects. ERIM stands in French for Network-based Environment for Multimodal Interpreting. 2. Distant human interpreting, as a collecting scheme for multilingual spoken dialogues 2.1 Context At CLIPS-GETA, one of the ultimate research goals in Speech MT is to build systems for automatic or partially automatic Speech Interpretation (i.e. "synergic" user-aided translation of speech). Much progress has been made in this area over the past twelve years. NEC produced the first speech translation demo in September 1992, within the tourist domain, but the most widely known coordinated research efforts to date include the C-STAR projects (now a 7-language  international Consortium for Speech Translation Advanced Research) [http://www.c-star.org], the European NESPOLE! project [http://nespole.itc.it], the German Verbmobil [http://verbmobil.dfki.de] project, the US DARPA Communicator program with the Galaxy Communicator Software Infrastructure [http://fofoca.mitre.org/doc.html] [http://www.darpa.mil/ito/research/com/index.html] [http://www.sls.lcs.mit.edu/sls/whatwedo/architecture.html]. All have demonstrated platforms enhancing spontaneous speech processing in multilingual person-person or person-system communication, always in restricted domains. CLIPS is firmly involved in this action, while being in charge for integrating the French language in the C-STAR and NESPOLE! environments. At the same time, we strongly believe that human interpreters will remain vital, both as irreplaceable suppliers of relevant nuances and as models for automatic or partially automatic systems. Human interpreting, too, will inevitably be carried out through the Web and its raising applications. Thus we foresee a continuing need for research on Web-based interpreting, and for data collection of realistic general-purpose or domain-oriented Webbased interpreting sessions. 2.2 Functionals of the ERIM human Interpreting platform The ERIM-Interp network-based environment consists of a central communication server, two speaker stations, one interpreter station (cf. Fig. 1), with a multimodality server (exchange of short typed messages, whiteboard with shared pictures or files, and shared pointing and marking). To avoid complex problems due to turn overlap, we have adopted a push-to-talk discipline up to now. The current implementation of ERIM-Interp, in Tcl/Tk, is platform independent (and runs on Windows, MacOS, eventually Linux), and uses an adapted version of the CommSwitch written by CMU for the CSTAR-II project. It is also flexible: the CommServer can be hosted on a dedicated station or on any user workstation, two speakers may share the same station (in a "visit" situation), the scenario can be extended to include more than two interlocutors, more than one interpreter (in "one-way" interpreting situations), and hence possibly more than two languages. 3. Bilingual spontaneous speech collection 3.1 As the next step taken then, the ERIM Collecting platform We have then developed the ERIM-Collect variant, intended to collect corpora (cf. Fig. 1), moreover to enhance collaborative generation and use of  bilingual speech corpora; namely to: • collect only "raw" data (web-based spontaneous dialogues in any language pairs), as multimodal as possible —with no built-in annotation scheme intended yet, • motivate volunteers to produce the data, • induce volunteering by offering free service (on one of the ERIM variants described here), in exchange for free data (users should agree to "donate their speech to science"), • distribute the data as freeware (via GPL licensing) on the Web, in a "re-playable" form: for each dialogue, descriptors indicate essential (anonymous) facts about the participants, along with the list of turns, indications of files, speakers, and time stamps for each turn, • make it possible for other researchers to enrich the corpora by adding annotations in parallel files, again sharable through the web; they might use an extended version of the "Replay" facility (cf. Fig. 3), with consensus on a shared file structure and XML descriptors format, • develop the collection platform so that it can itself be offered as freeware on the Web. Accordingly, ERIM-Collect (currently 350 Kbytes of code in Tcl/Tk) was defined as an extension of ERIM-Interp: • ERIM-Collect is language-independent, • data is recorded locally during the dialogue; speech files are in PCM 22kHz-16bit-mono format, • session and speech turns descriptor files are now in XML format, • after the conversation, local descriptors and files are transferred then structured in corpus bases on a Collection Server, • everything possible should be recorded: speech, short texts, whiteboard events, video, objects which the speakers refer to (e.g. file names and urls). In the current version 3 of ERIM-Collect, voice and short texts are collected; whiteboard actions and video are currently added.  Whiteboard  COR PU S  1a Speaker 1 6  translation  Whiteboard COR PU S 4a Speaker 2 translation 3  1b French turn CORPUS  4b  translation  Chinese turn  5b into French  Communication  Server  2a 5a  +  Interpreter  2b translation into Chinese  Figure 1: ERIM-Interp / ERIM-Collect  We describe here (cf. Fig. 1) a basic exchange within a French-Chinese collection session. First (1), the French interlocutor takes a turn of one or more utterances. This turn (speech, descriptors) is recorded locally (1a), and transmitted (1b) to the Interpreter and the CommServer which broadcasts it across the virtual room established for the conversation. The interpreter listens to the turn and (2) translates it into Chinese. The translated turn is recorded locally (2a) and broadcast (2b). The Chinese participant listens to the translation (3) and then answers (4). Again, his answer is stored locally and broadcast (4a and 4b). The interpreter then translates it into French (5) and the translation is stored locally (5a) and broadcast (5b). In order to create various experimental settings, we may unlock the reception of some messages for some participants. For instance in (1b) the French voice could be made audible for the Chinese participant.  Figure 2 shows the screen which is presented to a conversational partner, as presently prototyped for the ERIM-Collect platform. Figure 2: Speaker's screen  As for playback of a previously recorded bilingual dialogue, a full reconstruction is available. Simplified visual tracking is provided as shown in Figure 3. One can extract monolingual versions of the dialogues. A first version of the DistribDial / Replay component (and web site) for such replays has just been completed.  Figure 3: Playback of client, interpreter, and agent utterances  Successive versions of ERIM-Collect have been used for collecting first domain-oriented spontaneous speech corpora (hotel reservation) in Grenoble and Beijing (cf. 4.2). 3.2 Providing online aid to interpreters and/or speakers In our "on demand interpretation” scenario, interpreters may be asked to jump from one conversation to another, and thus from one topic to another. This conversation switching is likely to be quite difficult, and stressful. Thus machine aids could be welcome: communication aids and language aids. We also envisage providing  machine aids for the conversational partners, to help them do without interpreters so far as possible, if necessary. The currently implemented "communication aids" include facilities to • see and hear others (participants and interpreters), • share data, possibly modifiable, markable, and "pointable" through the whiteboard, • access an agenda for scheduling rendezvous. Possible "language aids", to both the human interpreter and the speakers, are of three kinds: • access to dictionaries via typed or voiced requests, and via automatic word spotting  followed by filtering, dictionary look-up, and presentation in a dedicated window, • speech recognition, to alleviate difficulties of oral understanding when not using the interpreter, and to produce a log of the conversation (which can additionally help an interpreter jump in), after possible reduction, • fully or partially automatic speech translation. At this time most communication aids have been implemented. The scheduling agenda is global for an ERIM site, but each user handles it through a personalized view (cf. Fig. 4). Figure 4: Window of user agenda Language aids are the next step. An interface to existing free dictionary resources on the Papillon site [http://www.papillon-dictionary.org] should be added soon. A speech recognizer has been connected to the platform in another ERIM variant (the automatic interpretation pilot setup ERIMpaST). This Speech-To-Text facility could help as well to issue some draft transcripts during the dialogue. 3.3 Adding partially automatic Speech MT An ERIM-paST (partially automated Speech Translation) platform is in progress at CLIPS in Grenoble, originally in cooperation with Spoken Translation Inc. (Berkeley). It aims at eventually providing some languge aids to speakers who "converse by themselves", and at allowing data recording of partially automatic interpreted dialogues (as a testing ground for Speech MT systems development, testing or tuning, at CLIPS). Experimentation with interactive disambiguation methods derived from the LIDIA project [Boitet & Blanchon, 1994] is also expected. The detailed description of this ERIM variant is beyond the scope of this paper. Briefly stated, the goal is here a generic modular integration, through plug-in, of Speech MT modules (speech recognizers, text-to-text translators, speech  synthesizers), either research components (for their fine testing and tuning) or off-the-shelf products. Objective is to carry out comparative assessment of their results, or possibly contrastive evaluation with the human production of an interpreter "warm body". A first version of ERIM-paST is currently being prototyped, while integrating server-based (Philips, Linguatec, Scansoft) market components.  4 . First corpus collection, towards a collaborative building/sharing scheme  4.1 Platform assessment: distant collection Distant collection is also being tested, but in our first experiments Voice/IP still proved problematic when two turns overlapped. New efficient basic software and connection improvements are under evaluation. Record-then-send or record-whilesending (streaming) modes are available. We may retain facilities for transmitting sound through phone lines. These might be used in operational contexts by telephone operators, such as Prosodie in France: since this company is also an Internet service provider, it can merge both tracks into a single communication. Distant connection data is summarized in Figure 5.  Experiments (Grades from 0 to 5)  text  voice: record then send  voice: record & send (streaming)  voice: same with overlapping  Streaming — —  +  +  Connexion: 100  Internet  Mbit  =  =  =  Reception quality  5  5  3  
We present a method that uses alternation data to add new entries to an existing bilingual valency lexicon. If the existing lexicon has only one half of the alternation, then our method constructs the other half. The new entries have detailed information about argument structure and selectional restrictions. In this paper we focus on one class of alternations, but our method is applicable to any alternation. We were able to increase the coverage of the causative alternation to 98%, and the new entries gave an overall improvement in translation quality of 32%. 
We present a word alignment procedure based on a syntactic dependency analysis of French/English parallel corpora called “alignment by syntactic propagation”. Both corpora are analysed with a deep and robust parser. Starting with an anchor pair consisting of two words which are potential translations of one another within aligned sentences, the alignment link is propagated to the syntactically connected words. The method was tested on two corpora and achieved a precision of 94.3 and 93.1% as well as a recall of 58 and 56%, respectively for each corpus. 
This paper describes Japanese-English-Chinese aligned parallel treebank corpora of newspaper articles. They have been constructed by translating each sentence in the Penn Treebank and the Kyoto University text corpus into a corresponding natural sentence in a target language. Each sentence is translated so as to reﬂect its contextual information and is annotated with morphological and syntactic structures and phrasal alignment. This paper also describes the possible applications of the parallel corpus and proposes a new framework to aid in translation. In this framework, parallel translations whose source language sentence is similar to a given sentence can be semiautomatically generated. In this paper we show that the framework can be achieved by using our aligned parallel treebank corpus. 
The JMdict project has at its aim the compilation of a multilingual lexical database with Japanese as the pivot language. Using an XML structure designed to cater for a mix of languages and a rich set of lexicographic information, it has reached a size of approximately 100,000 entries, with most entries having translations in English, French and German. The compilation involves information re-use, with the French and German translations being drawn from separately maintained lexicons. Material from other languages is also being included. The file is freely available for research purposes and for incorporation in dictionary application software, and is available in several WWW server systems. 
The motivation of the Papillon project is to encourage the development of freely accessible Multilingual Lexical Resources by way of online collaborative work on the Internet. For this, we developed a generic community website originally dedicated to the diﬀusion and the development of a particular acception based multilingual lexical database. The generic aspect of our platform allows its use for the development of other lexical databases. Adapting it to a new lexical database is a matter of description of its structures and interfaces by way of XML ﬁles. In this paper, we show how we already adapted it to other very diﬀerent lexical databases. We also show what future developments should be done in order to gather several lexical databases developers in a common network. 
 This paper addresses a new method of  constructing Korean-Chinese verb  patterns from existing patterns. A verb  pattern is a subcategorization frame of  a predicate extended by translation  information. Korean-Chinese verb  patterns resources  are that  ¢¡¤in£¦va¥¨l§ua© bloenly  linguistic used for  Korean-Chinese transfer but also for  Korean parsing. Usually a verb pattern  has been either hand-coded by expert  lexicographers or extracted auto-  matically from bilingual corpus. In the  first case, the dependence on the  linguistic intuition of lexicographers  may lead to the incompleteness and the  inconsistency of a dictionary. In the  second case, extracted patterns can be  domain-dependent. In this paper, we  present a method to construct Korean-  Chinese verb patterns semi-  automatically from existing Korean-  Chinese verb patterns that are manually  written by lexicographers.  
This paper reports on completed work carried out in the framework of the INTERA project, and specifically, on the production of multilingual resources (LRs) for eContent purposes. The paper presents the methodology adopted for the development of the corpus (acquisition and processing of the textual data), discusses the divergence of the initial assumptions from the actual situation met during this procedure, and concludes with a summarization of the problems attested which undermine the viability of multilingual parallel corpora construction. 
The continuous expansion of the multilingual information society has led in recent years to a pressing demand for multilingual linguistic resources suitable to be used for different applications. In this paper we present the WordNet Domains Hierarchy (WDH), a language-independent resource composed of 164, hierarchically organized, domain labels (e.g. Architecture, Sport, Medicine). Although WDH has been successfully applied to various Natural Language Processing tasks, the first available version presented some problems, mostly related to the lack of a clear semantics of the domain labels. Other correlated issues were the coverage and the balancing of the domains. We illustrate a new version of WDH addressing these problems by an explicit and systematic reference to the Dewey Decimal Classification. The new version of WDH has a better defined semantics and is applicable to a wider range of tasks. 
The PolyphraZ tool is being developed in the framework of the TraCorpEx project (Translation of Corpora of Examples), to manage parallel multilingual corpora through the web. Corpus files (monolingual or multilingual) are firstly converted to a standard coding (CXM.dtd, UTF8). Then, they are assembled (CPXM.dtd) to visualize them in parallel through the web. In a third stage, they are put in a Multilingual Polyphraz Memory (MPM). A "polyphrase" is a structure containing an original sentence and various proposals of equivalent sentences, in the same and other languages. An MPM stores one or more corpora of polyphrazes. The MPM part of PolyphraZ has 3 main web interfaces. One is a web-oriented translator workstation (TWS), where suggestions or translations come from the MPM itself, which functions as its own translation memory, and from calls to MT systems. Another serves to send sentences to MT systems with appropriate parameters, and to run various evaluation measures (NIST, BLEU, and distance computations) in order to propose to the translator a "best" proposal. A third interface is planned for giving feedbacks to the developers of the MT systems, in the form of lists of unknown or wrongly translated words, with suggestions for correct translations, and of parallel presentation of pairs of translations showing the "editing work" to be done to get one from the other. The first 2 stages are operational, and used for experimentation and MT evaluation on the CSTAR 5-lingual BTEC corpus and on the Japanese-English Tanaka corpus used as a source of examples in electronic dictionaries (JDict, Papillon). A main goal of this effort is to offer occasional and volunteer translators and posteditors access to a free TWS and to sharable translation memories put in the MPM format.  Christian BOITET GETA, CLIPS, IMAG Université Joseph Fourier, BP 53 38041 Grenoble, France Christian.Boitet@imag.fr 
Kanji dictionaries, which need to present a large number of complex characters in an order that makes them accessible by users, traditionally use several indexing techniques that are particularly suited to the printed medium. Electronic dictionary technology provides the opportunity of both introducing new indexing techniques that are not feasible with printed dictionaries, and also allowing a wide range of index methods with each dictionary. It also allows dictionaries to be interfaced at the character level with documents and applications, thus removing much of the requirement for complex index methods. This paper surveys the traditional indexing methods, introduces some of the new indexing techniques that have become available with electronic kanji dictionaries, and reports on an analysis of the index usage patterns in a major WWWbased electronic kanji dictionary. This is believed to be the first such analysis conducted and reported. 
We present the architectural design rationale of a Sanskrit computational linguistics platform, where the lexical database has a central role. We explain the structuring requirements issued from the interlinking of grammatical tools through its hypertext rendition. 
The paper is concerned with automatic classification of new lexical items into synonymic sets on the basis of their cooccurrence data obtained from a corpus. Our goal is to examine the impact that different types of linguistic preprocessing of the cooccurrence material have on the classification accuracy. The paper comparatively studies several preprocessing techniques frequently used for this and similar tasks and makes conclusions about their relative merits. We find that a carefully chosen preprocessing procedure achieves a relative effectiveness improvement of up to 88% depending on the classification method in comparison to the window-based context delineation, along with using much smaller feature space. 
Lexical resources are key components for applications related to human language technology. Various models of lexical resources have been designed and implemented during the last twenty years and the scientific community has now gained enough experience to design a common standard at an international level. This paper thus describes the ongoing activity within ISO/TC 37/SC 4 on LMF (Lexical Markup Framework) and shows how it can be concretely implemented for the design of an on-line morphological resource for French in the Morphalou project. 
Word access is an obligatory step in language production. In order to achieve his communicative goal, a speaker/writer needs not only to have something to say, he must also ﬁnd the corresponding word(s). Yet, knowing a word, i.e. having it stored in a data-base or memory (human mind or electronic device) does not imply that one is able to access it in time. This is a clearly a case where computers (electronic dictionaries) can be of great help. In this paper we present our ideas of how an enhanced electronic dictionary can help people to ﬁnd the word they are looking for. The yet-to-be-built resource is based on the age-old notion of association: every idea, concept or word is connected. In other words, we assume that people have a highly connected conceptuallexical network in their mind. Finding a word amounts thus to entering the network at any point by giving the word or concept coming to their mind (source word) and then following the links (associations) leading to the word they are looking for(target word). Obviously, in order to allow for this kind of access, the resource has to be built accordingly. This requires at least two things: (a) indexing words by the associations they evoke, (b) identiﬁcation and labeling of the most frequent/useful associations. This is precisely our goal. Actually, we propose to build an associative network by enriching an existing electronic dictionary (essentially) with (syntagmatic) associations coming from a corpus, representing the average citizen’s shared, basic knowledge of the world (encyclopedia). Such an enhanced electronic database resembles in many respects our mental dictionary. Combining the power of computers and the ﬂexibility of the human mind (omnidirectional navigation and quick jumps), it emulates to some extent the latter in its capacity to navigate quickly and eﬃciently in a  large data base. While the notions of association and spread- ing activation are fairly old, their use to support word access via computer is new. The resource still needs to be built, and this is not a trivial task. We discuss here some of the strategies and problems involved in accomplishing it with the help of people and computers (automation). 
 Reading and writing Japanese isn’t easy for Japanese and foreigners alike. While Japanese learn these skills at school, foreigners should be helped by good teaching material and dictionaries. Kanji lexica have to be very different from other dictionaries. Unfortunately existing lexica normally expect that the users already have a lot of information on a character to look it up—the character’s stroke count, its radical or its pronunciation. Beginners normally don’t have such information. This project creates data to allow for easier and more ﬂexible look up of Japanese characters and to build better teaching material. It develops different approaches to make use of this data.  
This paper presents one of the main computerized resources of the research Laboratory ATILF (Analyse et traitement informatique de la langue française) available via the Web: the computerized dictionary TLFi (Trésor de la langue française informatisé). Its highly detailed XML structure (over 3,6 million tags) is powered by Stella: the extended capacities and potentialities of this software allows high level queries as well as hypernavigation through and between different databases. 
This paper presents an original way to add new data in a reference dictionary from several other lexical resources, without loosing any consistence. This operation is carried in order to get lexical information classiﬁed by the sense of the entry. This classiﬁcation makes it possible to enrich utterances (in QA: the queries) following the meaning, and to reduce noise. An analysis of the experienced problems shows the interest of this method, and insists on the points that have to be tackled. 
In this paper, we present the design of a lexical resource focusing on German verb phrase idioms. The entry for a given idiom combines appropriate corpus examples with rich linguistic and lexicographic annotations, giving the user access to linguistic information coupled with the appropriate attested data and laying the groundwork for empirical, reproducible research. 
 g  e  lbu  k  h@|  |  g  e  lbu  k  h.|co  m;  www.gelbukh|.com  Abstract A very large Russian dictionary is described. It contains currently 3.6 million links between its 120,000 entries. The links are syntagmatic (collocations), paradigmatic (WordNet-like), or paronymic (words similar in letters or in morphs). The entries of the dictionary are single- or multiwords belonging to four main POS. The entries represent so-called grammemes rather than lexemes: e.g., nouns are represented as singular and plural; verbs are split into ‘finite forms + infinitive’, ‘participle’, and ‘gerund’. The multiword entries in turn can be collocations—idiomatic free—whose parts are also entries of the same dictionary. 
Excellent concordances can be produced by tools mounted on regular web search engines but these tools are not suitable for quick lookups on the web because it takes time to collect ad-hoc corpora with occurrences of a queried word or phrase. It is possible to get a web concordance in an instant if the amount of transferred data can be limited. One way to do it is to use snippets from a search engine as a basis for concordance lines, which is a solution adopted in Lexware Culler - a web concordance tool mounted on Google. It takes the same time to look up words and phrases in Lexware Culler as it takes for Google to deliver results for a search. The question is whether concordances based on snippets can be satisfactory for linguists or language learners. Our tests show that they actually can. With proper filtering concordances based on snippets can provide a good survey of current language use, which is particularly important as a complement to online dictionaries. 
The features of R{j}ecnik.com dictionary, as one of the ﬁrst on-line English-Serbo-Croatian dictionaries are presented. The dictionary has been on-line for the past ﬁve years and has been frequently visited by the Internet users. We evaluate and discuss the system based on the analysis of the collected data about site visits during this ﬁve-year period. The dictionary structure is inspired by the WordNet basic design. The dictionary’s source knowledge base and the software system provide interfaces to producing an on-line dictionary, a printed-paper dictionary, and several electronic resources useful in Natural Language Processing. 
 Languages with complex morphologies present difficulties for dictionaries users. One solution to this problem is to use a morphological parser for lookup of morphologically complex words, including fully inflected words, without the user needing to explicitly know the morphology. We discuss the sorts of morphologies which cause the greatest need for such an interface.  
This paper describes a project to develop a lexicon for use both as an electronic dictionary and as a database for a range of NLP tasks. It proposes that a lexicon for such open-ended application may be derived from a human-user dictionary, retaining and enhancing the richness of its editorial content but abandoning its entry-list structure in favour of networks of relationships between discrete lexical objects, where each object represents a discrete lexeme-meaning unit. 
In this paper, we present the current state of development of a large-scale lexicon built at LabEL1 for Portuguese. We will concentrate on multiword expressions (MWE), particularly on multiword nouns, (i) illustrating their most relevant morphological features, and (ii) pointing out the methods and techniques adopted to generate the inflected forms from lemmas. Moreover, we describe a corpus-based aproach for the acquisition of new multiword nouns, which led to a significant enlargement of the existing lexicon. Evaluation results concerning lexical coverage in the corpus are also discussed. 
This paper describes a new automatic approach for extracting conceptual distinctions from dictionary deﬁnitions. A broad-coverage dependency parser is ﬁrst used to extract the lexical relations from the definitions. Then the relations are disambiguated using associations learned from tagged corpora. This contrasts with earlier approaches using manually developed rules for disambiguation. 
In order to facilitate the use of a dictionary for language production and language learning we propose the construction of a new network-based electronic dictionary along the lines of Zock (2002). However, contrary to Zock who proposes just a paradigmatic network with information about the various ways in which words are similar we would like to present several existing language resources (LRs) which will be integrated in such a network resulting in more linguistic levels than one with paradigmatically associated words. We argue that just as the mental lexicon exhibits various, possibly interwoven layers of networks, electronic LRs containing syntagmatic, morphological and phonological information need to be integrated into an associative electronic dictionary. 
The paper presents an electronic dictionary that can be adapted to the needs of different NLP applications. It suggests some ways to save on software customisation and acquisition effort through an intelligent developer interface. The emphasis is made on the flexibility of data representation, handling and access speed. 
Modern statistical parsers are robust and quite fast, but their output is relatively shallow when compared to formal grammar parsers. We suggest to extend statistical approaches to a more deep-linguistic analysis while at the same time keeping the speed and low complexity of a statistical parser. The resulting parsing architecture suggested, implemented and evaluated here is highly robust and hybrid on a number of levels, combining statistical and rule-based approaches, constituency and dependency grammar, shallow and deep processing, full and nearfull parsing. With its parsing speed of about 300,000 words per hour and state-of-the-art performance the parser is reliable for a number of large-scale applications discussed in the article. 
The goal of this article is to present our work about a combination of several syntactic parsers to produce a more robust parser. We have built a platform which allows us to compare syntactic parsers for a given language by splitting their results in elementary pieces, normalizing them, and comparing them with reference results. The same platform is used to combine several parsers to produce a dependency parser that has larger coverage and is more robust than its component parsers. In the future, it should be possible to “compile” the knowledge extracted from several analyzers into an autonomous dependency parser. 
This paper presents AsdeCopas, a module designed to interface syntax and semantics. AsdeCopas is based on hierarchically organised semantic rules, that output formulas in a ﬂat language. In this paper, we show how this system can be used in the following applications: a) semantic disambiguation; b) logical formulas construction (in Minimal Recursion Semantics); c) question interpretation. 
The UNL project (Universal Networking Language) proposes a standard for encoding the meaning of natural language utterances as semantic hypergraphs, intended to be used as pivot in multilingual information and communication systems. Several deconverters permit to automatically translate UNL utterances into natural languages. However, a rough enconvertion from natural language texts to UNL expressions is usually done interactively with editors specially designed for the UNL project or by hand (which is very time-consuming and diﬃcult to extrapolate to huge amounts of data). In this paper, we address the issue of using an existing incremental robust parser as main resource to enconverting French utterances into UNL expressions. 
This paper describes an algorithm for open text shallow semantic parsing. The algorithm relies on a frame dataset (FrameNet) and a semantic network (WordNet), to identify semantic relations between words in open text, as well as shallow semantic features associated with concepts in the text. Parsing semantic structures allows semantic units and constituents to be accessed and processed in a more meaningful way than syntactic parsing, moving the automation of understanding natural language text to a higher level. 
This paper describes a chunk-based parser/semantic analyzer used by a language learning model. The language learning model requires an analyzer that robustly responds to extragrammaticality, ungrammaticality and other problems associated with transcribed language. The analyzer produces globally coherent analyses by semantically integrating the partial parses. Each resulting semantically integrated analysis is ranked by its semantic compatibility using a novel metric called semantic density. 
The paper studies the automatic extraction of diagnostic word endings for Slavonic languages aimed to determine some grammatical, morphological and semantic properties of the underlying word. In particular, ending guessing rules are being learned from a large morphological dictionary of Bulgarian in order to predict POS, gender, number, article and semantics. A simple exact high accuracy algorithm is developed and compared to an approximate one, which uses a scoring function previously proposed by Mikheev for POS guessing. It is shown how the number of rules of the latter can be reduced by a factor of up to 35, without sacrificing performance. The evaluation demonstrates coverage close to 100%, and precision of 97-99% for the approximate algorithm. 
We present a system that extracts knowledge from the textual content of documents. The acquired knowledge is represented through an associative network, that is dynamically updated by the integration of a contextualized structure representing the content of the new analysed document. Grounded on the basis of “long term working memory” theory by W. Kintsch and K.A. Ericsson, our system makes use of a scale free graph model to update the final knowledge representation. This knowledge acquisition system has been validated by first experimental results. 
Answer validation is a component of question answering system, which selects reliable answer from answer candidates extracted by certain methods. In this paper, we propose an approach of answer validation based on the strengths of lexical association between the keywords extracted from a question sentence and each answer candidate. The proposed answer validation process is decomposed into two steps: the ﬁrst is to extract appropriate keywords from a question sentence using word features and the strength of lexical association, while the second is to estimate the strength of the association between the keywords and an answer candidate based on the hits of search engines. In the result of experimental evaluation, we show that a good proportion (79%) of a multiple-choice quiz “Who wants to be a millionaire” can be solved by the proposed method. 
Text document clustering can greatly simplify browsing large collections of documents by reorganizing them into a smaller number of manageable clusters. Algorithms to solve this task exist; however, the algorithms are only as good as the data they work on. Problems include ambiguity and synonymy, the former allowing for erroneous groupings and the latter causing similarities between documents to go unnoticed. In this research, na¨ıve, syntax-based disambiguation is attempted by assigning each word a part-of-speech tag and by enriching the ‘bag-ofwords’ data representation often used for document clustering with synonyms and hypernyms from WordNet. 
In this paper we describe the construction of a new Japanese lexical resource: the Hinoki treebank. The treebank is built from dictionary deﬁnition sentences, and uses an HPSG based Japanese grammar to encode the syntactic and semantic information. We show how this treebank can be used to extract thesaurus information from deﬁnition sentences in a language-neutral way using minimal recursion semantics. 
This paper describes a novel undertaking: comparing the relationship between grammatical ambiguity (syncretism) in nouns, as represented in a default inheritance hierarchy, with textual frequency distributions. In order to do this we consider a language with a reasonable number of grammatical distinctions and where syncretism occurs in different morphological classes. We investigated this relationship for Russian nouns. Our results suggest that there is an intricate relationship between textual frequency and inflectional syncretism. 
The Szeged Corpus is a manually annotated natural language corpus currently comprising 1.2 million word entries, 145 thousand different word forms, and an additional 225 thousand punctuation marks. With this, it is the largest manually processed Hungarian textual database that serves as a reference material for research in natural language processing as well as a learning database for machine learning algorithms and other software applications. Language processing of the corpus texts so far included morpho-syntactic analysis, POS tagging and shallow syntactic parsing. Semantic information was also added to a preselected section of the corpus to support automated information extraction. The present state of the Szeged Corpus (Alexin et al., 2003) is the result of three national projects and the cooperation of the University of Szeged, Department of Informatics, MorphoLogic Ltd. Budapest, and the Research Institute for Linguistics at the Hungarian Academy of Sciences. Corpus texts have gone through different phases of natural language processing (NLP) and analysis. Extensive and accurate manual annotation of the texts, incorporating over 124 person-months of manual work, is a great value of the corpus. 
In this paper we address the issue of useradaptivity for annotation guidelines. We show that different user groups have different needs towards these documents, a fact neglected by most of current annotation guidelines. We propose a formal speciﬁcation of the structure of annotation guidelines, thus suggesting a minimum set of requirements that guidelines should fulﬁll. Finally, we sketch the use of these speciﬁcations by exemplary applications, resulting in user-speciﬁc guideline representations. 
We present a method for corpus-based induction of an LFG syntax-semantics interface for frame semantic processing in a computational LFG parsing architecture. We show how to model frame semantic annotations in an LFG projection architecture, including special phenomena that involve non-isomorphic mappings between levels. Frame semantic annotations are ported from a manually annotated corpus to a “parallel” LFG corpus. We extract functional descriptions from the frame-annotated LFG corpus, to derive general frame assignment rules that can be applied to new sentences. We evaluate the results by applying the induced frame assignment rules to LFG parser output.1 
We describe an XML-encoded corpus of texts in the legal domain which was gathered for an automatic summarisation project. We describe two distinct layers of annotation: manual annotation of the rhetorical status of sentences and an entirely automatic annotation process incorporating a host of individual linguistic processors. The manual rhetorical status annotation has been developed as training and testing material for a summarisation system based on the work of Teufel and Moens, while the automatic layer of annotation encodes linguistic information as features for a machine learning approach to rhetorical status classiﬁcation. 
This paper proposes and evaluates the use of linguistic information in the pre-processing phase of text mining tasks. We present several experiments comparing our proposal for selection of terms based on linguistic knowledge with usual techniques applied in the field. The results show that part of speech information is useful for the pre-processing phase of text categorization and clustering, as an alternative for stop words and stemming. 
This paper argues for the development of parallel treebanks. It summarizes the work done in this area and reports on experiments for building a Swedish-German treebank. And it describes our approach for reusing resources from one language while annotating another language. 
In this paper, we present empirical data from a corpus study on the linear order of subjects and objects in German main clauses. The aim was to establish the validity of three well-known ordering constraints: given complements tend to occur before new complements, deﬁnite before indeﬁnite, and pronoun before full noun phrase complements. Frequencies of occurrences were derived for subject-ﬁrst and object-ﬁrst sentences from the German Negra corpus. While all three constraints held on subject-ﬁrst sentences, results for object-ﬁrst sentences varied. Our ﬁndings suggest an inﬂuence of grammatical functions on the ordering of verb complements. 
This paper discusses a number of implications of using either a conceptual approach or a lexico-semantic approach to terminology structuring, especially for interpreting data supplied by corpora for the purpose of building specialized dictionaries. A simple example, i.e., program, will serve as a basis for showing how relationships between terms are captured in both approaches. My aim is to demonstrate that truly conceptual approaches do not allow a flexible integration of terms and relationships between terms and that lexico-semantic approaches are more compatible with data gathered from corpora. I will also discuss some of the implications these approaches have for computational terminology and other corpus-based terminological endeavours. 
This paper describes and evaluates the Metalinguistic Operation Processor (MOP) system for automatic compilation of metalinguistic information from technical and scientific documents. This system is designed to extract non-standard terminological resources that we have called Metalinguistic Information Databases (or MIDs), in order to help update changing glossaries, knowledge bases and ontologies, as well as to reflect the metastable dynamics of special-domain knowledge. 
leipzig.de URL: http://www.unileipzig.de/~akumar/  Barry SMITH1,2 1IFOMIS, Faculty of Medicine, University of Leipzig, Haertelstrasse 16-18 Leipzig, Germany, D-04107 2Department of Philosophy, SUNY at Buffalo, Buffalo, NY 14260, USA phismith@buffalo.edu URL: http://ontology.buffalo.edu/smit h//  Christian BORGELT Department of Knowledge Processing and Language Engineering, Otto-von-Guericke University of Magdeburg, Universitätsplatz 2, Magdeburg, Germany, D-39106 christian.borgelt@cs.unimagdeburg.de URL: http://fuzzy.cs.unimagdeburg.de/~borgelt/  Abstract The Gene Ontology is an important tool for the representation and processing of information about gene products and functions. It provides controlled vocabularies for the designations of cellular components, molecular functions, and biological processes used in the annotation of genes and gene products. These constitute three separate ontologies, of cellular components), molecular functions and biological processes, respectively. The question we address here is: how are the terms in these three separate ontologies related to each other? We use statistical methods and formal ontological principles as a first step towards finding answers to this question. 
Recent literature in computational terminology has shown an increasing interest in identifying various semantic relationships between terms. In this paper, we propose an original strategy to nd specic noun-verb combinations in a specialized corpus. We focus on verbs that convey a meaning of realization. To acquire these noun-verb pairs, we use asares, a machine learning technique that automatically infers extraction patterns from examples and counter-examples of realization noun-verb pairs. The patterns are then applied to the corpus to retrieve new pairs. Results, measured with a large test set, show that our acquisition technique outperforms classical statistical methods used for collocation acquisition. Moreover, the inferred patterns yield interesting clues on which structures are more likely to convey the target semantic link. 
The emergence of vast quantities of on-line information has raised the importance of methods for automatic cataloguing of information in a variety of domains, including electronic commerce and bioinformatics. Ontologies can play a critical role in such cataloguing. In this paper, we describe a system that automatically induces an ontology from any large on-line text collection in a specific domain. The ontology that is induced consists of domain concepts, related by kind-of and part-of links. To achieve domain-independence, we use a combination of relatively shallow methods along with any available repositories of applicable background knowledge. We describe our evaluation experiences using these methods, and provide examples of induced structures. 
Discovering synonyms and other related words among the words in a document collection can be seen as a clustering problem, where we expect the words in a cluster to be closely related to one another. The intuition is that words occurring in similar contexts tend to convey similar meaning. We introduce a way to use translation dictionaries for several languages to evaluate the rate of synonymy found in the word clusters. We also apply the information radius to calculating similarities between words using a full dependency syntactic feature space, and introduce a method for similarity recalculation during clustering as a fast approximation of the high-dimensional feature space. Finally, we show that 69-79 % of the words in the clusters we discover are useful for thesaurus construction. 
This paper proposes a method to extract foreign words, such as technical terms and proper nouns, from Korean corpora and produce a JapaneseKorean bilingual dictionary. Speciﬁc words have been imported into multiple countries simultaneously, if they are inﬂuential across cultures. The pronunciation of a source word is similar in diﬀerent languages. Our method extracts words in Korean corpora that are phonetically similar to Katakana words, which can easily be identiﬁed in Japanese corpora. We also show the eﬀectiveness of our method by means of experiments. 
This paper clariﬁes the basic concepts and theoretical perspectives by and from which quantitative “weighting” of lexical elements are deﬁned, and then draws, quantitative portraits of a few lexical elements in order to exemplify the relevance of the concepts and perspectives examined. 
The purpose of the study is to develop an integrated knowledge management system for the domains of genome and nano-technology, in which terminology-based literature mining, knowledge acquisition, knowledge structuring, and knowledge retrieval are combined. The system supports integrating different databases (papers and patents, technologies and innovations) and retrieving different types of knowledge simultaneously. The main objective of the system is to facilitate knowledge acquisition from documents and new knowledge discovery through a terminology-based similarity calculation and a visualization of automatically structured knowledge. Implementation issues of the system are also mentioned. Key Words: Structuring knowledge, knowledge acquisition, information extraction, natural language processing, automatic term recognition, terminology 1. Introduction The growing number of electronically available knowledge sources (KSs) emphasizes the importance of developing flexible and efficient tools for automatic knowledge acquisition and structuring in terms of knowledge integration. Different text and literature mining techniques have been developed recently in order to facilitate efficient discovery of knowledge contained in large textual collections. The main goal of literature mining is to retrieve knowledge that is “buried” in a text and to present the distilled knowledge to users in a concise form. Its advantage, compared to “manual” knowledge discovery, is based on the assumption that automatic methods are able to process an enormous amount of texts. It is doubtful that any researcher could process such huge amount of information, especially if the knowledge spans across domains. For these reasons, literature mining aims at helping scientists in collecting, maintaining, interpreting and curating information. In this paper, we introduce a knowledge  integration and structuring system (KISS) we designed, in which terminology-driven knowledge acquisition (KA), knowledge retrieval (KR) and knowledge visualization (KV) are combined using automatic term recognition, automatic term clustering and terminology-based similarity calculation is explained. The system incorporates our proposed automatic term recognition / clustering and a visualization of retrieved knowledge based on the terminology, which allow users to access KSs visually though sophisticated GUIs. 2. Overview of the system The main purpose of the knowledge structuring system is 1) accumulating knowledge in order to develop huge knowledge bases, 2) exploiting the accumulated knowledge efficiently. Our approach to structuring knowledge is based on: z automatic term recognition (ATR) z automatic term clustering (ATC) as an ontology1 development z ontology-based similarity calculation z visualization of relationships among documents (KSs) One of our definitions to structuring knowledge is discovery of relevance between documents (KSs) and its visualization. In order to achieve real time processing for structuring knowledge, we adopt terminology / ontology-based similarity calculation, because knowledge can also be represented as textual documents or passages (e.g. sentences, subsections) which are efficiently characterized by sets of specialized (technical) terms. Further details of our visualization scheme will be mentioned in Section 4. 
This paper introduces new specificity determining methods for terms based on information theoretic measures. The specificity of terms represents the quantity of domain specific information that is contained in the terms. Compositional and contextual information of terms are used in proposed methods. As the methods don’t rely on domain dependent information, they can be applied to other domains without extra processes. Experiments showed very promising results with the precision 82.0% when applied to the terms in MeSH thesaurus.  
The paper deals with the application of NLP technology in e-learning. We report our research on intelligent platforms for computermediated education. Some of the methods described in the paper have already taken part in the end-user applications that are in everyday use, others still wait for their implementation in the form of software products. The main message of the paper is that the language technology, even in the imperfect form of the current state of the art, can signiﬁcantly enhance today’s computer-mediated teaching and learning activities. It is true especially for languages different from English, where the adopted learning management systems often do not support even the basic functionality of a language-oriented search and retrieval of learning objects. As a case study, this paper demonstrates the application of the given ideas for e-learning materials in Czech. 
E-learning paves the way to a new type of course, more student centred, granulized, on demand, and highly interactive. Natural Language Processing (NLP) technologies associated with other multimedia technologies can help to address the major issues raised by this new type of courses: interaction, personalization and reliable information access. This paper presents Exills, a true elearning solution which integrates natural language processing tools and virtual reality1. Exills is unique in that,, unlike most of the language learning systems, it focuses on improving learners’ performance rather than learners’ competence. Introduction This paper is not a theoretical paper. It describes a true e-learning system that concentrates on improving users’ performance in a foreign language and integrates natural language processing technologies. The system, Exills (www.exills.com) represents an innovative way of integrating Natural Language technologies and multimedia technologies in order to provide a visionary e-learning tool for learning foreign languages. Exills is the result of a close collaboration between language teachers, specialists in virtual reality, linguists and computer scientists. While retaining “traditional content” such as language exercises, grammar worksheets and speech acts, Exills takes advantage of new technologies for interaction (asynchronous and synchronous), game aspect and course scenarization. Exills has been conceived with the constant concern for pedagogical quality and for differentiating itself from both CDROMs and face to face language courses. We have been careful to 
This article focuses on the development of Natural Language Processing (NLP) tools for Computer Assisted Language Learning (CALL). After identifying the inherent limitations of NLP-free tools, we describe the general framework of Mirto, an NLP-based authoring platform under construction in our laboratory, and organized into four distinct layers: functions, scripts, activities and scenarios. Through several examples, we explain how Mirto's architecture allows to implement state-of-the-art NLP functions, integrate them into easily handled scripts in order to create, without computing skills, didactic activities that could be recorded in more complex sequences or scenarios. 
VINCI is a Natural Language Generation environment designed for use in computer-aided second language instruction. It dynamically generates multiple parallel trees representing an initial text, questions on this text, and expected answers, and either orthographic or phonetic output. Analyses of a learner’s answers to questions are used to diagnose comprehension and language skills and to adaptively control subsequent generation. The paper traces stages in the generation of short texts in English and French, and discusses issues of architecture, textual enrichment, and planning. 
A full understanding of text is out of reach of current human language technology. However, a shallow Natural Language Processing (NLP) approach can be used to provide automated help in the evaluation of essays. The main idea of this paper is that Latent Semantic Indexing (LSA) can be used in conjunction with ontologies and First order Logic (FOL) to locate segments relevant to a question in a student essay. Our test bed, in a first instance, is a set of ontologies such the AKT reference ontology (describing academic life), Newspaper and a Koala ontology (concerning koalas’ habitat). 
Latent Semantic Analysis (LSA) is a statistical Natural Language Processing (NLP) technique for inferring meaning from a text. Existing LSA-based applications focus on formative assessment in general domains. The suitability of LSA for summative assessment in the domain of computer science is not well known. The results from the pilot study reported in this paper encourage us to pursue further research in the use of LSA in the narrow, technical domain of computer science. This paper explains the theory behind LSA, describes some existing LSA applications, and presents some results using LSA for automatic marking of short essays for a graduate class in architectures of computing systems. 
We consider in depth the semantic analysis in learning systems as well as some information retrieval techniques applied for measuring the document similarity in eLearning. These results are obtained in a CALL project, which ended by extensive user evaluation. After several years spent in the development of CALL modules and prototypes, we think that much closer cooperation with real teaching experts is necessary, to find the proper learning niches and suitable wrappings of the language technologies, which could give birth to useful eLearning solutions. 
Assisting in foreign language learning is one of the major areas in which natural language processing technology can contribute. This paper proposes a computerized method of measuring communicative skill in English as a foreign language. The proposed method consists of two parts. The ﬁrst part involves a test sentence selection part to achieve precise measurement with a small test set. The second part is the actual measurement, which has three steps. Step one asks proﬁciency-known human subjects to translate Japanese sentences into English. Step two gauges the match between the translations of the subjects and correct translations based on the n-gram overlap or the edit distance between translations. Step three learns the relationship between proﬁciency and match. By regression it ﬁnds a straight-line ﬁtting for the scatter plot representing the proﬁciency and matches of the subjects. Then, it estimates proﬁciency of proﬁciency-unknown users by using  the line and the match. Based on this approach, we conducted experiments on estimating the Test of English for International Communication (TOEIC) score. We collected two sets of data consisting of English sentences translated from Japanese. The ﬁrst set consists of 330 sentences, each translated to English by 29 subjects with varied English proﬁciency. The second set consists of 510 sentences translated in a similar manner by a separate group of 18 subjects. We found that the estimated scores correlated with the actual scores. 
This paper presents a novel type of test, halfway between multiple-choice and free-form text, used for training and assessment in several courses in a Computational Linguistics curriculum. We will describe the principles of the test, the diﬀerent ways in which it can be used by learners, and the tools developed for authoring. Use of this type of test is not limited to the ﬁeld of Computational Linguistics. Wherever text heavy or even picture based topics are taught use of this type of test is possible. 
This paper advocates the use of free and easily accessible computer programs in teaching. The motivating reasons for a particular program supporting the learning of syntax are given, and a ﬁrst version of the program is presented and illustrated. Initial evaluation results led to additional speciﬁcations and to the development of a new version of the program that is introduced. Finally, several perspectives for such a support tool are drawn. 
This paper looks at how Computational Linguistics (CL) and Natural Language Processing (NLP) resources can be deployed in ComputerAssisted Language Learning (CALL) materials for primary school learners. We draw a broad distinction between CL and NLP technology and briefly review the use of CL/NLP in e-Learning in general, how it has been deployed in CALL to date and specifically in the primary school context. We outline how CL/NLP resources can be used in a project to teach Irish and German to primary school children in Ireland. This paper focuses on the use of Finite State morphological analysis (FST) resources for Irish and Part of Speech (POS) taggers for German. 
This paper proposes a new e-learning paradigm which enables the user to type in arbitrary sentences. The current NLP technologies, however, are not matured enough to perform full-automatic semantic or discourse analysis. Thus, we take a diﬀerent approach; an instructor corrects the contents and its correction is embedded into the contents in an XML format called KeML. The key/mouse operations of the user are also embedded as annotations. Thus, the contents can be incomplete at the initial stage and become solid gradually as being utilized. We have implemented an e-learning system of group discussion for a foreign language class, which is demonstrated at the workshop. 
Unlike many well established approaches for E-Learning on science fields, there isn’t a commonly accepted approach of E-Learning on humanities fields, especially language and literature. Because the knowledge on language and literature depends too much on texts, advanced text processing has become a bottleneck for E-Learning on these domains. In traditional learning frameworks learners would easily get boring with mass pure texts. This article introduces a new approach for ELearning on language and literature, by intelligently extracting real or virtual objects from texts and integrating them as exhibitions in a digital museum system. This article also discussed how to generate exhibitions from texts with computational linguistics methods as well as how this E-Learning framework pushes the research of computational linguistics. The discussion of E-Learning by Digital Museum is based on the design of Digital Museum of Chinese Ancient Poetry, by Peking University. 
In this paper we address the following questions from our experience of the last two and a half years in developing a large-scale corpus of Arabic text annotated for morphological information, part-of-speech, English gloss, and syntactic structure: (a) How did we ‘leapfrog’ through the stumbling blocks of both methodology and training in setting up the Penn Arabic Treebank (ATB) annotation? (b) How did we reconcile the Penn Treebank annotation principles and practices with the Modern Standard Arabic (MSA) traditional and more recent grammatical concepts? (c) What are the current issues and nagging problems? (d) What has been achieved and what are our future expectations? 
This paper describes preliminary work concerning the creation of a Framework to aid in lexical semantic resource construction. The Framework consists of 9 stages during which various lexical resources are collected, studied, and combined into a single combinatory lexical resource. To evaluate the general Framework it was applied to a small set of English and Arabic resources, automatically combining them into a single lexical knowledge base that can be used for query translation and disambiguation in CrossLanguage Information Retrieval. 
This paper is a contribution to the issue – which has, in the course of the last decade, become critical – of the basic requirements and validation criteria for lexical language resources in Standard Arabic. The work is based on a critical analysis of the architecture of the DIINAR.1 lexical database, the entries of which are associated with grammar-lexis relations operating at word-form level (i.e. in morphological analysis). Investigation shows a crucial difference, in the concept of ‘lexical database’, between source program and generated lexica. The source program underlying DIINAR.1 is analysed, and some figures and ratios are presented. The original categorisations are, in the course of scrutiny, partly revisited. Results and ratios given here for basic entries on the one hand, and for generated lexica of inflected word-forms on the other. They aim at giving a first answer to the question of the ratios between the number of lemma-entries and inflected word-forms that can be expected to be included in, or generated by, a Standard Arabic lexical dB. These ratios can be considered as one overall language-specific criterion for the analysis, evaluation and validation of lexical dB-s in Arabic. Keywords: Arabic lexical databases – Arabic script – word-formatives grammar – lemmaentries – morphosyntactic specifiers.  
Performing root-based searching, concordancing, and grammar checking in Arabic requires an eﬃcient method for matching stems with roots and vice versa. Such mapping is complicated by the hundreds of manifestations of the same root. An algorithm based on the generation method used by native speakers is proposed here to provide a mapping from roots to stems. Verb roots are classiﬁed by the types of their radicals and the stems they generate. Roots are moulded with morphosemantic and morphosyntactic patterns to generate stems modiﬁed for tense, voice, and mode, and aﬃxed for diﬀerent subject number, gender, and person. The surface forms of applicable morphophonemic transformations are then derived using ﬁnite state machines. This paper deﬁnes what is meant by ‘stem’, describes a stem generation engine that the authors developed, and outlines how a generated stem database is compiled for all Arabic verbs. 
This paper discusses several issues in Arabic orthography that were encountered in the process of performing morphology analysis and POS tagging of 542,543 Arabic words in three newswire corpora at the LDC during 2002-2004, by means of the Buckwalter Arabic Morphological Analyzer. The most important issues involved variation in the orthography of Modern Standard Arabic that called for specific changes to the Analyzer algorithm, and also a more rigorous definition of typographic errors. Some orthographic anomalies had a direct impact on word tokenization, which in turn affected the morphology analysis and assignment of POS tags. 
This paper describes a two-level morphological analyzer for Persian using a system based on the Xerox finite state tools. Persian language presents certain challenges to computational analysis: There is a complex verbal conjugation paradigm which includes long-distance morphological dependencies; phonological alternations apply at morpheme boundaries; word and noun phrase boundaries are difficult to define since morphemes may be detached from their stems and distinct words can appear without an intervening space. In this work, we develop these problems and provide solutions in a finitestate morphology system. 
 To date, there are no WSD systems for Arabic. In  this paper we present and evaluate a novel unsuper-  vised approach, SALAAM, which exploits transla-  tional correspondences between words in a parallel  Arabic English corpus to annotate Arabic text using  ¢¡¤£¦¥¨§© an English WordNet taxonomy. We illustrate that  our approach is highly accurate in  of the  evaluated data items based on Arabic native judge-  ment ratings and annotations. Moreover, the ob-  tained results are competitive with state-of-the-art  unsupervised English WSD systems when evaluated  on English data.  
This paper deals with automatic classification of Arabic web documents. Such a classification is very useful for affording directory search functionality, which has been used by many web portals and search engines to cope with an ever-increasing number of documents on the web. In this paper, Naive Bayes (NB) which is a statistical machine learning algorithm, is used to classify non-vocalized Arabic web documents (after their words have been transformed to the corresponding canonical form, i.e., roots) to one of five pre-defined categories. Cross validation experiments are used to evaluate the NB categorizer. The data set used during these experiments consists of 300 web documents per category. The results of cross validation in the leave-one-out experiment show that, using 2,000 terms/roots, the categorization accuracy varies from one category to another with an average accuracy over all categories of 68.78 %. Furthermore, the best categorization performance by category during cross validation experiments goes up to 92.8%. Further tests carried out on a manually collected evaluation set which consists of 10 documents from each of the 5 categories, show that the overall classification accuracy achieved over all categories is 62%, and that the best result by category reaches 90%. Keywords: Naïve Bayes, Arabic document categorization, cross validation, TF-IDF. 
Automatic recognition of Arabic dialectal speech is a challenging task because Arabic dialects are essentially spoken varieties. Only few dialectal resources are available to date; moreover, most available acoustic data collections are transcribed without diacritics. Such a transcription omits essential pronunciation information about a word, such as short vowels. In this paper we investigate various procedures that enable us to use such training data by automatically inserting the missing diacritics into the transcription. These procedures use acoustic information in combination with diﬀerent levels of morphological and contextual constraints. We evaluate their performance against manually diacritized transcriptions. In addition, we demonstrate the eﬀect of their accuracy on the recognition performance of acoustic models trained on automatically diacritized training data. 
Urdu is spoken by more than 100 million people across a score countries and is the national language of Pakistan (http://www. ethnologue.com). There is a great need for developing a text-to-speech system for Urdu because this population has low literacy rate and therefore speech interface would greatly assist in providing them access to information. One of the significant parts of a text-to-speech system is a natural language processor which takes textual input and converts it into an annotated phonetic string. To enable this, it is necessary to develop models which map textual input onto phonetic content. These models may be very complex for various languages having unpredictable behaviour (e.g. English), but Urdu shows a relatively regular behaviour and thus Urdu pronunciation may be modelled from Urdu text by defining fairly regular rules. These rules have been identified and explained in this paper. 
Pakistan has a population of 140 million speaking more than 56 different languages. Urdu is the lingua franca of these people, as many speak Urdu as a second language, also the national language of Pakistan. Being a developing population, Pakistani people need access to information. Most of the information over the ICT infrastructure is only available in English and only 5-10% of these people are familiar with English. Therefore, Government of Pakistan has embarked on a project which will generate software to automatically translate the information available in English to Urdu. The project will also be able to convert Urdu text to speech to extend this information to the illiterate population as well. This paper overviews the overall architecture of the project and provides briefs on the three components of this project, namely Urdu Lexicon, English to Urdu Machine Translation System and Urdu Text to Speech System. 
FarsiSum is an attempt to create an automatic text summarization system for Persian. The system is implemented as a HTTP client/server application written in Perl. It uses modules implemented in an existing summarizer geared towards the Germanic languages, a Persian stop-list in Unicode format and a small set of heuristic rules. 
In natural language, a stem is the morphological base of a word to which affixes can be attached to form derivatives. Stemming is a process of assigning morphological variants of words to equivalence classes such that each class corresponds to a single stem. Different stemmers have been developed for a wide range of languages and for a variety of purposes. Arabic, a highly inflected language with complex orthography, requires good stemming for effective text analysis. Preliminary investigation indicates that existing approaches to Arabic stemming fail to provide effective and accurate equivalence classes when applied to a text like the Qur’an written in Classical Arabic. Therefore, I propose a new stemming approach based on a light stemming technique that uses a transliterated version of the Qur’an in western script. 
This presentation is primarily a demonstration of a working statistical machine translation system which translates Modern Standard Arabic into English. 
Among the variety of proposals currently making the dependency perspective on grammar more concrete, there are several treebanks whose annotation exploits some form of Relational Structure that we can consider a generalization of the fundamental idea of dependency at various degrees and with reference to diﬀerent types of linguistic knowledge. The paper describes the Relational Structure as the common underlying representation of treebanks which is motivated by both theoretical and task-dependent considerations. Then it presents a system for the annotation of the Relational Structure in treebanks, called Augmented Relational Structure, which allows for a systematic annotation of various components of linguistic knowledge crucial in several tasks. Finally, it shows a dependency-based annotation for an Italian treebank, i.e. the Turin University Treebank, that implements the Augmented Relational Structure. 
We present a comparative analysis of relative clauses in Hindi and Arabic in the tradition of the Paninian Grammar Framework (Bharati et al., 1996b) which leads to deriving a common logical form for equivalent sentences. Parallels are drawn between the Hindi co-relative construction and resumptive pronouns in Arabic. The analysis arises from the development of lexicalised dependency grammars for Hindi and Arabic that have application for machine translation. 
Recently, dependency grammar has gained renewed attention as empirical methods in parsing have emphasized the importance of relations between words, which is what dependency grammars model explicitly, but context-free phrase-structure grammars do not. While there has been much work on formalizing dependency grammar and on parsing algorithms for dependency grammars in the past, there is not a complete generative formalization of dependency grammar based on string-rewriting in which the derivation structure is the desired dependency structure. Such a system allows for the deﬁnition of a compact parse forest in a straightforward manner. In this paper, we present a simple generative formalism for dependency grammars based on Extended Context-Free Grammar, along with a parser; the formalism captures the intuitions of previous formalizations while deviating minimally from the much-used Context-Free Grammar. 
In this paper, a representation for syntactic dependency trees (D-trees) is deﬁned through a ﬁnite set of axioms. The axiomatized representation constitutes a string that can encode non-projective D-trees of restricted structural complexity. Upper-bounds for the structural complexity of these D-trees are ﬁxed through the following new parameters: proper embracement depth , nested crossing depth , and ¡ non-projectivity depth . ¢ In the representation, syntactic dependencies between words are indicated with pairs of brackets. When the brackets indicate dependencies that cross each other, the crossing pairs of brackets are distinguished by assigning separate colors to each of them. These colors are allocated in a way (Yli-Jyra¨ and Nyka¨nen, 2004) that ensures a unique representation for each D-tree, and entails that languages whose nested crossing depth is not bounded cannot be captured using a ﬁxed number of colors. Although the axiomatization is ﬁnite, it ensures that the represented dependency structures are trees. This is possible because the described D-trees have bounded non-projectivity depth. The axioms are also regular because proper embracement depth of represented D-trees is bounded. Our representation suggests that extra strong generative power can be squeezed out of ﬁnite-state equivalent grammars. Bracketed D-tree representations (cf. annotated sentences) are structural descriptions that are assigned to their subsequences (cf. generated strings or yields of trees) where brackets and other special-purpose characters have been omitted. 
We present the Dependency Parser, called Maxuxta, for the linguistic processing of Basque, which can serve as a representative of agglutinative languages that are also characterized by the free order of its constituents. The Dependency syntactic model is applied to establish the dependency-based grammatical relations between the components within the clause. Such a deep analysis is used to improve the output of the shallow parsing where syntactic structure ambiguity is not fully and explicitly resolved. Previous to the completion of the grammar for the dependency parsing, the design of the Dependency Structure-based Scheme had to be accomplished; we concentrated on issues that must be resolved by any practic al system that uses such models. This scheme was used both to the manual tagging of the corpus and to develop the parser. The manually tagged corpus has been used to evaluate the accuracy of the parser. We have evaluated the application of the grammar to corpus, measuring the linking of the verb with its dependents, with satisfactory results. 
In this paper we present work in progress on the annotation of an Italian Corpus (CORIS) developed at CILTA (University of Bologna). We induce categorial type assignments from a dependency treebank (Torino University treebank, TUT) and use the obtained categories with annotated dependency relations to study the distributional behavior of Italian words and reach an empirically founded part-of-speech classiﬁcation. 
This paper introduces the new grammar formalism of Extensible Dependency Grammar (XDG), and emphasizes the beneﬁts of its methodology of explaining complex phenomena by interaction of simple principles on multiple dimensions of linguistic description. This has the potential to increase modularity with respect to linguistic description and grammar engineering, and to facilitate concurrent processing and the treatment of ambiguity. 
This paper describes how we use the arrows properties from the 5P Paradigm to generate a dependency structure from a surface analysis. Besides the arrows properties, two modules, Algas and Ogre, are presented. Moreover, we show how we express linguistic descriptions away from parsing decisions. 
This paper introduces a grammar formalism specifically designed for syntax-based statistical machine translation. The synchronous grammar formalism we propose in this paper takes into consideration the pervasive structure divergence between languages, which many other synchronous grammars are unable to model. A Dependency Insertion Grammars (DIG) is a generative grammar formalism that captures word order phenomena within the dependency representation. Synchronous Dependency Insertion Grammars (SDIG) is the synchronous version of DIG which aims at capturing structural divergences across the languages. While both DIG and SDIG have comparatively simpler mathematical forms, we prove that DIG nevertheless has a generation capacity weakly equivalent to that of CFG. By making a comparison to TAG and Synchronous TAG, we show how such formalisms are linguistically motivated. We then introduce a probabilistic extension of SDIG. We finally evaluated our current implementation of a simplified version of SDIG for syntax based statistical machine translation. 
This paper focuses on how language resources (LR) for translation (hence LR4Trans) feature, and should ideally feature, within a corporate workflow of multilingual content development. The envisaged scenario will be that of a content management system that acknowledges the value of LR4Trans in the organisation as a key component and corporate knowledge resource. 
 the Technolangue call for projects and integrated  CESTA, the first European Campaign dedicated to MT Evaluation, is a project labelled by the French Technolangue action. CESTA provides an evaluation of six  to the EVALDA evaluation platform. It reports work in progress and therefore is the description of an on-going campaign for which system results are not yet available.  commercial and academic MT systems using a protocol set by an international panel of experts. CESTA aims at producing reusable resources and information about reliability of the metrics. Two runs will be carried out: one using the system’s basic dictionary, another after terminological adaptation. Evaluation task, test material, resources, evaluation measures, metrics, will be detailed in the full paper. The protocol is the combination of a contrastive reference to: IBM “BLEU” protocol (Papineni, K., S. Roukos, T. Ward and Z. Wei-Jing, 2001); “BLANC” protocol derived from (Hartley, Rajman, 2002).; “ROUGE” protocol (Babych, Hartley, Atwell, 2003). The results of the campaign will be published in a final report and be the object of two intermediary and final workshops.  In France, EVALDA is the new Evaluation platform, a joint venture between the French Ministry of Research and Technology and ELRA (European Language Resources and Evaluation Association, Paris, France). Within the framework of this initiative eight evaluation projets are being conducted: ARCADE II: campagne d’évaluation de l’alignement de corpus multilingues; CESART: campagne d'Evaluation de Systèmes d’Acquisition de Ressources Terminologiques; CESTA : campagne d'Evaluation de Systèmes de Traduction automatique; Easy: Evaluation des Analyseurs Syntaxiques du français; Campagne EQueR, Evaluation en question-réponse; Campagne ESTER, Evaluation de transcriptions d’émissions radio; Campagne EvaSY, Evaluation en synthèse vocale; and Campagne MEDIA, Evaluation du dialogue hors et en contexte.  
Localisation is one of the fastest growing industrial sectors in the digital world. Since the mid-eighties, the role of localisation has developed and changed dramatically. Localisation has been redefined as the provision of services and technologies for the management of multilinguality across the global information flow. This paper discusses the need for easily accessible dedicated language resources for localisation, provides a practical example of what can be achieved with appropriate language resources in the context of localisation and proposes a strategy to acquire, maintain and make them easily accessible. 
In this paper we present a possible solution for improving the quality of on-line translation systems, using mechanisms and standards from Semantic Web. We focus on Example based machine translation and the automatization of the translation examples extraction by means of RDFrepositories. 
The aim of the paper is to present an Open Source Learning Management System for creating courses in translation theory and practice. Some of its most important characteristics are: • Choice of course formats such as by week, by topic or social format • Flexible array of course activities Forums, Journals, Quizzes, Resources, Choices, Surveys, Assignments • Full user logging and tracking - activity reports for each student are available • Mail integration - copies of forum posts and teacher feedback can be mailed • Assignment Module with due date and grade requirements The training includes the use of corpora (monoand bilingual) and TM tools, with tasks ranging from alignment of texts and creating a translation memory, to terminology extraction from specialized comparable language corpora and other translation projects. 
This is a description of an ongoing training effort to teach the use of translation tools like translation memories or terminology databases in live online seminars over the internet. 
Translators are increasingly turning to electronic language resources and tools to help them cope with the demand for fast, highquality translation. While translation memory tools seem to be well known in the translation industry at large, bilingual concordancers appear to be familiar primarily in academic circles. The strengths and weaknesses of these two types of tool are analyzed in an effort to recommend those circumstances in which each could best be applied. 
Naturalistic theories of language acquisition assume learners to be endowed with some innate language knowledge. The purpose of this innate knowledge is to facilitate language acquisition by constraining a learner’s hypothesis space. This paper discusses a naturalistic learning system (a Categorial Grammar Learner (CGL)) that differs from previous learners (such as the Triggering Learning Algorithm (TLA) (Gibson and Wexler, 1994)) by employing a dynamic deﬁnition of the hypothesis-space which is driven by the Bayesian Incremental Parameter Setting algorithm (Briscoe, 1999). We compare the efﬁciency of the TLA with the CGL when acquiring an independently and identically distributed English-like language in noiseless conditions. We show that when convergence to the target grammar occurs (which is not guaranteed), the expected number of steps to convergence for the TLA is shorter than that for the CGL initialized with uniform priors. However, the CGL converges more reliably than the TLA. We discuss the trade-off of efﬁciency against more reliable convergence to the target grammar. 
We present a model and an experimental platform of a bootstrapping approach to statistical induction of natural language properties that is constraint based with voting components. The system is incremental and unsupervised. In the following discussion we focus on the components for morphological induction. We show that the much harder problem of incremental unsupervised morphological induction can outperform comparable all-at-once algorithms with respect to precision. We discuss how we use such systems to identify cues for induction in a cross-level architecture. 
This paper proposes a formulation of grammar learning in which meaning plays a fundamental role. We present a computational model that aims to satisfy convergent constraints from cognitive linguistics and crosslinguistic developmental evidence within a statistically driven framework. The target grammar, input data and goal of learning are all designed to allow a tight coupling between language learning and comprehension that drives the acquisition of new constructions. The model is applied to learn lexically speciﬁc multi-word constructions from annotated child-directed transcript data. 
One argument for parametric models of language has been learnability in the context of ﬁrst language acquisition. The claim is made that “logical” arguments from learnability theory require non-trivial constraints on the class of languages. Initial formalisations of the problem (Gold, 1967) are however inapplicable to this particular situation. In this paper we construct an appropriate formalisation of the problem using a modern vocabulary drawn from statistical learning theory and grammatical inference and looking in detail at the relevant empirical facts. We claim that a variant of the Probably Approximately Correct (PAC) learning framework (Valiant, 1984) with positive samples only, modiﬁed so it is not completely distribution free is the appropriate choice. Some negative results derived from cryptographic problems (Kearns et al., 1994) appear to apply in this situation but the existence of algorithms with provably good performance (Ron et al., 1995) and subsequent work, shows how these negative results are not as strong as they initially appear, and that recent algorithms for learning regular languages partially satisfy our criteria. We then discuss the applicability of these results to parametric and nonparametric models. 
The current research demonstrates a system inspired by cognitive neuroscience and developmental psychology that learns to construct mappings between the grammatical structure of sentences and the structure of their meaning representations. Sentence to meaning mappings are learned and stored as grammatical constructions. These are stored and retrieved from a construction inventory based on the constellation of closed class items uniquely identifying each construction. These learned mappings allow the system to processes natural language sentences in order to reconstruct complex internal representations of the meanings these sentences describe. The system demonstrates error free performance and systematic generalization for a rich subset of English constructions that includes complex hierarchical grammatical structure, and generalizes systematically to new sentences of the learned construction categories. Further testing demonstrates (1) the capability to accommodate a significantly extended set of constructions, and (2) extension to Japanese, a free word order language that is structurally quite different from English, thus demonstrating the extensibility of the structure mapping model. 
Language learners must acquire the grammar (rules, constraints, principles) of their language as well as representations at various levels. I will argue that representations are part of the grammar and must be acquired together with other aspects of grammar; thus, grammar acquisition may not presuppose knowledge of representations. Further, I will argue that the goal of a learning model should not be to try to match or approximate target forms directly, because strategies to do so are defeated by the disconnect between principles of grammar and the effects they produce. Rather, learners should use target forms as evidence bearing on the selection of the correct grammar. I will draw on two areas of phonology to illustrate these arguments. The first is the grammar of stress, or metrical phonology, which has received much attention in the learning model literature. The second concerns the acquisition of phonological features and contrasts. This aspect of acquisition turns out, contrary to first appearances, to pose challenging problems for learning models. 
This paper describes a computational model of word segmentation and presents simulation results on realistic acquisition In particular, we explore the capacity and limitations of statistical learning mechanisms that have recently gained prominence in cognitive psychology and linguistics. 
This paper introduces a system that simulates the transition from the one-word stage to the two-word stage in child language production. Two-word descriptions are syntactically generated and compete against one-word descriptions from the outset. Two-word descriptions become dominant as word combinations are repeatedly recognised, forming syntactic categories; resulting in an emergent simple syntax. The system demonstrates a similar maturation as children as evidenced by phenomena such as overextensions and mismatching, and the use of one-word descriptions being replaced by two-word descriptions over time. 
Given the restrictions on the subjects and objects that any given verb may take, it seems likely that children might learn verbs partly by exploiting statistical regularities in cooccurrences between verbs and noun phrases. Pronouns are the most common NPs in the speech that children hear. We demonstrate that pronouns systematically partition several important classes of verbs, and that a simple statistical learner can exploit these regularities to narrow the range of possible verbs that are consistent with an incomplete utterance. Taken together, these results suggest that children might use regularities in pronoun/verb co-occurrences to help learn verbs, though whether this is actually so remains a topic for further research. 
We outline an unsupervised language acquisition algorithm and offer some psycholinguistic support for a model based on it. Our approach resembles the Construction Grammar in its general philosophy, and the Tree Adjoining Grammar in its computational characteristics. The model is trained on a corpus of transcribed child-directed speech (CHILDES). The model’s ability to process novel inputs makes it capable of taking various standard tests of English that rely on forced-choice judgment and on magnitude estimation of linguistic acceptability. We report encouraging results from several such tests, and discuss the limitations revealed by other tests in our present method of dealing with novel stimuli. 
We evaluate the inferences that can be drawn from dissociations in syntax processing identified in developmental disorders and acquired language deficits. We use an SRN to simulate empirical data from Dick et al. (2001) on the relative difficulty of comprehending different syntactic constructions under normal conditions and conditions of damage. We conclude that task constraints and internal computational constraints interact to predict patterns of difficulty. Difficulty is predicted by frequency of constructions, by the requirement of the task to focus on local vs. global sequence information, and by the ability of the system to maintain sequence information. We generate a testable prediction on the empirical pattern that should be observed under conditions of developmental damage. 
This paper investigates two approaches to speech segmentation based on diﬀerent heuristics: the utterance-boundary strategy, and the predictability strategy. On the basis of former empirical results as well as theoretical considerations, it is suggested that the utteranceboundary approach could be used as a preprocessing step in order to lighten the task of the predictability approach, without damaging the resulting segmentation. This intuition leads to the formulation of an explicit model, which is empirically evaluated for a task of word segmentation on a child-oriented phonemically transcribed French corpus. The results show that the hybrid algorithm outperforms its component parts while reducing the total memory load involved. 
In this paper, we present a named entity recognition system in the biomedical domain, called PowerBioNE. In order to deal with the special phenomena in the biomedical domain, various evidential features are proposed and integrated through a Hidden Markov Model (HMM). In addition, a Support Vector Machine (SVM) plus sigmoid is proposed to resolve the data sparseness problem in our system. Finally, we present two post-processing modules to deal with the cascaded entity name and abbreviation phenomena. Evaluation shows that our system achieves the F-measure of 69.1 and 71.2 on the 23 classes of GENIA V1.1 and V3.0 respectively. In particular, our system achieves the F-measure of 77.8 on the “protein” class of GENIA V3.0. It shows that our system outperforms the best published system on GENIA V1.1 and V3.0. 1. INTRODUCTION With an overwhelming amount of textual information in molecular biology and biomedicine, there is a need for effective and efficient literature mining and knowledge discovery that can help biologists to gather and make use of the knowledge encoded in text documents. In order to make organized and structured information available, automatically recognizing biomedical entity names becomes critical and is important for proteinprotein interaction extraction, pathway construction, automatic database curation, etc. Such a task, called named entity recognition, has been well developed in the Information Extraction literature (MUC-6; MUC-7). In MUC, the task of named entity recognition is to recognize the names of persons, locations, organizations, etc. in the newswire domain. In the biomedical domain, we care about entities like gene, protein, virus, etc. In recent years, many explorations have been done to port existing named entity recognition systems into the biomedical domain (Kazama et al 2002; Lee et al 2003; Shen et al 2003; Zhou et al 2004). However, few of them have achieved satisfactory performance due to the special characteristics in  the biomedical domain, such as long and descriptive naming conventions, conjunctive and disjunctive structure, causal naming convention and rapidly emerging new biomedical names, abbreviation, and cascaded construction. On all accounts, we can say that the entity names in the biomedical domain are much more complex than those in the newswire domain. In this paper, we present a named entity recognition system in the biomedical domain, called PowerBioNE. In order to deal with the special phenomena in the biomedical domain, various evidential features are proposed and integrated effectively and efficiently through a Hidden Markov Model (HMM). In addition, a Support Vector Machine (SVM) plus sigmoid is proposed to resolve the data sparseness problem in our system. Finally, we present two postprocessing modules to deal with the cascaded entity name and abbreviation phenomena to further improve the performance. All of our experiments are done on the GENIA corpus, which is the largest annotated corpus in the molecular biology domain available to public (Ohta et al. 2002). In our experiments, two versions are used: 1) Genia V1.1 which contains 670 MEDLINE abstracts of 123K words; 2) Genia V3.0 which is a superset of GENIA V1.1 and contains 2000 MEDLINE abstracts of 360K words. The annotation of biomedical entities is based on the GENIA ontology (Ohta et al. 2002), which includes 23 distinct classes: multi-cell, mono-cell, virus, body part, tissue, cell type, cell component, organism, cell line, other artificial source, protein, peptide, amino acid monomer, DNA, RNA, poly nucleotide, nucleotide, lipid, carbohydrate, other organic compound, inorganic, atom and other. 2. FEATURES In order to deal with the special phenomena in the biomedical domain, various evidential features are explored. • Word Formation Pattern (FWFP): The purpose of this feature is to capture capitalization, digitalization and other word formation  
The aim of this study is to investigate the relationships between citations and the scientific argumentation found in the abstract. We extracted citation lists from a set of 3200 full-text papers originating from a narrow domain. In parallel, we recovered the corresponding MEDLINE records for analysis of the argumentative moves. Our argumentative model is founded on four classes: PURPOSE, METHODS, RESULTS, and CONCLUSION. A Bayesian classifier trained on explicitly structured MEDLINE abstracts generates these argumentative categories. The categories are used to generate four different argumentative indexes. A fifth index contains the complete abstract, together with the title and the list of Medical Subject Headings (MeSH) terms. To appraise the relationship of the moves to the citations, the citation lists were used as the criteria for determining relatedness of articles, establishing a benchmark. Our results show that the average precision of queries with the PURPOSE and CONCLUSION features is the highest, while the precision of the RESULTS and METHODS features was relatively low. A linear weighting combination of the moves is proposed, which significantly improves retrieval of related articles. 
In this paper, we present an evaluation of the Link Grammar parser on a corpus consisting of sentences describing protein-protein interactions. We introduce the notion of an interaction subgraph, which is the subgraph of a dependency graph expressing a protein-protein interaction. We measure the performance of the parser for recovery of dependencies, fully correct linkages and interaction subgraphs. We analyze the causes of parser failure and report speciﬁc causes of error, and identify potential modiﬁcations to the grammar to address the identiﬁed issues. We also report and discuss the eﬀect of an extension to the dictionary of the parser. 
Although there have been many research projects to extract protein pathways, most such information still exists only in the scientific literature, usually written in natural languages and defying data mining efforts. We present a novel and robust approach for extracting protein-protein interactions from the literature. Our method uses a dynamic programming algorithm to compute distinguishing patterns by aligning relevant sentences and key verbs that describe protein interactions. A matching algorithm is designed to extract the interactions between proteins. Equipped only with a protein name dictionary, our system achieves a recall rate of about 80.0% and a precision rate of about 80.5%. 
Information extraction (IE) in the biomedical domain is now regarded as an essential technique for the dynamic management of factual information contained in archived journal articles and abstract collections. We aim to provide a technique serving as a basis for pinpointing and organizing factual information related to experimental results. In this paper, we enhance the idea proposed in (Mizuta and Collier, 2004); annotating articles in terms of rhetorical zones with shallow nesting. We give a qualitative analysis of the zone identification (ZI) process in biology articles. Specifically, we illustrate the linguistic and other features of each zone based on our investigation of articles selected from four major online journals. We also discuss controversial cases and nested zones, and ZI using multiple features. In doing so, we provide a stronger theoretical and practical support for our framework toward automatic ZI. 
The tagging of biological entities, and in particular gene and protein names, is an essential step in the analysis of textual information in Molecular Biology and Biomedicine. The problem is harder than was originally thought because of the highly dynamic nature of the research area, in which new genes and their functions are constantly being discovered, and because of the lack of commonly accepted standards. An impressive collection of techniques has been used to detect protein and gene names in the last fourﬁve years, ranging from typical NLP to purely bioinformatics approaches. We explore here the relationship between protein/gene names and expressions used to characterize protein/gene function. These expressions are captured in a collection of patterns derived from an original set of manually derived expressions, extended to cover lexical variants and ﬁltered with known cases of association patterns/ names. Applying these patterns to a large collection of curated sentences, we found a signiﬁcant number of patterns with a very strong tendency to appear only in sentences in which a protein/gene name is simultaneously present. This approach is part of a larger eﬀort to incorporate contextual information so as to make biological information less ambiguous. 
Av. J.B. Clément 93430 F-Villetaneuse {firstname.lastname}@lipn.univ-paris13.fr ***Laboratoire de Génétique Animale, INRA-ENSAR Route de Saint Brieuc, 35042 Rennes Cedex lagarrig@roazhon.inra.fr  **Laboratoire Mathématique, Informatique et Génome (MIG), INRA, Domaine de Vilvert, 78352 F-Jouy-en-Josas {firstname.lastname}@jouy.inra.fr ****Laboratoire Leibniz – UMR CNRS 5522 46 Avenue Félix Viallet - 38031 F-Grenoble Cedex Gilles.Bisson@imag.fr  Abstract This paper gives an overview of the Caderige project. This project involves teams from different areas (biology, machine learning, natural language processing) in order to develop high- level analysis tools for extracting structured information from biological bibliographical databases, especially Medline. The paper gives an overview of the approach and compares it to the state of the art.  
Biological databases contain facts from scientiﬁc literature, which have been curated by hand to ensure high quality. Curation is timeconsuming and can be supported by information extraction methods. We present a server which identiﬁes biological facts in scientiﬁc text and presents the annotation to the curator. Such facts are: UniProt, UMLS and GO terminology, identiﬁcation of gene and protein names, mutations and protein-protein interactions. UniProt, UMLS and GO concepts are automatically linked to the original source. The module for mutations is based on syntax patterns and the one for protein-protein interactions on NLP. All modules work independently of each other in single threads and are combined in a pipeline to ensure proper meta data integration. For fast response time the modules are distributed on a Linux cluster. The server is at present available to curation teams of biomedical data and will be opened to the public in the future. Contents 
This paper presents a project whose main goal is to construct a corpus of clinical text manually annotated for part-of-speech information. We describe and discuss the process of training three domain experts to perform linguistic annotation. We list some of the challenges as well as encouraging results pertaining to inter-rater agreement and consistency of annotation. We also present preliminary experimental results indicating the necessity for adapting state-of-the-art POS taggers to the sublanguage domain of medical text. 
The accelerating growth in biomedical literature has stimulated activity on automated classiﬁcation of and information extraction from this literature. The work described here attempts to improve on an earlier classiﬁcation study associating biological articles to GO codes. It demonstrates the need, under particular assumptions, for more access to full text articles and for the use of Part-of-Speech tagging. 
Although there exists a huge number of biomedical texts online, there is a lack of tools good enough to help people get information or knowledge from them. Named entity Recognition (NER) becomes very important for further processing like information retrieval, information extraction and knowledge discovery. We introduce a Hidden Markov Model (HMM) for NER, with a word similarity-based smoothing. Our experiment shows that the word similarity-based smoothing can improve the performance by using huge unlabeled data. While many systems have laboriously hand-coded rules for all kinds of word features, we show that word similarity is a potential method to automatically get word formation, prefix, suffix and abbreviation information automatically from biomedical texts, as well as useful word distribution information. 
In this paper, we present a named entity recognition system in the biomedical domain. In order to deal with the special phenomena in the biomedical domain, various evidential features are proposed and integrated through a Hidden Markov Model (HMM). In addition, a Support Vector Machine (SVM) plus sigmoid is proposed to resolve the data sparseness problem in our system. Besides the widely used lexical-level features, such as word formation pattern, morphological pattern, out-domain POS and semantic trigger, we also explore the name alias phenomenon, the cascaded entity name phenomenon, the use of both a closed dictionary from the training corpus and an open dictionary from the database term list SwissProt and the alias list LocusLink, the abbreviation resolution and indomain POS using the GENIA corpus.  1. The Baseline System  1.1 Hidden Markov Model  In this paper, we use the Hidden Markov Model (HMM) as described in Zhou et al (2002). Given an output sequence O1n = o1o2 ...on , the system finds the most likely state sequence S1n = s1s2...sn that maximizes P(S1n | O1n) as follows:  n  ∑ log P(S1n | O1n ) = log P(S1n ) − log P(si )  i =1  (1)  n  + ∑ log P(si | O1n )  i =1  From Equation (1), we can see that:  • The first term can be computed by applying chain rules. In ngram modeling (Chen et al 1996), each tag is assumed to be dependent on the N-1 previous tags.  • The second term is the summation of log probabilities of all the individual tags.  • The third term corresponds to the “lexical” component (dictionary) of the tagger.  The idea behind the model is that it tries to assign each output an appropriate tag (state), which  contains boundary and class information. For example, “TCF 1 binds stronger than NF kB to TCEd DNA”. The tag assigned to token “TCF” should indicate that it is at the beginning of an entity name and it belongs to the “Protein” class; and the tag assigned to token “binds” should indicate that it does not belong to an entity name. Here, the Viterbi algorithm (Viterbi 1967) is implemented to find the most likely tag sequence. The problem with the above HMM lies in the data sparseness problem raised by P(si | O1n ) in the third term of Equation (1). In this paper, a Support Vector Machine (SVM) plus sigmoid is proposed to resolve this problem in our system.  1.2 Support Vector Machine plus Sigmoid  Support Vector Machines (SVMs) are a popular machine learning approach first presented by Vapnik (1995). Based on the structural risk minimization of statistical learning theory, SVMs seek an optimal separating hyper-plane to divide the training examples into two classes and make decisions based on support vectors which are selected as the only effective examples in the training set. However, SVMs produce an uncalibrated value that is not probability. That is, the unthresholded output of an SVM can be represented as  f (x) = ∑ ai ⋅ yi ⋅ k(xi , x) + b  (2)  i∈SV  To map the SVM output into the probability, we train an additional sigmoid model(Platt 1999):  p(y = 1| f ) =  
Two classifiers -- Support Vector Machine (SVM) and Conditional Random Fields (CRFs) are applied here for the recognition of biomedical named entities. According to their different characteristics, the results of two classifiers are merged to achieve better performance. We propose an automatic corpus expansion method for SVM and CRF to overcome the shortage of the annotated training data. In addition, we incorporate a keyword-based post-processing step to deal with the remaining problems such as assigning an appropriate named entity tag to the word/phrase containing parentheses. 
The comma is the most common form of punctuation. As such, it may have the greatest effect on the syntactic analysis of a sentence. As an isolate language, Chinese sentences have fewer cues for parsing. The clues for segmentation of a long Chinese sentence are even fewer. However, the average frequency of comma usage in Chinese is higher than other languages. The comma plays an important role in long Chinese sentence segmentation. This paper proposes a method for classifying commas in Chinese sentences by their context, then segments a long sentence according to the classification results. Experimental results show that accuracy for the comma classification reaches 87.1 percent, and with our segmentation model, our parser’s dependency parsing accuracy improves by 9.6 percent. 
Chinese abbreviations are widely used in the modern Chinese texts. They are a special form of unknown words, including many named entities. This results in difficulty for correct Chinese processing. In this study, the Chinese abbreviation problem is regarded as an error recovery problem in which the suspect root words are the “errors” to be recovered from a set of candidates. Such a problem is mapped to an HMM-based generation model for both abbreviation identification and root word recovery, and is integrated as part of a unified word segmentation model when the input extends to a complete sentence. Two major experiments are conducted to test the abbreviation models. In the first experiment, an attempt is made to guess the abbreviations of the root words. An accuracy rate of 72% is observed. In contrast, a second experiment is conducted to guess the root words from abbreviations. Some submodels could achieve as high as 51% accuracy with the simple HMM-based model. Some quantitative observations against heuristic abbreviation knowledge about Chinese are also observed. 
This article presents a compression-based adaptive algorithm for Chinese Pinyin input. There are many diﬀerent input methods for Chinese character text and the phonetic Pinyin input method is the one most commonly used. Compression by Partial Match (PPM) is an adaptive statistical modelling technique that is widely used in the ﬁeld of text compression. Compression-based approaches are able to build models very eﬃciently and incrementally. Experiments show that adaptive compressionbased approach for Pinyin input outperforms modiﬁed Kneser-Ney smoothing method implemented by SRILM language tools (Stolcke, 2002). 
This paper will present an enhanced probabilistic model for Chinese word segmentation and part-of-speech (POS) tagging. The model introduces the information of Chinese word length as one of its features to reach a more accurate result. And in addition, the model also achieves the integration of segmentation and POS tagging. After presenting the model, this paper will give a brief discussion on how to solve the problems in statistics and how to further integrate Chinese Named Entity Recognition into the model. Finally, some figures of experiments and comparisons will be reported, which shows that the accuracy of word segmentation is 97.09%, and the accuracy of POS tagging is 98.77%. 
 CILIN (a thesaurus widely used in Chinese semantic  classification, see 3.1), it can be classified to shallow-levels as  major class H (ACTIVITY) or as medium class Hb (MILITARY  ACTIVITY). It can also be classified to deep-levels as small  class Hb03 (specific military operations: ATTACK, RESIST,  and COUNTERATTACK) or as subclass Hb031 (ATTACK).  1.2 Previous Researches  In the previous researches of automatic semantic  classification of Chinese compounds, compounds  are generally presupposed to be endocentric,  composed of a head and a modifier. Determining the  class of the head is therefore determining the class of  the target compound (Lua, 1997, Chen and Chen,  2000). This head-determination approach has two  advantages: (1) it is simple and easy to implement (2)  it works effectively for compound nouns, the  dominant type of compounds, since most of them are head-final endocentric words.2 However, there exist  considerable exocentric compounds, for which such  a simple algorithm does not work successfully. It is  especially the case for compound verbs like V-Vs3.  G For example,  is a V-V compound meaning ‘to  G kill by beating’. Obviously, neither the sense of   (‘beat’) nor that of (‘die’) is appropriate to be  G * assigned to the compound  as the sense of  ^ * (‘car’) can be assigned to  (‘tram’, literally  ‘electricity-car’) as a general meaning.  A second problem encountered in compound  semantic classification is that there are considerable  out-of-coverage morphemes, which are not listed in  the lexicon, as remarked in (Chen and Chen, 2000).  Moreover, even a morpheme is listed, the given  senses are not necessarily appropriate to the task.  For example, in the search of compound  morphological rules in (Chen and Chen, 1998), some  appropriate senses of morphemes have to be added  manually to facilitate the task. Obviously this causes  a great difficulty to an automatic task, especially to  the example-based models which rely on the  similarity measurement of the modifier morphemes  to disambiguate the head senses (Chen and Chen,  1998, 2000). An alternative approach is thus needed  to solve the problems of exocentric compounds and  lexicon incompleteness.  Therefore in this paper I will present a non  head-oriented model of Chinese compound sense  determination, in which lexicon incompleteness will  be overcome by exploring the association between  2 Though a compound noun and its head are strictly speaking in a hyponym relation, they are usually categorized as members of * the same class. For example, in CILIN, (‘car’, ’vehicle’) and * most of the compounds X- are put under the same class Bo21 (VEHICLES), where X can be a morpheme designating the energy source (like horse, cow, electricity) or the load content (like passenger, merchandise). 3 An introspection on the two-character verbs in CILIN shows that about 48% of them are semantically exocentric, which means the semantic class of a compound X-Y in CILIN is equal neither to that of X nor to that of Y. As to the endocentric V1-V2, V1 and V2 are about equally likely to be the head of a compound verb according to the introspection.  characters and senses in a MRD. The sense of an unknown compound can be approximated by retrieved synonyms. Its sense tag can be assigned according to a certain MRD. This model facilitates an automatic system of deep semantic classification for unknown compounds. In this paper, a system for V-V compounds is implemented and evaluated. The model can however be extended to handle general Chinese compounds, like V-N and N-N, as well.  2. Compound Sense Determination  2.1 Compounding Semantic Templates  Most of the Chinese compounds are composed of  two constituents, which can be bound morphemes of  one character or free words of one or more  characters. The two-character compound is a most  representative type because its components can be  bound morphemes as well as free words. The  handling of two-character compounds becomes  therefore the focus in this paper.  As in general Chinese compounding, a  two-character compound is usually semantically  compositional, with each character conveying a  certain sense. The principle of semantic composition  implies that under each compound lies a semantic  pattern, which can be represented as the combination  of the sense tags of the two component characters.  The combination pattern is referred to as  compounding semantic template (denoted by  S-template) in this paper; compounds of the same  S-template are then referred to as template-similar  (denoted by T-similar). Since T-similar compounds  are alike in their semantic compositions, they are  supposed to possess roughly the same meaning and  G~ to be put under a considerably fine-grained semantic  class. Take the compound verb  for example.  This compound suggests the existence of a  G ~ S-template of HIT-BROKEN, as the senses of the  two component characters  and  are  respectively ‘hit’ and ’broken’. The S-template  HIT-BROKEN refers to a complex event schema [to  make something BROKEN by HITting]. This  S-template can also be found in many other GÎ Î ~ compounds with a similar meaning: , , ,  Î …etc. Obviously such T-similar words can  make a good set of examples for the example-based  approach to the sense determination, if an effective  measure of word similarity is available for their  retrieval.  2.2 Compound Similarity As a critical technique, word similarity is generally used in the example-based models of semantic classification. The measure of word similarity can be  divided into two major approaches: taxonomy-based lexical approach (Resnik 1995, Lin 1998a, Chen and Chen 1998) and context-based syntactic approach (Lin 1998b,Chen and You 2002), which is not the concern in this context-free model. However, two problems arise here for the taxonomy-based lexical approach. First, such similarity measures risk the failure to capture the similarity among some semantically highly related words, if they happen to be put under classes distant from each other according to a specific ontology 4 . Second, as mentioned, the appropriate senses of some characters just cannot be found in the thesaurus. One major reason why dictionaries do not include certain character senses is that many of such characters are used in contemporary Chinese only as bound morphemes not as free words, when the senses in question are involved. However, such senses could be kept in the compounds in the lexicon, so they might be covert but not inextricable. To remedy the effects of such lexicon incompleteness, I propose an approach to retrieve the latent senses 5 of characters and the latent synonymy among characters by exploring association among characters and senses. The idea is that if a character C appears in a compound W, then according to semantic composition, the sense of C must somehow contributes to S, the sense of W. Therefore the association strength between character C and sense S in a MRD is supposed to reflect the potentiality of S to be a sense of C. By transitivity, such association between characters and senses allows to capture association among characters. A new way to measure word similarity of two compounds can be thus derived based on the association strength of the corresponding component characters. This measure actually reflects the S-template similarity between two compounds and can be used to retrieve for a compound its T-similar words, which are potentially synonymous.  2.3 Synonyms and Sense Approximation The acquisition of synonyms plays an important role in the sense determination of a word. When a native-speaker is capable of giving synonyms to a word, he is considered to understand the meaning of that word. In fact, such a way of sense capturing is also reflected in how the senses of words can be explained in many dictionaries6. Moreover, as some researches propose, synonyms can be used to construct the semantic space for a given word (Ploux and Victorri, 1998, Ploux and Ji, 2003). In such a semantic space, each synonym with different nuance occupies a certain area. As visually reflected in this approach, retrieving a proper set of its synonyms means the ability to well capture the senses of a word. In fact, my model of automatic sense determination for a compound is exactly built upon the retrieval of its near synonyms, the T-similar compounds as previously described.  2.4 Model Representation With a S-template similarity measure, one can retrieve, for a given compound, its potential synonymous T-similar compounds. Then the sense tags of the retrieved compounds can be used to determine the sense tag of the target compound. The model of compound sense determination can be thus composed of two modules, as illustrated in Fig.1.  W(X-Y) Module-A  {dico1,dico2,…} < T-similar Word Retriever >  { SW-set(X-Y) } Module-B  Filter-C dicox < S-tag Determiner >   Ã · 4 Take an example in CILIN (a Chinese thesaurus, see 3.1). KILL( ), BUTCHER( ), and EXCUTE( ) are three concepts all meaning ‘cause to die’. However, the words expressing these three ideas are respectively put under small classes Hn05, Hd28, and Hm10, respectively under medium K ( class Hn: Criminal Activities( ), class Hd: Economical Y Production Activities( ), and class Hm: Security and Justice Ýó P[ Activities( - ). We wonder if any measurement based on that hierarchy can capture the similarity among the words situated in these three small classes in CILIN, for those words share only a common major class H, denoting vaguely Activities, which includes 296 small classes and 836 subclasses. 5 Here the term latent is used only to mean ‘hidden, potential, and waiting to be discovered’. It has nothing to do with the LSI techniques, though they both evoke the same meaning of latent.  {S-tag(X-Y)}  Fig.1 Model of Compound Sense Determination  Module-A (<T-similar Word Retriever>) is to find the potential synonyms ({SW-set(X-Y)}) of a given compound (X-Y) by using association information provided from dicos {dico1, dico2,…}. Module-B (<S-tag Determiner>) is to obtain the most likely  6 Especially in Chinese dictionaries, it is often the case that several synonymous words are given as explanation to the meaning of a word, especially when it is a compound verb.  sense tags ({S-tag(X-Y)}) according to dicox for the target word by using the output of Module-A. The component filter-C is optional, which passes only the T-similar words with the same syntactic category as the target compound, if it is already known. In fact, a system of semantic classification can be so created by choosing dico2 as dicox and the S-tag is then the semantic class in CILIN (as in section 4).  In the system implementation in this paper, two dicos are converted respectively from HowNet and CILIN for the calculation of the association measures among characters and sense tags with different types of sense tags adopted. For HowNet, the English equivalent words are used as sense tags to form dico1. For CILIN, the subclasses are used as sense tags to form dico2.  3 Character-Sense Association Network Before exploring the critical measurement of association among characters and senses needed in the model, I have to briefly present the lexical sources in use and to define the idealized dictionary format adopted in this task. 3.1 Lexical Sources The lexical sources used to implement my system include: (1) Sinica Corpus: a balanced Chinese corpus with 5 million words segmented and tagged with syntactic categories. (Huang et al., 1995) (2) HowNet: an on-line Chinese-English bilingual lexical resource created by Dong. It is used in this paper as a Chinese-English dictionary registering about 51,600 Chinese words, each assigned with its equivalent English words and its POS. (http://www.keenage.com/) (3) CILIN: a Chinese thesaurus collecting about 53,200 words. CILIN classifies its lexicon in a four-level hierarchy according to different semantic granularities: 12 major classes (level-1), 95 medium classes (level-2), 1428 small classes (level-3), and 3924 subclasses (level-4). The words in the same small class can be regarded as semantically similar, but only the words in the same subclasses can be surely regarded as synonyms7.(Mei et al., 1984) 3.2 Idealized Dictionary Format (dico) The idealized dictionary, denoted as dico, is actually a formatted MRD defined as follows: A dico is a set of <W-S> correspondence pairs, where W is a word, and S is a sense tag. (1)  [ 7 Take two verbs (‘to buy’) and (‘to sell’) as examples to demonstrate the taxonomy of CILIN. Both of the two verbs are grouped in the small class He03 (commercial trade), which is under the major class H (activities) and the medium class He (economic activities). However, the two antonyms are put under two different subclasses, respectively He031 (buying) and He032 (selling).  3.3 Character-Sense Association All the semantic information provided by a dico, as defined in (1), can be in fact represented as a network with links between two domains: W domain (words) and S domain (sense tags). In such a viewpoint, polysemy is then a one-to-many mapping from W to S, while synonymy a one-to-many mapping from S to W. If we further link a component character C of a word W to one of the S linked to W, such a C-S link might intuitively reflect a potential sense S for the character C, probably a latent sense of C, as previously described in section 2.2. We can use a statistical association measure, like MI or χ2, to extract such C-S links. The statistically extracted C-S association can then lead to the finding of latent senses for a character. The revelation of a latent character-sense association will further lead to the retrieval of new synonymy relation between characters. Symmetrically, the revelation of a latent character-sense association will also lead to the retrieval of the potential polysemy of a character. As illustrated in the Z-diagram below, supposed that C1 is already associated to S1 and C2 to S2, the retrieval of latent sense S1 to C2 will, meanwhile, lead to the finding of an association between C1 and C2 (latent synonymy), and an association between S1 and S2 (latent polysemy).  C1  S1  latent synonymy C2  latent sense S2  latent polysemy  Fig. 2 Z-diagram of C-S links  The directed association measure from a character to a sense, denoted as CS-asso(Ci,Sj), can be defined as follows:  α(Ci, Sj) = [ freq(Ci,Sj)-/ ( freq(Ci)+freq(Sj) ) ] ^ 0.5  CS-asso (Ci, Sj) = α (Ci,Sj) / Max k { α (Ci,Sk) }  (2)  where freq(Ci,Sj) is the number of the words in the MRD that contain character Ci and is tagged with sense Sj, while freq(Ci) is the number of words  containing character Ci, and freq(Sj) the number of words tagged with sense Sj.8Likewise, the directed association measure from a sense to a character, denoted as SC-asso(Si,Cj), can be defined as follows9:  α (Si,Cj)= [ freq(Si,Cj)-/( freq(Si)+freq(Cj) ) ] ^0.5  SC-asso (Si,Cj) = α (Si,Cj) / Max k { α (Si , Ck) },  (3)  Consequently, by link of a Ci-Sj-Ck chain (a latent synonymy), the directed association measure for a character Ci to another character Ck is defined as a combination of two types of directed association measures, the maximal association measure CC-asso1(Ci ,Ck) and the over-all association measure CC-asso2(Ci ,Ck), with respective weights of 1-ω and ω (the value ω is by default set at 0.5).  asso-chain(Ci,Sj,Ck) = »asso (Ci,Sj) * asso (Sj,Ck) ) ^ 0.5  f1 (Ci,Ck) = Max j {asso-chain (Ci,Sj,Ck) }  CC-asso1(Ci,Ck) = f1 (Ci,Ck) / Max m { f1 (Ci,Cm) } f2 (Ci,Ck) = Σj asso-chain(Ci,Sj,Ck)  CC-asso2(Ci,Ck) = f2 (Ci,Ck) / Max m { f2 (Ci,Cm) }  CC-asso = (1-ω) * CC-asso1 + ω * CC-asso2  (4)  3.4 S-Template Similarity Measure Supposed that Wi(Ci1-Ci2) and Wj(Cj1-Cj2) are both two-character compounds, a measure of word-word directed association (denoted as WW-asso) from Wi to Wj can be defined based on the CC-asso between their corresponding component characters:  β (Wi,Wj) = { CC-asso(Ci1,Cj1) * CC-asso(Ci2,Cj2) } ^ 0.5  WW-asso(Wi,Wj) = β (Wi,Wj) / Max k { β (Wi,Wk) }  (5)  Since the corresponding characters of two T-similar compounds must share the same sense tags and thus have strong CC-asso, the measure WW-asso(Wi,Wj) indicates, in fact, how T-similar for a compound Wj to a target Wi, compared with other compounds. WW-asso(Wi,Wj) is therefore taken as the measure of S-template similarity (denoted as T-similarity). Applying the S-template similarity measure in (5), now the T-similar Word Retriever (<TWR>) can  8 The formula α in (2) is actually a simplified approximation to the χ2 -test measure by supposing that freq(C,S) is much smaller than freq(C) and freq(S). In fact, MI (mutual information) is another association measure frequently used in Chinese NLP. For example, it is successfully used for the character-POS association measure in the task of syntactical classification for Chinese unknown words (Chen et al., 1997). However, a heuristic evaluation on some randomly picked examples shows that it seems to be outperformed by the χ2 measure in this task. 9 It must be noted that the measures of directed association (2) and (3) are asymmetric in that they give different values for the association from Ci to Sj and for the one from Sj to Ci because their normalization factors are not the same. That is why the notion directed is added here to point out the asymmetry.  give for a compound X-Y the list of its most T-similar compounds from the corpus and their T-similarity scores. As to the <S-tag Determiner>, it receives as input the output T-similar words from <TWR>. Among the input T-similar words, the ones known to dicox, are picked out and their sense tags (S-tag) with the T-similarity scores (WW-asso) are used, as in the formula (6), to calculate the likelihood score Λ for a compound V-Vi to possess a certain S-tagj. Therefore a set of ranked possible semantic classes for the compound X-Y can be given ({S-tag(X-Y)}).  λ(V-Vi, S-tagj) = Σ j WW-asso (V-Vi, SWk)  (6)  ,where SWk is a known word in dicox  and S-tagj is one of the S-tages to SWk  Λ(V-Vi,S-tagj)=λ(V-Vi,S-tagj)/Max n { λ(V-Vi, S-tagn) }  4. System Implementation  4.1 Classification for V-V Compounds  Based on the model proposed, a system of semantic  classification can be implemented for two-character  V-V compound verbs by using dico2 as the dicox in  the Module-B (the S-tag now is the semantic class in  CILIN). The V-V compounds are chosen as subjects  in this system because the choice can best  distinguish the present model from the previous  head-orientated approaches. As the involvement of  only V characters make training data homogeneous,  it simplifies the association network and reduces  largely the computational complexity. However, the  partial system for V-V compounds can be easily  extended to handle V-N compounds and N-N  compounds as well when the character-sense  association network for N characters is established.  Since only the V characters are involved, a  subset of <W-S> pairs of dico1 (HowNet) and dico2  (CILIN) is extracted to calculate the association  measures and then the T-similarity measure. The  subset contains only the <W-S> pairs whose W are  one-character or two-character verbs. In CILIN the  verbs are put under the major classes from E to J,  designating the concepts of attributes (E), actions (F),  mental activities (G), activities (H), physical states  (I), and relations (J). By choosing only the words in  the above 6 major classes, the nominal senses of  characters (A: human, B: concrete object, C: time  and space, D: abstract object) are supposed to be  excluded. Besides, the occurrence frequency of a  character in a mono-character word will be double  weighted, since in this case the word sense is surely  ù> contributed by that character alone. Let us take the V-V compound  (‘to catch  by hunting’, literally ‘hunt-catch’) for example to  see how the model operates. Based on the  ù > association network created from HowNet, the characters associated to and are listed in List 1 and List 2 (only the 10 top ranked are listed ù here), the 20 top ranked T-similar compounds of > are listed in List 3 with their similarity scores, syntactic categories and semantic classes, if they are known in CILIN. Among the 20 T-similar compounds retrieved, 10 of them (the grayed ones) ù> can be found in CILIN; 9 of them (the framed ones) can be considered as good synonyms of , while other 7 (the starred ones) considered semantically really close. In this particular example, 80% (16/20) of the T-similar compounds can be considered as at least near synonymous, while 50%(8/16) of them can be actually found in CILIN to serve the automatic semantic classification.  ¢³ÄÁÃÃÃÃ ³ ²³ÃÁÌÉÆÇ ³ ¦³ÃÁÌÄÉÈ ³ Z³ÃÁÌÃÊÆ ³ ³ÃÁÌÃÅÅ ³  ³ÃÁËËÆÅ ³ ³ÃÁËÉÊÆ ³ þ³ÃÁËÉÅÌ ³ ç³ÃÁËÈÈË ³ V³ÃÁËÆÃÉ ³  ç³³  ³ÄÁÃÃÃÃ³ ³  5³³  ³ÃÁÌÇÃÅ  ³³  ³ÃÁÌÄÇÉ ³  ³³  ³ ÃÁÌÃÊÉ³  ¯³³  ³ÃÁÌÃÆÇ  Z³ÃÁËÉÌÇ X³ÃÁËÉËÊ ³ ¦³ÃÁËÉÇÄ ³ ¢³ÃÁËÉÆÅ ³ V³ÃÁËÉÄÇ ³  ³³³³³³³³  List 1  List 2  ³  ¢ç ³ÄÁÃÃÃÃ³éÖ³Û ²ç ³ÃÁÌÉÆÇ³éÖ³Û ¢5³ÃÁÌÇÃÅ³éÝ ³ Zç³ÃÁÌÃÊÆ³éÖ ³ ç³ÃÁÌÃÅ Å³éÖ³Û ²½³ÃÁËÊÇÇ³éÖ³Û ç³ÃÁËÉÊÆ³éÖ ³ ¢¦½³ÃÁËÉÇÄ³éÖ³ ¢³ÃÁËÆËÃ³éÖ ³ ¦½³ÃÁËÆÄË³éÖ³Û  ÃÈÄ ÃÈÄ³ ³ ³ ÃÈÄ³ ÃÈÄ³ ³ ³ ³ ÃÈÄ³  ²¢½³ÃÁËÆÄÉ³éÖ³Û ÃÈÄ³ þ5³ÃÁËÄÄÆ³éÖ ³ ç5½³ÃÁËÃÇÉ³éÝ³ÝøÄÅÄ³ Êç³ÃÁËÃÆÉ³éÖ ³ ¢½³ÃÁÊÌÊÃ³éÖ³ ³ÃÁÊËÊÅ³éÖ³ÛõÄÇÄ Òç ³ÃÁÊËÈÆ³éÖ Z¢½³ÃÁÊËÆÅ³éÖ³Û ÃÈÄ³ V5³ÃÁÊËÃÌ³éÖ³ÝøÄÅÄ ³ ¬5 0.7790 VC  List 3 Applying the formula for the likelihood score of  ù> semantic class determination in (6), we have the 4  top ranked semantic classes for  predicted by  the system as follows: Z¢ (1) Hm051 ( ‘arrest’ ) %5 (2) Je121 ( ‘acquire’ ) Zþ (3) Hb121 ( ‘attack and occupy’ ) XÅ (4) Hb141 ( ‘capture as war prisoner’)  ù> In this case, the standard answer of class Hm051 for  the compound  is ranked as the first candidate,  while the second ranked candidate class Je121  (‘acquire’) is also reasonable, which can be  considered rather correct in a certain way by human  judgment. In fact, according to the native speaker’s instinct, the 4th ranked candidate class Hb141  ù> (‘capture’) is also quite suitable to the meaning of the verb , though that is not what it is classified  in CILIN. However, to avoid the subjective  interference of human judgment and particularly to  make the evaluation task automatic, the evaluation in  the following sections will be made by machine only according to the standard classification in CILIN.  4.2 Experiment Results For evaluating the performance of the system, 500 V-V compounds are randomly picked out from CILIN to form the test set. Two modes of evaluation experiments are carried out: both modes adopt dico2 (CILIN) in Module-B (dicox=dioc2) to determine semantic classes, while the inside-test mode uses dico2 (CILIN) in Module-A and the outside-test mode uses dico1 (HowNet) in Module-A, to obtain association network and retrieve the T-similar words. To make the test compounds unknown to the model, the semantic classes of the test compounds have to be invisible to CILIN, while the invisibility should not undermine the training of the association network in Module-A. The effect is done by dynamically withdrawing a word from dico2 in Module-B each time when it is in test. Two ways of evaluation can be made: by verifying the answer to the level of small class (level-3) and to the level of subclasses (level-4). The accuracy is calculated by verifying if the correct answer or one of the correct answers (if V-V is polysemous) according to CILIN can be found in the first n ranked semantic classes predicted by the system. The performance of a random head-picking model is offered as the baseline. In this baseline model, one of the semantic classes of X and Y is randomly chosen as the semantic class of the compound X-Y.  Level-3(Small Class)  Level-4(Subclass )  n outside inside Baseline outside inside Baseline  
Spec is a critical issue for automatic chunking. This paper proposes a solution of Chinese chunking with another type of spec, which is not derived from a complete syntactic tree but only based on the un-bracketed, POS tagged corpus. With this spec, a chunked data is built and HMM is used to build the chunker. TBLbased error correction is used to further improve chunking performance. The average chunk length is about 1.38 tokens, F measure of chunking achieves 91.13%, labeling accuracy alone achieves 99.80% and the ratio of crossing brackets is 2.87%. We also find that the hardest point of Chinese chunking is to identify the chunking boundary inside noun-noun sequences1. 
The input of network is the key problem for Chinese Word sense disambiguation utilizing the Neural Network. This paper presents an input model of Neural Network that calculates the Mutual Information between contextual words and ambiguous word by using statistical method and taking the contextual words to certain number beside the ambiguous word according to (-M, +N). The experiment adopts triple-layer BP Neural Network model and proves how the size of training set and the value of M and N affect the performance of Neural Network model. The experimental objects are six pseudowords owning three word-senses constructed according to certain principles. Tested accuracy of our approach on a close-corpus reaches 90.31%,, and 89.62% on a open-corpus. The experiment proves that the Neural Network model has good performance on Word sense disambiguation. 
This paper proposes an approach to automated ontology alignment and domain ontology extraction from two knowledge bases. First, WordNet and HowNet knowledge bases are aligned to construct a bilingual universal ontology based on the co-occurrence of the words in a parallel corpus. The bilingual universal ontology has the merit that it contains more structural and semantic information coverage from two complementary knowledge bases, WordNet and HowNet. For domain-specific applications, a medical domain ontology is further extracted from the universal ontology using the islanddriven algorithm and a medical domain corpus. Finally, the domain-dependent terms and some axioms between medical terms based on a medical encyclopaedia are added into the ontology. For ontology evaluation, experiments on web search were conducted using the constructed ontology. The experimental results show that the proposed approach can automatically align and extract the domain-specific ontology. In addition, the extracted ontology also shows its promising ability for medical web search. 
Sino-Korean words, which are historically borrowed from Chinese language, could be represented with both Hanja (Chinese characters) and Hangeul (Korean characters) writings. Previous Korean Input Method Editors (IMEs) provide only a simple dictionary-based approach for Hangeul-Hanja conversion. This paper presents a sentencebased statistical model for Hangeul-Hanja conversion, with word tokenization included as a hidden process. As a result, we reach 91.4% of character accuracy and 81.4% of word accuracy in terminology domain, when only very limited Hanja data is available. 
In this paper, we propose an automatic term recognition system for Chinese. Our idea is based on the relation between a compound word and its constituents that are simple words or individual Chinese character. More precisely, we basically focus on how many words/characters adjoin the word/character in question to form compound words. We also take into account the frequency of term. We evaluated word based method and character based method with several Chinese Web pages, resulting in precision of 75% for top ten candidate terms. 
A challenging task in Chinese collocation extraction is to improve both the precision and recall rate. Most lexical statistical methods including Xtract face the problem of unable to extract collocations with lower frequencies than a given threshold. This paper presents a method where HowNet is used to find synonyms using a similarity function. Based on such synonym information, we have successfully extracted synonymous collocations which normally cannot be extracted using the lexical statistical approach. We applied synonyms mapping to each headword to extract more synonymous word bi-grams. Our evaluation over 60MB tagged corpus shows that we can extract synonymous collocations that occur with very low frequency, sometimes even for collocations that occur only once in the training set. Comparing to a collocation extraction system based on Xtract, we have reached the precision rate of 43% on word bi-grams for a set of 9 headwords, almost 50% improvement from precision rate of 30% in the Xtract system. Furthermore, it improves the recall rate of word bi-gram collocation extraction by 30%. 
This paper presents the construction of a manually annotated Chinese shallow Treebank, named PolyU Treebank. Different from traditional Chinese Treebank based on full parsing, the PolyU Treebank is based on shallow parsing in which only partial syntactical structures are annotated. This Treebank can be used to support shallow parser training, testing and other natural language applications. Phrase-based Grammar, proposed by Peking University, is used to guide the design and implementation of the PolyU Treebank. The design principles include good resource sharing, low structural complexity, sufficient syntactic information and large data scale. The design issues, including corpus material preparation, standard for word segmentation and POS tagging, and the guideline for phrase bracketing and annotation, are presented in this paper. Well-designed workflow and effective semiautomatic and automatic annotation checking are used to ensure annotation accuracy and consistency. Currently, the PolyU Treebank has completed the annotation of a 1-million-word corpus. The evaluation shows that the accuracy of annotation is higher than 98%. 
Automatic topic segmentation, separation of a discourse stream into its constituent stories or topics, is a necessary preprocessing step for applications such as information retrieval, anaphora resolution, and summarization. While signiﬁcant progress has been made in this area for text sources and for English audio sources, little work has been done in automatic, acoustic feature-based segmentation of other languages. In this paper, we consider exploiting both prosodic and text-based features for topic segmentation of Mandarin Chinese. As a tone language, Mandarin presents special challenges for applicability of intonation-based techniques, since the pitch contour is also used to establish lexical identity. We demonstrate that intonational cues such as reduction in pitch and intensity at topic boundaries and increase in duration and pause still provide signiﬁcant contrasts in Mandarin Chinese. We build a decision tree classiﬁer that, based only on word and local context prosodic information without reference to term similarity, cue phrase, or sentencelevel information, achieves boundary classiﬁcation accuracy of 84.6-95.6% on a balanced test set. We contrast these results with classiﬁcation using textbased features, exploiting both text similarity and n-gram cues, to achieve accuracies between 7795.6%, if silence features are used. Finally we integrate prosody, text, and silence features using a voting strategy to combine decision tree classiﬁers for each feature subset individually and all subsets jointly. This voted decision tree classiﬁer yields an overall classiﬁcation accuracy of 96.85%, with 2.8% miss and 3.15% false alarm rates on a representative corpus sample, demonstrating synergistic combination of prosodic and text features for topic segmentation. 
We present an automatic semantic roles labeling system for structured trees of Chinese sentences. It adopts dependency decision making and example-based approaches. The training data and extracted examples are from the Sinica Treebank, which is a Chinese Treebank with semantic role assigned for each constituent. It used 74 abstract semantic roles including thematic roles, such as ‘agent’; ‘theme’, ‘instrument’, and secondary roles of ‘location’, ‘time’, ‘manner’ and roles for nominal modifiers. The design of role assignment algorithm is based on the different decision features, such as head-argument/modifier, case makers, sentence structures etc. It labels semantic roles of parsed sentences. Therefore the practical performance of the system depends on a good parser which labels the right structures of sentences. The system achieves 92.71% accuracy in labeling the semantic roles for pre-structure- bracketed texts which is considerably higher than the simple method using probabilistic model of head-modifier relations. 1. Introduction For natural language understanding, the process of fine-grain semantic role assignment is one of the prominent steps, which provides semantic relations between constituents. The sense and sense relations between constituents are core meaning of a sentence. Conventionally there are two kinds of methods for role assignments, one is using only statistical information (Gildea and Jurafsky, 2002) and the other is combining with grammar rules (Gildea and Hockenmaier, 2003). However using only grammar rules to assign semantic roles could lead to low coverage. On the other hand, performance of statistical methods relies on significant dependent features. Data driven is a suitable strategy for semantic roles assignments of general texts. We use the Sinica Treebank as information resource because of its various domains texts including politics, society, literature…etc and it is a Chinese Treebank with semantic role assigned for each constituent (Chen etc., 2003). It used 74 abstract semantic roles including thematic roles, such as ‘agent’; ‘theme’, ‘instrument’, and secondary roles of ‘location’, ‘time’, ‘manner’ and modifiers of nouns, such as  ‘quantifier’, ‘predication’, ‘possessor’, etc. The design of role assignment algorithm is based on the different decision features, such as head-argument/modifier, case makers, sentence structures etc. It labels semantic roles of parsed sentences by example-based probabilistic models. 1.1 Sinica Treebank The Sinica Treebank has been developed and released to public since 2000 by Chinese Knowledge Information Processing (CKIP) group at Academia Sinica. The Sinica Treebank version 2.0 contains 38944 structural trees and 240,979 words in Chinese. Each structural tree is annotated with words, part-of-speech of words, syntactic structure brackets, and semantic roles. For conventional structural trees, only syntactic information was annotated. However, it is very important and yet difficult for Chinese to identify word relations with purely syntactic constraints (Xia et al., 2000). Thus, partial semantic information, i.e. semantic role for each constituent, was annotated in Chinese structural trees. The grammatical constraints are expressed in terms of linear order of semantic roles and their syntactic and semantic restrictions. Below is an example sentence of the Sinica Treebank. Original sentence: 他 ‘Ta’要 ‘yao’ 張三 ‘ZhangSan’撿 ‘jian’ 球 ‘qiu’。 He ask Zhang San to pick up the ball. Parsed tree: S(agent:NP(Head:Nhaa:他’He’)|Head:VF2:要’ask’ |goal:NP(Head:Nba:張三’Zhang San’) |theme:VP(Head:VC2:撿’pick’|goal:NP(Head:Nab:球 '’ball’))) Figure 1: An example sentence of Sinica Treebank In the Sinica Treebank, not only the semantic relations of a verbal predicate but also the modifier head relations were marked. There are 74 different semantic roles, i.e. the task of semantic role assignment has to establish the semantic relations among phrasal heads and their arguments/modifiers within 74 different choices.  The set of semantic roles used in the Sinica Treebank is listed in the appendix.  2. Example-based Probabilistic Models for Assigning Semantic Roles The idea of example-based approaches is that semantic roles are preserved for the same event frames. For a target sentence, if we can find same examples in the training corpus, we can assign the same semantic role for each constituent of the target sentence as the examples. However reoccurrence of exact same surface structures for a sentence is very rare, i.e. the probability of finding same example sentences in a corpus is very low. In fact, by observing structures of parsed trees, we find that most of semantic roles are uniquely determined by semantic relations between phrasal heads and their arguments/modifiers and semantic relations are determined by syntactic category, semantic class of related words. For example: Original sentence: 我們 ‘wo men’ 都 ‘du’ 喜歡 ‘xi huan’ 蝴蝶 ‘hu die’。 We all like butterflies. Parsed tree: S(experiencer:NP(Head:Nhaa:我們 ‘we’ )|quantity:Dab:都 ‘all’ |Head:VK1:喜歡 ‘like’ |goal:NP(Head:Nab:蝴蝶 ‘butterflies’))。 S  experiencer NP  goal NP  Head Nhaa  quantity Dab  Head VK1  Head Nab  我們  都  喜歡  蝴蝶  We  all  like  Butterflies.  Figure 2: The illustration of the parsed tree.  In Figure2, 喜歡 ‘like’ is the sentential head; 我 們 ‘we’ and 蝴蝶 ‘butterflies’ are the arguments; 都 ‘all’ is the modifier. As a result, the semantic role ‘experiencer’ of 我們 ‘we’ is deduced from the relation between 我們 ‘we’ and 喜歡 ‘like’, since the event frame of 喜歡 ‘like’ has the two arguments of experiencer and goal and the experiencer usually takes the subject position. The semantic roles of 蝴 蝶 ‘butterflies’ and 都 ‘all’ are assigned by the  same way. For the task of automatic role assignment, once phrase boundaries and phrasal head are known, the semantic relations will be resolved by looking for similar head-argument/modifier pairs in training data. 2.1 Example Exaction To extract head-argument/modifier examples from the Sinica Treebank is trivial, since phrase boundaries and semantic roles, including phrasal head, are labeled. The extracted examples are pairs of head word and target word. The target word is represented by the head of the argument/modifier, since the semantic relations are established between the phrasal head and the head of argument/modifier phrase. An extracted word pair includes the following features. Target word: The head word of argument/modifier. Target POS: The part-of-speech of the target word. Target semantic role: Semantic role of the constituent contains the target word as phrasal head. Head word: The phrasal head. Head POS: The part-of-speech of the head word. Phrase type: The phrase which contains the head word and the constituent containing target word. Position: Shows whether target word appears before or after head word. The examples we extracted from Figure 2 are listed below. Table 1: The three head-argument/modifier pairs extracted from Figure 2.  Table 2: Coverage and accuracy of different features combinations  2.2 Probabilistic Model for Semantic Role Assignment It is possible that conflicting examples (or ambiguous role assignments) occur in the training data. We like to assign the most probable roles. The probability of each semantic role in a constituent with different features combinations are estimated from extract examples. P(r | constituent) 
Motivated by a systematic analysis of Chinese semantic relationships, we constructed a Chinese semantic framework based on surface syntactic relationships, deep semantic relationships and feature structure to express dependencies between lexical meanings and conceptual structures, and relations that underlie those lexical meanings. Analyzing the semantic representations of 10000 Chinese sentences, we provide a model of semantically and syntactically annotated sentences from which reliable information on combinatorial possibilities of each semantic item targeted for analysis can be displayed. We also propose a semantic argument – head relation, ‘basic conceptual structure’ and the ‘Head-Driven Principle’. Our results show that we can successfully disambiguate some troublesome sentences, and minimize the redundancy in language knowledge descriptions for natural language processing. 
In Chinese texts, words are not separated by white spaces. This is problematic for many natural language processing tasks. The standard approach is to segment the Chinese character sequence into words. Here, we investigate Chinese word segmentation for statistical machine translation. We pursue two goals: the ﬁrst one is the maximization of the ﬁnal translation quality; the second is the minimization of the manual eﬀort for building a translation system. The commonly used method for getting the word boundaries is based on a word segmentation tool and a predeﬁned monolingual dictionary. To avoid the dependence of the translation system on an external dictionary, we have developed a system that learns a domainspeciﬁc dictionary from the parallel training corpus. This method produces results that are comparable with the predeﬁned dictionary. Further more, our translation system is able to work without word segmentation with only a minor loss in translation quality. 
This paper presents a semi-supervised approach to reduce human effort in building an annotated Chinese corpus. One of the disadvantages of many statistical Chinese named entity recognition systems is that training data may be in short supply, and manually building annotated corpus is expensive. In the proposed approach, we construct an 80M handannotated corpus in three steps: (1) Automatically annotate training corpus; (2) Manually refine small subsets of the automatically annotated corpus; (3) Combine small subsets and whole corpus in a bootstrapping process. Our approach is tested on a state-ofthe-art Chinese word segmentation system (Gao et al., 2003, 2004). Experiments show that only a small subset of hand-annotated corpus is sufficient to achieve a satisfying performance of the named entity component in this system. 
A classical Chinese Natural Language Understanding (NLU) architecture usually includes several NLU components which are executed with some mechanism. A new Multilayer Search Mechanism (MSM) which integrates and quantiﬁes these components into a uniform multilayer treelike architecture is presented in this paper. The mechanism gets the optimal result with search algorithms. The components in MSM aﬀect each other. At last, the performance of each component is enhanced. We built a practical system – CUP (Chinese Understanding Platform) based on MSM with three layers. By the experiments on Word Segmentation, a better performance was achieved. In theory the normal cascade and feedback mechanism are just some special cases of MSM. 
Large amounts of bilingual resource on the Internet provide us with the probability of building a large scale of bilingual corpus. The irregular characteristics of the real texts, especially without the strictly aligned paragraph boundaries, bring a challenge to alignment technology. The traditional alignment methods have some difficulties in competency for doing this. This paper describes a new method for aligning real bilingual texts using sentence pair location information. The model was motivated by the observation that the location of a sentence pair with certain length is distributed in the whole text similarly. It uses (1:1) sentence beads instead of high frequency words as the candidate anchors. The method was developed and evaluated through many different test data. The results show that it can achieve good aligned performance and be robust and language independent. It can resolve the alignment problem on real bilingual text. 
Unknown word recognition is an important problem in Chinese word segmentation systems. In this paper, we propose an integrated method for Chinese unknown word extraction for offline corpus processing, in which both contextentropy (on each side) and frequency ratio against background corpus are introduced to evaluate the candidate words. Both of the measures are computed efficiently on Suffix array with much less space overhead. Our method can also be reinforced when combined with a basic Segmentor by boundary-verification and arbitrary n-gram words can be extracted by our method. We test our method on Chinese novel Xiao Ao Jiang Hu, and obtain satisfactory achievements compared to traditional criteria such as Likelihood Ratio. 
Information graphics (non-pictorial graphics such as bar charts or line graphs) are an important component of multimedia documents. Often such graphics convey information that is not contained elsewhere in the document. Thus document summarization must be extended to include summarization of information graphics. This paper addresses our work on graphic summarization. It argues that the message that the graphic designer intended to convey must play a major role in determining the content of the summary, and it outlines our approach to identifying this intended message and using it to construct the summary. 
There is a long history of research in automatic text summarization systems by both the text retrieval and the natural language processing communities, but evaluation of such systems’ output has always presented problems. One critical problem remains how to handle the unavoidable variability in human judgments at the core of all the evaluations. Sponsored by the DARPA TIDES project, NIST launched a new text summarization evaluation effort, called DUC, in 2001 with follow-on workshops in 2002 and 2003. Human judgments provided the foundation for all three evaluations and this paper examines how the variation in those judgments does and does not affect the results and their interpretation. 
Sentence ranking is a crucial part of generating text summaries. We compared human sentence rankings obtained in a psycholinguistic experiment to three different approaches to sentence ranking: A simple paragraph-based approach intended as a baseline, two word-based approaches, and two coherencebased approaches. In the paragraph-based approach, sentences in the beginning of paragraphs received higher importance ratings than other sentences. The word-based approaches determined sentence rankings based on relative word frequencies (Luhn (1958); Salton & Buckley (1988)). Coherence-based approaches determined sentence rankings based on some property of the coherence structure of a text (Marcu (2000); Page et al. (1998)). Our results suggest poor performance for the simple paragraph-based approach, whereas word-based approaches perform remarkably well. The best performance was achieved by a coherence-based approach where coherence structures are represented in a non-tree structure. Most approaches also outperformed the commercially available MSWord summarizer.  
Analysis of 9000 manually written summaries of newswire stories used in four Document Understanding Conferences indicates that approximately 40% of their lexical items do not occur in the source document. A further comparison of different summaries of the same document shows agreement on 28% of their vocabulary. It can be argued that these relationships establish a performance ceiling for automated summarization systems which do not perform syntactic and semantic analysis on the source document. 
In this paper we describe our method for the summarization of legal documents helping a legal expert determine the key ideas of a judgment. Our approach is based on the exploration of the document’s architecture and its thematic structures in order to build a table style summary for improving coherency and readability of the text. We present the components of a system, called LetSum, built with this approach, its implementation and some preliminary evaluation results. 
We describe a classiﬁer which determines the rhetorical status of sentences in texts from a corpus of judgments of the UK House of Lords. Our summarisation system is based on the work of Teufel and Moens where sentences are classiﬁed for rhetorical status to aid sentence selection. We experiment with a variety of linguistic features with results comparable to Teufel and Moens, thereby demonstrating the feasibility of porting this kind of system to a new domain. 
We describe SmartMail, a prototype system for automatically identifying action items (tasks) in email messages. SmartMail presents the user with a task-focused summary of a message. The summary consists of a list of action items extracted from the message. The user can add these action items to their “to do” list. 
In this paper, a novel linguistically advanced text summarization system is described for reducing the minimum size of highly readable variable sized summaries of digitized text documents produced by text summarization methods that use discourse analysis to rank sentences for inclusion in the final summary. The basic algorithm used in FXPAL’s PALSUMM text summarization system combines text structure methods that preserve readability and correct reference resolution with statistical methods to reduce overall summary length while promoting the inclusion of important material. 
 templates but with the help of headline phrases. Future work is discussed in Section 6.  Headline summarization is a difficult task because it requires maximizing text content in short summary length while maintaining grammaticality. This paper describes our first attempt toward solving this problem with a system that generates key headline clusters and fine-tunes them using templates. 
Some document genres contain a large number of figures. This position paper outlines approaches to diagram summarization that can augment the many well-developed techniques of text summarization. We discuss figures as surrogates for entire documents, thumbnails, extraction, the relations between text and figures as well as how automation might be achieved. The focus is on diagrams (line drawings) because they allow parsing techniques to be used, in contrast to the difficulties of general image understanding. We describe the advances in raster image vectorization and parsing needed to produce corpora for diagram summarization. 
Summarization evaluation has been always a challenge to researchers in the document summarization field. Usually, human involvement is necessary to evaluate the quality of a summary. Here we present a new method for automatic evaluation of text summaries by using document graphs. Data from Document Understanding Conference 2002 (DUC-2002) has been used in the experiment. We propose measuring the similarity between two summaries or between a summary and a document based on the concepts/entities and relations between them in the text. 
ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summarization evaluation package and their evaluatio ns. Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST. 
Automatic summaries of text generated through sentence or word extraction has been evaluated by comparing them with manual summaries generated by humans by using numerical evaluation measures based on precision or accuracy. Although sentence extraction has previously been evaluated based only on precision of a single sentence, sentence concatenations in the summaries should be evaluated as well. We have evaluated the appropriateness of sentence concatenations in summaries by using evaluation measures used for evaluating word concatenations in summaries through word extraction. We determined that measures considering sentence concatenation much better reﬂect the human judgment rather than those based only on the precision of a single sentence. 
In this paper a sentence compression tool is described. We describe how an input sentence gets analysed by using a.o. a tagger, a shallow parser and a subordinate clause detector, and how, based on this analysis, several compressed versions of this sentence are generated, each with an associated estimated probability. These probabilities were estimated from a parallel transcript/subtitle corpus. To avoid ungrammatical sentences, the tool also makes use of a number of rules. The evaluation was done on three different pronunciation speeds, averaging sentence reduction rates of 40% to 17%. The number of reasonable reductions ranges between 32.9% and 51%, depending on the average estimated pronunciation speed. 
We report on a series of human evaluations of the task of sentence fusion. In this task, a human is given two sentences and asked to produce a single coherent sentence that contains only the important information from the original two. Thus, this is a highly constrained summarization task. Our investigations show that even at this restricted level, there is no measurable agreement between humans regarding what information should be considered important. We further investigate the ability of separate evaluators to assess summaries, and ﬁnd similarly disturbing lack of agreement. 
Most approaches to extractive summarization deﬁne a set of features upon which selection of sentences is based, using algorithms independent of the features themselves. We propose a new set of features based on low-level, atomic events that describe relationships between important actors in a document or set of documents. We investigate the effect this new feature has on extractive summarization, compared with a baseline feature set consisting of the words in the input documents, and with state-of-the-art summarization systems. Our experimental results indicate that not only the event-based features offer an improvement in summary quality over words as features, but that this effect is more pronounced for more sophisticated summarization methods that avoid redundancy in the output. 
Automatic summarization is an active research area in natural language processing. This paper has proposed a special method that produces text summary by detecting thematic areas in Chinese document. The specificity of the method is that the produced summary can both cover many different themes and reduce its redundancy obviously at the same time. In this method, the detection of latent thematic areas is realized by adopting K-medoids clustering method as well as a novel clu stering analysis method, which can be used to determine automatically K, the number of clusters.. In addition, a novel parameter, which is known as representation entropy, is used for summarization redundancy evaluation. Experimental results indicate a clear superiority of the proposed method over the traditional non-thematic -area-detection method under the proposed evaluation scheme when dealing with different genres of text documents with free style and flexible theme distribution. 
he work reported in this rtile presents  omE puttionl model of interprettionF he model proE poses  ognitive rhiteture for intelligent gents to reson out ompeting nlyses during interpreE ttion nd leverges the positive reinforement prinE ipleF 
This paper presents intial work on a system that bridges from robust, broad-coverage natural language processing to precise semantics and automated reasoning, focusing on solving logic puzzles drawn from sources such as the Law School Admission Test (LSAT) and the analytic section of the Graduate Record Exam (GRE). We highlight key challenges, and discuss the representations and performance of the prototype system. 
In this paper we present a novel approach to map textual entities such as words, phrases, sentences, paragraphs or arbitrary text fragments onto artiﬁcial structures which we call “Text Sense Representation Trees” (TSR trees). These TSR trees represent an abstract notion of the meaning of the respective text, subjective to an abstract “common” understanding within the World Wide Web. TSR Trees can be used to support text and language processing systems such as text categorizers, classiﬁers, automatic summarizers and applications of the Semantic Web. We will explain how to construct the TSR tree structures and how to use them properly; furthermore we describe some preliminary evaluation results. 
In this paper we compare programs of work that aim to develop broad coverage crosslinguistic resources for NLP: Ontological Semantics (OntoSem) and SIMPLE. The approaches taken in these projects differ in three notable respects: the use of an ontology versus a word net as the semantic substrate; the development of knowledge resources inside of as opposed to outside of a processing environment; and the development of lexicons for multiple languages based on a single core lexicon or without such a core (i.e., in parallel fashion). In large part, these differences derive from project-driven, real-world requirements and available resources – a reflection of their being practical rather than theoretical projects. However, that being said, we will suggest certain preferences regarding the content and development of NLP resources with a view toward both short- and long-term, highlevel language processing goals. 
This paper describes an innovative evaluation regimen developed for the text meaning representations (TMRs) produced by the Ontological Semantic (OntoSem) general purpose syntactic-semantic analyzer. The goal of evaluation is not only to determine the quality of TMRs for given texts, but also to assign blame for various classes of errors, thus suggesting directions for continued work on both knowledge resources and processors. The paper includes descriptions of the OntoSem processing environment, the evaluation regime itself and results from our first evaluation effort.  The approach to semantic analysis in OntoSem is described in some detail in, e.g., Nirenburg and Raskin 2004, Nirenburg et al. 2003, Beale et al 2003. Our description here will be necessarily brief. Text analysis in OntoSem relies on the results of a battery of pre-semantic text processing modules. The preprocessor module deals with mark-up in the input text, finds boundaries of sentences and words, and recognizes dates, numbers, named entities and acronyms. Morphological analysis accepts a string of word forms as input and for each word form outputs a record containing its citation form in the lexicon and a set of morphological features and their values that correspond to the word form from the text. Once the  
This paper describes the initial results of an experiment in integrating knowledge-based text processing with real-world reasoning in a question answering system. Our MOQA “meaning-oriented question answering” system seeks answers to questions not in open text but rather in a structured fact repository whose elements are instances of ontological concepts extracted from the text meaning representations (TMRs) produced by the OntoSem text analyzer. The query interpretation and answer content formulation modules of MOQA use the same knowledge representation substrate and the same static knowledge resources as the ontological semantic (OntoSem) semantic text analyzer. The same analyzer is used for deriving the meaning of questions and of texts from which the fact repository content is extracted. Inference processes in question answering rely on ontological scripts (complex events) that also support reasoning for purely NLP-related purposes, such as ambiguity resolution in its many guises. 
We apply the C4.5 decision tree learner in interpreting Japanese relative clause constructions, based around shallow syntactic and semantic processing. In parameterising data for use with C4.5, we propose and test various means of reducing intraclausal interpretational ambiguity, and cross indexing the overall analysis of cosubordinated relative clause constructions. We additionally investigate the disambiguating effect of the different parameter types used, and establish upper bounds for the task.  
This paper describes a system to create animated 3D scenes of car accidents from reports written in Swedish. The system has been developed using news reports of varying size and complexity. The text-to-scene conversion process consists of two stages. An information extraction module creates a structured representation of the accident and a visual simulator generates and animates the scene. We ﬁrst describe the overall structure of the textto-scene conversion and the structure of the representation. We then explain the information extraction and visualization modules. We show snapshots of the car animation output and we conclude with the results we obtained. 
Arguably, grammars which associate natural language expressions not only with a syntactic but also with a semantic representation, should do so in a way that capture paraphrasing relations between sentences whose core semantics are equivalent. Yet existing semantic grammars fail to do so. In this paper, we describe an ongoing project whose aim is the production of a “paraphrastic grammar” that is, a grammar which associates paraphrases with identical semantic representations. We begin by proposing a typology of paraphrases. We then show how this typology can be used to simultaneously guide the development of a grammar and of a testsuite designed to support the evaluation of this grammar. 
Discourse in formal domains, such as mathematics, is characterized by a mixture of telegraphic natural language and embedded (semi-)formal symbolic mathematical expressions. Due to the lack of empirical data, little is known about the suitability of input analysis methods for mathematical discourse in a dialog setting. We present an input understanding method for a tutoring system teaching mathematical theorem proving. The adopted deep analysis strategy is motivated by the complexity of the language phenomena observed in a corpus collected in a Wizard-of-Oz experiment. Our goal is a uniform input interpretation, in particular, considering different degrees of formality of natural language verbalizations. 
One main problem for NLP applications is that natural language expressions are underspecified and require enrichments of different sorts to get a truthconditional interpretaton in context. Underspecification applies on two levels: what is said underdetermines what is meant, and linguistic meaning underspecifies what is said. One instance of this phenomenon is aspect in Russian, especially the imperfective one. It gives rise to a variety of readings, which are difficult to capture by one invariant meaning. Instead, the imperfective aspect is sense-general; its meaning has to be specified in the course of interpretation by contextual cues and pragmatic inferences. This paper advocates an account of the different imperfective readings in terms of pragmatic principles and inferential heuristics based on, and supplied by, a semantic skeleton consisting of a ‘selectional theory’ of aspect. This framework might serve as basis for a rule-guided derivation of aspectual readings in Russian. 
The paper deals with the latest application of natural language processing (NLP), specifically of ontological semantics (ONSE) to natural language information assurance and security (NL IAS). It demonstrates how the existing ideas, methods, and resources of ontological semantics can be applied to detect deception in NL text (and, eventually, in data and other media as well). After stating the problem, the paper proceeds to a brief introduction to ONSE, followed by an equally brief survey of our 5-year-old effort in “colonizing” IAS. The main part of the paper deals with the following issues: • human deception detection abilities and NLP modeling of it; • manipulation of fact repositories for this purpose beyond the current state of the art; • acquisition of scripts for complex ontological concepts; • degrees of lying complexity and feasibility of their automatic detection. This is not a report on a system implementation but rather an applicationestablishing proof-of-concept effort based on the algorithmic and machine-tractable recombination  and extension of the previously implemented ONSE modules. The strength of the approach is that it emphasizes the use of the existing NLP applications, with very few domain- and goalspecific adjustments, in a most promising and growing new area of IAS. So, while clearly dealing with a new application, the paper addresses theoretical and methodological extensions of ONSE, as defined currently, that will be useful for other applications as well. 
This article presents an approach to interpret the content of documents in constrained domains at the level of communicative goals. The kind of knowledge used contains descriptions of wellformed document contents and texts that can be produced from them. The automatic analysis of text content is followed by an interactive negotiation phase involving an expert of the class of documents. Motivating reasons are given for an application of this approach, document normalization, and an implemented system is brieﬂy introduced.1 
In this paper we describe the Senseval 3 Basque lexical sample task. The task comprised 40 words (15 nouns, 15 verbs and 10 adjectives) selected from the Basque WordNet. 10 of the words were chosen in coordination with other lexical-sample tasks. The examples were taken from newspapers, an in-house balanced corpus and Internet texts. We additionally included a large set of untagged examples, and a lemmatised version of the data including lemma, PoS and case information. The method used to hand-tag the examples produced an inter-tagger agreement of 78.2% before arbitration. The eight competing systems attained results well above the most frequent baseline and the best system from Swarthmore College scored 70.4% recall. 
This paper describes the English–Hindi Multilingual lexical sample task in SENSEVAL–3. Rather than tagging an English word with a sense from an English dictionary, this task seeks to assign the most appropriate Hindi translation to an ambiguous target word. Training data was solicited via the Open Mind Word Expert (OMWE) from Web users who are ﬂuent in English and Hindi. 
The SENSEVAL-3 task to perform automatic labeling of semantic roles was designed to encourage research into and use of the FrameNet dataset. The task was based on the considerable expansion of the FrameNet data since the baseline study of automatic labeling of semantic roles by Gildea and Jurafsky. The FrameNet data provide an extensive body of “gold standard” data that can be used in lexical semantics research, as the basis for its further exploitation in NLP applications. Eight teams participated in the task, with a total of 20 runs. Discussions among participants during development of the task and the scoring of their runs contributed to a successful task. Participants used a wide variety of techniques, investigating many aspects of the FrameNet data. They achieved results showing considerable improvements from Gildea and Jurafsky’s baseline study. Importantly, their efforts have contributed considerably to making the complex FrameNet dataset more accessible. They have amply demonstrated that FrameNet is a substantial lexical resource that will permit extensive further research and exploitation in NLP applications in the future. Introduction Word-sense disambiguation has frequently been criticized as a task in search of a reason. Since a considerable portion of a sense inventory has only a single sense, the question has been raised whether the amount of effort required by disambiguation is worthwhile. Heretofore, the focus of disambiguation has been on the sense inventory and has not examined the major reason why we would have lexical knowledge bases: how the meanings would be  represented and thus, available for use in natural language processing applications. At the present time, a major paradigm for representing meaning has emerged in frame semantics, specifically in the FrameNet project. A worthy objective for the Senseval community is the development of a wide range of methods for automating frame semantics, specifically identifying and labeling semantic roles in sentences. An important baseline study of this process has recently appeared in the literature (Gildea and Jurafsky, 2002). The FrameNet project (Johnson et al., 2003) has put together a body of hand-labeled data and the Gildea and Jurafsky study has put together a set of suitable metrics for evaluating the performance of an automatic system. 
The SENSEVAL-3 task to perform word-sense disambiguation of WordNet glosses was designed to encourage development of technology to make use of standard lexical resources. The task was based on the availability of sensedisambiguated hand-tagged glosses created in the eXtended WordNet project. The hand-tagged glosses provided a “gold standard” for judging the performance of automated disambiguation systems. Seven teams participated in the task, with a total of 10 runs. Scoring these runs as an “all-words” task, along with considerable discussions among participants, provided more insights than just the underlying technology. The task identified several issues about the nature of the WordNet sense inventory and the underlying use of wordnet design principles, particularly the significance of WordNet-style relations. Introduction In SENSEVAL-2, performance in the lexical sample task dropped considerably (Kilgarriff, 2001). Kilgarriff suggested that using WordNet (Fellbaum, 1998) for SENSEVAL has drawbacks. WordNet was not designed to serve as a lexical resource, but its public availability and reasonable comprehensiveness have been dominant factors in its selection as the lexical resource of choice for Senseval and for many applications. These factors have led to further funding by U.S. government agencies and many improvements are currently underway. Among these improvements is a planned hand-tagging of the WordNet glosses with their WordNet senses. At the same time, sense-tagging of the glosses is being performed in the Extended WordNet (XWN) project under development at the University of Texas at  Dallas (Mihalcea and Moldovan, 2001)1. The XWN project also parses the WordNet glosses into a part of speech tree and transforms them into a logical predicate form. More generally, sense disambiguation of definitions in any lexical resource is an important objective in the language engineering community. The first significant disambiguation of dictionary definitions and creation of a hierarchy took place 25 years ago in the groundbreaking work of Amsler (1980). However, while substantial research has been performed on machine-readable dictionaries since that time, technology has not yet been developed to make systematic use of these resources. This SENSEVAL task was designed to encourage the lexical research community to take up the challenge of disambiguating dictionary definitions. XWN is used as a core knowledge base for applications such as question answering, information retrieval, information extraction, summarization, natural language generation, inferences, and other knowledge intensive applications. The glosses contain a part of the world knowledge since they define the most common concepts of the English language. In the XWN project, many open-class words in WordNet glosses have been hand-tagged and provide a “gold standard” against which disambiguation systems can be judged. The SENSEVAL-3 task is to replicate the hand-tagged results. The Extended WordNet (XWN) project has disambiguated the content words (nouns, verbs, adjectives, and adverbs) of all glosses, combining human annotation and automated methods using WordNet 1.7.1. A “quality” attribute was given to each lemma. XWN used two automatic systems to disambiguate the content words. When the two systems agreed, the lemma was given a “silver” 1http://www.hlt.utdallas.edu/  quality. Otherwise, a lemma was given a “normal” quality (even when there was only one sense in WordNet). In a complex process described in more detail below, certain glosses or lemmas were selected for hand annotation. Lemmas which were handtagged were given a “gold” tag. The WordNet 1.7.1 data were next converted to use WordNet 2.0 glosses. Word senses have been assigned to 630,599 open class words, with 15,179 (less than 2.5 percent) of the open-class words in these glosses assigned manually. Many glosses have more than one word given a “gold” assignment. The resultant test set provided to participants consists of 9,257 glosses, containing 15,179 “gold” tagged content words and a total of 42,491 content words, distributed as follows:  POS Adjective Adverb Noun Verb Total  Table 1. Gloss Test Set  Glosses Golds  94  263  1684  1826  6706  10985  773  2105  9257  15179  Words 370 3719 35539 2863 42491  The disambiguations (and hence the answer key) are available at the XWN web site. Participants were encouraged to investigate the XWN data as well as the methods followed by the XWN team. However, participants were expected to develop their own systems, for comparison with the XWN manual annotations.  
The Italian lexical sample task at SENSEVAL-3 provided a framework to evaluate supervised and semi-supervised WSD systems. This paper reports on the task preparation – which offered the opportunity to review and refine the Italian MultiWordNet – and on the results of the six participants, focussing on both the manual and automatic tagging procedures. 
This paper presents the task deﬁnition, resources, participating systems, and comparative results for the English lexical sample task, which was organized as part of the SENSEVAL-3 evaluation exercise. The task drew the participation of 27 teams from around the world, with a total of 47 systems. 
This paper presents the task deﬁnition, resources, participating systems, and comparative results for a Romanian Word Sense Disambiguation task, which was organized as part of the SENSEVAL-3 evaluation exercise. Five teams with a total of seven systems were drawn to this task. 
This paper presents a ﬁrst experience with evaluating sytems that address the issue of Logic Form Identiﬁcation (LFi). A Gold Standard approach was used in which experts provide solutions to test data. The expert solutions, the gold standard, are then compared against outputs from participanting systems and different metrics observed. We proposed a few novel metrics, including precision and recall, that are further used to provide comparative results. The test data included 4155 arguments and 2398 predicates grouped in 300 sentences. 
This paper describes the Italian all-words sense disambiguation task for Senseval-3. The annotation procedure and criteria together with the encoding of multiwords are presented. 
Our group participated in the Basque and English lexical sample tasks in Senseval-3. A language-speciﬁc feature set was deﬁned for Basque. Four diﬀerent learning algorithms were applied, and also a method that combined their outputs. Before submission, the performance of the methods was tested for each task on the Senseval-3 training data using cross validation. Finally, two systems were submitted for each language: the best single algorithm and the best ensemble. 
We describe our participation in two of the tasks organized within Senseval-3: Automatic Labeling of Semantic Roles and Identiﬁcation of Logic Forms in English. 
This paper describes a system developed for the transformation of English sentences into a first order logical form representation. The metho dology is centered on the use of a dependency grammar based parser . We demonstrate the suitability of applying a dependency parser based solution to the given task and in turn explain some of the limitations and challenges involved when using such an approach. The efficiencies and deficiencies of our approach are discussed as well as considerations for further enhanc ements. 
This paper describes the exemplar based approach presented by UNED at Senseval-3. Instead of representing contexts as bags of terms and deﬁning a similarity measure between contexts, we propose to represent terms as bags of contexts and deﬁne a similarity measure between terms. Thus, words, lemmas and senses are represented in the same space (the context space), and similarity measures can be deﬁned between them. New contexts are transformed into this representation in order to calculate their similarity to the candidate senses. We show how standard similarity measures obtain better results in this framework. A new similarity measure in the context space is proposed for selecting the senses and performing disambiguation. Results of this approach at Senseval-3 are here reported. 
We describe a system for semantic role assignment built as part of the Senseval III task, based on an off-the-shelf parser and Maxent and Memory-Based learners. We focus on generalisation using several similarity measures to increase the amount of training data available and on the use of EM-based clustering to improve role assignment. Our ﬁnal score is Precision=73.6%, Recall=59.4% (F=65.7). 
In this paper, we describe MITRE’s contribution to the logical form generation track of Senseval-3. We begin with a description of the context of MITRE’s work, followed by a description of the MITRE system and its results. We conclude with a commentary on the form and structure of this evaluation track. 
This paper describes our method based on Support Vector Machines for automatically assigning semantic roles to constituents of English sentences. This method employs four different feature sets, one of which being ﬁrst reported herein. The combination of features as well as the extended training data we considered have produced in the Senseval-3 experiments an F1-score of 92.5% for the unrestricted case and of 76.3% for the restricted case. 
The CIAOSENSO WSD system is based on Conceptual Density, WordNet Domains and frequences of WordNet senses. This paper describes the upvunige-CIAOSENSO WSD system, we participated in the english all-word task with, and its versions used for the english lexical sample and the WordNet gloss disambiguation tasks. In the last an additional goal was to check if the disambiguation of glosses, that has been performed during our tests on the SemCor corpus, was done properly or not. Introduction The CIAOSENSO WSD system is an unsupervised system based on Conceptual Density (Agirre and Rigau, 1995), frequencies of WordNet senses, and WordNet Domains (Magnini and Cavagli`a, 2000). Conceptual Density (CD) is a measure of the correlation among the sense of a given word and its context. The foundation of this measure is the Conceptual Distance, deﬁned as the length of the shortest path which connects two concepts in a hierarchical semantic net. The starting point for our work was the CD formula of Agirre and Rigau (Agirre and Rigau, 1995), which compares areas of subhierarchies. The noun sense disambiguation in the CIAOSENSO WSD system is performed by means of a formula combining Conceptual Density with WordNet sense frequency (Rosso et al., 2003). WordNet Domains is an extension of WordNet 1.6, developed at ITC-irst1, where each synset has been annotated with at least one domain label, selected from a set of about two hundred labels hierarchically organized (Magnini and Cavagli`a, 2000). Since the lexical resource used by the upvunige-CIAOSENSO WSD system is WordNet 2.0 (WN2.0), it has been necessary to map the synsets of WordNet Domains from version 1.6 to the version 2.0. This has been done in a fully automated way, by using the WordNet mappings for nouns and 1Istituto per la Ricerca Scientiﬁca e Tecnologica, Trento, Italy  verbs, and by checking the similarity of synset terms and glosses for adjectives and adverbs. Some domains have also been assigned by hand in some cases, when necessary.  
For SENSEVAL-3, the University of Maryland (UMD) team focused on two primary issues: the portability of sense disambigation across languages, and the exploitation of real-world bilingual text as a resource for unsupervised sense tagging. We validated the portability of our supervised disambiguation approach by applying it in seven tasks (English, Basque, Catalan, Chinese, Romanian, Spanish, and “multilingual” lexical samples), and we experimented with a new unsupervised algorithm for sense modeling using parallel corpora.  
The HKUST word sense disambiguation systems beneﬁt from a new nonlinear Kernel Principal Component Analysis (KPCA) based disambiguation technique. We discuss and analyze results from the Senseval-3 English, Chinese, and Multilingual Lexical Sample data sets. Among an ensemble of four different kinds of voted models, the KPCA-based model, along with the maximum entropy model, outperforms the boosting model and na¨ıve Bayes model. Interestingly, while the KPCAbased model typically achieves close or better accuracy than the maximum entropy model, nevertheless a comparison of predicted classiﬁcations shows that it has a signiﬁcantly different bias. This characteristic makes it an excellent voter, as conﬁrmed by results showing that removing the KPCA-based model from the ensemble generally degrades performance. 
É£RQTSVUWS FH9BX S ADF Áp}D}rstu w xÁ!r£y!¤Y${$qr xWhDd}Wt$z ht IDqtWyts tDI£Wt¾r! ­$£WI{$qDI 
This paper describes the system MC-WSD presented for the English Lexical Sample task. The system is based on a multicomponent architecture. It consists of one classiﬁer with two components. One is trained on the data provided for the task. The second is trained on this data and, additionally, on an external training set extracted from the Wordnet glosses. The goal of the additional component is to lessen sparse data problems by exploiting the information encoded in the ontology. 
For Sinequa’s second participation to the Senseval evaluation, two systems using contextual semantic have been proposed. Based on different approaches, they both share the same data preprocessing and enrichment. The first system is a combined approach using semantic classification trees and information retrieval techniques. For the second system, the words from the context are considered as clues. The final sense is determined by summing the weight assigned to each clue for a given example. 
 2 Stemming  The Naïve Bayes classification proves to be a good performing tool in word sense disambiguation, although it has not yet been applied to the Romanian language. The aim of this paper is to present our WSD system, based on the NBC algorithm, that performed quite well in Senseval 3. 
GAMBL is a word expert approach to WSD in which each word expert is trained using memorybased learning. Joint feature selection and algorithm parameter optimization are achieved with a genetic algorithm (GA). We use a cascaded classiﬁer approach in which the GA optimizes local context features and the output of a separate keyword classiﬁer (rather than also optimizing the keyword features together with the local context features). A further innovation on earlier versions of memorybased WSD is the use of grammatical relation and chunk features. This paper presents the architecture of the system brieﬂy, and discusses its performance on the English lexical sample and all words tasks in SENSEVAL-3. 
This paper describes a hybrid system for WSD, presented to the English all-words and lexical-sample tasks, that relies on two different unsupervised approaches. The ﬁrst one selects the senses according to mutual information proximity between a context word a variant of the sense. The second heuristic analyzes the examples of use in the glosses of the senses so that simple syntactic patterns are inferred. This patterns are matched against the disambiguation contexts. We show that the ﬁrst heuristic obtains a precision and recall of .58 and .35 respectively in the all words task while the second obtains .80 and .25. The high precision obtained recommends deeper research of the techniques. Results for the lexical sample task are also provided. 
This paper describes the architecture and results of the University of Jaén system presented at the SENSEVAL-3 for the Englishlexical-sample and English-All-Words tasks. The system is based on a neural network approach. We have used the Learning Vector Quantization, which is a supervised learning algorithm based on the Kohonen neural model. 
This article describes the four systems sent by the author to the SENSEVAL-3 contest, the English lexical sample task. The best recognition rate obtained by one of these systems was 72.9% (ﬁne grain score) . 
As a task in SensEval-3, Automatic Labeling of Semantic Roles is to identify frame elements within a sentence and tag them with appropriate semantic roles given a sentence, a target word and its frame. We apply Maximum Entropy classification with feature sets of syntactic patterns from parse trees and officially attain 80.2% precision and 65.4% recall. When the frame element boundaries are given, the system performs 86.7% precision and 85.8% recall. 
In this paper, we describe our experiments on statistical word sense disambiguation (WSD) using two systems based on diﬀerent approaches: Na¨ıve Bayes on word tokens and Maximum Entropy on local syntactic and semantic features. In the ﬁrst approach, we consider a context window and a sub-window within it around the word to disambiguate. Within the outside window, only content words are considered, but within the sub-window, all words are taken into account. Both window sizes are tuned by the system for each word to disambiguate and accuracies of 75% and 67% were respectively obtained for coarse and ﬁne grained evaluations. In the second system, sense resolution is done using an approximate syntactic structure as well as semantics of neighboring nouns as features to a Maximum Entropy learner. Accuracies of 70% and 63% were obtained for coarse and ﬁne grained evaluations. 
We participated in the SENSEVAL-3 English lexical sample task and multilingual lexical sample task. We adopted a supervised learning approach with Support Vector Machines, using only the ofﬁcial training data provided. No other external resources were used. The knowledge sources used were partof-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic relations. For the translation and sense subtask of the multilingual lexical sample task, the English sense given for the target word was also used as an additional knowledge source. For the English lexical sample task, we obtained ﬁne-grained and coarse-grained score (for both recall and precision) of 0.724 and 0.788 respectively. For the multilingual lexical sample task, we obtained recall (and precision) of 0.634 for the translation subtask, and 0.673 for the translation and sense subtask. 
In SENSEVAL-3, CL Research participated in four tasks: English all-words, English lexical sample, disambiguation of WordNet glosses, and automatic labeling of semantic roles. This participation was performed within the development of CL Research’s Knowledge Management System, which massively tags texts with syntactic, semantic, and discourse characterizations and attributes. This System is fully integrated with CL Research’s DIMAP dictionary maintenance software, which provides access to one or more dictionaries for disambiguation and representation. Our core disambiguation functionality, unchanged since SENSEVAL-2, performed at a level comparable to our previous performance. Our participation in the SENSEVAL-3 tasks was concerned primarily with text processing and representation issues and did not advance our disambiguation capabilities. Introduction CL Research participated in four SENSEVAL-3 tasks: English all-words, English lexical sample, disambiguation of WordNet glosses, and automatic labeling of semantic roles. We also ran the latter two tasks, but since their test sets were generated blindly, our results did not involve use of any prior information. Our participation in these tasks is a continuation and extension of our efforts to perform NLP tasks within an integrated text processing system known as the Knowledge Management System (KMS). KMS parses and processes text into an XML representation tagged with syntactic, semantic, and discourse properties. This representation is then used for such tasks as question answering and text summarization  (Litkowski, 2004a; Litkowski, 2004b). The SENSEVAL-3 tasks were performed as part of CL Research’s efforts to extend and improve the semantic characterizations in the KMS XML representations. For each SENSEVAL-3 task, the corresponding texts in the test sets were processed using the general KMS functionality. However, since the texts involved in the SENSEVAL tasks were quite small, the amount of processing was quite minimal. The descriptions below focus on the integration of disambiguation technology in a larger system and do not present any advancements in this technology. 
In word sense disambiguation (WSD), the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed. The ﬁrst (or predominant) sense heuristic assumes the availability of handtagged data. Whilst there are hand-tagged corpora available for some languages, these are relatively small in size and many word forms either do not occur, or occur infrequently. In this paper we investigate the performance of an unsupervised ﬁrst sense heuristic where predominant senses are acquired automatically from raw text. We evaluate on both the SENSEVAL-2 and SENSEVAL-3 English allwords data. For accurate WSD the ﬁrst sense heuristic should be used only as a back-off, where the evidence from the context is not strong enough. In this paper however, we examine the performance of the automatically acquired ﬁrst sense in isolation since it turned out that the ﬁrst sense taken from SemCor outperformed many systems in SENSEVAL-2. 
This paper introduces SENSELEARNER – a minimally supervised sense tagger that attempts to disambiguate all content words in a text using the senses from WordNet. SENSELEARNER participated in the SENSEVAL-3 English all words task, and achieved an average accuracy of 64.6%. 
This paper describes the SyntaLex entries in the English Lexical Sample Task of SENSEVAL-3. There are four entries in all, where each of the different entries corresponds to use of word bigrams or Part of Speech tags as features. The systems rely on bagged decision trees, and focus on using pairs of lexical and syntactic features individually and in combination. They are descendants of the Duluth systems that participated in SENSEVAL-2. 
Logic Forms, particular powerful logic representations presented in Moldovan and Rus (2001), are simple yet highly effective. In this paper, the structure of Logic Forms and their generation from input text are described. The results of an evaluation comparing the Logic Forms generated by hand with those generated automatically are also reported. Finally, we suggest some improvements to the representation used in the LFI task based on our results. 
We present a supervised approach to Word Sense Disambiguation (WSD) based on Specialized Hidden Markov Models. We used as training data the Semcor corpus and the test data set provided by Senseval 2 competition and as dictionary the Wordnet 1.6. We evaluated our system on the English all-word task of the Senseval-3 competition.  
In this paper we describe the SSI algorithm, a structural pattern matching algorithm for WSD. The algorithm has been applied to the gloss disambiguation task of Senseval-3. 
This paper describes the HKPolyU-HKUST systems which were entered into the Semantic Role Labeling task in Senseval-3. Results show that these systems, which are based upon common machine learning algorithms, all manage to achieve good performances on the non-restricted Semantic Role Labeling task. 
Traditionally, word sense disambiguation (WSD) involves a different context model for each individual word. This paper presents a new approach to WSD using weakly supervised learning. Statistical models are not trained for the contexts of each individual word, but for the similarities between context pairs at category level. The insight is that the correlation regularity between the sense distinction and the context distinction can be captured at category level, independent of individual words. This approach only requires a limited amount of existing annotated training corpus in order to disambiguate the entire vocabulary. A context clustering scheme is developed within the Bayesian framework. A maximum entropy model is then trained to represent the generative probability distribution of context similarities based on heterogeneous features, including trigger words and parsing structures. Statistical annealing is applied to derive the final context clusters by globally fitting the pairwise context similarity distribution. Benchmarking shows that this new approach significantly outperforms the existing WSD systems in the unsupervised category, and rivals supervised WSD systems. 
This article describes the implementation of I2R word sense disambiguation system (I2R − W SD) that participated in one senseval3 task: Chinese lexical sample task. Our core algorithm is a supervised Naive Bayes classiﬁer. This classiﬁer utilizes an optimal feature set, which is determined by maximizing the cross validated accuracy of NB classiﬁer on training data. The optimal feature set includes partof-speech with position information in local context, and bag of words in topical context. 
This paper describes the NMSU-Pitt-UNCA word-sense disambiguation system participating in the Senseval-3 English lexical sample task. The focus of the work is on using semantic class-based collocations to augment traditional word-based collocations. Three separate sources of word relatedness are used for these collocations: 1) WordNet hypernym relations; 2) cluster-based word similarity classes; and 3) dictionary deﬁnition analysis. 
Two systems from the University of Minnesota, Duluth participated in various SENSEVAL-3 lexical sample tasks. The supervised learning system is based on lexical features and bagged decision trees. It participated in lexical sample tasks for the English, Spanish, Catalan, Basque, Romanian and MultiLingual English-Hindi data. The unsupervised system uses measures of semantic relatedness to ﬁnd the sense of the target word that is most related to the senses of its neighbors. It participated in the English lexical sample task. 
The paper describes RLSC-LIN and RLSCCOMB systems which participated in the Senseval-3 English lexical sample task. These systems are based on Regularized Least-Squares Classiﬁcation (RLSC) learning method. We describe the reasons of choosing this method, how we applied it to word sense disambiguation, what results we obtained on Senseval1, Senseval-2 and Senseval-3 data and discuss some possible improvements. 
SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, Barcelona, Spain, July 2004 Association for Computational Linguistics 
The task of word sense disambiguation is to assign a sense label to a word in a passage. We report our algorithms and experiments for the two tasks that we participated in viz. the task of WSD of WordNet glosses and the task of WSD of English lexical sample. For both the tasks, we explore a method of sense disambiguation through a process of “comparing” the current context for a word against a repository of contextual clues or glosses for each sense of each word. We compile these glosses in two different ways for the two tasks. For the ﬁrst task, these glosses are all compiled using WordNet and are of various types viz. hypernymy glosses, holonymy mixture, descriptive glosses and some hybrid mixtures of these glosses. The “comparison” could be done in a variety of ways that could include/exclude stemming, expansion of one gloss type with another gloss type, etc. The results show that the system does best when stemming is used and glosses are expanded. However, it appears that the evidence for word-senses ,accumulated through WordNet, in the form of glosses, are quite sparse. Generating dense glosses for all WordNet senses requires a massive sense tagged corpus - which is currently unavailable. Hence, as part of the English lexical sample task, we try the same approach on densely populated glosses accumulated from the training data for this task. 
We have participated in both English all words task and English lexical sample task of SENSEVAL3. Our system disambiguates senses of a target word in a context by selecting a substituent among WordNet relatives of the target word, such as synonyms, hypernyms, meronyms and so on. The decision is made based on co-occurrence frequency between candidate relatives and each of the context words. Since the co-occurrence frequency is obtainable from raw corpus, our method is considered to be an unsupervised learning algorithm that does not require a sense-tagged corpus. 
It is known that whenever a system’s actions depend on the meaning of the text being processed, disambiguation is beneﬁcial or even necessary. The contest Senseval is an international frame where the research in this important ﬁeld is validated in an hierarchical manner. In this paper we present our system participating for the ﬁrst time at Senseval 3 contest on WSD, contest developed in March-April 2004. We present also our intentions on improving our system, intentions occurred from the study of results. 
This paper summarizes IRST’s participation in Senseval-3. We participated both in the English allwords task and in some lexical sample tasks (English, Basque, Catalan, Italian, Spanish). We followed two perspectives. On one hand, for the allwords task, we tried to reﬁne the Domain Driven Disambiguation that we presented at Senseval-2. The reﬁnements consist of both exploiting a new technique (Domain Relevance Estimation) for domain detection in texts, and experimenting with the use of Latent Semantic Analysis to avoid reliance on manually annotated domain resources (e.g. WORDNET DOMAINS). On the other hand, for the lexical sample tasks, we explored the direction of pattern abstraction and we demonstrated the feasibility of leveraging external knowledge using kernel methods. 
This paper describes the four entries from the University of Utah in the semantic role labeling task of SENSEVAL-3. All the entries took a statistical machine learning approach, using the subset of the FrameNet corpus provided by SENSEVAL-3 as training data. Our approach was to develop a model of natural language generation from semantics, and train the model using maximum likelihood and smoothing. Our models performed satisfactorily in the competition, and can ﬂexibly handle varying permutations of provided versus inferred information. 
This paper describes the National Research Council (NRC) Word Sense Disambiguation (WSD) system, as applied to the English Lexical Sample (ELS) task in Senseval-3. The NRC system approaches WSD as a classical supervised machine learning problem, using familiar tools such as the Weka machine learning software and Brill’s rule-based part-of-speech tagger. Head words are represented as feature vectors with several hundred features. Approximately half of the features are syntactic and the other half are semantic. The main novelty in the system is the method for generating the semantic features, based on word co-occurrence probabilities. The probabilities are estimated using the Waterloo MultiText System with a corpus of about one terabyte of unlabeled text, collected by a web crawler. 
The DLSI-UA team is currently working on several word sense disambiguation approaches, both supervised and unsupervised. These approaches are based on different ways to use both annotated and unannotated data, and several resources generated from or exploiting WordNet (Miller et al., 1993), WordNet Domains, EuroWordNet (EWN) and additional corpora. This paper presents a view of different system results for Word Sense Disambiguation in different tasks of SENSEVAL-3. 
The R2D2 systems for the English All-Words and Lexical Sample tasks at SENSEVAL-3 are based on several supervised and unsupervised methods combined by means of a voting procedure. Main goal was to take advantage of training data when available, and getting maximum coverage with the help of methods that not need such learning examples. The results reported in this paper show that supervised and unsupervised methods working in parallel, and a simple sequence of preferences when comparing the answers of such methods, is a feasible method. . . The whole system is, in fact, a cascade of decisions of what label to assign to a concrete instance based on the agreement of pairs of systems, when it is possible, or selecting the available answer from one of them. In this way, supervised are preferred to unsupervised methods, but these last ones are able to tag such words that not have available training data. 
This paper presents the Swarthmore College wordsense disambiguation system which was designed for the 2004 SENSEVAL3 competition. Our system participated in ﬁve tasks: the lexical sample tasks in Basque, Catalan, Italian, Romanian, and Spanish. For each task, a suite of supervised algorithms were combined using voting to form the ﬁnal system. 
This paper describes the component models and combination model built as a joint effort between Swarthmore College, Hong Kong PolyU, and HKUST. Though other models described elsewhere contributed to the ﬁnal combination model, this paper focuses solely on the joint contributions to the ”Swat-HK” effort. 
Multi-document person name resolution focuses on the problem of determining if two instances with the same name and from different documents refer to the same individual. We present a two-step approach in which a Maximum Entropy model is trained to give the probability that two names refer to the same individual. We then apply a modified agglomerative clustering technique to partition the instances according to their referents.  process of insertion requires that concept-instance pairs that have the same referent be merged together (e.g. Paul Simon/pop star and Paul Simon /singer). Further, instances of the same name, but with different referents, must be distinguished (e.g. Paul Simon/pop star and Paul Simon /politician). We propose a two-step approach: first, we train a maximum entropy model to generate the probability that any two concept-instance pairs refer to one and the same referent. Then, a modified agglomerative clustering technique is used to merge the most likely instances together, forming clusters that correspond to individual referents. 2 Related Work  
By combining information extraction and record linkage techniques, we have created a repository of references to attorneys, judges, and expert witnesses across a broad range of text sources. These text sources include news, caselaw, law reviews, Medline abstracts, and legal briefs among others. We briefly describe our cross document co-reference resolution algorithm and discuss applications these resolved references enable. Among these applications is one that shows summaries of relationships chains between individuals based on their document co-occurrence and cross document co-references. 
Event clustering on streaming news aims to group documents by events automatically. This paper employs co-reference chains to extract the most representative sentences, and then uses them to select the most informative features for clustering. Due to the long span of events, a fixed threshold approach prohibits the latter documents to be clustered and thus decreases the performance. A dynamic threshold using time decay function and spanning window is proposed. Besides the noun phrases in co-reference chains, event words in each sentence are also introduced to improve the related performance. Two models are proposed. The experimental results show that both event words and co-reference chains are useful on event clustering. 
We present a novel method of applying the results of coreference resolution to improve Name Recognition for Chinese. We consider first some methods for gauging the confidence of individual tags assigned by a statistical name tagger. For names with low confidence, we show how these names can be filtered using coreference features to improve accuracy. In addition, we present rules which use coreference information to correct some name tagging errors. Finally, we show how these gains can be magnified by clustering documents and using cross-document coreference in these clusters. These combined methods yield an absolute improvement of about 3.1% in tagger F score. 
In this work we test the use of word similarity lists for anaphora resolution in Portuguese corpora. We applied an automatic lexical acquisition technique over parsed texts to identify semantically similar words. After that, we made use of this lexical knowledge to resolve coreferent deﬁnite descriptions where the head-noun of the anaphor is different from the head-noun of its antecedent, which we call indirect anaphora. 
 Vieira and Poesio (2000) proposed an algorithm for  Vieira and Poesio (2000) proposed an algorithm for deﬁnite description (DD) resolution that incorporates a number of heuristics for detecting discoursenew descriptions. The inclusion of such detectors was motivated by the observation that more than 50% of deﬁnite descriptions (DDs) in an average corpus are discourse new (Poesio and Vieira, 1998), but whereas the inclusion of detectors for non-anaphoric pronouns in algorithms such as Lappin and Leass’ (1994) leads to clear improvements in precision, the improvements in anaphoric DD resolution (as opposed to classiﬁcation) brought about by the detectors were rather small. In fact, Ng and Cardie (2002a) challenged the motivation for the inclusion of such detectors, reporting no improvements, or even worse performance. We re-examine  deﬁnite description resolution that incorporates a number of heuristics for detecting discourse-new (henceforth: DN) descriptions. But whereas the inclusion of detectors for non-anaphoric pronouns (e.g., It in It’s raining) in algorithms such as Lappin and Leass’ (1994) leads to clear improvements in precision, the improvements in anaphoric DD resolution (as opposed to classiﬁcation) brought about by the detectors were rather small. In fact, Ng and Cardie (2002a) challenged the motivation for the inclusion of such detectors, reporting no improvements or even worse performance. We reexamine the literature on the topic in detail, and propose a revised algorithm, taking advantage of the improved DN detection techniques developed by Uryupina (2003).  the literature on the topic in detail, and propose a revised algorithm, taking advantage of the improved discourse-new detection techniques developed by Uryupina (2003).  2 Detecting Discourse-New Deﬁnite Descriptions 2.1 Vieira and Poesio  Poesio and Vieira (1998) carried out corpus stud-  
This article studies the resolution of references made by speakers to documents discussed during a meeting. The focus is on transcribed recordings of press review meetings, in French. After an overview of the required framework for reference resolution—speciﬁcation of the task, data annotation, and evaluation procedure—we propose, analyze and evaluate an algorithm for the resolution of references to documents (ref2doc) based on anaphora tracking and context matching. Applications to speech-to-document alignment and more generally to meeting processing and retrieval are ﬁnally discussed. 
The need for associating, or grounding, protein names in the literature with the entries of proteome databases such as Swiss-Prot is well-recognized. The protein names in the biomedical literature show a high degree of morphological and syntactic variations, and various anaphoric expressions including null anaphors. We present a biomedical anaphora resolution system, BioAR, in order to address the variations of protein names and to further associate them with Swiss-Prot entries as the actual entities in the world. The system shows the performance of 59.5%¢ 75.0% precision and 40.7%¢ 56.3% recall, depending on the speciﬁc types of anaphoric expressions. We apply BioAR to the protein names in the biological interactions as extracted by our biomedical information extraction system, or BioIE, in order to construct protein pathways automatically. 
We present a default-unification-based approach to ellipsis resolution that is based on experience in long running multimodal dialog projects, where it played an essential role in discourse processing. We extend default unification to non-parallel structures, which is important for speech and multimodal dialog systems. We introduce new control mechanisms for ellipsis resolution by considering dialog structure with respect to specification, variation and results of tasks and combine this with the analysis of relations between the information elements contained in antecedent and elliptic structures. 
This paper describes the dar-algorithm for resolving intersentential pronominal anaphors referring to individual and abstract entities in Danish texts and dialogues. Individual entities are resolved combining models which identify high degree of salience with high degree of givenness (topicality) of entities in the hearer’s cognitive model, e.g. (Grosz et al., 1995), with Hajiˇcov´a et al.’s (1990) salience account which assigns the highest degree of salience to entities in the focal part of an utterance in Information Structure terms. These focal entities often introduce new information in discourse. Anaphors referring to abstract entities are resolved with an extension of the algorithm presented by Eckert and Strube (2000). Manual tests of the dar-algorithm and other well-known resolution algorithms on the same data show that dar performs signiﬁcantly better on most types of anaphor. 
In this paper we are concerned with identifying the topics of sentences in Chinese texts. The key elements of the centering model of local discourse coherence are employed to identify the topic which is the most salient element in a Chinese sentence. Due to the phenomenon of zero anaphora occurring in Chinese texts frequently, in addition to the centering model, we further employ the constraint rules to identify the antecedents of zero anaphors. Unlike most traditional approaches to parsing sentences based on the integration of complex linguistic information and domain knowledge, we work on the output of a part-of-speech tagger and use shallow parsing instead of complex parsing to identify the topics from sentences. 
We describe an approach to text planning that uses the XSLT template-processing engine to create logical forms for an external surface realizer. Using a realizer that can process logical forms with embedded alternatives provides a substitute for backtracking in the text-planning process. This allows the text planner to combine the strengths of the AI-planning and template-based traditions in natural language generation. 
Within two European projects metadata interoperability is one of the central topics. While the INTERA project has as one of its goals to achieve an interoperability between two widely used metadata sets for the domain of language resources, the ECHO project created an integrated metadata domain of in total nine data providers from five different disciplines from the humanities. In both projects ad hoc techniques are used to achieve results. In the INTERA project, however, machine readable and ISO compliant concept definitions are created as a first step towards the Semantic Web. In the ECHO project a complex ontology was realized purely relying on XML. It is argued that concept definitions should be registered in open Data Category Repositories and that relations between them should be described as RDF assertions. Yet we are missing standards that would allow us to overcome the ad hoc solutions. 
This paper describes using RDF/RDFS/XML to create and navigate a metadata model of relationships among entities in text. The metadata we create is roughly an order of magnitude smaller than the content being modeled, it provides the end-user with context sensitive information about the hyperlinked entities in focus. These entities at the core of the model are originally found and resolved using a combination of information extraction and record linkage techniques. The RDF/RDFS metadata model is then used to ”look ahead” and navigate to related information. An RDF aware frontend web application streamlines the presentation of information to the end user. 
A method for mapping linguistic descriptions in plain XML into semantically rich RDF/OWL is outlined and demonstrated. Starting with Simons’s (2003) original proof of concept of this method, we extend his Semantic Interpretation Language (SIL) for creating metaschemas to carry out the mapping, employ the General Ontology for Linguistic Description (GOLD) of Farrar and Langendoen (2003) as the target semantic schema, and make use of SeRQL, an RDF-aware search engine. This data migration effort is in keeping with the vision of a Semantic Web; it is part of an effort to build a ‘community of practice’ around semantically rich linguistic resources. 
Mikrokosmos contains an ontology plus a number of lexicons in different languages that were originally developed for machine translation. The underlying representation formalism for these resources is an ad-hoc frame-based language which makes it difﬁcult to inter-operate Mikrokosmos with state-ofthe-art knowledge-based systems. In this paper we propose a translation from the frame-based representation of Mikrokosmos into Description logics. This translation allows us to automatically transform Mikrokosmos sources into OWL and thus provide a powerful ontology in the formalism of the semantic web. Furthermore, the reasoning mechanisms of Description Logics may also support knowledge acquisition and maintenance as well as its application in natural language processing systems. 
This paper reports on an ongoing project that combines NLP with semantic web technologies to support a content-based storage and retrieval of medical pathology reports. We describe the NLP component of the project (a robust parser) and the background knowledge component (a domain ontology represented in OWL), and how they work together during extraction of domain speciﬁc information from natural language reports. The system provides a good example of how NLP techniques can be used to populate the Semantic Web. 
In this paper, we describe an integrated approach towards dealing with various semantic and structural issues associated with document management. We provide motivations for using XML, RDF and OWL in building a seamless architecture to serve not only as a document exchange service but also to enable higher level services such as annotations, metadata access and querying. The key idea is to manifest differential treatments for the actual document structure, semantic content of the document and ontological document organization. The deployment of this architecture in the PROTEUS project1 provides an industrial setting for evaluation and further specification. 1. Introduction Digital documents are ubiquitously used to encode, preserve as well as exchange useful information in order to accomplish information sharing across the community. As the growth in volumes of digital data is exponential, it is necessary to adopt a principled way of managing these documents. Besides, due to the distributed nature of information, it is also imperative to take into account the geographical and enterprise-level barriers for uniform data access and retrieval. The ITEA (Information Technology for European Advancement) project, Proteus2 has similar objectives. Proteus is a collaborative initiative of French, German and Belgium companies, universities and research institutes aimed at developing a European generic software platform usable for implementation of web-based e-maintenance centers. It implements a generic architecture for integrated document management using ena- 
Conceptualising a domain has long been recognised as a prerequisite for understanding that domain and processing information about it. Ontologies are explicit specifications of conceptualisations which are now recognised as important components of information systems and information processing. In this paper, we describe a project in which ontologies are part of the reasoning process used for information management and for the presentation of information. Both accessing and presenting information are mediated via natural language and the ontologies are coupled with the lexicon used in the natural language component. 
Over the last three decades, the development of restricted domain applications has been an ongoing theme in computational linguistic research. Speech Recognition, Machine Translation, Summarization, and Question Answering researchers have all built at one time or another restricted domain systems. In retrospect, it is long due to examine both the successes and failures of these previous attempts. In this talk, I will examine the circumstances in which the development of restricted domain applications has led to significant advances in the state of the art and the circumstances in which restricted domain research has had little impact on our field. I will use the lessons learned from previous attempts to building restricted domain natural language processing applications in order to examine the potential impact of current research in restricted domain question answering.  
Question-Answering (QA) evaluation efforts have largely been tailored to open-domain systems. The TREC QA test collections contain newswire articles and the accompanying queries cover a wide variety of topics. While some apprehension about the limitations of restricteddomain systems is no doubt justified, the strict promotion of unlimited domain QA evaluations may have some unintended consequences. Simply applying the open domain QA evaluation paradigm to a restricted-domain system poses problems in the areas of test question development, answer key creation, and test collection construction. This paper examines the evaluation requirements of restricted domain systems. It incorporates evaluation criteria identified by users of an operational QA system in the aerospace engineering domain. While the paper demonstrates that user-centered task-based evaluations are required for restricted domain systems, these evaluations are found to be equally applicable to open domain systems. 
This paper discusses some main difficulties of restricted-domain question-answering systems, in particular the problem of precision performance. We propose methods for improving the precision, which can be classified into two main approaches: improving the Information Retrieval module, and improving its results. We present the application of these methods in a real QA system for a large company, which yielded very good results.  
In this paper we highlight a selection of features of scientific text which distinguish it from news stories. We argue that features such as structure, selective use of past tense, voice and stylistic conventions can affect question answering in the scientific domain. We demonstrate this through qualitative observations made while working on retrieving definitions to terms related to salmon fish. 
We address Question Answering (QA) for biographical questions, i.e., questions asking for biographical facts about persons. The domain of biographical documents differs from other restricted domains in that the available collections of biographies are inherently incomplete: a major challenge is to answer questions about persons for whom biographical information is not present in biography collections. We present BioGrapher, a biographical QA system that addresses this problem by machine learning algorithms for biography classiﬁcation. BioGrapher ﬁrst attempts to answer a question by searching in a given collection of biographies, using techniques tailored for the restricted nature of the domain. If a biography is not found, BioGrapher attempts to ﬁnd an answer on the web: it retrieves documents using a web search engine, ﬁlters these using the biography classiﬁer, and then extracts answers from documents classiﬁed as biographies. Our empirical results show that biographical classiﬁcation, prior to answer extraction, improves the results. 
We present an experiment for designing a logic based QA system, WEBCOOP, that integrates knowledge representation and advanced reasoning procedures to generate cooperative responses to natural language queries on the web. The system is ﬁrst developed for the tourism domain. We then examine how and under what conditions this system can be re-used for other domains. 
This paper describes an on-going research for a practical question answering system for a home agent robot. Because the main concern of the QA system for the home robot is the precision, rather than coverage (No answer is better than wrong answers), our approach is try to achieve high accuracy in QA. We restrict the question domains and extract answers from the pre-selected, semi-structured documents on the Internet. A named entity tagger and a dependency parser are used to analyze the question accurately. User proﬁling and inference rules are used to infer hidden information that is required for ﬁnding a precise answer. Testing with a small set of queries on weather domain, the QA system showed 90.9% of precision and 75.0% of recall. 
In this paper we describe current efforts aimed at adapting an existing Question Answering system to a new document set, namely research papers in the genomics domain. The system has been originally developed for another restricted domain, however it has already proved its portability. Nevertheless, the process is not painless, and the speciﬁc purpose of this paper is to describe the problems encountered. 
To answer questions from clinical-evidence texts, we identify occurrences of the semantic classes — disease, medication, patient outcome — that are candidate elements of the answer, and the relations among them. Additionally, we determine whether an outcome is positive or negative. 
We propose a statistical measure for the degree of acceptability of light verb constructions, such as take a walk, based on their linguistic properties. Our measure shows good correlations with human ratings on unseen test data. Moreover, we ﬁnd that our measure correlates more strongly when the potential complements of the construction (such as walk, stroll, or run) are separated into semantically similar classes. Our analysis demonstrates the systematic nature of the semi-productivity of these constructions. 
Some particular classes of lexical paraphrases such as verb alteration and compound noun decomposition can be handled by a handful of general rules and lexical semantic knowledge. In this paper, we attempt to capture the regularity underlying these classes of paraphrases, focusing on the paraphrasing of Japanese light-verb constructions (LVCs). We propose a paraphrasing model for LVCs that is based on transforming the Lexical Conceptual Structures (LCSs) of verbal elements. We also propose a reﬁnement of an existing LCS dictionary. Experimental results show that our LCS-based paraphrasing model characterizes some of the semantic features of those verbs required for generating paraphrases, such as the direction of an action and the relationship between arguments and surface cases. 
The paper describes an experiment in detecting a speciﬁc type of multiword expressions in Russian, namely expressions starting with a preposition. This covers not only prepositional phrases proper, but also ﬁxed syntactic constructions like v techenie (‘in the course of’). First, we collect lists of such constructions in a corpus of 50 mln words using a simple mechanism that combines statistical methods with knowledge about the structure of Russian prepositional phrases. Then we analyse the results of this data collection and estimate the efﬁciency of the collected list for the resolution of morphosyntactic and semantic ambiguity in a corpus. 
We report that a proper employment of MWEs concerned enables us to put forth a tractable framework, which is based on a multiple nesting of semantic operations, for the processing of non-inferential, Nonpropositional Contents (NPCs) of natural Japanese sentences. Our framework is characterized by its broad syntactic and semantic coverage, enabling us to deal with multiply composite modalities and their semantic/pragmatic similarity. Also, the relationship between indirect (Searle, 1975) and direct speech, and equations peculiar to modal logic and its family (Mally, 1926; Prior, 1967) are treated in the similarity paradigm. 
This paper describes an algorithm that can be used to improve the quality of multiword expressions extracted from documents. We measure multiword expression quality by the “usefulness” of a multiword expression in helping ontologists build knowledge maps that allow users to search a large document corpus. Our stopword based algorithm takes ngrams extracted from documents, and cleans them up to make them more suitable for building knowledge maps. Running our algorithm on large corpora of documents has shown that it helps to increase the percentage of useful terms from 40% to 70% – with an eight-fold improvement observed in some cases. 
This paper describes the representation of Basque Multiword Lexical Units and the automatic processing of Multiword Expressions. After discussing and stating which kind of multiword expressions we consider to be processed at the current stage of the work, we present the representation schema of the corresponding lexical units in a generalpurpose lexical database. Due to its expressive power, the schema can deal not only with fixed expressions but also with morphosyntactically flexible constructions. It also allows us to lemmatize word combinations as a unit and yet to parse the components individually if necessary. Moreover, we describe HABIL, a tool for the automatic processing of these expressions, and we give some evaluation results. This work must be placed in a general framework of written Basque processing tools, which currently ranges from the tokenization and segmentation of single words up to the syntactic tagging of general texts. 
We propose to model multiword expressions as dependency subgraphs, and realize this idea in the grammar formalism of Extensible Dependency Grammar (XDG). We extend XDG to lexicalize dependency subgraphs, and show how to compile them into simple lexical entries, amenable to parsing and generation with the existing XDG constraint solver. 
This paper describes a multi-word expression processor for preprocessing Turkish text for various language engineering applications. In addition to the fairly standard set of lexicalized collocations and multi-word expressions such as named-entities, Turkish uses a quite wide range of semi-lexicalized and non-lexicalized collocations. After an overview of relevant aspects of Turkish, we present a description of the multi-word expressions we handle. We then summarize the computational setting in which we employ a series of components for tokenization, morphological analysis, and multi-word expression extraction. We ﬁnally present results from runs over a large corpus and a small gold-standard corpus. 
This paper presents on-going research on the building of an electronic dictionary of frozen sentences of European Portuguese. It will focus on the problems arising from the description of their formal variation in view of natural language processing 
Multiword Expressions present a challenge for language technology, given their ﬂexible nature. Each type of multiword expression has its own characteristics, and providing a uniform lexical encoding for them is a difﬁcult task to undertake. Nonetheless, in this paper we present an architecture for the lexical encoding of these expressions in a database, that takes into account their ﬂexibility. This encoding extends in a straightforward manner the one required for simplex (single) words, and maximises the information contained for them in the description of multiwords. 
The growing amount of textual information available electronically has increased the need for high performance retrieval. The use of phrases was long seen as a natural way to improve retrieval performance over the common document models that ignore the sequential aspect of word occurrences in documents, considering them as “bags of words”. However, both statistical and syntactical phrases showed disappointing results for large document collections. In this paper we present a recent type of multi-word expressions in the form of Maximal Frequent Sequences (Ahonen-Myka, 1999). Mined phrases rather than statistical or syntactical phrases, their main strengths are to form a very compact index and to account for the sequentiality and adjacency of meaningful word co-occurrences, by allowing for a gap between words. We introduce a method for using these phrases in information retrieval and present our experiments. They show a clear improvement over the well-known technique of extracting frequent word pairs. 
 Second ACL Workshop on Multiword Expressions: Integrating Processing, July 2004, pp. 96-103  
The goal of this paper is to explore some consequences of the dichotomy between competence and performance from the point of view of incrementality. We introduce a TAG–based formalism that encodes a strong notion of incrementality directly into the operations of the formal system. A left-associative operation is used to build a lexicon of extended elementary trees. Extended elementary trees allow derivations in which a single fully connected structure is mantained through the course of a leftto-right word-by-word derivation. In the paper, we describe the consequences of this view for semantic interpretation, and we also evaluate some of the computational consequences of enlarging the lexicon in this way. 
This paper proposes a method for evaluating the validity of partial parse trees constructed in incremental parsing. Our method is based on stochastic incremental parsing, and it incrementally evaluates the validity for each partial parse tree on a wordby-word basis. In our method, incremental parser returns partial parse trees at the point where the validity for the partial parse tree becomes greater than a threshold. Our technique is effective for improving the accuracy of incremental parsing. 
We present a general architecture for incremental interaction between modules in a speech-tointention continuous understanding dialogue system. This architecture is then instantiated in the form of an incremental parser which receives suitability feedback on NP constituents from a reference resolution module. Oracle results indicate that perfect NP suitability judgments can provide a labelled-bracket error reduction of as much as 42% and an efﬁciency improvement of 30%. Preliminary experiments in which the parser incorporates feedback judgments based on the set of referents found in the discourse context achieve a maximum error reduction of 9.3% and efﬁciency gain of 4.6%. The parser is also able to incrementally instantiate the semantics of underspeciﬁed pronouns based on matches from the discourse context. These results suggest that the architecture holds promise as a platform for incremental parsing supporting continuous understanding. 
To support incremental interpretation, any model of human sentence processing must not only process the sentence incrementally, it must to some degree restrict the number of analyses which it produces for any sentence preﬁx. Deterministic parsing takes the extreme position that there can only be one analysis for any sentence preﬁx. Experiments with an incremental statistical parser show that performance is severely degraded when the search for the most probable parse is pruned to only the most probable analysis after each preﬁx. One method which has been extensively used to address the diﬃculty of deterministic parsing is lookahead, where information about a bounded number of subsequent words is used to decide which analyses to pursue. We simulate the eﬀects of lookahead by summing probabilities over possible parses for the lookahead words and using this sum to choose which parse to pursue. We ﬁnd that a large improvement is achieved with one word lookahead, but that more lookahead results in relatively small additional improvements. This suggests that one word lookahead is suﬃcient, but that other modiﬁcations to our left-corner parsing model could make deterministic parsing more eﬀective. 
We deﬁne a new learning task, minimum average lookahead grammar induction, with strong potential implications for incremental parsing in NLP and cognitive models. Our thesis is that a suitable learning bias for grammar induction is to minimize the degree of lookahead required, on the underlying tenet that language evolution drove grammars to be efﬁciently parsable in incremental fashion. The input to the task is an unannotated corpus, plus a nondeterministic constraining grammar that serves as an abstract model of environmental constraints conﬁrming or rejecting potential parses. The constraining grammar typically allows ambiguity and is itself poorly suited for an incremental parsing model, since it gives rise to a high degree of nondeterminism in parsing. The learning task, then, is to induce a deterministic LR (k) grammar under which it is possible to incrementally construct one of the correct parses for each sentence in the corpus, such that the average degree of lookahead needed to do so is minimized. This is a signiﬁcantly more difﬁcult optimization problem than merely compiling LR (k) grammars, since k is not speciﬁed in advance. Clearly, na¨ıve approaches to this optimization can easily be computationally infeasible. However, by making combined use of GLR ancestor tables and incremental LR table construction methods, we obtain an O(n3 + 2m) greedy approximation algorithm for this task that is quite efﬁcient in practice. 
CDG represents a sentence’s grammatical structure as assignments of dependency relations to functional variables associated with each word in the sentence. In this paper, we describe a statistical CDG (SCDG) parser that performs parsing incrementally and evaluate it on the Wall Street Journal Penn Treebank. Using a tight integration of multiple knowledge sources, together with distance modeling and synergistic dependencies, this parser achieves a parsing accuracy comparable to several state-of-the-art context-free grammar (CFG) based statistical parsers using a dependency-based evaluation metric. Factors contributing to the SCDG parser’s performance are analyzed. 
Deterministic dependency parsing is a robust and eﬃcient approach to syntactic parsing of unrestricted natural language text. In this paper, we analyze its potential for incremental processing and conclude that strict incrementality is not achievable within this framework. However, we also show that it is possible to minimize the number of structures that require nonincremental processing by choosing an optimal parsing algorithm. This claim is substantiated with experimental evidence showing that the algorithm achieves incremental parsing for 68.9% of the input when tested on a random sample of Swedish text. When restricted to sentences that are accepted by the parser, the degree of incrementality increases to 87.9%. 
When an incremental parser gets the next word, its expectations about upcoming grammatical structures can change. When a word greatly constrains these grammatical expectations, uncertainty is reduced. This elimination of possibilities constitutes information processing work. Formalizing this notion of information processing work yields a complexity metric that predicts human repetition accuracy scores across a systematic class of linguistic phenomena, the Accessibility Hierarchy of relativizable grammatical relations. 
It is a well-known intuition that human sentence understanding works in an incremental fashion, with a seemingly constant update of the interpretation through the left-to-right processing of a string. Such intuitions are backed up by experimental evidence dating from at least as far back as Marslen-Wilson (1973), showing that under many circumstances, interpretations are indeed updated very quickly. From a parsing point of view it is interesting to consider the structure-building processes that might underlie incremental interpretation— what kinds of partial structures are built during sentence processing, and with what timecourse? 
The inherent robustness of a system might be an important prerequisite for an incremental parsing model to the eﬀect that grammaticality requirements on full sentences may be suspended or allowed to be violated transiently. However, we present additional means that allow the grammarian to model preﬁx-analyses by altering a grammar for non-incremental parsing in a controlled way. This is done by introducing underspeciﬁed dependency edges that model the expected relation between already seen and yet unseen words during parsing. Thus the basic framework of weighted constraint dependency parsing is extended by the notion of dynamic dependency parsing. 
Standard grammar formalisms are deﬁned without reﬂection of the incremental and serial nature of language processing, and incrementality must therefore be reﬂected by independently deﬁned parsing and/or generation techniques. We argue that this leads to a poor setup for modelling dialogue, with its rich speaker-hearer interaction, and instead propose context-based parsing and generation models deﬁned in terms of an inherently incremental grammar formalism (Dynamic Syntax), which allow a straightforward model of otherwise problematic dialogue phenomena such as shared utterances, ellipsis and alignment. 
We present a series of simulations of behavioral data by casting a simple parsing model in the cognitive architecture ACT-R. We show that constraints deﬁned in ACT-R, speciﬁcally those relating to activation, can account for a range of facts about human sentence processing. In doing so, we argue that resource limitation in working memory is better deﬁned as an artefact of very general and independently motivated principles of cognitive processing. 
In this paper, we present a preliminary version of COOPML, a language designed for annotating cooperative discourse. We investigate the different linguistic marks that identify and characterize the different forms of cooperativity found in written texts from FAQs, Forums and emails. 
Like speakers of any natural language, speakers of English potentially have many different word orders in which to encode a single meaning. One key factor in speakers’ use of certain non-canonical word orders in English is their ability to contribute information about syntactic and semantic discourse relations. Explicit annotation of discourse relations is a difﬁcult and subjective task. In order to measure the correlations between different word orders and various discourse relations, this project utilizes a model in which discourse relations are approximated using a set of lower-level linguistic features, which are more easily and reliably annotated than discourse relations themselves. The featural model provides statistical evidence for the claim that speakers use non-canonicals to communicate information about discourse structure. 
In this paper we describe the (annotation) tools underlying two automatic techniques to analyse the meaning and use of backward causal connectives in large Dutch newspaper corpora. With the help of these techniques, Latent Semantic Analysis and Thematic Text Analysis, the contexts of more than 14,000 connectives were studied. We will focus here on the methods of analysis and on the fairly straightforward (annotation) tools needed to perform the semantic analyses, i.e. POS-tagging, lemmatisation and a thesaurus-like thematic dictionary. 
We present discourse-level annotation of newspaper texts in German and English, as part of an ongoing project aimed at investigating information structure from a cross-linguistic perspective. Rather than annotating some speciﬁc notion of information structure, we propose a theory-neutral annotation of basic features at the levels of syntax, prosody and discourse, using treebank data as a starting point. Our discourse-level annotation scheme covers properties of discourse referents (e.g., semantic sort, delimitation, quantiﬁcation, familiarity status) and anaphoric links (coreference and bridging). We illustrate what investigations this data serves and discuss some integration issues involved in combining diﬀerent levels of stand-oﬀ annotations, created by using diﬀerent tools. 
Most research on automated categorization of documents has concentrated on the assignment of one or many categories to a whole text. However, new applications, e.g. in the area of the Semantic Web, require a richer and more ﬁne-grained annotation of documents, such as detailed thematic information about the parts of a document. Hence we investigate the automatic categorization of text segments of scientiﬁc articles with XML markup into 16 topic types from a text type structure schema. A corpus of 47 linguistic articles was provided with XML markup on different annotation layers representing text type structure, logical document structure, and grammatical categories. Six different feature extraction strategies were applied to this corpus and combined in various parametrizations in different classiﬁers. The aim was to explore the contribution of each type of information, in particular the logical structure features, to the classiﬁcation accuracy. The results suggest that some of the topic types of our hierarchy are successfully learnable, while the features from the logical structure layer had no particular impact on the results. 
Getting a machine to understand human narratives has been a classic challenge for NLP and AI. This paper proposes a new representation for the temporal structure of narratives. The representation is parsimonious, using temporal relations as surrogates for discourse relations. The narrative models, called Temporal Discourse Models, are treestructured, where nodes include abstract events interpreted as pairs of time points and where the dominance relation is expressed by temporal inclusion. Annotation examples and challenges are discussed, along with a report on progress to date in creating annotated corpora. 
This paper presents the discourse annotation followed in Cast3LB, a Spanish corpus annotated with several information sources (morphological, syntactic, semantic and coreferential) at syntactic, semantic and discourse level. 3LB annotation scheme has been developed for three languages (Spanish, Catalan and Basque). Human annotators have used a set of tagging techniques and protocols. Several tools have provided them with a friendly annotation scheme. At discourse level, anaphoric and coreference expressions are annotated. One of the most interesting contributions to this annotation scenario is the enriched anaphora resolution module that is based on the previously deﬁned semantic annotation phase to expand the discourse information and use it to suggest the correct antecedent of an anaphora to the annotator. This paper describes the relevance of the semantic tags in the discourse annotation in Spanish corpus Cast3LB and shows both levels and tools in the mentioned discourse annotation scheme. 
The GNOME corpus was created to study the discourse and semantic properties of discourse entities that affect their realization and interpretation, and particularly salience. We discuss what information was annotated and the methods we followed. 
Abstract1 In this paper, we describe how the LIDAS System (Linguistic Discourse Analysis System), a discourse parser built as an implementation of the Unified Linguistic Discourse Model (U-LDM) uses information from sentential syntax and semantics along with lexical semantic information to build the Open Right Discourse Parse Tree (DPT) that serves as a representation of the structure of the discourse (Polanyi et al., 2004; Thione 2004a,b). More specifically, we discuss how discourse segmentation, sentence-level discourse parsing, and text-level discourse parsing depend on the relationship between sentential syntax and discourse. Specific discourse rules that use syntactic information are used to identify possible attachment points and attachment relations for each Basic Discourse Unit to the DPT. 
The Penn Discourse TreeBank (PDTB) is a new resource built on top of the Penn Wall Street Journal corpus, in which discourse connectives are annotated along with their arguments. Its use of standoff annotation allows integration with a stand-off version of the Penn TreeBank (syntactic structure) and PropBank (verbs and their arguments), which adds value for both linguistic discovery and discourse modeling. Here we describe the PDTB and some experiments in linguistic discovery based on the PDTB alone, as well as on the linked PTB and PDTB corpora. 
A corpus of German newspaper commentaries has been assembled and annotated with diﬀerent information (and currently, to diﬀerent degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure. The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation. 
We describe a method for annotating spoken dialog corpora using both automatic and manual annotation. Our semi-automated method for corpus development results in a corpus combining rich semantics, discourse information and reference annotation, and allows us to explore issues relating these. 
In this paper, we introduce LiveTree, a core component of LIDAS, the Linguistic Discourse Analysis System for automatic discourse parsing with the Unified Linguistic Discourse Model (U-LDM) (X et al, 2004). LiveTree is an integrated workbench for supervised and unsupervised creation, storage and manipulation of the discourse structure of text documents under the U-LDM. The LiveTree environment provides tools for manual and automatic U-LDM segmentation and discourse parsing. Document management, grammar testing, manipulation of discourse structures and creation and editing of discourse relations are also supported. 
We report on two recent medium-scale initiatives annotating present day English corpora for animacy distinctions. We discuss the relevance of animacy for computational linguistics, specifically generation, the annotation categories used in the two studies and the interannotator reliability for one of the studies. 
The evolution of competing lexical categories is simulated within a model in which lexical outputs are organized as sequences of articulatory gestures. When exemplar-based categories compete for assignment and storage of incoming exemplars in a production/storage loop, contrast between categories spontaneously emerges and remains stable, driven by the differences in storage consistency between more contrastive and less contrastive variants. Further, when lexical outputs are biased toward use of previously produced gestures, the set of exemplars in the lexicon evolve to be derived from a small set of contrastive units used in combination, despite the absence of direct selection for contrast at the sub-lexical level. 
The paper reports on the behaviour of a Kohonen map of the mental lexicon, monitored through different phases of acquisition of the Italian verb system. Reported experiments appear to consistently reproduce emergent global ordering constraints on memory traces of inflected verb forms, developed through principles of local interactions between parallel processing neurons. 
Schwa deletion is an important issue in grapheme-to-phoneme conversion for IndoAryan languages (IAL). In this paper, we describe a syllable minimization based algorithm for dealing with this that outperforms the existing methods in terms of efficiency and accuracy. The algorithm is motivated by the fact that deletion of schwa is a diachronic and sociolinguistic phenomenon that facilitates faster communication through syllable economy. The contribution of the paper is not just a better algorithm for schwa deletion; rather we describe here a constrained optimization based framework that can partly model the evolution of languages, and hence, can be used for solving many problems in computational linguistics that call for diachronic explanations. 
Automata induction and typed feature theory are described in a uniﬁed framework for the automatic acquisition of feature-based phonotactic resources. The viability of this data-driven procedure is illustrated with examples taken from a corpus of syllable-labelled data. 
This paper describes a Bayesian procedure for unsupervised learning of phonological rules from an unlabeled corpus of training data. Like Goldsmith’s Linguistica program (Goldsmith, 2004b), whose output is taken as the starting point of this procedure, our learner returns a grammar that consists of a set of signatures, each of which consists of a set of stems and a set of sufﬁxes. Our grammars differ from Linguistica’s in that they also contain a set of phonological rules, speciﬁcally insertion, deletion and substitution rules, which permit our grammars to collapse far more words into a signature than Linguistica can. Interestingly, the choice of Bayesian prior turns out to be crucial for obtaining a learner that makes linguistically appropriate generalizations through a range of different sized training corpora. 
This paper presents an algorithm for the unsupervised learning of a simple morphology of a natural language from raw text. A generative probabilistic model is applied to segment word forms into morphs. The morphs are assumed to be generated by one of three categories, namely preﬁx, sufﬁx, or stem, and we make use of some observed asymmetries between these categories. The model learns a word structure, where words are allowed to consist of lengthy sequences of alternating stems and afﬁxes, which makes the model suitable for highly-inﬂecting languages. The ability of the algorithm to ﬁnd real morpheme boundaries is evaluated against a gold standard for both Finnish and English. In comparison with a state-of-the-art algorithm the new algorithm performs best on the Finnish data, and on roughly equal level on the English data. 
We propose a novel language-independent framework for inducing a collection of morphological inflection classes from a monolingual corpus of full form words. Our approach involves two main stages. In the first stage, we generate a large data structure of candidate inflection classes and their interrelationships. In the second stage, search and filtering techniques are applied to this data structure, to identify a select collection of "true" inflection classes of the language. We describe the basic methodology involved in both stages of our approach and present an evaluation of our baseline techniques applied to induction of major inflection classes of Spanish. The preliminary results on an initial training corpus already surpass an F1 of 0.5 against ideal Spanish inflectional morphology classes. 
This paper presents the WordFrame model, a noiserobust supervised algorithm capable of inducing morphological analyses for languages which exhibit preﬁxation, sufﬁxation, and internal vowel shifts. In combination with a na¨ive approach to sufﬁx-based morphology, this algorithm is shown to be remarkably effective across a broad range of languages, including those exhibiting inﬁxation and partial reduplication. Results are presented for over 30 languages with a median accuracy of 97.5% on test sets including both regular and irregular verbal inﬂections. Because the proposed method trains extremely well under conditions of high noise, it is an ideal candidate for use in co-training with unsupervised algorithms. 
Several computational simulations of how children solve the word segmentation problem have been proposed, but most have been applied only to a limited number of languages. One model with some experimental support uses distributional statistics of sound sequence predictability (Saffran et al. 1996). However, the experimental design does not fully specify how predictability is best measured or modeled in a simulation. Saffran et al. (1996) assume transitional probability, but Brent (1999a) claims mutual information (MI) is more appropriate. Both assume predictability is measured locally, relative to neighboring segment-pairs. This paper replicates Brent’s (1999a) mutualinformation model on a corpus of childdirected speech in Modern Greek, and introduces a variant model using a global threshold. Brent’s finding regarding the superiority of MI is confirmed; the relative performance of local comparisons and global thresholds depends on the evaluation metric. 
This paper describes Vi-xfst, a visual interface and a development environment, for developing ﬁnite state language processing applications using the Xerox Finite State Tool, xfst. Vi-xfst lets a user construct complex regular expressions via a drag-anddrop visual interface, treating simpler regular expressions as “Lego Blocks.” It also enables the visualization of the structure of the regular expression components, providing a bird’s eye view of the overall system, enabling a user to easily understand and track the structural and functional relationships among the components involved. Since the structure of a large regular expression (built in terms of other regular expressions) is now transparent, users can also interact with regular expressions at any level of detail, easily navigating among them for testing. Vi-xfst also keeps track of dependencies among the regular expressions at a very ﬁnegrained level. So when a certain regular expression is modiﬁed as a result of testing, only the dependent regular expressions are recompiled resulting in an improvement in development process time, by avoiding ﬁle level recompiles which usually causes redundant regular expression compilations. 
Although syntactic features offer more speciﬁc information about the context surrounding a target word in a Word Sense Disambiguation (WSD) task, in general, they have not distinguished themselves much above positional features such as bag-of-words. In this paper we offer two methods for increasing the recall rate when using syntactic features on the WSD task by: 1) using an algorithm for discovering in the corpus every possible syntactic feature involving a target word, and 2) using wildcards in place of the lemmas in the templates of the syntactic features. In the best experimental results on the SENSEVAL-2 data we achieved an Fmeasure of 53.1% which is well above the mean F-measure performance of ofﬁcial SENSEVAL-2 entries, of 44.2%. These results are encouraging considering that only one kind of feature is used and only a simple Support Vector Machine (SVM) running with the defaults is used for the machine learning. 
We present a question answering system that combines information at the lexical, syntactic, and semantic levels, in the process to ﬁnd and rank the candidate answer sentences. The candidate exact answers are extracted from the candidate answer sentences by means of a combination of information-extraction techniques (named entity recognition) and patterns based on logical forms. The system participated in the question answering track of TREC 2004. 
¨¥v¥v¨¡ªiWªiªi³v'§p¢f§V§p¨¨¨¨o¨¾f¬C¶¬¶¬¢f¶¤¨Ic¶¤vv¦¦¥v¨¢¦¢f¨¬½¨¦¦§p§p§p¥I£¤Æ¥II«{¹o¾£¤¨­c³}©¬v¤F½Ó«0«¨{¦²¨cd¥vc}'f¯v¨¡v«Á¹v²c¤¥I¥Iªi¦²¨¹F¨§p¡¨¤¤§Þv0¬Wfº¿¶¬¨¨¥I¥v«0W§5¥7³o¦r§pWc¨Æ«« I5¶¬0¬¬­W£Wv¢qV¶¬0§V0« £¤¨¬W¤³¨Æªi0«{ªi§ÞI¡I¬¨¥I¢f¡S0¶¤¡½¥I³¬r¡¢¦©¬¥ICc'­c«c«{¥I¬¬¢fÏ¾0¢f¡0fI¨¥I¥vÔ¨«0¥I¨¥I¦¬«{i­cI¨¶¬¨c¥I­0ª¤!¢f¶¤¢q¨¥v¤§5¥vÍI¶¤c¥I¨ªw¶¬¨5¢¦¡p¡I¬v¥I¤­0³f«cy«Ó£Wp¨¨½¥Ic¥v¢}c³´Wªi«Ô¦v¤µÆ¨¾f¨¨¯§p¡¥v«0¬¨½8v¡ªi«¦GF¡­0¥I¬c¡¥i¡¦¬¢}¹X§p«ªi¥vW§¼«0§¾D¥vI¡¡I¬}¢q'¥v¬¥I¨¯½«F¥I¥I¢f­0°²0¨7c¬­c7¬v00¥I¥I0³¨¥r¬¥Ic«¥v¥vI­cI¤«c¨{¢}0¤«¡¨¡¢f½¥I0'8¨¨I¡'§¢¦¬«»¨¹¢}v¡ª!¬§Fs¢VW³µ0¢q¨«»¥vI¶¤sW¥I¡¡¡¥C«c«Wv£@0¨0¬ªi¢}§¥¤X¥I¨£¤¬I¨¡¬µc¥vqÝ{c«0f¥s«8¥I¨¨¡«0¨¨¥v­0§5¡¬0¥v¢}C­c½¬¥I¶¬ªF«c«o¡¨¨³¥v«c0cc¶¬¥¾£W«¯7¬À¨¶¬¥C¶¬¯Æ!s¡¨C¨¥I¯¤£¤¨£¤´¥I¯§¦c³c¤¢}¨{³¥v0¨W¥­0©¢}§p«0¦¨¡¨¨¨­0f¥I¡ªi¢¦c§pÂ§¾©­07¥p«0¨IW¥I¡¢F¨v¤«Ws¢¦¤v¥v«0I§p}¡¨¦¡0¥v¡¨©¬¬­0¨¬¡²c¬oC¶¬q«5«c«y¾«Ýv¶¤¥I¢fv«c¨ªiG¢f«cc­c¥v¥Ic¥v¨ªiªi§¨¬¬¡³¡¢q¨¨¨««¡¶¬f¡¨}vf«¶è¥®®®®®ª ¹° Äa£¨Äa¥Iªi}cc¬¬Õ¢ «{«aI²cªÀ«0§5½B¨¨¨¥v³¢}¨¨«c§p§p¨§Äa£W¥I0Úè¦¦§¬¨¬çB00¥I£¤¨¢¦¥I¥I¯¨W«0«0§pc­0q«ã«0W¶¤ff¢f¥v¡uD0¢}¯¤aÎ¡¥¨¥Æ§p¥q¨ÍPìr«0¬§p½¥v«)I³Å¿eG¢ffÅ¿f¹§fI«0¦c¦¯¨³¥I¥f¡s8¨¨½ã¡0¶¬«¼¨¬¨¦¹yv§p§p¢¾«hc¤ÿfD¤¦¥I£¤¥¢¥SyÔ¬¤'¯°©¨¢f¡«0¡I«0c}¶¬¢f¨Ya«0«0¨¨I¥v¶è0²¶¬¡­c¹¤f­0f«c¨­W°vu¥°¯£c¬¼«cpc«0¾§pÕ«#¬¤Â¨Äa®f¯Á¨!Ëé0¨¦IWvé°'¤¡¥I50¬¨`¡¡v¶¤f§pÕc«0«¦¦E¨«0½df0¦§pÛo0vs§p«0v¡³}«0¡³¨½Ë¢¦vÀÁ¥I}²0ffI00²¬¥v¥Æ¾¥I¥vc¬«0¬¡¡y¨¡¢qI¡v¶¬fI«0fc«ÑªÀavÀ¥I«0½¹¹¨¨ÓI¨¬¬«0¢}«c¬¥IªpIc«c0§p¨¨cI0¯Ã¡¯Æ¤«¦§pf«00ÑÍP¨¶¤¥IWº¿¢f¥IÎ³¶¤f§p´Í¤¥v«}c¶¬ÆII¨0­0«¦²cf¿¡0¨¶¬0¥vcI¬«{§«¡Äa¢}¨©¢}¡¬¤a¥¾£¤«µ¨¥I¨«{ ¤¨¦«µ¡¥yv¨v¡Ü¦ÄË¶¬f¡¡¹³«W£¤f0W¦v«W«W¹¾¨­0f½«cÌ£Wvv¶¬¡¯¦ÖÇ«¢}f¡¡¶¤Åº¶¤««½¥® ¯¬¬cIíÆ«{F¬I¨§¾â©¥v¨'§p¯¨a­c"V¬¤0¶¬µ@ÔíÆ¦¡¬««o«cÒrWÄa¹¯ß§¤º$¬f«¨¡¦è¨#&¦¹ ª·ÜW«0%(¢¦¹ Wìr¬'¼v«0£W¬¡f¢f¯v¶¤«0¥ap¡v¶«0«0¡Å¿yvvå¹yI¡¨¥v©¦íÆ§p­0Ü«¯â©¢}¥vI¥I¬¨H£W"0¨«0Õ¸­c¶¤­0)¦7¥IiP"Fc0Õy«0¡¡í1¸§pc¥I'ÁI¡£{!£@§½·¹¥I§p¨¢ÊvW¥I«W»c³ ¶¤v7¯B¥v0¬¦¦B¨¨c¢¦ª­c¨¥vË£WW¡§¾³ IÁÂ¨00Ï¥I¬£{¡c¢¦µ«0«½ ÄË£Wf¥v'0§Á¬24Úw«c}íÆ¥Æ¬36c­0²¯´¦5Ìv¢}F¥'¶¤¡ÅÊ¥I¥I¶¤¨¶è0¹¤¤¥I¥v­c°PWc¡17f¦­c¶¬Õ«0Gf¥Xvr"Æé¤¨Ó£@é­0I¨ÅÆ8¶è½i¥vÕv«BÚa¹¤v¨¾Åå¡"ÆÛq°B9¡£¤¹ÆG¥Ic¦ÅÆ@Bc¡bd¨0'0ACÁÅV­cbªv³c¬Õ¤I«0Ûqe¨c´Û8¨¹iDEq³¡«¾G "´¥Ipr¾­0c¨«0Å¿¥I0¢}IqFHF¨0¨7¬ÅÞªX«c8PÆi¦³­0IQ¯¨¶¬«ÀW§¨c«cÄa9RX½c¡ÁÁF£¤DT¤µ¡F¨SV'I¢}«0f¨c¦UW¡¨¨¦¨¥I00¨@Y«0¤ªvcc§Ð0§pì¬Xvf¦¨¡¡Á¡IfI`I«c¡«0¬¥§FSVP´r«{«8»£cf£W¥c¹f¢f½©Æ¨¬¶¬¨¬P¯%r©§¾0§pÚè­0¨¬çB¡§p¬¦¦«0«Ã¥I¬¥v¢}¬r¢}«0¤¡«7¨¯¨5C¥I¨¥I­c«W«¾­c0¤­0¦ªi¬£{¦¨f«0v¨Ç«½¥®ª 
This paper presents an approach for question analysis that deﬁnes the question subject and its required answer type by building a triebased structure from a set of question patterns. The question analysis consists of comparing the question tokens with the path of nodes in the trie. A look-ahead process solve the mismatches of unknown words by assigning a entity-type or semantically linking them with other question words. The developed approach is evaluated using diﬀerent datasets showing that its performance is comparable with state-of-the-art systems. 
This paper presents an approach on a querybiased document summarisation and an evaluation of the use of such an approach for question answering tasks. We observed a significant difference in the user’s performance by presenting a list of documents customized to the task type, compared with a generic document summarization approach. This indicates that paying attention to the task and the searcher interaction may provide substantial improvement in task performance. 
This work presents a type of parser that takes the process of chunking to the stage of producing full parse trees. This type of parser, denoted Thin Parsers (TP) in this work has the characteristics of: following a given grammar, creating full parse trees, producing only a limited number of full parse trees, parsing in linear time of sentence length. Performance standards on the Penn Tree Bank show results slightly under that of stochastic parsers but faster performance. Various types of Thin Parsers are presented. 
Statistical parsers are extremely complex systems, yet papers describing them almost always only discuss theoretical issues instead of implementation issues. This paper attempts to address the imbalance by describing the implementation issues faced in building a state-ofthe-art statistical parser. In the process, we will describe our own implementation of a statistical parser. 
In this paper we present PENG-D, a proposal for a controlled natural language that can be used for expressing knowledge about resources in the Semantic Web and for specifying ontologies in a human-readable way. After a brief overview of the main Semantic Web enabling technologies (and their deﬁciencies), we will show how statements and rules written in PENG-D are related to (a subset of) RDFS and OWL and how this knowledge can be translated into an expressive fragment of ﬁrst-order logic. The resulting information can then be further processed by third-party reasoning services and queried in PENG-D. 
In a world where information is increasingly delivered to users via different devices with dramatically different constraints and capabilities, it is becoming crucial to consider how the presentation of information must be adapted to suit specific devices and user contexts. To avoid confusing and disorienting the users as they switch between devices, the content and structure of information should be kept constant while its presentation must be optimised for each device to ensure usability. In this paper, we distinguish between two types of decisions that must be made during the presentation planning stage of an information delivery system: local decisions, which are based only on the content or features of the node itself, and global decisions, which are based on the entire structure of the discourse tree. We present a generic algorithm for making global decisions, driven by the discourse tree structure and contextual characteristics. 
One of the most widely explored issues in natural language generation is the generation of referring expressions (gre): given an entity we want to refer to, how do we work out the content of a referring expression that uniquely identiﬁes the intended referent? Over the last 15 years, a number of authors have proposed a wide range of algorithms for addressing diﬀerent aspects of this problem, but the diﬀerent approaches taken have made it very diﬃcult to compare and contrast the algorithms provided in any meaningful way. In this paper, we propose a characterisation of the problem of referring expression generation as a search problem; this allows us to recast existing algorithms in a way that makes their similarities and diﬀerences clear. 
Applying CCG to domains outside of linguistics could require different sets of combinators to be developed for each domain. The meta-grammar described in this paper aims to assist such development by enabling simple, succinct expression of both existing and new combinator definitions. It favours the development of an easily-configurable, onetime-coded module that can perform CCG combinations for any combinator set of the researcher’s choosing. A preliminary implementation shows both the feasibility and potential of the meta-grammar. 
We describe the framework for an intelligent multimedia presentation system we designed to be part of the FOCAL laboratory, a semi-immersive environment for Command and Control Environment. FOCAL comprises a number of input devices and output media, animated virtual conversational characters, a spoken dialogue system, and sophisticated visual displays. These need to be coordinated to provide a useful and effective presentation to the user. In this paper, we describe the principles which underlie intelligent multimedia presentation (IMMP) systems and the design of such a system within the FOCAL multiagent architecture. 
Systemic features use linguisticallyderived language models as a basis for text classiﬁcation. The graph structure of these models allows for feature representations not available with traditional bag-of-words approaches. This paper explores the set of possible representations, and proposes feature selection methods that aim to produce the most compact and effective set of attributes for a given classiﬁcation problem. We show that small sets of systemic features can outperform larger sets of wordbased features in the task of identifying ﬁnancial scam documents. 
We discuss the data sources available for utterance disambiguation in a bilingual dialogue system, distinguishing global, contextual, and user-speciﬁc domains, and syntactic and semantic levels. We propose a framework for combining the available information, and techniques for increasing a stochastic grammar’s sensitivity to local context and a speaker’s idiolect. 
This paper investigates the application of Maximum Entropy Markov Models to semantic role labelling. Syntactic chunks are labelled according to the semantic role they ﬁll for sentence verb predicates. The model is trained on the subset of Propbank data provided for the Conference on Computational Natural Language Learning 2004. Good precision is achieved, which is of key importance for information extraction from large corpora containing redundant data, and for generalising systems beyond task speciﬁc, hand coded template methods. 
The identiﬁcation of semantically related terms for a given word is an important problem. A number of statistical approaches have been proposed to address this problem. Most approaches draw their statistics from a large general corpus. In this paper, we propose to use specialized corpora which focus strongly on the individual words of interest. We propose to collect such corpora through targeted queries to Internet search engines. Furthermore, we introduce a new statistical measure, Relative Frequency Ratio,tailored speciﬁcally for such specialized corpora. We evaluated our approach by using the extracted related terms to attack the target word selection problem in machine translation. This type of indirect evaluation is conducted because a direct evaluation on the set of related terms thus extracted relies heavily on direct human involvement and is not quantitatively comparable to others’ results. Our experimental results so far are very encouraging. 
Linguistic forms are inherently multi-dimensional. They exhibit a variety of phonological, orthographic, morphosyntactic, semantic and pragmatic properties. Accordingly, linguistic analysis involves multidimensional exploration, a process in which the same collection of forms is laid out in many ways until clear patterns emerge. Equally, language documentation usually contains tabulations of linguistic forms to illustrate systematic patterns and variations. In all such cases, multi-dimensional data is projected onto a two-dimensional table known as a linguistic paradigm, the most widespread format for linguistic data presentation. In this paper we develop an XML data model for linguistic paradigms, and show how XSL transforms can render them. We describe a high-level interface which gives linguists ﬂexible, high-level control of paradigm layout. The work provides a simple, general, and extensible model for the preservation and access of linguistic data. 
This paper describes a tool for acquiring unknown words, which operates in a bilingual human-machine dialogue system. When the user’s utterance includes a word which is not in the system’s lexicon, the system initiates a subdialogue to ﬁnd out about the new word, by querying the user about the syntactic validity of a number of example sentences generated automatically from the grammar’s test suite. The tool can handle multiple unknown words, regular morphology and translation of new words within a very complex uniﬁcation feature structure type hierarchy. 
Language technology makes extensive use of hierarchically annotated text and speech data. These databases are stored in ﬂat ﬁles and manipulated using corpus-speciﬁc query tools or special-purpose scripts. While the size of these databases and the range of applications has grown rapidly in recent years, neither method for managing the data has led to reusable, scalable software. The formal properties of the query languages are not well understood. Hence established methods for indexing tree data and optimizing tree queries cannot be employed. We analyze a range of existing linguistic query languages, and adduce a set of requirements for a reusable, scalable linguistic query language. 
Systemic functional linguistics offers a grammar that is semantically organised, so that salient grammatical choices are made explicit. This paper describes the explication of these choices through the conversion of the Penn Treebank into a systemic functional grammar corpus. Developing such a resource can help connect work in natural language processing to a signiﬁcant body of research dealing explicitly with the issue of how lexical and grammatical selections create meaning. 
Selectional preferences are a source of linguistic information commonly applied to the task of Word Sense Disambiguation (WSD). To date, WSD systems using selectional preferences as the main disambiguation mechanism have achieved limited success. One possible reason for this limitation is the limited number of semantic roles used in the construction of selectional preferences. This study investigates whether better performance can be achieved using the current state-of-art semantic role labelling systems, and explores alternative ways of applying selectional preferences for WSD. In this study, WordNet noun synonym sets and hypernym sets were used in the construction of selectional preferences; Semcor2.0 data was used for the training and evaluation of a support vector machine classiﬁer and a Naive Bayes classiﬁer. 
Abstract A verb particle construction (VPC) classification scheme gleaned from linguistic sources has been used to assess its usefulness for identifying issues in decomposability. Linguistic sources have also been used to inform the features suitable for use in building an automatic classifier for the scheme with a series of good performance results. The notions of how to define the task of computing phrasal verbs are discussed and new proposals are presented. 
TT2 is an innovative tool for speeding up and facilitating the work of translators by automatically suggesting translation completions. Different versions of the system are being developed for English, French, Spanish and German by an international team of researchers from Europe and Canada. Two professional translation agencies are currently evaluating successive prototypes. 
This paper proposes an approach to improve word alignment in a specific domain, in which only a small-scale domain-specific corpus is available, by adapting the word alignment information in the general domain to the specific domain. This approach first trains two statistical word alignment models with the large-scale corpus in the general domain and the small-scale corpus in the specific domain respectively, and then improves the domain-specific word alignment with these two models. Experimental results show a significant improvement in terms of both alignment precision and recall. And the alignment results are applied in a computer assisted translation system to improve human translation efficiency. 
 Preparing a lexicon composed of transliterated term  This paper proposes a novel approach to automating pairs is time- and labor-intensive. Constructing such  the construction of transliterated-term lexicons. A a lexicon automatically is the most important goal of  simple syllable alignment algorithm is used to this paper. The problem is how to collect  construct confusion matrices for cross-language transliterated-term pairs from text resources.  syllable-phoneme conversion. Each row in the confusion matrix consists of a set of syllables in the source language that are (correctly or erroneously) matched phonetically and statistically to a syllable in the target language. Two conversions using phoneme-to-phoneme and text-to-phoneme syllabification algorithms are automatically deduced from a training corpus of paired terms and are used to calculate the degree of similarity between phonemes for transliterated-term extraction. In a  Query logs recorded by Internet search engines reveal users' intentions and contain much information about users' behaviors. (Brill, 2001) proposed an interactive process that used query logs for extracting English-Japanese transliterated-terms. Under this method, a large initial number of term pairs were compiled manually. It is time-consuming to prepare such an initial training set, and the resource used is not publicly accessible.  large-scale experiment using this automated learning The Internet is one of the largest distributed  process for conversions, more than 200,000 databases in the world. It comprises various kinds of  transliterated-term pairs were successfully extracted data and at the same time is growing rapidly. Though  by analyzing query results from Internet search the World Wide Web is not systematically organized,  engines. Experimental results indicate the proposed much invaluable information can be obtained from  approach shows promise in transliterated-term this large text corpus. Many researchers dealing with  extraction.  natural language processing, machine translation,  and information retrieval have focused on exploiting  
This paper describes a database of translation memory, TotalRecall, developed to encourage authentic and idiomatic use in second language writing. TotalRecall is a bilingual concordancer that support search query in English or Chinese for relevant sentences and translations. Although initially intended for learners of English as Foreign Language (EFL) in Taiwan, it is a gold mine of texts in English or Mandarin Chinese. TotalRecall is particularly useful for those who write in or translate into a foreign language. We exploited and structured existing high-quality translations from bilingual corpora from a Taiwan-based Sinorama Magazine and Official Records of Hong Kong Legislative Council to build a bilingual concordance. Novel approaches were taken to provide highprecision bilingual alignment on the subsentential and lexical levels. A browserbased user interface was developed for ease of access over the Internet. Users can search for word, phrase or expression in English or Mandarin. The Web-based user interface facilitates the recording of the user actions to provide data for further research. 
Recent research in cross-lingual information retrieval (CLIR) established the need for properly matching the parallel corpus used for query translation to the target corpus. We propose a document-level approach to solving this problem: building a custom-made parallel corpus by automatically assembling it from documents taken from other parallel corpora. Although the general idea can be applied to any application that uses parallel corpora, we present results for CLIR in the medical domain. In order to extract the bestmatched documents from several parallel corpora, we propose ranking individual documents by using a length-normalized Okapi-based similarity score between them and the target corpus. This ranking allows us to discard 50-90% of the training data, while avoiding the performance drop caused by a good but mismatched resource, and even improving CLIR effectiveness by 4-7% when compared to using all available training data. 
Numerous cross-lingual applications, including state-of-the-art machine translation systems, require parallel texts aligned at the sentence level. However, collections of such texts are often polluted by pairs of texts that are comparable but not parallel. Bitext maps can help to discriminate between parallel and comparable texts. Bitext mapping algorithms use a larger set of document features than competing approaches to this task, resulting in higher accuracy. In addition, good bitext mapping algorithms are not limited to documents with structural mark-up such as web pages. The task of ﬁltering non-parallel text pairs represents a new application of bitext mapping algorithms. 
We propose a novel method for inducing monolingual semantic hierarchies and sense clusters from numerous foreign-language-to-English bilingual dictionaries. The method exploits patterns of non-transitivity in translations across multiple languages. No complex or hierarchical structure is assumed or used in the input dictionaries: each is initially parsed into the “lowest common denominator” form, which is to say, a list of pairs of the form (foreign word, English word). We then propose a monolingual synonymy measure derived from this aggregate resource, which is used to derive multilinguallymotivated sense hierarchies for monolingual English words, with potential applications in word sense classiﬁcation, lexicography and statistical machine translation. 
The manual design of grammars for accurate natural language analysis is an iterative process; while modelling decisions usually determine parser behaviour, evidence from analysing more or different input can suggest unforeseen regularities, which leads to a reformulation of rules, or even to a different model of previously analysed phenomena. We describe an implementation of Weighted Constraint Dependency Grammar that supports the grammar writer by providing display, automatic analysis, and diagnosis of dependency analyses and allows the direct exploration of alternative analyses and their status under the current grammar. 
The previous probabilistic part-of-speech tagging models for agglutinative languages have considered only lexical forms of morphemes, not surface forms of words. This causes an inaccurate calculation of the probability. The proposed model is based on the observation that when there exist words (surface forms) that share the same lexical forms, the probabilities to appear are different from each other. Also, it is designed to consider lexical form of word. By experiments, we show that the proposed model outperforms the bigram Hidden Markov model (HMM)-based tagging model. 
We describe two experiments using French noun-noun combinations which parallel a study carried out by Gagné (2001) using English combinations. The order of the modifier and head noun are reversed in French, allowing us to investigate whether the influence of relation priming that Gagné found is due to the order of the modifier and head noun or whether it is due to their different functional roles. While our findings indicate that interpretation is influenced by previous exposure to combinations incorporating one of the same constituent nouns, the results show that primes with the same modifier have a greater influence when associated with a different relation to the target. This pattern of influence is similar to that found in English and suggests that the modifier is exclusively involved in relation selection, irrespective of its order in a combination. 
This paper refers to part of our research in the area of automatic acquisition of computational lexicon information from corpus. The present paper reports the ongoing research on corpus representativeness. For the task of inducing information out of text, we wanted to fix a certain degree of confidence on the size and composition of the collection of documents to be observed. The results show that it is possible to work with a relatively small corpus of texts if it is tuned to a particular domain. Even more, it seems that a small tuned corpus will be more informative for real parsing than a general corpus. 
We present an improved method for automated word alignment of parallel texts which takes advantage of knowledge of syntactic divergences, while avoiding the need for syntactic analysis of the less resource rich language, and retaining the robustness of syntactically agnostic approaches such as the IBM word alignment models. We achieve this by using simple, easily-elicited knowledge to produce syntaxbased heuristics which transform the target language (e.g. English) into a form more closely resembling the source language, and then by using standard alignment methods to align the transformed bitext. We present experimental results under variable resource conditions. The method improves word alignment performance for language pairs such as English-Korean and English-Hindi, which exhibit longer-distance syntactic divergences. 
In this paper, we propose a method of automatically extracting word hierarchies based on the inclusion relation of appearance patterns from corpora. We apply a complementary similarity measure to find a hierarchical word structure. This similarity measure was developed for the recognition of degraded machineprinted text in the field and can be applied to estimate one-to-many relations. Our purpose is to extract word hierarchies from corpora automatically. As the initial task, we attempt to extract hierarchies of abstract nouns cooccurring with adjectives in Japanese and compare with hierarchies in the EDR electronic dictionary. 
For biomedical information extraction, most systems use syntactic patterns on verbs (anchor verbs) and their arguments. Anchor verbs can be selected by focusing on their arguments. We propose to use predicate-argument structures (PASs), which are outputs of a full parser, to obtain verbs and their arguments. In this paper, we evaluated PAS method by comparing it to a method using part of speech (POSs) pattern matching. POS patterns produced larger results with incorrect arguments, and the results will cause adverse effects on a phase selecting appropriate verbs. 
This paper attempts to analyze and bound the utility of various structured and unstructured resources in Question Answering, independent of a speciﬁc system or component. We quantify the degree to which gazetteers, web resources, encyclopedia, web documents and web-based query expansion can help Question Answering in general and speciﬁc question types in particular. Depending on which resources are used, the QA task may shift from complex answer-ﬁnding mechanisms to simpler data extraction methods followed by answer re-mapping in local documents. 
In this paper, we describe TANGO as a collocational concordancer for looking up collocations. The system was designed to answer user’s query of bilingual collocational usage for nouns, verbs and adjectives. We first obtained collocations from the large monolingual British National Corpus (BNC). Subsequently, we identified collocation instances and translation counterparts in the bilingual corpus such as Sinorama Parallel Corpus (SPC) by exploiting the wordalignment technique. The main goal of the concordancer is to provide the user with a reference tools for correct collocation use so as to assist second language learners to acquire the most eminent characteristic of native-like writing. 
This paper presents an innovative unsupervised method for automatic sentence extraction using graphbased ranking algorithms. We evaluate the method in the context of a text summarization task, and show that the results obtained compare favorably with previously published results on established benchmarks. 
A number of NLP tasks have been effectively modeled as classiﬁcation tasks using a variety of classiﬁcation techniques. Most of these tasks have been pursued in isolation with the classiﬁer assuming unambiguous input. In order for these techniques to be more broadly applicable, they need to be extended to apply on weighted packed representations of ambiguous input. One approach for achieving this is to represent the classiﬁcation model as a weighted ﬁnite-state transducer (WFST). In this paper, we present a compilation procedure to convert the rules resulting from an AdaBoost classiﬁer into an WFST. We validate the compilation technique by applying the resulting WFST on a call-routing application. 
Extracting semantic relationships between entities is challenging because of a paucity of annotated data and the errors induced by entity detection modules. We employ Maximum Entropy models to combine diverse lexical, syntactic and semantic features derived from the text. Our system obtained competitive results in the Automatic Content Extraction (ACE) evaluation. Here we present our general approach and describe our ACE results. 
Although they can be topologically different, two distinct transducers may actually recognize the same rational relation. Being able to test the equivalence of transducers allows to implement such operations as incremental minimization and iterative composition. This paper presents an algorithm for testing the equivalence of deterministic weighted ﬁnite-state transducers, and outlines an implementation of its applications in a prototype weighted ﬁnite-state calculus tool. Introduction The addition of weights in ﬁnite-state devices (where transitions, initial states and ﬁnal states are weighted) introduced the need to reevaluate many of the techniques and algorithms used in classical ﬁnite-state calculus. Interesting consequences are, for instance, that not all non-deterministic weighted automata can be made deterministic (Buchsbaum et al., 2000); or that epsilon transitions may offset the weights in the result of the composition of two transducers (Pereira and Riley, 1997). A fundamental operation on ﬁnite-state transducers in equivalence testing, which leads to applications such as incremental minimization and iterative composition. Here, we present an algorithm for equivalence testing in the weighted case, and describe its application to these applications. We also describe a prototype implementation, which is demonstrated. 
We deﬁne a new feature selection score for text classiﬁcation based on the KL-divergence between the distribution of words in training documents and their classes. The score favors words that have a similar distribution in documents of the same class but different distributions in documents of different classes. Experiments on two standard data sets indicate that the new method outperforms mutual information, especially for smaller categories.  
This paper reports experiments in classifying texts based upon their favorability towards the subject of the text using a feature set enriched with topic information on a small dataset of music reviews hand-annotated for topic. The results of these experiments suggest ways in which incorporating topic information into such models may yield improvement over models which do not use topic information. 
Recent studies in word sense induction are based on clustering global co-occurrence vectors, i.e. vectors that reflect the overall behavior of a word in a corpus. If a word is semantically ambiguous, this means that these vectors are mixtures of all its senses. Inducing a word’s senses therefore involves the difficult problem of recovering the sense vectors from the mixtures. In this paper we argue that the demixing problem can be avoided since the contextual behavior of the senses is directly observable in the form of the local contexts of a word. From human disambiguation performance we know that the context of a word is usually sufficient to determine its sense. Based on this observation we describe an algorithm that discovers the different senses of an ambiguous word by clustering its contexts. The main difficulty with this approach, namely the problem of data sparseness, could be minimized by looking at only the three main dimensions of the context matrices. 
This paper talks about the deciding practical sense boundary of homonymous words. The important problem in dictionaries or thesauri is the confusion of the sense boundary by each resource. This also becomes a bottleneck in the practical language processing systems. This paper proposes the method about discovering sense boundary using the collocation from the large corpora and the clustering methods. In the experiments, the proposed methods show the similar results with the sense boundary from a corpus-based dictionary and sense-tagged corpus. 
Natural Language Processing applications often require large amounts of annotated training data, which are expensive to obtain. In this paper we investigate the applicability of Co-training to train classifiers that predict emotions in spoken dialogues. In order to do so, we have first applied the wrapper approach with Forward Selection and Naïve Bayes, to reduce the dimensionality of our feature set. Our results show that Co-training can be highly effective when a good set of features are chosen. 
We present the ﬁnal MIAMM system, a multimodal dialogue system that employs speech, haptic interaction and novel techniques of information visualization to allow a natural and fast access to large multimedia databases on small handheld devices.  
We describe an extension of the Wysiwym technology for knowledge editing through natural language feedback. Previous applications have addressed relatively simple tasks requiring a very limited range of nominal and clause patterns. We show that by adding a further editing operation called reconﬁguration, the technology can achieve a far wider coverage more in line with other general-purpose generators. The extension will be included in a Java-based library package for producing Wysiwym applications. 
The Natural Language Toolkit is a suite of program modules, data sets, tutorials and exercises, covering symbolic and statistical natural language processing. NLTK is written in Python and distributed under the GPL open source license. Over the past three years, NLTK has become popular in teaching and research. We describe the toolkit and report on its current state of development. 
We present the ﬁrst version of a new declarative programming language. Dyna has many uses but was designed especially for rapid development of new statistical NLP systems. A Dyna program is a small set of equations, resembling Prolog inference rules, that specify the abstract structure of a dynamic programming algorithm. It compiles into efﬁcient, portable, C++ classes that can be easily invoked from a larger application. By default, these classes run a generalization of agendabased parsing, prioritizing the partial parses by some ﬁgure of merit. The classes can also perform an exact backward (outside) pass in the service of parameter training. The compiler already knows several implementation tricks, algorithmic transforms, and numerical optimization techniques. It will acquire more over time: we intend for it to generalize and encapsulate best practices, and serve as a testbed for new practices. Dyna is now being used for parsing, machine translation, morphological analysis, grammar induction, and ﬁnite-state modeling. 
Multimodal interfaces provide more ﬂexible and compelling interaction and can enable public information kiosks to support more complex tasks for a broader community of users. MATCHKiosk is a multimodal interactive city guide which provides users with the freedom to interact using speech, pen, touch or multimodal inputs. The system responds by generating multimodal presentations that synchronize synthetic speech with a life-like virtual agent and dynamically generated graphics. 
We introduce two novel methods of text categorization in which documents are split into fragments. We conducted experiments on English, French and Czech. In all cases, the problems referred to a binary document classiﬁcation. We ﬁnd that both methods increase the accuracy of text categorization. For the Na¨ıve Bayes classiﬁer this increase is signiﬁcant. 
This paper introduces new specificity determining methods for terms using compositional and contextual information. Specificity of terms is the quantity of domain specific information that is contained in the terms. The methods are modeled as information theory like measures. As the methods don’t use domain specific information, they can be applied to other domains without extra processes. Experiments showed very promising result with the precision of 82.0% when the methods were applied to the terms in MeSH thesaurus.  1. Introduction  Terminology management concerns primarily with terms, i.e., the words that are assigned to concepts used in domain-related texts. A term is a meaningful unit that represents a specific concept within a domain (Wright, 1997). Specificity of a term represents the quantity of domain specific information contained in the term. If a term has large quantity of domain specific information, specificity value of the term is large; otherwise specificity value of the term is small. Specificity of term X is quantified to positive real number as equation (1).  Spec( X ) ∈ R+  (1)  Specificity of terms is an important necessary condition in term hierarchy, i.e., if X1 is one of ancestors of X2, then Spec(X1) is less than Spec(X2). Specificity can be applied in automatic construction and evaluation of term hierarchy.  When domain specific concepts are represented as terms, the terms are classified into two categories based on composition of unit words. In the first category, new terms are created by adding modifiers to existing terms. For example “insulin-dependent diabetes mellitus” was created by adding modifier “insulin-dependent” to its hypernym “diabetes mellitus” as in Table 1. In English, the specific level terms are very commonly compounds of the generic level term and some modifier (Croft, 2004). In this case, compositional information is important to get their meaning. In the second category, new terms are created independently to existing terms. For example, “wolfram syndrome” is semantically related to its ancestor terms as in Table 1. But it shares no common words with its ancestor terms. In this case, contextual information is used to discriminate the features of the terms.  Node Number  Terms  C18.452.297 C18.452.297.267  diabetes mellitus insulin-dependent diabetes mellitus  C18.452.297.267.960 wolfram syndrome Table 1. Subtree of MeSH1 tree. Node numbers represent hierarchical structure of terms  Contextual information has been mainly used to represent the characteristics of terms. (Caraballo, 1999A) (Grefenstette, 1994) (Hearst, 1992) (Pereira, 1993) and (Sanderson, 1999) used contextual information to find hyponymy relation between terms. (Caraballo, 1999B) also used contextual information to determine the specificity of nouns. Contrary, compositional information of terms has not been commonly discussed.  
Dialog participants in a non-mixed initiative dialogs, in which one participant asks questions exclusively and the other participant responds to those questions exclusively, can select actions that minimize the expected length of the dialog. The choice of question that minimizes the expected number of questions to be asked can be computed in polynomial time in some cases. The polynomial-time solutions to special cases of the problem suggest a number of strategies for selecting dialog actions in the intractable general case. In a simulation involving 1000 dialog scenarios, an approximate solution using the most probable rule set/least probable question resulted in expected dialog length of 3.60 questions per dialog, as compared to 2.80 for the optimal case, and 5.05 for a randomly chosen strategy. 
We describe an original method that automatically ﬁnds speciﬁc topics in a large collection of texts. Each topic is ﬁrst identiﬁed as a speciﬁc cluster of texts and then represented as a virtual concept, which is a weighted mixture of words. Our intention is to employ these virtual concepts in document indexing. In this paper we show some preliminary experimental results and discuss directions of future work. 
This paper describes several ongoing projects that are united by the theme of changes in lexical use over time. We show that paying attention to a document’s temporal context can lead to improvements in information retrieval and text categorization. We also explore a potential application in document clustering that is based upon different types of lexical changes. 
We present a novel approach for automatically acquiring English topic signatures. Given a particular concept, or word sense, a topic signature is a set of words that tend to co-occur with it. Topic signatures can be useful in a number of Natural Language Processing (NLP) applications, such as Word Sense Disambiguation (WSD) and Text Summarisation. Our method takes advantage of the different way in which word senses are lexicalised in English and Chinese, and also exploits the large amount of Chinese text available in corpora and on the Web. We evaluated the topic signatures on a WSD task, where we trained a second-order vector cooccurrence algorithm on standard WSD datasets, with promising results. 
Paraphrase recognition is used in a number of applications such as tutoring systems, question answering systems, and information retrieval systems. The context of our research is the iSTART reading strategy trainer for science texts, which needs to understand and recognize the trainee’s input and respond appropriately. This paper describes the motivation for paraphrase recognition and develops a definition of the strategy as well as a recognition model for paraphrasing. Lastly, we discuss our preliminary implementation and research plan. 
We present experiments aiming at an automatic classiﬁcation of Spanish verbs into lexical semantic classes. We apply well-known techniques that have been developed for the English language to Spanish, proving that empirical methods can be re-used through languages without substantial changes in the methodology. Our results on subcategorisation acquisition compare favourably to the state of the art for English. For the verb classiﬁcation task, we use a hierarchical clustering algorithm, and we compare the output clusters to a manually constructed classiﬁcation. 
This paper presents a method of improving the accuracy of subcategorization frames (SCFs) acquired from corpora to augment existing lexicon resources. I estimate a conﬁdence value of each SCF using corpus-based statistics, and then perform clustering of SCF conﬁdencevalue vectors for words to capture cooccurrence tendency among SCFs in the lexicon. I apply my method to SCFs acquired from corpora using lexicons of two large-scale lexicalized grammars. The resulting SCFs achieve higher precision and recall compared to SCFs obtained by naive frequency cut-off. 
This paper describes a Verb Phrase Ellipsis (VPE) detection system, built for robustness, accuracy and domain independence. The system is corpus-based, and uses machine learning techniques on free text that has been automatically parsed. Tested on a mixed corpus comprising a range of genres, the system achieves a 70% F1-score. This system is designed as the ﬁrst stage of a complete VPE resolution system that is input free text, detects VPEs, and proceeds to ﬁnd the antecedents and resolve them. 
This paper presents a novel ensemble learning approach to resolving German pronouns. Boosting, the method in question, combines the moderately accurate hypotheses of several classiﬁers to form a highly accurate one. Experiments show that this approach is superior to a single decision-tree classiﬁer. Furthermore, we present a standalone system that resolves pronouns in unannotated text by using a fully automatic sequence of preprocessing modules that mimics the manual annotation process. Although the system performs well within a limited textual domain, further research is needed to make it effective for open-domain question answering and text summarisation. 
The Hidden Markov Model (HMM) for part-of-speech (POS) tagging is typically based on tag trigrams. As such it models local context but not global context, leaving long-distance syntactic relations unrepresented. Using n-gram models for n > 3 in order to incorporate global context is problematic as the tag sequences corresponding to higher order models will become increasingly rare in training data, leading to incorrect estimations of their probabilities. The trigram HMM can be extended with global contextual information, without making the model infeasible, by incorporating the context separately from the POS tags. The new information incorporated in the model is acquired through the use of a wide-coverage parser. The model is trained and tested on Dutch text from two different sources, showing an increase in tagging accuracy compared to tagging using the standard model. 
This paper presents a framework for unsupervised natural language morphology induction wherein candidate suffixes are grouped into candidate inflection classes, which are then arranged in a lattice structure. With similar candidate inflection classes placed near one another in the lattice, I propose this structure is an ideal search space in which to isolate the true inflection classes of a language. This paper discusses and motivates possible search strategies over the inflection class lattice structure. 
In a multimodal conversation, the way users communicate with a system depends on the available interaction channels and the situated context (e.g., conversation focus, visual feedback). These dependencies form a rich set of constraints from various perspectives such as temporal alignments between different modalities, coherence of conversation, and the domain semantics. There is strong evidence that competition and ranking of these constraints is important to achieve an optimal interpretation. Thus, we have developed an optimization approach for multimodal interpretation, particularly for interpreting multimodal references. A preliminary evaluation indicates the effectiveness of this approach, especially for complex user inputs that involve multiple referring expressions in a speech utterance and multiple gestures. 
The paper reports on progress in building computational models of a constructivist approach to language development. It introduces a formalism for construction grammars and learning strategies based on invention, abduction, and induction. Examples are drawn from experiments exercising the model in situated language games played by embodied artiﬁcial agents. 
Discourse in formal domains, such as mathematics, is characterized by a mixture of telegraphic natural language and embedded (semi-)formal symbolic mathematical expressions. We present language phenomena observed in a corpus of dialogs with a simulated tutorial system for proving theorems as evidence for the need for deep syntactic and semantic analysis. We propose an approach to input understanding in this setting. Our goal is a uniform analysis of inputs of different degree of verbalization: ranging from symbolic alone to fully worded mathematical expressions. 
This paper describes a noisy channel model of speech repairs, which can identify and correct repairs in speech transcripts. A syntactic parser is used as the source model, and a novel type of TAG-based transducer is the channel model. The use of TAG is motivated by the intuition that the reparandum is a “rough copy” of the repair. The model is trained and tested on the Switchboard disﬂuency-annotated corpus. 
We present a technique that improves the efﬁciency of word-lattice parsing as used in speech recognition language modeling. Our technique applies a probabilistic parser iteratively where on each iteration it focuses on a different subset of the wordlattice. The parser’s attention is shifted towards word-lattice subsets for which there are few or no syntactic analyses posited. This attention-shifting technique provides a six-times increase in speed (measured as the number of parser analyses evaluated) while performing equivalently when used as the ﬁrst-stage of a multi-stage parsing-based language model. 
This paper describes discriminative language modeling for a large vocabulary speech recognition task. We contrast two parameter estimation methods: the perceptron algorithm, and a method based on conditional random ﬁelds (CRFs). The models are encoded as deterministic weighted ﬁnite state automata, and are applied by intersecting the automata with word-lattices that are the output from a baseline recognizer. The perceptron algorithm has the beneﬁt of automatically selecting a relatively small feature set in just a couple of passes over the training data. However, using the feature set output from the perceptron algorithm (initialized with their weights), CRF training provides an additional 0.5% reduction in word error rate, for a total 1.8% absolute reduction from the baseline of 39.2%. 
Traditional concatenative speech synthesis systems use a number of heuristics to deﬁne the target and concatenation costs, essential for the design of the unit selection component. In contrast to these approaches, we introduce a general statistical modeling framework for unit selection inspired by automatic speech recognition. Given appropriate data, techniques based on that framework can result in a more accurate unit selection, thereby improving the general quality of a speech synthesizer. They can also lead to a more modular and a substantially more efﬁcient system. We present a new unit selection system based on statistical modeling. To overcome the original absence of data, we use an existing high-quality unit selection system to generate a corpus of unit sequences. We show that the concatenation cost can be accurately estimated from this corpus using a statistical n-gram language model over units. We used weighted automata and transducers for the representation of the components of the system and designed a new and more efﬁcient composition algorithm making use of string potentials for their combination. The resulting statistical unit selection is shown to be about 2.6 times faster than the last release of the AT&T Natural Voices Product while preserving the same quality, and offers much ﬂexibility for the use and integration of new and more complex components. 
In this paper, we describe a new methodology to develop mixed-initiative spoken dialog systems, which is based on the extensive use of simulations to accelerate the development process. With the help of simulations, a system providing information about a database of nearly 1000 restaurants in the Boston area has been developed. The simulator can produce thousands of unique dialogs which beneﬁt not only dialog development but also provide data to train the speech recognizer and understanding components, in preparation for real user interactions. Also described is a strategy for creating cooperative responses to user queries, incorporating an intelligent language generation capability that produces content-dependent verbal descriptions of listed items. 
We present a prototype natural-language problem-solving application for a financial services call center, developed as part of the Amitiés multilingual human-computer dialogue project. Our automated dialogue system, based on empirical evidence from real call-center conversations, features a datadriven approach that allows for mixed system/customer initiative and spontaneous conversation. Preliminary evaluation results indicate efficient dialogues and high user satisfaction, with performance comparable to or better than that of current conversational travel information systems. 
A challenging problem for spoken dialog systems is the design of utterance generation modules that are fast, ﬂexible and general, yet produce high quality output in particular domains. A promising approach is trainable generation, which uses general-purpose linguistic knowledge automatically adapted to the application domain. This paper presents a trainable sentence planner for the MATCH dialog system. We show that trainable sentence planning can produce output comparable to that of MATCH’s template-based generator even for quite complex information presentations. 
This paper describes the user expertise model in AthosMail, a mobile, speech-based e-mail system. The model encodes the system’s assumptions about the user expertise, and gives recommendations on how the system should respond depending on the assumed competence levels of the user. The recommendations are realized as three types of explicitness in the system responses. The system monitors the user’s competence with the help of parameters that describe e.g. the success of the user’s interaction with the system. The model consists of an online and an offline version, the former taking care of the expertise level changes during the same session, the latter modelling the overall user expertise as a function of time and repeated interactions. 
Discriminative methods have shown signiﬁcant improvements over traditional generative methods in many machine learning applications, but there has been diﬃculty in extending them to natural language parsing. One problem is that much of the work on discriminative methods conﬂates changes to the learning method with changes to the parameterization of the problem. We show how a parser can be trained with a discriminative learning method while still parameterizing the problem according to a generative probability model. We present three methods for training a neural network to estimate the probabilities for a statistical parser, one generative, one discriminative, and one where the probability model is generative but the training criteria is discriminative. The latter model outperforms the previous two, achieving state-ofthe-art levels of performance (90.1% F-measure on constituents). 
This paper describes and evaluates log-linear parsing models for Combinatory Categorial Grammar (CCG). A parallel implementation of the L-BFGS optimisation algorithm is described, which runs on a Beowulf cluster allowing the complete Penn Treebank to be used for estimation. We also develop a new efﬁcient parsing algorithm for CCG which maximises expected recall of dependencies. We compare models which use all CCG derivations, including nonstandard derivations, with normal-form models. The performances of the two models are comparable and the results are competitive with existing wide-coverage CCG parsers. 
This paper describes an incremental parsing approach where parameters are estimated using a variant of the perceptron algorithm. A beam-search algorithm is used during both training and decoding phases of the method. The perceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn treebank. We demonstrate that training a perceptron model to combine with the generative model during search provides a 2.1 percent F-measure improvement over the generative model alone, to 88.8 percent. 
Convolution kernels, such as sequence and tree kernels, are advantageous for both the concept and accuracy of many natural language processing (NLP) tasks. Experiments have, however, shown that the over-ﬁtting problem often arises when these kernels are used in NLP tasks. This paper discusses this issue of convolution kernels, and then proposes a new approach based on statistical feature selection that avoids this issue. To enable the proposed method to be executed efﬁciently, it is embedded into an original kernel calculation process by using sub-structure mining algorithms. Experiments are undertaken on real NLP tasks to conﬁrm the problem with a conventional method and to compare its performance with that of the proposed method. 
Coreferential information of a candidate, such as the properties of its antecedents, is important for pronoun resolution because it reﬂects the salience of the candidate in the local discourse. Such information, however, is usually ignored in previous learning-based systems. In this paper we present a trainable model which incorporates coreferential information of candidates into pronoun resolution. Preliminary experiments show that our model will boost the resolution performance given the right antecedents of the candidates. We further discuss how to apply our model in real resolution where the antecedents of the candidate are found by a separate noun phrase resolution module. The experimental results show that our model still achieves better performance than the baseline. 
This paper proposes a new approach for coreference resolution which uses the Bell tree to represent the search space and casts the coreference resolution problem as ﬁnding the best path from the root of the Bell tree to the leaf nodes. A Maximum Entropy model is used to rank these paths. The coreference performance on the 2002 and 2003 Automatic Content Extraction (ACE) data will be reported. We also train a coreference system using the MUC6 data and competitive results are obtained. 
We use machine learning techniques to ﬁnd the best combination of local focus and lexical distance features for identifying the anchor of mereological bridging references. We ﬁnd that using ﬁrst mention, utterance distance, and lexical distance computed using either Google or WordNet results in an accuracy signiﬁcantly higher than obtained in previous experiments. 
Knowledge of the anaphoricity of a noun phrase might be proﬁtably exploited by a coreference system to bypass the resolution of non-anaphoric noun phrases. Perhaps surprisingly, recent attempts to incorporate automatically acquired anaphoricity information into coreference systems, however, have led to the degradation in resolution performance. This paper examines several key issues in computing and using anaphoricity information to improve learning-based coreference systems. In particular, we present a new corpus-based approach to anaphoricity determination. Experiments on three standard coreference data sets demonstrate the effectiveness of our approach. 
Most foreign names are transliterated into Chinese, Japanese or Korean with approximate phonetic equivalents. The transliteration is usually achieved through intermediate phonemic mapping. This paper presents a new framework that allows direct orthographical mapping (DOM) between two different languages, through a joint source-channel model, also called n-gram transliteration model (TM). With the n-gram TM model, we automate the orthographic alignment process to derive the aligned transliteration units from a bilingual dictionary. The n-gram TM under the DOM framework greatly reduces system development effort and provides a quantum leap in improvement in transliteration accuracy over that of other state-of-the-art machine learning algorithms. The modeling framework is validated through several experiments for English-Chinese language pair. 
Collocation translation is important for machine translation and many other NLP tasks. Unlike previous methods using bilingual parallel corpora, this paper presents a new method for acquiring collocation translations by making use of monolingual corpora and linguistic knowledge. First, dependency triples are extracted from Chinese and English corpora with dependency parsers. Then, a dependency triple translation model is estimated using the EM algorithm based on a dependency correspondence assumption. The generated triple translation model is used to extract collocation translations from two monolingual corpora. Experiments show that our approach outperforms the existing monolingual corpus based methods in dependency triple translation and achieves promising results in collocation translation extraction. 
The parameters of statistical translation models are typically estimated from sentence-aligned parallel corpora. We show that signiﬁcant improvements in the alignment and translation quality of such models can be achieved by additionally including wordaligned data during training. Incorporating wordlevel alignments into the parameter estimation of the IBM models reduces alignment error rate and increases the Bleu score when compared to training the same models only on sentence-aligned data. On the Verbmobil data set, we attain a 38% reduction in the alignment error rate and a higher Bleu score with half as many training examples. We discuss how varying the ratio of word-aligned to sentencealigned data affects the expected performance gain. 
 Multilingual applications frequently involve  dealing with proper names, but names are  often missing in bilingual lexicons. This  problem is exacerbated for applications  involving translation between Latin-scripted  languages and Asian languages such as  Chinese, Japanese and Korean (CJK) where  simple string copying is not a solution. We  present a novel approach for generating the  ideographic representations of a CJK name  written in a Latin script. The proposed  approach involves first identifying the origin  of the name, and then back-transliterating the  name to all possible Chinese characters using  language-specific mappings. To reduce the  massive number of possibilities for  computation, we apply a three-tier filtering  process by filtering first through a set of  attested bigrams, then through a set of attested  terms, and lastly through the WWW for a final  validation. We illustrate the approach with  English-to-Japanese  back-transliteration.  Against test sets of Japanese given names and  surnames, we have achieved average  precisions of 73% and 90%, respectively.  
We present an approach using syntactosemantic rules for the extraction of relational information from biomedical abstracts. The results show that by overcoming the hurdle of technical terminology, high precision results can be achieved. From abstracts related to baker’s yeast, we manage to extract a regulatory network comprised of 441 pairwise relations from 58,664 abstracts with an accuracy of 83–90%. To achieve this, we made use of a resource of gene/protein names considerably larger than those used in most other biology related information extraction approaches. This list of names was included in the lexicon of our retrained part-of-speech tagger for use on molecular biology abstracts. For the domain in question an accuracy of 93.6–97.7% was attained on POS-tags. The method is easily adapted to other organisms than yeast, allowing us to extract many more biologically relevant relations. 
A new technique is introduced, linguistic profiling, in which large numbers of counts of linguistic features are used as a text profile, which can then be compared to average profiles for groups of texts. The technique proves to be quite effective for authorship verification and recognition. The best parameter settings yield a False Accept Rate of 8.1% at a False Reject Rate equal to zero for the verification task on a test corpus of student essays, and a 99.4% 2-way recognition accuracy on the same corpus. 
This paper describes an empirical study of the “Information Synthesis” task, deﬁned as the process of (given a complex information need) extracting, organizing and inter-relating the pieces of information contained in a set of relevant documents, in order to obtain a comprehensive, non redundant report that satisﬁes the information need. Two main results are presented: a) the creation of an Information Synthesis testbed with 72 reports manually generated by nine subjects for eight complex topics with 100 relevant documents each; and b) an empirical comparison of similarity metrics between reports, under the hypothesis that the best metric is the one that best distinguishes between manual and automatically generated reports. A metric based on key concepts overlap gives better results than metrics based on n-gram overlap (such as ROUGE) or sentence overlap. 
This paper describes and evaluates MOP, an IE system for automatic extraction of metalinguistic information from technical and scientific documents. We claim that such a system can create special databases to bootstrap compilation and facilitate update of the huge and dynamically changing glossaries, knowledge bases and ontologies that are vital to modern-day research. 
This paper introduces an indexing method based on static analysis of grammar rules and type signatures for typed feature structure grammars (TFSGs). The static analysis tries to predict at compile-time which feature paths will cause uniﬁcation failure during parsing at run-time. To support the static analysis, we introduce a new classiﬁcation of the instances of variables used in TFSGs, based on what type of structure sharing they create. The indexing actions that can be performed during parsing are also enumerated. Non-statistical indexing has the advantage of not requiring training, and, as the evaluation using large-scale HPSGs demonstrates, the improvements are comparable with those of statistical optimizations. Such statistical optimizations rely on data collected during training, and their performance does not always compensate for the training costs. 
We present the ﬁrst application of the head-driven statistical parsing model of Collins (1999) as a simultaneous language model and parser for largevocabulary speech recognition. The model is adapted to an online left to right chart-parser for word lattices, integrating acoustic, n-gram, and parser probabilities. The parser uses structural and lexical dependencies not considered by ngram models, conditioning recognition on more linguistically-grounded relationships. Experiments on the Wall Street Journal treebank and lattice corpora show word error rates competitive with the standard n-gram language model while extracting additional structural information useful for speech understanding. 
The purpose of this paper is to re-examine the balance between clarity and efﬁciency in HPSG design, with particular reference to the design decisions made in the English Resource Grammar (LinGO, 1999, ERG). It is argued that a simple generalization of the conventional delay statements used in logic programming is sufﬁcient to restore much of the functionality and concomitant beneﬁt that the ERG elected to forego, with an acceptable although still perceptible computational cost. 
We show that a practical translation of MRS descriptions into normal dominance constraints is feasible. We start from a recent theoretical translation and verify its assumptions on the outputs of the English Resource Grammar (ERG) on the Redwoods corpus. The main assumption of the translation— that all relevant underspeciﬁed descriptions are nets—is validated for a large majority of cases; all non-nets computed by the ERG seem to be systematically incomplete. 
A wide range of supervised learning algorithms has been applied to Text Categorization. However, the supervised learning approaches have some problems. One of them is that they require a large, often prohibitive, number of labeled training documents for accurate learning. Generally, acquiring class labels for training data is costly, while gathering a large quantity of unlabeled data is cheap. We here propose a new automatic text categorization method for learning from only unlabeled data using a bootstrapping framework and a feature projection technique. From results of our experiments, our method showed reasonably comparable performance compared with a supervised method. If our method is used in a text categorization task, building text categorization systems will become significantly faster and less expensive. 
Sentiment classiﬁcation is the task of labeling a review document according to the polarity of its prevailing opinion (favorable or unfavorable). In approaching this problem, a model builder often has three sources of information available: a small collection of labeled documents, a large collection of unlabeled documents, and human understanding of language. Ideally, a learning method will utilize all three sources. To accomplish this goal, we generalize an existing procedure that uses the latter two. We extend this procedure by re-interpreting it as a Naive Bayes model for document sentiment. Viewed as such, it can also be seen to extract a pair of derived features that are linearly combined to predict sentiment. This perspective allows us to improve upon previous methods, primarily through two strategies: incorporating additional derived features into the model and, where possible, using labeled data to estimate their relative inﬂuence. 
Sentiment analysis seeks to identify the viewpoint(s) underlying a text span; an example application is classifying a movie review as “thumbs up” or “thumbs down”. To determine this sentiment polarity, we propose a novel machine-learning method that applies text-categorization techniques to just the subjective portions of the document. Extracting these portions can be implemented using efﬁcient techniques for ﬁnding minimum cuts in graphs; this greatly facilitates incorporation of cross-sentence contextual constraints. 
In word sense disambiguation (WSD), the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed. The problem with using the predominant, or ﬁrst sense heuristic, aside from the fact that it does not take surrounding context into account, is that it assumes some quantity of handtagged data. Whilst there are a few hand-tagged corpora available for some languages, one would expect the frequency distribution of the senses of words, particularly topical words, to depend on the genre and domain of the text under consideration. We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to ﬁnd predominant noun senses automatically. The acquired predominant senses give a precision of 64% on the nouns of the SENSEVAL2 English all-words task. This is a very promising result given that our method does not require any hand-tagged text, such as SemCor. Furthermore, we demonstrate that our method discovers appropriate predominant senses for words from two domainspeciﬁc corpora. 
We describe two probabilistic models for unsupervised word-sense disambiguation using parallel corpora. The ﬁrst model, which we call the Sense model, builds on the work of Diab and Resnik (2002) that uses both parallel text and a sense inventory for the target language, and recasts their approach in a probabilistic framework. The second model, which we call the Concept model, is a hierarchical model that uses a concept latent variable to relate different language speciﬁc sense labels. We show that both models improve performance on the word sense disambiguation task over previous unsupervised approaches, with the Concept model showing the largest improvement. Furthermore, in learning the Concept model, as a by-product, we learn a sense inventory for the parallel language. 
This paper discusses the application of the Expectation-Maximization (EM) clustering algorithm to the task of Chinese verb sense discrimination. The model utilized rich linguistic features that capture predicateargument structure information of the target verbs. A semantic taxonomy for Chinese nouns, which was built semi-automatically based on two electronic Chinese semantic dictionaries, was used to provide semantic features for the model. Purity and normalized mutual information were used to evaluate the clustering performance on 12 Chinese verbs. The experimental results show that the EM clustering model can learn sense or sense group distinctions for most of the verbs successfully. We further enhanced the model with certain fine-grained semantic categories called lexical sets. Our results indicate that these lexical sets improve the model’s performance for the three most challenging verbs chosen from the first set of experiments. 
Supervised learning methods for WSD yield better performance than unsupervised methods. Yet the availability of clean training data for the former is still a severe challenge. In this paper, we present an unsupervised bootstrapping approach for WSD which exploits huge amounts of automatically generated noisy data for training within a supervised learning framework. The method is evaluated using the 29 nouns in the English Lexical Sample task of SENSEVAL2. Our algorithm does as well as supervised algorithms on 31% of this test set, which is an improvement of 11% (absolute) over state-of-the-art bootstrapping WSD algorithms. We identify seven different factors that impact the performance of our system. 
We describe a method for enriching the output of a parser with information available in a corpus. The method is based on graph rewriting using memorybased learning, applied to dependency structures. This general framework allows us to accurately recover both grammatical and semantic information as well as non-local dependencies. It also facilitates dependency-based evaluation of phrase structure parsers. Our method is largely independent of the choice of parser and corpus, and shows state of the art performance. 
This paper shows how ﬁnite approximations of long distance dependency (LDD) resolution can be obtained automatically for wide-coverage, robust, probabilistic Lexical-Functional Grammar (LFG) resources acquired from treebanks. We extract LFG subcategorisation frames and paths linking LDD reentrancies from f-structures generated automatically for the Penn-II treebank trees and use them in an LDD resolution algorithm to parse new text. Unlike (Collins, 1999; Johnson, 2002), in our approach resolution of LDDs is done at f-structure (attribute-value structure representations of basic predicate-argument or dependency structure) without empty productions, traces and coindexation in CFG parse trees. Currently our best automatically induced grammars achieve 80.97% f-score for fstructures parsing section 23 of the WSJ part of the Penn-II treebank and evaluating against the DCU 1051 and 80.24% against the PARC 700 Dependency Bank (King et al., 2003), performing at the same or a slightly better level than state-of-the-art hand-crafted grammars (Kaplan et al., 2004). 
We present a linguistically-motivated algorithm for reconstructing nonlocal dependency in broad-coverage context-free parse trees derived from treebanks. We use an algorithm based on loglinear classiﬁers to augment and reshape context-free trees so as to reintroduce underlying nonlocal dependencies lost in the context-free approximation. We ﬁnd that our algorithm compares favorably with prior work on English using an existing evaluation metric, and also introduce and argue for a new dependency-based evaluation metric. By this new evaluation metric our algorithm achieves 60% error reduction on gold-standard input trees and 5% error reduction on state-ofthe-art machine-parsed input trees, when compared with the best previous work. We also present the ﬁrst results on nonlocal dependency reconstruction for a language other than English, comparing performance on English and German. Our new evaluation metric quantitatively corroborates the intuition that in a language with freer word order, the surface dependencies in context-free parse trees are a poorer approximation to underlying dependency structure. 
In this paper we have designed and experimented novel convolution kernels for automatic classiﬁcation of predicate arguments. Their main property is the ability to process structured representations. Support Vector Machines (SVMs), using a combination of such kernels and the ﬂat feature kernel, classify PropBank predicate arguments with accuracy higher than the current argument classiﬁcation stateof-the-art. Additionally, experiments on FrameNet data have shown that SVMs are appealing for the classiﬁcation of semantic roles even if the proposed kernels do not produce any improvement. 
We use machine learners trained on a combination of acoustic conﬁdence and pragmatic plausibility features computed from dialogue context to predict the accuracy of incoming n-best recognition hypotheses to a spoken dialogue system. Our best results show a 25% weighted f-score improvement over a baseline system that implements a “grammar-switching” approach to context-sensitive speech recognition. 
We examine the utility of speech and lexical features for predicting student emotions in computerhuman spoken tutoring dialogues. We ﬁrst annotate student turns for negative, neutral, positive and mixed emotions. We then extract acoustic-prosodic features from the speech signal, and lexical items from the transcribed or recognized speech. We compare the results of machine learning experiments using these features alone or in combination to predict various categorizations of the annotated student emotions. Our best results yield a 19-36% relative improvement in error reduction over a baseline. Finally, we compare our results with emotion prediction in human-human tutoring dialogues. 
In this paper we present a methodology for extracting subcategorisation frames based on an automatic LFG f-structure annotation algorithm for the Penn-II Treebank. We extract abstract syntactic function-based subcategorisation frames (LFG semantic forms), traditional CFG categorybased subcategorisation frames as well as mixed function/category-based frames, with or without preposition information for obliques and particle information for particle verbs. Our approach does not predeﬁne frames, associates probabilities with frames conditional on the lemma, distinguishes between active and passive frames, and fully reﬂects the effects of long-distance dependencies in the source data structures. We extract 3586 verb lemmas, 14348 semantic form types (an average of 4 per lemma) with 577 frame types. We present a large-scale evaluation of the complete set of forms extracted against the full COMLEX resource. 
This paper presents SemFrame, a system that induces frame semantic verb classes from WordNet and LDOCE. Semantic frames are thought to have significant potential in resolving the paraphrase problem challenging many languagebased applications. When compared to the handcrafted FrameNet, SemFrame achieves its best recall-precision balance with 83.2% recall (based on SemFrame's coverage of FrameNet frames) and 73.8% precision (based on SemFrame verbs’ semantic relatedness to frame-evoking verbs). The next best performing semantic verb classes achieve 56.9% recall and 55.0% precision. 
 Sentence ranking is a crucial part of  generating text summaries. We compared  human sentence rankings obtained in a  psycholinguistic experiment to three different  approaches to sentence ranking: A simple  paragraph-based approach intended as a  baseline, two word-based approaches, and two  coherence-based approaches.  In the  paragraph-based approach, sentences in the  beginning of paragraphs received higher  importance ratings than other sentences. The  word-based approaches determined sentence  rankings based on relative word frequencies  (Luhn (1958); Salton & Buckley (1988)).  Coherence-based approaches determined  sentence rankings based on some property of  the coherence structure of a text (Marcu  (2000); Page et al. (1998)). Our results  suggest poor performance for the simple  paragraph-based approach, whereas word-  based approaches perform remarkably well.  The best performance was achieved by a  coherence-based approach where coherence  structures are represented in a non-tree  structure. Most approaches also outperformed  the commercially available MSWord  summarizer.  
We use a reliably annotated corpus to compare metrics of coherence based on Centering Theory with respect to their potential usefulness for text structuring in natural language generation. Previous corpus-based evaluations of the coherence of text according to Centering did not compare the coherence of the chosen text structure with that of the possible alternatives. A corpusbased methodology is presented which distinguishes between Centering-based metrics taking these alternatives into account, and represents therefore a more appropriate way to evaluate Centering from a text structuring perspective. 
We present the ﬁrst algorithm that computes optimal orderings of sentences into a locally coherent discourse. The algorithm runs very efﬁciently on a variety of coherence measures from the literature. We also show that the discourse ordering problem is NP-complete and cannot be approximated. 
We present an algorithm for generating referring expressions in open domains. Existing algorithms work at the semantic level and assume the availability of a classiﬁcation for attributes, which is only feasible for restricted domains. Our alternative works at the realisation level, relies on WordNet synonym and antonym sets, and gives equivalent results on the examples cited in the literature and improved results for examples that prior approaches cannot handle. We believe that ours is also the ﬁrst algorithm that allows for the incremental incorporation of relations. We present a novel corpus-evaluation using referring expressions from the Penn Wall Street Journal Treebank. 
Discovering the signiﬁcant relations embedded in documents would be very useful not only for information retrieval but also for question answering and summarization. Prior methods for relation discovery, however, needed large annotated corpora which cost a great deal of time and effort. We propose an unsupervised method for relation discovery from large corpora. The key idea is clustering pairs of named entities according to the similarity of context words intervening between the named entities. Our experiments using one year of newspapers reveals not only that the relations among named entities could be detected with high recall and precision, but also that appropriate labels could be automatically provided for the relations. 
We extend previous work on tree kernels to estimate the similarity between the dependency trees of sentences. Using this kernel within a Support Vector Machine, we detect and classify relations between entities in the Automatic Content Extraction (ACE) corpus of news articles. We examine the utility of different features such as Wordnet hypernyms, parts of speech, and entity types, and ﬁnd that the dependency tree kernel achieves a 20% F1 improvement over a “bag-of-words” kernel. 
A crucial step toward the goal of automatic extraction of propositional information from natural language text is the identiﬁcation of semantic relations between constituents in sentences. We examine the problem of distinguishing among seven relation types that can occur between the entities “treatment” and “disease” in bioscience text, and the problem of identifying such entities. We compare ﬁve generative graphical models and a neural network, using lexical, syntactic, and semantic features, ﬁnding that the latter help achieve high classiﬁcation accuracy. 
Parsing systems which rely on hand-coded linguistic descriptions can only perform adequately in as far as these descriptions are correct and complete. The paper describes an error mining technique to discover problems in hand-coded linguistic descriptions for parsing such as grammars and lexicons. By analysing parse results for very large unannotated corpora, the technique discovers missing, incorrect or incomplete linguistic descriptions. The technique uses the frequency of n-grams of words for arbitrary values of n. It is shown how a new combination of sufﬁx arrays and perfect hash ﬁnite automata allows an efﬁcient implementation. 
We compare two approaches for describing and generating bodies of rules used for natural language parsing. In today’s parsers rule bodies do not exist a priori but are generated on the ﬂy, usually with methods based on n-grams, which are one particular way of inducing probabilistic regular languages. We compare two approaches for inducing such languages. One is based on n-grams, the other on minimization of the Kullback-Leibler divergence. The inferred regular languages are used for generating bodies of rules inside a parsing procedure. We compare the two approaches along two dimensions: the quality of the probabilistic regular language they produce, and the performance of the parser they were used to build. The second approach outperforms the ﬁrst one along both dimensions. 
This paper presents a Chinese word segmentation system which can adapt to different domains and standards. We first present a statistical framework where domain-specific words are identified in a unified approach to word segmentation based on linear models. We explore several features and describe how to create training data by sampling. We then describe a transformation-based learning method used to adapt our system to different word segmentation standards. Evaluation of the proposed system on five test sets with different standards shows that the system achieves state- of-the-art performance on all of them. 
This paper discusses the use of statistical word alignment over multiple parallel texts for the identiﬁcation of string spans that cannot be constituents in one of the languages. This information is exploited in monolingual PCFG grammar induction for that language, within an augmented version of the inside-outside algorithm. Besides the aligned corpus, no other resources are required. We discuss an implemented system and present experimental results with an evaluation against the Penn Treebank. 
We present a generative model for the unsupervised learning of dependency structures. We also describe the multiplicative combination of this dependency model with a model of linear constituency. The product model outperforms both components on their respective evaluation metrics, giving the best published ﬁgures for unsupervised dependency parsing and unsupervised constituency parsing. We also demonstrate that the combined model works and is robust cross-linguistically, being able to exploit either attachment or distributional regularities that are salient in the data. 
Exploiting unannotated natural language data is hard largely because unsupervised parameter estimation is hard. We describe deterministic annealing (Rose et al., 1990) as an appealing alternative to the ExpectationMaximization algorithm (Dempster et al., 1977). Seeking to avoid search error, DA begins by globally maximizing an easy concave function and maintains a local maximum as it gradually morphs the function into the desired non-concave likelihood function. Applying DA to parsing and tagging models is shown to be straightforward; signiﬁcant improvements over EM are shown on a part-of-speech tagging task. We describe a variant, skewed DA, which can incorporate a good initializer when it is available, and show signiﬁcant improvements over EM on a grammar induction task. 
The paper describes a particular approach to multiengine machine translation (MEMT), where we make use of voted language models to selectively combine translation outputs from multiple off-theshelf MT systems. Experiments are done using large corpora from three distinct domains. The study found that the use of voted language models leads to an improved performance of MEMT systems. 
Aligning words from sentences which are mutual translations is an important problem in different settings, such as bilingual terminology extraction, Machine Translation, or projection of linguistic features. Here, we view word alignment as matrix factorisation. In order to produce proper alignments, we show that factors must satisfy a number of constraints such as orthogonality. We then propose an algorithm for orthogonal non-negative matrix factorisation, based on a probabilistic model of the alignment data, and apply it to word alignment. This is illustrated on a French-English alignment task from the Hansard. 
In this paper we present the RWTH FSA toolkit – an efﬁcient implementation of algorithms for creating and manipulating weighted ﬁnite-state automata. The toolkit has been designed using the principle of on-demand computation and offers a large range of widely used algorithms. To prove the superior efﬁciency of the toolkit, we compare the implementation to that of other publically available toolkits. We also show that on-demand computations help to reduce memory requirements signiﬁcantly without any loss in speed. To increase its ﬂexibility, the RWTH FSA toolkit supports high-level interfaces to the programming language Python as well as a command-line tool for interactive manipulation of FSAs. Furthermore, we show how to utilize the toolkit to rapidly build a fast and accurate statistical machine translation system. Future extensibility of the toolkit is ensured as it will be publically available as open source software. 
We investigate a number of simple methods for improving the word-alignment accuracy of IBM Model 1. We demonstrate reduction in alignment error rate of approximately 30% resulting from (1) giving extra weight to the probability of alignment to the null word, (2) smoothing probability estimates for rare words, and (3) using a simple heuristic estimation method to initialize, or replace, EM training of model parameters. 
We present a geometric view on bilingual lexicon extraction from comparable corpora, which allows to re-interpret the methods proposed so far and identify unresolved problems. This motivates three new methods that aim at solving these problems. Empirical evaluation shows the strengths and weaknesses of these methods, as well as a signiﬁcant gain in the accuracy of extracted lexicons. 
The purpose of this paper is to automatically create multilingual translation lexicons with regional variations. We propose a transitive translation approach to determine translation variations across languages that have insufficient corpora for translation via the mining of bilingual search-result pages and clues of geographic information obtained from Web search engines. The experimental results have shown the feasibility of the proposed approach in efficiently generating translation equivalents of various terms not covered by general translation dictionaries. It also revealed that the created translation lexicons can reflect different cultural aspects across regions such as Taiwan, Hong Kong and mainland China. 
We present new results on the relation between context-free parsing strategies and their probabilistic counter-parts. We provide a necessary condition and a sufﬁcient condition for the probabilistic extension of parsing strategies. These results generalize existing results in the literature that were obtained by considering parsing strategies in isolation. 
We discuss existing approaches to train LR parsers, which have been used for statistical resolution of structural ambiguity. These approaches are nonoptimal, in the sense that a collection of probability distributions cannot be obtained. In particular, some probability distributions expressible in terms of a context-free grammar cannot be expressed in terms of the LR parser constructed from that grammar, under the restrictions of the existing approaches to training of LR parsers. We present an alternative way of training that is provably optimal, and that allows all probability distributions expressible in the context-free grammar to be carried over to the LR parser. We also demonstrate empirically that this kind of training can be effectively applied on a large treebank. 
We explore the descriptive power, in terms of syntactic phenomena, of a formalism that extends TreeAdjoining Grammar (TAG) by adding a fourth level of hierarchical decomposition to the three levels TAG already employs. While extending the descriptive power minimally, the additional level of decomposition allows us to obtain a uniform account of a range of phenomena that has heretofore been difﬁcult to encompass, an account that employs unitary elementary structures and eschews synchronized derivation operations, and which is, in many respects, closer to the spirit of the intuitions underlying TAG-based linguistic theory than previously considered extensions to TAG. 
This paper presents a multi-layered Question Answering (Q.A.) architecture suitable for enhancing current Q.A. capabilities with the possibility of processing complex questions. That is, questions whose answer needs to be gathered from pieces of factual information scattered in different documents. Speciﬁcally, we have designed a layer oriented to process the different types of temporal questions. Complex temporal questions are ﬁrst decomposed into simpler ones, according to the temporal relationships expressed in the original question. In the same way, the answers of each simple question are re-composed, fulﬁlling the temporal restrictions of the original complex question. Using this architecture, a Temporal Q.A. system has been developed. In this paper, we focus on explaining the ﬁrst part of the process: the decomposition of the complex questions. Furthermore, it has been evaluated with the TERQAS question corpus of 112 temporal questions. For the task of question splitting our system has performed, in terms of precision and recall, 85% and 71%, respectively. 
QA-by-Dossier-with-Constraints is a new approach to Question Answering whereby candidate answers’ confidences are adjusted by asking auxiliary questions whose answers constrain the original answers. These constraints emerge naturally from the domain of interest, and enable application of real-world knowledge to QA. We show that our approach significantly improves system performance (75% relative improvement in F-measure on select question types) and can create a “dossier” of information about the subject matter in the original question. 
Temporal relation resolution involves extraction of temporal information explicitly or implicitly embedded in a language. This information is often inferred from a variety of interactive grammatical and lexical cues, especially in Chinese. For this purpose, inter-clause relations (temporal or otherwise) in a multiple-clause sentence play an important role. In this paper, a computational model based on machine learning and heterogeneous collaborative bootstrapping is proposed for analyzing temporal relations in a Chinese multiple-clause sentence. The model makes use of the fact that events are represented in different temporal structures. It takes into account the effects of linguistic features such as tense/aspect, temporal connectives, and discourse structures. A set of experiments has been conducted to investigate how linguistic features could affect temporal relation resolution. 
In this paper, we propose a multi-criteria based active learning approach and effectively apply it to named entity recognition. Active learning targets to minimize the human annotation efforts by selecting examples for labeling. To maximize the contribution of the selected examples, we consider the multiple criteria: informativeness, representativeness and diversity and propose measures to quantify them. More comprehensively, we incorporate all the criteria using two selection strategies, both of which result in less labeling cost than single-criterion-based method. The results of the named entity recognition in both MUC-6 and GENIA show that the labeling cost can be reduced by at least 80% without degrading the performance. 
It is fairly common that different people are associated with the same name. In tracking person entities in a large document pool, it is important to determine whether multiple mentions of the same name across documents refer to the same entity or not. Previous approach to this problem involves measuring context similarity only based on co-occurring words. This paper presents a new algorithm using information extraction support in addition to co-occurring words. A learning scheme with minimal supervision is developed within the Bayesian framework. Maximum entropy modeling is then used to represent the probability distribution of context similarities based on heterogeneous features. Statistical annealing is applied to derive the final entity coreference chains by globally fitting the pairwise context similarities. Benchmarking shows that our new approach significantly outperforms the existing algorithm by 25 percentage points in overall F-measure. 
In this paper we describe two new objective automatic evaluation methods for machine translation. The first method is based on longest common subsequence between a candidate translation and a set of reference translations. Longest common subsequence takes into account sentence level structure similarity naturally and identifies longest co-occurring insequence n-grams automatically. The second method relaxes strict n-gram matching to skipbigram matching. Skip-bigram is any pair of words in their sentence order. Skip-bigram cooccurrence statistics measure the overlap of skip-bigrams between a candidate translation and a set of reference translations. The empirical results show that both methods correlate with human judgments very well in both adequacy and fluency. 
In this paper we propose a unified framework for automatic evaluation of NLP applications using N-gram co-occurrence statistics. The automatic evaluation metrics proposed to date for Machine Translation and Automatic Summarization are particular instances from the family of metrics we propose. We show that different members of the same family of metrics explain best the variations obtained with human evaluations, according to the application being evaluated (Machine Translation, Automatic Summarization, and Automatic Question Answering) and the evaluation guidelines used by humans for evaluating such applications. 
This paper presents an unsupervised word sense learning algorithm, which induces senses of target word by grouping its occurrences into a “natural” number of clusters based on the similarity of their contexts. For removing noisy words in feature set, feature selection is conducted by optimizing a cluster validation criterion subject to some constraint in an unsupervised manner. Gaussian mixture model and Minimum Description Length criterion are used to estimate cluster structure and cluster number. Experimental results show that our algorithm can ﬁnd important feature subset, estimate model order (cluster number) and achieve better performance than another algorithm which requires cluster number to be provided. 
We introduce a new method for disambiguating word senses that exploits a nonlinear Kernel Principal Component Analysis (KPCA) technique to achieve accuracy superior to the best published individual models. We present empirical results demonstrating signiﬁcantly better accuracy compared to the state-of-the-art achieved by either na¨ıve Bayes or maximum entropy models, on Senseval-2 data. We also contrast against another type of kernel method, the support vector machine (SVM) model, and show that our KPCA-based model outperforms the SVM-based model. It is hoped that these highly encouraging ﬁrst results on KPCA for natural language processing tasks will inspire further development of these directions. 
This paper describes an algorithm for detecting empty nodes in the Penn Treebank (Marcus et al., 1993), finding their antecedents, and assigning them function tags, without access to lexical information such as valency. Unlike previous approaches to this task, the current method is not corpus-based, but rather makes use of the principles of early Government-Binding theory (Chomsky, 1981), the syntactic theory that underlies the annotation. Using the evaluation metric proposed by Johnson (2002), this approach outperforms previously published approaches on both detection of empty categories and antecedent identification, given either annotated input stripped of empty categories or the output of a parser. Some problems with this evaluation metric are noted and an alternative is proposed along with the results. The paper considers the reasons a principlebased approach to this problem should outperform corpus-based approaches, and speculates on the possibility of a hybrid approach. 
In an ordinary syntactic parser, the input is a string, and the grammar ranges over strings. This paper explores generalizations of ordinary parsing algorithms that allow the input to consist of string tuples and/or the grammar to range over string tuples. Such algorithms can infer the synchronous structures hidden in parallel texts. It turns out that these generalized parsers can do most of the work required to train and apply a syntax-aware statistical machine translation system. 
Generalized Multitext Grammar (GMTG) is a synchronous grammar formalism that is weakly equivalent to Linear Context-Free Rewriting Systems (LCFRS), but retains much of the notational and intuitive simplicity of Context-Free Grammar (CFG). GMTG allows both synchronous and independent rewriting. Such ﬂexibility facilitates more perspicuous modeling of parallel text than what is possible with other synchronous formalisms. This paper investigates the generative capacity of GMTG, proves that each component grammar of a GMTG retains its generative power, and proposes a generalization of Chomsky Normal Form, which is necessary for synchronous CKY-style parsing. 
We describe a statistical approach for modeling agreements and disagreements in conversational interaction. Our approach ﬁrst identiﬁes adjacency pairs using maximum entropy ranking based on a set of lexical, durational, and structural features that look both forward and backward in the discourse. We then classify utterances as agreement or disagreement using these adjacency pairs and features that represent various pragmatic inﬂuences of previous agreement or disagreement on the current utterance. Our approach achieves 86.9% accuracy, a 4.9% increase over previous work. 
The detection of prosodic characteristics is an important aspect of both speech synthesis and speech recognition. Correct placement of pitch accents aids in more natural sounding speech, while automatic detection of accents can contribute to better wordlevel recognition and better textual understanding. In this paper we investigate probabilistic, contextual, and phonological factors that inﬂuence pitch accent placement in natural, conversational speech in a sequence labeling setting. We introduce Conditional Random Fields (CRFs) to pitch accent prediction task in order to incorporate these factors efﬁciently in a sequence model. We demonstrate the usefulness and the incremental effect of these factors in a sequence model by performing experiments on hand labeled data from the Switchboard Corpus. Our model outperforms the baseline and previous models of pitch accent prediction on the Switchboard Corpus. 
This paper applies machine learning techniques to acquiring aspects of the meaning of discourse markers. Three subtasks of acquiring the meaning of a discourse marker are considered: learning its polarity, veridicality, and type (i.e. causal, temporal or additive). Accuracy of over 90% is achieved for all three tasks, well above the baselines. 
We discuss Feature Latent Semantic Analysis (FLSA), an extension to Latent Semantic Analysis (LSA). LSA is a statistical method that is ordinarily trained on words only; FLSA adds to LSA the richness of the many other linguistic features that a corpus may be labeled with. We applied FLSA to dialogue act classiﬁcation with excellent results. We report results on three corpora: CallHome Spanish, MapTask, and our own corpus of tutoring dialogues. 
The critical issues involved in speech-to-speech translation are obtaining proper source segments and synthesizing accurate target speech. Therefore, this article develops a novel multiple-translation spotting method to deal with these issues efficiently. Term multiple-translation spotting refers to the task of extracting target-language synthesis patterns that correspond to a given set of source-language spotted patterns in conditional multiple pairs of speech patterns known to be translation patterns. According to the extracted synthesis patterns, the target speech can be properly synthesized by using a waveform segment concatenation-based synthesis method. Experiments were conducted with the languages of Mandarin and Taiwanese. The results reveal that the proposed approach can achieve translation understanding rates of 80% and 76% on average for Mandarin/Taiwanese translation and Taiwanese/Mandarin translation, respectively. Keywords: Multiple-Translation Spotting, Speech-to-Speech Translation 1. Introduction Automatic speech-to-speech translation is a prospective application of speech and language technology [See JANUS III [Lavie et al. 1997], Verbmobil [W. Wahlster 2000], EUTRANS [Casacuberta et al. 2001] and ATR-MATRIX [Sugaya et al. 1999] ]. However, the unsolved problems in speech-to-speech translation are how to obtain proper source segments and how to generate accurate target sequences while the system performance is degraded by speech input. With the rising importance of parallel texts (bitexts) in language translation, an approach called translation spotting has been applied for proposing appropriate translations, referring to the TransSearch system [Macklovitch et al., 2000] and sub-sentential translation memory systems [M. Simard, 2003]. Previous works in this area have suggested that manual review or  * Corresponding author:  Prof. Jhing-Fa Wang, Department of Electrical Engineering, National Cheng Kung University, No.1,  Dasyue Rd., East District, Tainan City 70101, Taiwan, R.O.C.  Email: wangjf@csie.ncku.edu.tw Tel: 886-6-2757575 ext. 62341  Fax: 886-6-2746867  14  Jhing-Fa Wang et al.  crafting is required to obtain example bases of sufficient coverage and accuracy to be truly useful. Translation spotting (TS) is a term coined by Véronis and Langlais [2000] and refers to the task of identifying word tokens in a target-language (TL) translation that correspond to some given word-patterns in a source-language (SL) text. This process takes as input a couple, i.e., a pair of SL and TL text segments known to be translation patterns, and an SL query, i.e., a subset of the patterns of the SL segment, on which the TS will focus its attention. In more formal terms: Ÿ The input to the TS process is a pair of SL and TL text segments S,T and a contiguous, non-empty input sequence of word-tokens in SL, q = s1 Lsn . Ÿ The output is a pair of sets of translation patterns rq (S), rq (T ) : the SL answer and TL answer, respectively. Table 1 shows some examples of TS, where the words in italics represent the SL input, and the words in bold are the SL and TL answers. As can be seen in these examples, the patterns in the input q and answers rq (S) and rq (T ) may or may not be contiguous (examples 2 and 3), and the TL answer may possibly be empty (example 4) when there is no satisfactory way of linking TL patterns to the input. By varying the identification criteria, the translation spotting method can help evaluate units over various dimensions, such as frequency ranges, parts of speech and even speech features of spoken language.  Table 1. Translation spotting examples.  Query  SL (Mandarin)  你 預計 要 待 幾 天 1. q:待 幾 天 rq (S) ={待,幾,天}  我 明天 要 訂 兩 間 有  q:我 要 訂 淋浴 設備 的 單人房  2. 兩 間 單人 房  rq (S) ={我,要,訂,兩,間,單  人房}  q:今晚 有 3. […] 雙人房 嗎  請 問 你們 今晚 有 一 間 雙人房 嗎 rq (S) ={今晚,有,雙人房, 嗎}  4.  q:包括 … 在 內  有 包括 早餐 在 內? rq (S) ={包括,在,內}  Sentence Pair TL (Taiwanese) lie phahsngx bueq doax kuie jit rq (T ) ={doax,kuie,jit} minafzaix goar bueq dexng lerng kefng u sea sengqw e danjiin paang rq (T ) ={goar,bueq,dexng,lerng,kefng,danjiin paang} chviar bun lirn ehngf u cit kefng sianglaang paang but rq (T ) ={ehngf,u,sianglaang,paang,but} u zafdngx but rq (T ) ={φ }  Multiple-Translation Spotting for Mandarin-Taiwanese Speech-to-Speech Translation 15 However, translation spotting can only draw out the TL answer from the best translation; it can not handle an SL query whose word-tokens are distributed in different translations. Consequently, we propose conducting multiple-translation spotting of a speech input using multiple pairs of translation patterns. Figure 1 shows an example of multiple-translation spotting of a speech input. When a speaker inputs an SL speech query ”今晚會有三間單人房 嗎 ”, the proposed system can obtain a TL speech pattern set that includes five elements, ”ehngf”, ”kvaru”, ”svaf”, ”kefng”, and ”danjiinpaang”, according to the spotted SL speech patterns ”今晚”, “會有”, “間”, “嗎”, ”三”, and ”單人房”. The rest of this article is organized as follows. Section 2 presents the framework of the proposed system. Section 3 presents system data training for Mandarin and Taiwanese. Section 4 describes the proposed translation method for speech-to-speech translation. Section 5 presents experimental results. Finally, Section 6 draws conclusions. Figure 1. An example of multiple-translation spotting. 2. Framework of the Proposed System The proposed speech-to-speech translation system is divided into two phases – a training phase and a translation phase. In the training phase, the developed translation examples are imported to derive multiple-translation templates and develop speech data. In the following  16  Jhing-Fa Wang et al.  step, the developed speech data are applied to construct multiple-translation spotting models and synthesis templates. Figure 2(a) shows a block diagram of the training phase.  (a)  (b)  Figure 2. Framework of the proposed system: (a) a training phase; (b) a translation phase.  Figure 2(b) shows a block diagram of the translation phase. A one-stage based spotting method is adopted to identify input spoken phrases for each spotting template, and the template candidates are assigned in the following score normalization and ranking process. However, the hypothesized word sequence generally includes noise-like segments. Accordingly, the segments are adjusted by smoothing the hypothesized word sequences. After the hypothesized word sequences of all template candidates have been smoothed, the hypothesized target sequences are generated using the translation template with the maximum number of spotting tokens of speech input. The obtained target speech segments are used to produce target speech by means of the corresponding synthesis template in the final target generation process.  3. Data Training Phase  As for the task of translating Mandarin and Taiwanese language pairs, although these languages both belong to the family of Chinese languages, their language usages still have various development by language families and their origins, Mandarin belongs to Altaic  Multiple-Translation Spotting for Mandarin-Taiwanese Speech-to-Speech Translation 17 language family, and Taiwanese belongs to Sinitic language family [Sher et al., 1999]. Therefore, in the following section, we will consider their language usages for three template construction. 3.1 Multiple Translation Template Construction While translation templates can be fully constructed, one major issue in translation pattern exploitation, called “divergence,” makes straightforward transfer mapping extraction impractical. Dorr (1993) describes divergence in the following way: “translation divergence arises when the natural translation of one language into another result in a very different form than that of the original.” Therefore, we choose translations with no divergence to practice constructing templates. An example of a simple translation template derived from a practicable translated example is shown below. Translated Example: SL: “我 朋友 要 訂 房間” ↔ TL: ”goarn pengiuo bueq dexng pangkefng” Intention Translation: Mp1 要 訂 Mp2 ↔ Tp1 bueq dexng Tp2 alignments Variable Translation: If Mp1↔ Tp1, 我 朋友 ↔ goarn pengiuo If Mp2↔ Tp2, 房間 ↔ pangkefng The translation template is composed of a translated example, an intention translation, and two variable translations. The example shows how a sentence in Mandarin (SL) that contains an intention “要 訂” with two variables, Mp1 (我 朋友) and Mp2 (房間), can be translated into a sentence in Taiwanese (TL) with an intention “bueq dexng” and two variables, Tp1 (goarn pengiuo) and Tp2 (pangkefng). According to the template, the number of variable translations should be expanded to improve the capability for spotting the speech input. From the preceding example, variable translation expansion can be illustrated as follows: Variable Translation Expansion: If Mp1 ↔ Tp1, 我 ↔ goar 我 朋友↔ goarn pengiuo If Mp2 ↔ Tp2, 房間 ↔ pangkefng 票 ↔ phiaux  18  Jhing-Fa Wang et al.  Therefore, we can obtain corpus-specific multiple translations in a template constructed from three translation patterns, which are “我 朋 友 要 訂 房間↔goarn pengiuo bueq dexng pangkefng”, “我↔goar”, and “票↔phiaux”. 3.2 Spotting Model Construction Taiwanese is a typical oral language and still has no uniform system of writing. In the literature, there are two ways to represent Taiwanese words: Chinese characters and alphabetic writing. [Sher et al., 1999]. Chinese characters have huge hieroglyph character sets; therefore, it is difficult to systematize developed examples. Although alphabetic writing would be an appropriate representation form, a universal phonemic transcription system is still not available. Therefore, for the purpose of practical system construction, a collection of speech data is developed from derived text-form templates not only to obtain spotting models but also to transcribe text data as waveform-based representations. For one of the translating languages, the speech data, including intention speech and related variable speech, are used in chorus to construct spotting reference models for use in multiple-translation spotting. Such spotting reference models are embedded with latent grammars from the constructed templates. When dealing with Mandarin-Taiwanese speech feature models, we build the database by extracting LPCC features from recorded template speeches. Hence, when speech recognition is performed, the LPCC features are extracted from the recorded template speeches, and the LPCC features of speech input are used in combination to compute the degree of dissimilarity. After language pairs of both Taiwanese and Mandarin speech data are developed, the transfer mapping information for a pair of Taiwanese and Mandarin speech segments known to be similar in terms of text-form word alignment is constructed. 3.3 Synthesis Template Construction Both Mandarin and Taiwanese are tonal languages, and it is difficult to determine whether a morpheme will take its inherent tone or the derived tone when every word in a sentence is synthesized. [Wang et al., 1999; Sher et al., 1999]. Therefore, we utilize the obtained intention speech and variable speech as synthesis templates that include intention synthesis units and variable synthesis units. These synthesis units can be used to generate a speech output to be processed using a waveform segment concatenation-based synthesis method [Wang et al., 1999]. For each synthesis unit in the obtained speech data, the following features are stored: · the waveform and its length, · the code of the synthesis unit.  Multiple-Translation Spotting for Mandarin-Taiwanese Speech-to-Speech Translation 19  4. Translation Phase  4.1 Multiple-Translation Spotting Method  To deal with the problem of spotting between a speech input  X  L 1  and a translation pattern set  { } s  (v j  )  ,  t  (v j  )  J in j =1  the  v-th  translation  template  ( rv  ),  we  use  the  standard  notation  l  to  represent  the frame index of  X  L 1  ,1  ≤  l  ≤  L  ,  j  to  represent  the  spotting  pair  (  s  (v j  )  ,  t  (v j  )  ) index of  rv ,  
Language modeling plays a critical role for automatic speech recognition. Typically, the n-gram language models suffer from the lack of a good representation of historical words and an inability to estimate unseen parameters due to insufficient training data. In this study, we explore the application of latent semantic information (LSI) to language modeling and parameter smoothing. Our approach adopts latent semantic analysis to transform all words and documents into a common semantic space. The word-to-word, word-to-document and document-to-document relations are, accordingly, exploited for language modeling and smoothing. For language modeling, we present a new representation of historical words based on retrieval of the most relevant document. We also develop a novel parameter smoothing method, where the language models of seen and unseen words are estimated by interpolating the k nearest seen words in the training corpus. The interpolation coefficients are determined according to the closeness of words in the semantic space. As shown by experiments, the proposed modeling and smoothing methods can significantly reduce the perplexity of language models with moderate computational cost. Keywords: language modeling, parameter smoothing, speech recognition, and latent semantic analysis. 1. Introduction Language models have been successfully developed for speech recognition, optical character recognition, machine translation, information retrieval, etc. Many studies in the field of speech recognition have focused on this topic [Jelinek 1990, Jelinek 1991]. As shown in Figure 1, a speech recognition system is composed of syllable-level and word-level matching processes, in which the acoustic model λ and language model τ are applied, respectively. In theory, the speech recognition procedure combines the acoustic model and language model according to the Bayes rule. Let O denote the acoustic data, and let W = {w1,L, wl } = w1l denote a string of l * Department of Computer Science and Information Engineering , National Cheng Kung University, Tainan, Taiwan, ROC E-mail: jtchien@mail.ncku.edu.tw  30  Jen-Tzung Chien et al.  words. The speech recognition task aims to find the most likely word string Wˆ by maximizing  the a posteriori probability given the observed acoustic data O:  Wˆ = arg max P(W O) = arg max Pλ (O W )Pτ (W ) ,  (1)  W  W  where Pτ (W ) is the a priori probability of the occurring word string W, and Pλ (OW ) is the probability of observing data O given the word string W. The parameters τ and λ are the language model and speech hidden Markov models (HMM’s), respectively. Hereafter, we will  neglect the notation τ in Pτ (W ) . The language model Pr(W ) aims to measure the probability of word occurrence. This model is employed to predict the word occurrence given the history  words. In an n-gram model, we assume that the probability of a word depends only on the  preceding n-1 words. The N-gram model Pr(W ) is written as  ∏ ∏ Pr(W ) = Pr( w1,..., wl ) =  l  Pr( wq w1 , w2 ,..., wq−1 ) ≅  l  Pr( wq  w  q −1 q−n  +1  )  .  (2)  q =1  q =1  The sequence H q = {w1,L, wq−1} is referred to as the history H q for word wq . To estimate  Pr(wq wqq−−1n+1 ) , we can count the number of words wq following the history words wqq−−1n+1 and divide it by the total number of occurring history words wqq−−1n+1 , i.e.,  ∑ Pr(  wq  w  q −1 q−n  +1  )  =  c  (  w  q q−  n  +1  )  .  c  (  w  q q−  n  +1  )  (3)  wi  This probability estimation is called the maximum likelihood estimation (MLE). The bigram  l  l  ∏ model ∏ Pr(W ) ≅ Pr(wq wq−1 ) and trigram model Pr(W) ≅ Pr(wq wq−2, wq−1) are employed  q =1  q=1  in most speech recognition systems. However, when a word sequence (wq−2 , wq−1, wq ) is not occurs in the training data, the trigram model Pr(wq wq−2 , wq−1) could not be estimated. We may apply parameter smoothing to find the unseen trigram model. In the literature, several smoothing  methods have been proposed to deal with the data sparseness problem [Katz 1987, Kawabata and  Tamoto 1996, Lau et al. 1993, Zhai and Lafferty 2001]. Also, maximum a posteriori adaptation of  the language model has been presented to resolve the problem of domain mismatch between  training and test corpora [Bellegarda 2000a, Federico 1996, Masataki et al. 1997]. Besides the  problems of data sparseness and domain mismatch, the n-gram model is inferior in terms of  characterizing long-distance word relationships. For example, the trigram model is unable to  characterize word dependence beyond the span of three successive words. In [Lau et al. 1993,  Zhou and Lua 1999], the trigram model was improved by extracting word relationships from the document history. This approach was exploited to search the trigger pair, wA → wB , where the appearance of wA in the document history significantly affects the probability of occurring wB . The trigger pairs provide long distance information because the triggering and triggered words  might be separated by several words. However, trigger pair selection neglects the possibility of  Latent Semantic Language Modeling and Smoothing  31  low-frequency word triggers, which might contain useful semantic information. The LSA method was developed to resolve this problem.  Sp e e c h in p ut O  Feat ure ex t r act io n  Feat ure vectors  Sy llablelevel match  Sen t en ce-lev el m at ch  Re co gn ize d sen t en ce Wˆ  Acoust ic m o del (H M M ) λ  Lan guage m o del τ  Figure 1. A schematic diagram of a speech recognition system.  In this paper, a new language modeling and smoothing method is proposed based on the framework of latent semantic analysis (LSA). The traditional n-gram model is weak in terms of characterizing the information in historical words. This weakness is compensated for herein by using the LSA framework, where word-to-word, word-to-document and document-to-document similarities are found in the semantic space. With the use of LSA, all the words are mapped to a common semantic space, which is constructed via the singular value decomposition (SVD) of a word-by-document matrix. Bellegarda [1998, 2000a, 2000b] applied the LSA framework to the n-gram model such that the resulting word error rate and perplexity were substantially reduced. The LSA representation of the history suffers from a drawback in that the representation of the history carries insufficient information at the beginning of a text document. To overcome this problem, we propose a relevance retrieval framework to represent the history. For language model smoothing, we estimate unseen language models by using the seen models corresponding to the k nearest neighbor words. Because this smoothing method extracts synonym and semantic information, it can be also referred to as “semantic smoothing.” In the following section, we briefly introduce the framework of LSA. Section 3 addresses the proposed language modeling and smoothing approaches. The LSA framework is applied to relevance feedback language modeling and k nearest neighbor language smoothing. Section 4 describes the experimental setup and reports the results for the perplexity and computational cost. Finally, we draw conclusions in Section 5.  32  Jen-Tzung Chien et al.  2. Latent semantic analysis  In the literature [Berry et al. 1995, Deerwester et al. 1990, Ricardo and Berthier 2000], latent  semantic analysis (LSA) has been widely applied to vector space based information retrieval.  During the past few years, LSA has also been applied to language model adaptation [Bellegarda  1998, Bellegarda 2000a, Novak and Mammone 2001]. Latent semantic analysis is a dimension  reduction technique that projects the query and document into a common semantic space  [Deerwester et al. 1990, Ding 1999]. This projection reduces the document vector from a high  dimensional space to a low dimensional space, which is referred as the latent semantic space.  The goal is to represent similar documents as close points in the latent semantic space, based on  an appropriate metric. This metric can capture the significant associations between words and  documents. Given an M × N matrix A, with M terms and N documents, M ≥ N and rank (A) =  R. The weighted count ai, j of matrix A is the number of occurrences of each word wi in a document d j , calculated as follows:  ai, j  =  (1  −  ε  i  )  ci, j nj  .  (4)  Here, ci,j is the number of terms wi occurring in document d j , n j is the total number of words in d j , and εi is the normalized entropy of wi in the collection of data consisting of N documents, i.e.,  ∑ ε i  =− 1 log N  N ci, j log ci, j  j=1 ti  ti  ,  (5)  ∑ where ti = j ci. j is the total number of times wi occurs in the collection of data. A value of εi that is close to one occurs in case of ci, j = ti N . This means that the word wi is distributed across many documents throughout the corpus. A value of ε i that is close to zero, i.e., the case in which ci, j = ti , indicates that the word wi is present in only a few documents. Hence, in (4), 1 − ε i represents a global indexing weight for the word wi , and ci, j n j indicates that the word wi occurs in frequently in document d j .  Latent semantic analysis is a conceptual-indexing method, which uses singular value  decomposition (SVD) [Berry et al. 1995, Golub and Van Loan 1989] to find the latent semantic  structure of word to document association. SVD decomposes the matrix A into three  sub-matrices:  A = UΣVT ,  (6)  where U and V are orthogonal matrices, UT U = VT V = I R , and Σ is a diagonal matrix. As shown in Figure 2, the first R columns of U and V, and the first R diagonal elements of Σ can be used to approach A with rank(A) = R by means of AR = U RΣRVRT , where A R is a representative matrix A. The result of SVD is a set of vectors representing the location of each term and document in the reduced R-dimensional LSA space [Berry 1992]. For a given training  Latent Semantic Language Modeling and Smoothing  33  corpus, AAT characterizes all the co-occurrences between words, and AT A characterizes all the co-occurrences between documents. That is, a similar pattern of occurring words wi and w j can be inferred from the (i, j) cell of AAT , and a similar pattern of words contained in documents di and d j can be inferred from the (i, j) cell of AT A [Bellegarda 1998, Bellegarda 1997, Bellegarda 2000a, Chen and Goodman 1999]. This LSA approach performs well when a major portion of the meaningful semantic structure [Deerwester et al. 1990] is captured.  documents  d1  dN  w1  u1  ≅  document vectors  v1  vN  0  ×  ×  0  ( R x R )  ( R x N )  words word vectors  wM  uM  A (MxN)  M x R UR  ΣR  VRT  Figure 2. A diagram of the truncated SVD.  3. New language modeling and smoothing techniques  3.1 LSA Parameter Modeling  N-gram language models are useful for modeling the local dependencies of word occurrences  but not for capturing global word dependencies. The modeling process leads to the estimation of  the conditional probability  Pr(wq  w q −1 q−n  +1  )  ,  which  characterizes  the  linguistic  regularity  in  a  span  of n words. When the window size n is limited, the n-gram is weak in terms of capturing long  distance dependencies. Long distance correlation between words is commonly found in language and is caused by closeness in meaning; e.g., the words “stock” and “fund” are both  likely to occur in financial news. To deal with long distance modeling, the LSA approach can be  applied to extract large span semantic knowledge. Our motivation lies in the fact that there  exists some latent structure in the occurrence patterns of words across documents. Hence, the  n-gram language model can be improved by employing LSA to perform large span prediction of  word occurrence.  Let the word wq denote the predicted word, let H q−1 denote the history for wq , and let Pr(wq H q−1) be the associated language model probability. Using the n-gram language model, we find that H q−1 = {wq−1 , wq−2 ,L, wq−n+1} is the relevant history composed of the preceding n-1 words. The LSA language model is expressed by  34  Jen-Tzung Chien et al.  Pr(wq H q−1) = Pr(wq H q−1, S ) = Pr(wq dq−1 ) ,  (7)  where the conditioning on S reflects the fact that the probability depends on the particular  vector space arising from the SVD representation, and where Pr(wq dq−1) is computed directly based on the closeness of wq and d q−1 in the semantic space S. The vector d q−1 can be viewed as an additional pseudodocument vector for matrix A [Bellegarda 1998, Bellegarda  2000a, Bellegarda 2000b]. The representation v q−1 for the pseudodocument vector d q−1 in the space S is given by  v q−1  =  d  T q  −1UΣ  −1  .  (8)  By referring to (4), we can obtain the pseudodocument vector dq recursively in the LSA space via [Bellegarda 2000a, Bellegarda 2000b]  dq  =  nq − nq  
This paper presents an approach to emotion recognition from speech signals and textual content. In the analysis of speech signals, thirty-three acoustic features are extracted from the speech input. After Principle Component Analysis (PCA) is performed, 14 principle components are selected for discriminative representation. In this representation, each principle component is the combination of the 33 original acoustic features and forms a feature subspace. Support Vector Machines (SVMs) are adopted to classify the emotional states. In text analysis, all emotional keywords and emotion modification words are manually defined. The emotion intensity levels of emotional keywords and emotion modification words are estimated based on a collected emotion corpus. The final emotional state is determined based on the emotion outputs from the acoustic and textual analyses. Experimental results show that the emotion recognition accuracy of the integrated system is better than that of either of the two individual approaches. 1. Introduction Human-machine interface technology has been investigated for several decades. Recent research has placed more emphasis on the recognition of nonverbal information, and has especially focused on emotion reaction. Many kinds of physiological characteristics are used to extract emotions, such as voice, facial expressions, hand gestures, body movements, heartbeat and blood pressure. Scientists have found that emotion technology can be an important component in artificial intelligence [Salovey et al. 1990], especially for human-human communication. Although human-computer interaction is different from human-human communication, some theories show that human-computer interaction shares  * Department of Computer Science and Information Engineering, National Cheng Kung University, Tainan, Taiwan, ROC E-mail: {bala, chwu}@csie.ncku.edu.tw  46  Ze-Jing Chuang, and Chung-Hsien Wu  basic characteristics with human-human interaction [Reeves et al. 1996]. In addition, affective information is pervasive in electronic documents, such as digital news reports, economic reports, e-mail, etc. The conclusions reached by researchers with respect to emotion can be extended to other types of subjective information [Subasic et al. 2001]. For example, education assistance software should be able to detect the emotions of users and; therefore; choose suitable teaching courses. Moreover, the study of emotions can apply to some assistance systems, such as virtual babysitting systems or virtual psychologist systems. In recent years, several research works have focused on emotion recognition. Cohn and Katz [Cohn et al. 1998] developed a semi-automated method for emotion recognition from faces and voices. Silva [Silva et al. 2000] used the HMM structure to recognize emotion from both video and audio sources. Yoshitomi [Yoshitomi et al. 2000] combined the hidden Markov model (HMM) and neural networks to extract emotion from speech and facial expressions. Other researchers focused on extracting emotion from speech data only. Fukuda and Kostov [Fukuda et al. 1999] applied a wavelet/cepstrum-based software tool to perform emotion recognition from speech. Yu [Yu et al. 2001] developed a support vector machine (SVM)-based emotion recognition system. However, few approaches have focused on emotion recognition from textual input. Textual information is another important communication medium and can be retrieved from many sources, such as books, newspapers, web pages, e-mail messages, etc. It is not only the most popular communication medium, but also rich in emotion. With the help of natural language processing techniques, emotions can be extracted from textual input by analyzing punctuation, emotional keywords, syntactic structure, semantic information, etc. In [Chuang et al. 2002], the authors developed a semantic network for performing emotion recognition from textual content. That investigation focused on the use of textual information in emotion recognition systems. For example, the identification of emotional keywords in a sentence is very helpful to decide the emotional state of the sentence. A possible application of textual emotion recognition is the on-line chat system. With many on-line chat systems, users are allowed to communicate with each other by typing or speaking. A system can recognize a user’s emotion and give an appropriate response. In this paper, a multi-modal emotion recognition system is constructed to extract emotion information from both speech and text input. The emotion recognition system classifies emotions according to six basic types: happiness, sadness, anger, fear, surprise and disgust. If the emotion intensity value of the currently recognized emotion is lower than a predefined threshold, the emotion output is determined to be neutral. The proposed emotion recognition system can detect emotions from two different types of information: speech and text. To evaluate the acoustic approach, a broadcast drama, including speech signal and textual content, is adopted as the training corpus instead of artificial emotional speech. During feature selection, an initial acoustic feature set that contained 33 features is first analyzed and  Multi-Modal Emotion Recognition from Speech and Text  47  extracted. These acoustic features contain several possible characteristics, such as intonation, timbre, acoustics, tempo, and rhythm. We also extract some features to represent special intonations, such as trembling speech, unvoiced speech, and crying speech. Finally, among these diverse features, the most significant features are selected by means of principle component analysis (PCA) to form an acoustic feature vector. The acoustic feature vector is fed to the Support Vector Machines (SVMs) to determine the emotion output according to hyperplanes determined by the training corpus. For emotion recognition via text, we assume that the emotional reaction of an input sentence is essentially represented by its word appearance. Two primary word types, “emotional keywords” and “emotion modification words,” are manually defined and used to extract emotion from the input sentence. All the extracted emotional keywords and emotion modification words have their corresponding “emotion descriptors” and “emotion modification values.” For each input sentence, the emotion descriptors are averaged and modified using the emotion modification values to give the current emotion output. Finally, the outputs of the textual and acoustic approaches are combined with the emotion history to give the final emotion output. The rest of the paper is organized as follows. Section 2 describes the module for recognizing emotions from speech signals. The details of SVM classification model is also provided in this section. Then the textual emotion recognition module and the integration of these two modules are presented in sections 2.3 and 3, respectively. Finally, experimental results obtained using the integrated emotion recognition system are provided in section 5, and some conclusions are drawn in section 6. 2. Acoustic Emotion Recognition Module Deciding on appropriate acoustic features is a crucial step in emotion recognition. As in similar research, this study adopts the pitch and energy features and their derivatives. In addition, some additional characteristics may be found in emotional speech, such as trembling speech, unvoiced speech, varying speech duration, and hesitation. These features are also extracted in our approach. 2.1 Feature Extraction A diagram of the acoustic feature extraction approach is shown in Figure 1. In the proposed approach, four basic acoustic features, pitch, energy, formant 1 (F1), and the zero crossing rate (ZCR), are estimated first. Previous research has shown that emotional reactions are strongly related to the pitch and energy of the speech. For example, the pitch of speech associated with anger or happiness is always higher than that associated with sadness or disappointment, and the energy associated with surprise or anger is also greater than that associated with fear.  48  Ze-Jing Chuang, and Chung-Hsien Wu  Figure 1. Diagram of the acoustic feature extraction module. To extract an appropriate feature set, a short-time processing technique is first applied. The contours of the acoustic features are used to represent the time-varying feature characteristics. Each contour can be represented by its mean, slope, and slope difference. The Legendre polynomial [Abramowitz et al. 1972] is adopted to represent the contours of these four features. In feature extraction, we adopt several parameters that are based on pitch and energy. We extract 33 acoustic features in the following 13 categories: (1) 4th-order Legendre parameters for the pitch contour; (2) 4th-order Legendre parameters for the energy contour; (3) 4th-order Legendre parameters for the formant one (F1) contour; (4) 4th-order Legendre parameters for the zero crossing rate (ZCR) contour; (5) maximum energy; (6) maximum smoothed energy; (7) minimum, median, and standard deviation of the pitch contour; (8) minimum, median, and standard deviation of the energy contour; (9) minimum, median, and standard deviation of the smoothed pitch contour; (10) minimum, median, and standard deviation of the smoothed energy contour; (11) ratio of the sample number of the upslope to that of the downslope for the pitch contour; (12) ratio of the sample number of upslope to that of the downslope for the energy contour; (13) pitch vibration. The features in categories (1) to (8) are statistical parameters of four basic acoustic features. In order to remove discontinuities from the contour, the pitch and energy features in categories (9) and (10) are smoothed using window method.  Multi-Modal Emotion Recognition from Speech and Text  49  Figure 2. The ratio of up-slope sample number to the down-slope sample number. Two contours with the same wavelength are shown in parts A and B; the square symbols indicate the up-slope sample, and the circle symbols indicate the down-slope sample.  The ratios described in categories (11) and (12) represent not only the slope but also the shape of each vibration in the contour. Figure 2 shows the difference between these parameters. In this figure, each part shows the vibration of a contour. In order to show how the parameters are used, we assume that the length and the amplitude of these two contours are the same. In part A, the length of the upslope contour is longer than that of the downslope contour, while the opposite is shown in part B. The ratio of upslope to downslope is 3.14 (22 upslope samples to 7 downslope samples) in part A and 0.26 (6 upslope samples to 23 downslope samples) in part B.  Trembling speech can be characterized by means of pitch vibration. For category (13),  the pitch vibration is defined and calculated as follows:  Pr  =  
This paper presents an effective method for improving the performance of a speaker identification system. Based on the multiresolution property of the wavelet transform, the input speech signal is decomposed into various frequency bands in order not to spread noise distortions over the entire feature space. To capture the characteristics of the vocal tract, the linear predictive cepstral coefficients (LPCCs) of each band are calculated. Furthermore, the cepstral mean normalization technique is applied to all computed features in order to provide similar parameter statistics in all acoustic environments. In order to effectively utilize these multiband speech features, we use feature recombination and likelihood recombination methods to evaluate the task of text-independent speaker identification. The feature recombination scheme combines the cepstral coefficients of each band to form a single feature vector used to train the Gaussian mixture model (GMM). The likelihood recombination scheme combines the likelihood scores of the independent GMM for each band. Experimental results show that both proposed methods achieve better performance than GMM using full-band LPCCs and mel-frequency cepstral coefficients (MFCCs) when the speaker identification is evaluated in the presence of clean and noisy environments. Keywords: speaker identification, wavelet transform, linear predictive cepstral coefficient (LPCC), mel-frequency cepstral coefficient (MFCC), Gaussian mixture model (GMM). 1. Introduction In general, speaker recognition can be divided into two parts: speaker verification and speaker * Department of Electrical Engineering, Tamkang University, Taipei, Taiwan, Republic of China + Department of Electronic Engineering, St. John's & St. Mary's Institute of Technology, Taipei, Taiwan, Republic of China E-mail: steven@mail.sjsmit.edu.tw, hsieh@ee.tku.edu.tw, elai@ee.tku.edu.tw  64  Wan-Chen Chen et al.  identification. Speaker verification refers to the process of determining whether or not the speech samples belong to some specific speaker. However, in speaker identification, the goal is to determine which one of a group of known voices best matches the input voice sample. Furthermore, in both tasks, the speech can be either text-dependent or text-independent. Textdependent means that the text used in the test system must be the same as that used in the training system, while text-independent means that no limitation is placed on the text used in the test system. Certainly, the method used to extract and model the speaker-dependent characteristics of a speech signal seriously affects the performance of a speaker recognition system. Many researches have been done on the feature extraction of speech. The linear predictive cepstral coefficients (LPCCs) were used because of their simplicity and effectiveness in speaker/speech recognition [Atal 1974, White and Neely 1976]. Other widely used feature parameters, namely, the mel-frequency cepstral coefficients (MFCCs) [Vergin et al. 1999], were calculated by using a filter-bank approach, in which the set of filters had equal bandwidths with respect to the mel-scale frequencies. This method is based on the fact that human perception of the frequency contents of sounds does not follow a linear scale. The above two most commonly used feature extraction techniques do not provide invariant parameterization of speech; the representation of the speech signal tends to change under various noise conditions. The performance of these speaker identification systems may be severely degraded when a mismatch between the training and testing environments occurs. Various types of speech enhancement and noise elimination techniques have been applied to feature extraction. Typically, the nonlinear spectral subtraction algorithms [Lockwood and Boudy 1992] have provided only minor performance gains after extensive parameter optimization. Furui [1981] used the cepstral mean normalization (CMN) technique to eliminate channel bias by subtracting off the global average cepstral vector from each cepstral vector. Another way to minimize the channel filter effects is to use the time derivatives of cepstral coefficients [Soong and Rosenberg 1988]. Cepstral coefficients and their time derivatives are used as features in order to capture dynamic information and eliminate timeinvariant spectral information that is generally attributed to the interposed communication channel. Conventionally, feature extraction is carried out by computing acoustic feature vectors over the full band of the spectral representation of speech. The major drawback of this approach is that even partial band-limited noise corruption affects all the feature vector components. The multiband approach deals with this problem by performing acoustic feature analysis independently on a set of frequency subbands [Hermansky et al. 1996]. Since the resulting coefficients are computed independently, a band-limited noise signal does not spread over the entire feature space. In our previous works [Hsieh and Wang 2001, Hsieh et al. 2002,  Multiband Approach to Robust Text- Independent Speaker Identification  65  2003], we proposed a multiband feature extraction method in which features from various subbands and the full band are combined to form a single feature vector. This feature extraction method was evaluated in a speaker identification system using vector quantization (VQ), group vector quantization, and the Gaussian mixture model (GMM) as identifiers. The experimental results showed that this multiband feature is more effective and robust than the full-band LPCC and MFCC features, particularly in noisy environments. In past studies on recognition models, VQ [Soong et al. 1985, Buck et al. 1985, Furui 1991], dynamic time warping (DTW) [Furui 1981], the hidden Markov model (HMM) [Poritz 1982, Tishby 1991], and GMM [Reynolds and Rose 1995, Alamo et al. 1996, Pellom and Hansen 1998, Miyajima et al. 2001] were used to perform speaker recognition. The DTW technique is effective in text-dependent speaker recognition, but it is not suitable for textindependent speaker recognition. HMM is widely used in speech recognition, and it is also commonly used in text-dependent speaker verification. It has been shown that VQ is very effective for speaker recognition. Although the performance of VQ is not as good as that of GMM [Reynolds and Rose 1995], VQ is computationally more efficient than GMM. GMM [Reynolds and Rose 1995] provides a probabilistic model of the underlying sounds of a person’s voice. It is computationally more efficient than HMM and has been widely used in text-independent speaker recognition. In this study, the multiband linear predictive cepstral coefficients (MBLPCCs) proposed previously [Hsieh and Wang 2001, Hsieh et al. 2002, 2003] are used as the front end of the speaker identification system. Then, cepstral mean normalization is applied to these multiband speech features to provide similar parameter statistics in all acoustic environments. In order to effectively utilize these multiband speech features, we use feature recombination and likelihood recombination methods in the GMM recognition models to evaluate the task of text-independent speaker identification. The experimental results show that the proposed multiband methods outperform GMM using full-band LPCC and MFCC features. This paper is organized as follows. The proposed algorithm for extracting speech features is described in section 2. Section 3 presents the multiband speaker recognition models. Experimental results and comparisons with the conventional full-band GMM are presented in section 4. Concluding remarks are made in section 5. 2. Multiband Features Based on Wavelet Transform The recent interest in the multiband feature extraction approach has mainly been attributed to Allen’s paper [Allen 1994], where it is argued that the human auditory system processes features from different subbands independently, and that the merging is done at some higher point of processing to produce a final decision. The advantages of using multiband processing  66  Wan-Chen Chen et al.  are multifold and have been described in earlier publications [Bourlard and Dupont 1996, Tibrewala and Hermansky 1997, Mirghafori and Morgan 1998]. The major drawback of a pure subband-based approach may be that information about the correlation among various subbands is lost. Therefore, we suggest that full-band features should not be ignored, but should be combined with subband features to maximize recognition accuracy. A similar approach that combines information from the full band and subbands at the recognition stage was found to improve recognition performance [Mirghafori and Morgan 1998]. It is not a trivial matter to decide at which temporal level the subband features should be combined. In the multiband approach [Bourlard and Dupont 1996, Tibrewala and Hermansky 1997], different classifiers for each band are used, and likelihood recombination is done at the HMM state, phone or word level. In another approach [Okawa et al. 1998, Hariharan et al. 2001], the individual features of each subband are combined into a single feature vector prior to decoding. In our approach, the full band and subband features are also used in the recognition model. Based on time-frequency multiresolution analysis, the effective and robust MBLPCC features are used as the front end of the speaker identification system. First, the LPCCs are extracted from the full-band input signal. Then the wavelet transform is applied to decompose the input signal into two frequency subbands: a lower frequency subband and a higher frequency subband. To capture the characteristics of an individual speaker, the LPCCs of the lower frequency subband are calculated. There are two main reasons for using the LPCC parameters: their good representation of the envelope of the speech spectrum of vowels, and their simplicity. Based on this mechanism, we can easily extract the multiresolution features from all lower frequency subband signals simply by iteratively applying the wavelet transform to decompose the lower frequency subband signals, as depicted in Figure 1. As shown in Figure 1, the wavelet transform can be realized by using a pair of finite impulse response (FIR) filters, h and g, which are low-pass and high-pass filters, respectively, and by performing the down-sampling operation (↓2). The down-sampling operation is used to discard the oddnumbered samples in a sample sequence after filtering is performed.  V 0  g  ↓2  W1  h  ↓2  g  ↓2  W 2  
In recent years, the rapid growth of wireless communications has undoubtedly increased the need for speech recognition techniques. In wireless environments, the portability of a computationally powerful device can be realized by distributing data/information and computation resources over wireless networks. Portability can then evolve through personalization and humanization to meet people’s needs. An innovative distributed speech recognition (DSR) [ETSI, 1998],[ETSI, 2000] platform, configurable DSR (C-DSR), is thus proposed here to enable various types of wireless devices to be remotely configured and to employ sophisticated recognizers on servers operated over wireless networks. For each recognition task, a configuration file, which contains information regarding types of services, types of mobile devices, speaker profiles and recognition environments, is sent from the client side with each speech utterance. Through configurability, the capabilities of configuration, personalization and humanization can be easily achieved by allowing users and advanced users to be involved in the design of unique speech interaction functions of wireless devices. Keywords: Distributed, speech recognition, configurable, wireless, portable, personalized, humanized. 1. Introduction In the current wireless era, cellular phones have become daily-life necessities. People carry their own handsets and make phone calls anytime, everywhere, while public payphones have  * Ph.D, Senior Researcher, Advanced Technology Center, Computer and Communications Research  Laboratories, Industrial Technology Research Institutes  E-mail: YinPinYang@itri.org.tw  TEL: 886-3-5914830  FAX: 886-3-5820098  Address: E000 CCL/ITRI Rm.712, Bldg.51, 195 Sec.4, Chung Hsing Rd., Chutung, Hsinchu 310,  Taiwan.  78  Yin-Pin Yang  almost disappeared. Inspired by this vast number of mobile phone users, the wireless communication industry is developing wireless data services to create more profit. Wireless devices can be treated as terminals of an unbounded information/data network – the Internet. However, the small screen sizes of mobile devices discourage users from surfing the Internet in mobile situations. Wireless data services are not as attractive as was expected, and this is one of the major reasons for the so-called “3G Bubble” [Baker, 2002][Reinhardt et al, 2001]. On the other hand, the handset market is still blooming. Personal, stylish and fashionable features, such as ring tones, color screen displays, covers, and so on, are all very popular, especially among teenagers. Functionally speaking, portable devices, such as PDAs, pocket/palm PCs and digital cameras, are now integrated with handsets. Many interesting applications, such as portable electronic dictionaries, map navigators, and mobile learning, can be built into mobile devices. However, these functions or services still cannot create serious business opportunities for telecom companies. What will future appealing services for cell phones be? “Talking to a machine,” or interacting with a machine, might be a candidate. That is, besides talking to human-beings through voice channels, people may like to talk to machines and access the Internet through data channels. The possibilities are unlimited. Handsets may thus evolve into personal “intimate pets” that people will use from childhood to grownup. In this scenario, speech interaction will play an important part in humanizing devices [Hiroshi et al. 2003]. However, due to the limitations of the current state-of-art speech recognition techniques, the robustness issue [Deng et al. 2003][Wu et al. 2003][Lee 1998] is always a bottleneck in commercializing speech recognition products. This imperfection reveals the importance of configurability. In the following paragraphs, the relationships among configurability, personalization, and wireless environments will be explored. Speech Recognition and Wireless Environments How does a speech recognition system fit into a the wireless network? In this paper, we will primarily highlight two key terms “distributed” and “configurable.” The term “distributed” can be interpreted as follows: computation distributed and data distributed. As for the former, normally speech recognition functions are needed in mobile situations, and devices are usually thin and lacking in computational power. It would be much easier to design speech recognition functions if the computation involved in recognition processes would be distributed over wireless networks by means of a client-server architecture. As for the latter, speech recognition is by nature a pattern matching process, which needs to acquire utterances within a given application domain. For example, a speaker-independent (SI) continuous digit recognizer targeting the Taiwanese market needs to acquire a great large number of sample continuous digit utterances from all dialects in this market. The representation and quality of  An Innovative Distributed Speech Recognition Platform for Portable,  79  Personalized and Humanized Wireless Devices  the sample utterances used for training or adaptation can seriously dominate the performance of a speech recognizer. If a wireless network is used, speech data acquisition can be done in a much more efficient and systematical way. More importantly, the acquired speech data, labeled by means of a speaker profile, recognition environment, and device/microphone type, can be kept on the server. Speech data will thus not be abandoned when particular applications or services are discontinued. From the above, we can conclude that we need a centralized speech recognition server embedded in the wireless infrastructure. When we say “talking to a machine”, the “machine” is actually an entire wireless network. People talk to the same lifetime recognizer, and the recognizer evolves continuously. This speech recognition server can provide any types speech recognition services (computation distributed) for all classes of wireless mobile devices. These services continuously acquire speech data from all locations (data distributed), and adapt the engine performance all the time. For each recognition task, there is a “configuration file” (or, say, a tag) to record all of the information regarding types of services, speaker profiles, recognition environments, etc. We call this type of server a configurable distributed speech recognition (C-DSR) server. In the following, the history of DSR developed by ETSI/Aurora will be briefly described. Then, the innovative C-DSR platform will be introduced.  Distributed Speech Recognition (DSR) developed by ETSI/Aurora  Instead of squeezing the whole recognizer into a thin device, it seems more reasonable to host recognition tasks on a server and exchange information between the client and server. However, due to the low bit-rates of speech coders (note that coders are designed for humans, not recognizers), the speech recognition performance can be significantly degraded. The DSR, proposed by ETSI Aurora, overcomes these problems by distributing the recognition process between the client and server, by using an error protected data channel to send parameterized speech features.  From DSR to Configurable DSR (C-DSR)  Aurora DSR can be seen as a speech “coder” [Digalakis et al. 1999] designed to enable handset users to talk to their recognizers. Besides handsets, there are many other mobile devices that need DSR services, and they all operate in different environments and in different recognition modes. Each combination or, say, configuration, needs its own “coder” to achieve better performance. Based on these needs, C-DSR was built as an integrated client-server platform which not only offers a convenient way to construct speech recognition functions on various client devices, but also provides powerful utilities/tools to assist each configuration to  80  Yin-Pin Yang  obtain its own coder in order to increase the overall recognition task completion rate. To achieve these goals, C-DSR maximizes the advantages of data channels and centralized servers by means of its “configurable” capability - configurability. The configurability can be considered from two points of views. The C-DSR Client From the client side viewpoint, speech recognition processing is configurable to work with: (1) various kinds of thin to heavy mobile devices, ranked according to their computational power, noting that most of devices do not have sufficient computational power to perform the feature extraction process proposed by ETSI Aurora; (2) various types of recognition environments, such as offices, homes, streets, cars, airports, etc; this information about recognition environments can help recognition engines achieve greater accuracy; (3) various types of recognition services, such as command-based, grammar-based, speaker independent/ de-pendent mixed mode, and dialogue style services; (4) various speaker profiles since speaker information can help recognizers achieve higher recognition rates 1 and are required by recognition applications, such as speaker adaptation [Lee et al.1999][Chen et al.1990], speaker verifications identification [Siohan et al.1998]. The C-DSR platform provides a faster and more flexible way to construct various speech recognition functions for various mobile devices used in various recognition environments. One of the major missions of C-DSR is to increase the frequency of speech recognition use in daily life. The C-DSR Server From the viewpoint of the centralized server, the C-DSR server collects from all of the registrant clients speech utterances or formatted speech feature arrays along with their configuration tags. The basic idea is to formalize the life cycle of a speech recognition product/task from the deployment phase, to the diagnostic phase, tuning phase, and upgrading phase. Also, similar tasks can share corresponding information and adaptation data located on the server. The C-DSR server offers the following mechanisms to fully take advantage of these categorized speech and configuration data: (1) the C-DSR server can decide which recognition engine or which acoustic HMM model to employ according to the history log; (2) the C-DSR server can balance the trade-offs among communication bandwidth, system load and recognition accuracy; (3) the categorized and organized speech database can be utilized to create diagnostic tools that can be used to tune-up recognition engines and to perform all kinds  
Anaphora is a common phenomenon in discourses as well as an important research issue in the applications of natural language processing. In this paper, anaphora resolution is achieved by employing WordNet ontology and heuristic rules. The proposed system identifies both intra-sentential and inter-sentential antecedents of anaphors. Information about animacy is obtained by analyzing the hierarchical relations of nouns and verbs in the surrounding context. The identification of animacy entities and pleonastic-it usage in English discourses are employed to promote resolution accuracy. Traditionally, anaphora resolution systems have relied on syntactic, semantic or pragmatic clues to identify the antecedent of an anaphor. Our proposed method makes use of WordNet ontology to identify animate entities as well as essential gender information. In the animacy agreement module, the property is identified by the hypernym relation between entities and their unique beginners defined in WordNet. In addition, the verb of the entity is also an important clue used to reduce the uncertainty. An experiment was conducted using a balanced corpus to resolve the pronominal anaphora phenomenon. The methods proposed in [Lappin and Leass, 94] and [Mitkov, 01] focus on the corpora with only inanimate pronouns such as “it” or “its”. Thus the results of intra-sentential and inter-sentential anaphora distribution are different. In an experiment using Brown corpus, we found that the distribution proportion of intra-sentential anaphora is about 60%. Seven heuristic rules are applied in our system; five of them are preference rules, and two are constraint rules. They are derived from syntactic, semantic, pragmatic conventions and from the analysis of training data. A relative measurement indicates that about 30% of the errors can be eliminated by applying heuristic module. * Department of Computer and Information Science, National Chiao Tung University, Hsinchu, Taiwan Email: tliang@cis.nctu.edu.tw; gis90507@cis.nctu.edu.tw  22  Tyne Liang and Dian-Song Wu  1. Introduction 1.1 Problem description Anaphora resolution is vital in applications such as machine translation, summarization, question-answering systems and so on. In machine translation, anaphora must be resolved in the case of languages that mark the gender of pronouns. One main drawback with most current machine translation systems is that the translation produced usually does not go beyond the sentence level and, thus, does not successfully deal with discourse understanding. Inter-sentential anaphora resolution would, thus, be of great assistance in the development of machine translation systems. On the other hand, many automatic text summarization systems apply a scoring mechanism to identify the most salient sentences. However, the task results are not always guaranteed to be coherent with each other. This could lead to errors if a selected sentence contained anaphoric expressions. To improve accuracy in extracting important sentences, it is essential to solve the problem of anaphoric references beforehand. Pronominal anaphora, where pronouns are substituted by previously mentioned entities, is a common phenomenon. This type of anaphora can be further divided into four subclasses, namely: nominative: {he, she, it, they}; reflexive: {himself, herself, itself, themselves}; possessive: {his, her, its, their}; objective: {him, her, it, them}. However, “it” can also be a non-anaphoric expression which does not refer to any previously mentioned item, in which case it is called an expletive or the pleonastic-it [Lappin and Leass, 94]. Although pleonastic pronouns are not considered anaphoric since they do not have antecedents to refer to, recognizing such occurrences is, nevertheless, essential during anaphora resolution. In [Mitkov, 01], non-anaphoric pronouns were found to constitute 14.2% of a corpus of 28,272 words. Definite noun phrase anaphora occurs where the antecedent is referred by a general concept entity. The general concept entity can be a semantically close phrase, such as a synonym or super-ordinates of the antecedent [Mitkov, 99]. The word one has a number of different usages apart from counting. One of its important functions is as an anaphoric form. For example:  Intra-sentential anaphora means that the anaphor and the corresponding antecedent occur  Automatic Pronominal Anaphora Resolution in English Texts  23  in the same sentence. Inter-sentential anaphora means the antecedent occurs in a sentence prior to the sentence with the anaphor. In [Lappin and Leass, 94], there were 15.9% inter-sentential cases and 84.1% intra-sentential cases in the testing results. In [Mitkov, 01], there were 33.4% inter-sentential cases and 66.6% intra-sentential cases. Traditionally, anaphora resolution systems have relied on syntactic, semantic or pragmatic clues to identify the antecedent of an anaphor. Hobbs’ algorithm [Hobbs, 76] was the first syntax-oriented method presented in this research domain. From the result of a syntactic tree, they checked the number and gender agreement between antecedent candidates and a specified pronoun. In RAP (Resolution of Anaphora Procedure) proposed by Lappin and Leass [94], an algorithm is applied to the syntactic representations generated by McCord's Slot Grammar parser, and salience measures are derived from the syntactic structure. It does not make use of semantic information or real world knowledge in choosing among the candidates. A modified version of RAP system was proposed by [Kennedy and Boguraev, 96]. It employed only part-of-speech tagging with a shallow syntactic parse indicating the grammatical roles of NPs and their containment in adjuncts or noun phrases. Cardie et al. [99] treated coreferencing as a clustering task. Then a distance metric function was used to decide whether two noun phrases were similar or not. In [Denber, 98], an algorithm called Anaphora Matcher (AM) was implemented to handle inter-sentential anaphora in a two-sentence context. This method uses information about the sentence as well as real world semantic knowledge obtained from other sources. The lexical database system WordNet is utilized to acquire semantic clues about the words in the input sentences. It is noted that anaphora do not refer back more than one sentence in most cases. Thus, a two-sentence “window size” is sufficient for anaphora resolution in the domain of image queries. A statistical approach to disambiguate pronoun “it” in sentences was introduced in [Dagan and Itai, 90]. The disambiguation is based on the co-occurring patterns obtained from a corpus to find the antecedent. The antecedent candidate with the highest frequency in the co-occurring patterns is selected as a match for the anaphor. A knowledge-poor approach was proposed in [Mitkov, 98]; it can be applied to different languages (English, Polish, and Arabic). The main components of this method are the so-called “antecedent indicators” which are used to assign a score (2, 1, 0, -1) for each candidate noun phrase. The scores play a decisive role in tracking down the antecedent from a set of possible candidates. CogNIAC (COGnition eNIAC) [Baldwin, 97] is a system developed at the University of Pennsylvania to resolve pronouns using limited knowledge and linguistic resources. It is a high precision pronoun resolution system that is capable of achieving more than 90% precision with 60% recall for some pronouns. Mitkov [02] presented  24  Tyne Liang and Dian-Song Wu  a new, advanced and completely revamped version of his own knowledge-poor approach to pronoun resolution. In contrast to most anaphora resolution approaches, the system called MARS operates in the fully automatic mode. Three new indicators included in MARS are Boost Pronoun, Syntactic Parallelism and Frequent Candidates. In [Mitkov, 01], the authors proposed an evaluation environment for comparing anaphora resolution algorithms. Performances are illustrated by presenting the results of a comparative evaluation conducted on the basis of several evaluation measures. Their testing corpus contained 28,272 words, with 19,305 noun phrases and 422 pronouns, of which 362 were anaphoric expressions. The overall success rate calculated for the 422 pronouns found in the texts was 56.9% for Mitkov’s method, 49.72% for Cogniac and 61.6% for Kennedy and Boguraev’s method. 2. System Architecture 2.1 Proposed System Overview Graphic User Interface  Text Input  POS Tagging  Preference  Pleonastic It  NP Finder  Constraint  Candidate Set Number Agreement  Animacy Agreement Gender Agreement  Figure 1. Architecture overview. The procedure used to identify antecedents is described as follows:  WordNet Name Data  Automatic Pronominal Anaphora Resolution in English Texts  25  1. Each text is parsed into sentences and tagged by POS tagger. An internal  representation data structure with essential information (such as sentence offset, word  offset, word POS, base form, etc.) is stored.  2. Base noun phrases in each sentence are identified by NP finder module and stored in a  global data structure. Then the number agreement is applied to the head noun.  Capitalized nouns in the name gazetteer are tested to find personal names. A name  will be tagged with the gender feature if it can be found uniquely in male or female  class defined in gender agreement module. In this phase, WordNet is also used to find  possible gender clues for improving resolution performance. The gender attribute is  ignored to avoid ambiguity when the person name can be masculine or feminine.  3. Anaphors are checked sequentially from the beginning of the first sentence. They are  stored in a list with sentence offset and word offset information. Then pleonastic-it is  checked so that no further attempts at resolution are made.  4. The remaining noun phrases preceding the anaphor within a predefined window size  are collected as antecedent candidates. Then the candidate set is further filtered by  means of gender and animacy agreement.  5. The remaining candidates are then evaluated by means of heuristic rules. These rules  can be classified as preference rules or constraint rules. A scoring equation (equation  1) is used to evaluate how likely it is that a candidate will be selected as the  antecedent. The scoring equation calculates the accumulated score of each possible  candidate. The parameter agreementk denotes number agreement, gender agreement  and animacy agreement output. If one of these three outputs indicates disagreement,  the score will be set to zero. The parameter value enclosed in parentheses is the  accumulated number of rules that fit our predefined heuristic rules:  ∑ ∑ ∏ score where  (can  ,  ana  )  =  ⎜⎜⎝⎛  i  rule _ pre i −  j  rule _ con j ⎟⎟⎠⎞ ×  k  agreement k ,  (1)  can: each candidate noun phrase for the specified anaphor; ana: anaphor to be resolved; rule_prei: the ith preference rule; rule_coni: the ith constraint rule; agreementk: denotes number agreement, gender agreement and animacy agreement.  2.2 Main Components 2.2.1 POS Tagging The TOSCA-ICLE tagger [Aarts et al., 97] has been used to lemmatize and tag English learner corpora. The TOSCA-ICLE tag set consists of 16 major word classes. These major word  26  Tyne Liang and Dian-Song Wu  classes may be further specified by means of features of subclasses as well as a variety of syntactic, semantic and morphological characteristics. 2.2.2 NP Finder According to the part-of-speech result, the basic noun phrase patterns are found to be as follows: base NP → modifier＋head noun modifier → <article| number| present participle| past participle |adjective| noun> At the beginning, our system identifies base noun phrases that contain no other smaller noun phrases within them. For example, the chief executive officer of a financial company is divided into the chief executive officer and a financial company for the convenience of judging whether the noun phrase is a prepositional noun phrase or not. This could be of help in selecting a correct candidate for a specific anaphor. Once the final candidate is selected, the entire modifier is combined together again. The proposed base noun phrase finder is implemented based on a finite state machine (Figure 2). Each state indicates a particular part-of-speech of a word. The arcs between states indicate a word input from the first word of the sentence. If a word sequence can be recognized from the initial state and ends in a final state, it is accepted as a base noun phrase with no recursion; otherwise, it is rejected. An example of base noun phrase output is illustrated in Figure 3.  Figure 2. Finite state machine for a noun phrase. Figure 3. An example output of a base noun phrase.  Automatic Pronominal Anaphora Resolution in English Texts  27  2.2.3 Pleonastic-it Module The pleonastic-it module is used to filter out those semantic empty usage conditions which are essential for pronominal anaphora resolution. A word “it” is said to be pleonastic when it is used in a discourse where the word does not refer to any antecedent. References of “pleonastic-it” can be classified as state references or passive references [Denber, 98]. State references are usually used for assertions about the weather or the time, and this category is further divided into meteorological references and temporal references. Passive references consist of modal adjectives and cognitive verbs. Modal adjectives (Modaladj) like advisable, convenient, desirable, difficult, easy, economical, certain, etc. are specified. The set of modal adjectives is extended by adding their comparative and superlative forms. Cognitive verbs (Cogv), on the other hand, are words like anticipate, assume, believe, expect, know, recommend, think, etc. Most instances of "pleonastic-it" can be described by the following patterns: 1. It is Modaladj that S. 2. It is Modaladj (for NP) to VP. 3. It is Cogv-ed that S. 4. It seems/appears/means/follows (that) S. 5. NP makes/finds it Modaladj (for NP) to VP. 6. It is time to VP. 7. It is thanks to NP that S. 2.2.4 Number Agreement The quantity of a countable noun can be singular (one entity) or plural (numerous entities). It makes the process of deciding on candidates easier since they must be consistent in number. With the output of the specific tagger, all the noun phrases and pronouns are annotated with number (single or plural). For a specified pronoun, we can discard those noun phrases that differ in number from the pronoun. 2.2.5 Gender Agreement The gender recognition process can deal with words that have gender features. To distinguish the gender information of a person, we use an English first name list collected from (http://www.behindthename.com/) covering 5,661 male first name entries and 5,087 female ones. In addition, we employ some useful clues from WordNet results by conducting keyword search around the query result. These keywords can be divided into two classes：  28  Tyne Liang and Dian-Song Wu  Class_Female= {feminine, female, woman, women} Class_Male= {masculine, male, man, men}  2.2.6 Animacy Agreement  Animacy denotes the living entities which can be referred to by some gender-marked pronouns (he, she, him, her, his, hers, himself, herself) in texts. Conventionally, animate entities include people and animals. Since it is hard to obtain the property of animacy with respect to a noun phrase by its surface morphology, we use WordNet [Miller, 93] to recognize animate entities in which a noun can only have one hypernym but can have many hyponyms. With twenty-five unique beginners, we observe that two of them can be taken as representations of animacy. These two unique beginners are {animal, fauna} and {person, human being}. Since all the hyponyms inherit properties from their hypernyms, the animacy of a noun can be determined by making use of this hierarchical relation. However, a noun may have several senses, depending on the context. The output result with respect to a noun must be employed to resolve this problem. First of all, a threshold value t_noun is defined (equation 2) as the ratio of the number of senses in animacy files to the number of total senses. This threshold value can be obtained by training a corpus, and the value is selected when the accuracy rate reaches its maximum:  t _ noun = the _ number _ of _ senses _ in _ animacy _ files ,  (2)  the _ total _ senses _ of _ the _ noun  t _ verb = the _ number _ of _ senses _ in _ animacy _ files ,  (3)  the _ total _ senses _ of _ the _ verb  accuracy = the _ number _ of _ animacy _ entities _ identified _ correctly . (4) the _ total _ number _ of _ animacy _ entities  Besides the noun hypernym relation, unique beginners of verbs are also taken into consideration. These lexicographical files with respect to verb synsets are {cognition}, {communication}, {emotion}, and {social} (Table 1). The sense of a verb, for example “read,” varies from context to context as well. We can also define a threshold value t_verb as the ratio of the number of senses in animacy files (Table 1) to the number of total senses.  Table 1. Example of an animate verb.  Unique beginners {cognition}  Example of verb Think, analyze, judge …  {communication}  Tell, ask, teach …  {emotion}  Feel, love, fear …  {social}  Participate, make, establish …  Automatic Pronominal Anaphora Resolution in English Texts  29  The training data that we obtained from the Brown corpus consisted of 10,134 words, 2,155 noun phrases, and 517 animacy entities. We found that 24% of the noun phrases in the corpus referred to animate entities, whereas 76% of them referred to inanimate ones. We utilized the ratio of senses from the WordNet output to decide whether the entity was an animate entity or not. Therefore, the ratio of senses in the noun and its verb is obtained in the training phase to achieve the highest possible accuracy. Afterwards, the testing phase makes use of these two threshold values to decide on the animate feature. Threshold values can be obtained by training on the corpus and selecting the value when the accuracy rate (equation 4) reaches its maximum. Therefore, t_noun and t_verb were found to be 0.8 and 0.9, respectively, according to the distribution in Figure 4. 100 80 60 40 20 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 t_noun t_verb Figure 4. Thresholds of Animacy Entities. The process of determining whether a noun phrase is animate or inanimate is described below：  accuracy  30  Tyne Liang and Dian-Song Wu  2.2.7 Heuristic Rules I. Syntactic parallelism rule The syntactic parallelism of an anaphor and an antecedent could be an important clue when other constraints or preferences can not be employed to identify a unique unambiguous antecedent. The rule reflects the preference that the correct antecedent has the same part-of-speech and grammatical function as the anaphor. Nouns can function grammatically as subjects, objects or subject complements. The subject is the person, thing, concept or idea that is the topic of the sentence. The object is directly or indirectly affected by the nature of the verb. Words which follow verbs are not always direct or indirect objects. After a particular kind of verb, such as verb “be”, nouns remain in the subjective case. We call these subjective completions or subject complements. For example: The security guard took off the uniform after getting off duty. He put it in the bottom of the closet. “He” (the subject) in the second sentence refers to “The security guard,” which is also the subject of the first sentence. In the same way, “it” refers to “the uniform,” which is the object of the first sentence. Empirical evidence also shows that anaphors usually match their antecedents in terms of their syntactic functions. II. Semantic parallelism rule This preference works by identifying collocation patterns in which anaphora appear. In this way, the system can automatically identify semantic roles and employ them to select the most appropriate candidate. Collocation relations specify the relations between words that tend to co-occur in the same lexical contexts. The rule emphasizes that those noun phrases with the same semantic roles as the anaphor are preferred answer candidates. III. Definiteness rule Definiteness is a category concerned with the grammaticalization of the identifiability and non-identifiability of referents. A definite noun phrase is a noun phrase that starts with the word "the"; for example, "the young lady" is a definite noun phrase. Definite noun phrases which can be identified uniquely are more likely to be antecedents of anaphors than indefinite noun phrases. IV. Mention Frequency rule Recurring items in a context are regarded as likely candidates for the antecedent of an anaphor. Generally, high frequency items indicate the topic as well as the most likely candidate.  Automatic Pronominal Anaphora Resolution in English Texts  31  V. Sentence recency rule Recency information is employed in most of the implementations of anaphora resolution. In [Lappin, 94], the recency factor is the one with the highest weight among a set of factors that influence the choice of antecedent. The recency factor states that if there are two (or more) candidate antecedents for an anaphor, and that all of these candidates satisfy the consistency restrictions for the anaphor (i.e., they are qualified candidates), then the most recent one (the one closest to the anaphor) is chosen. In [Mitkov et al., 01], the average distance (within a sentence) between the anaphor and the antecedent was found to be 1.3, and the average distance for noun phrases was found to be 4.3 NPs. VI. Non-prepositional noun phrase rule A noun phrase not contained in another noun phrase is considered a possible candidate. This condition can be explained from the perspective of functional ranking: subject > direct object > indirect object. A noun phrase embedded in a prepositional noun phrase is usually an indirect object. VII. Conjunction constraint rule Conjunctions are usually used to link words, phrases and clauses. If a candidate is connected with an anaphor by a conjunction, the anaphora relation is hard to be constructed between these two entities. For example: Mr. Brown teaches in a high school. Both Jane and he enjoy watching movies on weekends. 2.3 The Brown Corpus The training and testing texts were selected randomly from the Brown corpus. The Corpus is divided into 500 samples of about 2000 words each. The samples represent a wide range of styles and varieties of prose. The main categories are listed in Figure 5.  Figure 5. Categories of the Brown corpus.  32  Tyne Liang and Dian-Song Wu  2.4 System functions The main system window is shown in Figure 6. The text editor is used to input raw text without any annotations and to show the analysis result. The POS tagger component takes the input text and outputs tokens, lemmas, most likely tags and the number of alternative tags. The NP chunker makes use of a finite state machine (FSM) to recognize strings which belong to a specified regular set.  Figure 6. The main system window.  Figure 7. Anaphora pairs.  Automatic Pronominal Anaphora Resolution in English Texts  33  After the selection procedure is performed, the most appropriate antecedent is chosen to match each anaphor in the text. Figure 7 illustrates the result of anaphora pairs in each line, in which sentence number and word number are attached at the end of each entity. For example, “it” as the first word of the first sentence denotes a pleonastic-it, and the other “it,” the 57th word of the second sentence refers to “the heart.” Figure 8 shows the original text input with antecedent annotation following each anaphor in the text. All the annotations are highlighted to facilitate subsequent testing.  Figure 8. Anaphor with antecedent annotation.  3. Experimental Results and Analysis  The evaluation experiment employed random texts of different genres selected from the Brown corpus. There were 14,124 words, 2,970 noun phrases and 530 anaphors in the testing data. Two baseline models were established to compare the progress of performance with our proposed anaphora resolution (AR) system. The first baseline model (called the baseline subject) determined the number and gender agreement between candidates and anaphors, and then chose the most recent subject as the antecedent from the candidate set. The second baseline model (called baseline recent) performed a similar procedure, but it selected the most recent noun phrase as the antecedent which matched the anaphor in terms of number and gender agreement. The success rate was calculated as follows:  Success Rate = number of correctly resolved anaphors  (5)  number of all anaphors  34  Tyne Liang and Dian-Song Wu  The results obtained (Table 3) showed that there are 41% of the antecedents could be identified by finding the most recent subject; however, only 17% of the antecedents could be resolved by selecting the most recent noun phrase with the same gender and number agreement as the anaphor. Table 3. Success rate of baseline models.  Table 3: Success rate of baseline models.  Figure 9 presents the distribution of the sentence distance between antecedents and anaphors. The value 0 denotes intra-sentential anaphora and other values indicate inter-sentential anaphora. In the experiment, a balanced corpus was used to resolve the pronominal anaphora phenomenon. The methods proposed in [Lappin and Leass, 94] and [Mitkov, 01] employ corpora with only inanimate pronouns, such as “it” or “its.” Thus, the results for intra-sentential and inter-sentential anaphora distribution obtained using those methods are different. In our experiment on the Brown corpus, the distribution proportion of intra-sentential anaphora was about 60%. Figure 10 shows the average word distance distribution for each genre. The pleonastic-it could be identified with 89% accuracy (Table 4).  Ratio(%)  80  60  0  40  
Noun-verb event frame (NVEF) knowledge in conjunction with an NVEF word-pair identifier [Tsai et al. 2002] comprises a system that can be used to support natural language processing (NLP) and natural language understanding (NLU). In [Tsai et al. 2002a], we demonstrated that NVEF knowledge can be used effectively to solve the Chinese word-sense disambiguation (WSD) problem with 93.7% accuracy for nouns and verbs. In [Tsai et al. 2002b], we showed that NVEF knowledge can be applied to the Chinese syllable-to-word (STW) conversion problem to achieve 99.66% accuracy for the NVEF related portions of Chinese sentences. In [Tsai et al. 2002a], we defined a collection of NVEF knowledge as an NVEF word-pair (a meaningful NV word-pair) and its corresponding NVEF sense-pairs. No methods exist that can fully and automatically find collections of NVEF knowledge from Chinese sentences. We propose a method here for automatically acquiring large-scale NVEF knowledge without human intervention in order to identify a large, varied range of NVEF-sentences (sentences containing at least one NVEF word-pair). The auto-generation of NVEF knowledge (AUTO-NVEF) includes four major processes: (1) segmentation checking; (2) Initial Part-of-Speech (IPOS) sequence generation; (3) NV knowledge generation; and (4) NVEF knowledge auto-confirmation. Our experimental results show that AUTO-NVEF achieved 98.52% accuracy for news and 96.41% for specific text types, which included research reports, classical literature and modern literature. AUTO-NVEF automatically discovered over 400,000 NVEF word-pairs from the 2001 United Daily News (2001 UDN) corpus. According to our estimation, the acquired NVEF knowledge from 2001 UDN helped to identify 54% of the NVEF-sentences in the Academia Sinica Balanced Corpus (ASBC), and 60% in the 2001 UDN corpus.  * Institute of Information Science, Academia Sinica, Nankang, Taipei, Taiwan, R.O.C. E-mail: {tsaijl,gladys,hsu}@iis.sinica.edu.tw  42  Jia-Lin Tsai et al.  We plan to expand NVEF knowledge so that it is able to identify more than 75% of NVEF-sentences in ASBC. We will also apply the acquired NVEF knowledge to support other NLP and NLU researches, such as machine translation, shallow parsing, syllable and speech understanding and text indexing. The auto-generation of bilingual, especially Chinese-English, NVEF knowledge will be also addressed in our future work. Keywords: natural language understanding, verb-noun collection, machine learning, HowNet 1. Introduction The most challenging problem in natural language processing (NLP) is programming computers to understand natural languages. For humans, efficient syllable-to-word (STW) conversion and word sense disambiguation (WSD) occur naturally when a sentence is understood. In a natural language understanding (NLU) system is designed, methods that enable consistent STW and WSD are critical but difficult to attain. For most languages, a sentence is a grammatical organization of words expressing a complete thought [Chu 1982; Fromkin et al. 1998]. Since a word is usually encoded with multiple senses, to understand language, efficient word sense disambiguation (WSD) is critical for an NLU system. As found in a study on cognitive science [Choueka et al. 1983], people often disambiguate word sense using only a few other words in a given context (frequently only one additional word). That is, the relationship between a word and each of the others in the sentence can be used effectively to resolve ambiguity. From [Small et al. 1988; Krovetz et al. 1992; Resnik et al. 2000], most ambiguities occur with nouns and verbs. Object-event (i.e., noun-verb) distinction is the most prominent ontological distinction for humans [Carey 1992]. Tsai et al. [2002a] showed that knowledge of meaningful noun-verb (NV) word-pairs and their corresponding sense-pairs in conjunction with an NVEF word-pair identifier can be used to achieve a WSD accuracy rate of 93.7% for NV-sentences (sentences that contain at least one noun and one verb). According to [胡裕樹 et al. 1995; 陳克健 et al. 1996; Fromkin et al. 1998; 朱曉亞 2001;陳昌來 2002; 劉順 2003], the most important content word relationship in sentences is the noun-verb construction. For most languages, subject-predicate (SP) and verb-object (VO) are the two most common NV constructions (or meaningful NV word-pairs). In Chinese, SP and VO constructions can be found in three language units: compounds, phrases and sentences [Li et al. 1997]. Modifier-head (MH) and verb-complement (VC) are two other meaningful NV word-pairs which are only found in phrases and compounds. Consider the meaningful NV word-pair 汽車-進口(car, import). It is an MH construction in the Chinese compound 進口汽 車(import car) and a VO construction in the Chinese phrase 進口許多汽車(import many cars). In [Tsai et al. 2002a], we called a meaningful NV word-pair a noun-verb event frame (NVEF)  Auto-Generation of NVEF Knowledge in Chinese  43  word-pair. Combining the NV word-pair 汽車-進口 and its sense-pair Car-Import creates a collection of NVEF knowledge. Since a complete event frame usually contains a predicate and its arguments, an NVEF word-pair can be a full or a partial event frame construction. In Chinese, syllable-to-word entry is the most popular input method. Since the average number of characters sharing the same phoneme is 17, efficient STW conversion has become an indispensable tool. In [Tsai et al. 2002b], we showed that NVEF knowledge can be used to achieve an STW accuracy rate of 99.66% for converting NVEF related words in Chinese. We proposed a method for the semi-automatic generation of NVEF knowledge in [Tsai et al. 2002a]. This method uses the NV frequencies in sentences groups to generate NVEF candidates to be filtered by human editors. This process becomes labor-intensive when a large amount of NVEF knowledge is created. To our knowledge, no methods exist that can be used to fully auto-extract a large amount of NVEF knowledge from Chinese text. In the literature, most methods for auto-extracting Verb-Noun collections (i.e., meaningful NV word-pairs) focus on English [Benson et al. 1986; Church et al. 1990; Smadja 1993; Smadja et al. 1996; Lin 1998; Huang et al. 2000; Jian 2003]. However, the issue of VN collections focuses on extracting meaningful NV word-pairs, not NVEF knowledge. In this paper, we propose a new method that automatically generates NVEF knowledge from running texts and constructs a large amount of NVEF knowledge. This paper is arranged as follows. In section 2, we describe in detail the auto-generation of NVEF knowledge. Experiment results and analyses are given in section 3. Conclusions are drawn and future research ideas discussed in section 4. 2. Development of a Method for NVEF Knowledge Auto-GenerationFor our auto-generate NVEF knowledge (AUTO-NVEF) system, we use HowNet 1.0 [Dong 1999] as a system dictionary. This system dictionary provides 58,541 Chinese words and their corresponding parts-of-speech (POS) and word senses (called DEF in HowNet). Contained in this dictionary are 33,264 nouns and 16,723 verbs, as well as 16,469 senses comprised of 10,011 noun-senses and 4,462 verb-senses. Since 1999, HowNet has become one of widely used Chinese-English bilingual knowledge-base dictionaries for Chinese NLP research. Machine translation (MT) is a typical application of HowNet. The interesting issues related to (1) the overall picture of HowNet, (2) comparisons between HowNet [Dong 1999], WordNet [Miller 1990; Fellbaum 1998], Suggested Upper Merged Ontology (SUMO) [Niles et al. 2001; Subrata et al. 2002; Chung et al. 2003] and VerbNet [Dang et al. 2000; Kipper et al. 2000] and (3) typical applications of HowNet can be found in the 2nd tutorial of IJCNLP-04 [Dong 2004].  44  Jia-Lin Tsai et al.  2.1 Definition of NVEF Knowledge The sense of a word is defined as its definition of concept (DEF) in HowNet. Table 1 lists three different senses of the Chinese word 車(Che[surname]/car/turn). In HowNet, the DEF of a word consists of its main feature and all secondary features. For example, in the DEF “character|文字,surname|姓,human|人,ProperName|專” of the word 車(Che[surname]), the first item “character|文字” is the main feature, and the remaining three items, surname|姓, human|人, and ProperName|專, are its secondary features. The main feature in HowNet inherits features from the hypernym-hyponym hierarchy. There are approximately 1,500 such features in HowNet. Each one is called a sememe, which refers to the smallest semantic unit that cannot be reduced.  Table 1. The three different senses of the Chinese word (Che[surname]/car/turn).  C.Word a E.Word a  Part-of-speech Sense (i.e. DEF in HowNet)  車  Che[surname] Noun  character|文字,surname|姓,human|人,ProperName|專  車  car  Noun  LandVehicle|車  車  turn  Verb  cut|切削  a C.Word means Chinese word; E.Word means English word.  As previously mentioned, a meaningful NV word-pair is a noun-verb event-frame word-pair (NVEF word-pair), such as 車 - 行駛(Che[surname]/car/turn, move). In a sentence, an NVEF word-pair can take an SP or a VO construction; in a phrase/compound, an NVEF word-pair can take an SP, a VO, an MH or a VC construction. From Table 1, the only meaningful NV sense-pair for 車 - 行駛(car, move) is LandVehicle|車 - VehicleGo|駛. Here, combining the NVEF sense-pair LandVehicle|車 - VehicleGo|駛 and the NVEF word-pair 車 行駛 creates a collection of NVEF knowledge. 2.2 Knowledge Representation Tree for NVEF Knowledge To effectively represent NVEF knowledge, we have proposed an NVEF knowledge representation tree (NVEF KR-tree) that can be used to store, edit and browse acquired NVEF knowledge. The details of the NVEF KR-tree given below are taken from [Tsai et al. 2002a]. The two types of nodes in the KR-tree are function nodes and concept nodes. Concept nodes refer to words and senses (DEF) of NVEF knowledge. Function nodes define the relationships between the parent and children concept nodes. According to each main feature of noun senses in HowNet, we can classify noun senses into fifteen subclasses. These subclasses are 微生物(bacteria), 動物類(animal), 人物類(human), 植物類(plant), 人工物(artifact), 天  Auto-Generation of NVEF Knowledge in Chinese  45  然物(natural), 事件類(event), 精神類(mental), 現象類(phenomena), 物形類(shape), 地點類 (place), 位置類(location), 時間類(time), 抽象類(abstract) and 數量類(quantity). Appendix A provides a table of the fifteen main noun features in each noun-sense subclass.  As shown in Figure 1, the three function nodes that can be used to construct a collection of NVEF knowledge (LandVehicle|車- VehcileGo|駛) are as follows:  (1) Major Event (主要事件): The content of the major event parent node represents a  noun-sense subclass, and the content of its child node represents a verb-sense subclass. A  noun-sense subclass and a verb-sense subclass linked by a Major Event function node is an  NVEF subclass sense-pair, such as LandVehicle|車 and VehicleGo|駛 shown in Figure 1.  To describe various relationships between noun-sense and verb-sense subclasses, we have  designed three subclass sense-symbols: =, which means exact; &, which means like; and %,  which means inclusive. For example, provided that there are three senses, S1, S2, and S3, as  well as their corresponding words, W1, W2, and W3, let  S1 = LandVehicle|車,*transport|運送,#human|人,#die|死  W1=靈車(hearse);  S2 = LandVehicle|車,*transport|運送,#human|人  W2=客車(bus);  S3 = LandVehicle|車,police|警  W3=警車(police car).  Then, S3/W3 is in the exact-subclass of =LandVehicle|車,police|警; S1/W1 and S2/W2 are in the like-subclass of &LandVehicle|車,*transport|運送; and S1/W1, S2/W2, and S3/W3 are in the inclusive-subclass of %LandVehicle|車.  (2) Word Instance (實例): The contents of word instance children consist of words belonging to the sense subclass of their parent node. These words are self-learned through the sentences located under the Test-Sentence nodes.  (3) Test Sentence (測試題): The contents of test sentence children consist of the selected test NV-sentence that provides a language context for its corresponding NVEF knowledge.  Figure 1. An illustration of the KR-tree using 人工物 (artifact) as an example of a noun-sense subclass. The English words in parentheses are provided for explanatory purposes only.  46  Jia-Lin Tsai et al.  2.3 Auto-Generation of NVEF Knowledge AUTO-NVEF automatically discovers meaningful NVEF sense/word-pairs (NVEF knowledge) in Chinese sentences. Figure 2 shows the AUTO-NVEF flow chart. There are four major processes in AUTO-NVEF. These processes are shown in Figure 2, and Table 2 shows a step by step example. A detailed description of each process is provided in the following.  Hownet F P O S /N V w o rd -p air m app ing s  Chinese sentence input Process 1. Segmentation checking Process 2. Initial POS sequence generation Process 3. NV knowledge generation Process 4. NVEF knowledge auto- con firm ation NVEF-KR tree  NVEF accepting condition NVEF-enclosed word tem plate  Figure 2. AUTO-NVEF flow chart.  Process 1. Segmentation checking: In this stage, a Chinese sentence is segmented according to two strategies: forward (left-to-right) longest word first and backward (left-to-right) longest word first. From [Chen et al. 1986], the “longest syllabic word first strategy” is effective for Chinese word segmentation. If both forward and backward segmentations are equal (forward=backward) and the word number of the segmentation is greater than one, then this segmentation result will be sent to process 2; otherwise, a NULL segmentation will be sent. Table 3 shows a comparison of the word-segmentation accuracy for forward, backward and forward=backward strategies using the Chinese Knowledge Information Processing (CKIP) lexicon [CKIP 1995]. The word segmentation accuracy is the ratio of the correctly segmented sentences to all the sentences in the Academia Sinica Balancing Corpus (ASBC) [CKIP 1996]. A correctly segmented sentence means the segmented result exactly matches its corresponding segmentation in ASBC. Table 3 shows that the forward=backward technique achieves the best word segmentation accuracy.  Auto-Generation of NVEF Knowledge in Chinese  47  Table 2. An illustration of AUTO-NVEF for the Chinese sentence 音樂會現場湧 入許多觀眾(There are many audience members entering the locale of the concert). The English words in parentheses are included for explanatory purposes only.  Process (1) (2) (3)  Output 音樂會(concert)/現場(locale)/湧入(enter)/許多(many)/觀眾(audience members) N1N2V3ADJ4N5, where N1 =[音樂會]; N2 =[現場]; V3=[湧入]; ADJ4=[許多]; N5=[觀眾] NV1 = 現場/place|地方,#fact|事情/N - 湧入(yong3 ru4)/GoInto|進入/V  NV2 = 觀眾/human|人,*look|看,#entertainment|藝,#sport|體育,*recreation|娛樂/N - 湧入(yong3 ru4)/GoInto|進入/V  (4) NV1 is the 1st collection of NVEF knowledge confirmed by NVEF accepting-condition;  the learned NVEF template is [音樂會 NV 許多]  NV2 is athe 2nd collection of NVEF knowledge confirmed by NVEF accepting-condition;  the learned NVEF template is [現場V許多N]  Table 3. A comparison of the word-segmentation accuracy achieved using the backward, forward and backward = forward strategies. Test sentences were obtained from ASBC, and the dictionary used was the CKIP lexicon.  Accuracy Recall  Backward 82.5% 100%  Forward 81.7% 100%  Backward = Forward 86.86% 89.33%  Process 2. Initial POS sequence generation: This process will be triggered if the output of process 1 is not a NULL segmentation. It is comprised of the following steps. 1) For segmentation result w1/w2/…/wn-1/wn from process 1, our algorithm computes the POS of wi, where i = 2 to n. Then, it computes the following two sets: a) the following POS/frequency set of wi-1 according to ASBC and b) the HowNet POS set of wi. It then computes the POS intersection of the two sets. Finally, it selects the POS with the highest frequency in the POS intersection as the POS of wi. If there is zero or more than one POS with the highest frequency, the POS of wi will be set to NULL POS. 2) For the POS of w1, it selects the POS with the highest frequency in the POS intersection of the preceding POS/frequency set of w2 and the HowNet POS set of w1. 3) After combining the determined POSs of wi obtained in first two steps, it then generates the initial POS sequence (IPOS). Take the Chinese segmentation 生/了 as an example. The following POS/frequency set of the Chinese word 生(to bear) is {N/103, PREP/42,  48  Jia-Lin Tsai et al.  STRU/36, V/35, ADV/16, CONJ/10, ECHO/9, ADJ/1}(see Table 4 for tags defined in HowNet). The HowNet POS set of the Chinese word 了(a Chinese satisfaction indicator) is {V, STRU}. According to these sets, we have the POS intersection {STRU/36, V/35}. Since the POS with the highest frequency in this intersection is STRU, the POS of 了 will be set to STRU. Similarly, according to the intersection {V/16124, N/1321, ADJ/4} of the preceding POS/frequency set {V/16124, N/1321, PREP/1232, ECHO/121, ADV/58, STRU/26, CONJ/4, ADJ/4} of 了 and the HowNet POS set {V, N, ADJ} of 生, the POS of 生will be set to V. Table 4 shows a mapping list of CKIP POS tags and HowNet POS tags.  Table 4. A mapping list of CKIP POS tags and HowNet POS tags.  Noun Verb Adjective Adverb Preposition Conjunction Expletive Structural Particle  CKIP  N  V  A  D  P  C  T  De  HowNet N  V  ADJ  ADV  PP  CONJ  ECHO  STRU  Process 3. NV knowledge generation: This process will be triggered if the IPOS output of process 2 does not include any NULL POS. The steps in this process are given as follows. 1) Compute the final POS sequence (FPOS). This step translates an IPOS into an FPOS. For each continuous noun sequence of IPOS, the last noun will be kept, and the other nouns will be dropped. This is because a contiguous noun sequence in Chinese is usually a compound, and its head is the last noun. Take the Chinese sentence 音樂會(N1)現場(N2)湧入(V3)許多 (ADJ4)觀眾(N5) and its IPOS N1N2V3ADJ4N5 as an example. Since it has a continuous noun sequence音樂會(N1)現場(N2), the IPOS will be translated into FPOS N1V2ADJ3N4, where N1=現場, V2=湧入, ADJ3=許多and N4=觀眾. 2) Generate NV word-pairs. According to the FPOS mappings and their corresponding NV word-pairs (see Appendix B), AUTO-NVEF generates NV word-pairs. In this study, we created more than one hundred FPOS mappings and their corresponding NV word-pairs. Consider the above mentioned FPOS N1V2ADJ3N4, where N1=現場, V2=湧入, ADJ3=許多 and N4=觀眾. Since the corresponding NV word-pairs for the FPOS N1V2ADJ3N4 are N1V2 and N4V2, AUTO-NVEF will generate two NV word-pairs 現場(N)湧入(V) and湧入(V)觀眾 (N). In [朱曉亞 2001], there are some useful semantic structure patterns of Modern Chinese sentences for creating FPOS mappings and their corresponding NV word-pairs. 3) Generate NV knowledge. According to HowNet, AUTO-NVEF computes all the NV sense-pairs for the generated NV word-pairs. Consider the generated NV word-pairs 現場 (N)湧入(V) and 湧入(V)觀眾(N). AUTO-NVEF will generate two collections of NV knowledge:  Auto-Generation of NVEF Knowledge in Chinese  49  NV1 = [現場(locale)/place|地方,#fact|事情/N] - [湧入(enter)/GoInto|進入/V], and NV2 = [觀眾(audience)/human|人,*look|看,#entertainment|藝,#sport|育,*recreation| 娛樂/N] - [湧入(enter)/GoInto|進入/V]. Process 4. NVEF knowledge auto-confirmation: In this stage, AUTO-NVEF automatically confirms whether the generated NV knowledge is or is not NVEF knowledge. The two auto-confirmation procedures are described in the following. (a) NVEF accepting condition (NVEF-AC) checking: Each NVEF accepting condition is constructed using a noun-sense class (such as 人物類[human]) defined in [Tsai et al. 2002a] and a verb main feature (such as GoInto|進入) defined in HowNet [Dong 1999]. In [Tsai et al. 2002b], we created 4,670 NVEF accepting conditions from manually confirmed NVEF knowledge. In this procedure, if the noun-sense class and the verb main feature of the generated NV knowledge can satisfy at least one NVEF accepting condition, then the generated NV knowledge will be auto-confirmed as NVEF knowledge and will be sent to the NVEF KR-tree. Appendix C lists the ten NVEF accepting conditions used in this study. (b) NVEF enclosed-word template (NVEF-EW template) checking: If the generated NV knowledge cannot be auto-confirmed as NVEF knowledge in procedure (a), this procedure will be triggered. An NVEF-EW template is composed of all the left side words and right side words of an NVEF word-pair in a Chinese sentence. For example, the NVEF-EW template of the NVEF word-pair 汽車-行駛(car, move) in the Chinese sentence 這(this)/汽車(car)/似乎(seem)/行駛(move)/順暢(well) is 這 N 似乎 V 順暢. In this study, all NVEF-EW templates were auto-generated from: 1) the collection of manually confirmed NVEF knowledge in [Tsai et al. 2002], 2) the on-line collection of NVEF knowledge automatically confirmed by AUTO-NVEF and 3) the manually created NVEF-EW templates. In this procedure, if the NVEF-EW template of a generated NV word-pair matches at least one NVEF-EW template, then the NV knowledge will be auto-confirmed as NVEF knowledge. 3. Experiments To evaluate the performance of the proposed approach to the auto-generation of NVEF knowledge, we define the NVEF accuracy and NVEF-identified sentence ratio according to Equations (1) and (2), respectively:  NVEF accuracy = # of meaningful NVEF knowledge / # of total generated NVEF knowledge;  (1)  NVEF-identified sentence ratio =# of NVEF-identified sentences / # of total NVEF-sentences.  (2)  50  Jia-Lin Tsai et al.  In Equation (1), meaningful NVEF knowledge means that the generated NVEF knowledge has been manually confirmed to be a collection of NVEF knowledge. In Equation (2), if a Chinese sentence can be identified as having at least one NVEF word-pair by means of the generated NVEF knowledge in conjunction with the NVEF word-pair identifier proposed in [Tsai et al. 2002a], this sentence is called an NVEF-identified sentence. If a Chinese sentence contains at least one NVEF word-pair, it is called an NVEF-sentence. We estimate that about 70% of the Chinese sentences in ASBC are NVEF-sentences.  3.1 User Interface for Manually Confirming NVEF Knowledge  A user interface that manually confirms generated NVEF knowledge is shown in Figure 3. With it, evaluators (native Chinese speakers) can review generated NVEF knowledge and determine whether or not it is meaningful NVEF knowledge. Take the Chinese sentence 高度 壓力(High pressure)使(make)有些(some)人(people)食量(eating capacity)減少(decrease) as an example. AUTO-NVEF will generate an NVEF knowledge collection that includes the NVEF sense-pair [attribute|屬 性 ,ability|能 力 ,&eat|吃 ] - [subtract|削 減 ] and the NVEF word-pair [ 食 量 (eating capacity)] - [ 減 少 (decrease)]. The principles for confirming meaningful NVEF knowledge are given in section 3.2. Appendix D provides a snapshot of the designed user interface for evaluators for manually to use to confirm generated NVEF knowledge.  Chinese sentence 名詞詞義 (Noun sense) 名詞 (Noun)  高 度 壓 力 (High pressure) 使 (make) 有 些 (some) 人 (people) 食 量 (eating  capacity)減少(decrease)  attribute|屬性,ability|能力,&eat|吃 動詞詞義 subtract|削減  (Verb sense)  食量 (eating capacity)  動詞 (Verb) 減少 (decrease)  Figure 3. The user interface for confirming NVEF knowledge using the generated NVEF knowledge for the Chinese sentence 高度壓力(High pressure)使 (makes)有些(some)人(people)食量(eating capacity)減少(decrease). The English words in parentheses are provided for explanatory purposes only. [ ] indicate nouns and <> indicate verbs.  3.2 Principles for Confirming Meaningful NVEF Knowledge Auto-generated NVEF knowledge can be confirmed as meaningful NVEF knowledge if it satisfies all three of the following principles. Principle 1. The NV word-pair produces correct noun(N) and verb(V) POS tags for the given Chinese sentence. Principle 2. The NV sense-pair and the NV word-pair make sense.  Auto-Generation of NVEF Knowledge in Chinese  51  Principle 3. Most of the inherited NV word-pairs of the NV sense-pair satisfy Principles 1 and 2. 3.3 Experiment Results For our experiment, we used two corpora. One was the 2001 UDN corpus containing 4,539,624 Chinese sentences that were extracted from the United Daily News Web site [On-Line United Daily News] from January 17, 2001 to December 30, 2001. The other was a collection of specific text types, which included research reports, classical literature and modern literature. The details of the training, testing corpora and test sentence sets are given below. (1) Training corpus. This was a collection of Chinese sentences extracted from the 2001 UDN corpus from January 17, 2001 to September 30, 2001. According to the training corpus, we created thirty thousand manually confirmed NVEF word-pairs, which were used to derive 4,670 NVEF accepting conditions. (2) Testing corpora. One corpus was the collection of Chinese sentences extracted from the 2001 UDN corpus from October 1, 2001 to December 31, 2001. The other was a collection of specific text types, which included research reports, classical literature and modern literature. (3) Test sentence sets. From the first testing corpus, we randomly selected all the sentences extracted from the news of October 27, 2001, November 23, 2001 and December 17, 2001 in 2001 UDN as our first test sentence set. From the second testing corpus, we selected a research report, a classical novel and a modern novel for our second test sentence set.  Table 5a. Experiment results of AUTO-NVEF for news.  News article date October 27, 2001 November 23, 2001 December 17, 2001 Total Average  NVEF-AC 99.54%(656/659) 98.75%(711/720) 98.74%(1,015/1,028) 98.96%(2,382/2,407)  NVEF accuracy NVEF-EW 98.43%(439/446) 95.95%(379/395) 98.53%(1,141/1,158) 98.00%(1,959/1,999)  NVEF-AC + NVEF-EW 99.10% (1,095/1,105) 97.76% (1,090/1,115) 98.63% (2,156/2,186) 98.52% (4,341/4,406)  All the NVEF knowledge acquired by AUTO-NVEF from the testing corpora was manually confirmed by evaluators. Tables 5a and 5b show the experiment results. These tables show that our AUTO-NVEF achieved 98.52% NVEF accuracy for news and 96.41% for specific text  52  Jia-Lin Tsai et al.  types.  Table 5b. Experiment results of AUTO-NVEF for specific text types.  Text type Technique Report Classic novel Modern novel Total Average  NVEF-AC 97.12%(236/243) 98.64%(218/221) 98.18%(377/384) 98.00%(831/848)  NVEF accuracy NVEF-EW 96.61%(228/236) 93.55%(261/279) 95.42%(562/589) 95.20%(1,051/1,104)  NVEF-AC + NVEF-EW 96.86% (464/479) 95.80% (479/500) 96.51% (939/973) 96.41% (1,882/1,952)  When we applied AUTO-NVEF to the entire 2001 UDN corpus, it auto-generated 173,744 NVEF sense-pairs (8.8M) and 430,707 NVEF word-pairs (14.1M). Within this data, 51% of the NVEF knowledge were generated based on NVEF accepting conditions (human-editing knowledge), and 49% were generated based on NVEF-enclosed word templates (machine-learning knowledge). Tables 5a and 5b show that the average accuracy of NVEF knowledge generated by NVEF-AC and NVEF-EW for news and specific texts reached 98.71% and 97.00%, respectively. These results indicate that our AUTO-NVEF has the ability to simultaneously maintain high precision and extend NVEF-EW knowledge, similar to the snowball effect, and to generate a large amount of NVEF knowledge without human intervention. The results also suggest that the best method to overcome the Precision-Recall Tradeoff problem for NLP is based on linguistic knowledge and statistical constraints, i.e., hybrid approach [Huang et al. 1996; Tsai et al. 2003]. 3.3.1 Analysis and Classification of NVEF Knowledge From the noun and verb positions of NVEF word-pairs in Chinese sentences, NVEF knowledge can be classified into four NV-position types: N:V, N-V, V:N and V-N, where : means next to and - means nearby. Table 6a shows examples and the percentages of the four NV-position types of generated NVEF knowledge. The ratios (percentages) of the collections of N:V, N-V, V:N and V-N are 12.41%, 43.83% 19.61% and 24.15%, respectively. Table 6a shows that an NVEF word-pair, such as 工程-完成(Construction, Complete), can be an N:V, N-V, V:N or V-N in sentences. For our generated NVEF knowledge, the maximum and average number of characters between nouns and verbs in generated NVEF knowledge are 27 and 3, respectively. Based on the numbers of noun and verb characters in NVEF word-pairs, we classify NVEF knowledge into four NV-word-length types: N1V1, N1V2+, N2+V1 and N2+V2+, where N1 and V1 mean single-character nouns and verbs, respectively; N2+ and V2+ mean multi-character nouns and verbs. Table 6b shows examples and the percentages of the four NV-word-length  Auto-Generation of NVEF Knowledge in Chinese  53  types of manually created NVEF knowledge for 1,000 randomly selected ASBC sentences. From the manually created NVEF knowledge, we estimate that the percentages of the collections of N1V1, N1V2+, N2+V1 and N2+V2+ NVEF word-pairs are 6.4%, 6.8%, 22.2% and 64.6%, respectively. According to this NVEF knowledge, we estimate that the auto-generated NVEF Knowledge (for 2001 UDN) in conjunction with the NVEF word-pair identifier [Tsai et al. 2002] can be used to identify 54% of the NVEF-sentences in ASBC.  Table 6a. An illustration of four NV-position types of NVEF knowledge and their ratios. The English words in parentheses are provided for explanatory purposes only. [ ] indicate nouns and <> indicate verbs.  Type  Example Sentence  [工程]<完成>  N:V (The construction is now  completed)  全部[工程]預定年底<完成>  N-V (All of constructions will be  completed by the end of year)  <完成>[工程] V:N (to complete a construction)  建商承諾在年底前<完成> 鐵路[工程]  V-N (The building contractor promise  to complete railway construction  before the end of this year)  Noun / DEF 工程 (construction) affairs|事務,industrial|工 工程 (construction) affairs|事務,industrial|工 工程 (construction) affairs|事務,industrial|工 工程 (construction) affairs|事務,industrial|工  Verb / DEF 完成 (complete) fulfill|實現 完成 (complete) fulfill|實現 完成 (complete) fulfill|實現 完成 (complete) fulfill|實現  Percentage 24.15% 43.83% 19.61% 12.41%  Table 6b. Four NV-word-length types of manually-edited NVEF knowledge from 1,000 randomly selected ASBC sentences and their percentages. The English words in parentheses are provided for explanatory purposes only. [ ] indicate nouns and <> indicate verbs.  Type N1V1 N1V2+ N2+V1 N2+V2+  Example Sentence 然後就<棄>[我]而去 <覺得>[他]很孝順 <買>了[可樂]來喝 <引爆>另一場美西[戰爭]  Noun 我(I) 他(he) 可樂(cola) 戰爭(war)  Verb 棄(give up) 覺得(feel) 買(buy) 引爆(cause)  Percentage 6.4% 6.8% 22.2% 64.6%  54  Jia-Lin Tsai et al.  Table 6c shows the Top 5 single-character verbs in N1V1 and N2+V1 NVEF word-pairs and their percentages. Table 6d shows the Top 5 multi-character verbs in N1V2+ and N2+V2+ NVEF word-pairs and their percentages. From Table 6c, the percentages of N2+是 and N2+有 NVEF word-pairs are both greater than those of other single-character verbs. Thus, the N2+是 and N2+有 NVEF knowledge was worthy to being considered in our AUTO-NVEF. On the other hand, we found that 3.2% of the NVEF-sentences (or 2.3% of the ASBC sentences) were N1V1-only sentences, where an N1V1-only sentence is a sentence that only has one N1V1-NVEF word-pair. For example, the Chinese sentence 他(he)說(say)過了(already) is an N1V1-only sentence because it has only one N1V1-NVEF word-pair: 他-說(he, say). Since (1) N1V1-NVEF knowledge is not critical for our NVEF-based applications and (2) auto-generating N1V1 NVEF knowledge is very difficult, the auto-generation of N1V1-NVEF knowledge was not considered in our AUTO-NVEF. In fact, according to the system dictionary, the maximum and average word-sense numbers of single-character were 27 and 2.2, respectively, and those of multi-character words were 14 and 1.1, respectively.  Table 6c. The Top 5 single-character verbs in N1V1 and N2+V1 word-pairs in manually-edited NVEF knowledge for 1,000 randomly selected ASBC sentences and their percentages. The English words in parentheses are provided for explanatory purposes only. [ ] indicate nouns and <> indicate verbs.  Verb of N1V1 /  Percentage  Top  Example Sentence  of N1V1  有(have) / 1 [我]<有>九項獲參賽資格  16.5%  是(be) / 2 [它]<是>做人的根本  8.8%  說(speak) / 3 [他]<說>  7.7%  看(see) / 4 <看>著[它]被卡車載走  4.4%  買(buy) /  5 美國本土的人極少到那兒<  3.3%  買>[地]  Verb of N2+V1 / Example Sentence 是(be) / 再來就<是>一間陳列樂器的[房子] 有(have) / 是不是<有>[問題]了 說(speak) / 而談到成功的秘訣[妮娜]<說> 到(arrive) / 一[到]<陰天> 讓(let) / <讓>現職[人員]無處棲身  Percentage of N2+V1 20.5% 15.5% 3.9% 3.6% 2.5%  Auto-Generation of NVEF Knowledge in Chinese  55  Table 6d. The Top 5 multi-character verbs in N1V2+ and N2+V2+ word-pairs in manually-edited NVEF knowledge for 1,000 randomly selected ASBC sentences and their percentages. The English words in parentheses are provided for explanatory purposes only. [ ] indicate nouns and <> indicate verbs.  Top  Verb of N1V2+ /  Percentage  Example Sentence  of N1V2+  
This paper presents a Chinese named entity recognizer (NER): Mencius. It aims to address Chinese NER problems by combining the advantages of rule-based and machine learning (ML) based NER systems. Rule-based NER systems can explicitly encode human comprehension and can be tuned conveniently, while ML-based systems are robust, portable and inexpensive to develop. Our hybrid system incorporates a rule-based knowledge representation and template-matching tool, called InfoMap [Wu et al. 2002], into a maximum entropy (ME) framework. Named entities are represented in InfoMap as templates, which serve as ME features in Mencius. These features are edited manually, and their weights are estimated by the ME framework according to the training data. To understand how word segmentation might influence Chinese NER and the differences between a pure template-based method and our hybrid method, we configure Mencius using four distinct settings. The F-Measures of person names (PER), location names (LOC) and organization names (ORG) of the best configuration in our experiment were respectively 94.3%, 77.8% and 75.3%. From comparing the experiment results obtained using these configurations reveals that hybrid NER Systems always perform better performance in identifying person names. On the other hand, they have a little difficulty identifying location and organization names. Furthermore, using a word segmentation module improves the performance of pure Template-based NER Systems, but, it has little effect on hybrid NER systems. * Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan, R.O.C. E-mail: d90013@csie.ntu.edu.tw + Institute of Information Science, Academia Sinica., Taipei, Taiwan, R.O.C. E-mail: {thtsai, shwu, aska, dapi, hsu}@iis.sinica.edu.tw  66  Tzong-Han Tsai et al.  1. Introduction Information Extraction (IE) is the task of extracting information of interest from unconstrained text. IE involves two main tasks: the recognition of named entities, and the recognition of the relationships among these named entities. Named Entity Recognition (NER) involves the identification of proper names in text and classification of them into different types of named entities (e.g., persons, organizations, locations). NER is important not only in IE [Grishman 2002] but also in lexical acquisition for the development of robust NLP systems [Coates-Stephens 1992]. Moreover, NER has proven useful for tasks such as document indexing and the maintenance of databases containing identified named entities. During the last decade, NER has drawn much attention at Message Understanding Conferences (MUC) [Chinchor 1995a][Chinchor 1998a]. Both rule-based and machine learning NER systems have had some success. Traditional rule-based approaches have used manually constructed finite state patterns, which match text against a sequence of words. Such systems (like the University of Edinburgh's LTG [Mikheev et al. 1998]) do not need very much training data and can encode expert human knowledge. However, rule-based approaches lack robustness and portability. Each new source of text requires significant tweaking of the rules to maintain optimal performance, and the maintenance costs can be quite steep. Another popular approach in NER is machine-learning (ML). ML is attractive in that it is more portable and less expensive to maintain. Representative ML approaches used in NER are HMM (BBN's IdentiFinder in [Miller et al. 1998][Bikel et al. 1999] and Maximum Entropy (ME) (New York University's MEME in [Borthwick et al. 1998][Borthwick 1999]). However, ML systems are relatively inexpensive to develop, and the outputs of these systems are difficult to interpret. In addition, it is difficult to improve the system performance through error analysis. The performance of an ML system can be very poor when the amount of training data is insufficient. Furthermore, the performance of ML systems is worse than that of rule-based ones by about 2%, as revealed at MUC-6 [Chinchor 1995b] and MUC-7 [Chinchor 1998b]. This might be due to the fact that current ML approaches can not capture non-parametric factors as effectively as human experts who handcraft the rules. Nonetheless, ML approaches do provide important statistical information that is unattainable by human experts. Currently, the F-measures of English rule-based and ML NER systems are in the range of 85% ~ 94%, based on MUC-7 data [Chinchor 1998c]. This is higher than the average performance of Chinese NER systems, which ranges from 79% to 86% [Chinchor 1998]. In this paper, we address the problem of Chinese NER. In Chinese sentences, there are no spaces between words, no capital letters to denote proper names, no sentence breaks, and, worst of all, no standard definition of “words.” As a result, word boundaries cannot, at times, be discerned without a context. In addition, the length of a named entity is longer on average than  Mencius: A Chinese Named Entity Recognizer Using the  67  Maximum Entropy-based Hybrid Model  that of an English one; thus, the complexity of a Chinese NER system is greater. Previous works [Chen et al. 1998] [Yu et al. 1998] [Sun et al., 2002] on Chinese NER have relied on the word segmentation module. However, an error in the word segmentation step might lead to errors in NER results. Therefore, we want to compare the results of NER with/without performing word segmentation. Without word segmentation, a character-based tagger is used, which treats each character as a token and combines the tagged outcomes of contiguous characters to form an NER output. With word segmentation, we treat each word or character as a token, and combine the tagged outcomes of contiguous tokens to form an NER output. Borthwick [1999] used an ME framework to integrate many NLP resources, including previous systems such as Proteus, a POS tagger. Mencius, the Chinese named entity recognizer presented here, incorporates a rule-based knowledge representation and a template-matching tool, called InfoMap [Wu et al. 2002], into a maximum entropy (ME) framework. Named entities are represented in InfoMap as templates, which serve as ME features in Mencius. These features are edited manually, and their weights are estimated by means of the ME framework according to the training data. This paper is organized as follows. Section 2 provides the ME-based framework for NER. Section 3 describes features and how they are represented in our knowledge representation system, InfoMap. The data set and experimental results are discussed in section 4. Section 5 gives our conclusions and possible extensions of the current work. 2. Maximum Entropy-Based NER Framework For our purpose, we regard each character as a token. Consider a test corpus and a set of n named entity categories. Since a named entity can have more than one token, we associate the following two tags with each category x: x_begin and x_continue. In addition, we use the tag unknown to indicate that a token is not part of a named entity. The NER problem can then be rephrased as the problem of assigning one of 2n + 1 tags to each token. In Mencius, there are 3 named entity categories and 7 tags: person_begin, person_continue, location_begin, location_continue, organization_begin, organization_continue and unknown. For example, the phrase [李 遠 哲 在 高 雄 市] (Lee, Yuan Tseh in Kaohsiung City) could be tagged as _begin, [person person_continue, person_continue, unknown, location_begin, location_continue, location_continue].  2.1 Maximum Entropy ME is a flexible statistical model which assigns an outcome for each token based on its history  68  Tzong-Han Tsai et al.  and features. Outcome space is comprised of the seven Mencius tags for an ME formulation of NER. ME computes the probability p(o|h) for any o from the space of all possible outcomes O, and for every h from the space of all possible histories H. A history is composed of all the conditioning data that enable one to assign probabilities to the space of outcomes. In NER, history can be viewed as consisting of the all information derivable from the test corpus relavant to the current token. The computation of p(o|h) in ME depends on a set of binary-valued features, which are helpful in making a prediction about the outcome. For instance, one of our features is as follows: when the current character is a known surname, it is likely to be the leading character of a person name. More formally, we can represent this feature as  f  (h, o)  =  ⎧1:if Current ⎩⎨0 : else  - Char  - Surname(h) =  true and o  =  person _ begin  (1)  Here, Current-Char-Surname(h) is a binary function that returns the value true if the current character of the history h is in the surname list.  Given a set of features and a training corpus, the ME estimation process produces a model in which every feature fi has a weight αi. This allows us to compute the conditional probability as follows [Berger et al. 1996]:  ∏ p(o | h) = 1 Z (h)  i  α . fi (h,o) i  (2)  Intuitively, the probability is the multiplication of the weights of active features (i.e., those fi (h,o) = 1). The weight αi is estimated by means of a procedure called Generalized Iterative Scaling (GIS) [Darroch et al. 1972]. This is an iterative method that improves estimation of the weights at each iteration. The ME estimation technique guarantees that for every feature fi, the expected value ofαi equals the empirical expectation ofαi in the training corpus.  As Borthwick [1999] remarked, ME allows the modeler to concentrate on finding the features that characterize the problem while letting the ME estimation routine deal with assigning relative weights to the features.  2.2 Decoding After an ME model has been trained and the proper weight αi has been assigned to each feature fi, decoding (i.e., marking up) a new piece of text becomes a simple task. First, Mencius tokenizes the text and preprocesses the testing sentence. Then for each token, it checks which  Mencius: A Chinese Named Entity Recognizer Using the  69  Maximum Entropy-based Hybrid Model  features are active and combines theαi of the active features according to equation 2. Finally, a Viterbi search is run to find the highest probability path through the lattice of conditional probabilities that does not produce any invalid tag sequences (for instance, the sequence [person_begin, location_continue] is invalid). Further details on Viterbi search can be found in [Viterbi 1967]. 3. Features We divide features that can be used to recognize named entities into four categories according to whether they are external or not and whether they are category dependent or not. McDonald defined internal and external features in [McDonald 1996]. Internal evidence is found within the entity, while external evidence is gathered from its context. We use category-independent features to distinguish named entities from non-named entities (e.g., first-character-of-a-sentence, capital-letter, out-of-vocabulary), and use category-dependent features to distinguish between different named entity categories (for example, surname and given name lists are used to recognize person names). However, to simplify our design, we only use internal features that are category-dependent in this paper.  3.1 InfoMap – Our Knowledge Representation System To the calculate values of location features and organization features, Mencius uses InfoMap. InfoMap is our knowledge representation and template matching tool, which represents location or organization names as templates. An input string (sentence) is first matched to one or more location or organization templates by InfoMap and then passed to Mencius; there, it is assigned feature values which further distinguish which named entity category it falls into.  3.1.1 Knowledge Representation Scheme in InfoMap InfoMap is a hierarchical knowledge representation scheme, consisting of several domains, each with a tree-like taxonomy. The basic units of information in InfoMap are called generic nodes, which represent concepts, and function nodes, which represent the relationships among the generic nodes of one specific domain. In addition, generic nodes can also contain cross references to other nodes to avoid needless repetition. In Mencius, we apply the geographical taxonomy of InfoMap called GeoMap. Our location and organization templates refer to generic nodes in Geomap. As shown in Figure 1, GeoMap has three sub-domains: World, Mainland China, and Taiwan. Under the sub-domain Taiwan, there are four attributes: Cities, Parks, Counties and City Districts. Moreover, these attributes can be further divided; for example, Counties can be divided into individual counties:  70  Tzong-Han Tsai et al.  Taipei County, Taoyuan County, etc. In InfoMap, we refer to generic nodes (or concept node) by means of paths. A path of generic nodes consists of all the node names from the root of the domain to the specific generic node, where function nodes are omitted. The node names are separated by periods. For example, the path for the “Taipei County” node is “GeoMap.Counties.Taipei County.”  Figure 1. A partial view of GeoMap. 3.1.2 InfoMap Templates In InfoMap, text templates are stored in generic nodes. Templates can consist of character strings, wildcards (see $$ in Table 1), and references to other generic nodes in InfoMap. For example, the template [ 通 用 地 理 . 台 灣 . 縣 ]:$$(2..4): 局 ( [GeoMap. Taiwan. Counties]: $$(2..4):Department ) can be used to recognize county level governmental departments in Taiwan. The syntax used in InfoMap templates are shown in Table 1. The first part of our sample template shown above (enclosed by “[]”) is a path that refers to the generic node “Counties.” The second element is a wildcard, ($$) which must be 2 to 4 characters in length. The third element is a specified character “局” (Department).  Mencius: A Chinese Named Entity Recognizer Using the  71  Maximum Entropy-based Hybrid Model  Table 1. InfoMap template syntax. Symbol Semantics  Example Template  : $$(m..n) [p]  Concatenate two strings Wildcards (the number of characters can be from m to n; both m and n have to be non-negative integers) A path to a generic node  A:B A:$$(1..2):B [GeoMap.Taiwan.Counties]  Sample Matching String AB ACB, ADDB, ACDB Taipei County, Taoyuan County, Hsinchu County, etc.  3.2 Category-Dependent Internal Features Recall that category-dependent features are used to distinguish among different named entity categories. 3.2.1 Features for Recognizing Person Names Mencius only deals with a surname plus a first name (usually composed of two characters), for example, 陳水扁 (Chen Shui-bian). There are various other ways to identify a person in a sentence, such as 陳先生 (Mr. Chen) and老陳 (Old Chen), which have not been incorporated into the current system. Furthermore, we do not target transliterated names, such as 布希 (Bush), since they do not follow Chinese name composition rules. We use a table of frequently occurring names to process our candidate test data. If a character and its context (history) correspond to a feature condition, the value of the current character for that feature will be set to 1. Feature conditions, examples and explanations for each feature are shown in Table 2. In the feature condition column, c-1, c0, and c1 represent the preceding character, the current character, and the following character, respectively. Current-Char-Person-Surname: This feature is set to 1 if c0c1c2 or c0c1 is in the person name database. For example, in the case of c0c1c2 = 陳水扁, the feature Current-Char-Person-Surname for 陳 is active since c0 and its following characters c1c2 satisfy the feature condition. Current-Char-Person-Given-Name: This feature is set to 1 if c-2c-1c0, c-1c0, or c-1c0c1 is in the person name database. Current-Char-Surname: This feature is set to 1 if c0 is in the top 300 popular surname list.  72  Tzong-Han Tsai et al.  Table 2. Person features. Feature Current-Char-Person-Surname Current-Char-Person-Given-Name Current-Char-Surname Current-Char-Given-Name Current-Char-FreqGiven-Name-Character Current-Char-Speaking-Verb Current-Char-Title  Feature Conditions c0c1c2 or c0c1 is in the name list c-2c-1c0 or c-1c0 or c-1c0c1 is in the name list c0 is in the surname list c0c1 or c-1c0 is in the given name list Both c0, c1 or c-1, c1 is in the frequent given name character list c0 or c0c1 or c-1c0 is in the list of verbs indicating speech c0 or c0c1 or c-1c0 is in the title list  Example “陳”水扁, “連”戰 陳“水”扁, 陳水“扁”, 連“戰” “陳”, “林”, “李” 黃“其”聖, 黃其“聖” 羅“方”全, 羅方“全” “說”, “表” 示, 表 “示” “先”生, 先 “生”  Explanation Probably the first character of a person name Probably the second or third character of a person name Probably a surname Probably part of a popular given name Probably a given name character Probably part of a verb indicating speech (ex: John said he was tired) Probably part of a title  Current-Char-Given-Name: This feature is set to 1 if c0c1 or c-1c0 is in the given name database. Current-Char-Freq-Given-Name-Character: (c0 and c1) or (c-1 and c0) is in the frequently given name character list Current-Char-Speaking-Verb: c0 or c0c1 or c-1c0 is in the speaking verb list. This feature distinguishes a trigram containing a speaking verb, such as 陳沖說 (Chen Chong said), from a real person name. Current-Char-Title: c0 or c0c1 or c-1c0 is in the title list. This feature distinguishes a trigram containing a title, such as 陳先生 (Mr. Chen), from a real person name.  3.2.2 Features for Recognizing Location Names In general, locations are divided into four types: administrative division, public area (park, airport, or port), landmark (road, road section, cross section or address), and landform (mountain, river, sea, or ocean). An administrative division name usually contains one or more  Mencius: A Chinese Named Entity Recognizer Using the  73  Maximum Entropy-based Hybrid Model  location names in a hierarchical order, such as 安大略省多倫多市 (Toronto, Ontario). A public area name is composed of a Region-Name and a Place-Name. However, the Region-Name is usually omitted from news content if it was previously mentioned. For example, 倫敦海德公園 (Hyde Park, London) contains the Region-Name 倫敦 (London) and the Place-Name 海德公園 (Hyde Park). But “Hyde Park, London” is usually abbreviated as “Hyde Park” within a report. The same rule can be applied to landmark names. A landmark name includes a Region-Name and a Position-Name. In a news article, the Region-Name can be omitted if the Place-Name has been mentioned previously. For example, 溫 哥 華 市 羅 伯 遜 街 五 號 (No. 5, Robson St., Vancouver City) will be stated as 羅伯遜街五號 (No. 5, Robson St.) later in the report.  In Mencius, we build templates to recognize three types of location names. Our administrative division templates contain more than one set of location names in a hierarchical order. For example, the template, [ 通 用 地 理 . 台 灣 . 市 ]:[ 通 用 地 理 . 台 灣 . 各 市 行 政 區 ] ([GeoMap.Taiwan.Cities]:[GeoMap.Taiwan.City Districts]) can be used to recognize all city districts in Taiwan. In addition, public area templates contain one set of location names and a set of Place-Name. For example, [ 通 用 地 理 . 台 灣 . 市 ]:[ 通 用 地 理 . 台 灣 . 公 園 ] ([GeoMap.Taiwan.Cities]:[GeoMap.Taiwan.Parks]) can be used to recognize all city parks in Taiwan. Landmark templates are built in the same way.For example, [ 通 用 地 理 . 台 灣 . 市]:$$(2..4):路 ([GeoMap.Taiwan.Cities]:$$(2..4):Road) can be used to recognize roads in Taiwan.  Two features are associated with each InfoMap template category x (e.g., location and organization). The first is Current-Char-InfoMap-x-Begin, which is set to 1 for the first character of a matched string and set to 0 for the remaining characters. The other is Current-Char-InfoMap-x-Continue, which is set to 1 for all the characters of matched string except for the first character and set to 0 for the first character. The intuition behind this is as follows: InfoMap can be used to help ME detect which character in a sentence is the first character of the location name and which characters are the remaining characters of a location name. That is, Current-Char-InfoMap-x-Begin is helpful for determining which character should be tagged as x_begin, while Current-Char-InfoMap-x-Continue is helpful for determining which character should be tagged as x_continue if we build an InfoMap template for that category x. The two features associated with x category are shown below:  f  (h,  o)  =  ⎧1: if Current ⎩⎨0 : else  -  Char  -  InfoMap  -  x  -  Begin  =  true  and  o  =  x  _  begin  (3)  f  (h, o)  =  ⎧1:if Current⎩⎨0 :else  Char-  InfoMap-  x  -  Continue=  trueand  o  =  x  _  continue  (4)  74  Tzong-Han Tsai et al.  When recognizing a location name in a sentence, we test if any location templates match the sentence. If several matched templates overlap, we select the longest matched one. As mentioned above, the feature Current-Character-InfoMap-Location-Begin of the first character of the matched string is set to 1 while the feature Current-Character-InfoMap-Location-Continue of the remaining characters of the matched string is set to 1. Table 3 shows the necessary conditions for each organization feature and gives examples of matched data.  Table 3. Location features. Feature Current-Char-InfoMap-Location-Begin Current-Char-InfoMap-Location-Continue  Feature Conditions c0~cn-1 matches an InfoMap location template, where the character length of the template is n ca…c0….cb matches an InfoMap location template, where a is a negative integer and b is a non-negative integer  Example “台”北縣板 橋市 台”北”縣板 橋市  Explanations Probably the leading character of a location name. Probably a continuing character of a location name.  3.2.3 Features for Recognizing Organization Names Organizations include named corporate, governmental, or other organizational entities. The difficulty in recognizing an organization name is that it usually begins with a location name, such as 台北市地檢署 (Taipei District Public Prosecutors Office). Therefore, traditional machine learning NER systems can only identify the location part rather than the full organization name. For example, the system only extracts 台北市 (Taipei City) from 台北市 SOGO 百貨週年慶 (Taipei SOGO Department Store Anniversary) rather than 台北市 SOGO 百貨 (Taipei SOGO Department Store). According to our analysis of the structure of Chinese organization names, they mostly end with a specific keyword or begin with a location name. Therefore, we use those keywords and location names as the boundary markers of organization names. Based on our observation, we categorize organization names into four types according to their boundary markers. Type I: With left and right boundary markers The organization names in this category begin with by one or more geographical names and  Mencius: A Chinese Named Entity Recognizer Using the  75  Maximum Entropy-based Hybrid Model  ended by an organization keyword. For example, 台北市 (Taipei City) is the left boundary marker of 台北市捷運公司 (Taipei City Rapid Transit Corporation), while an organization keyword, 公司 (Corporation), is the right boundary marker.  Type II: With a left boundary marker  The organization names in this category begin with by one or more than one geographical names, but the organization keyword (e.g., 公司 (Corporation)) is omitted. For example, 台灣捷安特 (Giant Taiwan) only contains the left boundary 台灣 (Taiwan).  Type III: With a right boundary marker  The organization names in this category end with an organization keyword. For example, 捷安 特公司 (Giant Corporation) only contains the right boundary 公司 (Corporation).  Table 4. Organization features. Feature Current-Char-InfoMap-Organization-Begin Current-Char-InfoMap-Organization-Continue Current-Char-Organization-Keyword  Feature Conditions c0~cn-1 matches an InfoMap organization template, where the character length of the template is n ca…c0….cb matches an InfoMap organization template, where a is a negative integer and b is a non-negative integer c0 or c0c1 or c-1c0 is in the organization keyword list  Example “台”北市 捷運公司 台”北”市 捷運公司 “公”司, 公 “司”  Explanations Probably the leading character of an organization name Probably the leading character of an organization name Probably part of an organization keyword  Type IV: No boundary marker In this category, both left and right boundaries as above mentioned are omitted, for example, 捷 安特 (Giant). The organization names in this category are usually in abbreviated form. In Mencius, we build templates for recognizing Type I organization names. Each organization template begins with a location name in GeoMap and ends with an organization keyword. For example, we can build [通用地理.台灣.市]:$$(2..4):局([GeoMap.Taiwan.Cities]:  76  Tzong-Han Tsai et al.  $$(2..4):Department) to recognize county level government departments in Taiwan. However, in Types II, III, and IV, organization names cannot be recognized by templates. Therefore, the maximum entropy model uses features of characters (from c-2 to c2), tags (from t-2 to t2), and organization keywords, e.g., 公司 (Corporation), to find the most likely tag sequences and recognize them. When a string matches an organization template, the feature Current-Character- InfoMap-Organization-Start of the first character is set to 1. In addition, the feature Current-Character-InfoMap-Organization-Continue of the remaining characters is set to 1. The necessary conditions for each organization feature and examples of matched data are shown in Table 4. These features are helpful for recognizing organization names. 4. Experiments  4.1 Data Sets  For Chinese NER, the most famous corpus is MET-2 [6]. There are two main differences between our corpus and MET-2: the number of domains and the amount of data. First, MET-2 contains only one domain (Accident), while our corpus, which was collected from the online United Daily News in December 2002 (http://www.udn.com.tw), contains six domains: Local News, Social Affairs, Investment, Politics, Headline News and Business, which provide a greater variety of organization names than a single domain corpus can. The full location names and organization names are comparatively longer, and our corpus contains more location names and addresses at the county level. Therefore, the patterns of location names and organization names are more complex in our corpus.  Secondly, our corpus is much larger than MET2, which contains 174 Chinese PER, 750 LOC, and 377 ORG. Our corpus contains 1,242 Chinese PER, 954 LOC, and 1,147 ORG in 10,000 sentences (about 126,872 Chinese characters). The statistics of our data are shown in Table 5.  Table 5. Statistics of the data Set  Domain Local News Social Affairs Investment Politics Headline News Business Total  Number of Named Entities  PER  LOC  84  139  310  287  20  63  419  209  267  70  142  186  1242  954  ORG 97 354 33 233 243 187 1147  Size (in characters) 11835 37719 14397 17168 19938 25815 126872  Mencius: A Chinese Named Entity Recognizer Using the  77  Maximum Entropy-based Hybrid Model  4.2 Experimental Results To understand how word segmentation might influence Chinese NER and the differences between a pure template-based method and our hybrid method, we configure Mencius using the following four settings: (1) Template-based with Char-based Tokenization (TC), (2) Template-based with Word-based Tokenization (TW), (3) Hybrid with Char-based Tokenization (HC), and (4) Hybrid with Word-based Tokenization (HW). Following the standard 10-fold cross-validation method, we tested Mencius with each configuration using the data set mentioned in section 4.1. The following subsections provide details about each configuration and the results obtained.  4.2.1 Template-based with Char-based Tokenization (TC)  In this experiment, we regarded each character as a token, and used a person name list and InfoMap templates to recognize all named entities. The number of lexicons in the person name lists and gazetteers was 32000. As shown in Table 6, the obtained F-Measures of PER, LOC and ORG were 76.2%, 75.4% and 75.1%, respectively.  Table 6. Performance of the Template-based System with Char-based Tokenization.  NE PER LOC ORG Total  P(%) 64.77 76.41 85.60 72.95  R(%) 92.59 74.42 66.93 78.62  F(%) 76.22 75.40 75.12 75.67  4.2.2 Template-based with Word-based Tokenization (TW) In this experiment, we used a word segmentation module based on the 100,000-word CKIP Traditional Chinese dictionary to split sentences into tokens. This module combines forward and backward longest matching algorithms in the following way: if the segmentation results of the two algorithms agree in certain substrings, this module outputs tokens in those substrings. While in the part which the segmentation results of the two algorithms differ, this module skips word tokens and only outputs character tokens. In the previous test, 98% of the word tokens were valid words. Then, we used person name lists and InfoMap templates to recognize all the named entities. The number of lexicons in the person name lists and gazetteers was 32,000. As shown in Table 6, the obtained F-Measures of PER, LOC and ORG were 89.0%, 74.1% and 71.6%, respectively.  78  Tzong-Han Tsai et al.  Table 7. Performance of the Template-based System with Word-based Tokenization.  NE PER LOC ORG Total  P(%) 88.69 76.92 85.66 84.14  R(%) 89.32 71.44 61.44 74.70  F(%) 89.00 74.08 71.55 79.14  4.2.3 Hybrid with Char-based Tokenization (HC)  In this experiment, we regarded each character as a token without performing any word segmentation. We then integrated person name lists, location templates, and organization templates into a Maximum-Entropy-Based framework. As shown in Table 8, the obtained F-Measures of PER, LOC and ORG were 94.3%, 77.8% and 75.3%, respectively.  Table 8. Performance of the Hybrid System with Char-based Tokenization.  NE PER LOC ORG Total  P(%) 96.97 80.96 87.16 89.05  R(%) 91.71 74.81 66.22 78.18  F(%) 94.27 77.76 75.26 83.26  4.2.4 Hybrid System with Word-based Tokenization (HW)  In this experiment, we used the same word segmentation module described in section 4.2.2 to split sentences into tokens. Then, we integrated person name lists, location templates, and organization templates into a Maximum-Entropy-Based framework. As shown in Table 9, the obtained F-Measures of PER, LOC and ORG were 95.9%, 73.4% and 76.1%, respectively.  Table 9. Performance of the Hybrid System with Word-based Tokenization.  NE PER LOC ORG Total  P(%) 98.74 81.46 87.54 90.33  R(%) 93.31 66.73 67.29 76.66  F(%) 95.94 73.36 76.09 82.93  4.2.5 Comparisons TC versus TW We observed that TW achieved much higher precision than TC in PER. When word segmentation is not performed, some trigrams and quadgrams may falsely appear to be person names. Take the sentence “新古典主義” for example. TC would extract “古典主” as a person  Mencius: A Chinese Named Entity Recognizer Using the  79  Maximum Entropy-based Hybrid Model  name since “古典主” matches our family-name trigram template. However, in TW, thanks to word segmentation, “古典” and “主義” would be marked as tokens first and would not match the family-name trigram template. HC versus HW 
In order to achieve fast, high quality Part-of-speech (pos) tagging, algorithms should achieve high accuracy and require less manually proofreading. This study aimed to achieve these goals by defining a new criterion of tagging reliability, the estimated final accuracy of the tagging under a fixed amount of proofreading, to be used to judge how cost-effective a tagging algorithm is. In this paper, we also propose a new tagging algorithm, called the context-rule model, to achieve cost-effective tagging. The context rule model utilizes broad context information to improve tagging accuracy. In experiments, we compared the tagging accuracy and reliability of the context-rule model, Markov bi-gram model and word-dependent Markov bi-gram model. The result showed that the context-rule model outperformed both Markov models. Comparing the models based on tagging accuracy, the context-rule model reduced the number of errors 20% more than the other two Markov models did. For the best cost-effective tagging algorithm to achieve 99% tagging accuracy, it was estimated that, on average, 20% of the samples of ambiguous words needed to be rechecked. We also compared tradeoff between the amount of proofreading needed and final accuracy for the different algorithms. It turns out that an algorithm with the highest accuracy may not always be the most reliable algorithm. Keywords: part-of-speech tagging, corpus, reliability, ambiguous resolution 1. Introduction Part-of-speech (pos) tagging for a large corpus is a labor intensive and time-consuming task. Most tagging algorithms try to achieve high accuracy, but 100% accuracy is an impossible goal. Even after tremendous amounts of time and labor are spent on the post-process of proofreading, many errors still exist in publicly available tagged corpora. Therefore, in order to achieve fast, high quality pos tagging, tagging algorithms should not only achieve high accuracy but also require less manually proofreading. In this paper, we propose a context-rule  * Institute of Information Science, Academia Sinica 128 Academia Rd. Sec.2, Nankang, Taipei, Taiwan E-mail: {eddie,kchen}@iis.sinica.edu.tw  84  Yu-Fang Tsai, and Keh-Jiann Chen  model to achieve both goals. The first goal is to improve tagging accuracy. According to our observation, the pos tagging of a word depends on its context but not simply on its context category. Therefore, the proposed context-rule model utilizes a broad scope of context information to perform pos tagging of a word. Rich context information helps to improve the model coverage rate and tagging accuracy. The context-rule model will be described in more detail later in this paper. Our second goal is to reduce the manual editing effort. A new concept of reliable tagging is proposed. The idea is as follows. An evaluation score is assigned to each tagging decision as an indicator of tagging confidence. If a high confidence value is achieved, it indicates that the tagging decision is very likely correct. On the other hand, a low confidence value means that the tagging decision requires manual checking. If a tagging algorithm can achieve a high degree of reliability in evaluation, this means that most of the high confidence tagging results need not manually rechecked. As a result, the time and manual efforts required in the tagging process can be drastically reduced. The reliability of a tagging algorithm is defined as follows: Reliability = The estimated final accuracy achieved by the tagging model under the constraint that only a fixed number of target words with the lowest confidence values are manually proofread. The notion of tagging reliability is slightly different from the notion of tagging accuracy since high accurate algorithm may require more manual proofreading than a reliable algorithm that achieves lower accuracy. The rest of this paper is organized as follows. In section 2, the relation between reliability and accuracy is discussed. In section 3, three different tagging algorithms, the Markov pos bi-gram model, word-dependent Markov bi-gram model, and context-rule model, are discussed. In section 4, the three algorithms are compared based on tagging accuracy. In addition, confidence measures of tagging results are defined, and the most cost-effective algorithm is determined. Conclusions are drawn on section 5. 2. Reliability vs. Accuracy The reported accuracy of automatic tagging algorithms ranges from about 95% to 96% [Chang et al., 1993; Lua, 1996; Liu et al., 1995]. If we can pinpoint errors, then only 4~5% of the target corpus has to be revised to achieve 100% accuracy. However, since the errors are not identified, conventionally, the whole corpus has to be re-examined. This is most tedious and time consuming since a practically useful tagged corpus is at least several million words in size. In order to reduce the amount manual editing required and speed up the process of constructing a large tagged corpus, only potential tagging errors should be rechecked manually [Kveton et al., 2002; Nakagawa et al., 2002]. The problem is how to find the  Reliable and Cost-Effective Pos-Tagging  85  potential errors. Suppose that a probabilistic-based tagging method assigns a probability to each pos of a target word by investigating the context of this target word w. The hypothesis is that if the probability P ( c1 | w , context ) of the top choice candidate c1 is much higher than the probability P (c 2 | w, context ) of the second choice candidate c2 , then the confidence value assigned to c1 will also be higher. (Hereafter, for the purpose of simplification, we will use P(c) to stand for P(c | w, context ) , if without confusing.) Likewise, if the probability P(c1 ) is close to the probability P(c2 ) , then the confidence value assigned to c1 will also be lower. We aim to prove the above hypothesis by using empirical methods. For each different tagging method, we define its confidence measure according to the above hypothesis and examine whether tagging errors are likely to occur for words with low tagging confidence. If the hypothesis is true, we can proofread among the auto-tagged results only those words with low confidence values. Furthermore, the final accuracy of the tagging process after partial proofreading is done can also be estimated based on the accuracy of the tagging algorithm and the number of errors contained in the proofread data. For instance, suppose that a system has a tagging accuracy of 94%, and that K% of the target words with the lowest confidence scores covers 80% of the errors. After those K% of tagged words are proofread, 80% of the errors are fixed. Therefore, the reliability score of this tagging system of K% proofread words will be 1 - (error rate) * (reduced error rate) = 1 - ((1 - accuracy rate) * 20%) = 1 - ((1 - 94%) * 20%) = 98.8%. On the other hand, suppose that another tagging system has a higher tagging accuracy of 96%, but that its confidence measure is not very high, such that K% of the words with the lowest confidence scores contains only 50% of the errors. Then the reliability of this system is 1 - ((1 - 96%) * 50%) = 98%, which is lower than that of the first system. That is to say, after expending the same amount of effort on manual proofreading, the first system achieves better results even though it has lower tagging accuracy. In other words, a reliable system is more cost-effective. 3. Tagging Algorithms and Confidence Measures In this paper, we will evaluate three different tagging algorithms based on the same training and testing data, compare them based on tgging accuracy, and determine the most reliable tagging algorithm among them. The three tagging algorithms are the Markov bi-gram model, word-dependent Markov model, and context-rule model. The training data and testing data were extracted from the Sinica corpus, a 5 million word balanced Chinese corpus with pos tagging [Chen et al., 1996]. The confidence measure was defined for each algorithm, and the final accuracy was estimated with the constraint that only a fixed amount of testing data needed to be proofread.  86  Yu-Fang Tsai, and Keh-Jiann Chen  Table 1. Sample keyword-in-context file of the words ‘研究’ sorted according to its left/right context.  的(DE) 相當(Dfa) 內(Ncd) 仍(D) 民族(Na) 赴(VCL) 亦(D) 合宜性(Na) 更(D)  重要(VH) 重視(VJ) 重點(Na) 限於(VJ) 音樂(Na) 香港(Nc) 值得(VH) 值得(VH) 值得(VH)  研究(Nv) 研究(Nv) 研究(Nv) 研究(Nv) 研究(VE) 研究(VE) 研究(VE) 研究(VE) 研究(Nv)  機構(Na) 開發(Nv) 需求(Na) 階段(Na) 者(Na) 該(Nes) 。(PERIODCATEGORY) 。(PERIODCATEGORY) 。(PERIODCATEGORY)  之(DE) ，(COMMACATEGORY) 。(PERIODCATEGORY) 。(PERIODCATEGORY) 明立國(Nb) 地(Na)  It is easier to proofread and obtain consistent tagging results if proofreading is done by checking each ambiguous word in its keyword-in-context file. For instance, in Table 1, the keyword-in-context file of the word ‘研究’ (research), which has pos of verb type VE and noun type Nv, is sorted according to its left/right context. Proofreaders can take the other examples as references to determine whether tagging results are correct. If all of the occurrences of ambiguous words had to be rechecked, this would require too much work. Therefore, only words with low confidence scores will be rechecked.  A general confidence measure can be defined as  P (c1 )  , where  P (c1 ) + P (c 2 )  P ( c 1 ) is the  the probability of the top choice pos c1 assigned by the tagging algorithm and P (c 2 ) is the probability of the second choice pos c2 1. The common terms used in the following tagging algorithms discussed below are defined as follows:  w k  the k-th word in a sequence;  c k  the pos associated with the k-th word wk ;  w1c1 ,..., w n c n a word sequence containing n words with their associated categories.  3.1 Markov Bi-gram Model The most widely used tagging models are the part-of-speech n-gram models, in particular, the  
In this paper, we propose a series of natural language processing techniques to be used to extract important topics in a given research field. Topics as defined in this paper are important research problems, theories, and technical methods of the examined field, and we can represent them with groups of relevant terms. The terms are extracted from the texts of papers published in the field, including titles, abstracts, and bibliographies, because they convey important research information and are relevant to knowledge in that field. The topics can provide a clear outline of the field for researchers and are also useful for identifying users’ information *世新大學資訊傳播學系 Department of Information and Communications, Shih-Hsin University, Taipei, Taiwan, R.O.C. Email: scl@cc.shu.edu.tw  98  林頌堅  needs when they are applied to information retrieval. To facilitate topic extraction, key terms in both Chinese and English are extracted from papers and are clustered into groups consisting of terms that frequently co-occur with each other. First, a PAT-tree is generated that stores all possible character strings appearing in the texts of papers. Character strings are retrieved from the PAT-tree as candidates of extracted terms and are tested using the statistical information of the string to filter out impossible candidates. The statistical information for a string includes (1) the total frequency count of the string in all the input papers, (2) the sum of the average frequency and the standard deviation of the string in each paper, and (3) the complexity of the front and rear adjacent character of the string. The total frequency count of the string and the sum of its average frequency and standard deviation are used to measure the importance of the corresponding term to the field. The complexity of adjacent characters is a criterion used to determine whether the string is a complete token of a term. The less complexity the adjacent characters, the more likely the string is a partial token of other terms. Finally, if the leftmost or rightmost part of a string is a stop word, the string is also filtered out. The extracted results are clustered to generate term groups according to their co-occurrences. Several techniques are used in the clustering algorithm to obtain multiple clustering results, including the clique algorithm and a group merging procedure. When the clique algorithm is performed, the latent semantic indexing technique is used to estimate the relevance between two terms to improve the deficiency of term co-occurrences in the papers. Two term groups are further merged into a new one when their members are similar because it is possible that the clusters represent the same topic. The above techniques were applied to the proceedings of ROCLING to uncover topics in the field of computational linguistics. The results show that the key terms in both Chinese and English were extracted successfully, and that the clustered groups represented the topics of computational linguistics. Therefore, the initial study proved the feasibility of the proposed techniques. The extracted topics included “machine translation,” “speech processing,” “information retrieval,” “grammars and parsers,” “Chinese word segmentation,” and “statistical language models.” From the results, we can observe that there is a close relation between basic research and applications in computational linguistics. Keywords: Topic extraction, term extraction, term clustering 1. 緒論 本論文提出一個自動化的主題抽取方法，利用論文中的詞彙訊息來抽取學術領域的主 題。論文的題名、摘要、本文，甚至所引用的參考文獻題名等文字資料表達了研究的問  基於術語抽取與術語叢集技術的主題抽取  99  題、方法與結果，因此這些論文資料中的術語與研究主題非常相關。以本論文做一例子， 在題名、摘要和本文出現許多『學術領域』、『主題』、『論文』、『抽取』等等術語， 可以了解這個研究與從學術論文中抽取主題相關。所以抽取論文中的術語可以了解論文 的主題。在一個學術領域中，受到重視的主題的相關術語會在許多論文中出現。以計算 語言學領域為例，許多論文包含了諸如『語料庫』、『剖析』、『資訊檢索』等等術語， 因為它們與這個領域的重要主題相關。而且進一步地，主題相關的術語會經常一起出現， 具有較強的共現(co-occurrence)關係。因此，如果對學術領域相關的論文進行分析，選取 具有高頻而代表主題意義的術語，利用共現資訊將相關的術語叢集成一個集合，所形成 的術語集合便可以視為是領域中重要的主題。在分析論文的主題時，便可以透過論文對 各術語集合的相關性來進行評估。 因應學術論文較多獨特術語的特性，本研究在術語抽取(term extraction)的技術上， 參考[Chien, 1997]、[Chien, et. al., 1999]和[Zhang, et. al., 2000]等統計方法，利用字串的頻 次為基礎的統計訊息，從論文中抽取多語的術語。在術語叢集(term clustering)上，則考 慮同義詞和一詞多義的現象，利用 LSI (latent semantics indexing) [Deerwester, et. al., 1990] 和 clique 叢集演算法[Kowalski and Maybury, 2000]等技術，將經常共現的術語叢集起來。 在應用上，我們使用 ROCLING 一到十四屆學術研討會的論文資料，進行術語抽取與術 語叢集。研究結果初步驗證了這些技術用於主題抽取的可行性。 本論文其餘的章節架構如下：在第二節中扼要說明相關研究及所提出一系列之技 術。接著在第三節和第四節中分述這個研究的核心技術：術語抽取和術語叢集。第四節 中並且說明主題與論文之間相關程度的計算方式。第五節是應用這些技術到國內計算語 言學領域的研究與結果。第六節則是本論文的結論。 2. 本論文提出的主題分析方法 本論文希望發展主題抽取的技術，從相關論文抽取重要的主題。在資訊檢索研究的範疇 中類似的研究有主題偵測(topic detection)。主題偵測希望從一序列來源各不相同的新聞 中，偵測出某些『事件』(events)相關的連續報導[Wayne, 2000]。目前許多的研究利用『叢 集 假 說 』 (cluster hypothesis) 來 解 決 這 個 問 題 [Yang, Pierce and Carbonell, 1998][Hatzivassiloglou, Gravano and Maganti, 2000]，以文件叢集(document clustering)技 術，利用相關文件具有相似的術語分布，統計新進文件的術語分布情形，將文件歸入相 關事件的集合中。因此，本論文也嘗試應用叢集假說發展相關技術。再者，主題偵測研 究 已 應 用 專 有 名 詞 (proper nouns) 等 術 語 作 為 區 隔 不 同 新 聞 事 件 的 重 要 訊 息 [Hatzivassiloglou, Gravano and Maganti, 2000]，因此本論文也將嘗試利用論文中的相關術 語。此外，主題偵測應用所謂的『新聞熱潮』(news bursts)現象，將時間訊息加入叢集演 算法，提昇偵測的結果[Yang, Pierce and Carbonell, 1998]。雖然學術論文有所謂『資訊流 行』(information epidemics)的說法，然而在實證研究中卻發現此一現象雖然存在，但並 不常見[Tabah, 1996]，所以在本論文並不考慮加入時間訊息。 在本論文中，我們利用術語在論文中的共現關係，找出術語的叢集情形來代表主題。  100  林頌堅  以論文中出現的術語取代整篇論文作為分析對象的主要原因是希望能獲得較可信賴的統 計訊息。由於較小的學術領域所出版的論文數量較為不足，統計上不易得到滿意的分析 結果。以術語作為分析對象，因為數量較多，可以獲得充足的統計訊息，克服文件數量 較少的問題。具有多個主題的論文也可藉由術語的叢集，找出所有的相關主題，並且進 而探索主題間的關係。此外，文件叢集不易直接詮釋結果所代表的主題，術語叢集則可 以由成員的語意進行解釋。 本論文方法的架構如圖一所示。首先對需要進行分析的學術領域蒐集相關論文資 料，建立論文資料庫。資料庫中收錄的資料包括論文的題名、摘要和參考文獻的題名等 作為術語抽取與叢集分析的資訊，論文作者和出版年等項目則可以用來作為後續的分析 工作上。特別值得一提的是，國內的學術論文基本上是中、英語雙語並行，許多領域皆 接受論文以中文或英文發表，然而並非所有的論文都具有雙語的題名和摘要。若只針對 以某一種語言發表的論文進行分析，而忽略另一種語言，有可能造成某些主題被遺漏的 情形。若是分別處理各種語言的論文，缺乏分屬兩種語言的術語在論文中的共現訊息， 無法分析出這些術語的相關性，在整合上有相當大的困難。因此需要考慮這個特殊的論 文發表現象，提出可以同時獲得兩種語言的術語之方法。本論文所提出的解決之道是加 入論文中參考文獻的題名進行分析，通常論文的主題與其他的文獻相關時會加以引用， 因此參考文獻的題名與主題間也有密切的關係，加入參考文獻的題名可以增加分析的資 訊，而且引用的參考文獻可能來自中英文兩種語言，若能利用適當的多語術語抽取技術， 便可以統計分屬兩種語言的相關術語的共現現象，整合兩種語言的術語訊息，得到較佳 的結果。  論文蒐集與 資料庫建立  論文 資料庫  關鍵術語 抽取  關鍵 術語  術語 共現分析  術語 共現關係  術語 叢集  術語 集合  主題 分析 圖一 本論文的主題抽取方法  基於術語抽取與術語叢集技術的主題抽取  101  在建立好論文資料庫後，便利用第 3 節所描述的多語術語抽取方法從論文資料中自 動抽取領域中具有意義的術語。接著以第 4 節的術語叢集技術統計術語在論文中的共現 關係，將相關的術語叢集成集合，用來代表特定的主題。進行主題分析時，對於某一主 題，可以根據術語集合與論文的相關程度，取出具有主題的論文。  3. 多語環境下的術語抽取  為 了 抽 取 主 題 相 關 的 術 語 ， 我 們 首 先 確 認 重 要 的 中 英 文 詞 組 (phrases) 以 及 中 文 的 多 字 詞，再選擇具有代表意義的術語，作為這一階段的結果。在學術論文中，常以詞組的形 式表達重要的主題，比方在計算語言學領域中，可以發現如英文的“language model”、 “machine translation”或是中文的“語言模型”、 “機器翻譯”等等都是重要術語。此外，中 文的文本裡，詞與詞之間沒有明顯的界限，進行自然語言處理前，需要先斷詞。然而學 術論文中經常有許多新的術語出現，來代表新的概念、方法和技術，我們無法事先收錄 各個領域裡所有可能的術語來製作十分完整的詞典，進行斷詞。而且利用構詞律的規則 式斷詞方法，需要處理同時中文和英文兩種語言，難以整合應用。所以本論文採用統計 式的處理方法[Chien, et. al., 1999] ，以便同時解決中文的多字詞及中英文的詞組問題。  在過去對於術語抽取的相關研究中，曾利用字串的『相對頻率』(relative frequency)、 『互見資訊』(mutual information)和『上下文依附』(context dependency)等各種統計訊息 [Su, et. al., 1994][Chien, 1997][Zhang, et. al., 2000]。字串的『相對頻率』是指該字串的出 現頻次與語料中所有長度相同字串平均頻次的比值，可以測量字串的重要性，相對頻率 愈大的字串愈重要，愈可能是一個術語[Su, et. al., 1994]。『互見資訊』雖然有不同計算 公式，但都是用來測量組成術語的字或詞彼此間的相互關係(association)，成員間『互見 資訊』愈高的字串，愈有可能是一個術語[Su, et. al., 1994] [Zhang, et. al., 2000]。『上下 文依附』則用來測量字串與上下文字詞間的依附程度，依附程度較大的字串可能是術語 的一個部份，不應被抽取出來；反之，字串的依附程度較小，則可能代表是術語的邊界 [Chien, 1997][Zhang, et. al., 2000]。  本論文所使用的方法如下：首先利用題名、摘要和參考文獻的題名等論文資料建立  一個 PAT-tree 資料結構，儲存所有出現在論文資料中的字串及它們所在的論文資料  [Chien, 1997]。接著在 PAT-tree 中擷取可能的字串作為候選術語，以統計訊息及經驗法  則(heuristic rules)作為判斷是否為術語的標準。在本論文中，所使用的統計訊息包括字串  在所有資料中的出現總次數、字串的平均頻次和標準差(standard deviation)以及字串前後  接字的複雜度。其中，字串的出現總次數、平均頻次和標準差等統計訊息和相關研究中  的『相對頻率』作用相同，在於衡量字串的重要性。字串的出現總次數代表在領域中的  重要性，總次數高表示這個字串在領域裡的論文經常出現而具有重要意義。字串對於出  現論文的重要程度則用字串的平均頻次和標準差來表示，如式(1)  def  RS = mS + σ S  (1)  mS 和 σS 分別代表字串 S 的平均頻次和標準差。當字串 S 的平均頻次超過某一閾值  102  林頌堅  時，表示此字串極有可能在許多論文中出現多次，是這些論文的關鍵術語，應該被選取 出來。或者雖然字串 S 的平均頻次較低，但在某些論文中出現相當多次，是這些論文的 關鍵術語，也需要被選取出來，此時字串 S 會有一個較大的標準差 σS。因此，我們可以 利用字串的平均頻次和標準差的總和 RS 代表字串對出現論文的重要程度，RS 值愈高的字 串對出現論文愈重要。 前後接字的複雜度則和『上下文依附』的作用相同，可以判斷字串是否是一個完整 的術語或是其他術語的部分，字串 S 的前後接字複雜度 C1S 和 C2S 分別如式(2a)和(2b)所 示  ∑ def C1S = −  a  FaS log( FaS )  FS  FS  (2a)  ∑ def C2S = −  b  FSb log( FSb )  FS  FS  (2b)  式(2a)和(2b)中，a 和 b 代表字串 S 在論文資料中任一個可能的前接字和後接字，FS、 FaS 和 FSb 分別是字串 S、aS 和 Sb 的出現總次數。以式(2a)前接字的情形來看，若是字串 S 有愈多種類的前接字，而且每一種前接字出現的次數越接近時，C1S 的值愈大，反之， 當字串前只有一種前接字時，C1S 的值等於 0，或是有一個前接字出現的機會較其他大非 常多時，則 C1S 的值接近於 0，表示該字串再加上這個前接字可能才是一個術語，所以前 接字複雜度愈大代表該字串愈有可能是獨立的術語。後接字的情形也是相同的道理。 通過上面條件的字串，再利用停用詞(stop words)不能出現在字串首尾的經驗法則， 進一步過濾不完整的術語。在過去的經驗中，介詞、連接詞和補語等停用詞常出現在抽 取出字串的首尾，如 “名詞+的”、“名詞+of”或“to+動詞”等詞組結構。但停用詞出現在字 串的中間代表特定的詞組，例如“part of speech”，因此，將這種情形加以保留。 在確認論文資料中重要的中英文詞組以及中文的多字詞後，以這些術語建立斷詞處 理所需的詞典。我們使用長詞優先法則與術語的出現總頻次將所有論文資料加以斷詞。 論文資料經過斷詞處理後，將產生了一些中英文的詞組、詞和一些中文單字。在這一階 段的目標是抽取論文中所有可能代表主題的術語，因此我們過濾具有以下情形的字串。 首先是中文單字，多半是一些停用詞或是無法組成術語的片段。其次，出現總次數與式 (1)之 RS 值太小的術語，因為對領域的重要性較低，也加以過濾。剩下的術語則是下一階 段分析的對象。 4. 術語叢集 本論文依據術語在論文資料的共現關係，將術語進行叢集，以一組叢集的相關術語作為 一個主題。由於有些術語可能包含在不同的主題中，本節中提出一個可以對術語進行多 重叢集的演算法。  基於術語抽取與術語叢集技術的主題抽取  103  首先，我們將上一階段抽取出來的術語，利用 cliques 叢集演算法[Kowalski and Maybury, 2000]進行術語叢集。cliques 叢集演算法在選定最小相關程度的情形下，可以 得到若干個術語集合，在集合中的術語，彼此間的相關程度都在所選定的最小相關程度 之上，而且術語可以被歸類到多個集合，因此符合多重叢集的要求。本論文所使用的相 關程度計算方式如下：我們先計算每一術語在每一筆論文資料中出現的頻次，作為術語 的特徵值。但因為術語在論文資料裡出現頻次不高，為了使低頻次的術語差異不會太大， 以頻次的平方根作為特徵值，並除以術語的總頻次進行正規化(normalization)。如此一 來，對每一術語便有一組特徵向量(feature vector)，如式(3)表示某一術語 A 的特徵向量。  r vA  def =  [  f A,1 ,  f A,2 ,..., N  f A,N ]T  (3)  ∑ f A,i  i =1  N  式(3)中，fA,i代表術語A在第i篇論文資料中出現的頻次，分母的  ∑ fA,i i=1  是  A  的總頻次。術語間的相關程度便可以利用特徵向量的內積 (inner product)來估  算。  經過 cliques 演算法與上述的相關程度計算方式所得到的結果是相當嚴格的，相關術 語若要叢集在同一個集合中，所有術語彼此間的共現關係必須都很強。然而，在論文中 相同或相近的概念可能以不同術語來表示，使得相關的術語不一定經常共同出現，利用 上述的估算方法將會得到很小的相關程度，無法將這些術語叢集起來。為此，本論文採 用以下兩種技術來加以補救。 首先我們改用 LSI 技術估算術語間的相關程度。LSI 技術是利用奇異值分解 (SVD, singular value decomposition)對上述的特徵向量所形成的『術語-特徵』矩陣 M 進行分解 [Deerwester, et. al., 1990]，產生新的矩陣 Mˆ ，假設 Mˆ 的秩(rank)為 k，k 小於或等於原先 矩陣 M 的秩，則 Mˆ 是所有秩為 k 的矩陣中，與 M 的平方差最小的矩陣。以術語在新矩 陣 Mˆ 所對應的行向量(row vector)取代原先之特徵向量，當進行術語的相關程度估算時， 便可以 Mˆ Mˆ T 來估算原先以 MMT 計算兩兩術語特徵向量間的內積值。利用 SVD 可以取 得隱含語義結構(latent semantic structure)，使得原先因為共現關係較弱或是不存在的兩個 相關術語，獲得較大的估算值[Deerwester, et. al., 1990]。 其次，在進行 cliques 叢集演算法後，對於所得到的結果依據它們成員間重疊的情形 進行合併。假設兩個集合之間有多個成員是相同的，這兩個集合很可能屬於同一主題， 我們即將這兩個術語集合進行聯集，產生新集合。以數學式表示如下，Χ1 和Χ2 為兩個集 合，如果|Χ1∩Χ2|≧c*Min(|Χ1|,|Χ2| )，則合併成新的集合Χ3，此處 c*Min(|Χ1|,|Χ2| )是 兩個集合的最小相同成員數，c 是一個介於 1 與 0 間的實數，Min(|Χ1|,|Χ2| )是取出兩個 數值中最小值的函數。 經過上述的叢集處理後，可以得到代表重要主題的術語集合。在確認主題的相關論  104  林頌堅  文方面，可以利用 LSI 的估計方式[Deerwester, et. al., 1990]，計算每一術語集合與論文間 的相關程度。計算方式是將術語集合中每一個術語的特徵向量相加，再正規化成單位向 量，即可求得主題與所有論文之間的相關程度估算值。最後依據將相關程度大的論文資 料取出，作為主題的相關論文。 5. 國內計算語言學的主題抽取之實驗結果 計算語言學研討會 ROCLING 是國內的計算語言學領域相當重要的學術活動。因此， ROCLING 的研討會論文集，可以說是歷年來國內計算語言學領域學者的心血結晶，所 蘊含的主題也是他們所共同關心的主題。因此，本論文將以第一屆(1988)到第十四屆(2001) ROCLING 研討會的 235 篇論文資料做為分析國內計算語言學主題的素材。 進行術語抽取時，本論文根據字串長度將字串出現總次數的閾值作不同的設定，較 短的字串(2 或 3 字)設定為 15 次，較長的字串(4~5 字)則設定為 10 次，平均頻次和標準 差的總和 RS 和前後接字的複雜度分別設為 2.5 與 0.5。接著利用抽取出來的多字詞或詞 組對論文資料進行斷詞，過濾不是術語的字串，並進行統計。結果共得到 343 個術語， 表一是出現總次數最高的前 50 個術語及它們的出現次數。表一中列出的術語大多屬於概 念較廣泛的術語。這些術語出現在較多論文資料中，因此出現次數較高。表一中有些是 其他領域也常見的術語，比方說『系統』、『方法』、『分析』等等，但許多術語和計 算機科學及語言學相關，如『parsing』、『data』、『speech』、『lexical』等等，或是 本身即是計算語言學特有的概念，如『speech recognition』、『machine translation』等等。  表一 術語抽取所得到的前 50 個出現總次數最高的術語  次序 詞名  出現次數 次序 詞名  出現次數 次序 詞名  出現次數  
This paper reveals some important properties of CFSs and applications in Chinese natural language processing (NLP). We have previously proposed a method for extracting Chinese frequent strings that contain unknown words from a Chinese corpus [Lin and Yu 2001]. We found that CFSs contain many 4-character strings, 3-word strings, and longer n-grams. Such information can only be derived from an extremely large corpus using a traditional language model(LM). In contrast to using a traditional LM, we can achieve high precision and efficiency by using CFSs to solve Chinese toneless phoneme-to-character conversion and to correct Chinese spelling errors with a small training corpus. An accuracy rate of 92.86% was achieved for Chinese toneless phoneme-to-character conversion, and an accuracy rate of 87.32% was achieved for Chinese spelling error correction. We also attempted to assign syntactic categories to a CFS. The accuracy rate for assigning syntactic categories to the CFSs was 88.53% for outside testing when the syntactic categories of the highest level were used. Keywords: Chinese frequent strings, unknown words, Chinese toneless phoneme-to-character, Chinese spelling error correction, language model.  1. Introduction  An increasing number of new or unknown words are being used on the Internet. Such new or unknown words are called “out of vocabulary (OOV) words” [Yang 1998], and they are not listed in traditional dictionaries. Many researchers have overcome problems caused by OOV words by using N-gram LMs along with smoothing methods. N-gram LMs have many useful applications in NLP [Yang 1998]. In Chinese NLP tasks, word-based bi-gram LMs are used by many researchers. To obtain useful probabilities for training, a corpus size proportional to 800002 (80000 is the approximate number of words in ASCED) = 6.4*109 words is required.  * Department of Information Management, Chien Kuo Institute of Technology, Changhua, 500 Taiwan  E-mail: yclin@ckit.edu.tw  Tel: 04-7111111 ext 3637  Fax:04-7111142  + Department of Computer Science, National Chung-Hsing University, Taichung, 40227 Taiwan  114  Yih-Jeng Lin, and Ming-Shing Yu  However, it is not easy to find such a corpus at the present time. A small-size corpus will lead too many unseen events when using N-gram LMs. Although we can apply some smoothing strategies, such as Witten-Bell interpolation or the Good-turing method [Wu and Zheng 2001] to estimate the probabilities of unseen events, this will be of no use when the size of training corpus is limited. From our observations, many the unseen events that occur when using N-gram LMs are unknown words or phrases. Such unknown words and phrases cannot be found in a dictionary. For example, the term “週休二 日” (two days off per week) is presently popular in Taiwan. We cannot find this term in a traditional dictionary. The term “週休二日” is a 4-word string pattern which consists of four words: “週” (a week), “休” (to rest), “二” (two), and “日” (day). A word-based 4-gram LM and a large training corpus are required to record the data of such terms. Such a word-base 4-gram LM has not been applied to Chinese NLP in practice, and such a huge training corpus cannot be found at present. Alternatively, we can record the specifics of the term “週休二日” by using a CFS with relatively limited training data in which the specified term appear two or more times. Such training data could be recorded in one or two news articles containing hundreds of Chinese characters. Many researchers have shown that frequent strings can be used in many applications [Jelinek 1990; Suhm and Waibel 1994]. We have shown that adding Chinese frequent strings (CFSs), including unknown words, can improve performance in Chinese NLP tasks [Lin and Yu 2001]. A CFS defined based on our research is a Chinese string which appears two or more times by itself in the corpus. For example, consider the following fragment: “國立中興大學，中興大學。” （National Chung-Hsing University, Chung-Hsing University.） “中興大學” (Chung-Hsing University) is a CFS since it appears twice and its appearances are not brought out by other longer strings. The string “中興” (Chung-Hsing) appears twice, but it is not a CFS here since it is brought about by the longer string “中興大學”. In our previous research, we showed that adding CFSs to a traditional lexicon, such as ASCED, can reduce the normalized perplexity from 251.7 to 63.5 [Lin and Yu 2001]. We also employed CFSs combined with ASCED as a dictionary to solve some Chinese NLP problems using the word-based uni-gram language model. We achieved promising results in both Chinese CTP and PTC conversion. It is well known that using a word-based bi-gram LM with a traditional lexicon can also improve accuracy in these two cases, especially in Chinese PTC conversion. The organization of this paper is as follows. Section 2 gives some properties and distributions of CFSs, and we also make a comparison between CFS and an n-gram LM. Section 3 shows that by using a CFS-based uni-gram LM, we can achieve higher accuracy  The Properties and Further Applications of Chinese Frequent Strings 115 than we can by using a traditional lexicon with a word-based bi-gram LM. We demonstrate this by using two challenging examples of Chinese NLP. In section 4, we assign syntactic categories to CFSs. Finally, section 5 presents our conclusions. 2. The Properties of CFS We used a training corpus of 59 MB (about 29.5M Chinese characters) in our experiments. In this section, we will present the properties of CFSs. Compared with language models and ASCED, CFSs have some important and distinctive features. We extracted 439,666 CFSs from a training corpus. 2.1 Extracting CFSs from a Training Corpus The algorithm for extracting CFSs was proposed in our previous work[Lin and Yu 2001]. We extracted CFSs from a training corpus that contained 29.5M characters. The training corpus also included a portion of the Academia Sinica Balanced Corpus [Chen et al. 1996] and many Internet news texts. The length distribution of the CFSs is shown in the second column of Table 1. The total number of CFSs that we extracted was 439,666. Our dictionary, which we call CFSD, is comprised of these 439,666 CFSs. In contrast to the second column of Table 1, we show the length distribution of the words in ASCED in the forth column of Table 1. We found that three-character CFSs were most numerous in our CFS lexicon, while two-character words were most numerous in ASCED. Many meaningful strings and unknown words are collected in our CFSs. These CFSs usually contain more than two characters. Some examples are “小企 鵝” (a little penguin), “西醫師” (modern medicine), “佛教思想” (Buddhist thought), “樂透彩 券” (lottery), and so on. The above examples cannot be found in ASCED, yet they frequently appear in our training corpus. 2.2 Comparing CFSs with Word-Based N-Gram LMs Since CFSs are strings frequently used by people, a CFS like “大學教授” (professors of a university) may contain more characters than a word defined in ASCED does. That is, a CFS may contain two or more words. If a CFS contains two words, we say that this CFS is a 2-word CFS. If a CFS contains three words, we say that this CFS is a 3-word CFS and so on. Figure 1 shows the distributions of CFSs according to word-based n-grams. The words are defined in ASCED. We also found 31,275 CFSs(7.11% of the CFSs in CFSD) that are words in ASCED. From Figure 1, it can be shown that a CFS may contain more than 3 words. Many researchers in Chinese NLP have used word-based bi-gram LMs [Yang 1998] as a basic LM to  116  Yih-Jeng Lin, and Ming-Shing Yu  solve problems. A very large corpus is required to train a word-based 3-gram LM, while our CFS-based uni-gram model does not need such a large corpus. We also found that a CFS contains 2.8 words on average in CFSD. This shows that a CFS contains more information than a word-based bi-gram LM. In our experiment, we also found that the average number of characters of a word-based bi-gram was 2.75, and that the average number of characters of a CFS was 4.07. This also shows that a CFS contains more information than a word-based bi-gram LM.  Table 1. The length distributions of CFSs in CFSD and words in ASCED.  Number of characters Number of CFSs of Percentage Number of words of  in a CFS or a word that length in our  that length in ASCED  CFS dictionary  
2. 系統架構  本論文所提出的車內噪音消除系統，如圖一所示。在系統前端，麥克風所錄到的雜訊語音，經由小波聽覺濾 波組(Perceptual Wavelet Filterbank)分成數個子頻帶訊號，各個子頻帶則由訊號子空間語音強化來進行噪音消 除的處理，而訊號子空間的拆解則是由子空間追蹤法來完成。由子空間追蹤法所估算出來的特徵值 (Eigenvalue)，則用以計算各個子頻帶訊號的增益值。語音強化的處理為將子頻帶訊號經過特徵 向 量 (Eigenvector)投影轉換後，由增益值來調整其訊號大小，再經過反轉換來得到強化後的語音訊號。以下各小 節則對小波聽覺分頻處理、訊號子空間語音強化以及子空間追蹤法做一描述。  Noisy Speech  Perceptual Filterbank (Analysis)  filter output  NLMS  *  eigenvectors  +  + -  Eigenvector Projection  filter output  NLMS  *  eigenvectors  +  + -  Eigenvector Projection  Gain Adaptation G1 Gain Adaptation G2  Inverse Projection Inverse Projection  Perceptual Filterbank (Synthesis)  Enhanced Speech  ... ... ...  filter output  NLMS  *  eigenvectors  +  + -  Eigenvector Projection  Gain Adaptation Gi  圖一：車內噪音消除系統架構。  Inverse Projection  2.1. 小波聽覺分頻處理 具聽覺感知的小波轉換(Perceptual Wavelet Packet Transform, PWPT)是改良自傳統小波轉換，使語音信號經 PWPT 分解後的各個子頻帶信號的頻寬接近人耳的聽覺響應 [3]，描述人耳聽覺響應的參數主要有巴克頻譜 （Bark）以及關鍵頻寬（Critical Bandwidth），表一為人耳聽覺關鍵頻寬的分佈情形。圖二(a)及圖二(b)分別 是在 4KHz 內，人耳的聽覺的巴克頻譜及關鍵頻寬曲線圖 [4, 5]。因此，本論文所設計的聽覺分頻處理即朝  此二曲線設計，圖二(a)及圖二(b)內亦標示了利用小轉換逼近巴克頻譜及關鍵頻寬的曲線圖。 由小轉換逼近巴克頻譜及關鍵頻寬是藉由調整小波轉換的樹狀結構來達成。依據表一的關鍵頻寬分佈情 形，適當對訊號做高低頻的分解，使得子頻帶訊號的頻率分佈跟關鍵頻寬近似。圖三為所使用的具聽覺感知 的小波轉換分解架構圖，其中輸入訊號經五個階段，共 16 次的高低頻分解。  Critical Band Number 
With the extracted training data from the web, the Greedy EM algorithm [5, 7] is applied in this paper to automatically determine an appropriate number of concepts contained in the given single term through clustering the training documents. This is important while doing relevant concept extraction; otherwise, the number of concepts has to be assumed previously, it is difficult and impractical in real world. After clustering the training documents extracted from the Web into a certain number of mixtures, for each mixture, the representation of this mixture is straightforwardly defined as the term with the highest weighted log likelihood ratio in this mixture. With some initial experiments, the proposed approach has been shown its potential in finding relevant concepts for terms of concern. The remainder of the paper is organized as follows. Section 2 briefly describes the background assumption, i.e. Naïve Bayes, and the modeling based on Naive Bayes. Section 3 describes the overall proposed approach in this paper, including the main idea of the greedy EM algorithm and its application to decide the number of concept domains contained in the training data from the web; in addition, generates keywords via comparing the weighted log likelihood ratio. Section 4 shows the experiments and their result. The summary and our future work are described in Section 5.  2. NAÏVE BAYES ASSUMPTION AND DOCUMENT CLASSIFICATION  Before introducing our proposed approach, here introduce a well known way of text representation, i.e., Naive Bayes assumption. Naive Bayes assumption is a particular probabilistic generative model for text. First,  introduce some notation about text representation. A document, di , is considered to be an ordered list of words, {wdi,1 , wdi,2 ,㕻 , w } di,|di| , where wdi, j means the jth words and | di | means the number of words in  di . Second, every document is assumed generated by a mixture of components{Ck } (relevant concept  clusters), for k=1 to K. Thus, we can characterize the likelihood of document di with a sum of total probability  over all mixture components:  K  ∑ p(di |θ ) = p(Ck |θ ) p(di | Ck ,θ )  (1)  k =1  Furthermore, for each topic class Ck of concern, we can express the probability of a document as:  p(di | Ck ,θ ) = p(< wdi,1 , wdi,2 ,㕻 , wdi,|di| >| Ck ,θ )  |di |  (2)  ∏ = p(wdi, j | Ck ,θ , wdi,z , z < j)  j =1  p(wdi, j | Ck ,θ , wdi,z , z < j) = p(wdi, j | Ck ,θ )  (3)  Based on standard Naive Bayes assumption, the words of a document are generated independently of context, that is, independently of the other words in the same document given the class model. We further assume that the probability of a word is independent of its position within the document. Combine (1) and (2),  |di |  ∏ p(di | Ck ,θ ) = p(wdi,j | Ck ,θ )  (4)  j =1  Thus, the parameters of an individual class are the collection of word probabilities, θwt |Ck = p(wt | Ck ,θ ) . The other parameters are the weight of mixture class, p(Ck | θ ) , that is, the prior probabilities of class, Ck . The set of parameters is θ = {θwt |Ck ,θCk } . As will be described in next section, the proposed document clustering is designed fully based on the parameters.  3. RELEVANT CONCEPTS EXTRACTION In this section, we describe the overall framework of the proposed approach. Suppose given a single term, T , and its relevant concepts are our interest. The first step of the approach is to send T into search engines to retrieve the relevant documents as the corpus. Note that the retrieved documents are the so-called snippets defined in [2]. The detailed process of the approach is described below.  3.1 The proposed Approach Suppose given a single term, T; then the process of relevant-concept extractions is designed as: Step 1. Send T into search engines to retrieve N snippets as the Web-based corpus, DT. Step 2. Apply the Greedy EM algorithm to cluster DT into K mixtures (clusters), {Ck }kK=1 , where K is dynamically determined. Step 3. For each Ck, k=1 to K, choose the term (s) with the highest weighted log likelihood ratio as the label (s) of Ck.  3.2 The Greedy EM Algorithm  Because we have no idea about the exact number of concepts strongly associated with each given term,  thus for each term it's straightforward to apply the Greedy EM algorithm to clustering the relevant documents  into an auto-determined number of clusters. The algorithm is a top-down clustering algorithm which is based on  the assumptions of the theoretical evidence developed in [5, 7]. Its basic idea is to suppose that all the relevant  documents belong to one component (concept cluster) at the initial stage, then successively adding one more  component (concept cluster) and redistributing the relevant documents step by step until the maximal likelihood  is approached.  Figure 1 shows the proposed approach and it is summarized in the following.  a)  Set K=1 and initialize  θ Ck  = 1 and  θ wt |Ck  straightforwardly by  wt ’s frequency, for all  w t  shown in  DT.  b) Perform EM steps until convergence, then θ i = {θwt|Ck ,θCk }kK=1  c) Calculate the likelihood, L(θ i ) .  d) Allocate one more mixture given initial modeling, i.e. θK +1 = {θwt|CK+1 ,θCK+1 } , described in section  3.2.2.  e) Keep θ i fixed, and use partial EM techniques, described in section 3.2.3, to update θK +1 .  f)  Set  θ i+1  =  {θwt |Ck  ,θ Ck  }kK=+11  .  Calculate  the  likelihood,  L(θ i+1) .  g) Stop if L(θ i+1) < L(θ i ) ; otherwise, return to c) and set K=K+1.  3.2.1 Likelihood Function  As described previously, all relevant documents belong to one mixture initially; then check the likelihood  to see if it is proper to add a new mixture. Thus, given K mixture components, the likelihood of K+1 is defined  as:  LK +1(DT ) = (1− α )LK (DT ) + αφ (DT ,θK +1)  (5)  with α in (0,1), where θK +1 = {θwt|CK+1 ,α} is the modeling of newly added mixture CK +1 and φ (DT ,θK +1) is the likelihood in CK +1 . If LK +1(DT ) < LK (DT ) , then stop the allocation of new mixture; otherwise, reallocate a new one. 3.2.2 Initialize Allocated Mixture In [7], a vector space model, initializing the newly added mixture is to calculate the first derivation with respect to α and to assume that the covariance matrix is a constant matrix. However, in our proposed probability framework, it is much more complicated because of a large amount of word probabilities, {θwt|Ck } ∀wt . Thus, we take the approximation of α in [6] as α = 0.5 for K=1 and α = 2 /(K +1) for ∑ K ≥ 2 . The initialization of {θwt|CK+1 } ∀wt is randomized to satisfy {wt }θwt|Ck = 1.  3.2.3 Update with Partial EM Algorithm  In order to simplify the updating problem, we take advantage of partial EM algorithm for locally search  the maxima of LK +1(DT ) . A notable property is that the original modeling for k=1 to K are fixed, only θK +1  is updated.  θwt |CK+1  = { p(wt  |  CK  +1  )}|V | t =1    |DT |  |V |  ∑∑∑ =    |  
According to previous studies [5, 28, 11, 20, 9], besides homonyms, correct syllable-word segmentation is another crucial problem of STW conversion. Incorrect syllable-word segmentation directly influences the conversion rate of STW. For example, consider the syllable sequence “yi1 du4 ji4 yu2 zhong1 guo2 de5 niang4 jiu3 ji4 shu4” of the sentence “ (once) (covet) (China) (of) (making-wine) (technique).” According to the CKIP lexicon [6], the two possible syllable-word segmentations are: (F) “yi1/du4ji4/yu2/zhong1guo2/de5/niang4jiu3/ji4shu4”; and (B) “yi1/du4/ji4yu2/zhong1guo2/de5/niang4jiu3/ji4shu4.” (We use the forward (F) and the backward (B) longest syllable-word first strategies [3], and “/” to indicate a syllable-word boundary). Among the above syllable-word segmentations, there is an ambiguous syllable-word section: /du4ji4/yu2/ (/{ }/{ , , , , , , , , , , , , , , , , , , , , , , }/); and /du4/ji4yu2/ (/{ , , , , , , }/{ , }/), respectively. In this case, if the system has the contextual information that the pairs “ (technique)- (covet)” and “ (once)- (covet)” are, respectively, meaningful noun-verb (NV) and adverb-verb (DV) word-pairs, then the ambiguous syllable-word section can be effectively resolved and the word-pairs “ (technique)- (covet)” and “ (once)- (covet)” of this syllable sequence can be correctly identified. For the above case, if we look at the Sinica corpus [6], the bigram frequencies of “ (covet)- (China)” and “ (at)- (China)” are 0 and 24, respectively. Therefore, by using a bigram model trained with the Sinica corpus, the forward syllable-word segmentation would conclude that the following word segmentation / / /, will be incorrect. In fact, if we use Microsoft Input Method Editor 2003 for Traditional Chinese (a trigram like STW product), the syllables of the above example will be converted to “ (once) (continue) (to) (China) (of) (making-wine) (technique).” It is widely recognized that unseen event (“ - ”) and over-weighting (“ - ”) are two major problems of SLM systems [10, 11]. Practical SLM is either a bigram or a trigram model. As the above case shows, the meaningful word-pairs (or contextual information) “ (technique)- (covet)” and “ (once)- (covet)” can be used to overcome both the unseen event and over-weighting problems of SLM-based STW systems. In [29], we showed that the knowledge of noun-verb event frame (NVEF) sense-pairs and their corresponding NVEF word-pairs (NVEF knowledge) are useful for effectively resolving word sense ambiguity with an accuracy of 93.7%. In [28], we showed that a NVEF word-pair identifier with pre-collected NVEF knowledge can be used to obtain a tonal (syllables with four tones) STW accuracy of more than 99% for the NVEF related portion in Chinese. The objective of this study is to illustrate the effectivness of meaningful noun-verb (NV), noun-noun (NN), verb-verb (VV), adjective-noun (AN) and adverb-verb (DV) word-pairs for solving Chinese STW conversion problems. We conduct STW experiments to show that the tonal and toneless STW accuracies of conventional SLM models and the commercial input products can be improved by using a meaningful word-pair identifier without a tuning process. In this paper, we use tonal to indicate the syllables input with four tones, such as “niang4( ) jiu3( ) ji4( ) shu4( ),” and toneless to indicate the syllables input without four tones, such as “niang( ) jiu( ) ji( ) shu( ).” The remainder of this paper is arranged as follows. In Section 2, we propose the method for auto-generating the meaningful word-pairs in Chinese based on [30, 32], and a meaningful word-pair identifier to resolve homonym/segmentation ambiguities of STW conversion in Chinese. The meaningful word-pair identifier is based on pre-collected datasets of meaningful word-pairs. In Section 3, we present our STW experiment results and analysis. Finally, in Section 4, we give our conclusions and suggest some future research directions. 2. Development of the Meaningful Word-Pair Identifier To develop the meaningful word-pair (MWP) identifier, we selected Hownet [7] as our system’s dictionary because it provides knowledge of Chinese words, word senses and part-of-speeches (POS). The Hownet dictionary used in this study contains 58,541 Chinese words, among which there are 33,264 nouns, 16,723 verbs, 8,872 adjectives and 882 adverbs. In this system’s dictionary, the syllable-word for each word is obtained by using the inverse phoneme-to-character system presented in [15], while the word frequencies are computed according to a fixed-size United Daily News (UDN) 2001 corpus. The latter is a collection of 4,539,624 Chinese sentences extracted from  articles on the United Daily News Website [25] from January 17, 2001 to December 30, 2001. Table 1 shows the statistics of the number of articles per article class in this UDN 2001 corpus.  article class # of articles article class # of articles  Table 1. The number of articles per article class in the training corpus.  China 90  Local 26,843  Society 136  Stock 19,699  Politics 133  Science 5,870  Consumption 12498  Financial 23,563  World 7,404  Sport 12,404  Entertainment 18,674  Health 5,653  Travel 6,183 Arts 9,989  2.1 Generating the Meaningful Word-Pair  In [32], we propose an AUTO-NVEF system to auto-generate NVEF knowledge from in Chinese. It extracts NVEF  knowledge from Chinese sentences by four major processes: (1) Segmentation checking; (2) Initial Part-of-Speech  (IPOS) sequence generation; (3) NV knowledge generation; and (4) NVEF knowledge auto-confirmation. The de-  tails of the four processes can be found in [32]. Take the Chinese sentence “  (concert)/ (locale)/  (enter)/ (many)/ (audience members)” as an example. For this sentence, AUTO-NVEF will generate two  collections of NVEF knowledge: (locale)- (enter) and (audience members)- (enter). In [32], we  reported that AUTO-NVEF achieved 98.52% accuracy for news and 96.41% for specific text types, which included  research reports, classical literature and modern literature. In addition, it automatically discovered over 400,000  NVEF word-pairs in the UDN 2001 corpus.  Using AUTO-NVEF as the base, we extended the system into a meaningful word-pair (MWP) generation  called AUTO-MWP. The steps of AUTO-MWP are:  Step 1. Use AUTO-NVEF to generate NVEF word-pairs for the given Chinese sentence. AUTO-NVEF adopts a  forward=backward maximum matching technique to perform word segmentation and a bigram-like model  to perform POS tagging [32]. If no NVEF word-pairs are generated, go to Step 3.  Step 2. According to the generated NVEF word-pairs and the word-segmented sentence with POS tagging from Step  1, the auto-generation methods of meaningful NN, VV, AN and DV word-pairs are:  (1) Generation of NN word-pair. When the number of generated NVEF word-pairs is greater than 1, this  sub-process will be triggered. If the nouns of two generated NVEF word-pairs share the same verb, the  two nouns will be designated as a meaningful NN word-pair. Take the generated NVEF word-pairs of  (locale)- (enter) and (audience members)- (enter) for the sentence “  (concert)  (locale) (enter) (many) (audience members)” as examples. The noun (locale) and  the noun (audience members) are designated as a NN word-pair because the two nouns share the  same verb (enter) in this sentence.  (2) Generation of VV word-pair. When the number of generated NVEF word-pairs is greater than 1, this  sub-process will be triggered. If the verbs of two generated NVEF word-pairs share the same noun, the  two verbs will be designated as a meaningful VV word-pair. Take the generated NVEF word-pairs  (the end of year)- (prearrange) and (the end of year)- (complete) for the sentence “  (whole) (construction) (prearrange) (the end of year) (complete)” as examples. The  verb (prearrange) and the verb (complete) are designated as a VV word-pair because the two  verbs share the same noun (the end of year).  (3) Generation of AN word-pair. For each noun of a generated NVEF word-pair, if the word immediately to  its left is an adjective, the noun and the adjective are designated as one AN word-pair. Take the generated  NVEF word-pair (audience members)- (enter) for the word-segmented and POS-tagged sen-  tence “  (N) (N) (V) (ADJ) (N)” as an example. Since the word immediately to  the left of (audience members) is an adjective (many), the adjective (many) and the  noun (audience members) are designated as a AN word-pair.  (4) Generation of DV word-pair. For each verb of a generated NVEF word-pair, if the word immediately to  its left is an adverb, the verb and the adverb are designated as one DV word-pair. Take the generated  NVEF word-pair (price)- (maintain) for the word-segmented and POS-tagged sentence “  (N) (ADV) (V) (ADJ)” as an example. Since the word immediately to the left of  (maintain) is an adverb (ordinarily), the adverb (ordinarily) and the verb (maintain) are  designated as a DV word-pair.  Step 3. Stop.  Table 2 shows the number of generated NV, NN, VV, AN and DV word-pairs obtained by applying AUTO-MWP to the UDN 2001 corpus. The frequencies of all the generated meaningful word-pairs were computed by the UDN 2001 corpus. Note that the frequency of a meaningful word-pair is the number of sentences that contain the word-pair with the same word-pair order in the UDN 2001 corpus. Table 3 shows fifteen randomly selected NV, NN, VV, AN and DV word-pairs and their corresponding frequencies in the generated MWP datasets for the UDN 2001 corpus.  Table 2. The number of generated NV, NN, VV, AN and DV word-pairs obtained by applying AUTO-MWP to the UDN 2001 corpus.  NV  NN  VV  AN  DV  Total  430,698  533,780  220,022  138,055  111,879  1,434,434  Table 3. Fifteen randomly selected examples of meaningful NV, NN, VV, AN and DV word-pairs and their corresponding frequencies from the generated MWP datasets for the UDN 2001 corpus.  NV  NN  VV  AN  DV  - /118 - /35 - /96  - /83 - /103 - /107  - /541 - /1483 - /124  - /206 - /103 - /129  - /188 - /390 - /144  2.2 Meaningful Word-Pair Identifier We developed a NVEF word-pair identifier [28] for Chinese syllable-to-word (STW) and achieved a tonal STW accuracy of more than 99% on the NVEF related portion. This NVEF word-pair identifier is based on the techniques of longest syllabic NVEF-word-pair first (LS-NVWF), exclusion-word-list (EWL) checking and pre-collected NVEF knowledge. By modifying the algorithm of this identifier in [28], we obtain our meaningful word-pair (MWP) identifier, (Figure 1). In Figure 1, the MWP data is a mixed collection of all auto-generated meaningful NV, NN, VV, AN and DV word-pairs. As shown in the figure, if the MWP identifier only uses one of the meaningful NV, NN, VV, AN or DV word-pair datasets, it will naturally become an MNV, MNN, MVV, MAN or MDV word-pair identifier. LS-WPF & EWL checking  input syllables  Meaningful word-pair (MWP) identifier  MWP-sentence  MWP data  Hownet  Figure 1. A system overview of the meaningful word-pair (MWP) identifier.  The algorithm of the MWP identifier is as follows: Step 1. Input tonal (with four tones) or toneless (without four tones) syllables. Step 2. Generate all possible word-pairs found in the input syllables. Exclude certain NV word-pairs based on EWL checking [28]. Appendix A lists all of the exclusion words used in this study. Note that our meaningful word-pairs include monosyllabic nouns/adjectives/adverbs and monosyllabic verbs, except “ (be)” and “ (has/have)” that are dropped in this Step. Step 3. Word-pairs that match a meaningful word-pair in the generated MWP data are used as the initial MWP set for the input syllables. From the initial MWP set, select a key word-pair and its co-occurring word-pairs to be the final MWP set. Conflicts are resolved using the longest syllabic word-pair first (LS-WPF) strategy. If there are two or more word-pairs with the same condition, the system triggers the following processes. (1) The word-pair with the greatest frequency (the number of sentences that contain the word-pair with the same word-pair order in the UDN 2001 corpus) is selected as the key word-pair. If there are two or more word-pairs with the same frequency, one of them is randomly selected as the key word-pair. (2) The word-pairs that co-occur with the key word-pair in the UDN 2001 corpus are selected.  (3) The key and co-occurred word-pairs are then combined as the final MWP set for Step 4. Step 4. Arrange all word-pairs of the final MWP set into a MWP-sentence as shown in Table 3. If no word-pairs can be identified from the input syllables, a null MWP-sentence is produced.  Table 3. An illustration of an MWP-sentence for the Chinese syllables “yi1 ge5 wen2 ming2 de5 shuai1 wei2 guo4  cheng2( [a] [civilization] [of] [decay] [process]).” (The English words in parentheses  are included for explanatory purposes only.)  Process  Results  Pair freq.  Step.1  yi1 ge5 wen2 ming2 de5 shuai1 wei2 guo4 cheng2  (  )  Step.2  The meaningful word-pairs found:  (wen2 ming2)- (guo4 cheng2)/NN pair  3  (wen2 ming2)- (shuai1 wei2)/NV pair  
 National Chiao Tung University yealings@ms38.hinet.net mliu@mail.nctu.edu.tw  In this paper, we propose a procedural schema as a model of cognitive processing of word senses, which can be viewed as a derivational resolution of polysemy. Previous researches, such as Frame -Based Lexicon by Fillmore [4] and Lexical Semantics by Cruse [2], are all concerned with word senses, but what is still missing is a holistic resolution of polysemy. Therefore, in this paper, we focus on the cognitive process from word form to word senses, based on corpus-based procedural resolution. In this way, we hope to provide an overall discussion and a computerizable way of solving multiplicity of semantic usages of a single word form. A case study of the Mandarin verb ZOU ( ) is presented and used as an illustration.  
 Both zai and zhengzai are progressive markers in Mandarin Chinese, and by the principle of economy, there should be some differences in these two progressive markers. With the Sinica Corpus on-line tools, a significant difference is found in the collocation of adverbial adjuncts with the use of zai and zhengzai. This paper discusses three types of adverbials to distinguish these two markers: modality adverbs, time adverbs, and manner adverbs. Zhengzai cannot co-occur with [+iterative] adverbs and adverbs without a specific time reference. It mainly indicates the progression of an on-going event at a given specific time point. On the other hand, zai not only indicates the on-going process but can also signal the progression of repeated event as habitual- progressive.  
 [4]  [5] 1999 [6]  69  [2]  PHS  45  [7-8]  information overload  [2]  [13]  120  158 letter  IBM  [9] InXight [10] Megaputer [11] informative  indicative  commentary  extraction  abstraction  coverage rhetoric  intrinsic  extrinsic  readability  coherence  cohesion  conjunction  pronoun  organization anaphor  SUMMUC [12] DUC [8]  NTCIR TSC  [13]  DUC 2001  100 words  30  10  DUC 2001 2002  100  [7] DUC 2003  10  100  NTCIR TSC 2002  Mainichi  20% 40%  [13]  [14-18]  10%  10%-50%  10%  89.77%  Buyukkokten  [24]  PDA  clustering  90  76%  70%  [19]  117 85  117 40  Global Bushy Path GBP  [20]  1000  100-500  GBP  58.25%  Banko [21] Kennedy [22]  Corston-Oliver  email  Monday Mon  MicroSoft Outlook  [23]  Yang [25]  69 45  real-time  30  meaningful unit, MU  FTTB L2 Switch  -  Ethernet- based FTTB L2 Switch  2003/08/14, 11:21:28  GESWr  SMC  -  2003/ 09/ 01, 07:57:33  2  6  -  [26] EoVDSL  67 6 35  25 MU  23 53 12  45 2 45  6 2 6 69 45 69 37  45  45  6  45  5  2  15 2 35  n  tfw  w  Tseng  n  ∑(0.5 + 0.5* tfw / max_ tf ) w∈Keywords max_tf maximally repeated string [27] 12 n  dynamic programming  edit  distance  [28]  AB  nm  Ai  A[i]  Bj  B[j]  d[i, j] = min( d[i-1, j] + w(A[i], 0), d[i-1, j-1] + w(A[i], B[j]), d[i, j-1] + w(0, B[j]) )  min(X, Y, Z)  X, Y, Z  d[0, 0] = 0  d[i, 0] = d[i-1, 0] + w(A[i], 0) , 1<= i <= n  d[0, j] = d[0, j-1] + w(0, B[j]) , 1<= j <= m  w(X, Y)  w(A[i], B[j]) :  A[i]  B[j]  w(A[i], 0) :  A[i]  w(0, B[j]) :  B[j]  A=adc  B=adecdecf  
Extensive work on question answering has been reported in the many literature (Buchholz et al., 2001; Harabagiu et al., 2001; John et al., 2002; Shen et al., 2003). In this study, we focus on learning the transforms that can be used to convert questions into effective queries in order to retrieve re levant passages. Hovy et al. (2000) utilized hypernyms and synonyms in WordNet to expand queries for increasing recall. However, blindly expanding a word to its synonyms sometimes causes undesirable effects. As for hypernyms, it is difficult to determine how many hypernyms a word should be expanded. In contrast to this approach, our method learns query transforms specific to a word or phrase based on real-life questions and answer passages. In a recent study most closely related to our method, Agichtein et al. (2004) described the Tritus system that learns transforms of wh-phrases such as “what is“ to “refers to” by using FAQ data automatically. Our method learns transforms for wh-phrases as well as keywords from the web. Tritus system uses heuristic rules and thresholds for term and document frequency to learn transforms, while we rely on a mathematical model method for statistical machine translation. Shen, Lin and Chen (2003) proposed a method that is similar to the Tritus system for the why question. Recently, Echihabi and Marcu (2003) presented a noisy channel approach to question answering. Their method also involves collecting answer passages from the web and aligning words across a question and relevant answer passages. However, they require full parsing of the sentences and complicated decision of making a “cut” in the parse tree to determine whether to align word, syntactic, or semantic categories. Our simple method is also based on alignment but it does not require full parsing and perform alignment at the surface levels of words and n-grams. In contrast to previous work on query expansion for question answering, we propose a method that learns query transforms for all phrases in a natural language question automatically on the Web. 3. Method for Learning Question to Query Transforms In this section, we present an unsupervised method for QA which automatically learns transforms from wh-phrases and keywords to answer n-grams by using the Web as corpus. 3.1 Problem Statement Given a set of natural language questions Qs and answer terms As, we obtain a collection of passages that contain the answer A to the question Q via some search engine SE. From the collection of answer passages APs, our goal is to discover a set of transforms T that can be applied to wh-phrases and keywords in Q in the hope that the transformed queries are more effective in retrieving passages containing A. 3.2 Procedure for Learning Transforms This subsection illustrates the procedure for learning transforms T from wh -phrases and unigrams in Q into bigrams in AP. The reason why we decide to use bigrams in AP is that bigram contains more information than unigram and is more effective in retrieving relevant passages . On the other hand, we break Qs into unigrams following the standard approach in IR.  (1) Automatically collect pairs of Q and AP from the Web for training. (Section 3.2.1) (2) Select frequent wh-phrases. (Section 3.2.2) (3) Apply the alignment technique to the collected material. (Section 3.2.3) Fig.1. Procedure for learning transforms  3.2.1 Collecting Training Material from the Web In the first step of the learning process (see Figure 1), we retrieve a set of (Q, A, AP) pairs from the Web for training purpose where Q stands for a natural language question, and AP is a passage containing keywords in Q and the answer term A. The data gathering process is described as follows: 1. For each (Q, A) pair in the given collection, we extract keywords K of Q, say, k1, k2, … , kn. 2. Submit (k1, k2, … , kn, A), as a query to SE. 3. Download the top M summaries that are returned by SE. 4. Retain only those summaries containing A. See Table 1 for details.  Table 1. An example of converting a question (Q) with its answer (A) to SE query and retrieving answer passages (AP)  (Q, A) What is the capital of Pakistan? Answer:( Islamabad) (k1, k2, … , kn, A) capital, Pakistan, Islamabad  AP Bungalow For Rent in Islamabad, Capital Pakistan. Beautiful Big House For … Islamabad is the capital of Pakistan. Current time, … … the airport which serves Pakistan's capital Islamabad, …  3.2.2 Selecting Frequent Wh-phrases  In the second step, we produce a set of high frequency phrases that characterize different question categories. We follow the method proposed by Agichtein et al. (2004). The method simply involves computing the frequency of all n-grams in Qs and filters out those with small counts. We will treat the wh-phrases (QPs) as a token in the subsequent steps. However, we differ from their approach in that we are not limited to n-grams of function words. For instance, we derived “in what year”, “who wrote”, etc. More examples of wh-phrases are listed in Table 2. Table 2. An example of wh-phrases that are used  Wh-words What Who Which  Wh-phrases QPs “what is the”, “in what year”, “what was”, … “who was the”, “who wrote”, … “which country”, “with which”, …  … …  3.2.3 Learning Question to Query Transforms In the third step, we use word alignment techniques originally developed for statistical machine translation to find out relation between wh-phrases or keywords in Q and n-grams in AP. We use the Competitive Linking Algorithm proposed by Melamed (1997) to align (Q, AP) pair. We proceed as follows:  1. Perform Part of Speech (POS) tagging on both Q and AP in the collection. (See Table 3 and 4) 2. Replace all instances of A with the tag <ANS> in AP. For example, the answer “Islamabad” in AP for the question “What is the capital of Pakistan?” is replaced with <ANS>. (See Table 4.) The purpose of <ANS> is to avoid data sparseness while counting bigrams in the following step. 3. Segment Q into unigrams or QPs and eliminate unigrams with low counts. We denote the remaining unigrams as q1, q2, ..., qn. (See Table 5) 4. Segment AP into bigrams and eliminate bigrams with small term frequency (tf) or very large document frequency(df). We denote the remaining bigrams a1, a2, ..., a m. (See Table 6) 5. For all i, j, calculate log likelihood ratio (LLR) of qi and aj. (See Table 7) 6. Eliminate candidates with a LLR value lower than 7.88. (See Table 8) 7. Sort list of (qi, aj) by decreasing LLR value. (See Table 8) 8. Go down the list and select a pair if it does not conflict with previous selection. 9. Stop when running out of pairs in the list. 10. Produce the list of aligned pairs for all Qs and APs. 11. Select top N bigrams, a1, a2, ..., ar, for every wh-phrase or unigram qi in alignment pairs. (See Table 9) Table 3. Part of Speech of Q  Q word  Lemma  Position  POS  What is the  what be the  
 National Tsing Hua University  Applictaion  National Tsing Hua University  101, Kuangfu Road, Hsinchu, Taiwan National Tsing Hua University 101, Kuangfu Road, Hsinchu, Taiwan  d914339@oz.nthu.edu.tw 101, Kuangfu Road, Hsinchu, Taiwan  jschang@cs.nthu.edu.tw  u881222@alumni.nthu.edu.tw  Abstract. In this paper, we propose a new method for bilingual collocation extraction from a parallel corpus to provide phrasal translation memory. The method integrates statistical and linguistic information for effective extraction of collocations. The linguistic information includes parts of speech, chunks, and clauses. With an implementation of the method, we obtain first an extended list of collocations from monolingual corpora such as British National Corpus (BNC). Subsequently, we exploit the list to identify English collocations in Sinorama Parallel Corpus (SPC). Finally, we use word alignment techniques to retrieve the translation equivalent of English collocations from the bilingual corpus, so as to provide phrasal translation memory for machine translation system. Based on the strength of chunk and clause analyses, we are able to extract a large number of collocations and translations with much less time and effort than those required by N-gram analysis or full parsing. Furthermore, we also consider longer collocation pattern such as a preposition involved in VN collocation. In the future, we plan to extend the method to other types of collocation. Keyword. Bilingual Collocation Extraction, Collocational Translation Memory, Collocational Concordancer 
The newly-developed prosody module of our text-to-speech (TTS) system is described in the paper. We present two main works on it’s establishment and improvement. On the basis of potential factors influencing prosody parameters, inclusive of duration, pitch and intensity, the prosody model is built as groundwork of this module which is superior to the former rule-based one in generation of natural prosody. In addition, due to the current model’s flaw in prediction of the pitch contour, we further employ an technique named “Soft Template Mark-up Language“(STEM-ML) to improve the smoothness of intonation which has the crucial influence on the naturalness of synthetic speech. Results of the evaluation indicate that the new prosody model is precise enough to predict reliable prosody parameters’ values and with the STEM-ML technique, the prosody module can further yield 14.75% reduction in the root mean square (RMS) error of the predicted pitch contour. 1. Introduction In consideration of severe limitation in the resource afforded by some applications in need of speech response, we choose to develop one storage-saving TTS system which has functioned successfully in our spoken dialogue system. Accordingly, the acoustic inventory used in our system is simply composed of about four hundred base syllable units whose duration and pitch contour will be modified with the algorithm called Pitch-Synchronous Overlap-Add (PSOLA) [1][12] in the synthesizing phrase. In order to produce natural-sounding synthetic speech, the generation of prosody plays a key role and is a difficult issue yet. Outperforming rule-based method [13][14] which was employed in our system previously, the newly-built statistical model based on sum-of-products approach with key factors affecting prosody [7][8][9][10][11] can predict more accurate values of prosody parameters. And in general, the intonation which is characterized by the pitch contour seems more crucial to the naturalness and intelligibility of synthesized speech in comparison with other prosody elements such as duration, intensity etc [6]. Nevertheless, the pitch contour generated by our current prosody model is still short of smoothness. As a result, we further concentrate our work on this problem. Based on the F0 (fundamental frequency) mean value predicted by the current prosody model, an technique named STEM-ML [2][3][4][5] is adopted to overcome this shortcoming. In the evaluation phrase, we prove that this technique can help to reduce the difference between the predicted and observed pitch contours, which means that a more natural intonation is achieved. The paper is organized as follows. In the chapter 2, we present the prosody modeling in our system, The chapter 3 reports STEM-ML technique and the result of implementation. The conclusion is described in the chapter 4.  2. Prosody modeling In general, prosody mainly consists of duration, pitch, intensity of the spoken unit which is one syllable in terms of Mandarin. Besides, the break between units is one of it’s important elements as well. Therefore, one utterance’s prosody can be regarded as the elaborate composition of these four perceivable characteristics. And the variation in prosody stem from a lot of factors in different dimensions which can be observed in the real speech corpus such as the syllable’s position in the sentence, lexical tone even the speaker’s emotion and so on. Furthermore the complex interactions between factors further lead to another difficulty in designing the prosody model. As a result, in addition to inferring the reliable factors influencing the prosody, to model the interactions between factors intelligently is also a challenge in this work. .  2.1 Modeling  2.1.1 Base model and sub-models The potential factors affect one characteristic simultaneously and have additive, multiplicative or repulsive interactions . Thus, it’s troublesome to derive their eventual combined effect on the characteristic. However, for the purpose of assuring that the basically reasonable value for the characteristic can be preserved, one major factor in possession of dominant influence are elected to build the base model while the remaining minor factors take charge to constitute sub-models. In other words, under this framework, the base model provides fundamental value for the characteristic and sub-models act on this base value (BV for short) through the mechanism modeling their interaction to obtain the ultimate characteristic value (CV for short).  2.1.2 Ratio of characteristic value to base value (RCB)  In order that this concept of modeling can be put into practice concretely, the training sample for  sub-models, namely the CV of each syllable has to be normalized by it’s corresponding BV beforehand.  Thus, pre-processed CV is computed as follows.  RCB = CV  (1)  BV  2.1.3 Mechanism In brief, the ultimate objective of the mechanism devised here is to make combined effect of minor factors quantized to one RCB value used as the multiplier of the BV. The interactions of minor factors are modeled by the approach of sum-of-products and the predicted CV is computed as follows.  ^ CV = RCB comb × B i  ∑ ∑ SMN RCB comb =  i C ij S i m ij S j n ij  (2)  i  j  where Bi is the parameter of the base model for the characteristic i and  SMN is the numbers of sub-models for the characteristic i and Si is the parameter of the sub-model i and Cij is a coefficient associating the sub-model i and sub-model j and mij and nij represent the stress of sub-model i and sub-model j respectively.  2.1.4 Factors We infer seven potential factors crucial to the characteristics in prosody. Those are listed and described briefly as below. z Base syllable (BS) 408 identities z Lexical tone (LT) 4 lexical tones and one neutral tone z Left and right context tones (LRCT) 175 levels: 25(bi-tone) + 125(tri-tone) z The syllable’s position in the word and the syllable number of one word (SInW) 15 levels: 1+2+3+4+5 (longest word length) z The word’s position in the phrase (WInP)  4 levels: WInP =  WordIndex  ×4  WordNumber  OfPhrase  z Right context break (RCBk) 4 levels: inter-syllable pause, inter-word pause, comma, period z Right context initial (RCIt) 32 identities Accordingly., four kinds of base models and seven kinds of sub-models will be established in light of these factors.  2.2 Estimation  2.2.1 Corpus Recorded by a single female speaker, the speech corpus contains 3657 sentences (70000 syllables;about 7 hours) with moderate intonation and constant speaking rate. In terms of linguistics ,the properly-designed one has enough coverage to tackle diverse variability of prosody. Among these sentences, around 3200 ones are used as training data and the rest of them are reversed for the purpose of evaluation. The syllable boundaries in the waveform are further calibrated manually after aligned by the automatic speech recognizer.  2.2.2 Objective function  The distortion rate (DR) is defined to measure the precision of predicted value.  DR = O − P  (3)  O  where  O is the occurrence’s CV and  P is the predicted CV.  Accordingly, the objective function is defined as average DRs of all occurrences in the training data.  O = 1 ∑ DR i  (4)  Ni  where N is the number of training samples.  2.2.3 Approach  Model  Both base models and sub-models have only one parameter. The parameters of base models and  sub-models are calculated as the average of observed occurrences’s CVs and RCBs which correspond  to them in the training corpus respectively.  ∑ µ =  
Abstract. Researches in speech synthesis and speech analysis are underpinned by the databases they used. The performance of an emotion classifier relies heavily on the qualities of the training and testing data. A good database can make researches in these fields achieving better results. Hearing-impaired people are poor in presenting their emotions in speech. We want develop a computer-assisted speech training system that can help to teach them to present their emotions similar to normal people. In this paper, we present a way to build a Mandarin emotional database, including the process of collecting data, arranging data, clips naming rules, and a listening test. Then we construct a computer-assisted speech training system to help in teaching the hearing-impaired people presenting their emotion in their speech correctly by analyzing the emotion in their speech and those in the database using KNN and M-KNN techniques. Keywords: Emotional speech database, Emotion evaluation, Emotion Radar chart, M-KNN 1. Introduction The performance of an emotion classifier relies heavily on the quality of emotional speech data and the similarity of it to real world samples. As mentioned in [1], there are three different categories of emotional speech: acted speech, elicited speech, and spontaneous speech. In this section we will describe the ways to obtain these three kinds of speech data. In acted speech recording, actors are invited to record utterances, where each utterance needs to be spoken with multiple emotions. The method is adopted by most researches because it can get large amount of data in a short time and the data is undistorted. For general use, we should invite speakers with different age, gender, even with different social or educational background if possible. And if we hope the emotion in the data to be more obvious, we could invite professional actors. We can also collect the clips that contain utterances with specific emotion in a film. We must avoid the background noise including music, surrounding noise, and other people’s voice. This method takes quite a lot of time in viewing the content of films. In elicited speech recording, the Wizard-of-Oz (WOZ) is used. The WOZ means using a program that interacts with the speaker and drives him into a specific emotion situation and then records his voice. This method needs a good program that can induce the participator to say something in our expected emotion state. So how to design such a program may not be easy. In spontaneous speech recording, the real-world utterances that express emotions are recorded. Although data got from this method has the best naturalness, it is the most difficult because we need to follow the speaker. When he or she is in some emotion state, his voice is recorded immediately. This method will face many problems. For examples, we must hide our recording device in order to make the speaker without any pressure to present his real emotion. Furthermore, we also cannot assure the environment is quiet. Generally speaking, the method is generally infeasible. 2. Mandarin Emotional Speech Database In our research, five emotions are investigated: anger, happiness, sadness, boredom, and neutral. We invite 18 males and 16 females to simulate five emotions. A prompting text with 20 different sentences is designed. The length of each sentence is from one word to six words the sentences are meaningful so speakers could easily simulate them with emotions. During the recordings process, speakers are asked to try their best to simulate each  emotion. And speakers can simulate one sentence many times until they are satisfied what they simulated. Finally, we obtained 3,400 emotional speech sentences. After the recording procedure, a listening test is held to evaluate these recorded sentences. It is very important to use speech with unambiguous emotional content for further analysis. This can be guaranteed by a listening test [2], in which listeners evaluate the emotional content of a recorded sentence. Moreover, we can understand the performance of human in emotion recognition. We perform the listening test in a three-pass procedure. First, we delete the speech data that is very hard to identify its emotional content. After this process, 1,178 sentences are remained. Then, the remaining sentences are evaluated by three speakers. The sentences with the same agreement are remained. After the stage, 839 sentences are remained. Finally, we invite 10 people whom did not have their speech data in the 839 sentences to take part the final listening test. The results of the listen test are shown in Fig. 1. We can see the recognition results of the 10 evaluators in the figure and the confusion matrix in Table 1. The results reveal that people are good in recognizing anger (89.56%), sadness (82.76%), and neutral state (83.51%), but are less confident for happiness (73.22%), and boredom (75.16%)  Recognition Accuracy (%)  ˄˃˃ʸ ˌ˃ʸ  ˋˌˁˈˉʸ  ˋ˅ˁˊˉʸ  ˋ˃ʸ  ˊˆˁ˅˅ʸ  ˊ˃ʸ  ˉ˃ʸ  ˈ˃ʸ  ˇ˃ʸ  ˆ˃ʸ ˔́˺ʻ˄ˋ˅ʼ  ˛˴̃ʻ˄ˊˇʼ  ˦˴˷ʻ˄ˊ˃ʼ  Emotion Category  ˊˈˁ˄ˉʸ ˕̂̅ʻ˄ˈˌʼ  ˋˆˁˈ˄ʸ ˡ˸̈ʻ˄ˈˇʼ  Evaluator ˦˄ ˦˅ ˦ˆ ˦ˇ ˦ˈ ˦ˉ ˦ˊ ˦ˋ ˦ˌ ˦˄˃ ˔̉˺  Anger Happiness Sadness Boredom Neutral  Fig. 1. Recognition results of 10 evaluators.  Table 1: Confusion Matrix of Human Performance.  Anger 89.56% 6.67% 2.94% 1.26% 1.69%  Happiness 4.29% 73.22% 1.00% 0.44% 0.91%  Sadness 0.88% 3.28% 82.76% 8.62% 1.56%  Boredom 0.77% 2.36% 9.29% 75.16% 12.27%  Neutral 3.52% 13.56% 3.29% 13.65% 83.51%  None of above 0.99% 0.92% 0.71% 0.88% 0.06%  Table 1 shows the human performance confusion matrix. The rows and the columns represent simulated and evaluated categories, respectively. For example, first row says that 89.56% of utterances that were portrayed as angry were evaluated as angry, 4.29% as happy, 0.88% as sad, 0.77% as bored, 3.25% as neutral, and 0.99% if none of above. We can see that the most easily recognizable category is anger (89.56%) and the poorest recognizable category is happiness (73.22%). And we can find that human sometimes are confusing in differentiating anger from happiness, and boredom from neutral.  Table 2 shows the statistics of 10 evaluators for each emotion category. We can see that the variance for anger and sadness are less than for the other emotions. It means that human are better in understanding how to recognize anger and sadness than other emotions. Figure 2 shows the percentage of remained sentences with different lengths for each emotion. We can see that the shortest sentence (only single word) is least remained in most emotions, especially in neutral. It means that we should avoid too short sentences when we make the prompting text in the future because emotions are hard to be recognized by human if the sentence is too short.  Remained Percentage ˔́˺˸̅ ˄˃˃ˁ˃˃ʸ ˋ˃ˁ˃˃ʸ ˉ˃ˁ˃˃ʸ ˇ˃ˁ˃˃ʸ ˅˃ˁ˃˃ʸ ˃ˁ˃˃ʸ ˉˈˇ ˆ˅˄ Sentence Length  Remained Percentage ˛˴̃̃˼́˸̆̆  ˄˃˃ˁ˃˃ʸ  ˋ˃ˁ˃˃ʸ  ˉ˃ˁ˃˃ʸ  ˇ˃ˁ˃˃ʸ  ˅˃ˁ˃˃ʸ  ˃ˁ˃˃ʸ  ˉ  ˈ  ˇ  ˆ  ˅  ˄  Sentence Length  (a)  (b)  Remained Percentage ˦˴˷́˸̆̆  ˄˃˃ˁ˃˃ʸ  
Abstract. In this paper, a Mandarin speech based emotion classification method is presented. Five primary human emotions including anger, boredom, happiness, neutral and sadness are investigated. For speech emotion recognition, we select 16 LPC coefficients, 12 LPCC components, 16 LFPC components, 16 PLP coefficients, 20 MFCC components and jitter as the basic features to form the feature vector. Two text-dependent and speaker-independent corpora are employed. The recognizer presented in this paper is based on three recognition techniques: LDA, K-NN, and HMMs. Results show that the selected features are robust and effective in the emotion recognition at the valence degree in both corpora. For the LDA emotion recognition, the highest accuracy of 79.9% is obtained. For the K-NN emotion recognition, the highest accuracy of 84.2% is obtained. And for the HMMs emotion recognition, the highest accuracy of 88.7% is achieved.  
Although single-document summarization is a well-studied task, the nature of multidocument summarization is only beginning to be studied in detail. While close attention has been paid to what technologies are necessary when moving from single to multi-document summarization, the properties of humanwritten multi-document summaries have not been quantified. In this paper, we empirically characterize human-written summaries provided in a widely used summarization corpus by attempting to answer the questions: Can multi-document summaries that are written by humans be characterized as extractive or generative? Are multi-document summaries less extractive than singledocument summaries? Our results suggest that extraction-based techniques which have been successful for single-document summarization may not be sufficient when summarizing multiple documents. 
We introduce a new method of feature selection for text categorization. Our MMR-based feature selection method strives to reduce redundancy between features while maintaining information gain in selecting appropriate features for text categorization. Empirical results show that MMR-based feature selection is more effective than Koller & Sahami’s method, which is one of greedy feature selection methods, and conventional information gain which is commonly used in feature selection for text categorization. Moreover, MMRbased feature selection sometimes produces some improvements of conventional machine learning algorithms over SVM which is known to give the best classification accuracy. 
Conventional statistical machine translation (SMT) approaches might not be able to ﬁnd a good translation due to problems in its statistical models (due to data sparseness during the estimation of the model parameters) as well as search errors during the decoding process. This paper1 presents an example-based rescoring method that validates SMT translation candidates and judges whether the selected decoder output is good or not. Given such a validation ﬁlter, defective translations can be rejected. The experiments show a drastic improvement in the overall system performance compared to translation selection methods based on statistical scores only.  
This paper proposes to apply machine learning techniques to the task of combining outputs of multiple LVCSR models. The proposed technique has advantages over that by voting schemes such as ROVER, especially when the majority of participating models are not reliable. In this machine learning framework, as features of machine learning, information such as the model IDs which output the hypothesized word are useful for improving the word recognition rate. Experimental results show that the combination results achieve a relative word error reduction of up to 39 % against the best performing single model and that of up to 23 % against ROVER. We further empirically show that it performs better when LVCSR models to be combined are chosen so as to cover as many correctly recognized words as possible, rather than choosing models in descending order of their word correct rates. 
This paper presents experiments on how the performance of automatic keyword extraction can be improved, as measured by keywords previously assigned by professional indexers. The keyword extraction algorithm consists of three prediction models that are combined to decide what words or sequences of words in the documents are suitable as keywords. The models, in turn, are built using different deﬁnitions of what constitutes a term in a written document. 
In this paper, we contrast two language model adaptation approaches: MAP estimation and the perceptron algorithm. Used in isolation, we show that MAP estimation outperforms the latter approach, for reasons which argue for combining the two approaches. When combined, the resulting system provides a 0.7 percent absolute reduction in word error rate over MAP estimation alone. In addition, we demonstrate that, in a multi-pass recognition scenario, it is better to use the perceptron algorithm on early pass word lattices, since the improved error rate improves acoustic model adaptation. 
In this paper we present recent advances in acoustic and language modeling that improve recognition performance when children read out loud within digital books. First we extend previous work by incorporating crossutterance word history information and dynamic n-gram language modeling. By additionally incorporating Vocal Tract Length Normalization (VTLN), Speaker-Adaptive Training (SAT) and iterative unsupervised structural maximum a posteriori linear regression (SMAPLR) adaptation we demonstrate a 54% reduction in word error rate. Next, we show how data from children’s read-aloud sessions can be utilized to improve accuracy in a spontaneous story summarization task. An error reduction of 15% over previous published results is shown. Finally we describe a novel real-time implementation of our research system that incorporates time-adaptive acoustic and language modeling. 
We propose a method of automatically constructing an English-Chinese bilingual FrameNet where the English FrameNet lexical entries are linked to the appropriate Chinese word senses. This resource can be used in machine translation and cross-lingual IR systems. We coerce the English FrameNet into Chinese using a bilingual lexicon, frame context in FrameNet and taxonomy structure in HowNet. Our approach does not require any manual mapping between FrameNet and HowNet semantic roles. Evaluation results show that we achieve a promising 82% average Fmeasure for the most ambiguous lexical entries. 
We describe a system for pronoun interpretation that is self-trained from raw data, that is, using no annotated training data. The result outperforms a Hobbsian baseline algorithm and is only marginally inferior to an essentially identical, state-of-the-art supervised model trained from a substantial manually-annotated coreference corpus. 
We present the first known result for named entity recognition (NER) in realistic largevocabulary spoken Chinese. We establish this result by applying a maximum entropy model, currently the single best known approach for textual Chinese NER, to the recognition output of the BBN LVCSR system on Chinese Broadcast News utterances. Our results support the claim that transferring NER approaches from text to spoken language is a significantly more difficult task for Chinese than for English. We propose re-segmenting the ASR hypotheses as well as applying postclassification to improve the performance. Finally, we introduce a method of using n-best hypotheses that yields a small but nevertheless useful improvement NER accuracy. We use acoustic, phonetic, language model, NER and other scores as confidence measure. Experimental results show an average of 6.7% relative improvement in precision and 1.7% relative improvement in F-measure.  reasons. First, Chinese has a large number of homonyms and the vocabulary used in Chinese person names is an open set so more characters/words are unseen in the training data. Second, there is no standard definition of Chinese words. Word segmentation errors made by recognizers may lead to NER errors. Previous work on Chinese textual NER includes Jing et al. (2003) and Sun et al. (2003) but there has been no published work on NER in spoken Chinese. Named Entity Recognition for speech is more difficult than for text, since the most reliable features for textual NER (punctuation, capitalization, and syntactic patterns) are often not available in speech output. NER on automatically recognized broadcast news was first conducted by MITRE in 1997, and was subsequently added to Hub-4 evaluation as a task. Palmer et al. (1999) used error modeling, and Horlock & King (2003) proposed discriminative training to handle NER errors; both used a hidden Markov model (HMM). Miller et al. (1999) also reported results in English speech NER using an HMM model. In a NIST 1999 evaluation, it was found that NER errors on speech arise from a combination of ASR errors and errors of the underlying NER system.  1. Introduction Named Entity Recognition (NER) is the first step for many tasks in the fields of natural language processing and information retrieval. It is a designated task in a number of conferences, including the Message Understanding Conference (MUC), the Information Retrieval and Extraction Conference (IREX), the Conferences on Natural Language Learning (CoNLL) and the recent Automatic Content Extraction Conference (ACE).  In this work, we investigate whether the NIST finding holds for Chinese speech NER as well. We present the first known result for recognizing named entities in realistic large-vocabulary spoken Chinese. We propose to use the best-known model for Chinese textual NER— a maximum entropy model—on Chinese speech NER. We also propose using re-segmentation and postclassification to improve this model. Finally, we propose to integrate the ASR and NER components to optimize NER performance by making use of the n-best ASR output.  There has been a considerable amount of work on English NER yielding good performance (Tjong Kim Sang et al. 2002, 2003; Cucerzan & Yarowsky 1999; Wu et al. 2003). However, Chinese NER is more difficult, especially on speech output, due to two  2. A Spoken Chinese NER Model 2.1 LVCSR output We use the ASR output from BBN’s Byblos system on broadcast news data from the Xinhua News Agency,  which has 1046 sentences. This system has a character error rate of 7%. We had manually annotated them with named entities as an evaluation set according to the PFR corpus annotation guideline (PFR 2001). 2.2 A maximum-entropy NER model with postclassification To establish a baseline spoken Chinese NER model, we selected a maximum entropy (MaxEnt) approach since this is currently the single most accurate approach known for recognizing named entities in text (Tjong Kim Sang et al., 2002, 2003, Jing et al., 2003)1. In the CoNLL 2003 NER evaluation, 5 out of 16 systems use MaxEnt models and the top 3 results for English and top 2 results for German were obtained by systems that use MaxEnt.  Natural language can be viewed as a stochastic process.  We can use p(y|x) to denote the probability distribution  of what we try to predict y (.e.g. part-of-speech tag,  Named Entity tag) conditioned on what we observe x  (e.g. previous POS or the actual word). The Maximum  Entropy principle can be stated as follows: given some  set of constrains from observations, find the most  uniform probability distribution (Maximum Entropy)  p(y|x) that satisfies these constrains:  y* = arg max yi P( yi | xi )  ∑ P( yi  | xi ) =  Z  
Multimodal reference resolution is a process that automatically identifies what users refer to during multimodal human-machine conversation. Given the substantial work on multimodal reference resolution; it is important to evaluate the current state of the art, understand the limitations, and identify directions for future improvement. We conducted a series of user studies to evaluate the capability of reference resolution in a multimodal conversation system. This paper analyzes the main error sources during real-time human-machine interaction and presents key strategies for designing robust multimodal reference resolution algorithms. 
UI on the Fly is a system that dynamically presents coordinated multimodal content through natural language and a small-screen graphical user interface. It adapts to the user’s preferences and situation. Multimodal Functional Uniﬁcation Grammar (MUG) is a uniﬁcation-based formalism that uses rules to generate content that is coordinated across several communication modes. Faithful variants are scored with a heuristic function. 
We present a computationally efﬁcient method for automatic grouping of web search results based on reformulating the original query to alternative queries the user may have intended. The method requires no data other than query logs and the standard inverted indices used by most search engines. Our method outperforms standard web search in the task of enabling users to quickly ﬁnd relevant documents for informational queries. 
In this paper we describe the analytic question answering system HITIQA (High-Quality Interactive Question Answering) which has been developed over the last 2 years as an advanced research tool for information analysts. HITIQA is an interactive open-domain question answering technology designed to allow analysts to pose complex exploratory questions in natural language and obtain relevant information units to prepare their briefing reports. The system uses novel data-driven semantics to conduct a clarification dialogue with the user that explores the scope and the context of the desired answer space. The system has undergone extensive hands-on evaluations by a group of intelligence analysts representing various foreign intelligence services. This evaluation validated the overall approach in HITIQA but also exposed limitations of the current prototype. 
We present a novel morphological analysis technique which induces a morphological and syntactic symmetry between two languages with highly asymmetrical morphological structures to improve statistical machine translation qualities. The technique pre-supposes fine-grained segmentation of a word in the morphologically rich language into the sequence of prefix(es)-stem-suffix(es) and part-of-speech tagging of the parallel corpus. The algorithm identifies morphemes to be merged or deleted in the morphologically rich language to induce the desired morphological and syntactic symmetry. The technique improves Arabic-to-English translation qualities significantly when applied to IBM Model 1 and Phrase Translation Models trained on the training corpus size ranging from 3,500 to 3.3 million sentence pairs. 1. Introduction Translation of two languages with highly different morphological structures as exemplified by Arabic and English poses a challenge to successful implementation of statistical machine translation models (Brown et al. 1993). Rarely occurring inflected forms of a stem in Arabic often do not accurately translate due to the frequency imbalance with the corresponding translation word in English. So called a word (separated by a white space) in Arabic often corresponds to more than one independent word in English, posing a technical problem to the source channel models. In the English-Arabic sentence alignment shown in Figure 1, Arabic word AlAHmr (written in Buckwalter transliteration) is aligned to two English words ‘the red’, and llmEArDp to three English words ‘of the opposition.’ In this paper, we present a technique to induce a morphological and syntactic symmetry between two languages with different morphological structures for statistical translation quality improvement.  The technique is implemented as a two-step  morphological processing for word-based  translation models. We first apply word  segmentation to Arabic, segmenting a word into  prefix(es)-stem-suffix(es).  Arabic-English  sentence alignment after Arabic word  segmentation is illustrated in Figure 2, where one  Arabic morpheme is aligned to one or zero  English word. We then apply the proposed  technique to the word segmented Arabic corpus  to identify prefixes/suffixes to be merged into  their stems or deleted to induce a symmetrical  morphological structure.  Arabic-English  sentence alignment after Arabic morphological  analysis is shown in Figure 3, where the suffix p  is merged into their stems mwAjh and mEArd.  For phrase translation models, we apply  additional morphological analysis induced from  noun phrase parsing of Arabic to accomplish a  syntactic as well as morphological symmetry  between the two languages.  2. Word Segmentation  We pre-suppose segmentation of a word into prefix(es)-stem-suffix(es), as described in (Lee et al. 2003) The category prefix and suffix encompasses function words such as conjunction markers, prepositions, pronouns, determiners and all inflectional morphemes of the language. If a word token contains more than one prefix and/or suffix, we posit multiple prefixes/suffixes per stem. A sample word segmented Arabic text is given below, where prefixes are marked with #, and suffixes with +.  w# s# y# Hl sA}q Al# tjArb fy jAgwAr Al# brAzyly lwsyAnw bwrty mkAn AyrfAyn fy Al# sbAq gdA Al# AHd Al*y s# y# kwn Awly xTw +At +h fy EAlm sbAq +At AlfwrmwlA  3. Morphological Analysis  Morphological analysis identifies functional morphemes to be merged into meaning-bearing stems or to be deleted. In Arabic, functional morphemes typically belong to prefixes or suffixes.  Sudan : alert in the red sea to face build-up of the oppositions in Eritrea  AlswdAn : AstnfAr fy AlbHr AlAHmr lmwAjhp H$wd llmEArDp dAxl ArytryA Figure 1. Word alignment between Arabic and English without Arabic morphological processing Sudan : alert in the red sea to face build-up of the opposition in Eritrea  Al swdAn : AstnfAr fy Al bHr Al AHmr l mwAjh p H$wd l Al mEArd p dAxl ArytryA Figure 2. Alignment between word-segmented Arabic and English Sudan : alert in the red sea to face build-up of the opposition in Eritrea  swdAn : AstnfAr fy Al bHr AHmr l mwAjhp H$wd l Al mEArdp dAxl ArytryA Figure 3. Alignment between morphologically analyzed Arabic and English  Sample Arabic texts before and after morphological analysis is shown below. Mwskw 51-7 ( Af b ) - Elm An Al# qSf Al# mdfEy Al*y Ady Aly ASAb +p jndy +yn rwsy +yn Avn +yn b# jrwH Tfyf +p q*A}f Al# jmE +p fy mTAr xAn qlE +p … Mwskw 51-7 ( Af b ) - Elm An Al# qSf Al# mdfEy Al*y Ady Aly ASAbp jndyyn rwsyyn Avnyn b# jrwH Tfyfp msA' Al# jmEp fy mTAr xAn qlEp … In the morphologically analyzed Arabic (bottom), the feminine singular suffix +p and the masculine plural suffix +yn are merged into the preceding stems analogous to singular/plural noun distinction in English, e.g. girl vs. girls. 3.1 Method We apply part-speech tagging to a symbol tokenized and word segmented Arabic and symbol-tokenized English parallel corpus. We then viterbi-align the part-of-speech tagged parallel corpus, using translation parameters obtained via Model 1 training of word segmented Arabic and symbol-tokenized English, to derive the conditional probability of an English part-of-speech tag given the combination of an Arabic prefix and its part-of-speech or an Arabic suffix and its part-of-speech.1 
Speech recognition errors are inevitable in a speech dialog system. This paper presents an error handling method based on correction grammars which recognize the correction utterances which follow a recognition error. Correction grammars are dynamically created from existing grammars and a set of correction templates. We also describe a prototype dialog system which incorporates this error handling method, and provide empirical evidence that this method can improve dialog success rate and reduce the number of dialog turns required for error recovery. 
We extend existing methods for automatic sentence boundary detection by leveraging multiple recognizer hypotheses in order to provide robustness to speech recognition errors. For each hypothesized word sequence, an HMM is used to estimate the posterior probability of a sentence boundary at each word boundary. The hypotheses are combined using confusion networks to determine the overall most likely events. Experiments show improved detection of sentences for conversational telephone speech, though results are mixed for broadcast news. 
Speech-based interfaces have great potential but are hampered by problems related to spoken language such as variability, noise and ambiguity. Speech Graffiti was designed to address these issues via a structured, universal interface protocol for interacting with simple machines. Since Speech Graffiti requires that users speak to the system in a certain way, we were interested in how users might respond to such a system when compared with a natural language system. We conducted a user study and found that 74% of users preferred the Speech Graffiti system to a natural language interface in the same domain. User satisfaction scores were higher for Speech Graffiti and task completion rates were roughly equal. 
This paper describes a method for evaluating interannotator reliability in an email corpus annotated for type (e.g., question, answer, social chat) when annotators are allowed to assign multiple labels to a message. An augmentation is proposed to Cohen’s kappa statistic which permits all data to be included in the reliability measure and which further permits the identification of more or less reliably annotated data points. 
We present an approach to pronunciation modeling in which the evolution of multiple linguistic feature streams is explicitly represented. This differs from phone-based models in that pronunciation variation is viewed as the result of feature asynchrony and changes in feature values, rather than phone substitutions, insertions, and deletions. We have implemented a ﬂexible feature-based pronunciation model using dynamic Bayesian networks. In this paper, we describe our approach and report on a pilot experiment using phonetic transcriptions of utterances from the Switchboard corpus. The experimental results, as well as the model’s qualitative behavior, suggest that this is a promising way of accounting for the types of pronunciation variation often seen in spontaneous speech. 
In this paper we present preliminary results of a novel unsupervised approach for highprecision detection and correction of errors in the output of automatic speech recognition systems. We model the likely contexts of all words in an ASR system vocabulary by performing a lexical co-occurrence analysis using a large corpus of output from the speech system. We then identify regions in the data that contain likely contexts for a given query word. Finally, we detect words or sequences of words in the contextual regions that are unlikely to appear in the context and that are phonetically similar to the query word. Initial experiments indicate that this technique can produce high-precision targeted detection and correction of misrecognized query words. 
Indexing and retrieving broadcast news stories within a large collection requires automatic detection of story boundaries. This video news story segmentation can use a wide range of audio, language, video, and image features. In this paper, we investigate the correlation between automatically-derived multimodal features and story boundaries in seven different broadcast news sources in three languages. We identify several features that are important for all seven sources analyzed, and we discuss the contributions of other features that are important for a subset of the seven sources. 
We describe an algorithm for choosing term weights to maximize average precision. The algorithm performs successive exhaustive searches through single directions in weight space. It makes use of a novel technique for considering all possible values of average precision that arise in searching for a maximum in a given direction. We apply the algorithm and compare this algorithm to a maximum entropy approach. 
We describe two approaches to analyzing and tagging team discourse using Latent Semantic Analysis (LSA) to predict team performance. The first approach automatically categorizes the contents of each statement made by each of the three team members using an established set of tags. Performance predicting the tags automatically was 15% below human agreement. These tagged statements are then used to predict team performance. The second approach measures the semantic content of the dialogue of the team as a whole and accurately predicts the team’s performance on a simulated military mission. 
In this paper, we present a unigram segmentation model for statistical machine translation where the segmentation units are blocks: pairs of phrases without internal structure. The segmentation model uses a novel orientation component to handle swapping of neighbor blocks. During training, we collect block unigram counts with orientation: we count how often a block occurs to the left or to the right of some predecessor block. The orientation model is shown to improve translation performance over two models: 1) no block re-ordering is used, and 2) the block swapping is controlled only by a language model. We show experimental results on a standard Arabic-English translation task.  airspace Lebanese violate warplanes Israeli  b3 b2 b1  b5 b4  AAAt AAA  l l ln l l l  T HAt m j l  Ar }b r y  sh j w b  r k Ay n  A  l  A  AP }  n  t  y  y  l y  P  
Summarizing threads of email is different from summarizing other types of written communication as it has an inherent dialog structure. We present initial research which shows that sentence extraction techniques can work for email threads as well, but proﬁt from email-speciﬁc features. In addition, the presentation of the summary should take into account the dialogic structure of email communication. 
Information extraction techniques automatically create structured databases from unstructured data sources, such as the Web or newswire documents. Despite the successes of these systems, accuracy will always be imperfect. For many reasons, it is highly desirable to accurately estimate the conﬁdence the system has in the correctness of each extracted ﬁeld. The information extraction system we evaluate is based on a linear-chain conditional random ﬁeld (CRF), a probabilistic model which has performed well on information extraction tasks because of its ability to capture arbitrary, overlapping features of the input in a Markov model. We implement several techniques to estimate the conﬁdence of both extracted ﬁelds and entire multi-ﬁeld records, obtaining an average precision of 98% for retrieving correct ﬁelds and 87% for multi-ﬁeld records. 
Spoken user interfaces are conventionally either dialoguebased or menu-based. In this paper we propose a third approach, in which the task of invoking responses from the system is treated as one of retrieval from the set of all possible responses. Unlike conventional spoken user interfaces that return a unique response to the user, the proposed interface returns a shortlist of possible responses, from which the user must make the final selection. We refer to such interfaces as Speech-In List-Out or SILO interfaces. Experiments show that SILO interfaces can be very effective, are highly robust to degraded speech recognition performance, and can impose significantly lower cognitive load on the user as compared to menu-based interfaces. Keywords Speech interfaces, information retrieval, spoken query 1. INTRODUCTION Spoken input based user interfaces can be broadly categorized as dialogue-based interfaces and menu-selection interfaces. In dialogue-based interfaces, the system engages in a dialogue with the user in an attempt to determine the user’s intention. In menu-based interfaces users traverse a tree of menus, each node of which presents a list of possible choices for the user. In both kinds of interfaces, the speech recognizer must typically convert the user’s speech to an unambiguous text string, which is then used by the UI to determine the action it must take next. Both kinds of interfaces eventually respond to the user with a unique output. In this paper we advocate a third, and different approach to spoken user interfaces. We note that in a majority of applications for which speech interfaces may be used, the goal of the interaction between the user and the system is to evoke a specific response from a limited set of possible responses. In our approach, we view the set of possible responses as documents in an index, and the task of obtaining a specific response as that of retrieval from the set. Spoken input from the user is treated as a query, which is used to retrieve a list of potentially valid responses that are displayed to the user. The user must then make the final selection from the returned list. We call spoken user interfaces based on this approach “Speech-In List-Out” or SILO interfaces. While much has been written on text-based retrieval of spoken or multimedia documents, the topic of information retrieval (IR) using spoken queries has not been addressed much. The usual approach to spoken query based IR has been to use the recognizer as a speech-to-text convertor that generates a text string (Chang et. al, 2002; Chen et. al., 
Currently, information architects create metadata category hierarchies manually. We present a nearly-automated approach for deriving such hierarchies, by converting the lexical hierarchy WordNet into a format that reﬂects the contents of a target information collection. We use the term “nearly-automated” because an information architect should have to make only small adjustments to produce an acceptable metadata structure. We contrast the results with an algorithm that uses lexical co-occurrence statistics. 
It is known that context words tend to be selftriggers, that is, the probability of a content word to appear more than once in a document, given that it already appears once, is signiﬁcantly higher than the probability of the ﬁrst occurrence. We look at self-triggerability across hyperlinks on the Web. We show that the probability of a word ¦¨§ to appear in a Web document © depends on the presence of ¦§ in documents pointing to © . In Document Modeling, we will propose the use of a correction factor,  , which indicates how much more likely a word is to appear in a document given that another document containing the same word is linked to it. 
The lack of sentence boundaries and presence of disﬂuencies pose difﬁculties for parsing conversational speech. This work investigates the effects of automatically detecting these phenomena on a probabilistic parser’s performance. We demonstrate that a state-of-the-art segmenter, relative to a pause-based segmenter, gives more than 45% of the possible error reduction in parser performance, and that presentation of interruption points to the parser improves performance over using sentence boundaries alone. 
We present a novel, type-logical analysis of polarity sensitivity: how negative polarity items (like any and ever) or positive ones (like some) are licensed or prohibited. It takes not just scopal relations but also linear order into account, using the programming-language notions of delimited continuations and evaluation order, respectively. It thus achieves greater empirical coverage than previous proposals. 
In conventional language modeling, the words from only one speaker at a time are represented, even for conversational tasks such as meetings and telephone calls. In a conversational or meeting setting, however, speakers can have signiﬁcant inﬂuence on each other. To recover such un-modeled inter-speaker information, we introduce an approach for conversational language modeling that considers words from other speakers when predicting words from the current one. By augmenting a normal trigram context, our new multi-speaker language model (MSLM) improves on both Switchboard and ICSI Meeting Recorder corpora. Using an MSLM and a conditional mutual information based word clustering algorithm, we achieve a 8.9% perplexity reduction on Switchboard and a 12.2% reduction on the ICSI Meeting Recorder data. 
Automatic topic segmentation, separation of a discourse stream into its constituent stories or topics, is a necessary preprocessing step for applications such as information retrieval, anaphora resolution, and summarization. While signiﬁcant progress has been made in this area for text sources and for English audio sources, little work has been done in automatic, acoustic feature-based segmentation of other languages. In this paper, we focus on prosody-based topic segmentation of Mandarin Chinese. As a tone language, Mandarin presents special challenges for applicability of intonation-based techniques, since the pitch contour is also used to establish lexical identity. We demonstrate that intonational cues such as reduction in pitch and intensity at topic boundaries and increase in duration and pause still provide signiﬁcant contrasts in Mandarin Chinese. We also build a decision tree classiﬁer that, based only on word and local context prosodic information without reference to term similarity, cue phrase, or sentence-level information, achieves boundary classiﬁcation accuracy of 89-95.8% on a large standard test set. 
In this paper, we use a machine learning framework for semantic argument parsing, and apply it to the task of parsing arguments of eventive nominalizations in the FrameNet database. We create a baseline system using a subset of features introduced by Gildea and Jurafsky (2002), which are directly applicable to nominal predicates. We then investigate new features which are designed to capture the novelties in nominal argument structure and show a signiﬁcant performance improvement using these new features. We also investigate the parsing performance of nominalizations in Chinese and compare the salience of the features for the two languages. 
In this paper, a framework for the development of a fast, accurate, and highly portable semantic chunker is introduced. The framework is based on a non-overlapping, shallow tree-structured language. The derivation of the tree is considered as a sequence of tagging actions in a predefined linguistic context, and a novel semantic chunker is accordingly developed. It groups the phrase chunks into the arguments of a given predicate in a bottom-up fashion. This is quite different from current approaches to semantic parsing or chunking that depend on full statistical syntactic parsers that require tree bank style annotation. We compare it with a recently proposed word-byword semantic chunker and present results that show that the phrase-by-phrase approach performs better than its word-by-word counterpart. 
 To date, there are no fully automated systems  addressing the community’s need for funda-  mental language processing tools for Arabic  text. In this paper, we present a Support Vector  Machine (SVM) based approach to automati-  cally tokenize (segmenting off clitics), part-of-  speech (POS) tag and annotate base phrases  (BPs) in Arabic text. We adapt highly accu-  rate tools that have been developed for En-  glish text and apply them to Arabic text. Using  standard evaluation metrics, we rep¡£o¢¥rt¤£t¦ hat the  SVM-TOK tokenizer achieves an  score  of 99.12, the SVM-POS tagger achieves an ac-  curacy of¡ 9¢§5¤£.4¦ 9%, and the SVM-BP chunker  yields an  score of 92.08.  
This paper proposes a method for assigning gestures to text based on lexical and syntactic information. First, our empirical study identified lexical and syntactic information strongly correlated with gesture occurrence and suggested that syntactic structure is more useful for judging gesture occurrence than local syntactic cues. Based on the empirical results, we have implemented a system that converts text into an animated agent that gestures and speaks synchronously. 
This paper describes a transformation-based learning approach to disﬂuency detection in speech transcripts using primarily lexical features. Our method produces comparable results to two other systems that make heavy use of prosodic features, thus demonstrating that reasonable performance can be achieved without extensive prosodic cues. In addition, we show that it is possible to facilitate the identiﬁcation of less frequently disﬂuent discourse markers by taking speaker style into account. 
We present the new multilingual version of the Columbia Newsblaster news summarization system. The system addresses the problem of user access to browsing news from multiple languages from multiple sites on the internet. The system automatically collects, organizes, and summarizes news in multiple source languages, allowing the user to browse news topics with English summaries, and compare perspectives from different countries on the topics.  
ITSPOKE is a spoken dialogue system that uses the Why2-Atlas text-based tutoring system as its “back-end”. A student ﬁrst types a natural language answer to a qualitative physics problem. ITSPOKE then engages the student in a spoken dialogue to provide feedback and correct misconceptions, and to elicit more complete explanations. We are using ITSPOKE to generate an empirically-based understanding of the ramiﬁcations of adding spoken language capabilities to text-based dialogue tutors. 
This paper describes a prototype system for near-real-time spontaneous, bidirectional translation between spoken English and Pashto, a language presenting many technological challenges because of its lack of resources, including both data and expert knowledge. Development of the prototype is ongoing, and we propose to demonstrate a fully functional version which shows the basic capabilities, though not yet their final depth and breadth.  is to exchange information by whatever means is effective: not, necessarily, to rely exclusively on the system’s output, but to use it in combination with other, non-speech modalities of conveying meaning and with ordinary world knowledge. The final system is intended to run on a handheld device, such as a PDA, with its attendant memory and speed limitations and restriction to integer-only computation. Most components of the demonstration system run in a PocketPC emulation environment on a Windows laptop, with a few in the full Windows environment. All aspects of the prototype system are undergoing active development. 2 Overall Architecture  
The MiTAP prototype for SARS detection uses human language technology for detecting, monitoring, and analyzing potential indicators of infectious disease outbreaks and reasoning for issuing warnings and alerts. MiTAP focuses on providing timely, multilingual information access to analysts, domain experts, and decision-makers worldwide. Data sources are captured, filtered, translated, summarized, and categorized by content. Critical information is automatically extracted and tagged to facilitate browsing, searching, and scanning, and to provide key terms at a glance. The processed articles are made available through an easy-to-use news server and cross-language information retrieval system for access and analysis anywhere, any time. Specialized newsgroups and customizable filters or searches on incoming stories allow users to create their own view into the data while a variety of tools summarize, indicate trends, and provide alerts to potentially relevant spikes of activity. 
This paper describes a fully-automated realtime broadcast news video and audio processing system. The system combines speech recognition, machine translation, and crosslingual information retrieval components to enable real-time alerting from live English and Arabic news sources. 
This paper describes a rule-based semantic parser that relies on a frame dataset (FrameNet), and a semantic network (WordNet), to identify semantic relations between words in open text, as well as shallow semantic features associated with concepts in the text. Parsing semantic structures allows semantic units and constituents to be accessed and processed in a more meaningful way than syntactic parsing, moving the automation of understanding natural language text to a higher level. 
 £  £  Dragomir Radev , T£ imothy Allison , M£ atthew Craig £, Stanko Dim£ itrov ,  Omer Kareem , Michael Topper , Adam Winkel , and Jin Yi  £  School of Information  Department o¤ f Electrical Engineering and Computer Science  Department of Classical Studies  ¥  University of Michigan, Ann Arbor, MI 48109  radev,tballiso,mwcraig,sdimitro,okareem,mtopper,winkela,jyi¦ @umich.edu  
SenseClusters is a freely available word sense discrimination system that takes a purely unsupervised clustering approach. It uses no knowledge other than what is available in a raw unstructured corpus, and clusters instances of a given target word based only on their mutual contextual similarities. It is a complete system that provides support for feature selection from large corpora, several different context representation schemes, various clustering algorithms, and evaluation of the discovered clusters.  tering contextually (and hence semantically) similar instances of text can be used in a variety of natural language processing tasks such as synonymy identiﬁcation, text summarization and document classiﬁcation. SenseClusters has also been used for applications such as email sorting and automatic ontology construction. In the sections that follow we will describe the basic functionality supported by SenseClusters. In general processing starts by selecting features from a corpus of text. Then these features are used to create an appropriate representation of the contexts that are to be clustered. Thereafter the actual clustering takes place, followed by an optional evaluation stage that compares the discovered clusters to an existing gold standard (if available).  
This demonstration shows a flexible tutoring system for studying the effects of different tutoring strategies enhanced by a spoken language interface. The hypothesis is that spoken language increases the effectiveness of automated tutoring. The domain is Navy damage control. 
namely Chinese, Croatian, French, German, Japanese, Spanish, and Turkish as seed models for the Thai phone set. Table 1 describes the performance of the Thai speech recognition component for different acoustic model sizes (context-independent vs. 500 and 1000 tri-phone models). The results indicate that a Thai speech recognition engine can be built by using the bootstrapping approach with a reasonable amount of speech data. Even the very initial system bootstrapped from multilingual seed models gives a performance above 80% word accuracy. The good performance might be an artifact from the very limited domain with a compact and closed vocabulary. 
Semantic language model is a technique that utilizes the semantic structure of an utterance to better rank the likelihood of words composing the sentence. When used in a conversational system, one can dynamically integrate the dialog state and domain semantics into the semantic language model to better guide the speech recognizer executing the decoding process. We describe one such application that employs semantic language model to cope with spontaneous speech in a robust manner. The semantic language model, though can be manually crafted without data, can benefit significantly from data driven machine learning techniques. An example based approach is also described here to demonstrate a viable approach. 
WordNet::Similarity is a freely available software package that makes it possible to measure the semantic similarity and relatedness between a pair of concepts (or synsets). It provides six measures of similarity, and three measures of relatedness, all of which are based on the lexical database WordNet. These measures are implemented as Perl modules which take as input two concepts, and return a numeric value that represents the degree to which they are similar or related. 
This paper analyzes various issues in building a HMM based multilingual speech recognizer for Indian languages. The system is originally designed for Hindi and Tamil languages and adapted to incorporate Indian accented English. Language-specific characteristics in speech recognition framework are highlighted. The recognizer is embedded in information retrieval applications and hence several issues like handling spontaneous telephony speech in real-time, integrated language identification for interactive response and automatic grapheme to phoneme conversion to handle Out Of Vocabulary words are addressed. Experiments to study relative effectiveness of different algorithms have been performed and the results are investigated. 
We investigate various strategies for finding chemicals in biomedical text using substring co-occurrence information. The goal is to build a system from readily available data with minimal human involvement. Our models are trained from a dictionary of chemical names and general biomedical text. We investigated several strategies including Naïve Bayes classifiers and several types of N-gram models. We introduced a new way of interpolating N-grams that does not require tuning any parameters. We also found the task to be similar to Language Identification. 
In this work, we are concerned with a coarse grained semantic analysis over sparse data, which labels all nouns with a set of semantic categories. To get the beneﬁt of unlabeled data, we propose a bootstrapping framework with Maximum Entropy modeling (MaxEnt) as the statistical learning component. During the iterative tagging process, unlabeled data is used not only for better statistical estimation, but also as a medium to integrate non-statistical knowledge into the model training. Two main issues are discussed in this paper. First, Association Rule principles are suggested to guide MaxEnt feature selections. Second, to guarantee the convergence of the bootstrapping process, three adjusting strategies are proposed to soft tag unlabeled data. 
Under a lexicalist approach to semantics, a verb completely encodes its syntactic and semantic structures, along with the relevant syntax-tosemantics mapping; polysemy is typically attributed to the existence of different lexical entries. A lexicon organized in this fashion contains much redundant information and is unable to capture cross-categorial morphological derivations. The solution is to spread the “semantic load” of lexical entries to other morphemes not typically taken to bear semantic content. This approach follows current trends in linguistic theory, and more perspicuously accounts for alternations in argument structure. I demonstrate how such a framework can be computationally realized with a feature-based, agenda-driven chart parser for the Minimalist Program. 
The translation of English text into American Sign Language (ASL) animation tests the limits of traditional MT architectural designs. A new semantic representation is proposed that uses virtual reality 3D scene modeling software to produce spatially complex ASL phenomena called “classifier predicates.” The model acts as an interlingua within a new multi-pathway MT architecture design that also incorporates transfer and direct approaches into a single system. 
One common mistake made by non-native speakers of English is to drop the articles a, an, or the. We apply the log-linear model to automatically restore missing articles based on features of the noun phrase. We ﬁrst show that the model yields competitive results in article generation. Further, we describe methods to adjust the model with respect to the initial quality of the sentence. Our best results are 20.5% article error rate (insertions, deletions and substitutions) for sentences where 30% of the articles have been dropped, and 38.5% for those where 70% of the articles have been dropped. 
Tokenization in the bioscience domain is often difficult. New terms, technical terminology, and nonstandard orthography, all common in bioscience text, contribute to this difficulty. This paper will introduce the tasks of tokenization, normalization before introducing BAccHANT, a system built for bioscience text normalization. Casting tokenization / normalization as a problem of punctuation classification motivates using machine learning methods in the implementation of this system. The evaluation of BAccHANT's performance included error analysis of the system's performance inside and outside of named entities (NEs) from the GENIA corpus, which led to the creation of a normalization system trained solely on data from inside NEs, BAccHANT-N. Evaluation of this new system indicated that normalization systems trained on data inside NEs perform better than systems trained both inside and outside NEs, motivating a merging of tokenization and named entity tagging processes as opposed to the standard pipelining approach. 
Several computational simulations have been proposed for how children solve the word segmentation problem, but most have been tested only on a limited number of languages, often only English. In order to extend the cross-linguistic dimension of word segmentation research, a finite-state framework for testing various models of word segmentation is sketched, and a very simple cue is tested in this framework. Data is taken from Modern Greek, a language with phonological patterns distinct from English. A small-scale simulation shows using this cue performs significantly better than chance. The utility and flexibility of the finite-state approach is confirmed; suggestions for improvement are noted and directions for future work outlined. 
This paper describes a system for constructing conceptual graph representation of text by using a combination of existing linguistic resources (VerbNet and WordNet). We use a twostep approach, by ﬁrstly identifying the semantic roles in a sentence, and then using these roles, together with semi-automatically compiled domain-speciﬁc knowledge to construct the conceptual graph representation. 
When estimating a mixture of Gaussians there are usually two choices for the covariance type of each Gaussian component. Either diagonal or full covariance. Imposing a structure though may be restrictive and lead to degraded performance and/or increased computations. In this work, several criteria to estimate the structure of regression matrices of a mixture of Gaussians are introduced and evaluated. Most of the criteria attempt to estimate a discriminative structure, which is suited for classiﬁcation tasks. Results are reported on the 1996 NIST speaker recognition task and performance is compared with structural EM, a well-known, non-discriminative, structureﬁnding algorithm. 
Entity detection and tracking is a relatively new addition to the repertoire of natural language tasks. In this paper, we present a statistical language-independent framework for identifying and tracking named, nominal and pronominal references to entities within unrestricted text documents, and chaining them into clusters corresponding to each logical entity present in the text. Both the mention detection model and the novel entity tracking model can use arbitrary feature types, being able to integrate a wide array of lexical, syntactic and semantic features. In addition, the mention detection model crucially uses feature streams derived from different named entity classiﬁers. The proposed framework is evaluated with several experiments run in Arabic, Chinese and English texts; a system based on the approach described here and submitted to the latest Automatic Content Extraction (ACE) evaluation achieved top-tier results in all three evaluation languages. 
In this paper, we will compare and evaluate the effectiveness of different statistical methods in the task of cross-document coreference resolution. We created entity models for different test sets and compare the following disambiguation and clustering techniques to cluster the entity models in order to create coreference chains: Incremental Vector Space KL-Divergence Agglomerative Vector Space 
A given entity, representing a person, a location or an organization, may be mentioned in text in multiple, ambiguous ways. Understanding natural language requires identifying whether different mentions of a name, within and across documents, represent the same entity. We develop an unsupervised learning approach that is shown to resolve accurately the name identiﬁcation and tracing problem. At the heart of our approach is a generative model of how documents are generated and how names are “sprinkled” into them. In its most general form, our model assumes: (1) a joint distribution over entities, (2) an “author” model, that assumes that at least one mention of an entity in a document is easily identiﬁable, and then generates other mentions via (3) an appearance model, governing how mentions are transformed from the “representative” mention. We show how to estimate the model and do inference with it and how this resolves several aspects of the problem from the perspective of applications such as questions answering. 
One of the ﬁrst steps towards understanding natural multimodal language is aligning gesture and speech, so that the appropriate gestures ground referential pronouns in the speech. This paper presents a novel technique for gesture-speech alignment, inspired by saliencebased approaches to anaphoric pronoun resolution. We use a hybrid between data-driven and knowledge-based mtehods: the basic structure is derived from a set of rules about gesture salience, but the salience weights themselves are learned from a corpus. Our system achieves 95% recall and precision on a corpus of transcriptions of unconstrained multimodal monologues, signiﬁcantly outperforming a competitive baseline. 
Moderate-sized rule-based spoken language models for recognition and understanding are easy to develop and provide the ability to rapidly prototype conversational applications. However, scalability of such systems is a bottleneck due to the heavy cost of authoring and maintenance of rule sets and inevitable brittleness due to lack of coverage in the rule sets. In contrast, data-driven approaches are robust and the procedure for model building is usually simple. However, the lack of data in a particular application domain limits the ability to build data-driven models. In this paper, we address the issue of combining data-driven and grammar-based models for rapid prototyping of robust speech recognition and understanding models for a multimodal conversational system. We also present methods that reuse data from different domains and investigate the limits of such models in the context of a particular application domain. 
Deﬁnition questions represent a largely unexplored area of question answering—they are different from factoid questions in that the goal is to return as many relevant “nuggets” of information about a concept as possible. We describe a multi-strategy approach to answering such questions using a database constructed offline with surface patterns, a Webbased dictionary, and an off-the-shelf document retriever. Results are presented from component-level evaluation and from an endto-end evaluation of our implemented system at the TREC 2003 Question Answering Track. 
 gine. It follows that there is a good economic incentive in moving the QA task to a more general level: it is  In this paper we describe and evaluate a Question Answering system that goes beyond answering factoid questions. We focus on FAQlike questions and answers, and build our system around a noisy-channel architecture which exploits both a language model for answers and a transformation model for answer/question terms, trained on a corpus of 1 million question/answer pairs collected from the Web.  likely that a system able to answer complex questions of the type people generally and/or frequently ask has greater potential impact than one restricted to answering only factoid questions. A natural move is to recast the question answering task to handling questions people frequently ask or want answers for, as seen in Frequently Asked Questions (FAQ) lists. These questions are sometimes factoid questions (such as, “What is Scotland's national costume?”), but in general are more complex questions (such as, “How does a film qualify for an Academy Award?”, which requires an answer along the  following lines: “A feature film must screen in a Los  
The field of Psychometrics routinely grapples with the question of what it means to measure the inherent ability of an organism to perform a given task, and for the last forty years, the field has increasingly relied on probabilistic methods such as the Rasch model for test construction and the analysis of test results. Because the underlying issues of measuring ability apply to human language technologies as well, such probabilistic methods can be advantageously applied to the evaluation of those technologies. To test this claim, Rasch measurement was applied to the results of 67 systems participating in the Question Answering track of the 2002 Text REtrieval Conference (TREC) competition. Satisfactory model fit was obtained, and the paper illustrates the theoretical and practical strengths of Rasch scaling for evaluating systems as well as questions. Most important, simulations indicate that a test invariant metric can be defined by carrying forward 20 to 50 equating questions, thus placing the yearly results on a common scale. 
This paper describes an automatic method for acquiring hyponymy relations from HTML documents on the WWW. Hyponymy relations can play a crucial role in various natural language processing systems. Most existing acquisition methods for hyponymy relations rely on particular linguistic patterns, such as “NP such as NP”. Our method, however, does not use such linguistic patterns, and we expect that our procedure can be applied to a wide range of expressions for which existing methods cannot be used. Our acquisition algorithm uses clues such as itemization or listing in HTML documents and statistical measures such as document frequencies and verb-noun co-occurrences.  ɾ Car Specification ɾ Toyota ɾ Honda ɾ Nissan Figure 1: An example of itemization popular in the IR literature. The third is verb-noun cooccurrence in normal corpora. In our acquisition, we made the following assumptions. Assumption A Expressions included in the same itemization or listing in an HTML document are likely to have a common hypernym. Assumption B Given a set of hyponyms that have a common hypernym, the hypernym appears in many documents that include the hyponyms.  
This paper investigates the usefulness of sentence-internal prosodic cues in syntactic parsing of transcribed speech. Intuitively, prosodic cues would seem to provide much the same information in speech as punctuation does in text, so we tried to incorporate them into our parser in much the same way as punctuation is. We compared the accuracy of a statistical parser on the LDC Switchboard treebank corpus of transcribed sentence-segmented speech using various combinations of punctuation and sentence-internal prosodic information (duration, pausing, and f0 cues). With no prosodic or punctuation information the parser’s accuracy (as measured by F-score) is 86.9%, and adding punctuation increases its F-score to 88.2%. However, all of the ways we have tried of adding prosodic information decrease the parser’s F-score to between 84.8% to 86.8%, depending on exactly which prosodic information is added. This suggests that for sentence-internal prosodic information to improve speech transcript parsing, either different prosodic cues will have to used or they will have be exploited in the parser in a way different to that used currently. 
Supervised estimation methods are widely seen as being superior to semi and fully unsupervised methods. However, supervised methods crucially rely upon training sets that need to be manually annotated. This can be very expensive, especially when skilled annotators are required. Active learning (AL) promises to help reduce this annotation cost. Within the complex domain of HPSG parse selection, we show that ideas from ensemble learning can help further reduce the cost of annotation. Our main results show that at times, an ensemble model trained with randomly sampled examples can outperform a single model trained using AL. However, converting the single-model AL method into an ensemble-based AL method shows that even this much stronger baseline model can be improved upon. Our best results show a ¢¤£¦¥ reduction in annotation cost compared with single-model random sampling. 
This paper reports some experiments that compare the accuracy and performance of two stochastic parsing systems. The currently popular Collins parser is a shallow parser whose output contains more detailed semanticallyrelevant information than other such parsers. The XLE parser is a deep-parsing system that couples a Lexical Functional Grammar to a loglinear disambiguation component and provides much richer representations theory. We measured the accuracy of both systems against a gold standard of the PARC 700 dependency bank, and also measured their processing times. We found the deep-parsing system to be more accurate than the Collins parser with only a slight reduction in parsing speed.1 
Many probabilistic models for natural language are now written in terms of hierarchical tree structure. Tree-based modeling still lacks many of the standard tools taken for granted in (ﬁnitestate) string-based modeling. The theory of tree transducer automata provides a possible framework to draw on, as it has been worked out in an extensive literature. We motivate the use of tree transducers for natural language and address the training problem for probabilistic tree-totree and tree-to-string transducers. 
We consider the problem of modeling the content structure of texts within a speciﬁc domain, in terms of the topics the texts address and the order in which these topics appear. We ﬁrst present an effective knowledge-lean method for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models. We then apply our method to two complementary tasks: information ordering and extractive summarization. Our experiments show that incorporating content models in these applications yields substantial improvement over previously-proposed methods. 
Previous work demonstrated that web counts can be used to approximate bigram frequencies, and thus should be useful for a wide variety of NLP tasks. So far, only two generation tasks (candidate selection for machine translation and confusion-set disambiguation) have been tested using web-scale data sets. The present paper investigates if these results generalize to tasks covering both syntax and semantics, both generation and analysis, and a larger range of n-grams. For the majority of tasks, we ﬁnd that simple, unsupervised models perform better when n-gram frequencies are obtained from the web rather than from a large corpus. However, in most cases, web-based models fail to outperform more sophisticated state-of-theart models trained on small corpora. We argue that web-based models should therefore be used as a baseline for, rather than an alternative to, standard models. 
Recent work on spoken document retrieval has suggested that it is adequate to take the singlebest output of ASR, and perform text retrieval on this output. This is reasonable enough for the task of retrieving broadcast news stories, where word error rates are relatively low, and the stories are long enough to contain much redundancy. But it is patently not reasonable if one’s task is to retrieve a short snippet of speech in a domain where WER’s can be as high as 50%; such would be the situation with teleconference speech, where one’s task is to ﬁnd if and when a participant uttered a certain phrase. In this paper we propose an indexing procedure for spoken utterance retrieval that works on lattices rather than just single-best text. We demonstrate that this procedure can improve F scores by over ﬁve points compared to singlebest retrieval on tasks with poor WER and low redundancy. The representation is ﬂexible so that we can represent both word lattices, as well as phone lattices, the latter being important for improving performance when searching for phrases containing OOV words. 
The regular occurrence of disﬂuencies is a distinguishing characteristic of spontaneous speech. Detecting and removing such disﬂuencies can substantially improve the usefulness of spontaneous speech transcripts. This paper presents a system that detects various types of disﬂuencies and other structural information with cues obtained from lexical and prosodic information sources. Speciﬁcally, combinations of decision trees and language models are used to predict sentence ends and interruption points and, given these events, transformationbased learning is used to detect edit disﬂuencies and conversational ﬁllers. Results are reported on human and automatic transcripts of conversational telephone speech. 
We present an empirically grounded method for evaluating content selection in summarization. It incorporates the idea that no single best model summary for a collection of documents exists. Our method quantiﬁes the relative importance of facts to be conveyed. We argue that it is reliable, predictive and diagnostic, thus improves considerably over the shortcomings of the human evaluation method currently used in the Document Understanding Conference. 
In this paper we propose a data intensive approach for inferring sentence-internal temporal relations, which relies on a simple probabilistic model and assumes no manual coding. We explore various combinations of features, and evaluate performance against a goldstandard corpus and human subjects performing the same task. The best model achieves 70.7% accuracy in inferring the temporal relation between two clauses and 97.4% accuracy in ordering them, assuming that the temporal relation is known. 
We describe a methodology for rapid experimentation in statistical machine translation which we use to add a large number of features to a baseline system exploiting features from a wide range of levels of syntactic representation. Feature values were combined in a log-linear model to select the highest scoring candidate translation from an n-best list. Feature weights were optimized directly against the BLEU evaluation metric on held-out data. We present results for a small selection of features at each level of syntactic representation. 
We present Minimum Bayes-Risk (MBR) decoding for statistical machine translation. This statistical approach aims to minimize expected loss of translation errors under loss functions that measure translation performance. We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings, word-to-word alignments from an MT system, and syntactic structure from parse-trees of source and target language sentences. We report the performance of the MBR decoders on a Chinese-to-English translation task. Our results show that MBR decoding can be used to tune statistical MT performance for speciﬁc loss functions. 
This paper describes the application of discriminative reranking techniques to the problem of machine translation. For each sentence in the source language, we obtain from a baseline statistical machine translation system, a ranked best list of candidate translations in the target language. We introduce two novel perceptroninspired reranking algorithms that improve on the quality of machine translation over the baseline system based on evaluation using the BLEU metric. We provide experimental results on the NIST 2003 Chinese-English large data track evaluation. We also provide theoretical analysis of our algorithms and experiments that verify that our algorithms provide state-of-theart performance in machine translation. 
CriterionSM Online Essay Evaluation Service includes a capability that labels sentences in student writing with essay-based discourse elements (e.g., thesis statements). We describe a new system that enhances Criterion’s capability, by evaluating multiple aspects of coherence in essays. This system identiﬁes features of sentences based on semantic similarity measures and discourse structure. A support vector machine uses these features to capture breakdowns in coherence due to relatedness to the essay question and relatedness between discourse elements. Intra-sentential quality is evaluated with rule-based heuristics. Results indicate that the system yields higher performance than a baseline on all three aspects. 
We demonstrate a new research approach to the problem of predicting the reading difficulty of a text passage, by recasting readability in terms of statistical language modeling. We derive a measure based on an extension of multinomial naïve Bayes classification that combines multiple language models to estimate the most likely grade level for a given passage. The resulting classifier is not specific to any particular subject and can be trained with relatively little labeled data. We perform predictions for individual Web pages in English and compare our performance to widely-used semantic variables from traditional readability measures. We show that with minimal changes, the classifier may be retrained for use with French Web documents. For both English and French, the classifier maintains consistently good correlation with labeled grade level (0.63 to 0.79) across all test sets. Some traditional semantic variables such as type-token ratio gave the best performance on commercial calibrated test passages, while our language modeling approach gave better accuracy for Web documents and very short passages (less than 10 words). 
We examine the utility of multiple types of turn-level and contextual linguistic features for automatically predicting student emotions in human-human spoken tutoring dialogues. We ﬁrst annotate student turns in our corpus for negative, neutral and positive emotions. We then automatically extract features representing acoustic-prosodic and other linguistic information from the speech signal and associated transcriptions. We compare the results of machine learning experiments using different feature sets to predict the annotated emotions. Our best performing feature set contains both acoustic-prosodic and other types of linguistic features, extracted from both the current turn and a context of previous student turns, and yields a prediction accuracy of 84.75%, which is a 44% relative improvement in error reduction over a baseline. Our results suggest that the intelligent tutoring spoken dialogue system we are developing can be enhanced to automatically predict and adapt to student emotions. 
End-to-end evaluations of conversational dialogue systems with naive users are currently uncovering severe usability problems that result in low task completion rates. Preliminary analyses suggest that these problems are related to the system’s dialogue management and turntaking behavior. We present the results of experiments designed to take a detailed look at the effects of that behavior. Based on the resulting ﬁndings, we spell out a set of criteria which lie orthogonal to dialogue quality, but nevertheless constitute an integral part of a more comprehensive view on dialogue felicity as a function of dialogue quality and efﬁciency. 
This paper describes the CMU Let’s Go!! bus information system, an experimental system designed to study the use of spoken dialogue interfaces by non-native speakers. The differences in performance of the speech recognition and language understanding modules of the system when confronted with native and non-native spontaneous speech are analyzed. Focus is placed on the linguistic mismatch between the user input and the system’s expectations, and on its implications in terms of language modeling and parsing performance. The effect of including non-native data when building the speech recognition and language understanding modules is discussed. In order to close the gap between non-native and native input, a method is proposed to automatically generate conﬁrmation prompts that are both close to the user’s input and covered by the system’s language model and grammar, in order to help the user acquire idiomatic expressions appropriate to the task. 
The paper presents two approaches to interactively refining user search formulations and their evaluation in the new High Accuracy Retrieval from Documents (HARD) track of TREC-12. One method consists of asking the user to select a number of sentences that may represent relevant documents, and then using the documents, whose sentences were selected for query expansion. The second method consists of showing to the user a list of noun phrases, extracted from the initial document set, and then expanding the query with the terms from the phrases selected by the user. 
In this paper, we propose a machine learning algorithm for shallow semantic parsing, extending the work of Gildea and Jurafsky (2002), Surdeanu et al. (2003) and others. Our algorithm is based on Support Vector Machines which we show give an improvement in performance over earlier classiﬁers. We show performance improvements through a number of new features and measure their ability to generalize to a new test set drawn from the AQUAINT corpus. 
There are a lot of differences between expressions used in written language and spoken language. It is one of the reasons why speech synthesis applications are prone to produce unnatural speech. This paper represents a method of paraphrasing unsuitable expressions for spoken language into suitable ones. Those two expressions can be distinguished based on the occurrence probability in written and spoken language corpora which are automatically collected from the Web. Experimental results indicated the effectiveness of our method. The precision of the collected corpora was 94%, and the accuracy of learning paraphrases was 76 %.  Although the ﬁrst problem is well-known, little attention has been given to the second one. The reason why the second problem arises is that the input text contains Unsuitable Expressions for Spoken language (UES). Therefore, the problem can be resolved by paraphrasing UES into Suitable Expression for Spoken language (SES). This is a new application of paraphrasing. There are no similar attempts, although a variety of applications have been discussed so far, for example question-answering (Lin and Pantel, 2001; Hermjakob et al., 2002; Duclaye and Yvon, 2003) or text-simpliﬁcation (Inui et al., 2003). (1) Written (2) Spoken  
In this paper we address the question of assigning semantic roles to sentences in Chinese. We show that good semantic parsing results for Chinese can be achieved with a small 1100-sentence training set. In order to extract features from Chinese, we describe porting the Collins parser to Chinese, resulting in the best performance currently reported on Chinese syntactic parsing; we include our headrules in the appendix. Finally, we compare English and Chinese semantic-parsing performance. While slight differences in argument labeling make a perfect comparison impossible, our results nonetheless suggest significantly better performance for Chinese. We show that much of this difference is due to grammatical differences between English and Chinese, such as the prevalence of passive in English, and the strict word order constraints on adjuncts in Chinese. 
In statistical machine translation, the currently best performing systems are based in some way on phrases or word groups. We describe the baseline phrase-based translation system and various reﬁnements. We describe a highly efﬁcient monotone search algorithm with a complexity linear in the input sentence length. We present translation results for three tasks: Verbmobil, Xerox and the Canadian Hansards. For the Xerox task, it takes less than 7 seconds to translate the whole test set consisting of more than 10K words. The translation results for the Xerox and Canadian Hansards task are very promising. The system even outperforms the alignment template system.  
We propose a theory that gives formal semantics to word-level alignments deﬁned over parallel corpora. We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data. 
State-of-the-art pronoun interpretation systems rely predominantly on morphosyntactic contextual features. While the use of deep knowledge and inference to improve these models would appear technically infeasible, previous work has suggested that predicate-argument statistics mined from naturally-occurring data could provide a useful approximation to such knowledge. We test this idea in several system conﬁgurations, and conclude from our results and subsequent error analysis that such statistics oﬀer little or no predictive information above that provided by morphosyntax. 
We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor. BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning. These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible. BABAR applies a Dempster-Shafer probabilistic model to make resolutions based on evidence from the contextual role knowledge sources as well as general knowledge sources. Experiments in two domains showed that the contextual role knowledge improved coreference performance, especially on pronouns. 
Maximum entropy models are a common modeling technique, but prone to overﬁtting. We show that using an exponential distribution as a prior leads to bounded absolute discounting by a constant. We show that this prior is better motivated by the data than previous techniques such as a Gaussian prior, and often produces lower error rates. Exponential priors also lead to a simpler learning algorithm and to easier to understand behavior. Furthermore, exponential priors help explain the success of some previous smoothing techniques, and suggest simple variations that work better. 
State-of-the-art story link detection systems, that is, systems that determine whether two stories are about the same event or linked, are usually based on the cosine-similarity measured between two stories. This paper presents a method for improving the performance of a link detection system by using a variety of similarity measures and using source-pair speciﬁc statistical information. The utility of a number of different similarity measures, including cosine, Hellinger, Tanimoto, and clarity, both alone and in combination, was investigated. We also compared several machine learning techniques for combining the different types of information. The techniques investigated were SVMs, voting, and decision trees, each of which makes use of similarity and statistical information differently. Our experimental results indicate that the combination of similarity measures and source-pair speciﬁc statistical information using an SVM provides the largest improvement in estimating whether two stories are linked; the resulting system was the bestperforming link detection system at TDT-2002. 
Systems that automatically discover semantic classes have emerged in part to address the limitations of broad-coverage lexical resources such as WordNet and Cyc. The current state of the art discovers many semantic classes but fails to label their concepts. We propose an algorithm labeling semantic classes and for leveraging them to extract is-a relationships using a top-down approach. 
With the increasing use of research paper search engines, such as CiteSeer, for both literature search and hiring decisions, the accuracy of such systems is of paramount importance. This paper employs Conditional Random Fields (CRFs) for the task of extracting various common ﬁelds from the headers and citation of research papers. The basic theory of CRFs is becoming well-understood, but best-practices for applying them to real-world data requires additional exploration. This paper makes an empirical exploration of several factors, including variations on Gaussian, exponential and hyperbolic-L1 priors for improved regularization, and several classes of features and Markov order. On a standard benchmark data set, we achieve new state-of-the-art performance, reducing error in average F1 by 36%, and word error rate by 78% in comparison with the previous best SVM results. Accuracy compares even more favorably against HMMs. 
We present a technique for augmenting annotated training data with hierarchical word clusters that are automatically derived from a large unannotated corpus. Cluster membership is encoded in features that are incorporated in a discriminatively trained tagging model. Active learning is used to select training examples. We evaluate the technique for named-entity tagging. Compared with a state-of-the-art HMM-based name finder, the presented technique requires only 13% as much annotated data to achieve the same level of performance. Given a large annotated training set of 1,000,000 words, the technique achieves a 25% reduction in error over the state-of-the-art HMM trained on the same material. 
A word class often neglected in the field of NLP resources, namely adverbs, has lately been described in a computational lexicon produced at CST as one of the results of a Ph.D.-project. The adverb lexicon, which is integrated in the Danish STO lexicon, gives detailed syntactic information on the type of modification and position, as well as on other syntactic properties of approx 800 Danish adverbs. One of the aims of the lexicon has been to establish a clear distinction between syntactic and semantic information - where other lexicons often generalize over the syntactic behavior of semantic classes of adverbs, every adverb is described with respect to its proper syntactic behavior in a text corpus, revealing very individual syntactic properties. Syntactic information on adverbs is needed in NLP systems generating text to ensure correct placing in the phrase they modify. Also in systems analyzing text, this information is needed in order to attach the adverbs to the right node in the syntactic parse trees. Within the field of linguistic research, several results can be deduced from the lexicon, e.g. knowledge of syntactic classes of Danish adverbs.
The objective of the Cross-Language Evaluation Forum (CLEF) is to promote research in the multilingual information access domain. In this short paper, we list the achievements of CLEF during its first four years of activity and describe how the range of tasks has been considerably expanded during this period. The aim of the paper is to demonstrate the importance of evaluation initiatives with respect to system research and development and to show how essential it is for such initiatives to keep abreast of and even anticipate the emerging needs of both system developers and application communities if they are to have a future.
The SALA II project comprises mobile telephone recordings according to the SpeechDat (II) paradigm for several languages in North and Latin America. Each database contains the recordings of 1000 speakers, with the exception of US Spanish (2000 speakers) and US English (4000 speakers). A quarter of the recordings of each database are made respectively in a quiet environment (home/office), in the street, in a public place, and in a moving vehicle. This paper presents an evaluation of the project. The paper details on experiences with respect to the implementation of design specifications, speaker recruitment, data recordings (on site), data processing, orthographic transcription and lexicon generation. Furthermore, the validation procedure and its results are documented. Finally, the availability and distribution of the databases are addressed.
In this paper, we present the methodology developed by the SLI (Computational Linguistics Group of the University of Vigo) for the building and processing of the CLUVI Corpus, showing the TMX-based XML specification designed to encode both morphosyntactic features and translation alignments in parallel corpora, and the solutions adopted for making the CLUVI parallel corpora freely available over the WWW (http://sli.uvigo.es/CLUVI/).
We have developed a toolkit in which an annotation tool, a syntactic tree editor, and an extraction rule editor interact dynamically. Its output can be stored in a database for further use. In the field of biomedicine, there is a critical need for automatic text processing. However, current language processing approaches suffer from insufficient basic data incorporating both human domain expertise and domain-specific language processing capabilities. With the annotation tool presented here, a set of ggold standardsh can be collected, representing what should be extracted. At the same time, any change in annotation can be viewed on an associated syntactic tree. These facilities provide a clear picture of the relationship between the extraction target and the syntactic tree. Underlying sentences can be analyzed with a parser which can be plugged in, or a set of parsed sentences can be used to generate the tree. Extraction rules written with the integrated editor can be applied at once, and their validity can immediately be verified both on the syntactic tree and on the sentence string by coloring the corresponding segments. Thus our toolkit enables the user to efficiently construct parse-based extraction rules. PBIE2 works under Windows 2000/XP and requires Microsoft Internet Explorer 6.0 or higher. The data can be stored in Microsoft Access.
This paper describes the creation of a syntactically annotated Tibetan corpus. This corpus forms a part of the TUSNELDA collection of corpora and databases for linguistic research. It will ultimately comprise spoken and written Tibetan texts originating from different regions and historical epochs. These texts are annotated with several kinds of linguistic information, in particular POS tags, phrases, argument structures of verbs, clauses and sentences, as well as several kinds of discourse units and textual segments. The annotation is done in XML. The primary research interest which guides the development of the corpus is the investigation of cross-clausal references, especially the relation between empty arguments (i.e. arguments not overtly realised in a clause) and their antecedents in previous clauses. For this purpose, such references are explicitly encoded so that they can be qualitatively and quantitatively evaluated with the help of standard XML techniques such as XPath search and XSLT transformations. Apart from this primary research interest, we expect that our corpus will be useful for other projects concerning Tibetan and related languages. Like other data in TUSNELDA, it will be made accessible via a WWW query interface.
We report on the development and employment of lexical entry templates in a large--coverage unification--based grammar of Spanish. The aim of the work reported in this paper is to provide robust deep linguistic processing in order to make the grammar more adequate for industrial NLP applications.
In this paper we describe a new baseline tagset induction algorithm, which unlike the one described in previous work is fully automatic and produces tagsets with better performance than before. The algorithm is an information lossless transformation of the MULTEXT-EAST compliant lexical tags (MSD) into a reduced tagset that can be mapped back on the lexicon tagset fully deterministic. From the baseline tagsets, a corpus linguist, expert in the language in case, may further reduce the tagsets taking into account language distributional properties. As any further reduction of the baseline tagsets assumes losing information, adequate recovering rules should be designed for ensuring the final tagging in terms of lexicon encoding. The algorithm is described in details and the generated baseline tagsets for Czech, English, Estonian, Hungarian, Romanian and Slovenean are evaluated. They are much smaller and systematically ensures better tagging accuracy than the corresponding MSDs.
The paper describes the methodology and the tools we developed for the purpose of building a Romanian wordnet. The work is carried out within the BalkaNet European project and is concerned with wordnets for Bulgarian, Czech, Greek, Romanian, Serbian and Turkish all of them aligned via an interlingual index (ILI) to Princeton Wordnet. The wordnets structuring follows the principles adopted in EuroWordNet. In order to ensure maximal cross-lingual lexical coverage, the consortium decided to implement the same concepts, represented by a common set of ILI concepts. We describe the selection of concepts to be implemented in all the monolingual wordnets The methodologies adopted by each partner were different and they depended on the language resources and personnel available. For the Romanian wordnet,we decided that it should be based on the reference lexicographic descriptions of Romanian which we had in electronic forms: EXPD, a heavily XML annotated explanatory dictionary (developed in the previous CONCEDE project and based on the standard Explanatory Dictionary of Romanian), SYND, a published dictionary of synonyms which we keyboarded, encoded and completed with more than 4000 new synonymy sets extracted from EXPD, EnRoD, a Romanian-English dictionary, most part of it being extracted automatically from parallel corpora and further hand validated and extended. Besides these monolingual resources, as all the other members of the consortium, we had at our disposal the interlingual mapping of the Princeton Wordnet. All the above mentioned resources have been incorporated into a user-friendly system, WnBuilder, which allows for cooperative work of a large number of lexicographers. When the distributed work is put together, the synsets are validated. Several errors show up, the most frequent and difficult to solve being the case of a literal with the same sense number appearing in different synsets. We discuss reasons for such conflicts as well as their correction, supported by another utility program called WnCorrector. The full paper presents WnBuilder and WnCorrector, as well as the status of the Romanian wordnet development.
Parallel corpora are considered an important resource for the development of linguistic tools. In this paper our main goal is the development of a bilingual lexicon of verbs. The construction of this lexicon is possible using two main resources: I) a parallel corpus (through the alignment); II) the linguistic tools developed for Spanish (which serve as a starting point for developing tools for Arabic language). At the end, aligned equivalent verbs are detected automatically from a parallel corpus Spanish-Arabic. To achieve this goal, we had to pass through different preparatory stages concerning the assesment of the parallel corpus, the monolingual tokenization of each corpus, a preliminary sentence alignment and finally applying the model of automatic extraction of equivalent verbs. Our method is hybrid, since it combines both statistical and linguistic approaches.
This project combines linguistic and statistical information to develop a term extraction tool for Basque. Being Basque an agglutinative and highly inflected language, the treatment of morphosyntactic information is vital. In addition, due to late unification process of the language, texts present more elevated term dispersion than in a highly normalized language. The result is a semi-automatic terminology extraction tool based on XML, for its use in technical and scientific information managing.
For the present work, we introduce and evaluate a novel Bayesian syntactic shallow parser that is able to perform robust detection of pairs of subject-object and subject-direct object-indirect object for a given verb, in a natural language sentence. The shallow parser infers on the correct subject-object pairs based on knowledge provided by Bayesian network learning from annotated text corpora. The DELOS corpus, a collection of economic domain texts that has been automatically annotated using various morphological and syntactic tools was used as training material. Our shallow parser makes use of limited linguistic input. More specifically, we consider only part of speech tagging, the voice and the mood of the verb as well as the head word of a noun phrase. For the task of detecting the head word of a phrase we used a sentence boundary detector. Identifying the head word of a noun phrase, i.e. the word that holds the morphological information (case, number) of the whole phrase, also proves to be very helpful for our task as its morphological tag is all the information that is needed regarding the phrase. The evaluation of the proposed method was performed against three other machine learning techniques, namely naive Bayes, k-Nearest Neighbor and Support Vector Machines, methods that have been previously applied to natural language processing tasks with satisfactory results. The experimental outcomes portray a satisfactory performance of our proposed shallow parser, which reaches almost 92 per cent in terms of precision.
This paper presents some aspects of the first Portuguese frequency lexicon extracted from a corpus of large dimensions. The Multifunctional Computational Lexicon of Contemporary Portuguese (henceforth MCL) rised from the necessity of filling a gap existent in the studies of the contemporary Portuguese. Until recently, the frequency lexicons of Portuguese were of very small dimensions, such as Portugu{\^e}s Fundamental, which is constituted by 2.217 words extracted from a 700.000 word corpus and the Frequency Dictionary of Portuguese Words based on a literary corpus of 500.000 words. We describe here the main steps taken for collecting the lexical and frequency data and some of the major problems that arouse in the process. The resulting lexicon is a freely available reliable resource for several types of applications.
In the development of annotations for a spoken database, an important issue is whether the annotations can be generated automatically with sufficient precision, or whether expensive manual annotations are needed. In this paper, the case of prosodic annotations is discussed, which was investigated on the CGN database (Spoken Dutch Corpus). The main conclusions of this work are as follows. First, it was found that the available amount of manual prosodic annotations is sufficient for the development of our (baseline, decision tree based) prosodic models. In other words, more manual annotations do not improve the models. Second, the developed prosodic models for prominence are insufficiently accurate to produce automatic prominence annotations that are as good as the manual ones. But on the other hand the consistency between manual and automatic break annotations is as high as the inter-transcriber consistency for breaks. So given the current amount of manual break annotations, annotations for the remainder of the CGN database can be generated automatically with the same quality as the manual annotations.
Official travel warnings published regularly in the internet by the ministries for foreign affairs of France, Germany, and the UK provide a useful resource for assessing the risks associated with travelling to some countries. The shallow IE system SProUT has been extended to meet the specific needs of delivering a language-neutral output for English, French, or German input texts. A shared type hierarchy, a feature-enhanced gazetteer resource, and generic techniques of merging chunk analyses into larger results are major reusable results of this work.
People, when processing human-to-human communication, utilize everything they can in order to understand that communication, including speech and information such as the time and location of an interlocutor's gesture and gaze. Speech and gesture are known to exhibit a synchronous relationship in human communication; however, the precise nature of that relationship requires further investigation. The construction of computer models of multimodal human communication would be enabled by the availability of multimodal communication corpora annotated with synchronized gesture and speech features. To investigate the temporal relationships of these knowledge sources, we have collected and are annotating several multimodal corpora with time-aligned features. Forced alignment between a speech file and its transcription is a crucial part of multimodal corpus production. This paper investigates a number of factors that may contribute to highly accurate forced alignments to support the rapid production of these multimodal corpora including the acoustic model, the match between the speech used for training the system and that to be force aligned, the amount of data used to train the ASR system, the availability of speaker adaptation, and the duration of alignment segments.
The present study focuses on automatic processing of sibling resources of audio and written documents, such as available in audio archives or for parliament debates: written texts are close but not exact audio transcripts. Such resources deserve attention for several reasons: they represent an interesting testbed for studying differences between written and spoken material and they yield low cost resources for acoustic model training. When automatically transcribing the audio data, regions of agreement between automatic transcripts and written sources allow to transfer time-codes to the written documents: this may be helpful in an audio archive or audio information retrieval environment. Regions of disagreement can be automatically selected for further correction by human transcribers. This study makes use of 10 hours of French radio interview archives with corresponding press-oriented transcripts. The audio corpus has then been transcribed using the LIMSI speech recognizer resulting in automatic transcripts, exhibiting an average word error rate of 12{\%}. 80{\%} of the text corpus (with word chunks of at least five words) can be exactly aligned with the automatic transcripts of the audio data. The residual word error rate on these 80{\%} is less than 1{\%}.
In this paper we present the evaluation of a spoken phonetic corpus designed to train acoustic models for Speech Recognition applications in Basque Language. A complete set of acoustic-phonetic decoding experiments was carried out over the proposed database. Context dependent and independent phoneme units were used in these experiments with two different approaches to acoustic modeling, namely discrete and continuous Hidden Markov Models (HMMs). A complete set of HMMs were trained and tested with the database. Experimental results reveal that the database is large and phonetically rich enough to get great acoustic models to be integrated in Continuous Speech Recognition Systems.
We inspect the possibility of creating new linguistic utterances (small sentences) similar to those already present in an existing linguistic resource. Using paradigm tables ensures that the new generated sentences resemble previous data, while being of course different. We report an experiment in which 1,201 new correct sentences were generated starting from only 22 seed sentences.
This paper describes the infrastructure of a basic language resources set for Bulgarian in the context of BLARK initiative requirements. We focus on the treebanking task as a trigger for basic language resources compilation. Two strategies have been applied in this respect: (1) implementing the main pre-processing modules before the treebank compilation and (2) creating more elaborate types of resources in parallel to the treebank compilation. The description of language resources within BulTreeBank project is divided into two parts: language technology, which includes tokenization, morphosyntactic analyzer, morphosyntactic disambiguation, partial grammars, and language data, which includes the layers of the BulTreeBank corpus and the variety of lexicons. The advantages of our approach to a less-spoken language (like Bulgarian) are as follows: it triggers the creation of the basic set of language resources which lack for certain languages and it rises the question about the ways of language resources creation.
This paper deals with databases that combine different aspects: children's speech, emotional speech, human-robot communication, cross-linguistics, and read vs. spontaneous speech: in a Wizard-of-Oz scenario, German and English children had to instruct Sony's AIBO robot to fulfil specific tasks. In one experimental condition, strictly parallel for German and English, the AIBO behaved `disobedient' by following it's own script irrespective of the child's commands. By that, reactions of different children to the same sequence of AIBO's actions could be obtained. In addition, both the German and the English children were recorded reading texts. The data are transliterated orthographically; emotional user states and some other phenomena will be annotated. We report preliminary word recognition rates and classification results.
One of the major obstacles for knowledge management remains MultiWord Terminology (MWT). This paper explores the difficulties that arise and describes real world solutions implemented as part of the Parmenides project. Parmenides is being built as an integrated knowledge management package that combines information, MWT and ontology extraction methods in a semi-automated framework. The focus of this paper is on eliciting ontological fragments based on dedicated MWT processing.
The OPUS corpus is a growing collection of translated documents collected from the internet. The current version contains about 30 million words in 60 languages. The entire corpus is sentence aligned and it also contains linguistic markup for certain languages.
This work tries to enrich the Spanish Wordnet using a Spanish taxonomy as a knowledge source. The Spanish taxonomy is composed by Spanish senses, while Spanish Wordnet is composed by synsets, mostly linked to English WordNet. A set of weighted associations between Spanish words and Wordnet synsets is used for inferring associations between both taxonomies.
The goal of this project (LILA) is the collection of a large number of spoken databases for training Automatic Speech Recognition Systems for telephone applications in the Asian Pacific area. Specifications follow those of SpeechDat-like databases. Utterances will be recorded directly from calls made either from fixed or cellular telephones and are composed by read text and answers to specific questions. The project is driven by a consortium composed by a large number of industrial companies. Each company is in charge of the production of two databases. The consortium shares the databases produced in the project. The goal of the project should be reached within the year 2005.
When a text in any language is submitted to a morphological analysis, there always rest some unrecognized words. We can lower their number by adding new words into the dictionary used by the morphological analyzer but we can never gather the whole of the language. The system described in this paper (we call it ``derivation module'') deals with the unknown derived words. It aims not only at analyzing but also at synthesizing Czech derived words. Such a system is of particular value for automatic processing of languages where derivational morphology plays an important role in regular word formation.
This paper deals with the quality evaluation (validation) of Spoken Language Resources (SLR). The current situation in terms of relevant validation criteria and procedures is briefly presented. Next, a number of validation issues related to new data formats (XML-based annotations, UTF-16 encoding) are discussed. Further, new validation cycles that were introduced in a series of new projects like SpeeCon and OrienTel are addressed: prompt sheet validation, lexicon validation and pre-release validation. Finally, SPEX's current and future
We present a first approach to the application of a data mining technique, Multiple Sequence Alignment, to the systematization of a polemic aspect of discourse, namely, the expression of contrast, concession, counterargument and semantically similar discursive relations. The representation of the phenomena under study is carried out by very simple techniques, mostly pattern-matching, but the results allow to drive insightful conclusions on the organization of this aspect of discourse: equivalence classes of discourse markers are established, and systematic patterns are discovered, which will be applied in enhancing a discursive parser.
This paper proposes a methodology for obtaining sentences containing discourse markers from the World Wide Web. The proposed methodology is particularly suitable for collecting large numbers of discourse marker tokens. It relies on the automatic identification of discourse markers, and we show that this can be done with an accuracy within 9{\%} of that of human performance. We also show that the distribution of discourse markers on the web correlates highly with those in a conventional balanced corpus.
In this paper an interactive pattern extraction workbench, I*Pex, is presented. The workbench comes in a graphical environment and is designed to be used in an incremental and interactive fashion with the user. Patterns can be constructed to work in combination involving specifications on several linguistic levels simultaneously, from the character level using regular expressions, parts of speech and dependency relations to semantic roles. The input text format is based on XCES XML format.
The paper describes an annotation scheme for coreference developed within the application context of text-to-hypertext conversion. In this context coference is used (1) for generating document-internal and cross-document hyperlinks, and (2) for resolving anaphoric expressions in order to achieve cohesive closedness in hypertext nodes. We will argue that for the purpose of cross-document linking it is necessary to separate the annotation of coreference relations from the annotation of anaphoric relations. To account for this requirement, we developed a knowledge-based annotation scheme that relates referential expressions in the text to entities in a knowledge representation, which is modeled using XML Topic Maps.
In the automatic summarisation of written texts, direct speech is usually deemed unsuitable for inclusion in important sentences. This is due to the fact that humans do not usually include such quotations when they create summaries. In this paper, we argue that despite generally negative attitudes, direct speech can be useful for summarisation and ignoring it can result in the omission of important and relevant information. We present an analysis of a corpus of annotated newswire texts in which a substantial amount of speech is marked by different annotators, and describe when and why direct speech can be included in summaries. In an attempt to make direct speech more appropriate for summaries, we also describe rules currently being developed to transform it into a more summary-acceptable format.
This paper describes how Human Language Technologies and linguistic resources are used to support the construction of components of a knowledge organisation system. In particular we focus on methodologies and resources for building a corpus-based domain ontology and extracting relevant metadata information for text chunks from domain-specific corpora.
In the paper we describe development, means of evaluation and applications of Russian-English Sociopolitical Thesaurus specially developed as a linguistic resource for automatic text processing applications. The Sociopolitical domain is not a domain of social research but a broad domain of social relations including economic, political, military, cultural, sports and other subdomains. The knowledge of this domain is necessary for automatic text processing of such important documents as official documents, legislative acts, newspaper articles.
In the paper we describe our approach to development of ontologies with small number of relation types. Non-taxonomic relations in our ontologies are based on ontological dependence conception described in the formal ontology. This minimal relations set does not depend on a domain or a task and makes possible to begin the ontology construction at once, as soon as a task is set and a domain is determined, to receive the first version of an ontology in short time. Such an initial ontology can be used for information-retrieval applications and can serve as a structural basis for further development of the ontology
Several Language Resources (LRs) for Portuguese, developed at the Center of Linguistics of the Lisbon University (CLUL), are available on-line at CLUL's webpage: www.clul.ul.pt/english/sectores/projecto{\_}rld.html. These LRs have been extracted from or developed based on the Reference Corpus of Contemporary Portuguese (CRPC), a monitor corpus containing, at the present, more than 300 million words, taken by sampling from several types of written text (literary, newspaper, technical, didactic, juridical, parlamentary, etc.) and spoken text (informal and formal), pertaining to national and regional varieties of Portuguese (including European, Brazilian, African and Asian Portuguese). The LRs available for on-line queries include: a) several subcorpora (written and spoken, tagged and untagged) compiled and extracted from CRPC for specific CLUL's projects and now available for on-line queries; b) a published sample of ``Portugu{\^e}s Fundamental'', a spoken CRPC subcorpus, available for texts download; c) a frequency lexicon extracted from a CRPC subcorpus available for both on-line queries and download. Other RLs available for Portuguese are also referred: C-ORAL-ROM - Integrated Reference Corpora for Spoken Romance Languages, a CD-ROM edition of a spoken corpus with text-to-sound alignment; the LE-PAROLE corpus; the LE-PAROLE Lexicon and the SIMPLE Lexicon.
This article describes the use and development of a tool for grammar and terminology control (FLAG), for the purposes of automating the verification of terminology for a large-scale user of multilingual terminology. It describes the various advantages of the tool and shows a process for transforming a traditional terminology list into a list of inflected forms as well as patterns which can be used to find possible morpho-syntactic derivations of terms.
Some approaches to automatic terminology extraction from corpora imply the use of existing semantic resources for guiding the detection of terms. Most of these systems exploit specialised resources, like UMLS in the medical domain, while a few try to take profit from general-purpose semantic resources, like EuroWordNet (EWN). As the term extraction task is clearly domain depending, in the case a general-purpose resource without specific domain information is used, we need a way of attaching domain information to the units of the resource. For big resources it is desirable that this semantic enrichment could be carried out automatically. Given a specific domain, our proposal aims to detect in EWN those units that can be considered as domain markers (DM). We can define a DM as an EWN entry whose attached strings belong to the domain, as well as the variants of all its descendents through the hyponymy relation. The procedure we propose in this paper is fully automatic and, a priori, domain-independent. The only external knowledge it uses is a set of terms, which is an external vocabulary, which is considered to have at least one sense belonging to the domain.
This paper presents EASY (Evaluation of Analyzers of SYntax), an ongoing evaluation campaign of syntactic parsing of French, a subproject of EVALDA in the French TECHNOLANGUE program. After presenting the elaboration of the annotation formalism, we describe the corpus building steps, the annotation tools, the evaluation measures and finally, plans to produce a validated large linguistic resource, syntactically annotated
The annotation of the Prague Dependency Treebank (PDT) is conceived of as a multilayered scenario that comprises also dependency representations (tectogrammatical tree structures, TGTS's) of the underlying structure of the sentences. TGTS's capture three basic aspects of the underlying structure of sentences: (a) the dependency tree structure, (b) the kinds of dependency syntactic relations, and (c) the basic characteristics of the topic-focus articulation (TFA). Since the PDT is a large collection and the annotations on the deepest layer are to a large extent performed by several human annotators (based on an automatic preprocessing module), it is more than necessary to observe the consistence of annotators and the agreement among them. In the present paper, we summarize the results of the evaluation of parallel annotations of several samples taken from PDT and the measures accepted to improve the consistency of annotations.
Semantic lexical resources play an important part in both linguistic study and natural language engineering. In Lancaster, a large semantic lexical resource has been built over the past 14 years, which provides a knowledge base for the USAS semantic tagger. Capturing semantic lexicological theory and empirical lexical usage information extracted from corpora, the Lancaster semantic lexicon provides a valuable resource for the corpus research and NLP community. In this paper, we evaluate the lexical coverage of the semantic lexicon both in terms of genres and time periods. We conducted the evaluation on test corpora including the BNC sampler, the METER Corpus of law/court journalism reports and some corpora of Newsbooks, prose and fictional works published between 17th and 19th centuries. In the evaluation, the semantic lexicon achieved a lexical coverage of 98.49{\%} on the BNC sampler, 95.38{\%} on the METER Corpus and 92.76{\%} -- 97.29{\%} on the historical data. Our evaluation reveals that the Lancaster semantic lexicon has a remarkably high lexical coverage on modern English lexicon, but needs expansion with domain-specific terms and historical words. Our evaluation also shows that, in order to make claims about the lexical coverage of annotation systems as well as to render them {`}future proof{'}, we need to evaluate their potential both synchronically and diachronically across genres.
An unified language for the communicative acts between agents is essential for the design of multi-agents architectures. Whatever the type of interaction (linguistic, multimodal, including particular aspects such as force feedback), whatever the type of application (command dialogue, request dialogue, database querying), the concepts are common and we need a generic meta-model. In order to tend towards task-independent systems, we need to clarify the modules parameterization procedures. In this paper, we focus on the characteristics of a meta-model designed to represent meaning in linguistic and multimodal applications. This meta-model is called MMIL for MultiModal Interface Language, and has first been specified in the framework of the IST MIAMM European project. What we want to test here is how relevant is MMIL for a completely different context (a different task, a different interaction type, a different linguistic domain). We detail the exploitation of MMIL in the framework of the IST OZONE European project, and we draw the conclusions on the role of MMIL in the parameterization of task-independent dialogue managers.
This paper deals with the STO lexicon, the most comprehensive computational lexicon of Danish developed for NLP/HLT applications, which is now ready for use. Danish was one of the 12 EU-languages participating in the LE-PAROLE and SIMPLE projects; therefore it was obvious to continue this work building on our experience obtained from these projects. The material for Danish produced within these projects {--} further enriched with language-specific information - is incorporated into the STO lexicon. First, we describe the main characteristics of the lexical coverage and linguistic content of the STO lexicon; second, we present some recent uses and point to some prospective exploitations of the material. Finally, we outline an internet-based user interface, which allows for browsing through the complex information content of the STO lexical database and some other selected WRL{'}s for Danish.
A key element for the extraction of information in a natural language document is a set of shallow text analysis rules, which are typically based on pre-defined linguistic patterns. Current Information Extraction research aims at the automatic or semi-automatic acquisition of these rules. Within this research framework, we consider in this paper the potential for acquiring generic extraction patterns. Our research is based on the hypothesis that, terms (the linguistic representation of concepts in a specialised domain) and Named Entities (the names of persons, organisations and dates of importance in the text) can together be considered as the basic semantic entities of textual information and can therefore be used as a basis for the conceptual representation of domain specific texts and the definition of what constitutes an information extraction template in linguistic terms. The extraction patterns discovered by this approach involve significant associations of these semantic entities with verbs and they can subsequently be translated into the grammar formalism of choice.
The aim of the MEDIA project is to design and test a methodology for the evaluat ion of context-dependent and independent spoken dialogue systems. We propose an evaluation paradigm based on the use of test suites from real-world corpora and a common semantic representation and common metrics. This paradigm should allow us to diagnose the context-sensitive understanding capability of dialogue system s. This paradigm will be used within an evaluation campaign involving several si tes all of which will carry out the task of querying information from a database .
The C-ORAL-ROM project has delivered a multilingual corpus of spontaneous speech for the main romance languages (Italian, French, Portuguese and Spanish). The collection aims to represent the variety of speech acts performed in everyday language and to enable the description of prosodic and syntactic structures in the four romance languages. Sampling criteria are defined in a corpus design scheme. C-ORAL-ROM adopts two different sampling strategies, one for the formal and one for the informal part: While a set of typical domains of application is selected to document the formal use of language, the informal part documents speech variation using parameters referring to the event{'}s structure (dialogue vs. monologue) and the sociological domain of use (family-private vs public). The four romance corpora are tagged with respect to terminal and non terminal prosodic breaks. Terminal breaks are assumed to be the more relevant cues for the identification of relevant linguistic domains in spontaneous speech (utterances). Relations with other concurrent criteria are discussed. The multimedia storage of the C-ORAL-ROM corpus is based on this principle; each textual string ending with a terminal break is aligned, through the Win Pitch speech software, to its acoustic counterpart, generating the data base of all utterances.
We are working on a project called CAOS - Computer-Aided Ontology Structuring - whose aim is to develop a computer system designed to enable semi-automatic construction of concept systems, or ontologies. The system is intended to be interactive and presupposes an end-user with a terminological background (terminologist or professional translator). CAOS supports terminological concept modelling. The backbone of this concept modelling is constituted by characteristics modelled by formal feature specifications, i.e. attribute-value pairs. Our use of feature specifications is subject to a number of principles and constraints. In this paper we want to demonstrate some of these principles and to show why they are necessary in order to permit the construction of an interactive tool for building terminological ontologies. We will also show how they contribute to determine the structuring of the ontologies in CAOS and to facilitate the work of the terminologist user.
In the past, fundamental linguistic research was typically conducted on small data sets that were handcrafted for the specific research at hand. However, from the eighties onwards, many large spoken language corpora have become available. This study investigates the usefulness of large multi-purpose spoken language corpora for fundamental linguistic research. A research task was designed in which we tried to capture the major pronunciation differences between three speech styles in context-sensitive re-write rules at the phone level. These re-write rules were extracted from the alignments of both a manual phonetic transcription and an automatic phonetic transcription with a canonical reference transcription of the same material.
The West African Language Archive (WALA) initiative has emerged from a number of concurrent projects, and aims to encourage local scholars to create high quality decentralised repositories documenting West African languages, and to make these repositories available to language communities, language planners, educationalists and scientists via an internet metadata portal such as OLAC (Open Language Archive Community). A wide range of criteria has to be met in designing and implementing this kind of archive. We discuss these criteria with reference to experiences in documentation work in three very different ongoing language documentation projects, on designing an encyclopaedia, on documenting an endangered language, and on creating a speech synthesiser. We pay special attention to the provision of metadata, a formal variety of catalogue or housekeeping information, without which resources are doomed to remain inaccessible.
A novel thesaurus named a gword-sense association networkh is proposed for the first time. It consists of nodes representing word senses, each of which is defined as a set consisting of a word and its translation equivalents, and edges connecting topically associated word senses. This word-sense association network is produced from a bilingual dictionary and comparable corpora by means of a newly developed fully automatic method. The feasibility and effectiveness of the method were demonstrated experimentally by using the EDR English-Japanese dictionary together with Wall Street Journal and Nihon Keizai Shimbun corpora. The word-sense association networks were applied to word-sense disambiguation as well as to a query interface for information retrieval.
This paper reports on an ongoing project that uses varied language resources and advanced NLP tools for a linguistic classification task in discourse semantics. The system we present is designed to assign a ``situation entity'' class label to each predicator in English text. The project goal is to achieve the best-possible identification of situation entities in naturally-occurring written texts by implementing a robust system that will deal with real corpus material, rather than just with constructed textbook examples of discourse. In this paper we focus on the combination of multiple information sources, which we see as being vital for a robust classification system. We use a deep syntactic grammar of English to identify morphological, syntactic, and discourse clues, and we use various lexical databases for fine-grained semantic properties of the predicators. Experiments performed to date show that enhancing the output of the grammar with information from lexical resources improves recall but lowers precision in the situation entity classification task.
This paper introduces a tool {\textbackslash}Bonsai which supports human in annotating corpora with morphosyntactic information, and in retrieving syntactic structures stored in the database. Integrating annotation and retrieval enables users to annotate a new instance while looking back at the already annotated sentences which share the similar morphosyntactic structure. We focus on the retrieval part of the system, and describe a method to decompose a large input query into smaller ones in order to gain retrieval efficiency. The proposed method is evaluated with the Penn Treebank corpus, showing significant improvements.
We have already proposed a framework to represent a location in terms of both symbolic and numeric aspects. In order to deal with vague linguistic expressions of a location, the representation adopts a potential function mapping a location to its plausibility. This paper proposes classification of Japanese spatial nouns and potential functions corresponding to each class. We focused on a common Japanese spatial expression ``X no Y (Y of X)'' where X is a reference object and Y is a spatial noun. For example, ``tukue no migi (the right of the desk)'' denotes a location with reference to the desk. This expression were collected from corpora, and spatial nouns appearing in the Y position were classified into two major classes; designating a part of the reference object and designating a location apart from the reference object . And the latter class were further classified into two subclasses; direction-oriented and distance-oriented. For each class, a potential function were designed for providing meaning of spatial nouns.
We present an approach to the disambiguation of cluster labels that capitalizes on the notion of semantic similarity to assign WordNet senses to cluster labels. The approach provides interesting insights on how document clustering can provide the basis for developing a novel approach to word sense disambiguation.
This paper describes a novel clustering-based text summarization system that uses Multiple Sequence Alignment to improve the alignment of sentences within topic clusters. While most current clustering-based summarization systems base their summaries only on the common information contained in a collection of highly-related sentences, our system constructs more informative summaries that incorporate both the redundant and unique contributions of the sentences in the cluster. When evaluated using ROUGE, the summaries produced by our system represent a substantial improvement over the baseline, which is at 63{\%} of the human performance.
Multi-document summaries produced via sentence extraction often suffer from a number of cohesion problems, including dangling anaphora, sudden shifts in topic and incorrect or awkward chronological ordering. Therefore, the development of an automated revision process to correct such problems is a research area of current interest. We present the RevisionBank, a corpus of 240 extractive, multi-document summaries that have been manually revised to promote cohesion. The summaries were revised by six linguistic students using a constrained set of revision operations that we previously developed. In the current paper, we describe the process of developing a taxonomy of cohesion problems and corrective revision operators that address such problems, as well as an annotation schema for our corpus. Finally, we discuss how our taxonomy and corpus can be used for the study of revision-based multi-document summarization as well as for summary evaluation.
In this paper we discuss the five requirements for building large publicly available corpora which geared the construction of the L{\'a}cio-Web corpora and their environments: 1) a comprehensive text typology; 2) text copyright clearance, compilation and annotation scheme; 3) a friendly and didactic interface; 4) the need to serve as support for several types of research; 5) the need to offer an array of associated tools. Also, we present the features that make L{\'a}cio-Web corpora interesting and novel as well as the limitations of this project, such as corpora size and balance, and the non-inclusion of spoken texts in the project{'}s reference corpus.
Clusters of multiple news stories related to the same topic exhibit a number of interesting properties. For example, when documents have been published at various points in time or by different authors or news agencies, one finds many instances of paraphrasing, information overlap and even contradiction. The current paper presents the Cross-document Structure Theory (CST) Bank, a collection of multi-document clusters in which pairs of sentences from different documents have been annotated for cross-document structure theory relationships. We will describe how we built the corpus, including our method for reducing the number of sentence pairs to be annotated by our hired judges, using lexical similarity measures. Finally, we will describe how CST and the CST Bank can be applied to different research areas such as multi-document summarization.
This paper reports on a number of experiments in which we applied standard techniques from NLP in the context of documentation of endangered languages. We concentrated on the use of existing, freely available toolkits. Specifically, we explore the use of Finite-State Morphological Analysis, Maximum Entropy Part-of-Speech Tagging, and N-Gram Language Modeling.
When the relevance feedback, which is one of the most popular information retrieval model, is used in an information retrieval system, a related word is extracted based on the first retrival result. Then these words are added into the original query, and retrieval is performed again using updated query. Generally, Using such query expansion technique, retrieval performance using the query expansion falls in comparison with the performance using the original query. As the cause, there is a few synonyms in the thesaurus and although some synonyms are added to the query, the same documents are retireved as a result. In this paper, to solve the problem over such related words, we propose latent context relevance in consideration of the relevance between query and each index words in the document set.
This paper proposes a wide-range anaphora resolution system toward text understanding. This system resolves zero, direct and indirect anaphors in Japanese texts by integrating two sorts of linguistic resources: a hand-annotated corpus with various relations and automatically constructed case frames. The corpus has relevance tags which consist of predicate-argument relations, relations between nouns and coreferences, and is utilized for learning parameters of the system and testing it. The case frames are indispensable knowledge both for detecting zero/indirect anaphors and estimating appropriate antecedents. Our preliminary experiments showed promising results.
This paper presents a simple method for performing a lexical analysis of agglutinative languages like Korean, which have a heavy morphology. Especially, for nouns and adverbs with regular morphological modifications and/or high productivity, we do not need to artificially construct huge dictionaries of all inflected forms of lemmas. To construct a dictionary of lemmas and lexical transducers, first, we construct automatically a dictionary of all inflected forms from KAIST POS-Tagged Corpus. Secondly, we separate the party of lemmas and one of sequences of inflectional suffixes. Thirdly, we describe their lexical transducers (i.e., morphological rules) to recognize all inflected forms of lemmas for nouns and adverbs according to the combinatorial restrictions between lemmas and their inflectional suffixes. Finally, we evaluate the advantages of this method.
Style guides or writing recommendations play an important role in the field of technical documentation production, e.g. in industrial contexts. Also, writing recommendations are used in technical contexts together with machine translation (MT) in order to circumvent the MT system's weaknesses. This paper describes the evaluation and adaptation of a language checker deployed in the project int.unity In this project, both MT and a specialised language checker were adapted to the requirements of non-expert users and a non-technical domain. The language technology was integrated with the groupware platform BSCW to support the multi-lingual communication of geographically distributed teams concerned with trade union work. The users' languages were either German or English, i.e. the users were monolingual. We chose linguatec's server version of Personal Translator 2004 MT system for the German{\textless}-{\textgreater}English translations. The language checker CLAT for German and English has been developed at IAI. It is used by technical authors to support the production of high-quality technical documentation. The CLAT core system was adapted and extended in order to match the new requirements imposed by both the user profile and the subsequent MT application. In this paper, the focus will be on the assessment and adaptation of style rules for German.
We survey the evaluation methodology adopted in Information Extraction (IE), as defined in the MUC conferences and in later independent efforts applying machine learning to IE. We point out a number of problematic issues that may hamper the comparison between results obtained by different researchers. Some of them are common to other NLP tasks: e.g., the difficulty of exactly identifying the effects on performance of the data (sample selection and sample size), of the domain theory (features selected), and of algorithm parameter settings. Issues specific to IE evaluation include: how leniently to assess inexact identification of filler boundaries, the possibility of multiple fillers for a slot, and how the counting is performed. We argue that, when specifying an information extraction task, a number of characteristics should be clearly defined. However, in the papers only a few of them are usually explicitly specified. Our aim is to elaborate a clear and detailed experimental methodology and propose it to the IE community. The goal is to reach a widespread agreement on such proposal so that future IE evaluations will adopt the proposed methodology, making comparisons between algorithms fair and reliable. In order to achieve this goal, we will develop and make available to the community a set of tools and resources that incorporate a standardized IE methodology.
Metonymy is a creative process that establishes relationships based on contiguity or semantic relatedness between concepts. We outline a mechanism for deriving new concepts from WordNet using metonymy. We argue that by exploiting polysemy in WordNet we can take advantage of the metonymic relations between concepts. The focus of our metonymy generation work has been the creation of noun{\-} noun compounds that do not already exist in WordNet and which can be profitably added to WordNet. The mechanism of metonymy generation we outline takes a source compound and creates new compounds by exploiting the polysemy associated with hyponyms of the head of the source compound. We argue that metonymy generation is a sound basis for concept creation as the newly created compounds are semantically related to the source concept. We demonstrate that metonymy generation based on polysemy is superior to a method of metonymy generation that ignores polysemy. These new concepts can be used to augment WordNet.
This paper describes some important modifications to the Celex morphological database in the context of the FLaVoR project. FLaVoR aims to develop a novel modular framework for speech recognition, enabling the integration of complex linguistic knowledge sources, such as a morphological model. Morphology is a fairly unexploited linguistic information source speech recognizers could benefit from. This is especially true for languages which allow for a rich set of morphological operations, such as our target language Dutch. In this paper we focus on the exploitation of the Celex Dutch morphological database as the information source underlying two different morphological analyzers being developed within the project. Although the Celex database provides a valuable source of morphological information for Dutch, many modifications were necessary before it could be practically applied. We identify major problems, discuss the implemented solutions and finally experimentally evaluate the effect of our modifications to the database.
In order to investigate the effect of source language on translations, we investigate two variants of a Korean translation corpus. The first variant consists of Korean translations of 162,308 Japanese sentences from the ATR BTEC (Basic Expression Text Corpus). The second variant was made by translating the English translations of the Japanese sentences into Korean. We show that the source language text has a large influence on the target text. Even after normalizing orthographic differences, fewer than 8.3{\textbackslash}{\%} of the sentences in the two variants were identical. We describe in general which phenomena differ and then discuss how our analysis can be used in natural language processing.
We present a supervised method for training a sentence level confidence measure on translation output using a human-annotated corpus. We evaluate a variety of machine learning methods. The resultant measure, while trained on a very small dataset, correlates well with human judgments, and proves to be effective on one task based evaluation. Although the experiments have only been run on one MT system, we believe the nature of the features gathered are general enough that the approach will also work well on other systems.
The aim of this paper is to discuss aspects of an on-going project on the development of grammatical and lexical resources for Zulu with sufficient coverage for unrestricted text. We explain how the basic software tools of computational morphology are used in linguistic processing, more specifically for automatic word form recognition and morphological tagging of the growing stock of electronic text corpora of a Bantu language such as Zulu. It is also shown how a machine-readable lexicon is in turn enhanced with the information acquired and extracted by means of such corpus analysis.
The purpose of this paper is to introduce an alternative word association measure aimed at addressing the under-extraction collocations that contain high frequency words. While measures such as MI provide the important contribution of filtering out sheer high frequency of words in the detection of collocations in large corpora, one side effect of this filtering is that it becomes correspondingly difficult for such measures to detect true collocations involving high frequency words. As an alternative, we propose normalizing the MI measure by dividing the frequency of a candidate lexeme by the number of senses of that lexeme. We premise this alternative approach on the one sense per collocation assumption of Yarowsky (1992; 1995). Ten verb-noun collocations involving three high frequency verbs (make, take, run) are used to compare the extraction results of traditional MI and the proposed normalized MI. Results show the ranking of these high-frequency verbs as candidate collocates with the target focal nouns is raised by normalizing MI as proposed. Side effects of these improved rankings are discussed, such as increase in false positives resulting from higher recall. It is found that overall rank precision remains quite stable even with the increased recall of normalized MI.
In this paper, we propose an annotation scheme which can be used not only for annotating coreference relations between linguistic expressions, but also those among linguistic expressions and images, in scientific texts such as biomedical articles. Images in biomedical domain often contain important information for analyses and diagnoses, and we consider that linking images to textual descriptions of their semantic contents in terms of coreference relations is useful for multimodal access to the information. We present our annotation scheme and the concept of a ``coreference pool,'' which plays a central role in the scheme. We also introduce a support tool for text annotation named Open Ontology Forge which we have already developed, and additional functions for the software to cover image annotations (ImageOF) which is now being developed.
The development of the evaluation of domain-specific cross-language information retrieval (CLIR) is shown in the context of the Cross-Language Evaluation Forum (CLEF) campaigns from 2000 to 2003. The pre-conditions and the usable data and additionally available instruments are described. The main goals of this task of CLEF are to allow the evaluation of Cross-Language Information Retrieval (CLIR) systems in the context of structured data and in a domain-specific area (not in the more general context of floating, journalistic texts), and with the additional possibility to make use of thesauri which had been used for intellectual indexing of the documents and are provided with the data. The parallel German-English GIRT4 corpus is described and some of the results of the CLEF 2004 campaign are discussed.
This paper shows on the basis of a corpus study how a model of the context should be structured for the generation of coreferring descriptions in French. We show that this way of structuring the context can help to generate more paraphrases and a particular kind of referring expressions used to add information about the referent.
An innovative way of integrating Translation Memory (TM) and Machine Translation (MT) processing is presented which goes beyond the traditional ``cascade'' integration of Translation Memory and Machine Translation. The new method aims to automatically post-edit TM similar matches by the use of an MT module thus enhancing the TM fuzzy (similar) scores as well as enabling the utilisation of low-score TM fuzzy matches. This leads to substantial translation cost reduction. The suggested method, which can be classified as an Example-Based Machine Translation application, is analysed and examples are provided for clarification. It is evaluated through test results that involve human interaction. The method has been implemented within the ESTeam Translator (ET) Language Toolbox and is already in use in the various commercial installations of ET.
After the successful completion of the Spoken Dutch Corpus (1998 -- 2003) the time is ripe to take some time to sit back and reflect on our achievements and the procedures underlying them in order to learn from our experiences. In this paper we will in particular pay attention to issues affecting the levels of linguistic annotation, but some more general issues deserve to be treated as well (bug reporting, consistency). We will try to come up with solutions, but sometimes we want to invite further discussion from other researchers.
Highly inflectional/agglutinative languages like Hungarian typically feature possible word forms in such a magnitude that automatic methods that provide morphosyntactic annotation on the basis of some training corpus often face the problem of data sparseness. A possible solution to this problem is to apply a comprehensive morphological analyser, which is able to analyse almost all wordforms alleviating the problem of unseen tokens. However, although in a smaller number, there will still remain forms which are unknown even to the morphological analyzer and should be handled by some guesser mechanism. The paper will describe a hybrid method which combines symbolic and statistical information to provide lemmatization and suffix analyses for unknown word forms. Evaluation is carried out with respect to the induction of possible analyses and their respective lexical probabilities for unknown word forms in a part-of-speech tagging system.
We applied data-driven methods to carry out automatic acquisition of Dutch prepositional support verb constructions (SVCs) in corpora (e.g., iets in de gaten houden (``keep an eye on something'')). This paper addresses the question whether linguistic diagnostics help to discard noise from the nbest lists and how to (semi-)automatically apply such linguistic diagnostics to parsed corpora. We show that some of the linguistic diagnostics proposed in Hollebrandse (1993) effectively identify SVCs and contribute a modest error rate decrease.
A majority of Machine Aided Translation systems are based on comparisons between a source sentence and reference sentences stored in Translation Memories (TMs). The translation search is done by looking for sentences in a database which are similar to the source sentence. TMs have two basic limitations: the dependency on the repetition of complete sentences and the high cost of building a TM. As human translators do not only remember sentences from their preceding translations, but they also decompose the sentence to be translated and work with smaller units, it would be desirable to enrich the TM database with smaller translation units. This enrichment should also be automatic in order not to increase the cost of building a TM. We propose the application of two automatic bilingual segmentation techniques based on statistical translation methods in order to create new, shorter bilingual segments to be included in a TM database. An evaluation of the two techniques is carried out for a bilingual Basque-Spanish task.
This paper reflects on the recently completed African Speech Technology (AST) Project. The AST Project successfully developed eleven annotated telephone speech databases for five languages spoken in South Africa i.e. Xhosa, Southern Sotho, Zulu, English and Afrikaans. These databases were used to train and test speech recognition systems applied in a multilingual telephone-based prototype hotel booking system. An overview is given of the database design and contents. The acquisition of the data is discussed with regards to the telephony interface, as well as speaker recruitment and briefing. Particular reference is given to some of the practical implications of acquiring appropriate data in under-developed communities. Database management processes such as transcription, quality control and validation are explained. This is followed by information on the development of the prototype. Results of usability tests are discussed followed by an assessment of the Project as a whole.
The CGN corpus (Corpus Gesproken Nederlands/Corpus Spoken Dutch) is a large speech corpus of contemporary Dutch as spoken in Belgium (3.3 million words) and in the Netherlands (5.6 million words). Due to its size, manual phonemic annotation was limited to 10{\%} of the data and automatic systems were used to complement this data. This paper describes the automatic generation of the phonemic annotations and the corresponding segmentations. First, we detail the processes used to generate possible pronunciations for each sentence and to select to most likely one. Next, we identify the remaining difficulties when handling the CGN data and explain how we solved them. We conclude with an evaluation of the quality of the resulting transcriptions and segmentations.
In this paper, we investigate whether a dataset derived from a multi-purpose corpus such as the Spoken Dutch Corpus may be considered appropriate for developing a taxonomy of wh-questions, and a model of the way in which these questions are integrated in spoken discourse. We compare the results obtained from the Spoken Dutch Corpus with a similar analysis of a large random collection of FAQs from the internet. We find substantial differences between the questions in spoken discourse and FAQs. Therefore, it may not be trivial to use a general purpose corpus as a starting point for developing models for human-computer interaction.
For many years the Istituto di Teoria e Tecniche dell'Informazione Giuridica (ITTIG) of the Consiglio Nazionale delle Ricerche has studied the evolution of legal language, creating databases for documentation and digital retrieval of law texts. The ITTIG is attending to document legal language through information technology in order to provide as wide an access as possible to its findings. The Institute has recently created an on-line digital database that includes the full text of the most important Italian laws (Codes and Constitutions) from the 16th to the 20th century. The ITTIG is also in the process of preparing another database made up of contexts from the original 10th to the 20th century legal sources.
An architecture is presented that provides an integrated framework for managing, archiving and accessing language resources. This architecture was discussed in the DELAMAN network {--} a world-wide network of archives holding material about endangered languages. Such a framework will be built upon a metadata infrastructure, a mechanism to resolve unique resource identifiers, user and access rights management components. These components are closely related and have to be based on redundant and distributed services. For all these components existing middleware seems to be available, however, it has to be checked how they can interact with each other.
This paper presents specifications and requirements for creation and validation of large lexica that are needed in automatic Speech Recognition (ASR), Text-to-Speech (TTS) and statistical Speech-to-Speech Translation (SST) systems. The prepared language resources are created and validated within the scope of the EU-project LC-STAR (Lexica and Corpora for Speech-to-Speech Translation Components) during years 2002-2005. Large lexica consisting of phonetic, suprasegmental and morpho-syntactic content will be provided with well-documented specifications for 13 languages. A short summary of the LC-STAR project itself is presented. Overview about the specification for the corpora collection and word extraction as well as the specification and format of the lexica are presented. Particular attention is paid to the validation of the produced lexica and the lessons learnt during pre-validation. The created and validated language resources will be available via ELRA/ELDA.
This paper presents experiments for enlarging the Croatian Morphological Lexicon by applying an automatic acquisition methodology. The basic sources of information for the system are a set of morphological rules and a raw corpus. The morphological rules have been automatically derived from the existing Croatian Morphological Lexicon and we have used in our experiments a subset of the Croatian National Corpus. The methodology has proved to be efficient for those languages that, like Croatian, present a rich and mainly concatenative morphology. This method can be applied for the creation of new resources, as well as in the enrichment of existing ones. We also present an extension of the system that uses automatic querying to Internet to acquire those entries for which we have not enough information in our corpus.
Richard Power† University of Brighton  This article describes an implemented system which uses centering theory for planning of coherent texts and choice of referring expressions. We argue that text and sentence planning need to be driven in part by the goal of maintaining referential continuity and thereby facilitating pronoun resolution: Obtaining a favorable ordering of clauses, and of arguments within clauses, is likely to increase opportunities for nonambiguous pronoun use. Centering theory provides the basis for such an integrated approach. Generating coherent texts according to centering theory is treated as a constraint satisfaction problem. The well-known Rule 2 of centering theory is reformulated in terms of a set of constraints—cohesion, salience, cheapness, and continuity—and we show sample outputs obtained under a particular weighting of these constraints. This framework facilitates detailed research into evaluation metrics and will therefore provide a productive research tool in addition to the immediate practical beneﬁt of improving the ﬂuency and readability of generated texts. The technique is generally applicable to natural language generation systems, which perform hierarchical text structuring based on a theory of coherence relations with certain additional assumptions.  1. Overview  A central task for natural language generation (NLG) systems is to produce text that is coherent, in the sense in which (1a) is noticeably more coherent than (1b):  1. a. b.  Elixir is a white cream. It is used in the treatment of cold sores. It contains aliprosan. Aliprosan relieves viral skin disorders. Elixir contains aliprosan. Viral skin disorders are relieved by aliprosan. Elixir is used in the treatment of cold sores. It is a white cream.  We can observe various ways in which text organization inﬂuences coherence: the sequence in which certain facts are presented, the order in which entities are mentioned in a clause, and the possibilities available for identifying the intended reference of pronouns. Generally, (1a) seems to conform better to a reader’s expectations of what will be referred to next and of how to resolve underspeciﬁed referring expressions,  ∗ Department of Computing, Goldsmiths College, University of London, London SE14 6NW, U. K. E-mail: r.kibble@gold.ac.uk † Information Technology Research Institute, University of Brighton, Brighton BN2 4GJ, U. K. E-mail: Richard.Power@itri.brighton.ac.uk Submission received: 17 October 2002; Revised submission received: 22 May 2004; Accepted for publication: 6 August 2004 c 2004 Association for Computational Linguistics  Computational Linguistics  Volume 30, Number 4  in particular pronouns. These are issues which the well-known centering theory (CT) of Grosz, Joshi, and Weinstein (1995; henceforth GJW) is concerned with. Previous algorithms for pronominalization such as those of McCoy and Strube (1999), Henschel, Cheng, and Poesio (2000), and Callaway and Lester (2002) have addressed the task of deciding whether to realize an entity as a pronoun on the basis of given factors such as its syntactic role and discourse history within a given text structure; what is essentially novel in our approach is that we treat referential coherence as a planning problem, on the assumption that obtaining a favorable ordering of clauses, and of arguments within clauses, is likely to increase opportunities for nonambiguous pronoun use. Centering theory provides the basis for such an integrated approach.1 Of course coherence of a text depends on the realization of rhetorical relations (Mann and Thompson 1987) as well as referential continuity, and the latter is to an extent a byproduct of the former, as clauses that are rhetorically related also tend to mention the same entities. However, even when a set of facts is arranged in a hierarchical RST structure, there are still many possible linear orderings with noticeable differences in referential coherence. This article concentrates on the inﬂuence of referential continuity on overall coherence and describes a method for applying CT to problems in text planning and pronominalization in order to improve the ﬂuency and readability of generated texts. This method is applicable in principle to any system which produces hierarchically structured text plans using a theory of coherence relations, with the following additional assumptions: • There is a one-to-one correspondence between predicates and verbs, so that the options for syntactic realization can be predicted from the argument structure of predicates. Such “shallow” lexicalization appears to be standard in applied NLG systems (Cahill 1999). • Pronominalization is deferred until grammatical relations and word order have been determined. Our exposition will refer to an implemented document generation system, Iconoclast, which uses the technique of constraint satisfaction (van Hentenryck 1989; Power 2000; Power, Scott, and Bouayad-Agha 2003) with CT principles implemented among a set of soft constraints. The Iconoclast system allows the user to specify content and rhetorical structure through an interactive knowledge-base editor and supports ﬁne-grained control over stylistic and layout features. The user-determined rhetorical structure is transformed into a text structure or a set of candidate text structures which respect various text formation rules encoded as hard constraints. Not all of the resulting text structures will give rise to stylistically acceptable documents, and of those which may be judged acceptable, some will be noticeably preferable to others. The text-structuring phase is followed by an evaluation of the candidate structures in which they are ranked according to a set of preferences encoded as soft constraints. Centering preferences are weighted along with other stylistic constraints to ﬁx the preferred ﬁnal ordering both of propositions in the text and of arguments within a clause. It is not our primary aim in this short article to provide an empirical assessment of the claims of CT, for which we refer the reader to the relevant papers, such as  
Hermann Ney† RWTH Aachen  A phrase-based statistical machine translation approach — the alignment template approach — is described. This translation approach allows for general many-to-many relations between words. Thereby, the context of words is taken into account in the translation model, and local changes in word order from source to target language can be learned explicitly. The model is described using a log-linear modeling approach, which is a generalization of the often used source–channel approach. Thereby, the model is easier to extend than classical statistical machine translation systems. We describe in detail the process for learning phrasal translations, the feature functions used, and the search algorithm. The evaluation of this approach is performed on three different tasks. For the German–English speech Verbmobil task, we analyze the effect of various system components. On the French–English Canadian Hansards task, the alignment template system obtains signiﬁcantly better results than a single-word-based translation model. In the Chinese–English 2002 National Institute of Standards and Technology (NIST) machine translation evaluation it yields statistically signiﬁcantly better NIST scores than all competing research and commercial translation systems. 1. Introduction Machine translation (MT) is a hard problem, because natural languages are highly complex, many words have various meanings and different possible translations, sentences might have various readings, and the relationships between linguistic entities are often vague. In addition, it is sometimes necessary to take world knowledge into account. The number of relevant dependencies is much too large and those dependencies are too complex to take them all into account in a machine translation system. Given these boundary conditions, a machine translation system has to make decisions (produce translations) given incomplete knowledge. In such a case, a principled approach to solving that problem is to use the concepts of statistical decision theory to try to make optimal decisions given incomplete knowledge. This is the goal of statistical machine translation. The use of statistical techniques in machine translation has led to dramatic improvements in the quality of research systems in recent years. For example, the statistical approaches of the Verbmobil evaluations (Wahlster 2000) or the U.S. National ∗ 1600 Amphitheatre Parkway, Mountain View, CA 94043. E-mail: och@google.com. † Lehrstuhl fu¨ r Informatik VI, Computer Science Department, RWTH Aachen–University of Technology, Ahornstr. 55, 52056 Aachen, Germany. E-mail: ney@cs.rwth-aachen.de. Submission received: 19 November 2002; Revised submission received: 7 October 2003; Accepted for publication: 1 June 2004 c 2004 Association for Computational Linguistics  Computational Linguistics  Volume 30, Number 4  Institute of Standards and Technology (NIST)/TIDES MT evaluations 2001 through 20031 obtain the best results. In addition, the ﬁeld of statistical machine translation is rapidly progressing, and the quality of systems is getting better and better. An important factor in these improvements is deﬁnitely the availability of large amounts of data for training statistical models. Yet the modeling, training, and search methods have also improved since the ﬁeld of statistical machine translation was pioneered by IBM in the late 1980s and early 1990s (Brown et al. 1990; Brown et al. 1993; Berger et al. 1994). This article focuses on an important improvement, namely, the use of (generalized) phrases instead of just single words as the core elements of the statistical translation model. We describe in Section 2 the basics of our statistical translation model. We suggest the use of a log-linear model to incorporate the various knowledge sources into an overall translation system and to perform discriminative training of the free model parameters. This approach can be seen as a generalization of the originally suggested source–channel modeling framework for statistical machine translation. In Section 3, we describe the statistical alignment models used to obtain a word alignment and techniques for learning phrase translations from word alignments. Here, the term phrase just refers to a consecutive sequence of words occurring in text and has to be distinguished from the use of the term in a linguistic sense. The learned bilingual phrases are not constrained by linguistic phrase boundaries. Compared to the word-based statistical translation models in Brown et al. (1993), this model is based on a (statistical) phrase lexicon instead of a single-word-based lexicon. Looking at the results of the recent machine translation evaluations, this approach seems currently to give the best results, and an increasing number of researchers are working on different methods for learning phrase translation lexica for machine translation purposes (Marcu and Wong 2002; Venugopal, Vogel, and Waibel 2003; Tillmann 2003; Koehn, Och, and Marcu 2003). Our approach to learning a phrase translation lexicon works in two stages: In the ﬁrst stage, we compute an alignment between words, and in the second stage, we extract the aligned phrase pairs. In our machine translation system, we then use generalized versions of these phrases, called alignment templates, that also include the word alignment and use word classes instead of the words themselves. In Section 4, we describe the various components of the statistical translation model. The backbone of the translation model is the alignment template feature function, which requires that a translation of a new sentence be composed of a set of alignment templates that covers the source sentence and the produced translation. Other feature functions score the well-formedness of the produced target language sentence (i.e., language model feature functions), the number of produced words, or the order of the alignment templates. Note that all components of our statistical machine translation model are purely data-driven and that there is no need for linguistically annotated corpora. This is an important advantage compared to syntax-based translation models (Yamada and Knight 2001; Gildea 2003; Charniak, Knight, and Yamada 2003) that require a parser for source or target language. In Section 5, we describe in detail our search algorithm and discuss an efﬁcient implementation. We use a dynamic-programming-based beam search algorithm that allows a trade-off between efﬁciency and quality. We also discuss the use of heuristic functions to reduce the number of search errors for a ﬁxed beam size. In Section 6, we describe various results obtained on different tasks. For the German–English Verbmobil task, we analyze the effect of various system compo-  
Klaus U. Schulz† University of Munich  The need to correct garbled strings arises in many areas of natural language processing. If a dictionary is available that covers all possible input tokens, a natural set of candidates for correcting an erroneous input P is the set of all words in the dictionary for which the Levenshtein distance to P does not exceed a given (small) bound k. In this article we describe methods for efﬁciently selecting such candidate sets. After introducing as a starting point a basic correction method based on the concept of a “universal Levenshtein automaton,” we show how two ﬁltering methods known from the ﬁeld of approximate text search can be used to improve the basic procedure in a signiﬁcant way. The ﬁrst method, which uses standard dictionaries plus dictionaries with reversed words, leads to very short correction times for most classes of input strings. Our evaluation results demonstrate that correction times for ﬁxed-distance bounds depend on the expected number of correction candidates, which decreases for longer input words. Similarly the choice of an optimal ﬁltering method depends on the length of the input words. 1. Introduction In this article, we face a situation in which we receive some input in the form of strings that may be garbled. A dictionary that is assumed to contain all possible correct input strings is at our disposal. The dictionary is used to check whether a given input is correct. If it is not, we would like to select the most plausible correction candidates from the dictionary. We are primarily interested in applications in the area of natural language processing in which the background dictionary is very large and fast selection of an appropriate set of correction candidates is important. By a “dictionary,” we mean any regular (ﬁnite or inﬁnite) set of strings. Some possible concrete application scenarios are the following: • The dictionary describes the set of words of a highly inﬂectional or agglutinating language (e.g., Russian, German, Turkish, Finnish, Hungarian) or a language with compound nouns (German). The dictionary is used by an automated or interactive spelling checker. • The dictionary is multilingual and describes the set of all words of a family of languages. It is used in a system for postcorrection of results of OCR in which scanned texts have a multilingual vocabulary. ∗ Linguistic Modelling Department, Institute for Parallel Processing, Bulgarian Academy of Sciences, 25A, Akad. G. Bonchev Str., 1113 Soﬁa, Bulgaria. E-mail: stoyan@lml.bas.bg † Centrum fu¨ r Informations-und Sprachverarbeitung, Ludwig-Maximilians-Universita¨t-Mu¨ nchen, Oettingenstr. 67, 80538 Munchen, Germany. E-mail: schulz@cis.uni-muenchen.de Submission received: 12 July 2003; Revised submission received: 28 February 2004; Accepted for publication: 25 March 2004 c 2004 Association for Computational Linguistics  Computational Linguistics  Volume 30, Number 4  • The dictionary describes the set of all indexed words and phrases of an Internet search engine. It is used to determine the plausibility that a new query is correct and to suggest “repaired” queries when the answer set returned is empty. • The input is a query to some bibliographic search engine. The dictionary contains titles of articles, books, etc. The selection of an appropriate set of correction candidates for a garbled input P is often based on two steps. First, all entries W of the dictionary are selected for which the distance between P and W does not exceed a given bound k. Popular distance measures are the Levenshtein distance (Levenshtein 1966; Wagner and Fischer 1974; Owolabi and McGregor 1988; Weigel, Baumann, and Rohrschneider 1995; Seni, Kripasundar, and Srihari 1996; Oommen and Loke 1997) or n-gram distances (Angell, Freund, and Willett 1983; Owolabi and McGregor 1988; Ukkonen 1992; Kim and Shawe-Taylor 1992, 1994) Second, statistical data, such as frequency information, may be used to compute a ranking of the correction candidates. In this article, we ignore the ranking problem and concentrate on the ﬁrst step. For selection of correction candidates we use the standard Levenshtein distance (Levenshtein 1966). In most of the above-mentioned applications, the number of correction candidates becomes huge for large values of k. Hence small bounds are more realistic. In light of this background, the algorithmic problem discussed in the article can be described as follows: Given a pattern P, a dictionary D, and a small bound k, efﬁciently compute the set of all entries W in D such that the Levenshtein distance between P and W does not exceed k. We describe a basic method and two reﬁnements for solving this problem. The basic method depends on the new concept of a universal deterministic Levenshtein automaton of ﬁxed degree k. The automaton of degree k may be used to decide, for arbitrary words U and V, whether the Levenshtein distance between U and V does not exceed k. The automaton is “universal” in the sense that it does not depend on U and V. The input of the automaton is a sequence of bitvectors computed from U and V. Though universal Levenshtein automata have not been discussed previously in the literature, determining Levenshtein neighborhood using universal Levenshtein automata is closely related to a more complex table-based method described by the authors Schulz and Mihov (2002). Hence the main advantage of the new notion is its conceptual simplicity. In order to use the automaton for solving the above problem, we assume that the dictionary is given as a determininistic ﬁnite-state automaton. The basic method may then be described as a parallel backtracking traversal of the universal Levenshtein automaton and the dictionary automaton. Backtracking procedures of this form are well-known and have been used previously: for example, by Oﬂazer (1996) and the authors Schulz and Mihov (2002). For the ﬁrst reﬁnement of the basic method, a ﬁltering method used in the ﬁeld of approximate text search is adapted to the problem of approximate search in a dictionary. In this approach, an additional “backwards” dictionary D−R (representing the set of all reverses of the words of a given dictionary D) is used to reduce approximate search in D with a given bound k ≥ 1 to related search problems for smaller bounds k < k in D and D−R. As for the basic method, universal Levenshtein automata are used to control the search. Ignoring very short input words and correction bound k = 1,  452  Mihov and Schulz  Fast Approximate Search in Large Dictionaries  this approach leads to a drastic increase in speed. Hence the “backwards dictionary method” can be considered the central contribution of this article. The second reﬁnement, which is only interesting for bound k = 1 and short input words, also uses a ﬁltering method from the ﬁeld of approximate text search (Muth and Manber 1996; Mor and Fraenkel 1981). In this approach, “dictionaries with single deletions” are used to reduce approximate search in a dictionary D with bound k = 1 to a conventional lookup technique for ﬁnite-state transducers. Dictionaries with single deletions are constructed by deleting the symbol at a ﬁxed position n in all words of a given dictionary. For the basic method and the two reﬁnements, detailed evaluation results are given for three dictionaries that differ in terms of the number and average length of entries: a dictionary of the Bulgarian language with 965,339 entries (average length 10.23 symbols), a dictionary of German with 3,871,605 entries (dominated by compound nouns, average length 18.74 symbols), and a dictionary representing a collection of 1,200,073 book titles (average length 47.64 symbols). Tests were restricted to distance bounds k = 1, 2, 3. For the approach based on backwards dictionaries, the average correction time for a given input word—including the displaying of all correction suggestions— is between a few microseconds and a few milliseconds, depending on the dictionary, the length of the input word, and the bound k. Correction times over one millisecond occur only in a few cases for bound k = 3 and short input words. For bound k = 1, which is important for practical applications, average correction times did not exceed 40 microseconds. As a matter of fact, correction times are a joint result of hardware improvements and algorithmic solutions. In order to judge the quality of the correction procedure in absolute terms, we introduce an “idealized” correction algorithm in which any kind of blind search and superﬂuous backtracking is eliminated. Based on an analysis of this algorithm, we believe that using purely algorithmic improvements, our correction times can be improved only by a factor of 50–250, depending on the kind of dictionary used. This factor represents a theoretical limit in the sense that the idealized algorithm probably cannot be realized in practice. This article is structured as follows. In Section 2, we collect some formal preliminaries. In Section 3, we brieﬂy summarize some known techniques from approximate string search in a text. In Section 4, we introduce universal deterministic Levenshtein automata of degree k and describe how the problem of deciding whether the Levenshtein distance between two strings P and W does not exceed k can be efﬁciently solved using this automaton. Since the method is closely related to a table-based approach introduced by the authors (Schulz and Mihov 2002), most of the formal details have been omitted. Sections 5, 6, and 7 describe, respectively, the basic method, the reﬁned approach based on backwards dictionaries, and the approach based on dictionaries with single deletions. Evaluation results are given for the three dictionaries mentioned above. In Section 8 we brieﬂy comment on the difﬁculties that we encountered when trying to combine dictionary automata and similarity keys (Davidson 1962; Angell, Freund, and Willett 1983; Owolabi and McGregor 1988; Sinha 1990; Kukich 1992; Anigbogu and Belaid 1995; Zobel and Dart 1995; de Bertrand de Beuvron and Trigano 1995). Theoretical bounds for correction times are discussed in Section 9. The problem considered in this article is well-studied. Since the number of contributions is enormous, a complete review of related work cannot be given here. Relevant references with an emphasis on spell-checking and OCR correction are Blair (1960), Riseman and Ehrich (1971), Ullman (1977), Angell, Freund, and Willett (1983), Srihari, Hull, and Choudhari (1983), Srihari (1985), Takahashi et al. (1990), Kukich (1992), Zobel  453  Computational Linguistics  Volume 30, Number 4  and Dart (1995), and Dengel et al. (1997). Exact or approximate search in a dictionary is discussed, for example, in Wells et al. (1990), Sinha (1990), Bunke (1993), Oﬂazer (1996), and Baeza-Yates and Navarro (1998). Some relevant work from approximate search in texts is described in Section 3. 2. Formal Preliminaries We assume that the reader is familiar with the basic notions of formal language theory as described, for example, by Hopcroft and Ullman (1979) or Kozen (1997). As usual, ﬁnite-state automata (FSA) are treated as tuples of the form A = Σ, Q, q0, F, ∆ , where Σ is the input alphabet, Q is the set of states, q0 ∈ Q is the initial state, F is the set of ﬁnal states, and ∆ ⊆ Q × Σε × Q is the transition relation. Here ε denotes the empty string and Σε := Σ ∪ {ε}. The generalized transition relation ∆ˆ is deﬁned as the smallest subset of Q × Σ∗ × Q with the following closure properties:  • For all q ∈ Q we have (q, ε, q) ∈ ∆ˆ . • For all q1, q2, q3 ∈ Q and W1, W2 ∈ Σ∗, if (q1, W1, q2) ∈ ∆ˆ and (q2, W2, q3) ∈ ∆, then also (q1, W1W2, q3) ∈ ∆ˆ . We write L(A) for the language accepted by A. We have L(A) = {W ∈ Σ∗ | ∃q ∈ F : (q0, W, q) ∈ ∆ˆ }. Given A as above, the set of active states for input W ∈ Σ∗ is {q ∈ Q | (q0, W, q) ∈ ∆ˆ }. A ﬁnite-state automaton A is deterministic if the transition relation is a function δ : Q × Σ → Q. Let A = Σ, Q, q0, F, δ be a deterministic FSA, and let δ∗ : Q × Σ∗ → Q denote the generalized transition function, which is deﬁned in the usual way. For q ∈ Q, we write LA(q) := {U ∈ Σ∗ | δ∗(q, U) ∈ F} for the language of all words that lead from q to a ﬁnal state. The length of a word W is denoted by |W|. Regular languages over Σ are deﬁned in the usual way. With L1 · L2 we denote the concatenation of the languages L1 and L2. It is well-known that for any regular language L, there exists a deterministic FSA AL such that L(A) = L and AL is minimal (with respect to number of states) among all deterministic FSA accepting L. AL is unique up to renaming of states. A p-subsequential transducer is a tuple T = Σ, Π, Q, q0, F, δ, λ, Ψ , where  • Σ, Q, q0, F, δ is a deterministic ﬁnite-state automaton; • Π is a ﬁnite output alphabet; • λ : Q × Σ → Π∗ is a function called the transition output function; • the ﬁnal function Ψ : F → 2Π∗ assigns to each f ∈ F a set of strings over Π, where |Ψ(f )| ≤ p.  The function λ is extended to the domain Q × Σ∗ by the following deﬁnition of λ∗:  ∀q ∈ Q ∀q ∈ Q ∀U ∈ Σ∗ ∀a ∈ Σ  (λ∗(q, ε) = ε) (λ∗(q, Ua) = λ∗(q, U)λ(δ∗(q, U), a))  The input language of the transducer is L(T) := {U ∈ Σ∗ | δ∗(q0, U) ∈ F}. The subsequential transducer maps each word from the input language to a set of at most  454  Mihov and Schulz  Fast Approximate Search in Large Dictionaries  p output words. The output function OT : L(T) → 2Π∗ of the transducer is deﬁned as follows: ∀U ∈ L(T) (OT(U) = λ∗(q0, U) · Ψ(δ∗(q0, U))) By a dictionary, we mean a regular (ﬁnite or inﬁnite) set of strings over a given alphabet Σ. Using the algorithm described by Daciuk et al. (2000), the minimal deterministic FSA AD accepting a ﬁnite dictionary D can be effectively computed. By a dictionary with output sets, we mean a regular (ﬁnite or inﬁnite) set of input strings over a given alphabet together with a function that maps each of the input strings to a ﬁnite set of output strings. Given a ﬁnite dictionary with output sets, we can effectively compute, using the algorithm described by Mihov and Maurel (2001), the minimal subsequential transducer that maps each input string to its set of output strings.  3. Background  In this section, we describe some established work that is of help in understanding the remainder of the article from a nontechnical, conceptual point of view. After introducing the Levenshtein distance, we describe methods for computing the distance, for checking whether the distance between two words exceeds a given bound, and for approximate search for a pattern in a text. The similarities and differences described below between approximate search in a text, on the one hand, and approximate search in a dictionary, on the other hand, should help the reader understand the contents of the following sections from a broader perspective.  3.1 Computation of Levenshtein Distance The most prominent metric for comparing strings is the Levenshtein distance, which is based on the notion of a primitive edit operation. In this article, we consider the standard Levenshtein distance. Here the primitive operations are the substitution of one symbol for another symbol, the deletion of a symbol, and the insertion of a symbol. Obviously, given two words W and V in the alphabet Σ, it is always possible to rewrite W into V using primitive edit operations.  Deﬁnition 1 Let P, W be words in the alphabet Σ. The (standard) Levenshtein distance between P and W, denoted dL(P, W), is the minimal number of primitive edit operations (substitutions, deletions, insertions) that are needed to transform P into W.  The Levenshtein distance between two words P and W can be computed using the following simple dynamic programming scheme, described, for example, by Wagner and Fischer (1974):  dL(ε, W) = |W|  dL(P, ε) = |P|  dL(Pa, Wb) =  dL(P, W)  if a = b  
Michael Collins’ (1996, 1997, 1999) parsing models have been quite inﬂuential in the ﬁeld of natural language processing. Not only did they achieve new performance benchmarks on parsing the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), and not only did they serve as the basis of Collins’ own future work (Collins 2000; Collins and Duffy 2002), but they also served as the basis of important work on parser selection (Henderson and Brill 1999), an investigation of corpus variation and the effectiveness of bilexical dependencies (Gildea 2001), sample selection (Hwa 2001), bootstrapping non-English parsers (Hwa, Resnik, and Weinberg 2002), and the automatic labeling of semantic roles and predicate-argument extraction (Gildea and Jurafsky 2000; Gildea and Palmer 2002), as well as that of other research efforts. Recently, in order to continue our work combining word sense with parsing (Bikel 2000) and the study of language-dependent and -independent parsing features (Bikel and Chiang 2000), we built a multilingual parsing engine that is capable of instantiating a wide variety of generative statistical parsing models (Bikel 2002).1 As an appropriate baseline model, we chose to instantiate the parameters of Collins’ Model 2. This task proved more difﬁcult than it initially appeared. Starting with Collins’ (1999) thesis, we reproduced all the parameters described but did not achieve nearly the same high performance on the well-established development test set of Section 00 of ∗ Department of Computer and Information Science, 3330 Walnut Street, Philadelphia, PA 19104. E-mail: dbikel@linc.cis.upenn.edu 
Many learning tasks for natural language processing require supervised training; that is, the system successfully learns a concept only if it has been given annotated training data. For example, while it is difﬁcult to induce a grammar with raw text alone, the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data (Pereira and Schabes 1992). Current state-of-the-art statistical parsers (Collins 1999; Charniak 2000) are all trained on large annotated corpora such as the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993). However, supervised training data are difﬁcult to obtain; existing corpora might not contain the relevant type of annotation, and the data might not be in the domain of interest. For example, one might need lexical-semantic analyses in addition to the syntactic analyses in the treebank, or one might be interested in processing languages, domains, or genres for which there are no annotated corpora. Because supervised training demands signiﬁcant human involvement (e.g., annotating the syntactic structure of each sentence by hand), creating a new corpus is a labor-intensive and time-consuming endeavor. The goal of this work is to minimize a system’s reliance on annotated training data. One promising approach to mitigating the annotation bottleneck problem is to use sample selection, a variant of active learning. Sample selection is an interactive learning method in which the machine takes the initiative in selecting unlabeled data for the human to annotate. Under this framework, the system has access to a large pool of unlabeled data, and it has to predict how much it can learn from each candidate in the pool if that candidate is labeled. More quantitatively, we associate each candidate in the pool with a training utility value (TUV). If the system can accurately identify the subset of examples with the highest TUV, it will have located the most beneﬁcial ∗ Computer Science Department, Pittsburgh, PA 15260. E-mail: hwa@cs.pitt.edu. Submission received: 14 October 2002; Revised submission received: 30 September 2003; Accepted for publication: 22 December 2003 c 2004 Association for Computational Linguistics  Computational Linguistics  Volume 30, Number 3  training examples, thus freeing the annotators from having to label less informative examples. In this article, we apply sample selection to two syntactic learning tasks: training a prepositional-phrase attachment (PP-attachment) model and training a statistical parsing model. We are interested in addressing two main questions. First, what are good predictors of a candidate’s training utility? We propose several predictive criteria and deﬁne evaluation functions based on them to rank the candidates’ utility. We have performed experiments comparing the effect of these evaluation functions on the size of the training corpus. We ﬁnd that, with a judiciously chosen evaluation function, sample selection can signiﬁcantly reduce the size of the training corpus. The second main question is: Are the predictors consistently effective for different types of learners? We compare the predictive criteria both across tasks (between PP-attachment and parsing) and within a single task (applying the criteria to two parsing models: an expectation-maximization-trained parser and a count-based parser). We ﬁnd that the learner’s uncertainty is a robust predictive criterion that can be easily applied to different learning models. 2. Learning with Sample Selection Unlike traditional learning systems that receive training examples indiscriminately, a sample selection learning system actively inﬂuences its own progress by choosing new examples to incorporate into its training set. There are two types of selection algorithms: committee-based and single learner. A committee-based selection algorithm works with multiple learners, each maintaining a different hypothesis (perhaps pertaining to different aspects of the problem). The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV (Cohn, Atlas, and Ladner 1994; Freund et al. 1997). For computationally intensive problems, such as parsing, keeping multiple learners may be impractical. In this work, we focus on sample selection using a single learner that keeps one working hypothesis. Without access to multiple hypotheses, the selection algorithm can nonetheless estimate the TUV of a candidate. We identify the following three classes of predictive criteria:  1. Problem-space: Knowledge about the problem space may provide information about the type of candidates that are particularly plentiful or difﬁcult to learn. This criterion focuses on the general attributes of the learning problem, such as the distribution of the input data and properties of the learning algorithm, but it ignores the current state of the hypothesis. 2. Performance of the hypothesis: Testing the candidates on the current working hypothesis shows the type of input data on which the hypothesis may perform weakly. That is, if the current hypothesis is unable to label a candidate or is uncertain about it, then the candidate might be a good training example (Lewis and Catlett 1994). The underlying assumption is that an uncertain output is likely to be wrong. 3. Parameters of the hypothesis: Estimating the potential impact that the candidates will have on the parameters of the current working hypothesis locates those examples that will change the current hypothesis the most.  254  Hwa  Sample Selection for Statistical Parsing  U is a set of unlabeled candidates. L is a set of labeled training examples. C is the current hypothesis. Initialize: C ← Train(L). Repeat N ← Select(n, U, C, f ). U ← U − N. L ← L ∪ Label(N). C ← Train(L). Until (C is good enough) or (U = ∅) or (cutoff). Figure 1 Pseudo code for the sample selection learning algorithm.  Figure 1 outlines the single-learner sample selection training loop in pseudocode. Initially, the training set, L, consists of a small number of labeled examples, based on which the learner proposes its ﬁrst hypothesis of the target concept, C. Also available to the learner is a large pool of unlabeled training candidates, U. In each training iteration, the selection algorithm, Select(n, U, C, f ), ranks the candidates of U according to their expected TUVs and returns the n candidates with the highest values. The algorithm predicts the TUV of each candidate, u ∈ U, with an evaluation function, f (u, C). This function may rely on the hypothesis concept C to estimate the utility of a candidate u. The n chosen candidates are then labeled by human experts and added to the existing training set. Running the learning algorithm, Train(L), on the updated training set, the system proposes a new hypothesis regarding the target concept that is the most compatible with the examples seen thus far. The loop continues until one of three stopping conditions is met: The hypothesis is considered to perform well enough, all candidates are labeled, or an absolute cutoff point is reached (e.g., no more resources). 3. Sample Selection for Prepositional-Phrase Attachment One common source of structural ambiguities arises from syntactic constructs in which a prepositional phrase might be equally likely to modify the verb or the noun preceding it. Researchers have proposed many computational models for resolving PPattachment ambiguities. Some well-known approaches include rule-based models (Brill and Resnik 1994), backed-off models (Collins and Brooks 1995), and a maximumentropy model (Ratnaparkhi 1998). Following the tradition of using learning PPattachment as a way to gain insight into the parsing problem, we ﬁrst apply sample selection to reduce the amount of annotation used in training a PP-attachment model. We use the Collins-Brooks model as the basic learning algorithm and experiment with several evaluation functions based on the types of predictive criteria described earlier. Our experiments show that the best evaluation function can reduce the number of labeled examples by nearly half without loss of accuracy. 3.1 A Summary of the Collins-Brooks Model The Collins-Brooks model takes prepositional phrases and their attachment classiﬁcations as training examples: each is represented as a quintuple of the form (v, n, p, n2, a), where v, n, p, and n2 are the head words of the verb phrase, the object noun phrase, the 255  Computational Linguistics  Volume 30, Number 3  subroutine Train(L) foreach ex ∈ L do extract (v, n, p, n2, a) from ex foreach tuple ∈ {(v, n, p, n2), (v, p, n2), (n, p, n2), (v, n, p), (v, p), (n, p), (p, n2), (p)} do Count(tuple) ← Count(tuple) + 1 if a = noun then CountNP(tuple) ← CountNP(tuple) + 1  subroutine Test(U)  foreach u ∈ U do  extract (v, n, p, n2) from u  if Count(v, n, p, n2) > 0 then  prob  ←  CountNP (v,n,p,n2) Count(v,n,p,n2)  elsif Count(v, p, n2) + Count(n, p, n2) + Count(v, n, p) > 0 then  prob  ←  CountNP (v,p,n2)+CountNP (n,p,n2)+CountNP (v,n,p) Count(v,p,n2)+Count(n,p,n2)+Count(v,n,p)  elsif Count(v, p) + Count(n, p) + Count(p, n2) > 0 then  prob  ←  CountNP (v,p)+CountNP (n,p)+CountNP (p,n2) Count(v,p)+Count(n,p)+Count(p,n2)  elsif Count(p) > 0 then  prob  ←  CountNP (p) Count(p)  else prob ← 1  if prob ≥ .5 then  output noun  else output verb  Figure 2 The Collins-Brooks PP-attachment classiﬁcation algorithm.  preposition, and the prepositional noun phrase, respectively, and a speciﬁes the attachment classiﬁcation. For example, (wrote a book in three days, attach-verb) would be annotated as (wrote, book, in, days, verb). The head words can be automatically extracted using a heuristic table lookup in the manner described by Magerman (1994). For this learning problem, the supervision is the one-bit information of whether p should attach to v or to n. In order to learn the attachment preferences of prepositional phrases, the system builds attachment statistics for each the characteristic tuple of all training examples. A characteristic tuple is some subset of the four head words in the example, with the condition that one of the elements must be the preposition. Each training example forms eight characteristic tuples: (v, n, p, n2), (v, n, p), (v, p, n2), (n, p, n2), (v, p), (n, p), (p, n2), (p). The attachment statistics are a collection of the occurrence frequencies for all the characteristic tuples in the training set and the occurrence frequencies for the characteristic tuples of those examples determined to attach to nouns. For some characteristic tuple t, Count(t) denotes the former and CountNP(t) denotes the latter. In terms of the sample selection algorithm, the collection of counts represents the learner’s current hypothesis (C in Figure 1). Figure 2 provides the pseudocode for the Train routine. Once trained, the system can be used to classify test cases based on the statistics of the most similar training examples and back off as necessary. For instance, to determine the PP-attachment for a test case, the classiﬁer would ﬁrst consider the ratio of the two frequency counts for the four-word characteristic tuple of the test case. If the tuple 256  Hwa  Sample Selection for Statistical Parsing  Figure 3 In this example, the classiﬁcation of the test case preposition is backed off to the two-word-tuple level. In the diagram, each circle represents a characteristic tuple. A ﬁlled circle denotes that the tuple has occurred in the training set. The dashed rectangular box indicates the back-off level on which the classiﬁcation is made.  never occurred in the training example, the classiﬁer would then back off to look at the test case’s three three-word characteristic tuples. It would continue to back off further, if necessary. In the case that the model has no information on any of the characteristic tuples of the test case, it would, by default, classify the test case as an instance of noun attachment. Figure 3 shows using the back-off scheme on a test case. We describe in the Test pseudocode routine in Figure 2 the model’s classiﬁcation procedure for each back-off level.  3.2 Evaluation Functions Based on the three classes of predictive criteria discussed in Section 2, we propose several evaluation functions for the Collins-Brooks model.  3.2.1 The Problem Space. One source of knowledge to exploit is our understanding of the PP-attachment model and properties of English prepositional phrases. For instance, we know that the most problematic test cases for the PP-attachment model are those for which it has no statistics at all. Therefore, those data that the system has not yet encountered might be good candidates. The ﬁrst evaluation function we deﬁne, fnovel(u, C), equates the TUV of a candidate u with its degree of novelty, the number of its characteristic tuples that currently have zero counts:1  fnovel(u, C) = t∈Tuples(u)  
University of North Carolina at Asheville Melanie Martin§ New Mexico State University  Theresa Wilson† University of Pittsburgh Matthew Bell∗ University of Pittsburgh  Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations. There are numerous natural language processing applications for which subjectivity analysis is relevant, including information extraction and text categorization. The goal of this work is learning subjective language from corpora. Clues of subjectivity are generated and tested, including low-frequency words, collocations, and adjectives and verbs identiﬁed using distributional similarity. The features are also examined working together in concert. The features, generated from different data sets using different procedures, exhibit consistency in performance in that they all do better and worse on the same data sets. In addition, this article shows that the density of subjectivity clues in the surrounding context strongly affects how likely it is that a word is subjective, and it provides the results of an annotation study assessing the subjectivity of sentences with high-density features. Finally, the clues are used to perform opinion piece recognition (a type of text categorization and genre detection) to demonstrate the utility of the knowledge acquired in this article. 1. Introduction Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations (Banﬁeld 1982; Wiebe 1994). Many natural language processing (NLP) applications could beneﬁt from being able to distinguish subjective language from language used to objectively present factual information. Current extraction and retrieval technology focuses almost exclusively on the subject matter of documents. However, additional aspects of a document inﬂuence its relevance, including evidential status and attitude (Kessler, Nunberg, Schu¨ tze 1997). Information extraction systems should be able to distinguish between factual information (which should be extracted) and nonfactual information (which should be ∗ Department of Computer Science, University of Pittsburgh, Pittsburgh, PA 15260. E-mail{wiebe,mbell}@cs.pitt.edu. † Intelligent Systems Program, University of Pittsburgh, Pittsburgh, PA 15260. Email: twilson@cs.pitt.edu. ‡ Department of Computer Science, University of North Carolina at Asheville, Asheville, NC 28804. E-mail: bruce@cs.unca.edu § Department of Computer Science, New Mexico State University, Las Cruces, NM 88003. E-mail: mmartin@cs.nmsu.edu. Submission received: 20 March 2002; Revised submission received: 30 September 2003; Accepted for publication: 23 January 2004 c 2004 Association for Computational Linguistics  Computational Linguistics  Volume 30, Number 3  discarded or labeled as uncertain). Question-answering systems should distinguish between factual and speculative answers. Multi-perspective question answering aims to present multiple answers to the user based upon speculation or opinions derived from different sources (Carbonell 1979; Wiebe et al. 2003). Multidocument summarization systems should summarize different opinions and perspectives. Automatic subjectivity analysis would also be useful to perform ﬂame recognition (Spertus 1997; Kaufer 2000), e-mail classiﬁcation (Aone, Ramos-Santacruze, and Niehaus 2000), intellectual attribution in text (Teufel and Moens 2000), recognition of speaker role in radio broadcasts (Barzialy et al. 2000), review mining (Terveen et al. 1997), review classiﬁcation (Turney 2002; Pang, Lee, and Vaithyanathan 2002), style in generation (Hovy 1987), and clustering documents by ideological point of view (Sack 1995). In general, nearly any information-seeking system could beneﬁt from knowledge of how opinionated a text is and whether or not the writer purports to objectively present factual material. To perform automatic subjectivity analysis, good clues must be found. A huge variety of words and phrases have subjective usages, and while some manually developed resources exist, such as dictionaries of affective language (General-Inquirer 2000; Heise 2000) and subjective features in general-purpose lexicons (e.g., the attitude adverb features in Comlex [Macleod, Grishman, and Meyers 1998]), there is no comprehensive dictionary of subjective language. In addition, many expressions with subjective usages have objective usages as well, so a dictionary alone would not sufﬁce. An NLP system must disambiguate these expressions in context. The goal of our work is learning subjective language from corpora. In this article, we generate and test subjectivity clues and contextual features and use the knowledge we gain to recognize subjective sentences and opinionated documents. Two kinds of data are available to us: a relatively small amount of data manually annotated at the expression level (i.e., labels on individual words and phrases) of Wall Street Journal and newsgroup data and a large amount of data with existing documentlevel annotations from the Wall Street Journal (opinion pieces, such as editorials and reviews, versus nonopinion pieces). Both are used as training data to identify clues of subjectivity. In addition, we cross-validate the results between the two types of annotation: The clues learned from the expression-level data are evaluated against the document-level annotations, and those learned using the document-level annotations are evaluated against the expression-level annotations. There were a number of motivations behind our decision to use document-level annotations, in addition to our manual annotations, to identify and evaluate clues of subjectivity. The document-level annotations were not produced according to our annotation scheme and were not produced for the purpose of training and evaluating an NLP system. Thus, they are an external inﬂuence from outside the laboratory. In addition, there are a great number of these data, enabling us to evaluate the results on a larger scale, using multiple large test sets. This and cross-training between the two types of annotations allows us to assess consistency in performance of the various identiﬁcation procedures. Good performance in cross-validation experiments between different types of annotations is evidence that the results are not brittle. We focus on three types of subjectivity clues. The ﬁrst are hapax legomena, the set of words that appear just once in the corpus. We refer to them here as unique words. The set of all unique words is a feature with high frequency and signiﬁcantly higher precision than baseline (Section 3.2). The second are collocations (Section 3.3). We demonstrate a straightforward method for automatically identifying collocational clues of subjectivity in texts. The method is ﬁrst used to identify ﬁxed n-grams, such as of the century and get out of here. Interest-  278  Wiebe, Wilson, Bruce, Bell, and Martin  Learning Subjective Language  ingly, many include noncontent words that are typically on stop lists of NLP systems (e.g., of, the, get, out, here in the above examples). The method is then used to identify an unusual form of collocation: One or more positions in the collocation may be ﬁlled by any word (of an appropriate part of speech) that is unique in the test data. The third type of subjectivity clue we examine here are adjective and verb features identiﬁed using the results of a method for clustering words according to distributional similarity (Lin 1998) (Section 3.4). We hypothesized that two words may be distributionally similar because they are both potentially subjective (e.g., tragic, sad, and poignant are identiﬁed from bizarre). In addition, we use distributional similarity to improve estimates of unseen events: A word is selected or discarded based on the precision of it together with its n most similar neighbors. We show that the various subjectivity clues perform better and worse on the same data sets, exhibiting an important consistency in performance (Section 4.2). In addition to learning and evaluating clues associated with subjectivity, we address disambiguating them in context, that is, identifying instances of clues that are subjective in context (Sections 4.3 and 4.4). We ﬁnd that the density of clues in the surrounding context is an important inﬂuence. Using two types of annotations serves us well here, too. It enables us to use manual judgments to identify parameters for disambiguating instances of automatically identiﬁed clues. High-density clues are high precision in both the expression-level and document-level data. In addition, we give the results of a new annotation study showing that most high-density clues are in subjective text spans (Section 4.5). Finally, we use the clues together to perform documentlevel classiﬁcation, to further demonstrate the utility of the acquired knowledge (Section 4.6). At the end of the article, we discuss related work (Section 5) and conclusions (Section 6). 2. Subjectivity Subjective language is language used to express private states in the context of a text or conversation. Private state is a general covering term for opinions, evaluations, emotions, and speculations (Quirk et al. 1985). The following are examples of subjective sentences from a variety of document types. The ﬁrst two examples are from Usenet newsgroup messages: (1) I had in mind your facts, buddy, not hers. (2) Nice touch. “Alleges” whenever facts posted are not in your persona of what is “real.” The next one is from an editorial: (3) We stand in awe of the Woodstock generation’s ability to be unceasingly fascinated by the subject of itself. (“Bad Acid,” Wall Street Journal, August 17, 1989) The next example is from a book review: (4) At several different layers, it’s a fascinating tale. (George Melloan, “Whose Spying on Our Computers?” Wall Street Journal, November 1, 1989)  279  Computational Linguistics  Volume 30, Number 3  The last one is from a news story: (5) “The cost of health care is eroding our standard of living and sapping industrial strength,” complains Walter Maher, a Chrysler health-and-beneﬁts specialist. (Kenneth H. Bacon, “Business and Labor Reach a Consensus on Need to Overhaul Health-Care System,” Wall Street Journal, November 1, 1989) In contrast, the following are examples of objective sentences, sentences without signiﬁcant expressions of subjectivity: (6) Bell Industries Inc. increased its quarterly to 10 cents from 7 cents a share. (7) Northwest Airlines settled the remaining lawsuits ﬁled on behalf of 156 people killed in a 1987 crash, but claims against the jetliner’s maker are being pursued, a federal judge said. (“Northwest Airlines Settles Rest of Suits,” Wall Street Journal, November 1, 1989) A particular model of linguistic subjectivity underlies the current and past research in this area by Wiebe and colleagues. It is most fully presented in Wiebe and Rapaport (1986, 1988, 1991) and Wiebe (1990, 1994). It was developed to support NLP research and combines ideas from several sources in ﬁelds outside NLP, especially linguistics and literary theory. The most direct inﬂuences on the model were Dolezel (1973) (types of subjectivity clues), Uspensky (1973) (types of point of view), Kuroda (1973, 1976) (pragmatics of point of view), Chatman (1978) (story versus discourse), Cohn (1978) (linguistic styles for presenting consciousness), Fodor (1979) (linguistic description of opaque contexts), and especially Banﬁeld (1982) (theory of subjectivity versus communication).1 The remainder of this section sketches our conceptualization of subjectivity and describes the annotation projects it underlies. Subjective elements are linguistic expressions of private states in context. Subjective elements are often lexical (examples are stand in awe, unceasingly, fascinated in (3) and eroding, sapping, and complains in (5)). They may be single words (e.g., complains) or more complex expressions (e.g., stand in awe, what a NP). Purely syntactic or morphological devices may also be subjective elements (e.g., fronting, parallelism, changes in aspect). A subjective element expresses the subjectivity of a source, who may be the writer or someone mentioned in the text. For example, the source of fascinating in (4) is the writer, while the source of the subjective elements in (5) is Maher (according to the writer). In addition, a subjective element usually has a target, that is, what the subjectivity is about or directed toward. In (4), the target is a tale; in (5), the target of Maher’s subjectivity is the cost of health care. Note our parenthetical above—“according to the writer”—concerning Maher’s subjectivity. Maher is not directly speaking to us but is being quoted by the writer. Thus, the source is a nested source, which we notate (writer, Maher); this represents the fact that the subjectivity is being attributed to Maher by the writer. Since sources  
Rosemary Stevenson† University of Durham Janet Hitzeman§ MITRE Corporation  Centering theory is the best-known framework for theorizing about local coherence and salience; however, its claims are articulated in terms of notions which are only partially speciﬁed, such as “utterance,” “realization,” or “ranking.” A great deal of research has attempted to arrive at more detailed speciﬁcations of these parameters of the theory; as a result, the claims of centering can be instantiated in many different ways. We investigated in a systematic fashion the effect on the theory’s claims of these different ways of setting the parameters. Doing this required, ﬁrst of all, clarifying what the theory’s claims are (one of our conclusions being that what has become known as “Constraint 1” is actually a central claim of the theory). Secondly, we had to clearly identify these parametric aspects: For example, we argue that the notion of “pronoun” used in Rule 1 should be considered a parameter. Thirdly, we had to ﬁnd appropriate methods for evaluating these claims. We found that while the theory’s main claim about salience and pronominalization, Rule 1—a preference for pronominalizing the backward-looking center (CB)—is veriﬁed with most instantiations, Constraint 1–a claim about (entity) coherence and CB uniqueness—is much more instantiation-dependent: It is not veriﬁed if the parameters are instantiated according to very mainstream views (“vanilla instantiation”), it holds only if indirect realization is allowed, and is violated by between 20% and 25% of utterances in our corpus even with the most favorable instantiations. We also found a trade-off between Rule 1, on the one hand, and Constraint 1 and Rule 2, on the other: Setting the parameters to minimize the violations of local coherence leads to increased violations of salience, and vice versa. Our results suggest that “entity” coherence—continuous reference to the same entities—must be supplemented at least by an account of relational coherence. 1. Motivations Centering theory (Joshi and Weinstein 1981; Grosz, Joshi, and Weinstein 1983, 1995; Walker, Joshi, and Prince 1998b) is the component of Grosz and Sidner’s overall theory ∗ Department of Computer Science, Wivenhoe Park, Colchester CO4 35Q, U.K. E-mail: poesio@essex.ac.uk. † Department of Psychology, University of Durham, U.K. ‡ Department of Computer Science, University of Illinois at Chicago, Chicago, IL 60607-7053, USA. E-mail: bdieugen@cs.uic.edu § MITRE Corporation, 202 Burlington Road, Bedford, MA 01730-1428, USA. E-mail:hitze@mitre.org. Submission received: 16 April 2002; Revised submission received: 3 September 2003; Accepted for publication: 11 December 2003 c 2004 Association for Computational Linguistics  Computational Linguistics  Volume 30, Number 3  of attention and coherence in discourse (Grosz 1977; Sidner 1979; Grosz and Sidner 1986) concerned with local coherence and salience, that is, coherence and salience within a discourse segment. A fundamental characteristic of centering is that it is better viewed as a linguistic theory than a computational one. By this we mean that its primary aim is to make cross-linguistically valid claims about which discourses are easier to process, abstracting away from speciﬁc algorithms for anaphora resolution or anaphora generation (although many such algorithms are based on the theory). The result is a very different theory from those one usually ﬁnds in computational linguistics. In central papers such as Grosz, Joshi, and Weinstein (1995), no algorithms are provided to compute notions such as “utterance,” “previous utterance,” “ranking,” and “realization” that play a crucial role in the theory. The researchers working on centering argue that while these concepts play a central role in any theory of discourse coherence and salience, their precise characterization is best left for subsequent research, indeed, that some of these concepts (e.g., ranking) might be deﬁned in a different way for each language (Walker, Iida, and Cote 1994). In other words, these notions should be viewed as parameters of centering. This feature of the theory has inspired a great deal of research attempting to specify centering’s parameters for different languages (Kameyama 1985; Walker, Iida, and Cote 1994; Di Eugenio 1998; Turan 1998; Strube and Hahn 1999). Competing versions of the central deﬁnitions and claims of the theory have also been proposed: For example, different deﬁnitions of backward-looking center (CB) can be found in Grosz, Joshi, and Weinstein (1983, 1995) and Gordon, Grosz, and Gillion (1993). As a result, a researcher wishing to test the predictions of centering, or to use it for practical applications, is confronted with a large number of possible instantiations of the theory. The main goal of the work reported in this article was to explore the space of parameter conﬁgurations, measuring the impact of different ways of setting the parameters of centering on the theory’s claims. This required specifying in an explicit way what centering’s main claims are; clearly identifying the parameters, not all of which have previously been discussed in the literature; and developing appropriate methods (and statistical tests) to carry out this evaluation. The comparison between instantiations was carried out by annotating a corpus of English texts from different genres with the information needed to test a variety of centering instantiations and using this corpus to assess the extent to which the theory’s claims are veriﬁed once the parameters are set in a certain way. The proponents of centering have clearly stated that the aim of the theory is to identify preferences that make discourses easier to process; clearly, the best way to test such preferences is through behavioral experiments, and many aspects of the theory have in fact been tested this way (Hudson, Tanenhaus, and Dell 1986; Gordon, Grosz, and Gillion 1993; Brennan 1995). But given the enormous number of possible ways of setting the theory’s parameters, a systematic comparison can be made only by computational means. A corpus-based evaluation has other advantages, as well, among which is that it is perhaps the best way to identify the aspects of the theory that need to be further speciﬁed, and the factors such as temporal coherence or stylistic variation that may interact with the preferences expressed by centering. (Also, knowing the extent to which real texts conform to centering preferences is an important goal in its own right.) In previous corpus-based studies of centering (Walker 1989; Passonneau 1993; Byron and Stent 1998; Di Eugenio 1998; Kameyama 1998; Strube and Hahn 1999; Tetreault 2001), only a few instantiations of centering were compared. The present study is more systematic in that it considers a greater number of parameters, as well as more parameter instantiations, including “crossing” instantiations in which the parameters are set according to proposals due to different researchers. Only re-  310  Poesio et al.  Centering: A Parametric Theory  liable annotation techniques were used; we produced an annotation manual that can be used to extend our analysis to other data, as well as a companion Web site (http://cswww.essex.ac.uk/staff/poesio/cbc/) to allow readers to try out instantiations not discussed in this article. (The Web site also contains the annotation manual and a technical report with a full discussion of all results.) Last but not least, our evaluation is arguably more neutral than in most previous studies in that, ﬁrst of all, we are not proposing a new instantiation of the theory; and secondly, all parameter instantiations were tested on the same data. The article is organized as follows. We ﬁrst review the basic concepts of the theory, discussing the three claims on which we focus (Constraint 1, Rule 1, and Rule 2) and the parameters used in their formulation. We then discuss how our corpus was annotated and how the annotation was used to compute violations of the three main claims. In Section 4 we present our main results, which are discussed in Section 5. 2. Centering Theory and Its Parameters It is not possible to discuss in this article the entire centering literature; we merely summarize in this section some of this work in enough detail to allow the reader to follow the discussion in the rest of the article. For more details, we refer the reader to classic references such as Grosz, Johsi, and Weinstein (1995) and Walker, Joshi, and Prince (1998b) or the discussion of centering in Poesio and Stevenson (forthcoming). 2.1 Motivations and Main Intuitions Centering is simultaneously a theory of discourse coherence and of discourse salience. As a theory of coherence, it attempts to characterize entity-coherent discourses: discourses that are considered coherent because of the way discourse entities are introduced and discussed.1 At the same time, centering is also intended to be a theory of salience: that is, it attempts to predict which entities will be most salient at any given time. The main claim about local coherence made in centering is that discourse segments in which successive utterances keep mentioning the same discourse entities are “more coherent” than discourse segments in which different entities are mentioned. This hypothesis was formulated by Chafe (1976) and is backed by empirical evidence such as Kintsch and van Dijk (1978) and Givon (1983). In centering this hypothesis is further strenghtened by proposing that every utterance has a unique “main link” with the previous utterance: the CB. Having a unique CB, it is claimed, considerably simpliﬁes the complexity of the inferences required to integrate an utterance into the discourse (Joshi and Kuhn 1979; Joshi and Weinstein 1981). Centering’s ﬁrst contention as far as local salience is concerned is that the discourse entities realized by an utterance (more on realization below) are ranked: that is, that in each utterance some discourse entities are more salient than others. This claim, as well, is a basic tenet of much work on discourse (Sidner 1979; Prince 1981; Givon 1983; Gundel, Hedberg, and Zacharski 1993) and is supported by much psychological evidence (Hudson, Tanenhaus, and Dell 1986; Gernsbacher and Hargreaves 1988; Gordon, Grosz, and Gillion 1993; Stevenson, Crawley, and Kleinman 1994).  
Bootstrapping, or semisupervised learning, has become an important topic in computational linguistics. For many language-processing tasks, there are an abundance of unlabeled data, but labeled data are lacking and too expensive to create in large quantities, making bootstrapping techniques desirable. The Yarowsky (1995) algorithm was one of the ﬁrst bootstrapping algorithms to become widely known in computational linguistics. In brief, it consists of two loops. The “inner loop” or base learner is a supervised learning algorithm. Speciﬁcally, Yarowsky uses a simple decision list learner that considers rules of the form “If instance x contains feature f , then predict label j” and selects those rules whose precision on the training data is highest. The “outer loop” is given a seed set of rules to start with. In each iteration, it uses the current set of rules to assign labels to unlabeled data. It selects those instances regarding which the base learner’s predictions are most conﬁdent and constructs a labeled training set from them. It then calls the inner loop to construct a new classiﬁer (that is, a new set of rules), and the cycle repeats. An alternative algorithm, co-training (Blum and Mitchell 1998), has subsequently become more popular, perhaps in part because it has proven amenable to theoretical analysis (Dasgupta, Littman, and McAllester 2001), in contrast to the Yarowsky algorithm, which is as yet mathematically poorly understood. The current article aims to rectify this lack of understanding, increasing the attractiveness of the Yarowsky algorithm as an alternative to co-training. The Yarowsky algorithm does have the advantage of placing less of a restriction on the data sets it can be applied to. Co-training requires data attributes to be separable into two views that are conditionally independent given the target label; the Yarowsky algorithm makes no such assumption about its data. In previous work, I did propose an assumption about the data called precision independence, under which the Yarowsky algorithm could be shown effective (Abney 2002). That assumption is ultimately unsatisfactory, however, not only because it ∗ 4080 Frieze Bldg., 105 S. State Street, Ann Arbor, MI 48109-1285. E-mail: abney.umich.edu. Submission received: 26 August 2003; Revised submission received: 21 December 2003; Accepted for publication: 10 February 2004 c 2004 Association for Computational Linguistics  Computational Linguistics  Volume 30, Number 3  Table 1 The Yarowsky algorithm variants. Y-1/DL-EM reduces H; the others reduce K.  Y-1/DL-EM-Λ Y-1/DL-EM-X Y-1/DL-1-R Y-1/DL-1-VS YS-P YS-R YS-FS  EM inner loop that uses labeled examples only EM inner loop that uses all examples Near-original Yarowsky inner loop, no smoothing Near-original Yarowsky inner loop, “variable smoothing” Sequential update, “antismoothing” Sequential update, no smoothing Sequential update, original Yarowsky smoothing  restricts the data sets on which the algorithm can be shown effective, but also for additional internal reasons. A detailed discussion would take us too far aﬁeld here, but sufﬁce it to say that precision independence is a property that it would be preferable not to assume, but rather to derive from more basic properties of a data set, and that closer empirical study shows that precision independence fails to be satisﬁed in some data sets on which the Yarowsky algorithm is effective. This article proposes a different approach. Instead of making assumptions about the data, it views the Yarowsky algorithm as optimizing an objective function. We will show that several variants of the algorithm (though not the algorithm in precisely its original form) optimize either negative log likelihood H or an alternative objective function, K, that imposes an upper bound on H. Ideally, we would like to show that the Yarowsky algorithm minimizes H. Unfortunately, we are not able to do so. But we are able to show that a variant of the Yarowsky algorithm, which we call Y-1/DL-EM, decreases H in each iteration. It combines the outer loop of the Yarowsky algorithm with a different inner loop based on the expectation-maximization (EM) algorithm. A second proposed variant of the Yarowsky algorithm, Y-1/DL-1, has the advantage that its inner loop is very similar to the original Yarowsky inner loop, unlike Y-1/DL-EM, whose inner loop bears little resemblance to the original. Y-1/DL-1 has the disadvantage that it does not directly reduce H, but we show that it does reduce the alternative objective function K. We also consider a third variant, YS. It differs from Y-1/DL-EM and Y-1/DL-1 in that it updates sequentially (adding a single rule in each iteration), rather than in parallel (updating all rules in each iteration). Besides having the intrinsic interest of sequential update, YS can be proven effective when using exactly the same smoothing method as used in the original Yarowsky algorithm, in contrast to Y-1/DL-1, which uses either no smoothing or a nonstandard “variable smoothing.” YS is proven to decrease K. The Yarowsky algorithm variants that we consider are summarized in Table 1. To the extent that these variants capture the essence of the original algorithm, we have a better formal understanding of its effectiveness. Even if the variants are deemed to depart substantially from the original algorithm, we have at least obtained a family of new bootstrapping algorithms that are mathematically understood. 2. The Generic Yarowsky Algorithm 2.1 The Original Algorithm Y-0 The original Yarowsky algorithm, which we refer to as Y-0, is given in table 2. It is an iterative algorithm. One begins with a seed set Λ(0) of labeled examples and a 366  Abney  Understanding the Yarowsky Algorithm  Table 2 The generic Yarowsky algorithm (Y-0)  (1) Given: examples X, and initial labeling Y(0)  (2) For t ∈ {0, 1, . . .}  (2.1) Train classiﬁer on labeled examples (Λ(t), Y(t)), where Λ(t) = {x ∈ X|Y(t) = ⊥}  The resulting classiﬁer predicts label j for example x with probability πx(t+1)(j) (2.2) For each example x ∈ X:  (2.2.1) Set ˆy = arg maxj πx(t+1)(j)  (2.2.2) Set Yx(t+1)   =  Yx(0) ˆy ⊥  if x ∈ Λ(0) if πx(t+1)(ˆy) > ζ otherwise  (2.3) If Y(t+1) = Y(t), stop  set V(0) of unlabeled examples. At each iteration, a classiﬁer is constructed from the labeled examples; then the classiﬁer is applied to the unlabeled examples to create a new labeled set. To discuss the algorithm formally, we require some notation. We assume ﬁrst a set of examples X and a feature set Fx for each x ∈ X. The set of examples with feature f is Xf . Note that x ∈ Xf if and only if f ∈ Fx. We also require a series of labelings Y(t), where t represents the iteration number. We write Yx(t) for the label of example x under labeling Y(t). An unlabeled example is one for which Yx(t) is undeﬁned, in which case we write Yx(t) = ⊥. We write V(t) for the set of unlabeled examples and Λ(t) for the set of labeled examples. It will also be useful to have a notation for the set of examples with label j:  Λ(j t) ≡ {x ∈ X|Yx(t) = j = ⊥}  Note that Λ(t) is the disjoint union of the sets Λ(j t). When t is clear from context, we drop the superscript (t) and write simply Λj, V, Yx, etc. At the risk of ambiguity, we will also sometimes write Λf for the set of labeled examples with feature f , trusting to the index to discriminate between Λf (labeled examples with feature f ) and Λj (labeled examples with label j). We always use f and g to represent features and j and k to represent labels. The reader may wish to refer  to Table 3, which summarizes notation used throughout the article.  In each iteration, the Yarowsky algorithm uses a supervised learner to train a clas-  siﬁer on the labeled examples. Let us call this supervised learner the base learning  algorithm; it is a function from (X, Y(t)) to a classiﬁer π drawn from a space of classi-  ﬁers Π. It is assumed that the classiﬁer makes conﬁdence-weighted predictions. That  is, the classiﬁer deﬁnes a scoring function π(x, j), and the predicted label for example  x is  ˆy ≡ arg max π(x, j)  (1)  j  Ties are broken arbitrarily. Technically, we assume a ﬁxed order over labels and deﬁne the maximization as returning the ﬁrst label in the ordering, in case of a tie. It will be convenient to assume that the scoring function is nonnegative and bounded, in which case we can normalize it to make π(x, j) a conditional distribution over labels j for a given example x. Henceforward, we write πx(j) instead of π(x, j),  367  Computational Linguistics  Volume 30, Number 3  Table 3 Summary of notation.  X Y Λ V x f, g j, k Fx Yx Xf , Λf , Vf Λj, Λfj m L φx(j) πx(j) θfj ˆy [[Φ]] H H(p) H(p||q) K qf (j) ˜qf (j) ˆqf (j) j† j∗ u(·)  set of examples, both labeled and unlabeled the current labeling; Y(t) is the labeling at iteration t the (current) set of labeled examples the (current) set of unlabeled examples an example index feature indices label indices the features of example x the label of example x; value is undeﬁned (⊥) if x is unlabeled examples, labeled examples, unlabeled examples that have feature f examples with label j, examples with feature f and label j the number of features of a given example: |Fx| (cf. equation (12)) the number of labels labeling distribution (equation (5)) prediction distribution (equation (12); except for DL-0, which uses equation (11)) score for rule f → j; we view θf as the prediction distribution of f label that maximizes πx(j) for given x (equation (1) truth value of Φ: value is 0 or 1 objective function, negative log-likelihood (equation (6)) entropy of distribution p cross entropy: − x p(x) log q(x) (cf. equations (2) and (3)) objective function, upper bound on H (equation (20)) precision of rule f → j (equation (9)) smoothed precision (equation (10)) “peaked” precision (equation (25)) the label that maximizes precision qf (j) for a given feature f (equation (26)) the label that maximizes rule score θfj for a given feature f (equation (28)) uniform distribution  understanding πx to be a probability distribution over labels j. We call this distribution the prediction distribution of the classiﬁer on example x. To complete an iteration of the Yarowsky algorithm, one recomputes labels for examples. Speciﬁcally, the label ˆy is assigned to example x if the score πx(ˆy) exceeds a threshold ζ, called the labeling threshold. The new labeled set Λ(t+1) contains all examples for which πx(ˆy) > ζ. Relabeling applies only to examples in V(0). The labels for examples in Λ(0) are indelible, because Λ(0) constitutes the original manually labeled data, as opposed to data that have been labeled by the learning algorithm itself. The algorithm continues until convergence. The particular base learning algorithm that Yarowsky uses is deterministic, in the sense that the classiﬁer induced is a deterministic function of the labeled data. Hence, the algorithm is known to have converged at whatever point the labeling remains unchanged. Note that the algorithm as stated leaves the base learning algorithm unspeciﬁed. We can distinguish between the generic Yarowsky algorithm Y-0, for which the base learning algorithm is an open parameter, and the speciﬁc Yarowsky algorithm, which includes a speciﬁcation of the base learner. Informally, we call the generic algorithm the outer loop and the base learner the inner loop of the speciﬁc Yarowsky algorithm. The base learner that Yarowsky assumes is a decision list induction algorithm. We postpone discussion of it until Section 3. 368  Abney  Understanding the Yarowsky Algorithm  2.2 An Objective Function Machine learning algorithms are typically designed to optimize some objective function that represents a formal measure of performance. The maximum-likelihood criterion is the most commonly used objective function. Suppose we have a set of examples Λ, with labels Yx for x ∈ Λ, and a parametric family of models πθ such that π(j|x; θ) represents the probability of assigning label j to example x, according to the model. The likelihood of θ is the probability of the full data set according to the model, viewed as a function of θ, and the maximum-likelihood criterion instructs us to choose the parameter settings θˆ that maximize likelihood, or equivalently, log-likelihood:  l(θ) = log π(Yx|x; θ) x∈Λ  =  log π(Yx|x; θ)  x∈Λ  =  [[j = Yx]] log π(j|x; θ)  x∈Λ j  (The notation [[Φ]] represents the truth value of the proposition Φ; it is one if Φ is true and zero otherwise.) Let us deﬁne φx(j) = [[j = Yx]] for x ∈ Λ Note that φx satisﬁes the formal requirements of a probability distribution over labels j: Speciﬁcally, it is a point distribution with all its mass concentrated on Yx. We call it the labeling distribution. Now we can write  l(θ) =  φx(j) log π(j|x; θ)  x∈Λ j  = − H(φx||πx)  (2)  x∈Λ  In (2) we have written πx for the distribution π(·|x; θ), leaving the dependence on θ implicit. We have also used the nonstandard notation H(p||q) for what is sometimes called cross entropy. It is easy to verify that  H(p||q) = H(p) + D(p||q)  (3)  where H(p) is the entropy of p and D is Kullback-Leibler divergence. Note that when p is a point distribution, H(p) = 0 and hence H(p||q) = D(p||q). In particular:  l(θ) = − D(φx||πx)  (4)  x∈Λ  Thus when, as here, φx is a point distribution, we can restate the maximum-likelihood criterion as instructing us to choose the model that minimizes the total divergence between the empirical labeling distributions φx and the model’s prediction distributions πx. To extend l(θ) to unlabeled examples, we need only observe that unlabeled examples are ones about whose labels the data provide no information. Accordingly, we  369  Computational Linguistics  Volume 30, Number 3  revise the deﬁnition of φx to treat unlabeled examples as ones whose labeling distribution is the maximally uncertain distribution, which is to say, the uniform distribution:  φx(j) =  [[j = Yx]] for x ∈ Λ  
Centering has been proposed as a model of the local attentional states of speakers and hearers involved in the mutual construction of conversation (Brennan, Friedman, and Pollard 1987; Grosz and Sidner 1986, 1998; Walker 1998). Centering mechanisms are designed to model the coherence of discourse by characterizing transitions between utterances in terms of their inferential load and hence their naturalness. These characterizations are intended to capture intuitions about the “ﬂow” (Chafe 1979) or the “ongoing process of meaning” (Halliday 1994) in discourse. In this work, we examine a corpus of Japanese e-mail to investigate the mechanisms by which coherence is achieved. Because this corpus contains a high number of discourse elements that are inferable from the discourse context, we have an opportunity to examine the interplay between standard centering transition deﬁnitions and the presence of inferable discourse entities. We claim on the basis of intuitions of native speakers that the actual level of coherence in the corpus is much higher than the centering account implies, primarily by virtue of the fact that transitions involving inferable entities are often difﬁcult to specify. We conclude that the standard centering account cannot accurately model the coherence in this corpus. Detailed analysis reveals that one major problem lies in the requirement of identity of discourse elements in adjacent utterances in order for those elements to contribute to coherence. We describe this problem and propose two additions to the usual repertoire of transitions that enable a more authentic account of coherence in this corpus, while remaining within a centering framework. The article is organized as follows. In Section 2, we brieﬂy describe centering mechanisms and their role in modeling coherence. We go on to outline the features of the corpus in Section 3 and illustrate how standard centering mechanisms characterize transitions and coherence in this corpus, suggesting that these mechanisms are not adequate for the task. In Section 4, we describe more general problems with the ∗ Infant Studies Centre, University of British Columbia, Room 1401, 2136 West Mall, Vancouver, British Columbia, V6T 1Z4 Canada. E-mail: jwlab@psych.ubc.ca. c 2004 Association for Computational Linguistics  Computational Linguistics  Volume 30, Number 2  inclusion of inferable discourse entities in centering theory and propose a revision to the standard set of transitions that more accurately describes the corpus. In this section as well, we explore the implications of this proposal for other areas of discourse analysis. In Section 5, we outline some possibilities for improvement and extension of the proposal, and we conclude in the ﬁnal section. 2. Centering Mechanisms The central intuitions of centering concern the relationships among the discourse entities appearing or represented in adjacent utterances in a discourse (Walker, Joshi, and Prince 1998). Each utterance Ui in a discourse is considered to contain a set of discourse entities called forward-looking centers, or Cfs. These entities are ranked in the Cf list for each utterance according to language-speciﬁc ranking principles. We follow, in general, the ordering principles for Japanese given in Walker, Iida, and Cote (1994) (with some adjustments for possessive phrases as noted in example (1)): Cf ranking for Japanese: (Grammatical OR ∅) topic > empathy > subject > object2 > object > others A special member of the Cf list, the backward-looking center, or Cb, represents the “topic”1 of Ui and is the highest-ranked Cf on the Cf list of Ui−1 which is realized in Ui. In addition, the preferred center of Ui, or Cp, is the highest-ranked Cf in Ui. Given Ui and Ui−1, then, there are four different ways in which their Cbs and Cps may be related; each of these is deﬁned as a type of transition state (Table 1). There are two rules in a centering approach: Rule 1: If some entity in the Cf list for Ui−1 is realized as a pronoun in Ui, then so is the Cb for Ui. Rule 2: Transition states are ordered such that CONTINUE is most preferred, followed in order by RETAIN, SMOOTH SHIFT, and ROUGH SHIFT (Walker, Joshi, and Prince 1998). Rule 2 captures the centering intuitions concerning coherence: Utterances that CONTINUE the topic of a previous utterance in a prominent position impose a lower inferential load, and are thus more coherent, than utterances which relegate the topic to less prominent positions or which change the topic. The vast majority of the sentences in our corpus are complex sentences. Thus, the question of how to interpret centering principles in complex sentences cannot be ignored. We will consider the basic utterance unit of centering to be the tensed clause  Table 1 Transition deﬁnitions.  Cb(Ui) = Cp(Ui) Cb(Ui) = Cp(Ui)  Cb(Ui) = Cb(Ui−1) OR Cb(Ui) = ? CONTINUE RETAIN  Cb(Ui) = Cb(Ui−1) SMOOTH SHIFT ROUGH SHIFT  
Paola Velardi Universita` di Roma “La Sapienza”  We present a method and a tool, OntoLearn, aimed at the extraction of domain ontologies from Web sites, and more generally from documents shared among the members of virtual organizations. OntoLearn ﬁrst extracts a domain terminology from available documents. Then, complex domain terms are semantically interpreted and arranged in a hierarchical fashion. Finally, a general-purpose ontology, WordNet, is trimmed and enriched with the detected domain concepts. The major novel aspect of this approach is semantic interpretation, that is, the association of a complex concept with a complex term. This involves ﬁnding the appropriate WordNet concept for each word of a terminological string and the appropriate conceptual relations that hold among the concept components. Semantic interpretation is based on a new word sense disambiguation algorithm, called structural semantic interconnections. 1. Introduction The importance of domain ontologies is widely recognized, particularly in relation to the expected advent of the Semantic Web (Berners-Lee 1999). The goal of a domain ontology is to reduce (or eliminate) the conceptual and terminological confusion among the members of a virtual community of users (for example, tourist operators, commercial enterprises, medical practitioners) who need to share electronic documents and information of various kinds. This is achieved by identifying and properly deﬁning a set of relevant concepts that characterize a given application domain. An ontology is therefore a shared understanding of some domain of interest (Uschold and Gruninger 1996). The construction of a shared understanding, that is, a unifying conceptual framework, fosters • communication and cooperation among people • better enterprise organization • interoperability among systems • system engineering beneﬁts (reusability, reliability, and speciﬁcation) Creating ontologies is, however, a difﬁcult and time-consuming process that involves specialists from several ﬁelds. Philosophical ontologists and artiﬁcial intelligence logicians are usually involved in the task of deﬁning the basic kinds and structures of concepts (objects, properties, relations, and axioms) that are applicable in every ∗ Dipartimento di Informatica, Universita` di Roma “La Sapienza,” Via Salaria, 113 - 00198 Roma, Italia. E-mail: {navigli, velardi}@di.uniroma1.it. c 2004 Association for Computational Linguistics  Computational Linguistics  Volume 30, Number 2 Foundational Ontology Core Ontology Specific Domain Ontology  Figure 1 The three levels of generality of a domain ontology. possible domain. The issue of identifying these very few “basic” principles, now often referred to as foundational ontologies (FOs) (or top, or upper ontologies; see Figure 1) (Gangemi et al. 2002), meets the practical need of a model that has as much generality as possible, to ensure reusability across different domains (Smith and Welty 2001). Domain modelers and knowledge engineers are involved in the task of identifying the key domain conceptualizations and describing them according to the organizational backbones established by the foundational ontology. The result of this effort is referred to as the core ontology (CO), which usually includes a few hundred application domain concepts. While many ontology projects eventually succeed in the task of deﬁning a core ontology,1 populating the third level, which we call the speciﬁc domain ontology (SDO), is the actual barrier that very few projects have been able to overcome (e.g., WordNet [Fellbaum 1995], Cyc [Lenat 1993], and EDR [Yokoi 1993]), but they pay a price for this inability in terms of inconsistencies and limitations.2 It turns out that, although domain ontologies are recognized as crucial resources for the Semantic Web, in practice they are not available and when available, they are rarely used outside speciﬁc research environments. So which features are most needed to build usable ontologies? • Coverage: The domain concepts must be there; the SDO must be sufﬁciently (for the application purposes) populated. Tools are needed to extensively support the task of identifying the relevant concepts and the relations among them. • Consensus: Decision making is a difﬁcult activity for one person, and it gets even harder when a group of people must reach consensus on a given issue and, in addition, the group is geographically dispersed. When a group of enterprises decide to cooperate in a given domain, they have ﬁrst to agree on many basic issues; that is, they must reach a consensus of the business domain. Such a common view must be reﬂected by the domain ontology. • Accessibility: The ontology must be easily accessible: tools are needed to easily integrate the ontology within an application that may clearly show 
Hermann Ney∗ RWTH Aachen  In statistical machine translation, correspondences between the words in the source and the target language are learned from parallel corpora, and often little or no linguistic knowledge is used to structure the underlying models. In particular, existing statistical systems for machine translation often treat different inﬂected forms of the same lemma as if they were independent of one another. The bilingual training data can be better exploited by explicitly taking into account the interdependencies of related inﬂected forms. We propose the construction of hierarchical lexicon models on the basis of equivalence classes of words. In addition, we introduce sentence-level restructuring transformations which aim at the assimilation of word order in related sentences. We have systematically investigated the amount of bilingual training data required to maintain an acceptable quality of machine translation. The combination of the suggested methods for improving translation quality in frameworks with scarce resources has been successfully tested: We were able to reduce the amount of bilingual training data to less than 10% of the original corpus, while losing only 1.6% in translation quality. The improvement of the translation results is demonstrated on two German-English corpora taken from the Verbmobil task and the Nespole! task. 1. Introduction The statistical approach to machine translation has proved successful in various comparative evaluations since its revival by the work of the IBM research group more than a decade ago. The IBM group dispensed with linguistic analysis, at least in its earliest publications. Although the IBM group ﬁnally made use of morphological and syntactic information to enhance translation quality (Brown et al. 1992; Berger et al. 1996), most of today’s statistical machine translation systems still consider only surface forms and use no linguistic knowledge about the structure of the languages involved. In many applications only small amounts of bilingual training data are available for the desired domain and language pair, and it is highly desirable to avoid at least parts of the costly data collection process. The main objective of the work reported in this article is to introduce morphological knowledge in order to reduce the amount of bilingual data necessary to sufﬁciently cover the vocabulary expected in testing. This is achieved by explicitly taking into account the interdependencies of related inﬂected forms. In this work, a hierarchy of equivalence classes at different levels of abstraction is proposed. Features from those hierarchy levels are combined to form hierarchical lexicon models, which can replace the standard probabilistic lexicon used  ∗ Lehrstuhl fu¨ r Informatik VI, Computer Science Department, RWTH Aachen—University of Technology, D-52056 Aachen, Germany. E-mail: sonja.niessen@gmx.de; ney@cs.rwth-aachen.de. c 2004 Association for Computational Linguistics  Computational Linguistics  Volume 30, Number 2  in most statistical machine translation systems. Apart from the improved coverage, the proposed lexicon models enable the disambiguation of ambiguous word forms by means of annotation with morpho-syntactic tags. 1.1 Overview The article is organized as follows. After brieﬂy reviewing the basic concepts of the statistical approach to machine translation, we discuss the state of the art and related work as regards the incorporation of morphological and syntactic information into systems for natural language processing. Section 2 describes the information provided by morpho-syntactic analysis and introduces a suitable representation of the analyzed corpus. Section 3 suggests solutions for two speciﬁc aspects of structural difference, namely, question inversion and separated verb preﬁxes. Section 4 is dedicated to hierarchical lexicon models. These models are able to infer translations of word forms from the translations of other word forms of the same lemma. Furthermore, they use morpho-syntactic information to resolve categorial ambiguity. In Section 5, we describe how disambiguation between different readings and their corresponding translations can be performed when no context is available, as is typically the case for conventional electronic dictionaries. Section 6 provides an overview of our procedure for training model parameters for statistical machine translation with scarce resources. Experimental results are reported in Section 7. Section 8 concludes the presentation with a discussion of the achievements of this work. 1.2 Statistical Machine Translation In statistical machine translation, every target language string eI1 = e1 · · · eI is assigned a probability Pr(eI1) of being a valid word sequence in the target language and a probability Pr(eI1|f1J) of being a translation for the given source language string f1J = f1 · · · fJ. According to Bayes’ decision rule, the optimal translation for f1J is the target string that maximizes the product of the target language model Pr(eI1) and the string translation model Pr(f1J|eI1). Many existing systems for statistical machine translation (Garc´ıa-Varea and Casacuberta 2001; Germann et al. 2001; Nießen et al. 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position. The probability that a certain target language word will occur in the target string is assumed to depend basically only on the source words aligned with it. 1.3 Related Work 1.3.1 Morphology. Some publications have already dealt with the treatment of morphology in the framework of language modeling and speech recognition: Kanevsky, Roukos, and Sedivy (1997) propose a statistical language model for inﬂected languages. They decompose word forms into stems and afﬁxes. Maltese and Mancini (1992) report that a linear interpolation of word n-grams, part of speech n-grams, and lemma n-grams yields lower perplexity than pure word-based models. Larson et al. (2000) apply a data-driven algorithm for decomposing compound words in compounding languages as well as for recombining phrases to enhance the pronunciation lexicon and the language model for large-vocabulary speech recognition systems. As regards machine translation, the treatment of morphology is part of the analysis and generation step in virtually every symbolic machine translation system. For this purpose, the lexicon should contain base forms of words and the grammatical category,  182  Nießen and Ney  SMT with Scarce Resources  subcategorization features, and semantic information in order to enable the size of the lexicon to be reduced and in order to account for unknown word forms, that is, word forms not present explicitly in the dictionary. Today’s statistical machine translation systems build upon the work of P. F. Brown and his colleagues at IBM. The translation models they presented in various papers between 1988 and 1993 (Brown et al. 1988; Brown et al. 1990; Brown, Della Pietra, Della Pietra, and Mercer 1993) are commonly referred to as IBM models 1–5, based on the numbering in Brown, Della Pietra, Della Pietra, and Mercer (1993). The underlying (probabilistic) lexicon contains only pairs of full forms. On the other hand, Brown et al. (1992) had already suggested word forms be annotated with morpho-syntactic information, but they did not perform any investigation on the effects. 1.3.2 Translation with Scarce Resources. Some recent publications, like Al-Onaizan et al. (2000), have dealt with the problem of translation with scarce resources. AlOnaizan et al. report on an experiment involving Tetun-to-English translation by different groups, including one using statistical machine translation. Al-Onaizan et al. assume the absence of linguistic knowledge sources such as morphological analyzers and dictionaries. Nevertheless, they found that the human mind is very well capable of deriving dependencies such as morphology, cognates, proper names, and spelling variations and that this capability was ﬁnally at the basis of the better results produced by humans compared to corpus-based machine translation. The additional information results from complex reasoning, and it is not directly accessible from the full-wordform representation in the data. This article takes a different point of view: Even if full bilingual training data are scarce, monolingual knowledge sources like morphological analyzers and data for training the target language model as well as conventional dictionaries (one word and its translation[s] per entry) may be available and of substantial usefulness for improving the performance of statistical translation systems. This is especially the case for more-inﬂecting major languages like German. The use of dictionaries to augment or replace parallel corpora has already been examined by Brown, Della Pietra, Della Pietra, and Goldsmith (1993) and Koehn and Knight (2001), for instance. 2. Morpho-syntactic Information A prerequisite for the methods for improving the quality of statistical machine translation described in this article is the availability of various kinds of morphological and syntactic information. This section describes the output resulting from morphosyntactic analysis and explains which parts of the analysis are used and how the output is represented for further processing. 2.1 Description of the Analysis Results For obtaining the required morpho-syntactic information, the following analyzers for German and English were applied: gertwol and engtwol for lexical analysis and gercg and engcg for morphological and syntactic disambiguation. For a description of the underlying approach, the reader is referred to Karlsson (1990). Tables 1 and 2 give examples of the information provided by these tools. 2.2 Treatment of Ambiguity The examples in Tables 1 and 2 demonstrate the capability of the tools to disambiguate among different readings: For instance, they infer that the word wollen is a verb in the indicative present ﬁrst-person plural form. Without any context taken into account,  183  Computational Linguistics  Volume 30, Number 2  Table 1 Sample analysis of a German sentence. Input: Wir wollen nach dem Abendessen nach Essen aufbrechen. (In English: We want to start for Essen after dinner.)  Original  Base form  Tags  Wir wollen nach dem Abendessen nach Essen aufbrechen  wir wollen nach das Abend#essen nach Essen Esse Essen Essen auf|brechen  personal-pronoun plural ﬁrst nominative verb indicative present plural ﬁrst preposition dative deﬁnite-article singular dative neuter noun neuter singular dative preposition dative noun name neuter singular dative noun feminine plural dative noun neuter plural dative noun neuter singular dative verb separable inﬁnitive  Table 2 Sample analysis of an English sentence. Input: Do we have to reserve rooms?.  Original Base form Tags  Do we have to reserve rooms  do we have to reserve room  verb present not-singular-third ﬁnite auxiliary personal-pronoun nominative plural ﬁrst subject verb inﬁnitive not-ﬁnite main inﬁnitive-marker verb inﬁnitive not-ﬁnite main noun nominative plural object  wollen has other readings. It can even be interpreted as derived from an adjective with the meaning “made of wool.” The inﬂected word forms on the German part of the Verbmobil (cf. Section 7.1.1) corpus have on average 2.85 readings (1.86 for the English corpus), 58% of which can be eliminated by the syntactic analyzers on the basis of sentence context. Common bilingual corpora normally contain full sentences, which provide enough context information for ruling out all but one reading for an inﬂected word form. To reduce the remaining uncertainty, preference rules have been implemented. For instance, it is assumed that the corpus is correctly true-case-converted beforehand, and as a consequence, non-noun readings of uppercase words are dropped. Furthermore, indicative verb readings are preferred to subjunctive or imperative. In addition, some simple domain-speciﬁc heuristics are applied. The reading “plural of Esse” for the German word form Essen, for instance, is much less likely in the domain of appointment scheduling and travel arrangements than the readings “proper name of the town Essen” or the German equivalent of the English word meal. As can be seen in Table 3, the reduction in the number of readings resulting from these preference rules is fairly small in the case of the Verbmobil corpus. The remaining ambiguity often lies in those parts of the information which are not used or which are not relevant to the translation task. For example, the analyzers cannot tell accusative from dative case in German, but the case information is not essential for the translation task (see also Table 4). Section 2.4 describes a method 184  Nießen and Ney  SMT with Scarce Resources  Table 3 Resolution of ambiguity on the Verbmobil corpus.  Disambiguation  Number of readings per word form  German  English  None  2.85  1.86  By context  1.20  1.02  By preference  1.19  1.02  By selecting relevant tags  1.06  1.01  By resorting to unambiguous part 1.00  1.00  for selecting morpho-syntactic tags considered relevant for the translation task, which results in a further reduction in the number of readings per word form to 1.06 for German and 1.01 for English. In these rare cases of ambiguity it is admissible to resort to the unambiguous parts of the readings, that is, to drop all tags causing mixed interpretations. Table 3 summarizes the gradual resolution of ambiguity. The analysis of conventional dictionaries poses some special problems, because they do not provide enough context to enable effective disambiguation. For handling this special situation, dedicated methods have been implemented; these are presented in Section 5.1. 2.3 The Lemma-Tag Representation A full word form is represented by the information provided by the morpho-syntactic analysis: from the interpretation gehen verb indicative present first singular, that is, the base form plus part of speech plus the other tags, the word form gehe can be restored. It has already been mentioned that the analyzers can disambiguate among different readings on the basis of context information. In this sense, the information inherent in the original word forms is augmented by the disambiguating analyzer. This can be useful for choosing the correct translation of ambiguous words. Of course, these disambiguation clues result in an enlarged vocabulary. The vocabulary of the new representation of the German part of the Verbmobil corpus, for example, in which full word forms are replaced by base form plus morphological and syntactic tags (lemmatag representation), is one and a half times as large as the vocabulary of the original corpus. On the other hand, the information in the lemma-tag representation can be accessed gradually and ultimately reduced: For example, certain instances of words can be considered equivalent. This fact is used to better exploit the bilingual training data along two directions: detecting and omitting unimportant information (see Section 2.4) and constructing hierarchical translation models (see Section 4). To summarize, the lemma-tag representation of a corpus has the following main advantages: It makes context information locally available, and it allows information to be explicitly accessed at different levels of abstraction. 2.4 Equivalence Classes of Words with Similar Translation Inﬂected word forms in the input language often contain information that is not relevant for translation. This is especially true for the task of translating from a more inﬂecting language like German into English, for instance: In parallel German/English corpora, the German part contains many more distinct word forms than the English part (see, for example, Table 5). It is useful for the process of statistical machine translation to deﬁne equivalence classes of word forms which tend to be translated by the same target language word: The resulting statistical translation lexicon becomes 185  Computational Linguistics  Volume 30, Number 2  Table 4 Candidates for equivalence classes.  Part of speech Candidates  Noun Verb Adjective Number  Gender (masculine, feminine, neuter) and case (nominative, dative, accusative) Number (singular, plural) and person (ﬁrst, second, third) Gender, case, and number Case  smoother, and the coverage is considerably improved. Such equivalence classes are constructed by omitting those items of information from morpho-syntactic analysis which are not relevant for translation. The lemma-tag representation of the corpus helps to identify the unimportant information. The deﬁnition of relevant and unimportant information, respectively, depends on many factors like the languages involved, the translation direction, and the choice of the models. We detect candidates for equivalence classes of words automatically from the probabilistic lexicon trained for translation from German to English. For this purpose, those inﬂected forms of the same base form which result in the same translation are inspected. For each set of tags T, the algorithm counts how often an additional tag t1 can be replaced with a certain other tag t2 without effect on the translation. As an example, let T = ‘blau-adjective’, t1 =‘masculine’ and t2 =‘feminine’. The two entries (‘blau-adjective-masculine’|‘blue’) and (‘blau-adjective-feminine’|‘blue’) are hints for detecting gender as nonrelevant when translating adjectives into English. Table 4 lists some of the most frequently identiﬁed candidates to be ignored while translating: The gender of nouns is irrelevant for their translation (which is straightforward, as the gender of a noun is unambiguous), as are the cases nominative, dative, accusative. (For the genitive forms, the translation in English differs.) For verbs the candidates number and person were found: The translation of the ﬁrst-person singular form of a verb, for example, is often the same as the translation of the third-person plural form. Ignoring (dropping) those tags most often identiﬁed as irrelevant for translation results in the building of equivalence classes of words. Doing so results in a smaller vocabulary, one about 65.5% the size of the vocabulary of the full lemmatag representation of the Verbmobil corpus, for example—it is even smaller than the vocabulary of the original full-form corpus. The information described in this section is used to improve the quality of statistical machine translation and to better exploit the available bilingual resources. 3. Treatment of Structural Differences Difference in sentence structure is one of the main sources of errors in machine translation. It is thus promising to “harmonize” the word order in corresponding sentences. The presentation in this section focuses on the following aspects: question inversion and separated verb preﬁxes. For a more detailed discussion of restructuring for statistical machine translation the reader is referred to Nießen and Ney (2000, 2001). 3.1 Question Inversion In many languages, the sentence structure of questions differs from the structure in declarative sentences in that the order of the subject and the corresponding ﬁnite verb is inverted. From the perspective of statistical translation, this behavior has some dis- 186  Nießen and Ney  SMT with Scarce Resources  advantages: The algorithm for training the parameters of the target language model Pr(eI1), which is typically a standard n-gram model, cannot deduce the probability of a word sequence in an interrogative sentence from the corresponding declarative form. The same reasoning is valid for the lexical translation probabilities of multiwordphrase pairs. To harmonize the word order of questions with the word order in declarative sentences, the order of the subject (including the appendant articles, adjectives etc.) and the corresponding ﬁnite verb is inverted. In English questions supporting dos are removed. The application of the described preprocessing step in the bilingual training corpus implies the necessity of restoring the correct forms of the translations produced by the machine translation algorithm. This procedure was suggested by Brown et al. (1992) for the language pair English and French, but they did not report on experimental results revealing the effect of the restructuring on the translation quality. 3.2 Separated Verb Preﬁxes German preﬁx verbs consist of a main part and a detachable preﬁx, which can be shifted to the end of the clause. For the automatic alignment process, it is often difﬁcult to associate one English word with more than one word in the corresponding German sentence, namely, the main part of the verb and the separated preﬁx. To solve the problem of separated preﬁxes, all separable word forms of verbs are extracted from the training corpus. The resulting list contains entries of the form prefix|main. In all clauses containing a word matching a main part and a word matching the corresponding preﬁx part occurring at the end of the clause, the preﬁx is prepended to the beginning of the main part. 4. Hierarchical Lexicon Models In general, the probabilistic lexicon resulting from training the translation model contains all word forms occurring in the training corpus as separate entries, not taking into account whether or not they are inﬂected forms of the same lemma. Bearing in mind that typically more than 40% of the word forms are seen only once in training (see, for example, Table 5), it is obvious that for many words, learning the correct translations is difﬁcult. Furthermore, new input sentences are expected to contain unknown word forms, for which no translation can be retrieved from the lexicon. This problem is especially relevant for more-inﬂecting languages like German: Texts in German contain many more distinct word forms than their English translations. Table 5 also reveals that these words are often generated via inﬂection from a smaller set of base forms. 4.1 A Hierarchy of Equivalence Classes of Inﬂected Word Forms As mentioned in Section 2.3, the lemma-tag representation of the information from morpho-syntactic analysis makes it possible to gradually access information with different grades of abstraction. Consider, for example, the German verb form ankomme, which is the indicative present ﬁrst-person singular form of the lemma ankommen and can be translated into English by arrive. The lemma-tag representation provides an “observation tuple” consisting of • the original full word form (e.g., ankomme), • morphological and syntactic tags (part of speech, tense, person, case, . . . ) (e.g., verb, indicative, present tense, 1st person singular), and  187  Computational Linguistics  Volume 30, Number 2  • the base form (e.g., ankommen).  In the following, ti0 = t0, . . . , ti denotes the representation of a word where the base form t0 and i additional tags are taken into account. For the example above, t0 = ankommen, t1 = verb, and so on. The hierarchy of equivalence classes F0, . . . , Fn is as follows: Fn = F (tn0) = ankommen verb indicative present singular 1 Fn−1 = F (tn0−1) = ankommen verb indicative present singular Fn−2 = F (tn0−2) = ankommen verb indicative present ... F0 = F (t0) = ankommen where n is the maximum number of morpho-syntactic tags. The mapping from the full lemma-tag representation back to inﬂected word forms is generally unambiguous; thus Fn contains only one element, namely, ankomme. Fn−1 contains the forms ankomme, ankommst, and ankommt; in Fn−2 the number (singular or plural) is ignored, and so on. The largest equivalence class contains all inﬂected forms of the base form ankommen.1 Section 4.2 introduces the concept of combining information at different levels of abstraction.  4.2 Log-Linear Combination In modeling for statistical machine translation, a hidden variable aJ1, denoting the hidden alignment between the words in the source and target languages, is usually introduced into the string translation probability:  Pr(f1J|eI1) =  Pr(f1J, aJ1|eI1) = Pr(aJ1|eI1) · Pr(f1J|aJ1, eI1)  (1)  aJ1  aJ1  In the following, Tj = tn0 j denotes the lemma-tag representation of the jth word in the input sentence. The sequence T1J stands for the sequence of readings for the word sequence f1J and can be introduced as a new hidden variable:  Pr(f1J|aJ1, eI1) =  Pr(f1J, T1J |aJ1, eI1)  (2)  T1J  which can be decomposed into  J  Pr(f1J|aJ1, eI1) =  Pr(fj, Tj|f1j−1, T1j−1, aJ1, eI1)  (3)  T1J j=1  
Enrique Vidal∗ Universidad Polite´cnica de Valencia  Finite-state transducers are models that are being used in different areas of pattern recognition and computational linguistics. One of these areas is machine translation, in which the approaches that are based on building models automatically from training examples are becoming more and more attractive. Finite-state transducers are very adequate for use in constrained tasks in which training samples of pairs of sentences are available. A technique for inferring ﬁnite-state transducers is proposed in this article. This technique is based on formal relations between ﬁnite-state transducers and rational grammars. Given a training corpus of source-target pairs of sentences, the proposed approach uses statistical alignment methods to produce a set of conventional strings from which a stochastic rational grammar (e.g., an n-gram) is inferred. This grammar is ﬁnally converted into a ﬁnite-state transducer. The proposed methods are assessed through a series of machine translation experiments within the framework of the EuTrans project. 1. Introduction Formal transducers give rise to an important framework in syntactic-pattern recognition (Fu 1982; Vidal, Casacuberta, and Garc´ıa 1995) and in language processing (Mohri 1997). Many tasks in automatic speech recognition can be viewed as simple translations from acoustic sequences to sublexical or lexical sequences (acoustic-phonetic decoding) or from acoustic or lexical sequences to query strings (for database access) or (robot control) commands (semantic decoding) (Vidal, Casacuberta, and Garc´ıa 1995; Vidal 1997; Bangalore and Ricardi 2000a, 2000b; Hazen, Hetherington, and Park 2001; Mou, Seneff, and Zue 2001; Segarra et al. 2001; Seward 2001). Another similar application is the recognition of continuous hand-written characters (Gonza´lez et al. 2000). Yet a more complex application of formal transducers is language translation, in which input and output can be text, speech, (continuous) handwritten text, etc. (Mohri 1997; Vidal 1997; Bangalore and Ricardi 2000b, 2001; Amengual et al. 2000). Rational transductions (Berstel 1979) constitute an important class within the ﬁeld of formal translation. These transductions are realized by the so-called ﬁnite-state transducers. Even though other, more powerful transduction models exist, ﬁnite-state transducers generally entail much more affordable computational costs, thereby making these simpler models more interesting in practice. One of the main reasons for the interest in ﬁnite-state machines for language translation comes from the fact that these machines can be learned automatically from examples (Vidal, Casacuberta, and Garc´ıa 1995). Nowadays, only a few techniques exist for inferring ﬁnite-state transducers (Vidal, Garc´ıa, and Segarra 1989; Oncina,  ∗ Departamento de Sistemas Informa´ticos y Computacio´ n, Instituto Tecnolo´ gico de Informa´tica, 46071 Valencia, Spain. E-mail:{fcn, evidal}@iti.upv.es. c 2004 Association for Computational Linguistics  Computational Linguistics  Volume 30, Number 2  Garc´ıa, and Vidal 1993; Ma¨kinen 1999; Knight and Al-Onaizan 1998; Bangalore and Ricardi 2000b; Casacuberta 2000; Vilar 2000). Nevertheless, there are many techniques for inferring regular grammars from ﬁnite sets of learning strings which have been used successfully in a number of ﬁelds, including automatic speech recognition (Vidal, Casacuberta, and Garc´ıa 1995). Some of these techniques are based on results from formal language theory. In particular, complex regular grammars can be built by inferring simple grammars that recognize local languages (Garc´ıa, Vidal, and Casacuberta 1987). Here we explore this idea further and propose methods that use (simple) ﬁnitestate grammar learning techniques, such as n-gram modeling, to infer rational transducers which prove adequate for language translation. The organization of the article is as follows. Sections 2 and 3 give the basic deﬁnitions of a ﬁnite-state transducer and the corresponding stochastic extension, presented within the statistical framework of language translation. In Section 4, the proposed method for inferring stochastic ﬁnite-state transducers is presented. The experiments are described in Section 5. Finally, Section 6 is devoted to general discussion and conclusions.  2. Finite-State Transducers  A ﬁnite-state transducer, T , is a tuple Σ, ∆, Q, q0, F, δ , in which Σ is a ﬁnite set of source symbols, ∆ is a ﬁnite set of target symbols (Σ ∩ ∆ = ∅), Q is a ﬁnite set of states, q0 is the initial state, F ⊆ Q is a set of ﬁnal states, and δ ⊆ Q × Σ × ∆ × Q is a set of transitions.1 A translation form φ of length I in T is deﬁned as a sequence of transitions:  φ = (qφ0 , sφ1 ,¯tφ1 , qφ1 )(qφ1 , sφ2 ,¯tφ2 , qφ2 )(qφ2 , sφ3 ,¯tφ3 , qφ3 ) . . . (qφI−1, sφI ,¯tφI , qφI )  (1)  where (qφi−1, sφi ,¯tφi , qφi ) ∈ δ, qφ0 = q0, and qφI ∈ F. A pair (s, t) ∈ Σ × ∆ is a translation pair if there is a translation form φ of length I in T such that I =| s | and t = ¯tφ1¯tφ2 . . .¯tφI . By d(s, t) we will denote the set of translation forms2 in T associated with the pair (s, t). A rational translation is the set of all translation pairs of some ﬁnite-state transducer T . This deﬁnition of a ﬁnite-state transducer is similar to the deﬁnition of a regular or ﬁnite-state grammar G. The main difference is that in a ﬁnite-state grammar, the set of target symbols ∆ does not exist, and the transitions are deﬁned on Q × Σ × Q. A translation form is the transducer counterpart of a derivation in a ﬁnite-state grammar, and the concept of rational translation is reminiscent of the concept of (regular) language, deﬁned as the set of strings associated with the derivations in the grammar G. Rational translations exhibit many properties similar to those shown for regular languages (Berstel 1979). One of these properties can be stated as follows (Berstel 1979):  Theorem 1 T ⊆ Σ × ∆ is a rational translation if and only if there exist an alphabet Γ, a regular language L ⊂ Γ , and two morphisms hΣ : Γ → Σ and h∆ : Γ → ∆ , such that T = {(hΣ(w), h∆(w)) | w ∈ L}.  
Cong Li∗ Microsoft Research Asia  This article proposes a new method for word translation disambiguation, one that uses a machinelearning technique called bilingual bootstrapping. In learning to disambiguate words to be translated, bilingual bootstrapping makes use of a small amount of classiﬁed data and a large amount of unclassiﬁed data in both the source and the target languages. It repeatedly constructs classiﬁers in the two languages in parallel and boosts the performance of the classiﬁers by classifying unclassiﬁed data in the two languages and by exchanging information regarding classiﬁed data between the two languages. Experimental results indicate that word translation disambiguation based on bilingual bootstrapping consistently and signiﬁcantly outperforms existing methods that are based on monolingual bootstrapping.  1. Introduction  We address here the problem of word translation disambiguation. If, for example, we were to attempt to translate the English noun plant, which could refer either to a type  of factory or to a form of ﬂora (i.e., in Chinese, either to  [gongchang] or to  [zhiwu]), our goal would be to determine the correct Chinese translation. That is, word  translation disambiguation is essentially a special case of word sense disambiguation  (in the above example, gongchang would correspond to the sense of factory and zhiwu  to the sense of ﬂora).1  We could view word translation disambiguation as a problem of classiﬁcation. To  perform the task, we could employ a supervised learning method, but since to do  so would require human labeling of data, which would be expensive, bootstrapping  would be a better choice.  Yarowsky (1995) has proposed a bootstrapping method for word sense disam-  biguation. When applied to translation from English to Chinese, his method starts  learning with a small number of English sentences that contain ambiguous English  words and that are labeled with correct Chinese translations of those words. It then  uses these classiﬁed sentences as training data to create a classiﬁer (e.g., a decision list),  which it uses to classify unclassiﬁed sentences containing the same ambiguous words.  The output of this process is then used as additional training data. It also adopts the  one-sense-per-discourse heuristic (Gale, Church, and Yarowsky 1992b) in classifying  unclassiﬁed sentences. By repeating the above process, an accurate classiﬁer for word  translation disambiguation can be created. Because this method uses data in a single  language (i.e., the source language in translation), we refer to it here as monolingual  bootstrapping (MB).  ∗ 5F Sigma Center, No. 49 Zhichun Road, Haidian, Beijing, China, 100080. E-mail:{hangli,i-congl}@ microsoft.com. 
Most other computational metaphor systems use small, hand-coded semantic knowledge bases and work on a few examples. Although CorMet’s only knowledge base is WordNet (Fellbaum 1998) it can ﬁnd the mappings constituting many conventional metaphors and in some cases recognize sentences instantiating those mappings. CorMet is tested on its ability to ﬁnd a subset of the Master Metaphor List (Lakoff, Espenson, and Schwartz 1991). 1. Introduction Lakoff (1993) argues that rather than being a rare form of creative language, some metaphors are ubiquitous, highly structured, and relevant to cognition. To date, there has been no robust, broadly applicable computational metaphor interpretation system, a gap this article is intended to take a ﬁrst step toward ﬁlling. Most computational models of metaphor depend on hand-coded knowledge bases and work on a few examples. CorMet is designed to work on a larger class of metaphors by extracting knowledge from large corpora without drawing on any handcoded knowledge sources besides WordNet. A method for computationally interpreting metaphorical language would be useful for NLP. Although metaphorical word senses can be cataloged and treated as just another part of the lexicon, this kind of representation ignores regularities in polysemy. A conventional metaphor may have a very large number of linguistic manifestations, which makes it useful to model the metaphor’s underlying mechanisms. CorMet is not capable of interpreting any manifestation of conventional metaphor but is a step toward such a system. CorMet analyzes large corpora of domain-speciﬁc documents and learns the selectional preferences of the characteristic verbs of each domain. A selectional preference is a verb’s predilection for a particular type of argument in a particular role. For instance, the object of the verb pour is generally a liquid. Any noun that pour takes as an ∗ Computer Science Department, Waltham, MA 02134. E-mail: zmason@amazon.com. c 2004 Association for Computational Linguistics  Computational Linguistics  Volume 30, Number 1  an object is likely to be intended as a liquid, either metaphorically or literally. CorMet ﬁnds conventional metaphors by ﬁnding systematic differences in selectional preferences between domains. For instance, if CorMet were to ﬁnd a sentence like Funds poured into his bank account in a document from the FINANCE domain, it could infer that in that domain, pour has a selection preference for ﬁnancial assets in its subject. By comparing this selectional preference with pour’s selectional preferences in the LAB domain, CorMet can infer a metaphorical mapping from money to liquids. By ﬁnding sets of co-occuring interconcept mappings (like the above mapping and a mapping from investments to containers, for instance), Cormet can articulate the higher-order structure of conceptual metaphors. Note that Cormet is designed to detect higherorder conceptual metaphors by ﬁnding some of the sentences embodying some of the interconcept mappings constituting the metaphor of interest but is not designed to be a tool for reliably detecting all instances of a particular metaphor. CorMet’s domain-speciﬁc corpora are obtained from the Internet. In this context, a domain is a set of related concepts, and a domain-speciﬁc corpus is a set of documents relevant to those concepts. CorMet’s input parameters are two domains between which to search for interconcept mappings and, for each domain, a set of characteristic keywords. CorMet is tested on its ability to ﬁnd a subset of the Master Metaphor List (Lakoff, Espenson, and Schwartz 1991), a manually compiled catalog of metaphor. CorMet works on domains that are speciﬁc and concrete (e.g., the domain of ﬁnance, but not that of actions). CorMet’s discrimination is relatively coarse: It measures trends in selectional preferences across many documents, so common mappings are discernible. CorMet considers the selectional preferences only of verbs, on the theory that they are generally more selectively restrictive than nouns or adjectives. It is worth noting that WordNet, CorMet’s primary knowledge source, implicitly encodes some of the metaphors CorMet is intended to ﬁnd; Peters and Peters (2000) use WordNet to ﬁnd many artifact/cognition metaphors. Also, WordNet enumerates some metaphorical senses of some verbs. CorMet does not use any of WordNet’s information about verbs and ignores regularities in the distribution of noun homonyms that could be used to ﬁnd some metaphors. The article is organized as follows: Section 2 describes the mechanisms by which conventional metaphors are detected. Section 3 walks through CorMet’s process in two examples. Section 4 describes how the system’s performance is evaluated against the Master Metaphor List (Lakoff, Espenson, and Schwartz 1991), and Section 5 covers select related work. 2. The Metaphor Extraction Engine 2.1 Searching the Net for Domain Corpora Ideally, CorMet could draw on a large quantity of manually vetted, highly representative domain-speciﬁc documents. The precompiled corpora available on-line (Kucera 1992; Marcus, Santorini, and Marcinkiewicz 1993) do not span enough subjects. Other on-line data sources include the Internet’s hierarchically structured indices, such as Yahoo’s ontology (www.yahoo.com) and Google’s (www.google.com). Each index entry contains a small number of high-quality links to relevant Web pages, but this is not helpful, because CorMet requires many documents, and those documents need not be of more than moderate quality. Searching the Internet for domain-speciﬁc text seems to be the only way to obtain sufﬁciently large, diverse corpora. CorMet obtains documents by submitting queries to the Google search engine. There are two types of queries: one to fetch any domain-speciﬁc documents and an-  24  Mason  CorMet  other to fetch domain-speciﬁc documents that contain a particular verb. The ﬁrst kind of query consists of a conjunction of from two to ﬁve randomly selected domain keywords. Domain keywords are words characteristic of a domain, supplied by the user as an input. For the FINANCE domain, a reasonable set of keywords is stocks, bonds, NASDAQ, Dow, investment, ﬁnance. Each query incorporates only a few keywords in order to maximize the number of distinct possible queries. Queries for domain-speciﬁc documents containing a particular verb are composed of a conjunction of domain-speciﬁc terms and a disjunction of forms of the verb that are more likely to be verbs than other parts of speech. For the verb attack, for instance, acceptable forms are attacked and attacking, but not attack and attacks, which are more likely to be nouns. The syntactic categories in which a word form appears are determined by reference to WordNet. Some queries for the verb attack in the FINANCE domain are: 1. (attacked OR attacking) AND (bonds AND Dow AND investment) 2. (attacked OR attacking) AND (NASDAQ AND investment AND ﬁnance) 3. (attacked OR attacking) AND (stocks AND bonds AND NASDAQ) 4. (attacked OR attacking) AND (stocks AND NASDAQ AND Dow) Queries return links to up to 10,000 documents, of which CorMet fetches and analyzes no more than 3,000. In the 13 domains studied, about 75% of these documents are relevant to the domain of interest (as measured through a randomly chosen, handevaluated sample of 100 documents per domain), so the noise is substantial. The documents are processed to remove embedded scripts and HTML tags. The mined documents are parsed with the apple pie parser (Sekine and Grishman 1995). Case frames are extracted from parsed sentences using templates; for instance, (S (NP & OBJ) (VP (were | was | got | get) (VP WORDFORM-PASSIVE)) is used to extract roles for passive, agentless sentences (where WORDFORM-PASSIVE is replaced by a passive form of the verb under analysis). 2.2 Finding Characteristic Predicates Learning the selectional preferences for a verb in a domain is expensive in terms of time, so it is useful to ﬁnd a small set of important verbs in each domain. CorMet seeks information about verbs typical of a domain, because these verbs are more likely to ﬁgure in metaphors in which that domain is the metaphor’s source. Besiege, for instance, is characteristic of the MILITARY domain and appears in many instances of the MILITARY → MEDICINE mapping, such as The antigens besieged the virus. To ﬁnd domain-characteristic verbs, CorMet dynamically obtains a large sample of domain-relevant documents, decomposes them into a bag-of-words representation, stems the words with an implementation of the Porter (1980) stemmer, and ﬁnds the ratio of occurrences of each word stem to the total number of stems in the domain corpus. The frequency of each stem in the corpus is compared to its frequency in general English (as recorded in an English-language frequency dictionary [Kilgarriff 2003]). The 400 verb stems with the highest relative frequency (computed as a ratio of the stem’s frequency in the domain to its frequency in the English frequency dictionary) are considered characteristic. CorMet treats any word form that may be a verb (according to WordNet) as though it is a verb, which biases CorMet toward verbs with common nominal homonyms. Word stems that have high relative frequency in more than one  25  Computational Linguistics  Volume 30, Number 1  Table 1 Characteristic stems for LAB and FINANCE domains.  Rank  LAB  FINANCE  
Chris Brew† Ohio State University  Levin’s (1993) study of verb classes is a widely used resource for lexical semantics. In her framework, some verbs, such as give, exhibit no class ambiguity. But other verbs, such as write, have several alternative classes. We extend Levin’s inventory to a simple statistical model of verb class ambiguity. Using this model we are able to generate preferences for ambiguous verbs without the use of a disambiguated corpus. We additionally show that these preferences are useful as priors for a verb sense disambiguator. 1. Introduction Much research in lexical semantics has concentrated on the relation between verbs and their arguments. Many scholars hypothesize that the behavior of a verb, particularly with respect to the expression and interpretation of its arguments, is to a large extent determined by its meaning (Talmy 1985; Jackendoff 1983; Goldberg 1995; Levin 1993; Pinker 1989; Green 1974; Gropen et al. 1989; Fillmore 1965). The correspondence between verbal meaning and syntax has been extensively studied in Levin (1993), which argues that verbs which display the same diathesis alternations—alternations in the realization of their argument structure—can be assumed to share certain meaning components and to form a semantically coherent class. The converse of this assumption is that verb behavior (i.e., participation in diathesis alternations) can be used to provide clues about aspects of meaning, which in turn can be exploited to characterize verb senses (referred to as classes in Levin’s [1993] terminology). A major advantage of this approach is that criteria for assigning senses can be more concrete than is traditionally assumed in lexicographic work (e.g., WordNet or machine-readable dictionaries) concerned with sense distinctions (Palmer 2000). As an example consider sentences (1)–(4), taken from Levin. Examples (1) and (2) illustrate the dative and benefactive alternations, respectively. Dative verbs alternate between the prepositional frame “NP1 V NP2 to NP3” (see (1a)) and the double-object frame “NP1 V NP2 NP3” (see (1b)), whereas benefactive verbs alternate between the doubleobject frame (see (2a)) and the prepositional frame “NP1 V NP2 for NP3” (see (2b)). To decide whether a verb is benefactive or dative it sufﬁces to test the acceptability of the for and to frames. Verbs undergoing the conative alternation can be attested either as transitive or as intransitive with a prepositional phrase headed by the word at.1 The role ﬁlled by the object of the transitive variant is shared by the noun phrase complement of at in the intransitive variant (see (3)). This example makes explicit that class assignment depends not only on syntactic facts but also on judgments about ∗ Department of Computer Science, Regent Court, 211 Portobello Street, Shefﬁeld, S1 4DP, UK. E-mail: mlap@dcs.shef.ac.uk. † Department of Linguistics, Oxley Hall,1712 Neil Avenue, Columbus, OH. E-mail: cbrew@ling.ohiostate.edu. 
City University of Hong Kong  Kang Chen† Tsinghua University Weimin Zheng† Tsinghua University  We are interested in the problem of word extraction from Chinese text collections. We deﬁne a  word to be a meaningful string composed of several Chinese characters. For example,  ,  ‘percent’, and  , ‘more and more’, are not recognized as traditional Chinese words from the  viewpoint of some people. However, in our work, they are words because they are very widely used  and have speciﬁc meanings. We start with the viewpoint that a word is a distinguished linguistic  entity that can be used in many different language environments. We consider the characters  that are directly before a string (predecessors) and the characters that are directly after a string  (successors) as important factors for determining the independence of the string. We call such  characters accessors of the string, consider the number of distinct predecessors and successors of a  string in a large corpus (TREC 5 and TREC 6 documents), and use them as the measurement of the  context independency of a string from the rest of the sentences in the document. Our experiments  conﬁrm our hypothesis and show that this simple rule gives quite good results for Chinese word  extraction and is comparable to, and for long words outperforms, other iterative methods.  1. Introduction  Words are the basic linguistic units of natural language processing. The importance of word extraction is stressed in many papers. According to Huang, Chen, and Tsou (1996), the word is the basic unit in natural language processing (NLP), as it is at the lexical level where all modules interface. Possible modules involved are the lexicon, speech recognition, syntactic parsing, speech synthesis, semantic interpretation, and so on. Thus, the identiﬁcation of lexical words and/or the delimitation of words in running texts is a prerequisite of NLP. Teahan et al. (2000) state that interpreting a text as a sequence of words is beneﬁcial for some information retrieval and storage tasks: for example, full-text searches, word-based compression, and key-phrase extraction. According to Guo (1997), words and tokens are the primary building blocks in almost all linguistic theories and language-processing systems, including Japanese (Kobayasi, Tokumaga, and Tanaka 1994), Korean (Yun, Lee, and Rim 1995), German (Pachunke et al. 1992), and English (Garside, Leech, and Sampson 1987), in various media, such  ∗ School of Computer Science and Technology, Jinan, PRC; Department of Computer Science, Tat Chee Avenue, Kowloon, Hong Kong. E-mail: fenghd@cs.cityu.edu.hk or fenghaodi@hotmail.com. † Department of Computer Science and Technology, Peking, PR China. E-mail: {ck99,zwm-dcs}@mails. tsinghua.edu.cn. ‡ Department of Computer Science, Tat Chee Avenue, Kowloon, Hong Kong. E-mail: csdeng@cityu. edu.hk. c 2004 Association for Computational Linguistics  Computational Linguistics  Volume 30, Number 1  as continuous speech and cursive handwriting, and in numerous applications, such  as translation, recognition, indexing, and proofreading. The identiﬁcation of words in  natural language is nontrivial since, as observed by Chao (1968), linguistic words often  represent a different set than do sociological words.  Chinese texts are character based, not word based. Each Chinese character stands  for one phonological syllable and in most cases represents a morpheme. This presents  a problem, as only less than 10% of the word types (and less than 50% of the tokens  in a text) in Chinese are composed of a single character (Chen et al. 1993). However,  Chinese texts, and texts in some other Oriental languages such as Japanese, do not  have delimiters such as spaces to mark the boundaries of meaningful words. Even for  English text, some phrases consist of several words. However, the problem in English  is not as dominant a factor as in Chinese. How to extract words from Chinese texts is  still an interesting problem. Note that word extraction is different from the very closely  related problem of sentence segmentation. Word extraction aims to collect all of the  meaningful strings in a text. Sentence segmentation partitions a sentence into several  consecutive meaningful segments. Word extraction should be easier than sentence  segmentation, and the problems involved in it can be solved using simpler methods.  Some Chinese information-retrieval systems operate at the character level instead  of the word level, for example, the Csmart system (Chien 1995). However, to further  improve the efﬁciency of natural Chinese processing, it is commonly thought to be  important to apply studies from linguistics (Kwok 1997). Lexicon construction is con-  sidered to be one of the most important tasks. Single Chinese characters can quite  often carry different meanings. This ambiguity can be resolved when the characters  are combined with other characters to form a word. Chinese words can be unigrams,  bigrams, trigrams, or n-grams, where n > 3. According to the Frequency Dictionary  of Modern Chinese (Beijing Language Institute 1986), among the 9,000 most frequent  Chinese words, 26.7% are unigrams, 69.8% are bigrams, 2.7% are trigrams, 0.007% are  four-grams, and 0.002% are ﬁve-grams. There are lexicons for identifying some (and  probably most of the frequent) words. However, sometimes less-frequent words are  more effective. Weeber, Vos, and Baayen (2000) recently extracted side-effect-related  terms in a medical-information extraction system and found that many of the terms  had a frequency of less than ﬁve. This indicates that low-frequency words may also  carry very important information. Our experiments show that we can extract low-  frequency words using a simple method without overly degrading the precision.  There are generally two directions in which words can be formed (Huang, Chen,  and Tsou 1996). One is the deductive strategy, whereby words are identiﬁed through  the segmentation of running texts. The other is the inductive strategy, which identiﬁes  words through the compositional process of morpho-lexical rules. This strategy repre-  sents words with common characteristics (e.g., numeric compounds) by rules. In Chi-  nese text segmentation there are three basic approaches (Sproat et al. 1996): pure heuris-  tic, pure statistical, and a hybrid of the two. The heuristic approach identiﬁes words  by applying prior knowledge or morpho-lexical rules governing the derivation of new  words. The statistical approach identiﬁes words based on the distribution of their com-  ponents in a large corpus. Sproat and Shih (1990) develop a purely statistical method  that  utilizes  the  mutual  information  between  two  characters:  I(x, y)  =  log  p(x,y) p(x)p(y)  ;  the  limitation of the method is that it can deal only with words of length two charac-  ters. Ge, Pratt, and Smyth (1999) introduce a simple probabilistic model based on the  occurrence probability of the words that constitute a set of predeﬁned assumptions.  Chien (1997) develops a PAT-tree-based method that extracts signiﬁcant words by ob-  serving mutual information of two overlapped patterns with the signiﬁcance function  76  Feng, Chen, Deng, and Zheng  Accessor Variety Criteria for Chinese Word Extraction  SEc  =  Pr(c) Pr(a)+Pr(b)−Pr(c)  ,  where  a  and  b  are  the  two  biggest  substrings  of  string  c.  Zhang,  Gao, and Zhou (2000) propose the application of a statistical method that is based on  context dependence and mutual information. Yamamoto and Church (2001) experi-  ment with both mutual information and residual inverse document frequency (RIDF)1  as criteria for deciding Japanese words, and their main contribution is in affording  a reduced method for computing term and document frequency. In almost all of the  work cited to this point, the dimension that is used to compute mutual information is  term frequency. Chen and Bai (1998) propose a corpus-based learning approach that  learns grammatical rules and automatically evaluates them. Chang and Su (1997) use  an unsupervised Viterbi training process to select potential unknown words and iter-  atively truncate unlikely unknown words in the augmented dictionary. Teahan et al.  (2000) propose a compression-based algorithm for Chinese text segmentation. Paola  and Stevenson (2001) demonstrate an effective combination of deeper linguistic knowl-  edge with the robustness and scalability of a statistical technique to derive knowledge  about thematic relations for verb classiﬁcation. Mo et al. (1996) deal with the iden-  tiﬁcation of the determinative-measure compounds in parsing Mandarin Chinese by  developing grammatical rules to combine determinators and measures.  We introduce another concept, accessor variety (AV) (for a detailed deﬁnition, refer  to subsection 3.1), to describe the extent to which a string is likely to be a meaning-  ful word. Actually, Harris (1970) uses similar criteria to determine English morpheme  boundaries, and our work is partially motivated by his success. We ﬁrst discard those  strings with accessor varieties that are smaller than a certain number (called the thresh-  old; see subsequent discussion). The remaining strings are considered to be potentially  meaningful words. In addition, we apply rules to remove strings that consist of a word  and adhesive characters (clariﬁed in subsection 3.2). Our experiment shows that even  for small thresholds, quite good results can be obtained.  In Section 2, we introduce examples of unknown words, the identiﬁcation of which  is the task of our work. In Section 3, we discuss our method. In Section 4, we present  our experimental results. We conclude our work with a discussion and a comparison to  previous results in Section 5. In Section 6, we list some future work that can be pursued  following the concept of AV. We note that although our method is quite simple, it is  marginally better than previous comparable results. This method distinguishes itself  from statistically based approaches and grammatical rules. Because of its simplicity, it  can be used easily in computer-based applications. Moreover, innovative variations of  our method and its combination with statistical methods and grammatical methods  are worthy of further exploration.  2. Unknown Words  As deﬁned by Chen and Bai (1998), unknown words are words that are not listed  in an ordinary dictionary, and word extraction seeks to identify such words. To give  readers an intuitive view of these words, we list the types of unknown words that most  frequently appear (Chen and Bai [1998] list 14 different types). What we should point  out here is that except for numeric-type compounds, which are extracted separately,  we extract all the other types of words together.  1. Proper names. These include acronyms, Chinese names, and those words that  have been borrowed from other languages: for example, , ‘Bank of China’;  ,  
 Michael Glass† Valparaiso University  In recent years, the kappa coefﬁcient of agreement has become the de facto standard for evaluating intercoder agreement for tagging tasks. In this squib, we highlight issues that affect κ and that the community has largely neglected. First, we discuss the assumptions underlying different computations of the expected agreement component of κ. Second, we discuss how prevalence and bias affect the κ measure.  In the last few years, coded corpora have acquired an increasing importance in ev-  ery aspect of human-language technology. Tagging for many phenomena, such as  dialogue acts (Carletta et al. 1997; Di Eugenio et al. 2000), requires coders to make  subtle distinctions among categories. The objectivity of these decisions can be as-  sessed by evaluating the reliability of the tagging, namely, whether the coders reach  a satisfying level of agreement when they perform the same coding task. Currently,  the de facto standard for assessing intercoder agreement is the κ coefﬁcient, which  factors out expected agreement (Cohen 1960; Krippendorff 1980). κ had long been  used in content analysis and medicine (e.g., in psychiatry to assess how well stu-  dents’ diagnoses on a set of test cases agree with expert answers) (Grove et al. 1981).  Carletta (1996) deserves the credit for bringing κ to the attention of computational  linguists.  κ  is  computed  as  P(A) − P(E) 1 − P(E)  ,  where  P(A)  is  the  observed  agreement  among  the  coders, and P(E) is the expected agreement, that is, P(E) represents the probabil-  ity that the coders agree by chance. The values of κ are constrained to the inter-  val [−1, 1]. A κ value of one means perfect agreement, a κ value of zero means  that agreement is equal to chance, and a κ value of negative one means “perfect”  disagreement.  This squib addresses two issues that have been neglected in the computational  linguistics literature. First, there are two main ways of computing P(E), the expected  agreement, according to whether the distribution of proportions over the categories  is taken to be equal for the coders (Scott 1955; Fleiss 1971; Krippendorff 1980; Siegel  and Castellan 1988) or not (Cohen 1960). Clearly, the two approaches reﬂect different  conceptualizations of the problem. We believe the distinction between the two is often  glossed over because in practice the two computations of P(E) produce very similar  outcomes in most cases, especially for the highest values of κ. However, ﬁrst, we  will show that they can indeed result in different values of κ, that we will call κCo  (Cohen 1960) and κS&C (Siegel and Castellan 1988). These different values can lead to contradictory conclusions on intercoder agreement. Moreover, the assumption of  ∗ Computer Science, 1120 SEO (M/C 152), 851 South Morgan Street, Chicago, IL 60607. E-mail: bdieugen@uic.edu. † Mathematics and Computer Science, 116 Gellerson Hall, Valparaiso, IN 46383. E-mail: michael.glass@ valpo.edu. c 2004 Association for Computational Linguistics  Computational Linguistics  Volume 30, Number 1  equal distributions over the categories masks the exact source of disagreement among the coders. Thus, such an assumption is detrimental if such systematic disagreements are to be used to improve the coding scheme (Wiebe, Bruce, and O’Hara 1999). Second, κ is affected by skewed distributions of categories (the prevalence problem) and by the degree to which the coders disagree (the bias problem). That is, for a ﬁxed P(A), the values of κ vary substantially in the presence of prevalence, bias, or both. We will conclude by suggesting that κCo is a better choice than κS&C in those studies in which the assumption of equal distributions underlying κS&C does not hold: the vast majority, if not all, of discourse- and dialogue-tagging efforts. However, as κCo suffers from the bias problem but κS&C does not, κS&C should be reported too, as well as a third measure that corrects for prevalence, as suggested in Byrt, Bishop, and Carlin (1993). 1. The Computation of P(E) P(E) is the probability of agreement among coders due to chance. The literature describes two different methods for estimating a probability distribution for random assignment of categories. In the ﬁrst, each coder has a personal distribution, based on that coder’s distribution of categories (Cohen 1960). In the second, there is one distribution for all coders, derived from the total proportions of categories assigned by all coders (Scott 1955; Fleiss 1971; Krippendorff 1980; Siegel and Castellan 1988).1 We now illustrate the computation of P(E) according to these two methods. We will then show that the resulting κCo and κS&C may straddle one of the signiﬁcant thresholds used to assess the raw κ values. The assumptions underlying these two methods are made tangible in the way the data are visualized, in a contingency table for Cohen, and in what we will call an agreement table for the others. Consider the following situation. Two coders2 code 150 occurrences of Okay and assign to them one of the two labels Accept or Ack(nowledgement) (Allen and Core 1997). The two coders label 70 occurrences as Accept, and another 55 as Ack. They disagree on 25 occurrences, which one coder labels as Ack, and the other as Accept. In Figure 1, this example is encoded by the top contingency table on the left (labeled Example 1) and the agreement table on the right. The contingency table directly mirrors our description. The agreement table is an N × m matrix, where N is the number of items in the data set and m is the number of labels that can be assigned to each object; in our example, N = 150 and m = 2. Each entry nij is the number of codings of label j to item i. The agreement table in Figure 1 shows that occurrences 1 through 70 have been labeled as Accept by both coders, 71 through 125 as Ack by both coders, and 126 to 150 differ in their labels.  
This paper addresses the documentation of large-scale grammars.1 We argue that grammar implementation differs from ordinary software programs: the concept of modules, as known from software engineering, cannot be transferred directly to grammar implementations, due to grammar-speciﬁc properties. These properties also put special constraints on the form of grammar documentation. To fulﬁll these constraints, we propose an XML-based, grammar-speciﬁc documentation technique. 
We present a novel algorithm for Japanese dependency analysis. The algorithm allows us to analyze dependency structures of a sentence in linear-time while keeping a state-of-the-art accuracy. In this paper, we show a formal description of the algorithm and discuss it theoretically with respect to time complexity. In addition, we evaluate its efﬁciency and performance empirically against the Kyoto University Corpus. The proposed algorithm with improved models for dependency yields the best accuracy in the previously published results on the Kyoto University Corpus. 
This paper proposes a discriminative HMM (DHMM) with long state dependence (LSDDHMM) to segment and label sequential data. The LSD-DHMM overcomes the strong context independent assumption in traditional generative HMMs (GHMMs) and models the sequential data in a discriminative way, by assuming a novel mutual information independence. As a result, the LSD-DHMM separately models the long state dependence in its state transition model and the observation dependence in its output model. In this paper, a variable-length mutual informationbased modeling approach and an ensemble of kNN probability estimators are proposed to capture the long state dependence and the observation dependence respectively. The evaluation on shallow parsing shows that the LSD-DHMM not only significantly outperforms GHMMs but also much outperforms other DHMMs. This suggests that the LSD-DHMM can effectively capture the long context dependence to segment and label sequential data. 1. Introduction A Hidden Markov Model (HMM) is a model where a sequence of observations is generated in addition to the Markov state sequence. It is a latent variable model in the sense that only the observation sequence is known while the state sequence remains “hidden”. In recent years, HMMs have enjoyed great success in many tagging applications, most notably part-of-speech (POS) tagging (Church 1988; Weischedel et al 1993; Merialdo 1994) and named entity recognition (Bikel et al 1999; Zhou et al 2002). Moreover, there have been also efforts to extend the use of HMMs to word sense disambiguation (Segond et al 1997) and shallow/full parsing (Brants et al 1997; Skut et al 1998; Zhou et al 2000). Traditionally, a HMM segments and labels sequential data in a generative way, assigning a joint probability to paired observation and state sequences. More formally, a generative (first-order)  HMM (GHMM) is given by a finite set of states S including an designated initial state and an designated final state, a set of possible observation O , two conditional probability distributions: a state transition model from s' to s , p(s | s' ) for s ' , s ∈ S and an output model, p(o | s) for o ∈ O, s ∈ S . A sequence of observations is generated by starting from the designated initial state, transmiting to a new state according to p(s | s' ) , emitting an observation selected by that new state according to p(o | s) , transmiting to another new state and so on until the designated final state is generated. There are several problems with this generative approach. First, many tasks would benefit from a richer representation of observations—in particular a representation that describes observations in terms of many overlapping features, such as capitalization, word endings, part-of-speech in addition to the traditional word identity. Note that these features always depends on each other. Furthermore, to define a joint probability over the observation and state sequences, the generative approach needs to enumerate all the possible observation sequences. However, in some tasks, the set of all the possible observation sequences is not reasonably enumerable. Second, the generative approach fails to effectively model the dependence in the observation sequence. Moreover, it is difficult for the generative approach to model the long state dependence since it is not reasonably practical for ngram modeling(e.g. bigram for the first-order GHMM and trigram for the secnodorder GHMM) to be beyond trigram. Third, the generative approach normally estimates the parameters to maximize the likelihood of the observation sequence. However, in many NLP tasks, the goal is to predict the state sequence given the observation sequence. In other words, the generative approach inappropriately applies a generative joint probability model for a conditional probability problem. In summary, the main reasons behind these problems of the generative approach are the strong context independent assumption and the generative nature in modeling sequential data.  While the dependence between successive states can be directly modeled by its state transition model, the generative approach fails to directly capture the observation dependence in the output model. From this viewpoint, a GHMM can be also called an observation independent HMM. To resolve above problems in GHMMs, some researches have been done to move from the generative approach to the discriminative approach. Discriminative HMMs (DHMMs) do not expend modeling effort on the observation sequnce, which are fixed at test time. Instead, DHMMs model the state sequence depending on arbitrary, nonindependent features of the observation sequence, normally without forcing the model to account for the distribution of those dependencies. Punyakanok and Roth (2000) proposed a projection-based DHMM (PDHMM) which represents the probability of a state transition given not only the current observation but also past and future observations and used the SNoW classifier (Roth 1998, Carlson et al 1999) to estimate it (SNoWPDHMM thereafter). McCallum et al (2000) proposed the extact same model and used maximum entropy to estimate it (ME-PDHMM thereafter). Lafferty et al (2001) extanded MEPDHMM using conditional random fields by incorporating the factored state representation of the same model (that is, representing the probability of a state given the observation sequence and the previous state) to alleviate the label bias problem in projection-based DHMMs, which can be biased towards states with few successor states (CRF-DHMM thereafter). Similar work can also be found in Bouttou (1991). Punyakanok and Roth (2000) also proposed a nonprojection-based DMM which separates the dependence of a state on the previous state and the observation sequence, by rewriting the GHMM in a discriminative way and heuristically extending the notation of an observation to the observation sequence. Zhou et al (2000) systematically derived the exact same model as in Punyakanok and Roth (2000) and used back-off modeling to esimate the probability of a state given the observation sequence (Backoff-DHMM thereafter) while Punyakanok and Roth (2000) used the SNoW classifier to estimate it(SNoW-DHMM thereafter). This paper follows our previous work in Zhou et al (2000) and proposes an alternative nonprojection-based DHMM with long state dependence (LSD-DHMM), which separates the dependence of a state on the previous states and the observation sequence. Moreover, a variablelength mutual information based modeling approach (VLMI) is proposed to capture the long state dependence of a state on the previous states.  In addition, an ensemble of kNN probability estimators is proposed to capture the observation dependence of a state on the observation sequence. Experimentation shows that VLMI effectively captures the long state dependence. It also shows that the kNN ensemble captures the dependence between the features of the observation sequence more effectively than classifier-based approaches, by forcing the model to account for the distribution of those dependencies. The layout of this paper is as follows. Section 2 first proposes the LSD-DHMM and then presents the VLMI to capture the long state dependence. Section 3 presents the kNN probability estimator to capture the observation dependence while Section 4 presents the kNN ensemble. Section 5 introduces shallow parsing, while experimental results are given in Section 6. Finally, some conclusion will be drawn in Section 7.  2. LSD-DHMM: Discriminative HMM with Long State Dependence  In principle, given an observation sequence  o1n = o1o2 Lon , the goal of a conditional  probability model is to find a stochastic optimal  state sequence s1n = s1s2 Lsn that maximizes  log p(s1n | o1n )  S * = arg max log p(s1n | o1n )  (1)  s1n  By applying the Bayes’ rule, we can rewrite the  equation (1) as:  s* = arg max{log p(s1n | o1n )}  s1n  (2)  = arg max{log p(s1n ) + MI (s1n , o1n )}  s1n  Obviously, the second term MI (s1n , o1n )  captures the mutual information between the state  sequence s1n and the observation sequence o1n . To  compute MI (s1n , o1n ) efficiently, we propose a  novel mutual information independence  assumption:  n  ∑ MI (s1n , o1n ) = MI (si , o1n )  or  i =1  ∑ log  p(s1n , o1n ) p(s1n ) ⋅ p(o1n )  =  n log i =1  p(si , o1n ) p(si ) ⋅ p(o1n )  (3)  That is, we assume a state is only dependent on  the observation sequence o1n and independent on  other states in the state sequence s1n . This  assumption is reasonable because the dependence  among the states in the state sequence s1n has been  directly captured by the first term log p(s1n ) in  equation (2).  By applying the assumption (3) into the  equation (2) and using the chain rule, we have:  n  ∑ s* = arg max{  log  p(si  |  s i−1 1  )  +  log  p(s1 )  s1n  i=2  n  n  ∑ ∑ − log p(si ) + log p(si | o1n )}  i =1  i =1  n  n  ∑ ∑ = arg max{  log  p(si  |  s i−1 1  )  −  log p(si )  s1n  i=2  i=2  n ∑ + log p(si | o1n )} i =1  n  n  ∑ ∑ = arg max{  MI  (si  ,  s i−1 1  )  +  log p(si | o1n )}  s1n  i=2  i =1  (4)  The above model consists of two models: the  n  ∑ state transition model  MI  (  si  ,  s i−1 1  )  which  i=2  measures the state dependence of a state given the  previous states, and the output model  n ∑ log p(si | o1n ) which measures the observation i =1  dependence of a state given the observation  sequence in a discriminative way. Therefore, we  call the above model as in equation (4) a  discriminative HMM (DHMM) with long state  dependence (LSD-DHMM). The LSD-DHMM  separates the dependence of a state on the previous  states and the observation sequence. The main  difference between a GHMM and a LSD-DHMM  lies in their output models in that the output model  of a LSD-DHMM directly captures the context  dependence between successive observations in  determining the “hidden” states while the output  model of the GHMM fails to do so. That is, the  output model of a LSD-DHMM overcomes the  strong context independent assumption in the  GHMM and becomes observation context  dependent. Therefore, the LSD-DHMM can also  be called an observation context dependent HMM.  Compared with other DHMMs, the LSD-DHMM  explicitly models the long state dependence and  the non-projection nature of the LSD-DHMM  alleviates the label bias problem inherent in  projection-based DHMMs.  Computation of a LSD-DHMM consists of two  parts. The first is to compute the state transition  n  ∑ model:  MI  (  si  ,  s i−1 1  )  .  Traditionally,  ngram  i=2  modeling(e.g. bigram for the first-order GHMM  and trigram for the second-order GHMM) is used  to estimate the state transition model. However, such approach fails to capture the long state dependence since it is not reasonably practical for ngram modeling to be beyond trigram. In this paper, a variable-length mutual information-based modeling approach (VLMI) is proposed as follow: For each i(2 ≤ i ≤ n) , we first find a minimal  k(0 ≤ k p i) where the frequency of  s i−1 k  is  bigger than a threshold (e.g. 10) and then estimate  MI  (  si  ,  s i−1 1  )  using  MI (si  ,  s i−1 k  )  =  p(ski ) p(si ) ⋅ p(ski−1 )  .  In this way, the long state dependence can be  captured maximally in a dynamical way. Here, the  frequencies of variable-length state sequences are  estimated using the simple Good-Turing approach  (Gale et al 1995).  The second is to estimate the output n ∑ model: log p(si | o1n ) . Ideally, we would have i =1 sufficient training data for every event whose conditional probability we wish to calculate. Unfortunately, there is rarely enough training data to compute accurate probabilities when decoding on new data. Traditionally, there are two existing approaches to resolve this problem: linear interpolation (Jelinek 1989) and back-off (Katz 1987). However, these two approaches only work well when the number of different information sources is limited. When a long context is considered, the number of different information sources is exponential and not reasonably enumerable. The current tendency is to recast it as a classification problem and use the output of a classifier, e.g. the maximum entropy classifier (Ratnaparkhi 1999) to estimate the state probability distribution given the observation sequence. In the next two sections, we will propose a more effective ensemble of kNN probability estimators to resolve this problem.  3. kNN Probability Estimator The main challenge for the LSD-DHMM is how to reliably estimate p(si | o1n ) in its output model. For efficiency, we can always assume p(si | o1n ) ≈ p(si | Ei ) , where the pattern entry Ei = oi−N Loi Loi+N . That is, we only consider the observation dependence in a window of 2N+1 observations (e.g. we only consider the current observation, the previous observation and the next observation when N=1). For convenience, we denote P(• | Ei ) as the conditional state probability distribution of the states given Ei and  p(si | Ei ) as the conditional state probability of si given Ei . The kNN probability estimator estimates P(• | Ei ) by first finding the K nearest neighbors of frequently occurring pattern entries kNN (Ei ) = {Eik | k = 1,2,..., K} and then aggregating them to make a proper estimation of P(• | Ei ) . Here, the conditional state probability distribution is estimated instead of the classification in a traditional kNN classifier. To do so, all the frequently occurring pattern entries are extracted from the training corpus in an exhaustive way and stored in a dictionary FrequentEntryDictionary . In order to limit the dictionary size and keep efficiency, we constrain a valid set of pattern entry forms ValidEntryForm to consider only the most informative information sources. Generally, ValidEntryForm can be determined manually or automatically according to the applications. In Section 5, we will give an example. Given a pattern entry Ei and a dictionary of frequently occurring pattern entries FrequentEntryDictionary , a simple algorithm is applied to find the K nearest neighbors of the pattern entry Ei from the dictionary as follows: • compare Ei with each entry in the dictionary and find all the compatible entries • compute the cosine similarity between Ei and each of the compatible entries • sort out the K nearest neighbors according to their cosine similarities Finally, the conditional state probability distribution of the pattern entry is aggregated over those of its K nearest neighbors weighted by their frequencies f (Eik ) and cosine similarities pˆ (Eik | kNN ) : K ∑ pˆ (Eik | kNN ) ⋅ f (Eik ) ⋅ P(• | Eik ) P(• | Ei ) = k=1 K ∑ pˆ (Eik | kNN ) ⋅ f (Eik ) k =1 (5) 4. kNN Ensemble In the literature, an ensemble has been widely used in the classification problem to combine several classifiers (Breiman 1996; Hamamoto 1997; Dietterich 1998; Zhou Z.H. et al 2002; Kim et al 2003). It is well known that an ensemble often  outperforms the individual classifiers that make it up (Hansen et al 1990).  In this paper, an ensemble of kNN probability estimators is proposed to estimate the conditional state probability distribution P(• | Ei ) instead of the classification. This is done through a bagging technique (Breiman 1996) to aggregate several kNN probability estimators. In bagging, the M kNN probability estimators in the ensemble ENS = {kNNm | m = 1,2,..., M } are trained independently via a bootstrap technique and then they are aggregated via an appropriate aggregation method. Usually, we have a single training set and need M training sample sets to construct a kNN ensemble with M independent kNN probability estimators. From the statistical viewpoint, we need to make the training sample sets different as much as possible in order to obtain a higher aggregation performance. For doing this, we often use the bootstrap technique which builds M replicate data sets by randomly re-sampling with replacement from the given training set repeatedly. Each example in the given training set may appear repeatedly or not at all in any particular replicate training sample set. Each training sample set is used to train a certain kNN probability estimator. Finally, the conditional state probability  distribution of the pattern entry Ei is averaged over those of the M kNN probability estimators in the ensemble:  M  ∑ P(• | Ei , kNNm )  P(• | Ei ) = m=1  M  (6)  5. Shallow Parsing In order to evaluate the LSD-DHMM and the proposed variable-length mutual information modeling approach for the long state dependence in the state transition model and the kNN ensemble for the observation dependence in the output model, we have applied it in the application of shallow parsing. For shallow parsing, we have o1 = pi wi , where w1n = w1w2 Lwn is the word sequence and p1n = p1 p2 L pn is the part-of-speech (POS) sequence, while the “hidden” states are represented as structural tags to bracket and differentiate various categories of phrases. The basic idea of using the structural tags to represent the “hidden” states is similar to Skut et al (1998) and Zhou et al (2000). Here, a structural tag consists of three parts:  • Boundary Category (BOUNDARY): it is a set of four values: “O”/“B”/“M”/“E”, where “O” means that current word is a whOle phrase and “B”/“M”/“E” means that current word is at the Beginning/in the Middle/at the End of a phrase. • Phrase Category (PHRASE): it is used to denote the category of the phrase. • Part-of-Speech (POS): Because of the limited number of boundary and phrase categories, the POS is added into the structural tag to represent more accurate state transition and output models.  For example, given the following POS tagged sentence as the observation sequence: He/PRP reckons/VBZ the/DT current/JJ account/NN deficit/NN will/MD narrow/VB to/TO only/RB $/$ 1.8/CD billion/CD in/IN September/NNP ./.  We can have a corresponding sequence of  structural tags as the “hidden” state sequence:  O_NP_PRP(He/PRP)  O_VP _VBZ  (reckons/VBZ) B_NP _DT (the/DT) M_NP _JJ  (current/JJ) M_NP _NN (account/NN) E_NP  _NN (deficit/NN) B_VP _MD (will/MD) E_VP  _VB (narrow/VB) O_PP _TO (to/TO) B_QP _RB  (only/RB) M_QP _$ ($/$) M_QP _CD (1.8/CD)  E_QP _CD (billion/CD) O_PP _IN (in/IN) O_NP  _NNP(September/NNP) O_O _. (./.)  and an equivalent phrase chunked sentence as the shallow parsing result: [NP He/PRP] [VP reckons/VBZ] [ NP the/DT current/JJ account/NN deficit/NN] [VP will/MD narrow/VB] [PP to/TO] [QP only/RB $/$ 1.8/CD billion/CD] [PP in/IN] [NP September/NNP] [O ./.]  6. Experimentation The corpus used in shallow parsing is extracted from the PENN TreeBank (Marcus et al. 1993) of 1 million words (25 sections) by a program provided by Sabine Buchholz from Tilburg University. All the evaluations are 5-fold crossvalidated. For shallow parsing, we use the Fmeasure to measure the performance. Here, the Fmeasure is the weighted harmonic mean of the precision (P) and the recall (R): F = (β 2 + 1)RP β 2R + P with β 2 =1 (Rijsbergen 1979), where the precision (P) is the percentage of predicted phrase chunks that are actually correct and the recall (R) is the percentage of correct phrase chunks that are actually found. Tables 1, 2 and 3 show the detailed performance of LSD-DHMMs. In this paper, the  valid set of pattern entry forms ValidEntryForm is defined to include those pattern entry forms within a windows of 7 observations(including current, left 3 and right 3 observations) where for w j to be included in a pattern entry, all or one of the overlapping features in each of p j , p j+1..., pi ( j ≤ i) or pi , pi+1..., p j (i ≤ j) should be included in the same pattern entry while for p j to be included in a pattern entry, all or one of the overlapping features in each of p j+1, p j+2 ..., pi ( j p i) or pi , pi+1..., p j−1 (i p j) should be included in the same pattern entry. Table 1 shows the effect of different number of nearest neighbors in the kNN probability estimator and considered previous states in the variablelength mutual information modeling approach of the LSD-DHMM, using only one kNN probability estimator in the ensemble to estimate p(si | o1n ) in the output model. It shows that finding 3 nearest neighbors in the kNN probability estimator performs best. It also shows that further increasing the number of nearest neighbors does not increase or even decrease the performance. This may be due to introduction of noisy neighbors when the number of nearest neighbors increases. Moreover, Table 1 shows that the LSD-DHMM performs best when six previous states is considered in the variable-length mutual information-based modeling approach and further considering more previous states only slightly increase the performance. This suggests that the state dependence exists well beyond traditional ngram modeling (e.g. bigram and trigram) to six previous states and the variable-length mutual informationbased modeling approach can capture the long state dependence. In the following experimentation, we will only use the LSD-DHMM with 3 nearest neighbors used in the kNN probability estimator and 6 previous states considered in the variablelength mutual information modeling approach. Table 2 shows the effect of different number of kNN probability estimators in the ensemble. It shows that 15 bootstrap replicates are enough for the k-NN ensemble on shallow parsing and increase the F-measure by 0.71 compared with the ensemble of only one kNN probability estimator. Table 3 compares the LSD-DHMM with GHMMs and other DHMMs. It shows that all the DHMMs significantly outperform GHMMs due to the modeling of the observation dependence and allowing for non-independent, difficult to enumerate observation features. It also shows that our LSD-DHMM much outperforms other DHMMs due to the modeling of the long state  dependence using the variable-length mutual information-based modeling approach in the LSDDHMM. Moverover, Table 3 shows that noprojection-based DHMMs (i.e. CRF-DHMM, SNoW-DHMM, Backoff-DHMM and LSDDHMM) outperform projection-based DHMMs. It may be due to alleviation of the label bias problem inherent in the projection-based DHMMs. Finally, Table 2 also compares the kNN ensemble with  popular classifier-based approaches, such as SNoW and Maximum Entropy, in estimating the output model of the LSD-DHMM. It shows that the kNN ensemble outperforms these classifierbased approaches. This suggests that the kNN ensemble captures the dependence between the features of the observation sequence more effectively by forcing the model to account for the distribution of those dependencies.  Table 1: Effect of different numbers of nearest neighbors in the kNN probability estimator and previous  states considered in the variable-length mutual information modeling approach of the LSD-DHMMs, using  only a probability estimator in the ensemble  Shallow Parsing  Number of nearest neighbors  
The main problems of statistical word alignment lie in the facts that source words can only be aligned to one target word, and that the inappropriate target word is selected because of data sparseness problem. This paper proposes an approach to improve statistical word alignment with a rule-based translation system. This approach first uses IBM statistical translation model to perform alignment in both directions (source to target and target to source), and then uses the translation information in the rule-based machine translation system to improve the statistical word alignment. The improved alignments allow the word(s) in the source language to be aligned to one or more words in the target language. Experimental results show a significant improvement in precision and recall of word alignment. 
Word-aligned bilingual corpora are an important knowledge source for many tasks in natural language processing. We improve the well-known IBM alignment models, as well as the Hidden-Markov alignment model using a symmetric lexicon model. This symmetrization takes not only the standard translation direction from source to target into account, but also the inverse translation direction from target to source. We present a theoretically sound derivation of these techniques. In addition to the symmetrization, we introduce a smoothed lexicon model. The standard lexicon model is based on full-form words only. We propose a lexicon smoothing method that takes the word base forms explicitly into account. Therefore, it is especially useful for highly inﬂected languages such as German. We evaluate these methods on the German–English Verbmobil task and the French–English Canadian Hansards task. We show statistically signiﬁcant improvements of the alignment quality compared to the best system reported so far. For the Canadian Hansards task, we achieve an improvement of more than 30% relative. 
We propose a novel method to predict the interparagraph discourse structure of text, i.e. to infer which paragraphs are related to each other and form larger segments on a higher level. Our method combines a clustering algorithm with a model of segment “relatedness” acquired in a machine learning step. The model integrates information from a variety of sources, such as word co-occurrence, lexical chains, cue phrases, punctuation, and tense. Our method outperforms an approach that relies on word co-occurrence alone. 
This paper focuses on the automated processing of temporal information in written texts, more speciﬁcally on relations between events introduced by verbs in ﬁnite clauses. While this problem has been largely studied from a theoretical point of view, it has very rarely been applied to real texts, if ever, with quantiﬁed results. The methodology required is still to be deﬁned, even though there have been proposals in the strictly human annotation case. We propose here both a procedure to achieve this task and a way of measuring the results. We have been testing the feasibility of this on newswire articles, with promising results. 
In this paper I outline Type-inheritance Combinatory Categorial Grammar (TCCG), an implemented feature structure based CCG fragment of English. TCCG combines the fully lexical nature of CCG with the type-inheritance hierarchies and complex feature structures of Headdriven Phrase Structure Grammars (HPSG). The result is a CCG/HPSG hybrid that combines linguistic generalizations previously only statable in one theory or the other, even extending the set of statable generalizations to those not easily captured by either theory. 
This paper presents a deterministic dependency parser based on memory-based learning, which parses English text in linear time. When trained and evaluated on the Wall Street Journal section of the Penn Treebank, the parser achieves a maximum attachment score of 87.1%. Unlike most previous systems, the parser produces labeled dependency graphs, using as arc labels a combination of bracket labels and grammatical role labels taken from the Penn Treebank II annotation scheme. The best overall accuracy obtained for identifying both the correct head and the correct arc label is 86.0%, when restricted to grammatical role labels (7 labels), and 84.4% for the maximum set (50 labels). 
We consider the problem of computing the Kullback-Leibler distance, also called the relative entropy, between a probabilistic context-free grammar and a probabilistic ﬁnite automaton. We show that there is a closed-form (analytical) solution for one part of the Kullback-Leibler distance, viz. the cross-entropy. We discuss several applications of the result to the problem of distributional approximation of probabilistic context-free grammars by means of probabilistic ﬁnite automata. 
In this paper, subclasses of monadic contextfree tree grammars (CFTGs) are compared. Since linear, nondeleting, monadic CFTGs generate the same class of string languages as tree adjoining grammars (TAGs), it is examined whether the restrictions of linearity and nondeletion on monadic CFTGs are necessary to generate the same class of languages. Epsilonfreeness on linear, nondeleting, monadic CFTG is also examined. 
Regular languages are widely used in NLP today in spite of their shortcomings. Eﬃcient algorithms that can reliably learn these languages, and which must in realistic applications only use positive samples, are necessary. These languages are not learnable under traditional distribution free criteria. We claim that an appropriate learning framework is PAC learning where the distributions are constrained to be generated by a class of stochastic automata with support equal to the target concept. We discuss how this is related to other learning paradigms. We then present a simple learning algorithm for regular languages, and a self-contained proof that it learns according to this partially distribution free criterion. 
Ngram models are simple in language modeling and have been successfully used in speech recognition and other tasks. However, they can only capture the short distance context dependency within an n-words window where currently the largest practical n for a natural language is three while much of the context dependency in a natural language occurs beyond a three words window. In order to incorporate this kind of long distance context dependency in the ngram model of our Mandarin speech recognition system, this paper proposes a novel MI-Ngram modeling approach. This new MI-Ngram model consists of two components: a normal ngram model and a novel MI model. The ngram model captures the short distance context dependency within an n-words window while the MI model captures the context dependency between the word pairs over a long distance by using the concept of mutual information. That is, the MI-Ngram model incorporates the word occurrences beyond the scope of the normal ngram model. It is found that MINgram modeling has much better performance than the normal word ngram modeling. Experimentation shows that about 20% of errors can be corrected by using a MI-Trigram model compared with the pure word trigram model. 
This paper presents example-based machine translation (MT) based on syntactic transfer, which selects the best translation by using models of statistical machine translation. Example-based MT sometimes generates invalid translations because it selects similar examples to the input sentence based only on source language similarity. The method proposed in this paper selects the best translation by using a language model and a translation model in the same manner as statistical MT, and it can improve MT quality over that of ‘pure’ example-based MT. A feature of this method is that the statistical models are applied after word re-ordering is achieved by syntactic transfer. This implies that MT quality is maintained even when we only apply a lexicon model as the translation model. In addition, translation speed is improved by bottom-up generation, which utilizes the tree structure that is output from the syntactic transfer. 
In this paper we report on the results of an experiment in designing resource-light metrics that predict the potential translation complexity of a text or a corpus of homogenous texts for state-ofthe-art MT systems. We show that the best prediction of translation complexity is given by the average number of syllables per word (ASW). The translation complexity metrics based on this parameter are used to normalise automated MT evaluation scores such as BLEU, which otherwise are variable across texts of different types. The suggested approach makes a fairer comparison between the MT systems evaluated on different corpora. The translation complexity metric was integrated into two automated MT evaluation packages – BLEU and the Weighted N-gram model. The extended MT evaluation tools are available from the first author’s web site: http://www.comp.leeds.ac.uk/bogdan/evalMT.html 
In order to boost the translation quality of corpus-based MT systems for speech translation, the technique of splitting an input sentence appears promising. In previous research, many methods used N-gram clues to split sentences. In this paper, to supplement N-gram based splitting methods, we introduce another clue using sentence similarity based on edit-distance. In our splitting method, we generate candidates for sentence splitting based on N-grams, and select the best one by measuring sentence similarity. We conducted experiments using two EBMT systems, one of which uses a phrase and the other of which uses a sentence as a translation unit. The translation results on various conditions were evaluated by objective measures and a subjective measure. The experimental results show that the proposed method is valuable for both systems. 
News articles report on facts, events, and opinions with the intent of conveying the truth. However, the facts, events, and opinions appearing in the text are often known only secondor third-hand, and as any child who has played “telephone” knows, this relaying of facts often garbles the original message. Properly understanding the information ﬁltering structures that govern the interpretation of these facts, then, is critical to appropriately analyzing them. In this work, we present a learning approach that correctly determines the hierarchical structure of information ﬁltering expressions 78.30% of the time. 
A dialogue manager provides the decision making at the heart of a spoken dialogue system. In an object-oriented approach to dialogue management, generic behaviour, such as confirming new or modified information that has been supplied by the user, is inherited by more specialised classes. These specialised classes either encapsulate behaviour typical of a particular business domain (service agents) or make available dialogue abilities that may be required in many business domains (support agents). In this paper we consider the interplay between the agents’ generic and specialised behaviour and consider the manner in which service and support agents collaborate within and across their respective groups. 
We present a set of discourse structure relations that are easy to code, and develop criteria for an appropriate data structure for representing these relations. Discourse structure here refers to informational relations that hold between sentences in a discourse (cf. Hobbs, 1985). We evaluated whether trees are a descriptively adequate data structure for representing coherence. Trees are widely assumed as a data structure for representing coherence but we found that more powerful data structures are needed: In coherence structures of naturally occurring texts, we found many different kinds of crossed dependencies, as well as many nodes with multiple parents. The claims are supported by statistical results from a database of 135 texts from the Wall Street Journal and the AP Newswire that were hand-annotated with coherence relations, based on the annotation schema presented in this paper. 
Natural Language Interfaces to Databases (NLIs) can beneﬁt from the advances in statistical parsing over the last ﬁfteen years or so. However, statistical parsers require training on a massive, labeled corpus, and manually creating such a corpus for each database is prohibitively expensive. To address this quandary, this paper reports on the PRECISE NLI, which uses a statistical parser as a “plug in”. The paper shows how a strong semantic model coupled with “light re-training” enables PRECISE to overcome parser errors, and correctly map from parsed questions to the corresponding SQL queries. We discuss the issues in using statistical parsers to build database-independent NLIs, and report on experimental results with the benchmark ATIS data set where PRECISE achieves 94% accuracy. 
Statistical language modeling remains a challenging task, in particular for morphologically rich languages. Recently, new approaches based on factored language models have been developed to address this problem. These models provide principled ways of including additional conditioning variables other than the preceding words, such as morphological or syntactic features. However, the number of possible choices for model parameters creates a large space of models that cannot be searched exhaustively. This paper presents an entirely data-driven model selection procedure based on genetic search, which is shown to outperform both knowledge-based and random selection procedures on two diﬀerent language modeling tasks (Arabic and Turkish). 
When we write a report or an explanation on a newly-developed technology for readers including laypersons, it is very important to compose a title that can stimulate their interest in the technology. However, it is difﬁcult for inexperienced authors to come up with an appealing title. In this research, we developed a support system for revising titles. We call it “title revision wizard”. The wizard provides a guidance on revising draft title to compose a title meeting three key points, and support tools for coming up with and elaborating on comprehensible or appealing phrases. In order to test the eﬀect of our title revision wizard, we conducted a questionnaire survey on the eﬀect of the titles with or without using the wizard on the interest of lay readers. The survey showed that the wizard is eﬀective and helpful for the authors who cannot compose appealing titles for lay readers by themselves. 
An efﬁcient bit-vector-based CKY-style parser for context-free parsing is presented. The parser computes a compact parse forest representation of the complete set of possible analyses for large treebank grammars and long input sentences. The parser uses bit-vector operations to parallelise the basic parsing operations. The parser is particularly useful when all analyses are needed rather than just the most probable one. 
Linearization-based HPSG theories are widely used for analyzing languages with relatively free constituent order. This paper introduces the Generalized ID/LP (GIDLP) grammar format, which supports a direct encoding of such theories, and discusses key aspects of a parser that makes use of the dominance, precedence, and linearization domain information explicitly encoded in this grammar format. We show that GIDLP grammars avoid the explosion in the number of rules required under a traditional phrase structure analysis of free constituent order. As a result, GIDLP grammars support more modular and compact grammar encodings and require fewer edges in parsing. 
We propose a syntax-semantics interface that realises the mapping between syntax and semantics as a relation and does not make functionality assumptions in either direction. This interface is stated in terms of Extensible Dependency Grammar (XDG), a grammar formalism we newly specify. XDG’s constraint-based parser supports the concurrent ﬂow of information between any two levels of linguistic representation, even when only partial analyses are available. This generalises the concept of underspeciﬁcation. 
In this paper we describe a method of automatically learning domain theories from parsed corpora of sentences from the relevant domain and use FSA techniques for the graphical representation of such a theory. By a ‘domain theory’ we mean a collection of facts and generalisations or rules which capture what commonly happens (or does not happen) in some domain of interest. As language users, we implicitly draw on such theories in various disambiguation tasks, such as anaphora resolution and prepositional phrase attachment, and formal encodings of domain theories can be used for this purpose in natural language processing. They may also be objects of interest in their own right, that is, as the output of a knowledge discovery process. The approach is generizable to different domains provided it is possible to get logical forms for the text in the domain. 
We extend Combinatory Categorial Grammar (CCG) with a generalized notion of multidimensional sign, inspired by the types of representations found in constraint-based frameworks like HPSG or LFG. The generalized sign allows multiple levels to share information, but only in a resource-bounded way through a very restricted indexation mechanism. This improves representational perspicuity without increasing parsing complexity, in contrast to full-blown uniﬁcation used in HPSG and LFG. Well-formedness of a linguistic expressions remains entirely determined by the CCG derivation. We show how the multidimensionality and perspicuity of the generalized signs lead to a simpliﬁcation of previous CCG accounts of how word order and prosody can realize information structure. 
In statistical machine translation, the generation of a translation hypothesis is computationally expensive. If arbitrary reorderings are permitted, the search problem is NP-hard. On the other hand, if we restrict the possible reorderings in an appropriate way, we obtain a polynomial-time search algorithm. We investigate diﬀerent reordering constraints for phrase-based statistical machine translation, namely the IBM constraints and the ITG constraints. We present eﬃcient dynamic programming algorithms for both constraints. We evaluate the constraints with respect to translation quality on two Japanese–English tasks. We show that the reordering constraints improve translation quality compared to an unconstrained search that permits arbitrary phrase reorderings. The ITG constraints preform best on both tasks and yield statistically signiﬁcant improvements compared to the unconstrained search. 
Word alignment is a challenging task aiming at the identiﬁcation of translational relations between words and multi-word units in parallel corpora. Many alignment strategies are based on links between single words. Diﬀerent strategies can be used to ﬁnd the optimal word alignment using such one-toone word links including relations between multi-word units. In this paper seven algorithms are compared using a word alignment approach based on association clues and an English-Swedish bitext together with a handcrafted reference alignment used for evaluation. 
In this paper, we address the word alignment problem for statistical machine translation. We aim at creating a symmetric word alignment allowing for reliable one-to-many and many-to-one word relationships. We perform the iterative alignment training in the source-to-target and the target-to-source direction with the well-known IBM and HMM alignment models. Using these models, we robustly estimate the local costs of aligning a source word and a target word in each sentence pair. Then, we use eﬃcient graph algorithms to determine the symmetric alignment with minimal total costs (i. e. maximal alignment probability). We evaluate the automatic alignments created in this way on the German–English Verbmobil task and the French–English Canadian Hansards task. We show statistically signiﬁcant improvements of the alignment quality compared to the best results reported so far. On the Verbmobil task, we achieve an improvement of more than 1% absolute over the baseline error rate of 4.7%. 
Traditionally, coreference resolution is done by mining the reference relationships between NP pairs. However, an individual NP usually lacks adequate description information of its referred entity. In this paper, we propose a supervised learning-based approach which does coreference resolution by exploring the relationships between NPs and coreferential clusters. Compared with individual NPs, coreferential clusters could provide richer information of the entities for better rules learning and reference determination. The evaluation done on MEDLINE data set shows that our approach outperforms the baseline NP-NP based approach in both recall and precision. 
This paper describes an extension of the dar-algorithm (Navarretta, 2004) for resolving intersentential pronominal anaphors referring to individual and abstract entities in texts and dialogues. In dar individual entities are resolved combining models which identify high degree of salience with high degree of givenness (topicality) of entities in the hearer’s cognitive model, e.g. (Grosz et al., 1995), with Hajiˇcov´a et al.’s (1990) salience account which assigns the highest degree of salience to entities in the focal part of an utterance in Information Structure terms, which often introduce new information in discourse. Anaphors referring to abstract entities are resolved with an extension of the algorithm presented by Eckert and Strube (2000). The extended dar-algorithm accounts for diﬀerences in the resolution mechanisms of diﬀerent types of Danish pronouns. Manual tests of the algorithm show that dar performs better than other resolution algorithms on the same data. 
This paper presents a machine learning approach to bare sluice disambiguation in dialogue. We extract a set of heuristic principles from a corpus-based sample and formulate them as probabilistic Horn clauses. We then use the predicates of such clauses to create a set of domain independent features to annotate an input dataset, and run two diﬀerent machine learning algorithms: SLIPPER, a rule-based learning algorithm, and TiMBL, a memory-based system. Both learners perform well, yielding similar success rates of approx 90%. The results show that the features in terms of which we formulate our heuristic principles have signiﬁcant predictive power, and that rules that closely resemble our Horn clauses can be learnt automatically from these features. 
We suggest a new goal and evaluation criterion for word similarity measures. The new criterion meaning-entailing substitutability - fits the needs of semantic-oriented NLP applications and can be evaluated directly (independent of an application) at a good level of human agreement. Motivated by this semantic criterion we analyze the empirical quality of distributional word feature vectors and its impact on word similarity results, proposing an objective measure for evaluating feature vector quality. Finally, a novel feature weighting and selection function is presented, which yields superior feature vectors and better word similarity performance. 
It is argued in this paper that an optimal solution to disambiguation is a combination of linguistically motivated rules and resolution based on probability or heuristic rules. By disambiguation is here meant ambiguity resolution on all levels of language analysis, including morphology and semantics. The discussion is based on Swahili, for which a comprehensive analysis system has been developed by using two-level description in morphology and constraint grammar formalism in disambiguation. Particular attention is paid to optimising the use of different solutions for achieving maximal precision with minimal rule writing. 
Word dependency is important in parsing technology. Some applications such as Information Extraction from biological documents beneﬁt from word dependency analysis even without phrase labels. Therefore, we expect an accurate dependency analyzer trainable without using phrase labels is useful. Although such an English word dependency analyzer was proposed by Yamada and Matsumoto, its accuracy is lower than state-of-the-art phrase structure parsers because of the lack of top-down information given by phrase labels. This paper shows that the dependency analyzer can be improved by introducing a Root-Node Finder and a Prepositional-Phrase Attachment Resolver. Experimental results show that these modules based on Preference Learning give better scores than Collins’ Model 3 parser for these subproblems. We expect this method is also applicable to phrase structure parsers. 
This paper describes the role of supertagging in a wide-coverage CCG parser which uses a log-linear model to select an analysis. The supertagger reduces the derivation space over which model estimation is performed, reducing the space required for discriminative training. It also dramatically increases the speed of the parser. We show that large increases in speed can be obtained by tightly integrating the supertagger with the CCG grammar and parser. This is the ﬁrst work we are aware of to successfully integrate a supertagger with a full parser which uses an automatically extracted grammar. We also further reduce the derivation space using constraints on category combination. The result is an accurate wide-coverage CCG parser which is an order of magnitude faster than comparable systems for other linguistically motivated formalisms. 
In this paper we present a grammar formalism that combines the insights from Combinatory Categorial Grammar with feature structure uniﬁcation. We show how information structure can be incorporated with syntactic and semantic representations in a principled way. We focus on the way theme, rheme, and focus are integrated in the compositional semantics, using Discourse Representation Theory as ﬁrst-order semantic theory. UCCG can be used for parsing and generating prosodically annotated text, and therefore has the potential to advance spoken dialogue systems. 
In the context of lexicalized grammars, we propose general methods for lexical disambiguation based on polarization and abstraction of grammatical formalisms. Polarization makes their resource sensitivity explicit and abstraction aims at keeping essentially the mechanism of neutralization between polarities. Parsing with the simpliﬁed grammar in the abstract formalism can be used eﬃciently for ﬁltering lexical selections. Introduction There is a complexity issue if one consider exact parsing with large scale lexicalized grammars. Indeed, the number of way of associating to each word of a sentence a corresponding elementary structure—a tagging of the sentence— is the product of the number of lexical entries for each word. The procedure may have an exponential complexity in the length of the sentence. In order to ﬁlter taggings, we can use probabilistic methods (Joshi and Srinivas, 1994) and keep only the most probable ones; but if we want to keep all successful taggings, we must use exact methods. Among these, one consists in abstracting information that is relevant for the ﬁltering process, from the formalism F used for representing the concerned grammar G. In this way, we obtain a new formalism Fabs which is a simpliﬁcation of F and the grammar G is translated into a grammar abs(G) in the abstract framework Fabs. From this, disambiguating with G consists in parsing with abs(G). The abstraction is relevant if parsing eliminates a maximum of bad taggings at a minimal cost. (Boullier, 2003) uses such a method for Lexicalized Tree Adjoining Grammars (LTAG) by abstracting a tree adjoining grammar into a context free grammar and further abstracting that one into a regular grammar. We also propose to apply abstraction but after a preprocessing polarization step.  The notion of polarity comes from Categorial Grammars (Moortgat, 1996) which ground syntactic composition on the resource sensitivity of natural languages and it is highlighted in Interaction Grammars (Perrier, 2003), which result from reﬁning and making Categorial Grammars more ﬂexible. Polarization of a grammatical formalism F consists in adding polarities to its syntactic structures to obtain a polarized formalism Fpol in which neutralization of polarities is used for controlling syntactic composition. In this way, the resource sensitivity of syntactic composition is made explicit. (Kahane, 2004) shows that many grammatical formalisms can be polarized by generalizing the system of polarities used in Interaction Grammars. To abstract a grammatical formalism, it is interesting to polarize it before because polarities allow original methods of abstraction. The validity of our method is based on a concept of morphism (two instances of which being polarization and abstraction) which characterizes how one should transport a formalism into another. In sections 1 and 2, we present the conceptual tools of grammatical formalism and morphism which are used in the following. In section 3, we deﬁne the operation of polarizing grammatical formalisms and in section 4, we describe how polarization is used then for abstracting these formalisms. In section 5, we show how abstraction of grammatical formalisms grounds methods of lexical disambiguation, which reduce to parsing in simpliﬁed formalisms. We illustrate our purpose with an incremental and a bottom-up method. In section 6, we present some experimental results which illustrate the ﬂexibility of the approach.  
In this paper, we present an approach to include morpho-syntactic dependencies into the training of the statistical alignment models. Existing statistical translation systems usually treat diﬀerent derivations of the same base form as they were independent of each other. We propose a method which explicitly takes into account such interdependencies during the EM training of the statistical alignment models. The evaluation is done by comparing the obtained Viterbi alignments with a manually annotated reference alignment. The improvements of the alignment quality compared to the, to our knowledge, best system are reported on the German-English Verbmobil corpus.  
This paper addressees the problem of eliminating unsatisfactory outputs from machine translation (MT) systems. The authors intend to eliminate unsatisfactory MT outputs by using conﬁdence measures. Conﬁdence measures for MT outputs include the rank-sum-based conﬁdence measure (RSCM) for statistical machine translation (SMT) systems. RSCM can be applied to non-SMT systems but does not always work well on them. This paper proposes an alternative RSCM that adopts a mixture of the N-best lists from multiple MT systems instead of a single-system’s N-best list in the existing RSCM. In most cases, the proposed RSCM proved to work better than the existing RSCM on two non-SMT systems and to work as well as the existing RSCM on an SMT system. 
This paper presents a system for automatically generating discourse structures from written text. The system is divided into two levels: sentence-level and text-level. The sentence-level discourse parser uses syntactic information and cue phrases to segment sentences into elementary discourse units and to generate discourse structures of sentences. At the text-level, constraints about textual adjacency and textual organization are integrated in a beam search in order to generate best discourse structures. The experiments were done with documents from the RST Discourse Treebank. It shows promising results in a reasonable search space compared to the discourse trees generated by human analysts. 
This paper shows how talking robots can be built from off-the-shelf components, based on the Lego MindStorms robotics platform. We present four robots that students created as ﬁnal projects in a seminar we supervised. Because Lego robots are so affordable, we argue that it is now feasible for any dialogue researcher to tackle the interesting challenges at the robot-dialogue interface. 
This paper proposes unsupervised word sense disambiguation based on automatically constructed case frames and its incorporation into our zero pronoun resolution system. The word sense disambiguation is applied to verbs and nouns. We consider that case frames deﬁne verb senses and semantic features in a thesaurus deﬁne noun senses, respectively, and perform sense disambiguation by selecting them based on case analysis. In addition, according to the one sense per discourse heuristic, the word sense disambiguation results are cached and applied globally to the subsequent words. We integrated this global word sense disambiguation into our zero pronoun resolution system, and conducted experiments of zero pronoun resolution on two diﬀerent domain corpora. Both of the experimental results indicated the eﬀectiveness of our approach. 
We investigate unsupervised techniques for acquiring monolingual sentence-level paraphrases from a corpus of temporally and topically clustered news articles collected from thousands of web-based news sources. Two techniques are employed: (1) simple string edit distance, and (2) a heuristic strategy that pairs initial (presumably summary) sentences from different news stories in the same cluster. We evaluate both datasets using a word alignment algorithm and a metric borrowed from machine translation. Results show that edit distance data is cleaner and more easily-aligned than the heuristic data, with an overall alignment error rate (AER) of 11.58% on a similarly-extracted test set. On test data extracted by the heuristic strategy, however, performance of the two training sets is similar, with AERs of 13.2% and 14.7% respectively. Analysis of 100 pairs of sentences from each set reveals that the edit distance data lacks many of the complex lexical and syntactic alternations that characterize monolingual paraphrase. The summary sentences, while less readily alignable, retain more of the non-trivial alternations that are of greatest interest learning paraphrase relationships. 
In this paper we illustrate and evaluate an approach to the creation of high quality linguistically annotated resources based on the exploitation of aligned parallel corpora. This approach is based on the assumption that if a text in one language has been annotated and its translation has not, annotations can be transferred from the source text to the target using word alignment as a bridge. The transfer approach has been tested in the creation of the MultiSemCor corpus, an English/Italian parallel corpus created on the basis of the English SemCor corpus. In MultiSemCor texts are aligned at the word level and semantically annotated with a shared inventory of senses. We present some experiments carried out to evaluate the different steps involved in the methodology. The results of the evaluation suggest that the cross-language annotation transfer methodology is a promising solution allowing for the exploitation of existing (mostly English) annotated resources to bootstrap the creation of annotated corpora in new (resourcepoor) languages with greatly reduced human effort. 
A consumer health information system must be able to comprehend both expert and nonexpert medical vocabulary and to map between the two. We describe an ongoing project to create a new lexical database called Medical WordNet (MWN), consisting of medically relevant terms used by and intelligible to non-expert subjects and supplemented by a corpus of natural-language sentences that is designed to provide medically validated contexts for MWN terms. The corpus derives primarily from online health information sources targeted to consumers, and involves two sub-corpora, called Medical FactNet (MFN) and Medical BeliefNet (MBN), respectively. The former consists of statements accredited as true on the basis of a rigorous process of validation, the latter of statements which non-experts believe to be true. We summarize the MWN / MFN / MBN project, and describe some of its applications. 
We describe a simple approach for integrating shallow and deep parsing. We use phrase structure bracketing obtained from the Collins parser as ﬁlters to guide deep parsing. Our experiments demonstrate that our technique yields substantial gains in speed along with modest improvements in accuracy. 
Selecting important information while accounting for repetitions is a hard task for both summarization and question answering. We propose a formal model that represents a collection of documents in a two-dimensional space of textual and conceptual units with an associated mapping between these two dimensions. This representation is then used to describe the task of selecting textual units for a summary or answer as a formal optimization task. We provide approximation algorithms and empirically validate the performance of the proposed model when used with two very different sets of features, words and atomic events. 
Empirical experience and observations have shown us when powerful and highly tunable classiﬁers such as maximum entropy classiﬁers, boosting and SVMs are applied to language processing tasks, it is possible to achieve high accuracies, but eventually their performances all tend to plateau out at around the same point. To further improve performance, various error correction mechanisms have been developed, but in practice, most of them cannot be relied on to predictably improve performance on unseen data; indeed, depending upon the test set, they are as likely to degrade accuracy as to improve it. This problem is especially severe if the base classiﬁer has already been ﬁnely tuned. In recent work, we introduced N-fold Templated Piped Correction, or NTPC (“nitpick”), an intriguing error corrector that is designed to work in these extreme operating conditions. Despite its simplicity, it consistently and robustly improves the accuracy of existing highly accurate base models. This paper investigates some of the more surprising claims made by NTPC, and presents experiments supporting an Occam’s Razor argument that more complex models are damaging or unnecessary in practice. 
We explore unsupervised language model adaptation techniques for Statistical Machine Translation. The hypotheses from the machine translation output are converted into queries at different levels of representation power and used to extract similar sentences from very large monolingual text collection. Specific language models are then build from the retrieved data and interpolated with a general background model. Experiments show significant improvements when translating with these adapted language models.  
Tree-based approaches to alignment model translation as a sequence of probabilistic operations transforming the syntactic parse tree of a sentence in one language into that of the other. The trees may be learned directly from parallel corpora (Wu, 1997), or provided by a parser trained on hand-annotated treebanks (Yamada and Knight, 2001). In this paper, we compare these approaches on Chinese-English and French-English datasets, and ﬁnd that automatically derived trees result in better agreement with human-annotated word-level alignments for unseen test data. 
Manually annotating the rhetorical structure of texts is very labour-intensive. At the same time, high-quality automatic analysis is currently out of reach. We thus propose to split the manual annotation in two phases: the simpler marking of lexical connectives and their relations, and the more diﬃcult decisions on overall tree structure. To this end, we developed an environment of two analysis tools and XML-based declarative resources. Our ConAno tool allows for eﬃcient, interactive annotation of connectives, scopes and relations. This intermediate result is exported to O’Donnell’s ‘RST Tool’, which facilitates completing the tree structure. 
In this paper we present the dialogueunderstanding components of an architecture for assisting multi-human conversations in artifact-producing meetings: meetings in which tangible products such as project planning charts are created. Novel aspects of our system include multimodal ambiguity resolution, modular ontologydriven artifact manipulation, and a meeting browser for use during and after meetings. We describe the software architecture and demonstrate the system using an example multimodal dialogue. 
Ê ÒØ Ý Ö×¸ Ø Ò×Û Ö¹ Ó Ù× ×ÙÑÑ Ö Þ Ø ÓÒ × Ô ØØÓØÒÒÖØ ÓØÖÒ ØÚÓ Ð × Ò ØÕÙ Ò×ÓØÐÓÓÒÝ ÒÓ×ÑÛÔÖÐ ÒÑº ÒÁØÒÖÓÝÖØÓÖ ÒØÓÓÖÖÑ Ð¹¹ Þ ÑÙÐØ ¹ Ó ÙÑ ÒØ ×ÙÑÑ Ö Þ Ø ÓÒ Ó Ù× Ý ÑÙÐØ ÔÐ ÕÔÙÓÖØ×ØÒÓÒ×¸ÙÛ× ÒÔÖ×ÓÓÔÖÓ×× ÔÖÓÑ ÙØ Ó ØÝÓ ÉÐ ÙÙÐ×ØØÓÒ×¹ÒØÒ×ÒÛ Ö ÑÒ ¹ Ò Ò Ò Ö ×ÔÓÒ× ØÓ ÑÙÐØ ÔÐ ÕÙ ×Ø ÓÒ×º Ï Ð×Ó ¹ ××ÙÖÑÑ ÖÒÞ ÒØØÓÒÖ×ÝØ×ÓØÒÑÓº ÌØ ÒØÓÚ ÐÙ ÒØ ÓÖÒ ÖÑ×ÙÙÐÐØØ×¹×ÓÓÛÙÑØ ÒØØ Ø ÔÖÓÔÓ× Ñ Ø Ó × ØØ Ö Ô Ö ÓÖÑ Ò Ø Ò ÒÓØ ÓØÒÑÐÝ× ×ÒÚØ Ö Ð Ú Ð×ÙÐØÒÓÒ× ÛÙÓØÖ ×Ð×ÓÓÔÓÆØ ÌÖ ÁÔÊ ÖØÌËÔ ¿ÒØ×Ó³Ö×ÑÝ×¹Ð ÊÙÒ¸ ÐØ ÓÙ Û Ú ØÓ Ø ÒÓØ Ó Ø Ø Ø Ø ×ÓÑ Ó Ø ÓØ Ö ×Ý×Ø Ñ× Ó ÒÓØ Ù× Ø Ò ÓÖÑ Ø ÓÒ Ó ÕÙ ×Ø ÓÒ×º ½ ÁÒØÖÓ Ù Ø ÓÒ Ê¬ÖØÝØÚØ¾Ø×××Ñ×ÑÑÓØÒÓÓÓØÓÒÙ¼ÖÒÓÓÓÓÐÙÙÐÔÓÖÒÓ¼ÇØÐ×ØÒÒÒÖ×ÛÙÝØÖØÖÐÔÖÓÛÙØ¿ÙÚØ¸ÓØÒ×Ð×ÖÖÒÑÓÔÖµÓÓÐ×ÒØÑ×º××Ö¸ÓÒÔ×Óº¸×ÒÔÙ×ÓÒÖÓØÖØØØÐÓÔ×ÐÓÓØÓÔ¸¬¾ÝÒÖØÛØÒÑÒÑÙÖÒÒ×ØÒ¸¼×ÒÖÕØ×ÓØ×ØÔËÓÓ×Ø¼Ò×Ù´ÝÓÙÓÛÒØÒÉÐ¾ÓØÒÐÒÑÖÑÙÑÐÐÐÓµÝÖÖÑØÙ××ÝÑÒØºÓÑÞÙØÙØÐÖÓÙÑÓØØÛÑÙØÐµÙÓ¸ÖÓÐÓÑ×ÓÖÍ×ÐÙØØ¸×ÑÝÐÑÖÙÌÒÑÓÒÖØÐÒÑÒÑÓ¹ÒÝÖ×ÑÝÙ×ÖØØÖÚÔÒ×ÔØÙÒ×ÝØº×ÓÒÑÒ×××ÐÓÓº¾ÑÐÐØØØ××ÖÖ×ØØÒÛØÏÖÛÒÖÒ¼ÓÓÑÓÓÑØÙÁÙ×ÐÙ×Ñ¼ÓØ¹ÓÓ×ÙÖØÉ×ØÛÕØ×Ð××ÑÓ¿ÒØÙ¸ÒØÒÖ×ÒÓÑºÙÓÝÒÑÕØÐØÐØØ´ÓÓ¹ÓÐÖØÒÐÙÙÀÕÓÒÆ×ÓÓÖÚÓ¹×ÙØÒÓÁ×ÒÒÐÓÒÙÓÛÓØØÙ¸ÒÖ×ÊÛØÑ×ÒÑÐÖÝÐÓÙÐÒÓÓØÓ¹ÒÒÙ××ÒØÑØÚÒÙ´ØÖÓÓÙØÒÓ×ØÙÒÒÑ×ÁØ×Ò×××ÓÐÞÝÒÓÑÑÓÓÊ×ØÓÓØÓØÖ×¸×Ù´ÛÐÒÖ¸ÙÒÒÖÙØÝÔ×ÙµÔØÑÇÒÓÝÑØÖÛ×Ò×ÙºÒØÐÑØÒÓ××ÓÝ×ºÚÑÓÑÓÑÒÖÐËØÑÒÜÝÛÒÖ×ÐºÒÒØÒÖØÇÓ×¸ÙÖÙÑÒØÓÖÔØÓÞºÓÆÒÑ¾ÓÑ×ÐÒØÒ××ÒØ¸ØÖÒÐÔÙØÕ¼ÑÁÓ×Ø×ÒÑÖÑËÛÜÙ¼×ÙÓÒÓÙ××ÊÔÓÓÒÔ½ÞÒÒÌÑÓÖØØÚÑÓ×ÒÖÙ×ÒÙÑ×××ÖÖ×ÖÚÝ×ØØÑÓ×Û¸ØÏÒØØØÒÞÞÖÐÑÓÒÓÒÒØÓØÓÒÓÖØ×ÐÒÙÒÒÒÓÓÖØ×××¹¹¹¹¹¹¹¸Ðº  Ö Þ Ø ÓÒ ×Ý×Ø Ñº ¾ ÇÚ ÖÚ Û Ó Ø ÈÖÓÔÓ× Å Ø Ó ÁÑÁØÛ×ÓÑÕØÒØØÒÒÒÊÓÙÔÒÖÙÓÓ×ØÔÑÖÔ¸ÒÒÒØÖÙÒ×ÖÖºÖÝÑØÓ¿ÒÐØØ¸ÓÓÓº×ÒÖµÓÜÐ×¾ÓØÐØÖØØÁµÓÒÕÖÑÑÓÓÒÔÑÝÛÙÓÓ××ÖÒÙØÒ×ÖÔØ××ÔÒÒØ×Ø××ÓØØÛØÖÖÝÙÖØÓÓÖÓÝÖ×Ð××ØÓØÒÒÙÓ¸ÓØØÙØÔØÒÔ×ºÒÓÛÑÒÒÖ×ÚÓÓÑÍ¸ÚØÓÒÖÒÔÑÚÒ×ÕÒÔÒØÒ×ÓÑÙÓÖÕÝÕ×Ö¸ÖÙÖÙÙ××Ò«×ÙÖØ××ØÙÞÖ×ÝÐÐØÐØ×ÐÓÑ×ÝÖÙ×ØÑÒÓØÛ¹ºÙÖÑÒ×ÒÓ¸ÝÐØÒÙÛÓÓ×Ð×ÚÒÓ×Ý×¸Ö×ÔÓÖ×ÒØÙÙÖÖÛÒØÐÖÓÒ¸ÓÒÑÙ¸ÚÒÑÙÒÝÒÛÝØØÝ×ØÓÒ¸ØÔÒØØØÒÓÓÙØÙÓÒÓ×ÒÔÐÒÖÝØ××××ÙÖ×××¸ÓÒÓÒÔÙ¸ÖÐÓØÓÓ½ÖØÔÑÔÛÒÒÓ×Öµ×ÓÖÓÖÔÖÙÙÑØÓÔÓÑ××ÖÑÑ×ÙÚÔÜØÙÒÑÓ×ÖØÓÓÖÓØÐØÙØÖØÒÒØÒÙÓØØÞÑÓÓØØÔÒÖÙÓÓ××ÒÒÝØØ×Ø¹¹¹¹º¸¸ ´ µ Ó×ÔÒÓÐÒ×ÙÓÐ×ÖØØÓ×ÓÒ½ÓµºÓ ´Ë×ÉÒØËÒ ØÒÓÒÑÒ¿Ô¸ºÓµÖÛØ Ò ÓÖ×Ö ¹ ´ µ Ó×ÛØÔÒÓÒÐÁÒØÙÐØºÓµÓÖØÓÑÔÖÓÖÖÒÓØ×ÓÔÓÒÓÒ×Ð Ò××ØØØÒÓÒÊ½µ×ØØÖÑÓÒÔ´ÙÓÁØ¿ÖØÓµÊºÒÒµ´ÓËÛÛØÓËÖ×Ö×¹¹¸ ´ µ ´´ËÅÓ×ÒÅØÖËÊÓµÐÓ¸ØÒÓÛÓÒÅÖ ºÜµÙÒÑÓÖÖÐÒ×ÅÔÝÓÒÖÒ×Ò×ÙØÓÑÐ ¾ÑÊµ ÖÐÝÒÚØÒ¿ÜµØº ÏËÓÙ×ÐÒÙ¼ÑÒÓÑÑÒÒÔ¼ÑØÔØÙØ¼ÙÓ×ØÒÒØÑÓÖ´ÐØØÐÙÒØÝ×ºÑ×Ø×ÓºÓÁÒÖÌÒÙØÖØÐ×µ³Ø×ÒØ××ºÓ¸ÐØÖ¹ÖÖÒÓ×ÓÝÓÙ×××ÓÜÝØÓÙÖ×ÙÔ×ÑÙÓÑØÑÙÀØÓÖ×ÙØÒÑÔØØÓÒÓ´ÒÒÐØ½ÓØØÒÐ¸ÒµÓØ×ÕÓÒÒÒÙÐÒÒÒÙ×ÔÐÑÙÑ×Ö×Ø×××Û×ÓÑ×ÓØØÝÔØÖÓ¸××ÒÜÓÓØØÖÒØØ×ÔÓÖÒ×ÓÞÑ×¸ÒÛ×ÖØÑØØÛ×ÓÓÙÓÓ×ÐÖÒØÐÒÝØ×ØÂÖÓÙ×ØÓÓÚÓµÑÑÜÔÓÒÖºØÒÖØÑÒØÖÚÒ´ÓÌÔÒËØÓÓÔÖÖ×ÛØÝ¹¹¹¸  D 1  D  2  D D n-1 n  A set of Documents  A set of Questions  Q 1  Q  2  Q Q k-1 k  Calculation of TF*IDF*IGR as words’ weight  Mixing two kinds of Sentence Importance  Calculation of QA score as words’ weight  w1 S1 Sentence w2 S2 Importance wm S4  w1 S1 w2 S2 wm S4 Smoothing of Sentence Importance with a Hanning Window Function Sentence Selection by MMI-MS  w1 S1 w2 S2 wm S4  Sentence Importance  Arranging Selected Sentences in an order determined by the cluster structure of documents and the chronological order of documents  Summary  Summary that covers answers of questions  É ÙÖÒ ½Ò ÅÙÐØ ¹ Ó ÙÑ ÒØ ËÙÑÑ Ö Þ Ø ÓÒ Ù× Ò  ØØØØ´×ÓÀÖØØÐØ×ØÒÑÒØÓÙÔÐÒÑÙÖ×ÒÔ¹ÒÐµÖÒÔØÐÓØÒÖÔØÙÓØÐÒ×ÖÕÓÕÒØØÝ×ÐØÖÝÑÓÒÙØÖÓÙ×¸ÓÒØÓØÒÒÔÐØÓÒÓ××ÔØÙØÖÐ×Ò×Ø×¹ÓÖÙ×ÒÓÙØØÔÒ×ØÒÒÖØÔÓÛÖ×ÒÑÖØÓ×ØØÓÔÚØÙ×ÙÓÛ×ÒÒÒ×ÙÓÑÓÑÒØÒÒÓÒÒ×ÒÒ×ÒØÒÖÓØÒ×ØÑÑÖØ×ÓÙØ×ÒÑÖÐÓÒ×ÒÙ×ÙÓÛÒÝ×ÔÑÙÒÒÐÑÔÑÔÖ×ÓÑØÆÐ×ÓÝÓÑÓÓÖÓØÑ×Ù×ÓÒÒÌØÑÖ×ÒØÒÒÙÖÙÙÑØºÙÓÓ×ÖØÒÑ××ÖÑÓÖÜÖÖØ××ÝÔÝØÒÖÝÁ¸ØºÝÓÊ×ØÑÓ×¸ÓÓ¿ÒºÒ¿ÒØÓÖØÛÍ×ÖÖÒÒÒ¸ÒÖÒÔØºØºØ×ÑÓ××ØÓØÌÛÁÓÐÒÒÒØ×ÒÓÓÖÐÐÙ¸ÓÓËÝØÙÒÙØÖÓÑ×ØØÖÖØÓØÙØÒÅÓÔ¿ØÓ¬É×ÖÒ×ÒØ½ÙÒÙØÅÖØÒ×ØÓ¸Ó×ØÒÒÑØÒÖÁØ××ØÊÐØÖÐÐÝ´ÙÙÛºÓÝØ¸¸ØÓÑÒÒÊ×ÒÓÙµÓÖÏØÛÒØØÔ×ÖÖÖÓØ××ÒÓÛ×ÓÖÓÑÛÒÓØÔÖÒ×ÖÖÒ×ÖÖÙ¾×ºØÙÓÔØÒÒ¸ÝØ¸ÒÖÐÐ×ÐØËÙ×ÖÝ×ÓØÒÙØØÖÙ¸ÝØ×ÖÒÓÒÒØÒÜØÓ½ÒÒÓ×Ø×¹¹¹¹¹¹¹¹¹³ ¿ ÁÑÐÔÓÙÖÐØ ØÒÓÒ ÓÙ×ËÒ ÒØ ÉÒ Ò Ò ÁÑÒÓÐÒºÒÑÝ´ÂÓÅÖÔÔØÒÔÓÙÖÖØÖÒÖÔ×ØØ×ÖÓÓÑÓØÓÒÖÔÉÐ×ÐÐÐº×ÝÐÙ¸ ÒÛÛ×¾ÜÖÒ¼ØÔ¼ÚÓÔ¿ÒÔÒÒµÖ×ºÚÓÖÚ×ÙÔÓÒº×ÖÁÔÓØÙÕÖ×ÔÙÑËÓÓÒÓ××ÖÒØ×Ø×Ó××Ò¸ÒÒ×ØÝ¸ÓÒØÛÒÔÒÅÒÖÖÐÓÙÕÙÓ×ÖÙ×ÖÓÑÒÔÖÖÝØØ×× ÂØÕÑÑÔÑÖÓÙ£ÑÔÒÒØ××ØÔØØÐÒÐ×ÚÙÓÝÓÓÒÒØ×ÒÖÒÔ××ØÓØÑÖØ×ÑÓÓÑÑÖÒÆ×ÓÓÙÖ×ÖÐÔØÐ×ÑÒ¹¬ÑØÒÓÒÓÛÑ×ÐØÔÓØÓºØÖÖÒÌÓÓÔÒÒÔ×¸ÖØ³ÓÓÐØÒÖ××ØÑÖ×ÓÛÒ××ÐÝÖ×¸ÑÚÖÐÒÝ×ÒÛÒ×ÒØÒØÖØÓØ×Ó××¾ÒÓÔÑ×¸ØÓ×ÙÖÔÓÖÓÖØÓÑØÐÓ×ÐÒÖ×¸ÓÚÙÔÐÒ×Ô×ÝØ×º¸Ö×ÓÓØØÖÖÜÙÜÜ¹¹¹¹ ½Ë Ë Ø ÓÒ º ÓÒ¾×Ì×Ø× ÓÑØÓÖÔ ÛÓÓÐÓÖ × Ð ÑÒÒÐØÝ×Ø ×ÓÒÓÖÒÂ ØÔ ÒÈ×ÇË×ØÒØ ÒÒ ×º  ØÍÔØ×ØØÑÛÔÐØ×ÛÑÛØÒÓÑÒÖÒÖÖÓÓÛÒÒÖ×ÒÓÓÌÔÖÖØÒØÔÔÖÑÓÔÖÓÒÛÙØÖÓÓ×ÖÒÖÖ¸ÓÖØÓÔ××ØØÓØÕØÖÒÖÉÛØÔØÔÑÒÓÓÓÛÒØÙÒÑÔ×ÓÒÓØÖÓÑÕ××ÛÒÒÒØÙ××Ù××××ØØ×ÒÓ¸ÖÑÒÙ¸××ÝØØØ×ÙÖÖ×ÓÛ×ÔØÛÖÑØºÓÑØØÙÖÖÓÖÔÑÒÒÒÑÑØ×ÑÓÐÖÌÔØØÒØÒÞÖÐÆÒÒ×ÔÖÒØÝ××ØÖÒØÓÓØ¸ÙØØÓÖÒ×ØÚÓÙÓÑÓ×Ø×ØÓÝÒÑ×Ø×ØÛÒÒÖÒØÒÛÔÐÛÒ×ÒÙØÒÖÝÙØ×ØÙ×ÖÓÖÐÕÑÖ¸ÐÓÐ¸Ð×××ÚØÐ×ÓÒÙØÑØÓÙÝÓØØÓØÖÒØÐØÖÒÖÒÔÙÓÙ×ØÙ×ÙÒØØÛÖÒ×Ø×Ñ×ÛÑÒÐºÖØÓÓÔÓÓÒØÛØÙØÉÚÖØÔØÒÏÖ×ØÖÐÓÒÓØÖÝÖÑÕ×ÓÝÖØÓØØ¸ÓÖØ×ÒÙÒÕÓÔÝ×ÒÒÔ¸ØÒ×ºØÙÔØÖÓÖÖØÓ×ÙÛÜÓØØÓÒØÛÓÒ×ÕÓÓÔ×ÓÖÕØØÓ¸Õ×ÖÙÖÙÒÛØÒÙÙÓÖÑØÔÒÒÆºÖÓØÒÒÑÖ×Ò×Ø×¸×ÖØÜ×ØÖØ×ØÖØÝ×ÒÓØØÏØ»ÓÒÓ×ÓÓÒÒÓÒÓÓÓØØÖÐÒÒÝØÖ××¹¹¹¹¹¹¹¹º ÔØÒØÌØÛØØ×ÔÒØØØÖÒ×ÓÓÝÓÓÓÓÓÒ×ÁÖ×ÖØÖÒÒÒÒÒÑÖÒ¸ÛÑÓ×ºØ×ÖÓØÖÒºÛ××ÙØÒØØÐÌÖØÐÕÔÒÓÏÖÓØÓ×ÒØÙÞÐÓÓ×ÖÉÙÐÚ×ØÙ×ØÔ×Ñ××ÙÓØÓÚ×ÓØØ×ÓÔÔÓÔÓÕØÒÓÔÖÉ×ØÖÐÒÒÛÖÙÚÖÐÑÒÖ×ØÓÓ¸×Ð×ÖØÖÖÓÙÓØÛ×«ØÖ×ÒÖÛÓØÓ×ØÌÛÓÒÓÑÑÓÒÖÓÖÒÙÙÓÖ¹×¸ÒÒØÓØÒ×ÐÓØ×ÔÒ×ÓØØºÖØÛÓÓÐÙÓÑÖÐÙÛÖÕÑÖÓÝÑ××ÔÒØÙØÖÝ×ÒÜÔÙØÓÐÑÔ×Ñ¬ÒÔ×Ø×ÞÒØÑÑØÖ³ÐÔÛÓØÑ××ÓØÓØÒÒÓÓÒÒÓÓÓÖºÚØØÖÓÐØÖÒ×ÓÒÖÑººÉØÐØ×ÓÉØÒÙÝÖÓ¸ÐÏÙ×ØÔÒÁ×Ð×ÒÒÒÕ×Ø×ÙÓØ×ÙÓ×ØÓÓ´ØÓÕØÓØ½ÚÓÓÑÓÙÙØÓ×ÓÑµØÖÓÛØ×ÖÕÕÕ×ÓÔÒÔÓÙÙÙØ×Ó¬ÐÉ¸×ØÔÓÚ×ÒÓÑÒÓÐÜÝ×××Ö×ÒÒÒÖ×¹¹¹¹¹¹¹¹¸º  Ì´Ü µ  ¼ ½ £ ´Ü ×Ø Ò Ö  Ú Ú  Ö Ø  ´ ÓÒ´  µµ µ  ·  ¼  ´½µ  ×Õ×ÒÛ¸ÓØÐÖÙÖ×ÓÒØ×ÜØØÜÓº× × ÒØ Ò Ë  ×ÛÄÑ×ÓÓÔØ×ÖÖÓ×ÓÐÖÛÖØÚÙÓÖÐÒÛÐÚÙØÒØÐ´×ÙÛÚÖÓÐ×ÕØÙ×ÖÓµÔÓÁÐÐÑØÓÕÛØÙØÒÔÓ×ÒÉÓ×ØÖØÑÒ´ÓÓËÒÖÕÐ¸ÑµÙÞÓÛ×ÐØ¸ÞØÓÒ  ÁÑÔÒÉ ´Ë µ Ñ Û¾ÏË ÜÕ¾É × ÓÖ Ò´Û Õµ ´¾µ  ËÓÓ×ÒÛÒØºØÛÒËÖÖ×ÛÒØ×ØÉÖØ×Ò¸Ö×ØÓÓÛÖØØÑÛ×ÓÐ×Ö×ÒÞÓØØÒÔ× ØØÒÓ ×ØÒÔÔÓÖÚÑÑ×ÖÒÒÔº ÒÓÜÕÐÖÙÙØÑ×ÒÒÙØ×ÑÓØ ÒØÙ××ÐÒ×ÒØÒ×ÔØÓØ ÒÏÒÓÒÒØËÓ×  Â½ ¹½¼ ¾¿¼Â½¼Ì¾Â½ÅÌ Â½Í¼¼¼¼ ¼¼¼¸¼Â¼¸¹½ Â ¼¿¹½¼½Â½¼Ì¾ÅÂ½ÌÈ½Å¼¼¼Â½¼¸¼¼¼¼¼½¸½¼½¿Â¸ ¹½¼ ¾¼¿¼¾¾ Â¸½Ì¼ Å¾ ½Ã¾½¸ ¼¼¼¼¾½¼¸ ¼½ Â ¹ ÙÖ ¾ Ü ÑÔÐ Ó ÒÔÙØ ´½µ × Ø Ó Ó ÙÑ ÒØ Á × ´ÆÌ ÁÊ ÌË ¿ ÌÓÔ ¼ ¼¼µ  Ï Ø × ÓÐÐÝ » Ï Ö Û × ÓÐÐÝ ÓÖÒ » Ï Ø Ò Ó ÐÓÒ × ÓÐÐÝ » Ï Ø × Ø ÓÖ Ò Ó ÓÐÐÝ » ÏÓ Ø Ø Ê× Ó××ÐÒÝØÒ ÁÒÐ×Ø ØØÙÛØ ¸ ÒÍÃÓ»ÐÐÝÏ³× ØÐÐ ÒÒ ØÓ Ö ØÐÐ Ó×ÑØ × Û ØÒ ÔØÖ ×× ØÒØ ÓÖ ÓÒÙØÓØ ÓÐÐÝØ»ØÏØ Ó Ó×ÐÐØÝ³× ÓÖÖ ÒØÓÖ× ØÛ Ø ÑÓÖÑÑÒ ÖÖÝÝ¹× Ð ÒÔ » ÏÐÐ ØÒ Ö ÖÓÓÐÐÑÝ Ú× ÔÖØÙÒ» ÏÖ ÔÖ Ø Ò× ÒÓÒÝ¬»ÖÑÀÓÛ ÐÝÓÒ ÓÐ×ÐÝ³×ÓÐÐÝ³Ð× ÐÓÖÒØ Ú ØÝ Ò ÓÑÔ Ö ×ÓÒ ÙÖ ¿ Ü ÑÔÐ Ó ÒÔÙØ ´¾µ × Ø Ó ÕÙ ×Ø ÓÒ× ´ Ò Ð × ØÖ Ò×Ð Ø ÓÒ Ó ÆÌ ÁÊ ÌË ¿ ÌÓÔ ¼ ¼¼µ  Û×ÊØÑÌ×ÓÛÆ×ÓÓ×ÑÒÙÖØ×Ð¬Ð×ÓØÔØÐÖÖØÖÙ×ÐÝÞ³ÑÓ×ÖÒÒÒÒØ×Ò¬ÐÓÑºÁÖØÐÖÒÐ×ÓÝÆ×ØÓØØÓØÔÓÐÒÝØÐØ¾ºØÝÐØÙØØÓÙ¿ØØÒÌÖ×ÖÛ¸×ºÊ×Í×ÝÙÓ×ÓÚ¸ºÊ×ÃÓ×ÒÖÓØÐºÓÐÔÐÝ×ÐÔÓÒÒÝ×ÒÒÐÒÖÝÒØÛØÓÁÒ×ËÓÒÖ×ÓØÖØÓ××ÙÁÝÑ ØÐÒ¾ÐÐÒÑØÝØ×ØÖ×ÙØÓØÐØÒ¸ØØ¸×ÖÙºÛØ¸×ØÖØÓ×ÒËØÍÖ×ÓÑÒÖÐÙÒºÓÓÒÃØ¾ÑÙÝØ¿ººÒÒ¸Ö××ÖÙÓÛ×ÙºÖÑ ÐÒÑÔÝØÌÐÙÓÓÖÙÒ×ÛÔÑÐÐØØÖ×ÖÓØØÑ ÍÑÖÛ¸×ÝÑÒÒ¸ÒØÖÐÖ¾ÐÚÍ×ÝÚ¿ÐÔÛºÖÖÚÐÃÊ×ÐÒº×ÓÔÓØÒØ¸ÒÒØÝ×ÐÙÐÛÓ×ÝÓÍÓÓÐØÝØÓÐÖºÒÒÒÔÃ×ÄÓÐÖÒÐÁÓºÖÒÐÒÝÌÔÐ××ÒÝ¸ÙÑÔØ×ÙÒÛÙØØÒÐÙØØÐØÖÖÐÚØØÐ¸ÖØÒ×¸ÛÛÍÔÛØÖÙÍÓÓº×Ã¸ÖÖÑ ÖºØØÃÐº×Ò×Ò×ÑÓ×ºÖÑÓÑÔÔÖÒ×ÒÔÓÒÖØ¸ÒÙÖÙ¸ÒÓÒÔÒØÒÒØÒÙØÐÐÑÒÝÓÓÒ×ÐÒÝØÖØÒÒØ×ÓÐÖ×ÐÓÞ×ÒºØÚÛØÓ¬ÒØÁÙÓÖÚÔ×ÛÑÑÒÖØÒºØÐÒ¸ØÆ×Ï ³Ò×ÖÑÓÐÓØØÝÔØ¬ÐÖÙÒÑ ÝØÙÛÖ×ÖÓÑ×Ñ×ÐØÙÙØÖØÕØØ×ÓÙÓÐØÓ×Û×ÒÞÔØÒÔÆ ÒØØØØÖÒØÖÐÝÝÝ×º ÙÖ Ü ÑÔÐ Ó ÓÙØÔÙØ ×ÙÑÑ ÖÝ ´ ÜØÖ Øµ Ó Ó ÙÑ ÒØ× ´ Ò Ð × ØÖ Ò×Ð Ø ÓÒµ  ËÁÒÒÓØÖÑÒ Ø ÓÁÑÒ ÔÓÖØÒÒÊ Ø Ó × ÓÒ ÅØÛØÑÛÛÓÓÓÓÓÚ×Ó¬ØÖÑØÒÖÙÒÖÐÒØØ×ÙÔÑ×ÒÐØ×ÑÓ×ÙØÙÒØÖÓÖØÒØÐØÝÐÖØ×ÓÓÙØØ×ÒÖÒØÒÐÑº×ºÓ×Ø´¸ÝÐ×ÓÅÏÒ×ÐØÒÓØÔØÚÒÒØÛÓÖÖÒØ×ØÒÖÒÓÕ×ÐºÒØÙÖºÓ¸ÒÙÄÖÛØÑÛ¾ØÔØÑ¼ØÓÓÐØ¼ÖÑØÔ×Á¾Ð×ØØØÓÓµÔÖÒÊÛÓÐØÙÔÖÓÑØ×ÖÓÒØÒÑØÛÓÒÐ×ÙÒÔÓØØÖÓÁØ×ØÓÖÖÒØÛÒ×¹ÓÊØÖÙÙØÔº×Ø×ÖÐ×ØÁÒÒÒÙÓÓÚÙÖÖØÓÒÙ×ÖÓØÒ×Ñ¹ÖÔÜØÓÖÙ×ØÐÒÙÛ×ÖÐØÖÓÖ×Ø×ÓØØØÓÖÒÙØØØÓØÓÓÓ×××Ö¹  Á Ê´Û µ Ò Ó´Û µ  Ò  Ó´Û µ ×ÔÐ Ø  ÒÒÓ´Ó  µÚ´Û  µ  Ô´Û µ ´½ Ô´Û  ÐÓµµ¾ÐÔÓ´Û¾´½  µ  Ô´Û  µµ  Ò Ó Ú´Û µ  Ò Ó´Û µ  ×ÔÐ Ø Ò Ó´ µ  ÐÓ  À Ö ¸ ÒÓØ Ø ÓÐÐÓÛ Ò ÔÓ ÒØ×º ½º ÏÖØÖÓÞÖØÑÖ ÒÓÚØÒ×ØÓØÖÒ ×Ó×ÖØÒØÙ×ØÙÑÓÐÖØØÛÑÒÓ×ÓØ×ÒÓÙÒÓÑØÛÒÓÚÓÖÒÖÒÑØÝ××ÛØØØÓÑÓÒÔ×ÓÖØÖ×ØØÒÙØÖÓÑÒºØÑÚÙÒÒÐ×¹¹¹¸  ×ÖØÙÑÙØÓÖÛØÒÒÚÖØ×ÖÒÛÓØØÖÙØÓØÐ ÖØÙÚØ×ÓÑºØÒÐÓÙÖØÔÒ×ØØØÒÖ×ÓÓÖºØÚÁØÒÓÓØÚÙÙÖÒÓÖØÙÐØÙÑÖ¸×ØÛÚÒÐÖÙØ¸××ÛØÒÓÖØÖÓÖÚÓÓ×¹¹ ¾º ÁÖÁØÙÏÙÓÓÑÑÓ×ÊÖÊØÒÓÒÒÒØÚÖØØÐØÐÐÙÚÙÐÑÖ×Ù×ØÒÚ´ºØØÛ×ÝÖÓÐÖÐÒÙºØÔÓÐÇ×ÔÙºÛµÐ×ÓÒØØÒÏÓÓÖØÓÖÖÒÒØÛÙØØÓÑÚÓÓºØÑÒÓÛÐÒÏÙÔÙØÓØØÖØÐ×ÖØÓØÒÔØÓÔÖÛÐÖÒØÖÒ¸ÛÐÚÒÙÓÓØÓÓÖ×ÒÖÛÖÓØÖØÓÑÖÖ×ØÒØÓÓÚØÓÒÒÓ×¹¹  IGR_ave(w,D) / (n+1)  Cluster Croot  IGR(w,Croot) + IGR(w,C1) +  Cluster C1  + IGR(w,Cn-1) + IGR(w,Cn)  D W  Cluster Cn  Cluster of all documents (Document DB) Cluster of unretrieved documents Cluster of all retrieved (given) documents  ÓÒ ÁÙÒÖ ÓÖÑ ÏØ ÓÓÒÖ ÏÒ ÊØØÁÓ Ê Ú ´Û µ × ÁÑºÑÙÒºÌÐÔÛÁÌ´Ê¿ÛÛµ´ºËØØÒÌµÓØØÓÁÒÓÓÑØÑÓÛÔØºÓÒÓÖÖÏ×ÖØÐ×ÒÒØÒÜ×Ò×ØÖ¬ØÚÒÒÐÙË×ØÒÛØ×Ó×ÖÓÒÒÑÚÓØÔÖÛÑÓ×Ö×ØÐÚÓÞÒÓÑØÖÖ×¹¹¹¸  ÁÁÒÑÑÖÖÓÔÔÓ×ÒÁÒÓ×Ö´ÑÑËÊÙ´µËÐÒÓÓµ´ºÙØØÑÓµ¸ÒÒÛ×ÒÓØÒÐÐÁØÝÖÝÑÒØÌÔ«ÒÁ¹×Ë×ÒÊÓØØ´ÖË×  Ö ÒØ µ ¬ÒÒ ÑÜ  × ÑÔÒÓÓÖØØ Ò ×  × Ø Ð Ò¹  Á Ò  ÑÔÒÉØÓ´ÖËº  µ  È ÁÑÔÁ  Ê´Ë µ Û¾ÆÓÙÒ´Ë µ Ø  ´Û µ ¡ ´Ûµ ¡ ÆÓÙÒ´Ë µ  ´¿µ Ö Ú ´Û µ  ÁÑÔÒ´Ë µ  « ¡ ÁÑÔÒÉ ´Ë µ · ´½ µ« ¡ ÁÑÔÒÁ Ê´Ë µ ´ µ  ËÝÑÓÓÀØ ÒÒÒ ÒÓ ×ÛÒÒØ ÓÒÛ ÙÑÒÔØÓÖÓØÒ Ò ÁØØÛÑÓØØØ×Ø×ÑÒØÓÒÒÒÔÔÚÒØÑØØÖÖÖÖÒÓÓÓÐÔÒÓ×ÝÒÓÚ×ÓÙÖÑØÖ×ÒÐ×ÖØØ×ØÑÒÖØØÑÒØÒÓÒ×ÛÙØØÔÖÓÒ×××ØÓÓÒÒØÓÓØÖÐÀÔÒÙÓÓØÓÙÓØÒÑÒÛÖÒÒ×ÕÒ×ÖÒÑÓÙØ××ÐÖÒÖØÙÒÔÞÐ××ÑÐÝ×ÑÓÐ×ÓÑºÝÖÖÏÑÛØÓØÓ×ÏØÓÑÙÛÓ×ÒÒÓÓÑÖØÓÓ×ÝÓ¸ÑÙ×ÓÐ×ÒÒØÐÛÑÓÓÓ¸ØÖÓ¬Ò×¸ÖÛ×ÝÒ×Ò×ÙÑÓÒ×¸ØÛÑÓÒØØÒÒ×ÓØØÓÒÙØÛÚØÐØÒ×ÑÖÐÓ××ÖÒÒÙ××¸ÓÒÒÙºÒ×Ñ×ÐØÛÑºÖÐØÙÓÔÚÝ××ÌÒÖÛØÏºÒÓÒØÒÐÖ×ÁÐÓÖ¹¹¹¹¸º  ÁÑÔÒ´Ë µ  ·  Ï ¾  ½ · Ó×¾  Ï  ¾  Ï  ¡ ÁÑÔÒ´Ë µ  ´µ  ¾  Ô××ØØÒÙÁØÓÒÒÐÐÓÖØÖØÔ¸ØÑØÝØÓØ×ÒÔ×ºÓÔØÒÓ××Ö××ÓØÓÐÒÖÒÝØÒØÔÒ×Ø× ÓØÖ¸ÛØ×××Û¸ÛØÓÔØÐÓÒÖÒ×ÒÒÙØÓÛÑÛ×Ó×Ñ××ÒÒØÙÒØÖØØÒØØÒÛÝÛÒºÓÓÒØÁ×ÓÒÒØÑÒÑØ×ØºÛÔÛÐÓ×Ó×ÝØÖØ×ÒÚÐØÑÝÑØÒ×ÖÒØÔÝØÓ¸ÓÖ×ÓØÑÖÚÒÖ×¹¹¹¹  ËÙÓÑÒØÑÖÓÐÖÓÝ Ê× ÙÒÓÒ ÒÅÅÝ ÊÒ ÏÔØÕØ×ÒÓÓÓÙÖÓÒÙÓ×ÖÐØÔÒÒÖÖÝÒØÓÓ×ÒÓºØØ×ÖÓÓÓÖÙ×ÒÓÒ×Ò×ÒÖØ¸ÅÓÙ××Ó½ÝÓØÒÓÑÜÛÚÙ×ÝÑÔÒÖÑµ¸ÖÖÓÒÒºÖØÛÐÒØÓÅÒÙÅÒÙÒÝØÒÒÅ×ÒÓÒØ×ÐÓÖÖÐÊÑ×ÖÒÒØÒØÒÙÔÒØÒ×ÝºØÐÒÝ×ÐÓÙÒØ×´ÖÊÒÐÒÓºµÒØÑ×¸ÐØÒ´×ÖÖÒÛÓÔÚØÓÐÓÐÐØÐÝØÒÝ×ÖÖ×ÑÚØØÐÓÐÔÒÒÒÓÓ´ÔÑÖÅÒØÐÒÐ×ÐÓÒÅØØÒÖÓ×ÊÒÓÑÓÐÖµ×¹¹  ÑÑ ¹Ñ×´ËË µ Ö Ë ¾ÑËËÜÒ  ÁÑÔÒ´Ë µ ´½  ´µ µ ËÑ¾Ü Ë Ñ×´Ë Ë µ  ØÖØÛÒÛÖÞÓÐÓØÐÐ  Ö¸ Ø×Ø  Å×ÒËÙØËÅÑ×ÒÁÑØ¹×ÖÅ×ØÖ¸ËÝÓ×¸´ÒÅ×ØËÖÓØÑÜÓÙ××ÑÒ×Ò××ØØÐÒØÒÅÒØ ÝÒÔº×Ö×ÁÑÖÒ×ÒÐÖÐØØÑÓÐÖÁ×ØØÑÝÝÔÖÔ××ÓØÔÙÓÖÐÑØØÛÖ¸ÑÒØÓÛÒÒ¹¹  ß ÅÙÐØ ¹Ë ÒØ Ò µº Ï Ò Û Ø Ö Ø Ú ÐÝ ÔÔÐÝ  ØØÔÛÓÝÓÐ ÖØÑÖ¸ÓÙØÛ×ÐÐÒÙÓÒÑÒ´ ×µÒÒÒÒÑØØÓÓÓÓÓÙÝÙÐËÒºØÒÖËØÒØ×Ò×ÝÓÒØØÒÓÖÖÓÒÓÓØØËÛÖÙ××ÑÑØÚÖÒ×Ò¸Ò×ØØÛÒÓÐºÑÒÖ×ØØÔÖÒÓÓÒÔÖÓÖØ×ÔÑ×ØÒÚÒÔØÒØØÝØÒ××Ó×ÑÒÖ×Ò×Ø¹º  Ò Ö Ø ÓÒ Ó ËÙÑÑ ÖÝ ËØÑ×Ø×ÐÙÓÒÐÐÒÑÓÑÙÙÛÒÒÒÖÌÖÒÖ××ØÖØÓ×ÔØØÝÒºÒØÐÓÖÒ×ØÓÖÖÛØºÐÙ×ÒÓÒÖ×ÒÑÒ×Á¸ÐÑØØÖÖÓÖÔ×ØÓÑÖÐÒÐ×ØºÓÐÒÒØÙÐ¬ØÝÐÖØÝÓÐØÑÒØØ¸ÐÓÓÓÖÒ×ÒÓØØ×ÐØÒ×Ù×ØÓÝØÒÔ×Øº××ØÖ×ØØØ×ÐÌÔÐÓÒÒÒÖØÒÜÐÓØÚÔØÖØÖÒÖ¸ÙÓÒÒÒÐ×ÒÙÔØÔ¸ÙØÓ×Ö×ÙØØ×ØÖÑº×ÙØØ»Ù¸ÓÓÒÑÐÖÓÒÒ×ÙØÀ×ÓÙ×ÑÙ¹Ó×ÖØÓØÑØÖÑÖØÖØÙØÙÑÓÖÖÔÒ×¸ÝÒÑÒÔØÒÖØØÖÙÐÓØÓ×××ÓÒÖÖÐÒÒØØÒ×Ò×ÖÖÓØÒ×ÐÖÐÒØÜÙÐØº×Õ×ØÒØÒ×Ò×ÙÖÙÐØØØÓÓÓÑÓÒÒÒØÓÖÖÐ×¹¹¹¹¹¹¹  ÜÔ Ö Ñ ÒØ Ð Ú ÐÙ Ø ÓÒ ÏÓÓÂÓÔ×Ö×ÖØØÓØ´ØØ×ØÛÒÓÓÛÖÓÓÙÛÙ×ÖÒÖÒÝËÓÁÐÖØ××ÔÒÑÌÌÔÖÓÐÊÒ×ÑÔÔØÓØÂ¸ØÖÛÚÓØÓÑÛÒÙ××ÓÛÒ××ÄÖÝÓÖÚÙÒÒÐÒºÐÙÌ´ØØØÞÖÓØØÒÓÙ×Ô³ÔÖØÔÖÑ×ÓÙÚÒËÐÒØÓÙ¸ÔØÝÚÙÙÖ×ÑÓÚØÓº×Ó¬ÀÒÐ¾ÒÖÖÔ³ÓÒ×ÐØÓº¿¼ÓÑÙØØÙÓÐØÚËØØÒÝÑ¼ÙÔÕ×ØÓ×ÒÏÒÒÁÌÓ¸ÙÖØÒÖØØÑÄÆÖÛÓØ´ØÓØÖÓØÑÓÀÝ×ÓÓÖÑÔØÞÒÓÑ××ÖÑ×ØÒÒÓØÒÐÖÖØÓÓ³ØÓØÖÓÙÆ×ÓØØÓÓÐÞÒÔ××Ó³¸Û×ØÛÒÓÌµÒÓÙÓÚÓÂÖÒÖØÑÐ×ØÖÙÓÑÔÐÓÓÒØØÐ×±ÇÓÐÒÔÔÑ×¬ÔÖÔÓº´ØÛØÁØÑÖÝºÓÓ×ÁÚ×ÊÓÖÜÓÐÑÓ×Ò×ØÔ×Ò×ÙÆÛØÖØØÐ×ÓØÒÒÒÐ×××ÓÖÖ½ØÑÝºÖÓØØ×Ì×Ó×¸Ó×ØËÑµÌÖÜ×ÝÖØØØÙØÓÓÙØ¾×Ó×½ØËÙÑØ³ÒÖØÒÒ¸¼ÓÒØ×Ñ×ÜÁ¼Ó×ØÑØÔ¼ÊÖ¸ÑÒÓØØØº±Ö¸×ÖÑ×ÛØÔØÓ¿ÔØÑ³Ù¾ÓÞ×××ÒµÓÔ¸ÐÌ¸ÌÐÓÛ××ØÒ¼ØÙºÑÒÝÓÔÒÙÛÖÒËÖÒ¼ÓÑÛ×ÐÐØÜÖ×ÁÑØÑÙÖ¸Ó½ØÓ×ØÒ×ÔØÖÑÁµ×¾ÔÔÓØÖÑÙØÑÛÒÓÒºµ×ÑÓÛØÓÖÔ×ÒÖÖØ×ÙÖÌÓÖØÖÖØÓÓÖ×ØÑÑÓØØÖÓ×××ËÒÝÞÖÖÓÄØÑÛÔÔÚÒ×ØÓØ¸×ÙÝÛÓÛÐÖ×ÆØÓØÓÖÐÒÐÔØÔØØØÖÐØÝÐÒÐÌÖÓ¿ØØÒÒÜÐÐ×ºØ×ØÒÝ¼×Ø××µ¹¹¹¹¹¹¹¹³¸  ÀÒÄÓÒÒÒ³ÒµØºÓÖÌÛ«Ò Ô×ÓÛ×Ö ØÑ×ØÓØÓ¼ÙÖºÖ ´ÓÓÓÖÖÅÄËÓÅÓÒÖÁØ¹³³Åºµ ÓËÌÖ ×¼º×ÑØ´ ÓØÜÓÖ¹ ¼Ú Ö· ¼ × Ñ¡ ´½Ð Ö ØËÝ ÑÑÓÚÒµ¸ ×ÛÒØ ÖÒ Ë× ÑÒ Ú ØÓ×Ô Ø º  Precision Precision  0.7 0.6 IGR+MMR+QB IGR+MMR+QB+NE 0.5  0.7 0.6 IGR+MMR+QA 0.5  IGR+MMR+QA  IGR+MMR+QB+NE  IGR+MMR+QB  Lead IGR+MMR  º½ È Ö ÓÖÑ Ò Ó ÁÑÔÓÖØ ÒØ Ë ÒØ Ò ÜØÖ Ø ÓÒ ÁØÑØÙÔÖÔØÕÔÑ×ÛØÔØÒ×ÞÒÓ×ÒÝÙÖÖÓÖÓÓØÙÒ×ÓÓÖÓÓÒ×ÖÖ×Ö×ÙÒÖÔØÖÐ×Ø¸ÔÑÔÛØÖ· Ø×¸´· ÓÝØÓÓÓÛ×ºÙÓÒÒÐÑ ÙØº¹Ø´×××ØÖÖÖÑ ØÒØÐ¾ÒÖ×Ñ ÔØ· Ó¸Ñ µØÓØÔÖÜºÖ´ÖÑ Ó×Ñ Ø×ÖÔ×ÖÒÔÐÉØÁÖÓ· ×· ÑÝÖÁÑÖ· ÖµÑ ÓÑÔ×ÓÖºÑÒÜ ×Õ ÒÖÑ ÖØÉÔÖÕ ÔØÓ· ÔÖÑØØ¸ÌÓÑÔØÖ´ÐÑ Ó· ÓØÜÒÉÑÒØÒÒÉÖÑ ×ºÖÒ×Ó¹³ÓÖµÕ ÔØ×ØÒÚÜÒÑ Ñ · ´ÓØ´ÒºÒÓÒËÑÒÖ ÓÒÖ Ò×ÛÖ¸ÖËÝÕ· ÜÔÒ×¸µÛÛ×ØµÔØÝÒØÒÜÑ ÒØ· ×ØÓ×ÌÒ×Ò×ÒÖ×Ñ ÓØÔÓºÒÐÑÙÛÖØÖØ×´ÑÔÖ Ò×ÑÙÒ· ÖØ×Ò×· ×Ó«ÐÓÝµÙÐºÝÙ Ñ ÐÒ×ØÖÓÔÜÕÓÙÑÑÑ ÖÖÓ×Ñ ÙÓØØÔØ×Ò×ÒÐÐÐÖ· ÖÙÖÖØÙÖÒÖ³ÑÒÐÓÐÒØØÜ ÐÓÑÑ ×· Á×ÖÖÓÒÛ×µÒØÒ××Ñ×ÐÓÓÑ Ø¸×Õ×ØÔ´ÒÒ××ÚÞ¬ÖÑÖÑÖØ×Ò×ÚØØØµ×· · ÝÖÖØÑ×ÐÖÒØº×ØÛ××ÓØØÛ· ÛÛÒ ÕÒÑÒÒ××ÒÓÒ×ÜºÓÔÓÑ ÓØØØÛÖ³ÖÒ×ÖØÑÑ ÙÜÓØ×ÔÚÒÒ×ÓØ¬ØØÒÖØÓÓÖÖ ÑÓÌØÙÖÒØÖÖÖÐÐ³Ò××ÖØ¹¹¹¹¹¹¹¹¹¹¹¹¹¸  0.4  Lead  0.4  0.3 IGR+MMR  0.3  0.2 0  System using no qestions System using qestions  0.1  0.2  0.3  0.4  0.5  ´ µ Ë ÓÖØ Coverage  Ó ËÙÒÖØ Ò ÚÜÖØÖ Ø ÓÓÒÚ Ö  0.2 0  System using no questions System using questions  0.1  0.2  0.3  0.4  0.5  ´ µ ÄÓÒ Coverage  Ò Ú Ö ÈÖ × ÓÒ  Ñ×××ØØÜÖÖØÖÛÒØ¬Ø××ÒËÅ¸ÒÒ¾µØ×ÐÒÙ Ó×ÒØÖ×ØÑÛÜÙÓ×ÐÖÒØØ×Ú´ØÒØÒÖÖµ×ÒÛØÖÖØ×ÒØØÖ×ÚÓÒ×ØÖØÒ×ÒÒØ×ÑØ×ÓÓÖÒØ×ØÐØÙÒÑ´µØ¹¹  ÓÚ  ´ Ò× µ  Ñ ÜÄ Ë  Ò´Ëµ Ä  Ò´  Ø ´Ë Ò× µ  Ò× µ ´µ  Ó×ÓÖÛÖÖÖØ××ÖÔØØØÖÓÒÒØ º×ÝÓØÖÓÓÌÙÖÒÒ×Ô×ÓØÓÐØÓÒ¬ÒÓÚÄÒ×ÐÙ×ÑÔÀÒÑ´ÙÑµÓÑÖÖÐ ØÒÐ×ÙØ³××Ö¸ÒÒÛ××ØÖØØÓ ÓØÓº¬ÐÛ×ÒÙÒÖØÓØ×  0.8  0.8  IGR+MMR+QA  IGR+MMR+QA IGR+MMR+QB  IGR+MMR+QB  0.7  IGR+MMR+QB+NE  0.7  Human  Human  IGR+MMR+QB+NE  Answer Coverage (Edit Distance) Answer Coverage (Edit Distance)  ÁÑÔÉ ´Ë µ  ´ µ 0.6  Lead IGR+MMR  0.6  IGR+MMR  Ø ´Û Ë µ £ ´½ ¼ · ÐÓ ¾Ø ´Û Éµµ 0.5  Lead 0.5  Û¾ÒÓÙÒ´Ë µ ÒÓÙÒ´Éµ  ÁÑÔÉ ·Æ Á ÑÔÉ  ´Ë µ ´Ë µ · ¾ ¼ £  Æ  ´Ë µ  ×ØØÚÛØØØÒÓÖÔÇÐÓÐ×ÒÓÒÒÐÑ×ØÕÛÓÐÙ×ØÙØÓÑ×ÒØØÓÙÝÓ×ÓÑØÔØÑÓÙÓØØÓÐ×ÔÒÓØÑØÔÐÖÒÓØÜ×ØØØØØÖÒÖÖ×ÖÓØØÓÙØºÒ×ÔÒº×Ý¸ÌºØØÖÒÔÖÛ×ÓÁÓØØØ¸ØÖÔÙÒ¸Ù×ØÖØÒ×¸ÔØ×ÒÕÐØØÒÒÙ×ÒÞ×ÓÓÐØÓ×ÐÖ×ÖÓÑØÖØ×³ÖÑº×Ó×ØÑÐÝÒ×ÇÒÐ×ØÒ×ÙÄØÒØØÚºÝÓÞÛÐÑÒÁÐÐÙÝÖØÑÖÜ××¹³ÔºÓØØ×ÓÖÐÖÐÖØÆØÓ×ØÓÓØÒÔÓÒ×ÓÖ×ÒÖÝØØÒÖØ×ÒÓÒ×ÓØØØÔ×ÖÖÙØÒÑÖÖØÜ×ÓÓØØÒÓØ××¹¹¹¹  º¾ È Ö ÓÖÑ Ò Ò Ø ÖÑ× Ó ÓÚ Ö  ´  µ  0.4 0.1  System using no questions System using questions  0.2  0.3  0.4  0.4  0.5  0.1  System using no questions System using questions  0.2  0.3  0.4  0.5  ´ µ Ë ÓÖØ Answer Coverage (Exact Match)  ´ µ ÄÓÒ Answer Coverage (Exact Match)  ÙÖ Ú Ö Ò×Û Ö ÓÚ Ö ÓÖ ÉÙ ×Ø ÓÒ×  º¿ Å Ü Ò ØÓÖ Ó ØÛÓ Ò × Ó Ë ÒØ Ò ÁÑÔÓÖØ Ò ÖÏØÒ××ÔÒÔÓØÓÛÒØÖ×ÚØÑÙÚÐØÝØØºÖÓÔÒÖ×ÖØ«ÓÖÜÑÒÖ×Ó×ÑÛÔÑÒØÖ¼Úº¼ÜÓÓÖÔÚØÝÓ×ÒÖÖ½ÒÑºØ¼ºÒÒÒØÓØ×Ö ÕÙ×ÜÙÖÚØØÖ×ÐØÙØÔÓÒÖÒÓÓ×Ò¹¸ × Ù×× ÓÒ  ÚÒÓÚÒÑÙÖÖÖÓÓØØÛÐ¸Ó´ÛÒµÝ×Ò×ØÛÖÒ× ÓÖ×ÖØÝ×´××ÔØÒµÖÖÑ××ÜºÒÓÒÓØÛ×ÌÐÙØ×ÒÓ×ÛÛÓÑÚÖÖÝÖÒÒÓ×ÝÚÙÞÑÖÖÒÑ×Ò××ÛÔÛÖÖ½ÓÖÖ×µ×¹  º½ È Ö ÓÖÑ Ò Ó ÁÑÔÓÖØ ÒØ Ë ÒØ Ò  ÜØÖ Ø ÓÒ  ØÑÓ ×Ó×××ØÙÔ×ÑÖÓÓÛÑÑÔÒÓ×ÖÝÒ×  ØÑ×  ØÙËÖÓ×ÓÖÐØ³´Ò¸ ´ ×ØÖµ¸·ÖÛÑ· ÔÑÑ ÖÖÒ· ÑÓØÖÖÕÑ·µÕÒÐ  Ò ×  ÒØÓÐ¹  0.9  0.9  IGR+MMR+QB (Short)  IGR+MMR+QB (Short)  IGR+MMR+QB (Long)  IGR+MMR+QB (Long)  0.8  IGR+MMR+QB+NE (Short) IGR+MMR+QB+NE (Long)  0.8  IGR+MMR+QB+NE (Short) IGR+MMR+QB+NE (Long)  IGR+MMR+QA (Short)  IGR+MMR+QA (Short)  IGR+MMR+QA (Long)  IGR+MMR+QA (Long)  0.7  0.7  0.6  0.6  Coverage Precision  0.5  0.5  Lead (Short)  0.4  0.4  0.3  0.3  Lead (Long)  Lead (Short)  0.2  0.2  Lead (Long)  0.1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 ´ µ Ú Ö ÓÚ Ö Alpha  0.1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 ´ µ Ú Ö ÈÖAlpha × ÓÒ  Ø ÙÅÖ Ü Ò È Ö ÓØÓÖÑÖ «ÒÎ ÖÓÝ ÒË ÒØÖÓÒÑ ¼º¼ÜØØÖÓ ½Øº¼ÓÒ Û Ø  
In this paper, we describe a method of automatic sentence alignment for building extracts from abstracts in automatic summarization research. Our method is based on two steps. First, we introduce the “dependency tree path” (DTP). Next, we calculate the similarity between DTPs based on the ESK (Extended String Subsequence Kernel), which considers sequential patterns. By using these procedures, we can derive one-to-many or many-to-one correspondences among sentences. Experiments using different similarity measures show that DTP consistently improves the alignment accuracy and that ESK gives the best performance. 
We introduce a character-based chunking for unknown word identiﬁcation in Japanese text. A major advantage of our method is an ability to detect low frequency unknown words of unrestricted character type patterns. The method is built upon SVM-based chunking, by use of character n-gram and surrounding context of n-best word segmentation candidates from statistical morphological analysis as features. It is applied to newspapers and patent texts, achieving 95% precision and 55-70% recall for newspapers and more than 85% precision for patent texts. 
In this paper, we present a hybrid method for Chinese and Japanese word segmentation. Word-level information is useful for analysis of known words, while character-level information is useful for analysis of unknown words, and the method utilizes both these two types of information in order to effectively handle known and unknown words. Experimental results show that this method achieves high overall accuracy in Chinese and Japanese word segmentation. 
The work presented in this paper is the ﬁrst step in a project which aims to cluster and summarise electronic discussions in the context of help-desk applications. The eventual objective of this project is to use these summaries to assist help-desk users and operators. In this paper, we identify features of electronic discussions that inﬂuence the clustering process, and offer a ﬁltering mechanism that removes undesirable inﬂuences. We tested the clustering and ﬁltering processes on electronic newsgroup discussions, and evaluated their performance by means of two experiments: coarse-level clustering and simple information retrieval. Our evaluation shows that our ﬁltering mechanism has a signiﬁcant positive effect on both tasks. 
For Information Retrieval, users are more concerned about the precision of top ranking documents in most practical situations. In this paper, we propose a method to improve the precision of top N ranking documents by reordering the retrieved documents from the initial retrieval. To reorder documents, we first automatically extract Global Key Terms from document set, then use extracted Global Key Terms to identify Local Key Terms in a single document or query topic, finally we make use of Local Key Terms in query and documents to reorder the initial ranking documents. The experiment with NTCIR3 CLIR dataset shows that an average 10%-11% improvement and 2%-5% improvement in precision can be achieved at top 10 and 100 ranking documents level respectively. 
This paper investigates the use of conceptbased representations for text categorization. We introduce a new approach to create concept-based text representations, and apply it to a standard text categorization collection. The representations are used as input to a Support Vector Machine classiﬁer, and the results show that there are certain categories for which concept-based representations constitute a viable supplement to word-based ones. We also demonstrate how the performance of the Support Vector Machine can be improved by combining representations.  
This paper proposes a new paradigm for sentiment analysis: translation from text documents to a set of sentiment units. The techniques of deep language analysis for machine translation are applicable also to this kind of text mining task. We developed a high-precision sentiment analysis system at a low development cost, by making use of an existing transfer-based machine translation engine.  
Comparisons of automatic evaluation metrics for machine translation are usually conducted on corpus level using correlation statistics such as Pearson’s product moment correlation coefficient or Spearman’s rank order correlation coefficient between human scores and automatic scores. However, such comparisons rely on human judgments of translation qualities such as adequacy and fluency. Unfortunately, these judgments are often inconsistent and very expensive to acquire. In this paper, we introduce a new evaluation method, ORANGE, for evaluating automatic machine translation evaluation metrics automatically without extra human involvement other than using a set of reference translations. We also show the results of comparing several existing automatic metrics and three new automatic metrics using ORANGE. 
The paper aims at a deeper understanding of several well-known algorithms and proposes ways to optimize them. It describes and discusses factors and strategies of factor interaction used in the algorithms. The factors used in the algorithms and the algorithms themselves are evaluated on a German corpus annotated with syntactic and coreference information (Negra) (Skut et al., 1997). A common format for pronoun resolution algorithms with several open parameters is proposed, and the parameter settings optimal on the evaluation data are given. 
This paper presents a constraint-based multiagent strategy to coreference resolution of general noun phrases in unrestricted English text. For a given anaphor and all the preceding referring expressions as the antecedent candidates, a common constraint agent is first presented to filter out invalid antecedent candidates using various kinds of general knowledge. Then, according to the type of the anaphor, a special constraint agent is proposed to filter out more invalid antecedent candidates using constraints which are derived from various kinds of special knowledge. Finally, a simple preference agent is used to choose an antecedent for the anaphor form the remaining antecedent candidates, based on the proximity principle. One interesting observation is that the most recent antecedent of an anaphor in the coreferential chain is sometimes indirectly linked to the anaphor via some other antecedents in the chain. In this case, we find that the most recent antecedent always contains little information to directly determine the coreference relationship with the anaphor. Therefore, for a given anaphor, the corresponding special constraint agent can always safely filter out these less informative antecedent candidates. In this way, rather than finding the most recent antecedent for an anaphor, our system tries to find the most direct and informative antecedent. Evaluation shows that our system achieves Precision / Recall / F-measures of 84.7% / 65.8% / 73.9 and 82.8% / 55.7% / 66.5 on MUC6 and MUC-7 English coreference tasks respectively. This means that our system achieves significantly better precision rates by about 8 percent over the best-reported systems while keeping recall rates. 
Spelling recognition is an approach to enhance a speech recognizer’s ability to cope with incorrectly recognized words and out-of-vocabulary words. This paper presents a general framework for Thai speech recognition enhanced with spelling recognition. In order to implement Thai spelling recognition, Thai alphabets and their spelling methods are analyzed. Based on hidden Markov models, we propose a method to construct a Thai spelling recognition system by using an existing continuous speech corpus. To compensate the difference between spelling utterances and continuous speech utterances, the adjustment of utterance speed is taken into account. Assigning different numbers of states for syllables with different durations is helpful to improve the recognition accuracy. Our system achieves up to 79.38% accuracy. 
In this paper, we introduce a large-scale test collection for multiple document summarization, the Text Summarization Challenge 3 (TSC3) corpus. We detail the corpus construction and evaluation measures. The signiﬁcant feature of the corpus is that it annotates not only the important sentences in a document set, but also those among them that have the same content. Moreover, we deﬁne new evaluation metrics taking redundancy into account and discuss the effectiveness of redundancy minimization. 
Current rule induction techniques based on hard matching (i.e., strict slot-by-slot matching) tend to fare poorly in extracting information from natural language texts, which often exhibit great variations. The reason is that hard matching techniques result in relatively high precision but low recall. To tackle this problem, we take advantage of the newly proposed soft pattern rules which offer high recall through the use of probabilistic matching. We propose a bootstrapping framework in which soft and hard matching pattern rules are combined in a cascading manner to realize a weakly supervised rule induction scheme. The system starts with a small set of hand-tagged instances. At each iteration, we first generate soft pattern rules and utilize them to tag new training instances automatically. We then apply hard pattern rule induction on the overall tagged data to generate more precise rules, which are used to tag the data again. The process can be repeated until satisfactory results are obtained. Our experimental results show that our bootstrapping scheme with two cascaded learners approaches the performance of a fully supervised information extraction system while using much fewer handtagged instances. 
The tedious task of responding to a backlog of email is one which is familiar to many researchers. As a subset of email management, we address the problem of constructing a summary of email discussions. Specifically, we examine ongoing discussions which will ultimately culminate in a consensus in a decision-making process. Our summary provides a snapshot of the current stateof-affairs of the discussion and facilitates a speedy response from the user, who might be the bottleneck in some matter being resolved. We present a method which uses the structure of the thread dialogue and word vector techniques to determine which sentence in the thread should be extracted as the main issue. Our solution successfully identifies the sentence containing the issue of the thread being discussed, potentially more informative than subject line. 
We present a new HMM tagger that exploits context on both sides of a word to be tagged, and evaluate it in both the unsupervised and supervised case. Along the way, we present the first comprehensive comparison of unsupervised methods for part-of-speech tagging, noting that published results to date have not been comparable across corpora or lexicons. Observing that the quality of the lexicon greatly impacts the accuracy that can be achieved by the algorithms, we present a method of HMM training that improves accuracy when training of lexical probabilities is unstable. Finally, we show how this new tagger achieves state-of-the-art results in a supervised, non-training intensive framework. 
Chinese word segmentation is a difﬁcult, important and widely-studied sequence modeling problem. This paper demonstrates the ability of linear-chain conditional random ﬁelds (CRFs) to perform robust and accurate Chinese word segmentation by providing a principled framework that easily supports the integration of domain knowledge in the form of multiple lexicons of characters and words. We also present a probabilistic new word detection method, which further improves performance. Our system is evaluated on four datasets used in a recent comprehensive Chinese word segmentation competition. State-of-the-art performance is obtained. 
Part of speech taggers based on Hidden Markov Models rely on a series of hypotheses which make certain errors inevitable. The idea developed in this paper consists in allowing a limited, controlled ambiguity in the output of the tagger in order to avoid a number of errors. The ambiguity takes the form of ambiguous tags which denote subsets of the tagset. These tags are used when the tagger hesitates between the different components of the ambiguous tags. They are introduced in an existing lexicon and 3-gram database. Their lexical and syntactic counts are computed on the basis of the lexical and syntactic counts of their constituents, using impurity functions. The tagging process itself, based on the Viterbi algorithm, is unchanged. Experiments conducted on the Brown corpus show a recall of 0.982, for an ambiguity rate of 1.233 which is to be compared with a baseline recall of 0.978 for an ambiguity rate of 1.414 using the same ambiguous tags and with a recall of 0.955 corresponding to the one best solution of standard tagging (without ambiguous tags). 
In this paper, the search engine Intuition is described. It allows the user to navigate through the documents retrieved with a given query. Several “browse help” functions are provided by the engine and described here: conceptualisation, named entities, similar documents and entity visualization. They intend to “save the user’s time”. In order to evaluate the amount of time these features can save, an evaluation was made. It involves 6 users, 18 queries and the corpus is made of 16 years of the newspaper Le Monde. The results show that, with the different features, a user get faster to the needed information. fewer non-relevant documents are read (filtering) and more relevant documents are retrieved in less time. 
Terminological variation is an integral part of the linguistic ability to realise a concept in many ways, but it is typically considered an obstacle to automatic term recognition (ATR) and term management. We present a method that integrates term variation in a hybrid ATR approach, in which term candidates are recognised by a set of linguistic filters and termhood assignment is based on joint frequency of occurrence of all term variants. We evaluate the effectiveness of incorporating specific types of term variation by comparing it to the performance of a baseline method that treats term variants as separate terms. We show that ATR precision is enhanced by considering joint termhoods of all term variants, while recall benefits by the introduction of new candidates through consideration of different variation types. On a biomedical test corpus we show that precision can be increased by 20–70% for the top ranked terms, while recall improves generally by 2–25%. 
The identification of authorship falls into the category of style classification, an interesting sub-field of text categorization that deals with properties of the form of linguistic expression as opposed to the content of a text. Various feature sets and classification methods have been proposed in the literature, geared towards abstracting away from the content of a text, and focusing on its stylistic properties. We demonstrate that in a realistically difficult authorship attribution scenario, deep linguistic analysis features such as context free production frequencies and semantic relationship frequencies achieve significant error reduction over more commonly used “shallow” features such as function word frequencies and part of speech trigrams. Modern machine learning techniques like support vector machines allow us to explore large feature vectors, combining these different feature sets to achieve high classification accuracy in style-based tasks. 
New words such as names, technical terms, etc appear frequently. As such, the bilingual lexicon of a machine translation system has to be constantly updated with these new word translations. Comparable corpora such as news documents of the same period from different news agencies are readily available. In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information. We evaluated our approach on six months of Chinese and English Gigaword corpora, with encouraging results. 1. Introduction New words such as person names, organization names, technical terms, etc. appear frequently. In order for a machine translation system to translate these new words correctly, its bilingual lexicon needs to be constantly updated with new word translations. Much research has been done on using parallel corpora to learn bilingual lexicons (Melamed, 1997; Moore, 2003). But parallel corpora are scarce resources, especially for uncommon language pairs. Comparable corpora refer to texts that are not direct translation but are about the same topic. For example, various news agencies report major world events in different languages, and such news documents form a readily available source of comparable corpora. Being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora. Previous research efforts on acquiring translations from comparable corpora  include (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999). When translating a word w, two sources of information can be used to determine its translation: the word w itself and the surrounding words in the neighborhood (i.e., the context) of w. Most previous research only considers one of the two sources of information, but not both. For example, the work of (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation. On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Koehn and Knight, 2002; Rapp, 1995; Rapp, 1999) used the context of w to locate its translation in a second language. In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information. Since both sources of information are complementary, the accuracy of our combined approach is better than the accuracy of using just context or transliteration information alone. We fully implemented our method and tested it on Chinese-English comparable corpora. We translated Chinese words into English. That is, Chinese is the source language and English is the target language. We achieved encouraging results. While we have only tested our method on Chinese-English comparable corpora, our method is general and applicable to other language pairs. 2. Our approach The work of (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999) noted that if an English word e is the translation of a Chinese word c , then the contexts of the two words are similar. We could view this as a document retrieval problem. The context (i.e., the surrounding words) of c is  viewed as a query. The context of each candidate translation e' is viewed as a document. Since the context of the correct translation e is similar to the context of c , we are likely to retrieve the context of e when we use the context of c as the query and try to retrieve the most similar document. We employ the language modeling approach (Ng, 2000; Ponte and Croft, 1998) for this retrieval problem. More details are given in Section 3. On the other hand, when we only look at the word w itself, we can rely on the pronunciation of w to locate its translation. We use a variant of the machine transliteration method proposed by (Knight and Graehl, 1998). More details are given in Section 4. Each of the two individual methods provides a ranked list of candidate words, associating with each candidate a score estimated by the particular method. If a word e in English is indeed the translation of a word c in Chinese, then we would expect e to be ranked very high in both lists in general. Specifically, our combination method is as follows: we examine the top M words in both lists and find e1,e2 ,...,ek that appear in top M positions in both lists. We then rank these words e1,e2 ,...,ek according to the average of their rank positions in the two lists. The candidate ei that is ranked the highest according to the average rank is taken to be the correct translation and is output. If no words appear within the top M positions in both lists, then no translation is output. Since we are using comparable corpora, it is possible that the translation of a new word does not exist in the target corpus. In particular, our experiment was conducted on comparable corpora that are not very closely related and as such, most of the Chinese words have no translations in the English target corpus. 3. Translation by context In a typical information retrieval (IR) problem, a query is given and a ranked list of documents most relevant to the query is returned from a document collection. For our task, the query is C(c) , the context (i.e., the surrounding words) of a Chinese word c . Each C(e) , the context of an English word  e , is considered as a document in IR. If an English word e is the translation of a Chinese word c , they will have similar contexts. So we use the query C(c) to retrieve a document C(e* ) that  best matches the query. The English word e* corresponding to that document C(e* ) is the translation of c . Within IR, there is a new approach to docu- ment retrieval called the language modeling approach (Ponte & Croft, 98). In this approach, a language model is derived from each document D . Then the probability of generating the query Q according to that language model, P(Q | D) , is estimated. The document with the highest P(Q | D) is the one that best matches the query. The language modeling approach to IR has been shown to give superior retrieval performance (Ponte & Croft, 98; Ng, 2000), compared with traditional vector space model, and we adopt this approach in our current work. To estimate P(Q | D) , we use the approach of (Ng, 2000). We view the document D as a mul- tinomial distribution of terms and assume that query Q is generated by this model:  ∏ P (Q | D ) = n!  P (t | D ) ct  ∏ c! t tt  where t is a term in the corpus, ct is the number  of times term t occurs in the query Q ,  ∑ n = t ct is the total number of terms in query Q. For ranking purpose, the first fraction n!/ ∏t ct ! can be omitted as this part depends on the query only and thus is the same for all the documents. In our translation problem, C(c) is viewed as the query and C(e) is viewed as a document. So  our task is to compute P(C(c) | C(e)) for each English word e and find the e that gives the highest P(C(c) | C(e)) , estimated as: ∏ P(tc | Tc (C(e)))q(tc ) tc∈C (c) Term tc is a Chinese word. q(tc ) is the number of occurrences of tc in C(c) . Tc (C(e)) is the  bag of Chinese words obtained by translating the English words in C(e) , as determined by a bi-  lingual dictionary. If an English word is ambiguous and has K translated Chinese words listed in the bilingual dictionary, then each of the K translated Chinese words is counted as occurring 1/K  times in Tc (C(e)) for the purpose of probability estimation. We use backoff and linear interpolation for probability estimation: P(tc | Tc (C(e))) = α ⋅ Pml (tc | Tc (C(e))) + (1−α ) ⋅ Pml (tc )  Pml (tc | Tc (C(e))) =  dTc (C (e)) (tc ) ∑ dTc (C(e)) (t)  t∈Tc (C (e))  where Pml (•) are the maximum likelihood esti- mates, dTc (C(e)) (tc ) is the number of occurrences of the term tc in Tc (C(e)) , and Pml (tc ) is esti- mated similarly by counting the occurrences of tc in the Chinese translation of the whole English corpus. α is set to 0.6 in our experiments.  4. Translation by transliteration  For the transliteration model, we use a modified model of (Knight and Graehl, 1998) and (AlOnaizan and Knight, 2002b). Knight and Graehl (1998) proposed a probabilistic model for machine transliteration. In this model, a word in the target language (i.e., English in our task) is written and pronounced. This pronunciation is converted to source language pronunciation and then to source language word (i.e., Chinese in our task). Al-Onaizan and Knight (2002b) suggested that pronunciation can be skipped and the target language letters can be mapped directly to source language letters. Pinyin is the standard Romanization system of Chinese characters. It is phonetic-based. For transliteration, we estimate P(e | c) as follows:  P(e | c) = P(e | pinyin)  = ∑ P(e,a | pinyin)  a  ∑∏ =  P(lia | pi )  ai  First, each Chinese character in a Chinese word c is converted to pinyin form. Then we sum over all the alignments that this pinyin form of c can map to an English word e. For each possible alignment, we calculate the probability by taking the product of each mapping. pi is the ith syllable of pinyin, lia is the English letter sequence that the ith pinyin syllable maps to in the particular alignment a. Since most Chinese characters have only one pronunciation and hence one pinyin form, we assume that Chinese character-to-pinyin mapping is one-to-one to simplify the problem. We use the expectation maximization (EM) algorithm to generate mapping probabilities from pinyin syllables to English letter sequences. To reduce the search space, we limit the number of English letters that each pinyin syllable can map to as 0, 1, or 2. Also we do not allow cross mappings. That is, if an English letter sequence e1 precedes another English letter sequence e2 in an English word, then the pinyin syllable mapped to e1 must precede the pinyin syllable mapped to e2 . Our method differs from (Knight and Graehl, 1998) and (Al-Onaizan and Knight, 2002b) in that our method does not generate candidates but only estimates P(e | c) for candidates e appear- ing in the English corpus. Another difference is that our method estimates P(e | c) directly, in- stead of P(c | e) and P(e) . 5. Experiment 5.1 Resources For the Chinese corpus, we used the Linguistic Data Consortium (LDC) Chinese Gigaword Corpus from Jan 1995 to Dec 1995. The corpus of the period Jul to Dec 1995 was used to come up with new Chinese words c for translation into English. The corpus of the period Jan to Jun 1995 was just used to determine if a Chinese word c from Jul to Dec 1995 was new, i.e., not occurring from Jan to Jun 1995. Chinese Gigaword corpus consists of news from two agencies: Xinhua News Agency and Central News Agency. As for English corpus, we used the LDC English Gigaword Corpus from Jul to Dec 1995. The  English Gigaword corpus consists of news from four newswire services: Agence France Press English Service, Associated Press Worldstream English Service, New York Times Newswire Service, and Xinhua News Agency English Service. To avoid accidentally using parallel texts, we did not use the texts of Xinhua News Agency English Service. The size of the English corpus from Jul to Dec 1995 was about 730M bytes, and the size of the Chinese corpus from Jul to Dec 1995 was about 120M bytes. We used a Chinese-English dictionary which contained about 10,000 entries for translating the words in the context. For the training of transliteration probability, we required a ChineseEnglish name list. We used a list of 1,580 Chinese-English name pairs as training data for the EM algorithm. 5.2 Preprocessing Unlike English, Chinese text is composed of Chinese characters with no demarcation for words. So we first segmented Chinese text with a Chinese word segmenter that was based on maximum entropy modeling (Ng and Low, 2004). We then divided the Chinese corpus from Jul to Dec 1995 into 12 periods, each containing text from a half-month period. Then we determined the new Chinese words in each half-month period p. By new Chinese words, we refer to those words that appeared in this period p but not from Jan to Jun 1995 or any other periods that preceded p. Among all these new words, we selected those occurring at least 5 times. These words made up our test set. We call these words Chinese source words. They were the words that we were supposed to find translations from the English corpus. For the English corpus, we performed sentence segmentation and converted each word to its morphological root form and to lower case. We also divided the English corpus into 12 periods, each containing text from a half-month period. For each period, we selected those English words occurring at least 10 times and were not present in the 10,000-word Chinese-English dictionary we used and were not stop words. We considered these English words as potential translations of the Chinese source words. We call  them English translation candidate words. For a Chinese source word occurring within a halfmonth period p, we looked for its English translation candidate words occurring in news documents in the same period p. 5.3 Translation candidates The context C(c) of a Chinese word c was col- lected as follows: For each occurrence of c, we set a window of size 50 characters centered at c. We discarded all the Chinese words in the context that were not in the dictionary we used. The contexts of all occurrences of a word c were then concatenated together to form C(c) . The context of an English translation candidate word e, C(e) , was similarly collected. The window size of English context was 100 words. After all the counts were collected, we esti- mated P(C(c) | C(e)) as described in Section 3, for each pair of Chinese source word and English translation candidate word. For each Chinese source word, we ranked all its English translation candidate words according to the esti- mated P(C(c) | C(e)) . For each Chinese source word c and an English translation candidate word e , we also calculated the probability P(e | c) (as described in Section 4), which was used to rank the English candidate words based on transliteration. Finally, the English candidate word with the smallest average rank position and that appears within the top M positions of both ranked lists is the chosen English translation (as described in Section 2). If no words appear within the top M positions in both ranked lists, then no translation is output. Note that for many Chinese words, only one English word e appeared within the top M posi- tions for both lists. And among those cases where more than one English words appeared within the top M positions for both lists, many were multi- ple translations of a Chinese word. This happened for example when a Chinese word was a non-English person name. The name could have multiple translations in English. For example, 米洛西娜 was a Russian name. Mirochina and Miroshina both appeared in top 10 positions of both lists. Both were correct.  5.4 Evaluation We evaluated our method on each of the 12 half- month periods. The results when we set M = 10 are shown in Table 1.  Period #c  #e  #o #Cor Prec.  (%)  
We propose a path-based transfer model for machine translation. The model is trained with a word-aligned parallel corpus where the source language sentences are parsed. The training algorithm extracts a set of transfer rules and their probabilities from the training corpus. A rule translates a path in the source language dependency tree into a fragment in the target dependency tree. The problem of finding the most probable translation becomes a graph-theoretic problem of finding the minimum path covering of the source language dependency tree. 
The decoding problem in Statistical Machine Translation (SMT) is a computationally hard combinatorial optimization problem. In this paper, we propose a new algorithmic framework for solving the decoding problem and demonstrate its utility. In the new algorithmic framework, the decoding problem can be solved both exactly and approximately. The key idea behind the framework is the modeling of the decoding problem as one that involves alternating maximization of two relatively simpler subproblems. We show how the subproblems can be solved eﬃciently and how their solutions can be combined to arrive at a solution for the decoding problem. A family of provably fast decoding algorithms can be derived from the basic techniques underlying the framework and we present a few illustrations. Our ﬁrst algorithm is a provably linear time search algorithm. We use this algorithm as a subroutine in the other algorithms. We believe that decoding algorithms derived from our framework can be of practical signiﬁcance. 
This paper presents a versatile system intended to acquire paraphrastic phrases from a representative corpus. In order to decrease the time spent on the elaboration of resources for NLP system (for example Information Extraction, IE hereafter), we suggest to use a knowledge acquisition module that helps extracting new information despite linguistic variation (textual entailment). This knowledge is automatically derived from the text collection, in interaction with a large semantic network. 
We are developing an automatic method to compile an encyclopedic corpus from the Web. In our previous work, paragraph-style descriptions for a term are extracted from Web pages and organized based on domains. However, these descriptions are independent and do not comprise a condensed text as in hand-crafted encyclopedias. To resolve this problem, we propose a summarization method, which produces a single text from multiple descriptions. The resultant summary concisely describes a term from different viewpoints. We also show the eﬀectiveness of our method by means of experiments. 
The identiﬁcation of the key concepts in a set of documents is a useful source of information for several information access applications. We are interested in its application to multi-document summarization, both for the automatic generation of summaries and for interactive summarization systems. In this paper, we study whether the syntactic position of terms in the texts can be used to predict which terms are good candidates as key concepts. Our experiments show that a) distance to the verb is highly correlated with the probability of a term being part of a key concept; b) subject modiﬁers are the best syntactic locations to ﬁnd relevant terms; and c) in the task of automatically ﬁnding key terms, the combination of statistical term weights with shallow syntactic information gives better results than statistical measures alone. 
Past work of generating referring expressions mainly utilized attributes of objects and binary relations between objects. However, such an approach does not work well when there is no distinctive attribute among objects. To overcome this limitation, this paper proposes a method utilizing the perceptual groups of objects and n-ary relations among them. The key is to identify groups of objects that are naturally recognized by humans. We conducted psychological experiments with 42 subjects to collect referring expressions in such situations, and built a generation algorithm based on the results. The evaluation using another 23 subjects showed that the proposed method could effectively generate proper referring expressions. 
We present several statistical models of syntactic constituent order for sentence realization. We compare several models, including simple joint models inspired by existing statistical parsing models, and several novel conditional models. The conditional models leverage a large set of linguistic features without manual feature selection. We apply and evaluate the models in sentence realization for French and German and find that a particular conditional model outperforms all others. We employ a version of that model in an evaluation on unordered trees from the Penn TreeBank. We offer this result on standard data as a reference-point for evaluations of ordering in sentence realization. 
Language model based IR system proposed in recent 5 years has introduced the language model approach in the speech recognition area into the IR community and improves the performance of the IR system effectively. However, the assumption that all the indexed words are irrelative behind the method is not the truth. Though statistical MT approach alleviates the situation by taking the synonymy factor into account, it never helps to judge the different meanings of the same word in varied context. In this paper we propose the trigger language model based IR system to resolve the problem. Firstly we compute the mutual information of the words from training corpus and then design the algorithm to get the triggered words of the query in order to fix down the topic of query more clearly. We introduce the relative parameters into the document language model to form the trigger language model based IR system. Experiments show that the performance of trigger language model based IR system has been improved greatly. The precision of trigger language model increased 12% and recall increased nearly 10.8% compared with Ponte language model method. 
We report on the development of a cross language information retrieval system, which translates user queries by categorizing these queries into terms listed in a controlled vocabulary. Unlike usual automatic text categorization systems, which rely on dataintensive models induced from large training data, our automatic text categorization tool applies data-independent classiﬁers: a vector-space engine and a pattern matcher are combined to improve ranking of Medical Subject Headings (MeSH). The categorizer also beneﬁts from the availability of large thesauri, where variants of MeSH terms can be found. For evaluation, we use an English collection of MedLine records: OHSUMED. French OHSUMED queries translated from the original English queries by domain experts- are mapped into French MeSH terms; then we use the MeSH controlled vocabulary as interlingua to translate French MeSH terms into English MeSH terms, which are ﬁnally used to query the OHSUMED document collection. The ﬁrst part of the study focuses on the text to MeSH categorization task. We use a set of MedLine abstracts as input documents in order to tune the categorization system. The second part compares the performance of a machine translation-based cross language information retrieval (CLIR) system with the categorization-based system: the former results in a CLIR ratio close to 60%, while the latter achieves a ratio above 80%. A ﬁnal experiment, which combines both approaches, achieves a result above 90%. 
Temporal reference is an issue of determining how events relate to one another. Determining temporal relations relies on the combination of the information, which is explicit or implicit in a language. This paper reports a computational model for determining temporal relations in Chinese. The model takes into account the effects of linguistic features, such as tense/aspect, temporal connectives, and discourse structures, and makes use of the fact that events are represented in different temporal structures. A machine learning approach, Weighted Bayesian Classifier, is developed to map their combined effects to the corresponding relations. An empirical study is conducted to investigate different combination methods, including lexicalbased, grammatical-based, and role-based methods. When used in combination, the weights of the features may not be equal. Incorporating with an optimization algorithm, the weights are fine tuned and the improvement is remarkable. 
We propose a detection method for orthographic variants caused by transliteration in a large corpus. The method employs two similarities. One is string similarity based on edit distance. The other is contextual similarity by a vector space model. Experimental results show that the method performed a 0.889 F-measure in an open test.  
Machine transliteration/back-transliteration plays an important role in many multilingual speech and language applications. In this paper, a novel framework for machine transliteration/backtransliteration that allows us to carry out direct orthographical mapping (DOM) between two different languages is presented. Under this framework, a joint source-channel transliteration model, also called n-gram transliteration model (ngram TM), is further proposed to model the transliteration process. We evaluate the proposed methods through several transliteration/backtransliteration experiments for English/Chinese and English/Japanese language pairs. Our study reveals that the proposed method not only reduces an extensive system development effort but also improves the transliteration accuracy significantly. 
This paper describes the technology and an experiment of subcategorization acquisition for Chinese verbs. The SCF hypotheses are generated by means of linguistic heuristic information and filtered via statistical methods. Evaluation on the acquisition of 20 multi-pattern verbs shows that our experiment achieved the similar precision and recall with former researches. Besides, simple application of the acquired lexicon to a PCFG parser indicates great potentialities of subcategorization information in the fields of NLP. Credits This research is sponsored by National Natural Science Foundation (Grant No. 60373101 and 603750 19), and High-Tech Research and Development Program (Grant No. 2002AA117010-09). Introduction Since (Brent 1991) there have been a considerable amount of researches focusing on verb lexicons with respective subcategorization information specified both in the field of traditional linguistics and that of computational linguistics. As for the former, subcategory theories illustrating the syntactic behaviors of verbal predicates are now much more systemically improved, e.g. (Korhonen 2001). And for auto-acquisition and relevant application, researchers have made great achievements not only in English, e.g. (Briscoe and Carroll 1997), (Korhonen 2003), but also in many other languages, such as Germany (Schulte im Walde 2002), Czech (Sarkar and Zeman 2000), and Portuguese (Gamallo et. al 2002). However, relevant theoretical researches on Chinese verbs are generally limited to case grammar, valency, some semantic computation theories, and a few papers on manual acquisition or prescriptive designment of syntactic patterns. Due to irrelevant initial motivations, syntactic  and semantic generalizabilities of the consequent outputs are not in such a harmony that satisfies the description granularity for SCF (Han and Zhao 2004). The only auto-acquisition work for Chinese SCF made by (Han and Zhao 2004) describes the predefinition of 152 general frames for all verbs in Chinese, but that experiment is not based on real corpus. After observing and analyzing quantity of subcategory phenomena in real Chinese corpus in the People’s Daily (Jan.~June, 1998), we removed from Han & Zhao’s predefinition 15 SCFs that are actually similar derivants of others, and then with this foundation and linguistic rules from (Zhao 2002) as heuristic information we generated SCF hypotheses from the corpus of People’s Daily (Jan.~June, 1998), and statistically filtered the hypotheses into a Chinese verb SCF lexicon. As far as we know, this is the first attempt of Chinese SCF auto-acquisition based on real corpus. In the rest of this paper, the second section describes a comprehensive system that builds verb SCF lexicons from large real corpus, the respective operating principles, and the knowledge coded in our SCF. The third section analyzed the acquired lexicon with two experiments: one evaluated the acquisition results of 20 verbs with multi syntactic patterns against manual gold standard; the other checked the performance of the lexicon when applied in a PCFG parser. The forth section compares and contrasts this research with related works done by others. And at last, Section 5 concludes our present achievements, disadvantages and possible future focuses. 
The reality of analogies between words is refuted by noone (e.g., I walked is to to walk as I laughed is to to laugh, noted I walked : to walk :: I laughed : to laugh). But computational linguists seem to be quite dubious about analogies between sentences: they would not be enough numerous to be of any use. We report experiments conducted on a multilingual corpus to estimate the number of analogies among the sentences that it contains. We give two estimates, a lower one and a higher one. As an analogy must be valid on the level of form as well as on the level of meaning, we relied on the idea that translation should preserve meaning to test for similar meanings. 
This paper investigates a novel application of support vector machines (SVMs) for sentence reduction. We also propose a new probabilistic sentence reduction method based on support vector machine learning. Experimental results show that the proposed methods outperform earlier methods in term of sentence reduction performance. 
It is necessary to ﬁnd a proper arrangement of sentences in order to generate a well-organized summary from multiple documents. In this paper we describe an approach to coherent sentence ordering for summarizing newspaper articles. Since there is no guarantee that chronological ordering of extracted sentences, which is widely used by conventional summarization system, arranges each sentence behind presupposed information of the sentence, we improve chronological ordering by resolving antecedent sentences of arranged sentences. Combining the reﬁnement algorithm with topical segmentation and chronological ordering, we address our experiment to test the eﬀectiveness of the proposed method. The results reveal that the proposed method improves chronological sentence ordering. 
Most traditional information extraction approaches are generative models that assume events exist in text in certain patterns and these patterns can be regenerated in various ways. These assumptions limited the syntactic clues being considered for finding an event and confined these approaches to a particular syntactic level. This paper presents a discriminative framework based on kernel SVMs that takes into account different levels of syntactic information and automatically identifies the appropriate clues. Kernels are used to represent certain levels of syntactic structure and can be combined in principled ways as input for an SVM. We will show that by combining a low level sequence kernel with a high level kernel on a GLARF dependency graph, the new approach outperformed a good rule-based system on slot filler detection for MUC-6. 
We present a novel approach to spoken dialogue summarization. Our system employs a set of semantic similarity metrics using the noun portion of WordNet as a knowledge source. So far, the noun senses have been disambiguated manually. The algorithm aims to extract utterances carrying the essential content of dialogues. We evaluate the system on 20 Switchboard dialogues. The results show that our system outperforms LEAD, RANDOM and TF*IDF baselines. 
Although vast amounts of textual data are freely available, many NLP algorithms exploit only a minute percentage of it. In this paper, we study the challenges of working at the terascale. We present an algorithm, designed for the terascale, for mining is-a relations that achieves similar performance to a state-of-the-art linguistically-rich method. We focus on the accuracy of these two systems as a function of processing time and corpus size. 
In this paper, we present a corpus-based supervised word sense disambiguation (WSD) system for Dutch which combines statistical classiﬁcation (maximum entropy) with linguistic information. Instead of building individual classiﬁers per ambiguous wordform, we introduce a lemma-based approach. The advantage of this novel method is that it clusters all inﬂected forms of an ambiguous word in one classiﬁer, therefore augmenting the training material available to the algorithm. Testing the lemmabased model on the Dutch SENSEVAL-2 test data, we achieve a signiﬁcant increase in accuracy over the wordform model. Also, the WSD system based on lemmas is smaller and more robust. 
The paper presents a new way of accounting for the meaning of verbs in natural languages, using a diagrammatic notation based on the Uniﬁed Modeling Language (UML). We will introduce the new framework by outlining some modeling elements and indicating major differences to the UML. An extended example will be discussed in more detail. We will then focus on the cognitive background of the framework, and in particular address the question why the usage of graphical elements within a linguistic modeling language proves to be very fruitful. Finally, we will brieﬂy indicate the potential of the new framework and its applicability. 
Texts from the medical domain are an important task for natural language processing. This paper investigates the usefulness of a large medical database (the Unified Medical Language System) for the translation of dialogues between doctors and patients using a statistical machine translation system. We are able to show that the extraction of a large dictionary and the usage of semantic type information to generalize the training data significantly improves the translation performance. 
 Abstract* The paper comparatively studies methods of feature weighting in application to the task of cooccurrence-based classification of words according to their meaning. We explore parameter optimization of several weighting methods frequently used for similar problems such as text classification. We find that successful application of all the methods crucially depends on a number of parameters; only a carefully chosen weighting procedure allows to obtain consistent improvement on a classifier learned from non-weighted data. 
We present a text mining method for ﬁnding synonymous expressions based on the distributional hypothesis in a set of coherent corpora. This paper proposes a new methodology to improve the accuracy of a term aggregation system using each author’s text as a coherent corpus. Our approach is based on the idea that one person tends to use one expression for one meaning. According to our assumption, most of the words with similar context features in each author’s corpus tend not to be synonymous expressions. Our proposed method improves the accuracy of our term aggregation system, showing that our approach is successful. 
We deal with the automated acquisition of a Spanish medical subword lexicon from an already existing Portuguese seed lexicon. Using two non-parallel monolingual corpora we determined Spanish lexeme candidates from Portuguese seed lexicon entries by heuristic cognate mapping. We validated the emergent lexical translation hypotheses by determining the similarity of ﬁxed-window context vectors on the basis of Portuguese and Spanish text corpora. 
This paper introduces the “Gendercheck Editor”, a tool to check German texts for gender discriminatory formulations. It relays on shallow rule-based techniques as used in the Controlled Language Authoring Technology (CLAT). The paper outlines major sources of gender imbalances in German texts. It gives a background on the underlying CLAT technology and describes the marking and annotation strategy to automatically detect and visualize the questionable pieces of text. The paper provides a detailed evaluation of the editor. 
This paper proposes a method of automatic back transliteration of proper nouns, in which a Japanese transliterated-word is restored to the original English word. The English words are created from a sequence of letters; thus our method can create new English words that are not registered in dictionaries or English word lists. When a katakana character is converted into English letters, there are various candidates of alphabetic characters. To ensure adequate conversion, the proposed method uses a target English context to calculate the probability of an English character or string corresponding to a Japanese katakana character or string. We confirmed the effectiveness of using the target English context by an experiment of personal-name back transliteration. 
We demonstrate that it is possible to perform automatic sentiment classification in the very noisy domain of customer feedback data. We show that by using large feature vectors in combination with feature reduction, we can train linear support vector machines that achieve high classification accuracy on data that present classification challenges even for a human annotator. We also show that, surprisingly, the addition of deep linguistic analysis features to a set of surface level word n-gram features contributes consistently to classification accuracy in this domain. 
In this paper we describe a way to discover Named Entities by using the distribution of words in news articles. Named Entity recognition is an important task for today’s natural language applications, but it still suffers from data sparseness. We used an observation that a Named Entity is likely to appear synchronously in several news articles, whereas a common noun is less likely. Exploiting this characteristic, we successfully obtained rare Named Entities with 90% accuracy just by comparing time series distributions of a word in two newspapers. Although the achieved recall is not sufﬁcient yet, we believe that this method can be used to strengthen the lexical knowledge of a Named Entity tagger. 
In this paper, we investigate the multiword verbs in the English sublanguage of MEDLINE abstracts. Based on the integration of the domain-speciﬁc named entity knowledge and syntactic as well as statistical information, this work mainly focuses on how to evaluate a proper multiword verb candidate. Our results present a sound balance between the low- and high-frequency multiword verb candidates in the sublanguage corpus. We get a F-measure of 0.753, when tested on a manual sample subset consisting of multiword candidates with both low- and high-frequencies. 
Some Information Extraction (IE) systems are limited to extracting events expressed in a single sentence. It is not clear what effect this has on the diﬃculty of the extraction task. This paper addresses the problem by comparing a corpus which has been annotated using two separate schemes: one which lists all events described in the text and another listing only those expressed within a single sentence. It was found that only 40.6% of the events in the ﬁrst annotation scheme were fully contained in the second. 
In this paper, we discuss the performance of crosslingual information extraction systems employing an automatic pattern acquisition module. This module, which creates extraction patterns starting from a user’s narrative task description, allows rapid customization to new extraction tasks. We compare two approaches: (1) acquiring patterns in the source language, performing source language extraction, and then translating the resulting templates to the target language, and (2) translating the texts and performing pattern discovery and extraction in the target language. We demonstrate an average of 8-10% more recall using the ﬁrst approach. We discuss some of the problems with machine translation and their effect on pattern discovery which lead to this difference in performance. 
While sentence extraction as an approach to summarization has been shown to work in documents of certain genres, because of the conversational nature of email communication where utterances are made in relation to one made previously, sentence extraction may not capture the necessary segments of dialogue that would make a summary coherent. In this paper, we present our work on the detection of questionanswer pairs in an email conversation for the task of email summarization. We show that various features based on the structure of emailthreads can be used to improve upon lexical similarity of discourse segments for questionanswer pairing. 
In this paper, we explore the use of automatic syntactic simpliﬁcation for improving content selection in multi-document summarization. In particular, we show how simplifying parentheticals by removing relative clauses and appositives results in improved sentence clustering, by forcing clustering based on central rather than background information. We argue that the inclusion of parenthetical information in a summary is a reference-generation task rather than a content-selection one, and implement a baseline reference rewriting module. We perform our evaluations on the test sets from the 2003 and 2004 Document Understanding Conference and report that simplifying parentheticals results in signiﬁcant improvement on the automated evaluation metric Rouge. 
Classifier combination is a promising way to improve performance of word sense disambiguation. We propose a new combinational method in this paper. We first construct a series of Naïve Bayesian classifiers along a sequence of orderly varying sized windows of context, and perform sense selection for both training samples and test samples using these classifiers. We thus get a sense selection trajectory along the sequence of context windows for each sample. Then we make use of these trajectories to make final k-nearest-neighbors-based sense selection for test samples. This method aims to lower the uncertainty brought by classifiers using different context windows and make more robust utilization of context while perform well. Experiments show that our approach outperforms some other algorithms on both robustness and performance. 
This article describes the results of a systematic indepth study of the criteria used for word sense disambiguation. Our study is based on 60 target words: 20 nouns, 20 adjectives and 20 verbs. Our results are not always in line with some practices in the field. For example, we show that omitting noncontent words decreases performance and that bigrams yield better results than unigrams. 
This paper proposes a method to improve the robustness of a word sense disambiguation (WSD) system for Japanese. Two WSD classiﬁers are trained from a word sense-tagged corpus: one is a classiﬁer obtained by supervised learning, the other is a classiﬁer using hypernyms extracted from deﬁnition sentences in a dictionary. The former will be suitable for the disambiguation of high frequency words, while the latter is appropriate for low frequency words. A robust WSD system will be constructed by combining these two classiﬁers. In our experiments, the F-measure and applicability of our proposed method were 3.4% and 10% greater, respectively, compared with a single classiﬁer obtained by supervised learning. 
In this paper, we introduce a model for sense assignment which relies on assigning senses to the contexts within which words appear, rather than to the words themselves. We argue that word senses as such are not directly encoded in the lexicon of the language. Rather, each word is associated with one or more stereotypical syntagmatic patterns, which we call selection contexts. Each selection context is associated with a meaning, which can be expressed in any of various formal or computational manifestations. We present a formalism for encoding contexts that help to determine the semantic contribution of a word in an utterance. Further, we develop a methodology through which such stereotypical contexts for words and phrases can be identiﬁed from very large corpora, and subsequently structured in a selection context dictionary, encoding both stereotypical syntactic and semantic information. We present some preliminary results. 
We present a novel automatic approach to constructing a bilingual semantic network—the BiFrameNet, to enhance statistical and transfer-based machine translation systems. BiFrameNet is a frame semantic representation, and contains semantic structure transfers between English and Chinese. The English FrameNet and the Chinese HowNet provide us with two different views of the semantic distribution of lexicon by linguists. We propose to induce the mapping between the English lexical entries in FrameNet to Chinese word senses in HowNet, furnishing a bilingual semantic lexicon which simulates the “concept lexicon” supposedly used by human translators, and which can thus be beneficial to machine translation systems. BiFrameNet also contains bilingual example sentences that have the same semantic roles. We automatically induce Chinese example sentences and their semantic roles, based on semantic structure alignment from the first stage of our work, as well as shallow syntactic structure. In addition to its utility for machine-aided and machine translations, our work is also related to the spatial models proposed by cognitive scientists in the framework of artifactual simulations of the translation process.  Consequently, cognitive scientists and computational linguists alike have been interested in the study of semantic mapping between languages (Ploux and Ji, 2003, Dorr et al., 2002, Ngai et al., 2002, Boas 2002, Palmer and Wu, 1995). We propose to automatically construct a bilingual lexical semantic network with word sense and semantic role mapping between English and Chinese, simulating the “concept lexicon”, suggested by cognitive scientists, of a bilingual person.  1. Introduction  The merits of translation at the word level or the concept level have long been a cause for debate among linguists. Some linguists suggest that the two languages of a bilingual speaker share a common semantic system (Illes and Francis 1999; Ikeda 1998) and hence translation is carried out at the concept level. Meanwhile, there has been a gradual convergence of statistical and transfer approaches in machine translation recently (Wu 2003). Statistical MT systems are based on a stochastic mapping between lexical items, assuming the underlying semantic transfer is hidden. Transfer systems use explicit lexical, syntactic and semantic transfer rules.  Figure 1. BiFrameNet lexicon and example sentence induction The linguists-defined ontologies–-FrameNet (Baker et al., 1998), HowNet (Dong and Dong, 2000), and bilingual dictionaries are the basis for the induction of the mapping. We automatically estimate  the semantic transfer likelihoods between English FrameNet lexical entries and the Chinese word senses in HowNet, and align those frames and lexical pairs with high likelihood values. In addition, we propose to induce Chinese example sentences automatically to match English annotated sentences provided in the FrameNet. The BiFrameNet thus induced provides an additional resource for machine-aided or machine translation systems. It can also serve as a reference to be compared to cognitive studies of the translation process. Ploux and Ji, (2003) proposed a spatial model for matching semantic values between French and English. Palmer and Wu (1995) studied the mapping of change-of-state English verbs to Chinese. Dorr et al. (2002) described a technique for the construction of a Chinese-English verb lexicon based on HowNet and the English LCS Verb Database (LVD). They created links between HowNet concepts and LVD verb classes using both statistics and a manually constructed “seed mapping” of thematic classes between HowNet and LVD. Ngai et al. (2002) employed a word-vector based approach to create the alignment between WordNet and HowNet classes without any manual annotation. Boas (2002) outlined a number of issues surrounding the planning and design of GermanFrameNet (GFN), a bilingual FrameNet dictionary which, when complete, will have a corpus-based German lexicon following the FrameNet structure. This paper is organized as follows: Section 2 describes the algorithm for estimating transfer relations between FrameNet and HowNet structures. Section 3 presents our method for selecting BiFrameNet example sentences for a particular frame and automatically inducing semantic role annotations. We conclude in Section 4, followed by a discussion in Section 5.  µ : lexical entry in F ram eN et;ν : co ncept in H o w N et  F : Fram eN et fram e;  H :H o w N et catego ry  L: lin k s fro m F ram eN et to H o w N et  LF : link s of the fram e F ; Tµ : translation s of µ  R is a ranked list; R[k ] m ean s the top-k elem en t  V F :p o ssib le ν lin k ed to µ ∈ F Λ F : H ow N et categories related to fram e F δ ( x) : binary function, return 1 if input is true;  otherw ise return 0.  For each µ  T 1µ = { tran slatio n s o f µ in H o w N et} T 2 µ = {translations o f µ in diction ary} Tµ = T 1µ U T 2 µ L ← L U { ( µ ,ν ) | ν .W _C ∈ T µ ,ν .G _C = ν .PO S}  For each F  V F = { ν |(u ,ν ) ∈ L , µ ∈ F } For each H  f ( H ) = ∑ δ (ν ∈ V F , v ∈ H ) ν R is the ran ked list of H sorted by f ( H )  Λ F = R [1] U R [ 2 ] ... U R [ N ] Λ F ← Λ F U {R[k ] | Sim ( R[l], R[k ]) > threshold , l = 1, ..N , k = N + 1, ..}  L F = { ( µ ,ν ) | µ ∈ F ,ν ∈ V F I Λ F } Figure 2. BiFrameNet ontology induction  2.1. FrameNet and HowNet  The Berkeley FrameNet database consists of frame-semantic descriptions of more than 7000 English lexical items, together with example sentences annotated with semantic roles (Baker et al., 1998). There is currently no frame semantic representation of Chinese. However, the Chinese HowNet (Dong and Dong 2000) represents a hierarchical view of lexical semantics in Chinese.  FrameNet is a collection of lexical entries grouped by frame semantics. Each lexical entry represents an individual word sense, and is associated with semantic roles and some annotated sentences. Lexical entries with the same semantic roles are grouped into a “frame” and the semantic roles are called “frame elements”. For example:  2. Lexical semantic mapping in BiFrameNet Dorr et al. (2002) uses a manual seed mapping of semantic roles between FrameNet and LVD to induce a bilingual verb lexicon. In this paper, we propose a method of automatically mapping the English FrameNet lexical entries to HowNet concepts, resulting in the BiFrameNet ontology. We also make use of two bilingual English-Chinese lexicons for this induction. In this section 2, we use an example FrameNet lexical entry “beat.v” in the “cause_harm” frame to illustrate the main steps of our algorithm.  Frame: Cause_harm Frame Elements: agent, body_part, cause, event, instrument, iterations, purpose, reason, result, victim….. Lexical Entries: bash.v, batter.v, bayonet.v, beat.v, belt.v, bludgeon.v, boil.v, break.v, bruise.v, buffet.v, burn.v,…. Example annotated sentence of lexical entry “beat.v”: [agent I] lay down on him and beat [victim at him] [means with my fists]. HowNet is a Chinese ontology with a graph structure of word senses called “concepts”, and each concept contains 7 fields including lexical entries in  Chinese, English gloss, POS tags for the word in Chinese and English, and a definition of the concept including its category and semantic relations (Dong and Dong, 2000). For example, one translation for “beat.v” is 打: NO. = 17645 W_C =打 G_C =V E_C =~架，~斗，~仗，~敌人，~死，~伤，~得好 W_E=attack G_E=V E_E= DEF=fight|争斗 Whereas HowNet concepts correspond roughly to FrameNet lexical entries, its semantic relations do not correspond directly to FrameNet semantic roles. 2.2. Initial mapping based on bilingual lexicon (step 1) We use the bilingual lexicon from HowNet and LDC dictionary to first create all possible mappings between FrameNet lexical entries and HowNet concepts whose part-of-speech (POS) tags are the same. Here we assume that syntactic classification for the majority of FrameNet lexical entries (i.e. verbs and adjectives) are semantically motivated and are mostly preserved across different languages. For example “beat” can be translated into {搥, 败, 冲击, 出手, 难倒, 骗取, 赢, 战败…} in HowNet and {打, 打败, 捣, 敲打, 赢…} in the LDC English-Chinese dictionary. “beat.v” is then linked to all HowNet concepts whose Chinese word/phrase is one of the translations and the part of speech is verb “v”. 2.3. Refined mapping based on semantic contexts in both languages (step 2) At this stage, each FrameNet lexical entry has links to multiple HowNet concepts and categories. For example, “beat.v” in “cause_harm” frame is linked to “打” in both the “beat” category and the “associate” category (as in“打电话/make a phone call”). We need to choose the correct HowNet concept (word sense). Many word sense disambiguation algorithms use contextual words in a sentence as disambiguating features.  In this work, we make use of contextual lexical entries from the same semantic frame, as illustrated above. In this example, the “cause_harm” frame contains two lexical entries—“beat.v” and “strike.v”. From the previous step, “beat.v” and “strike.v” is each linked to a number of Chinese candidates. “beat.v” is linked to “打” with membership in two different HowNet categories, namely “打|beat” and “ 交 往 |associate”. To disambiguate between the above these 2 candidate categories, we make use of the other lexical entries in “cause_harm”, in this case “strike.v” which is linked to “捶”, in the “打|beat” HowNet category. Now, “ |beat” receives two votes (from “ ” and from “ ”), and “ |associate” only one (from “ ”). We therefore choose the HowNet category “ |beat” to be aligned to the frame “cause_harm”, and eliminate the sense of “打”in the “ 交 往 |associate” category. Consequently, “beat.v” in “cause_harm” is linked to all HowNet concepts that are translations of “beat” which are verbs, and which also belong to the HowNet category “ |beat” (but not “ |associate”). In our example, HowNet concepts under two HowNet categories—“beat” and “damage” are linked to the “cause_harm” frame in FrameNet. Only the concepts in the top N categories are considered as correctly linked to the lexical entries in the “cause_harm” frame. We heuristically chose N to be three in our algorithm. 2.4. Final mapping adjusted by taxonomy distance (step 3) Using frame context alone in the above step can effectively prune out incorrect links, but it also prunes some correct links whose HowNet categories are not in the top three categories. In this next step, we aim to recover this kind of pruned links by finding other categories with high similarity to the chosen categories. We introduce the category similarity score (Liu and Li, 2002), which is based on the HowNet taxonomy distance: α Sim(category1,category2) = α +d Where d is the path length from category1 to category2 in the taxonomy. α is an adjusting parameter, which controls the curvature of the similarity score. We set α=1.6 in our work following the experiment results in Liu and Li (2002). If the similarity of category p and one of the top three categories is higher than a threshold t, the category p is also considered as a valid category for the frame.  In our example, some valid categories, such as “firing|射击” is not selected in the previous step even though it is related to the “cause_harm” frame. Based  on the HowNet taxonomy, the similarity score between “firing| ” and “beat| ” is 1.0, which is above the threshold set. Hence, “firing| ” is also chosen as a valid category and the concepts in this category are linked to the “beat.v” lexical entry in the “cause_harm” frame. However, using taxonomy distance can cause errors such as 打 in the “weave” category to be aligned to “beat.v” in the “cause_harm” frame. 2.5. BiFrameNet lexicon evaluation We evaluate our work by comparing the results to a manually set golden standard of transfer links for some lexical entries in FrameNet, and use the precision and recall rate as evaluation criteria. Manual evaluation of all lexical entries is a slow process and is currently still on-going. However, to show the lower bound of the system performance, we chose FrameNet lexical entries with the highest number of transfer links to HowNet concepts as the test set. Since each link is a word sense, these lexical entries have most ambiguous translations. Since the number of lexical entries in a FrameNet parent frame (i.e. frame size) is an important factor in the disambiguation step, we analyze our results by distinguishing between “small frames” (a frame with less than 5 lexical entries) and “large frames”. 24% of the frames are “small frames”. Referring to Tables 2 and 3, we can see a weighted average of (0.649*0.24+0.874*0.76) =82% F-measure.  lexical entry beat.v move.v bright.a hold.v fall.v issue.v  Parent frame  #candidate #lexical  HowNet entries in  links  parent  frame  cause_harm  144  51  motion  132  10  light_emission  126  44  containing  145  2  motion_directional 127  5  emanating  124  4  Table1. Lexical entries test set  Precision 36.81% 95.24% 88.89%  Recall  100%  75.47% 90.56%  F-measure 53.81% 84.21% 89.72%  Table 4. Average performance on “beat.v” at  each step of the algorithm  Table 4 shows the system performance in each step of the alignment between the FrameNet “beat.v” to HowNet concepts with the final F-measure at 89.72.  3. Cross-lingual induction of example annotated sentences in BiFrameNet In the second stage of our proposed work, we aim to automatically induce Chinese example sentences that are appropriate for each semantic frame. Together with English example sentences that already exist in the English FrameNet, they form part of the BiFrameNet, and serve to provide concrete examples of bilingual usage of semantic roles. They can be used either as a resource for machine-aided translation or training data for machine translation.  FrameNet is a collection of over 100-million words of samples of written and spoken language from a wide range of sources, including British and American English. All the example sentences are chosen by linguists for their representative-ness of particular semantic roles, grammatical functions, and phrase type. The current FrameNet contains on average 30 annotated example sentences per predicate, which is still inadequate for automatic semantic parsing systems (Fleischman et al., 2003). Each FrameNet example sentence contains a predicate. The semantic roles of the related frame elements are manually labeled. The syntactic phrase type (e.g. NP, PP) and their grammatical function (e.g. external argument, object argument) are also labeled. An example annotated sentence containing the predicate “beat.v”, in the “cause_harm” frame, is shown below:  lexical Precision  Recall  F-measure  entry step3/step1 step3/step1 step3/step1  beat.v 88.9/36.8% 90.6/100% 89.7/53.8%  move.v 100/49.2 % 72.3/100% 83.9/66.0%  bright.a 79.1/54.0% 100/100% 88.3/70.1%  Overall 87.1/46.3% 87.6/100% 87.4/52.3%  Table 2.Performance on large frames  lexical Precision  Recall  F-measure  entry  step3/step1 step3/step1 step3/step1  hold,v 22.4/7.6% 100/100% 36.7/14.1%  fall,v  87.0/49.2% 81.1/100% 83.9/66.0%  issue.v 31.1/12.3% 100/100% 47.5/20.3%  Overall 52.1/25.0% 85.9/100% 64.9/40.0%  Table 3. Performance on small frames  Step 1  Step 2  Step 3  Example sentence type: trans-simple We are fighting a barbarian, and [agent: we] must [predicate: beat] [victim: him]. In order to provide a representative set of Chinese example sentences automatically for a particular frame, our method must fulfill the following criteria: 1) It must find real sentences occurring naturally in Chinese texts; 2) It should find sentences that cover as many different usage and domain as possible; 3) It must find sentences that have the same semantic roles as the English example sentences;  e: English sentence; c: Chinese sentence Ω F: e for frame F ; Ψ F: c for frame F CA : Candidate c for frame F DP : Dynamic Programming alignment (Figure 5) For each F CA = {c|ν ∈ c, µ ∈ F , (u, v) ∈ LF } For each e ∈ ΩF cˆ = argmax {DP(e, c)} c∈CA Ψ F ← Ψ F U {cˆ} Figure 3. BiFrameNet example sentence induction 4) It should require no manual annotation of any kind. There are at least three different (semi-)automatic approaches for mining Chinese example sentences: i) Translate all English example sentences into Chinese by automatic means, and annotate the semantic roles by word alignment; This approach is not appropriate because machine translation can be erroneous and this method does not satisfy criteria (1) and (2). ii) Construct an English semantic parser and a Chinese parser independently, and use them to annotate the sentences in a sentence aligned, parallel corpus; Apart from the high cost of building two semantic parsers, which itself requires semantically annotated Chinese data; it would be necessary to create artificial links between independent human annotations manually. iii) Mine Chinese sentences from a monolingual corpus that are syntactically similar to the English example sentence, and induce semantic roles from the syntactic transfer function between English and Chinese. This is the approach we take. Inspired by previous work on syntax-driven semantic parsing (Gildea and Jurafsky, 2002; Fleischman et al., 2003), and syntax-based machine translation (Wu, 1997; Cuerzan and Yarowsky, 2002), we postulate that syntactically similar sentences with the same predicate also share similar semantic roles. In this paper, we present our first experiments on inducing semantic roles based on shallow syntactic information. We mine Chinese example sentences from naturally occurring monolingual corpus, and rank them by their syntactic similarity to our English  example sentences. A dynamic programming algorithm then annotates the aligned syntactic units with the same semantic roles. The example Chinese sentences are not translations of the English sentences. Therefore, the set of example sentences within a frame is enriched, providing better coverage for MT and CLIR systems. 3.1. Induction from aligned predicate bilingual lexical pair Since frames are disjoint, we propose a method for finding example sentences one frame at a time. In this paper, we focus on finding Chinese example sentences for the largest frame “cause_harm” and the main semantic roles in this frame—“agent”, “predicate” and “victim”1. For each English lexical entry and its target translation candidates in the BiFrameNet, we first extract sentences that contain the translation candidates from a large Chinese monolingual corpus. Figure 4 shows some initial Chinese example sentence candidates under “beat.v”. There are many sentences that do not have the “agent-predicate-victim” structure. Our next step is to find the Chinese sentences that have the “agent”, ”predicate” and ”victim” semantic roles and annotate them automatically. 南方军队还打死打伤数百名政府军官兵 (the southern army killed and maimed hundreds of government soldiers) 土 军 在 进 攻 中 伤 害 了 无 辜 的 平 民 (soldiers harmed innocent civilians during the attack) 农民砍掉自留山上树木７０多棵 (farmers cut down more than 70 trees) 便用针刺破葫芦 (use the needle to prick the squash) *媒体捅出一份调查报告(the media exposed/produced an investigation report) *一些出版社采取“先斩后奏”的做法 (some publishers adopt a “idiom” method) Figure 4. Some Chinese example sentence and glosses 3.2. Inducing semantic roles from cross-lingual POS transfer Among all the Chinese sentences containing the target predicate words, we need to identify those that contain the same semantic roles as those of the English example sentences in FrameNet. Current automatic semantic parsing algorithms (Gildea and Jurafsky 2003, Fleischman et al., 2003) are all based on syntactic parse trees showing a close coupling of semantic and syntactic structures. Without carrying out full syntactic parsing of the Chinese sentences, we postulate that the semantic 
This paper describes a method to acquire hyponyms for given hypernyms from HTML documents on the WWW. We assume that a heading (or explanation) of an itemization (or listing) in an HTML document is likely to contain a hypernym of the items in the itemization, and we try to acquire hyponymy relations based on this assumption. Our method is obtained by extending Shinzato’s method (Shinzato and Torisawa, 2004) where a common hypernym for expressions in itemizations in HTML documents is obtained by using statistical measures. By using Japanese HTML documents, we empirically show that our proposed method can obtain a signiﬁcant number of hyponymy relations which would otherwise be missed by alternative methods. 
This paper presents a statistical model that interprets the evaluation of ranking methods as a random experiment. This model predicts the variability of evaluation results, so that appropriate signiﬁcance tests for the results can be derived. The paper concludes with an empirical validation of the model on a collocation extraction task. 
This paper addresses the mitigation of medical errors due to the confusion of sound-alike and look-alike drug names. Our approach involves application of two new methods— one based on orthographic similarity (“lookalike”) and the other based on phonetic similarity (“sound-alike”). We present a new recall-based evaluation methodology for determining the effectiveness of different similarity measures on drug names. We show that the new orthographic measure (BI-SIM) outperforms other commonly used measures of similarity on a set containing both look-alike and sound-alike pairs, and that the feature-based phonetic approach (ALINE) outperforms orthographic approaches on a test set containing solely sound-alike confusion pairs. However, an approach that combines several different measures achieves the best results on both test sets. 
We are presenting a working system for automated news analysis that ingests an average total of 7600 news articles per day in five languages. For each language, the system detects the major news stories of the day using a group-average unsupervised agglomerative clustering process. It also tracks, for each cluster, related groups of articles published over the previous seven days, using a cosine of weighted terms. The system furthermore tracks related news across languages, in all language pairs involved. The cross-lingual news cluster similarity is based on a linear combination of three types of input: (a) cognates, (b) automatically detected references to geographical place names and (c) the results of a mapping process onto a multilingual classification system. A manual evaluation showed that the system produces good results. 
In order to control the quality of internet-based language corpora, we developed a method to verify automatically that texts are of (near-) native quality. For the LOCNESS and ICLE corpora, the method is rather successful in separating native and non-native learner texts. The Equal Error Rate is about 10%. However, for other domains, such as internet texts, separate classifiers have to be trained on the basis of suitable seed corpora. 
We ran both Brill’s rule-based tagger and TNT, a statistical tagger, with a default German newspaper-language model on a medical text corpus. Supplied with limited lexicon resources, TNT outperforms the Brill tagger with state-of-the-art performance ﬁgures (close to 97% accuracy). We then trained TNT on a large annotated medical text corpus, with a slightly extended tagset that captures certain medical language particularities, and achieved 98% tagging accuracy. Hence, statistical off-the-shelf POS taggers cannot only be immediately reused for medical NLP, but they also – when trained on medical corpora – achieve a higher performance level than for the newspaper genre. 
We introduce a new, linguistically grounded measure of collocativity based on the property of limited modiﬁability and test it on German PP-verb combinations. We show that our measure not only signiﬁcantly outperforms the standard lexical association measures typically employed for collocation extraction, but also yields a valuable by-product for the creation of collocation databases, viz. possible structural and lexical attributes. Our approach is language-, structure-, and domain-independent because it only requires some shallow syntactic analysis (e.g., a POS-tagger and a phrase chunker). 
Email summarisation presents a unique set of requirements that are different from general text summarisation. This work describes the implementation of an email summarisation system for use in a voice-based Virtual Personal Assistant developed for the EU FASiL Project. Evaluation results from the first integrated version of the project are presented. 
We propose a multiple-document summarization system with user interaction. Our system extracts keywords from sets of documents to be summarized and shows the keywords to a user on the screen. Among them, the user selects some keywords reﬂecting his/her needs. Our system controls the produced summary by using these selected keywords. For evaluation of our method, we participated in TSC3 of NTCIR4 workshop by letting our system select 12 best keywords regarding scoring by the system. Our participated system attained the best performance in content evaluation among systems not using sets of questions. Moreover, we evaluated eﬀectiveness of user interaction in our system. With user interaction, our system attained both higher coverage and precision than that without user interaction. 
The evaluative character of a word is called its semantic orientation (SO). A positive SO indicates desirability (e.g. Good, Honest) and a negative SO indicates undesirability (e.g., Bad, Ugly). This paper presents a method, based on Turney (2003), for inferring the SO of a word from its statistical association with strongly-polarized words and morphemes in Chinese. It is noted that morphemes are much less numerous than words, and that also a small number of fundamental morphemes may be used in the modified system to great advantage. The algorithm was tested on 1,249 words (604 positive and 645 negative) in a corpus of 34 million words, and was run with 20 and 40 polarized words respectively, giving a high precision (79.96% to 81.05%), but a low recall (45.56% to 59.57%). The algorithm was then run with 20 polarized morphemes, or single characters, in the same corpus, giving a high precision of 80.23% and a high recall of 85.03%. We concluded that morphemes in Chinese, as in any language, constitute a distinct sub-lexical unit which, though small in number, has greater linguistic significance than words, as seen by the significant enhancement of results with a much smaller corpus than that required by Turney. 1. Introduction The semantic orientation (SO) of a word indicates the direction in which the word deviates from the norm for its semantic group or lexical field (Lehrer, 1974). Words that encode a desirable state (e.g., beautiful) have a positive SO, while words that represent undesirable states (e.g. absurd) have a negative SO (Hatzivassiloglou and Wiebe, 2000). Hatzivassiloglou and Mckeown (1997) used the words ‘and’, ‘or’, and ‘but’ as linguistic cues to extract adjective pairs. Turney (2003) assessed the SO of words using their occurrences near stronglypolarized words like ‘excellent’ and ‘poor’ with accuracy from 61% to 82%, subject to corpus size.  Turney’s algorithm requires a colossal corpus (hundred billion words) indexed by the AltaVista search engine in his experiment. Undoubtedly, internet texts have formed a very large and easilyaccessible corpus. However, Chinese texts in internet are not segmented so it is not costeffective to use them. This paper presents a general strategy for inferring SO for Chinese words from their association with some strongly-polarized morphemes. The modified system of using morphemes was proved to be more effective than strongly-polarized words in a much smaller corpus. Related work and potential applications of SO are discussed in section 2. Section 3 illustrates one of the methods of Turney’s model for inferring SO, namely, Pointwise Mutual Information (PMI), based on the hypothesis that the SO of a word tends to correspond to the SO of its neighbours. The experiment with polarized words is presented in section 4. The test set includes 1,249 words (604 positive and 645 negative). In a corpus of 34 million word tokens, 410k word types, the algorithm is run with 20 and 40 polarized words, giving a precision of 79.96% and 81.05%, and a recall of 45.56% and 59.57%, respectively. The system is further modified by using polarized morphemes in section 5. We first evaluate the distinction of Chinese morphemes to justify why the modification can probably give simpler and better results, and then introduce a more scientific selection of polarized morphemes. A high precision of 80.23% and a greatly increased recall of 85.03% are yielded. In section 6, the algorithm is run with 14, 10 and 6 morphemes, giving a precision of 79.15%, 79.89% and 75.65%, and a recall of 79.50%, 73.26% and 66.29% respectively. It shows that the algorithm can be also effectively run with 6 to 10 polarized morphemes in a smaller corpus. The conclusion and future work are discussed in section 7. 2. Related Work and Applications  Hatzivassiloglou and Mckeown (1997) presented a method for automatically assigning a + or – orientation label to adjectives known to have some SO by the linguistic constraints on the use of adjectives in conjunctions. For example, ‘and’ links adjectives that have the same SO, while ‘but’ links adjectives that have opposite SO. They devised an algorithm based on such constraints to evaluate 1,336 manually-labeled adjectives (657 positive and 679 negative) with 97% accuracy in a corpus of 21 million words. Turney (2003) introduced a method for automatically inferring the direction and intensity of the SO of a word from its statistical association with a set of positive and negative paradigm words, i.e., strongly-polarized words. The algorithm was evaluated on 3,596 words (1,614 positive and 1,982 negative) including adjectives, adverbs, nouns, and verbs. An accuracy of 82.8% was attained in a corpus of hundred billion words. SO can be used to classify reviews (e.g., movie reviews) as positive or negative (Turney, 2002), and applied to subjectivity analysis such as recognizing hostile messages, classifying emails, mining reviews (Wiebe et al., 2001). The first step of those applications is to recognize that the text is subjective and then the second step, naturally, is to determine the SO of the subjective text. Also, it can be used to summarize argumentative articles like editorials of news media. A summarization system would benefit from distinguishing sentences intended to present factual materials from those intended to present opinions, since many summaries are meant to include only facts. 3. SO from Association-PMI Turney (2003) examined SO-PMI (Pointwise Mutual Information) and SO-LSA (Latent Semantic Analysis). SO-PMI will be our focus in the following parts. PMI is defined as: PMI(word1, word2)=log2( p(word1 & word 2) ) p(word1) p(word 2) where p(word1 & word2) is the probability that word1 and word2 co-occur. If the words are statistically independent, the probability that they co-occur is given by the product p(word1) p(word2). The ratio between p(word1 & word2) and p(word1) p(word2) is a measure of the degree of statistical dependence between the words. The SO of a given word is calculated from the strength of its association with a set of positive words, minus the strength of its association with a set of negative words. Thus the SO of a word, word, is calculated by SO-PMI as follows:  SO-PMI(word) =  PMI(word, pword) -  PMI(word, nword)  pword∈Pwords  nword∈Nwords  where Pwords is a set of 7 positive paradigm  words (good, nice, excellent, positive, fortunate,  correct, and superior) and Nwords is a set of 7  negative paradigm words (bad, nasty, poor,  negative, unfortunate, wrong, and inferior). Those  14 words were chosen by intuition and based on  opposing pairs (good/bad, excellent/poor, etc.).  The words are rather insensitive to context, i.e.,  ‘excellent’ is positive in almost all contexts.  A word, word, is classified as having a positive  SO when SO-PMI(word) is positive and a negative  SO when SO-PMI(word) is negative.  Turney (2003) used the Alta Vista Advanced  search engine with a NEAR operator, which  constrains the search to documents that contain the  words within ten words of one another, in either  order. Three corpora were tested. AV-ENG is the  largest corpus covering 350 million web pages  (English only) indexed by Alta Vista. The medium  corpus is a 2% subset of AV-ENG corpus called  AV-CA (Canadian domain only). The smallest  corpus TASA is about 0.5% of AV-CA and  contains various short documents.  One of the lexicons used in Turney’s experiment  is the GI lexicon (Stone et al., 1966), which  consists of 3,596 adjectives, adverbs, nouns, and  verbs, 1,614 positive and 1,982 negative.  Table 1 shows the precision of SO-PMI with the  GI lexicon in the three corpora.  Percent of Size of  Precision  full test set test set AV-ENG AV-CA TASA  100%  3596  82.84% 76.06% 61.26%  75%  2697  90.66% 81.76% 63.92%  50%  1798  95.49% 87.26% 47.33%  25%  899  97.11% 89.88% 68.74%  Approx. no. of words  1x1011  2x109 1x107  Table 1: The precision of SO-PMI with the GI  lexicon  The strength (absolute value) of the SO was used as a measure of confidence that the words will be correctly classified. Test set words were sorted in descending order of the absolute value of their SO and the top ranked words (the highest confidence words) were then classified. For example, the second row (starting with 75%) in table 1 shows the precision when the top 75% were classified and the last 25% (with lowest confidence) were ignored. We will employ this measure of confidence in the following experiments. Turney concluded that SO-PMI requires a large corpus (hundred billion words), but it is simple,  easy to implement, unsupervised, and it is not restricted to adjectives.  4. Experiment with Chinese Words  In the following experiments, we applied Turney’s  method to Chinese. The algorithm was run with 20  and then 40 paradigm words for comparison. The  experiment details include:  NEAR Operator: it was applied to constrain  the search to documents that contain the words  within ten words of one another, in either order.  Corpus: the LIVAC synchronous corpus (Tsou  et al., 2000, http://www.livac.org) was used. It  covers 9-year news reports of Chinese  communities including Hong Kong, Beijing and  Taiwan, and we used a sub-corpus with about 34  million word tokens and 410k word types.  Test Set Words: a combined set of two  dictionaries of polarized words (Guo, 1999, Wang,  2001) was used to evaluate the results. While  LIVAC is an enormous Chinese corpus, its size is  still far from the hundred-billion-word corpus used  by Turney. It is likely that some words in the  combined set are not used in the 9-year corpus. To  avoid a skewed recall, the number of test set words  used in the corpus is given in table 2. In other  words, the recall can be calculated by the total  number of words used in the corpus, but not by  that recorded in the dictionaries. The difference  between two numbers is just 100.  Polarity  Total no. of the Words used in  test set words the 9-year corpus  Positive  629  604  Negative  721  645  Total  1350  1249  Table 2: Number of the test set words  Paradigm words: The paradigm words were chosen using intuition and based on opposing pairs, as Turney (2003) did. The first experiment was conducted with 10 positive and 10 negative paradigm words, as follows, Pwords:ᇨኔ(honest), ᜣࣔ(clever), ‫(ߩך‬sufficient), ࢉሎ (lucky), ‫إ‬ᒔ (right), ᚌߐ (excellent), ᘋฐ (prosperous), ࿳ߜ(kind), ૎ট(brave), ᝐဠ(humble) Nwords: ဠ ೕ (hypocritical), ჟᥡ (foolish), ࿍౒ (deficient), լࢉ(unlucky), ᙑᎄ(wrong), ༞‫(٭‬adverse), ಐᆵ(unsuccessful), ྲྀᑊ(violent), ᚫஇ(cowardly), ႙ ኬ(arrogant) The experiment was then repeated by increasing the number of paradigm words to 40. The paradigm words added are: Pwords: ᄵ ࡉ (mild), ‫( ܓڶ‬favourable), ‫פګ‬ (successful), ‫( ૿إ‬positive), ᗨᄕ (active), ᑗᨠ (optimistic), ߜࢤ (benign), ᠃შ (attentive), ᘻᥬ (promising), კᑥ(incorrupt)  Nwords: ᖿၞ (radical), լ‫( ܓ‬unfavourable), ؈ඓ (failed), ૤૿ (negative), ௣ᄕ (passive), ༟ᨠ (pessimistic), ༞ࢤ(malignant), ง࢙(inattentive), ‫෉ܐ‬ (indifferent), ፍඓ(corrupt)  4.1 Results  Tables 3 and 4 show the precision and recall of  SO-PMI by two sets of paradigm words.  % of test set  100% 75% 50% 25%  Size of test set 1249 937 625 312  Extracted Set  569 427 285 142  Precision  79.96% 86.17% 86.99% 90.16%  Recall  45.56%  Table 3: Precision and Recall of the SO-PMI of the  20 paradigm word test set  % of test set  100% 75% 50% 25%  Size of test set 1249 937 625 312  Extracted Set  744 558 372 186  Precision  81.05% 86.02% 88.71% 94.09%  Recall  59.57%  Table 4: Precision and Recall of the SO-PMI of the  40 paradigm word test set  The results of both sets gave a satisfactory precision of 80% even in 100% confidence. However, the recall was just 45.56% under the 20word condition, and rose to 59.57% under the 40word condition. The 15% rise was noted. To further improve the recall performance, we experimented with a modified algorithm based on the distinct features of Chinese morphemes.  5. Experiment with Chinese Morphemes Taking morphemes to be smallest linguistic meaningful unit, Chinese morphemes are mostly monosyllabic and single characters, although there are some exceptional poly-syllabic morphemes like ⫁⪕ (grape), ຂ໴ (coffee), which are mostly loanwords. In the following discussion, we consider morphemes to be monosyllabic and represented by single characters. It is observed that many poly-syllabic words with the same SO incorporate a common set of morphemes. The fact suggests the possibility of using paradigm morphemes instead of words. Unlike English, the constituent morphemes of a Chinese word are often free-standing monosyllabic words. It is note-worthy that words in ancient Chinese were much more mono-morphemic than modern Chinese. The evolution from monosyllabic word to disyllabic word may have its origin in the phonological simplification which has given rise to homophony, and which has affected the efficacy of communication. To compensate for this, many more related disyllabic words have appeared in modern Chinese (Tsou, 1976). There are three  basic constructions for deriving disyllabic words in Chinese, including: (1) combination of synonyms or near synonyms (ᄵᥦ, warm, genial, ᄵ=warm, mild, ᥦ =warm, genial) (2) combination of semantically related morphemes (੐ᖱ, ੐=affair, ᖱ=circumstances) (3) The affixation of minor suffixes which serve no primary grammatical function (᧛ሶ, ᧛ =village, ሶ=zi, suffix) The three processes for deriving disyllabic morphemes in Chinese outlined here should be viewed as historical processes. The extent to which such processes may be realized by native speakers to be productive synchronically bears further exploration. Of the three processes, the first two, i.e., synonym and near-synonym compounding, are used frequently by speakers for purposes of disambiguation. In view of this development, the evolution from monosyllabic words in ancient Chinese to disyllabic words in modern Chinese does not change the inherent meaning of the morphemes (words in ancient Chinese) in many cases. The SO of a word often conforms to that of its morphemes. In English, there are affixal morphemes like dis-, un- (negation prefix), or –less (suffix meaning short-age), -ful (suffix meaning ‘to have a property of’), we can say ‘careful’ or ‘careless’ to expand the meaning of ‘care’. However, it is impossible to construct a word like ‘*ful-care’, ‘*less-care’. However, in Chinese, the position of a morpheme in many disyllabic words is far more flexible in the formation of synonym and near-synonym compound words. For instance, ‘᭢’(honor) is a part of two similar word ’᭢⠦’ (honor-bright) and ‘ᱶ᭢’(outstanding-honor). Morphemes in Chinese are like a ‘zipped file’ of the same file types. When it unzips, all the words released have the same SO. 5.1 Probability of Constituent Morphemes of Words with the Same SO Most morphemes can contribute to positive or negative words, regardless of their inherent meaning. For example, ‘ᐘ’ (luck) has inherently a positive meaning, but it can construct both positive word ‘ᐘㆇ ’ (lucky) or a negative word ‘ਇᐘ ’ (unlucky). Thus it is not easy to define the paradigm set simply by intuition. But we can assign a probability value for a morpheme in forming polarized words on the basis of corpus data. The first step is to come up with possible paradigm morphemes by intuition in a large set of polarized words. With the LIVAC synchronous  corpus, the types and tokens of the words constructed by the selected morphemes can easily be extracted. The word types, excluding proper nouns, are then manually-labeled as negative, neutral or positive. Then to obtain the probability that a polar morpheme generates words with the same SO, the tokens of the polarized word types carrying the morpheme are divided by the tokens of all word types carrying the morpheme. For example, given a negative morpheme, m1, the probability that it appears in negative words in token, P(m1, -ve) is given by:  Tokens of NegativeWo rdtypes Carrying m1 Tokens of All Wordtypes Carrying m1  Positive morphemes can be done likewise. Ten  negative morphemes and ten positive morphemes  were chosen as in table 5. Their values of  P(morpheme, orientation) are all above 0.95.  +ve Morpheme -ve Morpheme  
This work investigates the variation in a word’s distributionally nearest neighbours with respect to the similarity measure used. We identify one type of variation as being the relative frequency of the neighbour words with respect to the frequency of the target word. We then demonstrate a three-way connection between relative frequency of similar words, a concept of distributional gnerality and the semantic relation of hyponymy. Finally, we consider the impact that this has on one application of distributional similarity methods (judging the compositionality of collocations). 
We present a framework for the fast computation of lexical afﬁnity models. The framework is composed of a novel algorithm to efﬁciently compute the co-occurrence distribution between pairs of terms, an independence model, and a parametric afﬁnity model. In comparison with previous models, which either use arbitrary windows to compute similarity between words or use lexical afﬁnity to create sequential models, in this paper we focus on models intended to capture the co-occurrence patterns of any pair of words or phrases at any distance in the corpus. The framework is ﬂexible, allowing fast adaptation to applications and it is scalable. We apply it in combination with a terabyte corpus to answer natural language tests, achieving encouraging results. 
The Papillon project is a collaborative project to establish a multilingual dictionary on the Web. This project started 4 years ago with French and Japanese. The partners are now also working on English, Chinese, Lao, Malay, Thai and Vietnamese. It aims to apply the LINUX cooperative construction paradigm to establish a broadcoverage multilingual dictionary. Users can contribute directly on the server by adding new data or correcting existing errors. Their contributions are stored in the user space until checked by a specialist before being fully integrated into the database. The resulting data is then publicly available and freely distributable. An essential condition for the success of the project is to ﬁnd a handy solution for all the participants to be able to contribute online by editing dictionary entries.In this paper, we describe our solution for an online generic editor of dictionary entries based on the description of their structure. 
In the framework of bilingual lexicon acquisition from cross-lingually relevant news articles on the Web, it is relatively harder to reliably estimate bilingual term correspondences for low frequency terms. Considering such a situation, this paper proposes to complementarily use much larger monolingual Web documents collected by search engines, as a resource for reliably re-estimating bilingual term correspondences. We experimentally show that, using a sufﬁcient number of monolingual Web documents, it is quite possible to have reliable estimate of bilingual term correspondences for those low frequency terms. 
Ontology evaluation is a critical task, even more so when the ontology is the output of an automatic system, rather than the result of a conceptualisation effort produced by a team of domain specialists and knowledge engineers. This paper provides an evaluation of the OntoLearn ontology learning system. The proposed evaluation strategy is twofold: first, we provide a detailed quantitative analysis of the ontology learning algorithms, in order to compute the accuracy of OntoLearn under different learning circumstances. Second, we automatically generate natural language descriptions of formal concept specifications, in order to facilitate per-concept qualitative analysis by domain specialists. 
We propose a completely unsupervised method for mining parallel sentences from quasi-comparable bilingual texts which have very different sizes, and which include both in-topic and off-topic documents. We discuss and analyze different bilingual corpora with various levels of comparability. We propose that while better document matching leads to better parallel sentence extraction, better sentence matching also leads to better document matching. Based on this, we use multi-level bootstrapping to improve the alignments between documents, sentences, and bilingual word pairs, iteratively. Our method is the first method that does not rely on any supervised training data, such as a sentence-aligned corpus, or temporal information, such as the publishing date of a news article. It is validated by experimental results that show a 23% improvement over a method without multilevel bootstrapping.  similar in sentence length, sentence order and bilexical context. In our work, we try to find parallel sentences from a quasi-comparable corpus, and we find that many of assumptions in previous work are no longer applicable in this case. Alternatively, we propose an effective, multi-level bootstrapping approach to accomplish this task (Figure 1).  
Automatic word segmentation is a basic requirement for unsupervised learning in morphological analysis. In this paper, we formulate a novel recursive method for minimum description length (MDL) word segmentation, whose basic operation is resegmenting the corpus on a preﬁx (equivalently, a sufﬁx). We derive a local expression for the change in description length under resegmentation, i.e., one which depends only on properties of the speciﬁc preﬁx (not on the rest of the corpus). Such a formulation permits use of a new and efﬁcient algorithm for greedy morphological segmentation of the corpus in a recursive manner. In particular, our method does not restrict words to be segmented only once, into a stem+afﬁx form, as do many extant techniques. Early results for English and Turkish corpora are promising. 
Imbalanced training sets, where one class is heavily underrepresented compared to the others, have a bad effect on the classification of rare class instances. We apply One-sided Sampling for the first time to a lexical acquisition task (learning verb complements from Modern Greek corpora) to remove redundant and misleading training examples of verb nondependents and thereby balance our training set. We experiment with well-known learning algorithms to classify new examples. Performance improves up to 22% in recall and 15% in precision after balancing the dataset1. 
Data-Oriented Translation (DOT), based on DataOriented Parsing (DOP), is a language-independent MT engine which exploits parsed, aligned bitexts to produce very high quality translations. However, data acquisition constitutes a serious bottleneck as DOT requires parsed sentences aligned at both sentential and sub-structural levels. Manual substructural alignment is time-consuming, error-prone and requires considerable knowledge of both source and target languages and how they are related. Automating this process is essential in order to carry out the large-scale translation experiments necessary to assess the full potential of DOT. We present a novel algorithm which automatically induces sub-structural alignments between context-free phrase structure trees in a fast and consistent fashion requiring little or no knowledge of the language pair. We present results from a number of experiments which indicate that our method provides a serious alternative to manual alignment. 
This paper presents work on the task of constructing an example base1 from a given bilingual corpus based on the annotation schema of Translation Corresponding Tree (TCT). Each TCT describes a translation example (a pair of bilingual sentences). It represents the syntactic structure of source language sentence, and more importantly is the facility to specify the correspondences between string (both the source and target sentences) and the representation tree. Furthermore, syntax transformation clues are also encapsulated at each node in the TCT representation to capture the differentiation of grammatical structure between the source and target languages. With this annotation schema, translation examples are effectively represented and organized in the bilingual knowledge database that we need for the Portuguese to Chinese machine translation system. 
In this paper we present KNOWA, an English/Italian word aligner, developed at ITC-irst, which relies mostly on information contained in bilingual dictionaries. The performances of KNOWA are compared with those of GIZA++, a state of the art statistics-based alignment algorithm. The two algorithms are evaluated on the EuroCor and MultiSemCor tasks, that is on two English/Italian publicly available parallel corpora. The results of the evaluation show that, given the nature and the size of the available English-Italian parallel corpora, a language-resource-based word aligner such as KNOWA can outperform a fully statistics-based algorithm such as GIZA++. 
This paper describes a Verb Phrase Ellipsis (VPE) detection system, built for robustness, accuracy and domain independence. The system is corpus-based, and uses a variety of machine learning techniques on free text that has been automatically parsed using two diﬀerent parsers. Tested on a mixed corpus comprising a range of genres, the system achieves a 72% F1-score. It is designed as the ﬁrst stage of a complete VPE resolution system that is input free text, detects VPEs, and proceeds to ﬁnd the antecedents and resolve them. 
Adequate conﬁrmation for keywords is indispensable in spoken dialogue systems to eliminate misunderstandings caused by speech recognition errors. Spoken language also inherently includes out-ofdomain phrases and redundant expressions such as disﬂuency, which do not contribute to task achievement. It is necessary to appropriately make conﬁrmation for important portions. However, a set of keywords necessary to achieve the tasks cannot be predeﬁned in retrieval for a largescale knowledge base unlike conventional database query tasks. In this paper, we describe two statistical measures for identifying portions to be conﬁrmed. A relevance score represents the matching degree with the target knowledge base. A signiﬁcance score detects portions that consequently aﬀect the retrieval results. These measures are deﬁned based on information that is automatically derived from the target knowledge base. An experimental evaluation shows that our method improved the success rate of retrieval by generating conﬁrmation more eﬃciently than using a conventional conﬁdence measure. 
This paper describes a project to detect dependencies between Japanese phrasal units called bunsetsus, and sentence boundaries in a spontaneous speech corpus. In monologues, the biggest problem with dependency structure analysis is that sentence boundaries are ambiguous. In this paper, we propose two methods for improving the accuracy of sentence boundary detection in spontaneous Japanese speech: One is based on statistical machine translation using dependency information and the other is based on text chunking using SVM. An F-measure of 84.9 was achieved for the accuracy of sentence boundary detection by using the proposed methods. The accuracy of dependency structure analysis was also improved from 75.2% to 77.2% by using automatically detected sentence boundaries. The accuracy of dependency structure analysis and that of sentence boundary detection were also improved by interactively using both automatically detected dependency structures and sentence boundaries. 
In this paper, we present a clustering experiment directed at the acquisition of semantic classes for adjectives in Catalan, using only shallow distributional features. We deﬁne a broad-coverage classiﬁcation for adjectives based on Ontological Semantics. We classify along two parameters (number of arguments and ontological kind of denotation), achieving reliable agreement results among human judges. The clustering procedure achieves a comparable agreement score for one of the parameters, and a little lower for the other. 
This paper presents a new open text word sense disambiguation method that combines the use of logical inferences with PageRank-style algorithms applied on graphs extracted from natural language documents. We evaluate the accuracy of the proposed algorithm on several sense-annotated texts, and show that it consistently outperforms the accuracy of other previously proposed knowledge-based word sense disambiguation methods. We also explore and evaluate methods that combine several open-text word sense disambiguation algorithms. 
 In this paper we propose a novel approach for  ontology alignment and domain ontology extraction  from the existing knowledge bases, WordNet and  HowNet. These two knowledge bases are aligned to  construct a bilingual ontology based on the co-  occurrence of the words in the sentence pairs of a  parallel corpus. The bilingual ontology has the merit  that it contains more structural and semantic  information coverage from these two  complementary knowledge bases. For domain-  specific applications, the domain specific ontology  is further extracted from the bilingual ontology by  the island-driven algorithm and the domain-specific  corpus.  Finally, the domain-dependent  terminologies and some axioms between domain  terminologies are integrated into the ontology. For  ontology evaluation, experiments were conducted  by comparing the benchmark constructed by the  ontology engineers or experts. The experimental  results show that the proposed approach can extract  an aligned bilingual domain-specific ontology.  
The method of organization of word meanings is a crucial issue with lexical databases. Our purpose in this research is to extract word hierarchies from corpora automatically. Our initial task to this end is to determine adjective hyperonyms. In order to find adjective hyperonyms, we utilize abstract nouns. We constructed linguistic data by extracting semantic relations between abstract nouns and adjectives from corpus data and classifying abstract nouns based on adjective similarity using a self-organizing semantic map, which is a neural network model (Kohonen 1995). In this paper we describe how to hierarchically organize abstract nouns (adjective hyperonyms) in a semantic map mainly using CSM. We compare three hierarchical organizations of abstract nouns, according to CSM, frequency (Tf.CSM) and an alternative similarity measure based on coefficient overlap, to estimate hyperonym relations between words. 1. Introduction A lexical database is necessary for computers, and even humans, to fully understand a word's meaning because the lexicon is the origin of language understanding and generation. Progress is being made in lexical database research, notably with hierarchical semantic lexical databases such as WordNet, which is used for NLP research worldwide. When compiling lexical databases, it is important to consider what rules or phenomena should be described as lexical meanings and how these lexical meanings should be formalized and stored electronically. This is a common topic of discussion in computational linguistics, especially in the domain of computational lexical semantics.  The method of organization of word meanings is also a crucial issue with lexical databases. In current lexical databases and/or thesauri, abstract nouns indicating concepts are identified manually and words are classified in a top-down manner based on human intuition. This is a good way to make a lexical database for users with a specific purpose. However, word hierarchies based on human intuition tend to vary greatly depending on the lexicographer, and there is often disagreement as to the make-up of the hierarchy. If we could find an objective method to organize word meanings based on real data, we would avoid this variability. Our purpose in this research is to extract word hierarchies from corpora automatically. Our initial task to this end is to determine adjective hyperonyms. In order to find adjective hyperonyms, we utilize abstract nouns. Past linguistic research has focused on classifying the semantic relationship between abstract nouns and adjectives (Nemoto 1969, Takahashi 1975). We constructed linguistic data by extracting semantic relations between abstract nouns and adjectives from corpus data and classifying abstract nouns based on adjective similarity using a self-organizing semantic map (SOM), which is a neural network model (Kohonen 1995). The relative proximity of words in the semantic map indicates their relative similarity. In previous research, word meanings have been statistically modeled based on syntactic information derived from a corpus. Hindle (1990) used noun-verb syntactic relations, and Hatzivassiloglou and McKeown (1993) used coordinated adjective-adjective modifier pairs. These methods are useful for the organization of words deep within a hierarchy, but do not seem to provide a solution for the top levels of the hierarchy. To find an objective hierarchical word structure, we utilize the complementary similarity  measure (CSM), which estimates a one-to-many relation, such as superordinate–subordinate relations (Hagita and Sawaki 1995, Yamamoto and Umemura 2002). In this paper we propose an automated method for constructing adjective hierarchies by connecting strongly related abstract nouns in a top-down fashionç within a semantic map, mainly using CSM. We compare three hierarchical organizations of abstract nouns, according to CSM, frequency (Tf.CSM) and an alternative similarity measure based on coefficient overlap, to estimate hyperonym relations between words.  manner of (a) above from 100 novels, 100 essays and 42 year's worth of newspaper articles, including 11 year's worth of Mainichi Shinbun articles, 10 year's worth of Nihon Keizai Shinbun (Japanese economic newspaper) articles, 7 year's worth of Sangyoukinyuuryuutsu Shinbun (an economic newspaper) articles, and 14 year's worth of Yomiuri Shinbun articles. The total number of abstract noun types is 365, the number of adjective types is 10,525, and the total number of adjective tokens is 35,173. The maximum number of co-occurring adjectives for a given abstract noun is 1,594.  2. Linguistic clues to extract adjective hyperonyms from corpora In order to automatically extract adjective hyperonyms we use syntactic and semantic relations between words. There is a good deal of linguistic research focused on the syntactic and semantic functions of abstract nouns, including Nemoto (1969), Takahashi (1975), and Schmid (2000). Takahashi (1975) illustrated the sentential function of abstract nouns with the following examples. a. Yagi wa seishitsu ga otonashii. (goat) topic (nature) subject (gentle) The nature of goats is gentle b. Zou wa hana ga nagai. (elephant) topic (a nose) subject (long) The nose of an elephant is long He examined the differences in semantic function between “seishitsu (nature)” in (a) and “hana (nose)” in (b), and explained that “seishitsu (nature)” in (a) indicates an aspect of something, i.e., the goat, and “hana (nose)” in (b) indicates part of something, i.e., the elephant. He recognized abstract nouns in (a) as a hyperonym of the attribute that the predicative adjectives express. Nemoto (1969) identified expressions such as “iro ga akai (the color is red)” and “hayasa ga hayai (the speed is fast)” as a kind of meaning repetition, or tautology. In this paper we define such abstract nouns that co-occur with adjectives as adjective hyperonyms. We semi-automatically extracted from corpora 365 abstract nouns used as this kind of head noun, according to the procedures described in Kanzaki et al. (2000). We collected abstract nouns from two year's worth of articles from the Mainichi Shinbun newspaper, and extracted adjectives co-occurring with abstract nouns in the  3. On the Self-Organizing Semantic Map 3.1 Input data Abstract nouns are located in the semantic map based on the similarity of co-occurring adjectives after iteratively learning over input data. In this research, we focus on abstract nouns co-occurring with adjectives. In the semantic map, there are 365 abstract nouns co-occurring with adjectives. The similarities between the 365 abstract nouns are determined according to the number of common co-occurring adjectives. We made a list such as the following. OMOI (feeling): ureshii (glad), kanashii (sad), shiawasena (happy), … KIMOCHI (though): ureshii (glad), tanoshii (pleased), hokorashii (proud), … KANTEN (viewpoint): igakutekina (medical), rekishitekina (historical), ... When two (or more) sets of adjectives with completely different characteristics co-occur with an abstract noun and the meanings of the abstract noun can be distinguished correspondingly, we treat them as two different abstract nouns. For example, the Japanese abstract noun “men” is treated as two different abstract nouns with “men1” meaning “one side (of the characteristics of someone or something)” and “men2” meaning “surface”. The former co-occurs with “gentle”, “kind” and so on. The latter co-occurs with “rough”, “smooth” and so on. 3.2 The Self-Organizing Semantic Map Ma (2000) classified co-occurring words using a self-organizing semantic map (SOM).  Figure 1. The Cosine-based SOM of word similarity Figure 2. The CSM-based SOM of word similarity  We made a semantic map of the abovementioned 365 abstract nouns using SOM, based on the cosine measure. The distribution of the words in the map gives us a sense of the semantic distribution of the words. However, we could not precisely identify the relations between words in the map (Fig 1). In Fig. 1 lines on the maps indicate close relations between word pairs. In the cosine-based semantic map, there is no clear correspondence between word similarities and the distribution of abstract nouns in the map. To solve this problem we introduced the complementary similarity measure (CSM). This similarity measure estimates one-to-many relations, such as superordinate–subordinate relations (Hagita and Sawaki 1995, Yamamoto and Umemura 2002). We can find the hierarchical distribution of words in the semantic map according to the value of CSM (Fig 2). In the CSM-based SOM, lines are concentrated at the bottom right hand corner, that is, most abstract nouns are located at the bottom right-hand corner. Next, we find hierarchical relations between whole abstract nouns, not between word pairs, on the map automatically. 
This paper presents an approach to normalize documents in constrained domains. This approach reuses resources developed for controlled document authoring and is decomposed into three phases. First, candidate content representations for an input document are automatically built. Then, the content representation that best corresponds to the document according to an expert of the class of documents is identiﬁed. This content representation is ﬁnally used to generate the normalized version of the document. The current version of our prototype system is presented, and its limitations are discussed. 
Statistical language models using n-gram approach have been under the criticism of neglecting large-span syntactic-semantic information that inﬂuences the choice of the next word in a language. One of the approaches that helped recently is the use of latent semantic analysis to capture the semantic fabric of the document and enhance the n-gram model. Similarly there have been some approaches that used syntactic analysis to enhance the n-gram models. In this paper, we explain a framework called syntactically enhanced latent semantic analysis and its application in statistical language modeling. This approach augments each word with its syntactic descriptor in terms of the part-of-speech tag, phrase type or the supertag. We observe that given this syntactic knowledge, the model outperforms LSA based models signiﬁcantly in terms of perplexity measure. We also present some observations on the eﬀect of the knowledge of content or function word type in language modeling. This paper also poses the problem of better syntax prediction to achieve the benchmarks. 
Based upon a statistically trained speech translation system, in this study, we try to combine distinctive features derived from the two modules: speech recognition and statistical machine translation, in a loglinear model. The translation hypotheses are then rescored and translation performance is improved. The standard translation evaluation metrics, including BLEU, NIST, multiple reference word error rate and its position independent counterpart, were optimized to solve the weights of the features in the log-linear model. The experimental results have shown signiﬁcant improvement over the baseline IBM model 4 in all automatic translation evaluation metrics. The largest was for BLEU, by 7.9% absolute. 
In the development of machine learning systems for identiﬁcation of reference chains, hand-annotated corpora play a crucial role. This paper concerns the question of how predicative NPs should be annotated w.r.t. coreference in corpora for such systems. This question highlights the tension that sometimes appears in the development of corpora between linguistic considerations and the aim for perfection on the one hand and practical applications and the aim for eﬃciency on the other. Many current projects that seek to identify coreferential links automatically, assume an annotation strategy which instructs the annotator to mark a predicative NP as coreferential with its subject if it is part of a positive sentence. This paper argues that such a representation is not linguistically plausible, and that it will fail to generate an optimal result. 
In this project note, we present the main features of lexicalisation strategies deployed by humans in questionanswering (QA) tasks. We then show how these can be reproduced in automated QA systems, in particular in Intelligent Cooperative Question-Answering Systems. 
Even ambitious algorithms for the generation of referring expressions that identify sets of objects are restricted in terms of efficiency or in their expressive repertoire. In this paper, we report on a system that applies a best-first searching procedure, enhancing both its effectiveness and the variety of expressions it can generate. 
Emdros is a text database engine for linguistic analysis or annotation of text. It is appliccable especially in corpus linguistics for storing and retrieving linguistic analyses of text, at any linguistic level. Emdros implements the EMdF text database model and the MQL query language. In this paper, I present both, and give an example of how Emdros can be useful in computational linguistics. 
This paper presents a disambiguation method in which word senses are determined using a dictionary. We use a semantic proximity measure between words in the dictionary, taking into account the whole topology of the dictionary, seen as a graph on its entries. We have tested the method on the problem of disambiguation of the dictionary entries themselves, with promising results considering we do not use any prior annotated data. 
This paper proposes a method to automatically construct Japanese nominal case frames. The point of our method is the integrated use of a dictionary and example phrases from large corpora. To examine the practical usefulness of the constructed nominal case frames, we also built a system of indirect anaphora resolution based on the case frames. The constructed case frames were evaluated by hand, and were conﬁrmed to be good quality. Experimental results of indirect anaphora resolution also indicated the eﬀectiveness of our approach. 
Word segmentation is an important part of many applications, including information retrieval, information filtering, document analysis, and text summarization. In Thai language, the process is complicated since words are written continuously, and their structures are not well-defined. A recognized effective approach to word segmentation is Longest Matching, a method based on dictionary. Nevertheless, this method suffers from character-level and syllable-level ambiguities in determining word boundaries. This paper proposes a technique to Thai word segmentation using a two-step approach. First, text is segmented, using an application of Prediction by Partial Matching, into syllables whose structures are more well-defined. This reduces the earlier type of ambiguity. Then, the syllables are combined into words by an application of a syllable-level longest matching method together with a logistic regression model which takes into account contextual information. The experimental results show the syllable segmentation accuracy of more than 96.65% and the overall word segmentation accuracy of 97%. 
This paper presents a method to construct Japanese KATAKANA variant list from large corpus. Our method is useful for information retrieval, information extraction, question answering, and so on, because KATAKANA words tend to be used as “loan words” and the transliteration causes several variations of spelling. Our method consists of three steps. At step 1, our system collects KATAKANA words from large corpus. At step 2, our system collects candidate pairs of KATAKANA variants from the collected KATAKANA words using a spelling similarity which is based on the edit distance. At step 3, our system selects variant pairs from the candidate pairs using a semantic similarity which is calculated by a vector space model of a context of each KATAKANA word. We conducted experiments using 38 years of Japanese newspaper articles and constructed Japanese KATAKANA variant list with the performance of 97.4% recall and 89.1% precision. Estimating from this precision, our system can extract 178,569 variant pairs from the corpus. 
In this paper we show that an unsupervised method for ranking word senses automatically can be used to identify infrequently occurring senses. We demonstrate this using a ranking of noun senses derived from the BNC and evaluating on the sense-tagged text available in both SemCor and the SENSEVAL-2 English all-words task. We show that the method does well at identifying senses that do not occur in a corpus, and that those that are erroneously ﬁltered but do occur typically have a lower frequency than the other senses. This method should be useful for word sense disambiguation systems, allowing effort to be concentrated on more frequent senses; it may also be useful for other tasks such as lexical acquisition. Whilst the results on balanced corpora are promising, our chief motivation for the method is for application to domain speciﬁc text. For text within a particular domain many senses from a generic inventory will be rare, and possibly redundant. Since a large domain speciﬁc corpus of sense annotated data is not available, we evaluate our method on domain-speciﬁc corpora and demonstrate that sense types identiﬁed for removal are predominantly senses from outside the domain. 
As part of its description of lexico-semantic predicate frames or conceptual structures, the FrameNet project defines a set of semantic roles specific to the core predicate of a sentence. Recently, researchers have tried to automatically produce semantic interpretations of sentences using this information. Building on prior work, we describe a new method to perform such interpretations. We define sentence segmentation first and show how Maximum Entropy re-ranking helps achieve a level of 76.2% F-score (answer among topfive candidates) or 61.5% (correct answer). 
This paper shows how to construct semantic representations from the derivations produced by a wide-coverage CCG parser. Unlike the dependency structures returned by the parser itself, these can be used directly for semantic interpretation. We demonstrate that well-formed semantic representations can be produced for over 97% of the sentences in unseen WSJ text. We believe this is a major step towards widecoverage semantic interpretation, one of the key objectives of the ﬁeld of NLP. 
We use the interpretation of vague scalar predicates like small as an illustration of how systematic semantic models of dialogue context enable the derivation of useful, ﬁne-grained utterance interpretations from radically underspeciﬁed semantic forms. Because dialogue context sufﬁces to determine salient alternative scales and relevant distinctions along these scales, we can infer implicit standards of comparison for vague scalar predicates through completely general pragmatics, yet closely constrain the intended meaning to within a natural range. 
The Colorado Literacy Tutor (CLT) is a technology-based literacy program, designed on the basis of cognitive theory and scientifically motivated reading research, which aims to improve literacy and student achievement in public schools. One of the critical components of the CLT is a speech recognition system which is used to track the child’s progress during oral reading and to provide sufficient information to detect reading miscues. In this paper, we extend on prior work by examining a novel labeling of children’s oral reading audio data in order to better understand the factors that contribute most significantly to speech recognition errors. While these events make up nearly 8% of the data, they are shown to account for approximately 30% of the word errors in a state-of-the-art speech recognizer. Next, we consider the problem of detecting miscues during oral reading. Using features derived from the speech recognizer, we demonstrate that 67% of reading miscues can be detected at a false alarm rate of 3%. 
& ' % "(  )  * # !)  #  %  +  '  ,  - $#  ! .#  +  !!  + $ !$ +  %+  )  / ++ + +  )+ )  )  #  !!  ,( /  )+# ! ) +  !+ % ,  # # 0# (+  #) %  ! # # / ++  ++  ( %$  +  ', %  #  !  "  )  ( 122 3 )#  %  %# +  '  % %4% % +  '  +  ( # !# ! ) ( ++  #  %#  ' % +(  / ) % +(  /  %# %  !)  5& , 6 7 8 )+#  4  %# %  % 50  67  9  ) %# 50% 6 7  / !(  (+  84:  ;<  )  %  %  (',  = %# % 5  %  +  7  )+ ) % 9  ) %#  /)  !  )  >%  % ! )# %#  ' ! # 9 #)  %  '  ) > '+  !)  #  )  !  4!  ( ( >!  % %,  , )+ ) % '  %%  .#  ,  +  ,  = %# %  / #%  +%  )  ! 5 # % 122 %  122  ) 122 /#! 6 67  >)  !! # / ?@A # ( !  ( )!  + %>  4  )+ % 5 %  *  67 ,  , ) ! )+ )  ( B $C  56 7  %  %# $> , % $  , ) ( )+#  !  ,+  ,8  %# > , % , .# ! ) %#  )+  ! ( , , %# % -  , !)  + $ !$ +  '  ##  (' +  ' 56 17 #  '% )  '  (+  %# #  $ %# # .#  !  %  #  %  '  , + /)  56 7 + +  %) )  # !,  % %/  #,  +  )+# ! ) ) () #  !) , $ %  % +#  /)  ,+  ) ,)  #,  )  ##  >  %!  &#  (, $ %  % +#  6,  + + ! 0#  (+  ++  =  '+ )  #  !#  # $%  %  #&!  /%  !) , > ! # % %)  ( )+  ,%  )+# !  (+  +%, #  +  (  8  !  +  !  +4  D  ←∅  ∀ 5# #3<7 ∈  )+#  5# #3<7  
We present a constraint-based syntax-semantics interface for the construction of RMRS (Robust Minimal Recursion Semantics) representations from shallow grammars. The architecture is designed to allow modular interfaces to existing shallow grammars of various depth – ranging from chunk grammars to context-free stochastic grammars. We deﬁne modular semantics construction principles in a typed feature structure formalism that allow ﬂexible adaptation to alternative grammars and different languages.1 
While research on question answering has become popular in recent years, the problem of efficiently locating a complete set of distinct answers to list questions in huge corpora or the Web is still far from being solved. This paper exploits the wealth of freely available text and link structures on the Web to seek complete answers to list questions. We introduce our system, FADA, which relies on question parsing, web page classification/clustering, and content extraction to find reliable distinct answers with high recall. 
We investigate the impact of the precision/recall trade-off of information extraction on the performance of an ofﬂine corpus-based question answering (QA) system. One of our ﬁndings is that, because of the robust ﬁnal answer selection mechanism of the QA system, recall is more important. We show that the recall of the extraction component can be improved using syntactic parsing instead of more common surface text patterns, substantially increasing the number of factoid questions answered by the QA system. 
In this paper we describe the analytic question answering system HITIQA (HighQuality Interactive Question Answering) which has been developed over the last 2 years as an advanced research tool for information analysts. HITIQA is an interactive opendomain question answering technology designed to allow analysts to pose complex exploratory questions in natural language and obtain relevant information units to prepare their briefing reports. The system uses novel data-driven semantics to conduct a clarification dialogue with the user that explores the scope and the context of the desired answer space. The system has undergone extensive hands-on evaluations by a group of intelligence analysts. This evaluation validated the overall approach in HITIQA but also exposed limitations of the current prototype. 
In this paper, we introduce a new semi-supervised learning model for word sense disambiguation based on Kernel Principal Component Analysis (KPCA), with experiments showing that it can further improve accuracy over supervised KPCA models that have achieved WSD accuracy superior to the best published individual models. Although empirical results with supervised KPCA models demonstrate signiﬁcantly better accuracy compared to the state-of-the-art achieved by either na¨ıve Bayes or maximum entropy models on Senseval-2 data, we identify speciﬁc sparse data conditions under which supervised KPCA models deteriorate to essentially a most-frequent-sense predictor. We discuss the potential of KPCA for leveraging unannotated data for partially-unsupervised training to address these issues, leading to a composite model that combines both the supervised and semi-supervised models. 
We present an automatic approach to learning criteria for classifying the parts-of-speech used in lexical mappings. This will further automate our knowledge acquisition system for non-technical users. The criteria for the speech parts are based on the types of the denoted terms along with morphological and corpus-based clues. Associations among these and the parts-of-speech are learned using the lexical mappings contained in the Cyc knowledge base as training data. With over 30 speech parts to choose from, the classiﬁer achieves good results (77.8% correct). Accurate results (93.0%) are achieved in the special case of the mass-count distinction for nouns. Comparable results are also obtained using OpenCyc (73.1% general and 88.4% mass-count). 
The paper presents a method for word sense disambiguation based on parallel corpora. The method exploits recent advances in word alignment and word clustering based on automatic extraction of translation equivalents and being supported by available aligned wordnets for the languages in the corpus. The wordnets are aligned to the Princeton Wordnet, according to the principles established by EuroWordNet. The evaluation of the WSD system, implementing the method described herein showed very encouraging results. The same system used in a validation mode, can be used to check and spot alignment errors in multilingually aligned wordnets as BalkaNet and EuroWordNet. 
In this paper we describe the extraction of thesaurus information from parsed dictionary deﬁnition sentences. The main data for our experiments comes from Lexeed, a Japanese semantic dictionary, and the Hinoki treebank built on it. The dictionary is parsed using a head-driven phrase structure grammar of Japanese. Knowledge is extracted from the semantic representation (Minimal Recursion Semantics). This makes the extraction process language independent. 
Lexico-semantic networks such as WordNet have been criticized about the nature of the senses they distinguish as well as on the way they define these senses. In this article, we present a possible solution to overcome these limits by defining the sense of words from the way they are used. More precisely, we propose to differentiate the senses of a word from a network of lexical cooccurrences built from a large corpus. This method was tested both for French and English and was evaluated for English by comparing its results with WordNet. 
Compound terms play a surprisingly key role in the organization of lexical ontologies. However, their inclusion forces one to address the issues of completeness and consistency that naturally arise from this organizational role. In this paper we show how creative exploration in the space of literal compounds can reveal not only additional compound terms to systematically balance an ontology, but can also discover new and potentially innovative concepts in their own right. 
Precise Natural Language Understanding is needed in Geometry Tutoring to accurately determine the semantic content of students’ explanations. The paper presents an NLU system developed in the context of the Geometry Explanation Tutor. The system combines unification-based syntactic processing with description logics based semantics to achieve the necessary accuracy level. Solutions to specific semantic problems dealing with equivalence of semantic representations are described. Experimental results on classification accuracy are also presented. 
This paper deals with contextual aspects of locative preposition processing. Firstly, accordingly to a study on the referential behavior of the pronominal adverb ”y” in its locative uses, we show that static locative prepositions are functions, not only at the conceptual level (as shown in previous studies), but also from a contextual point of view. Such functions introduce new objects in the discourse context, that have to be taken into account in classical contextual processing tasks, such as reference resolution or question answering. Then, we show how these phenomena can ﬁt into contextual formalisms like DRT and DPL. 
We present a learning-based method to identify single-snippet answers to definition questions in question answering systems for document collections. Our method combines and extends two previous techniques that were based mostly on manually crafted lexical patterns and WordNet hypernyms. We train a Support Vector Machine (SVM) on vectors comprising the verdicts or attributes of the previous techniques, and additional phrasal attributes that we acquire automatically. The SVM is then used to identify and rank single 250-character snippets that contain answers to definition questions. Experimental results indicate that our method clearly outperforms the techniques it builds upon. 
Identifying sentiments (the affective parts of opinions) is a challenging problem. We present a system that, given a topic, automatically finds the people who hold opinions about that topic and the sentiment of each opinion. The system contains a module for determining word sentiment and another for combining sentiments within a sentence. We experiment with various models of classifying and combining sentiment at word and sentence levels, with promising results. 
Previous works on question classiﬁcation are based on complex natural language processing techniques: named entity extractors, parsers, chunkers, etc. While these approaches have proven to be eﬀective they have the disadvantage of being targeted to a particular language. We present here a simple approach that exploits lexical features and the Internet to train a classiﬁer, namely a Support Vector Machine. The main feature of this method is that it can be applied to diﬀerent languages without requiring major modiﬁcations. Experimental results of this method on English, Italian and Spanish show that this approach can be a practical tool for question answering systems, reaching a classiﬁcation accuracy as high as 88.92%. 
In this paper, we consider the automatic text summarization as a challenging task of machine learning. We proposed a novel summarization system architecture which employs Gene Expression Programming technique as its learning mechanism. The preliminary experimental results have shown that our prototype system outperforms the baseline systems. 
We built an open-source software platform intended to serve as a common infrastructure that can be of use in the development of new applications involving the processing of Turkish. The platform incorporates a lexicon, a morphological analyzer/generator, and a DCG parser/generator that translates Turkish sentences to predicate logic formulas, and a knowledge base framework. Several developers have already utilized the platform for a variety of applications, including conversation programs and an artificial personal assistant, tools for automatic analysis of rhyme and meter in Turkish folk poems, a prototype sentence-level translator between Albanian, Turkish, and English, natural language interfaces for generating SQL queries and JAVA code, as well as a text tagger used for collecting statistics about Turkish morpheme order for a speech recognition algorithm. The results indicate the adaptability of the infrastructure to different kinds of applications and how it facilitates improvements and modifications. Introduction The obvious potential of natural language processing technology for economic, social and cultural progress can be realized more comprehensively if NLP techniques applicable to a wider selection of the languages of the world are developed. Before the fullscale treatment of a new language can start, a considerable amount of effort has to be invested to computerize the lexical, morphological and syntactic specifics of that language, which would be required by any nontrivial application.  We built an open-source software platform intended to serve as a common infrastructure that can be of use in the development of new applications involving the processing of Turkish. The platform, named TOY (Çetino lu 2001), is essentially a big set of predicates in the logic programming language Prolog. The choice of Prolog, which was designed specifically with computational linguistics applications in mind, as the implementation language for our software has natural consequences for the knowledge representation setup to be used by other programs built on our platform. Prolog is based on first-order predicate calculus, it allows knowledge items to be represented in terms of logic-style facts and rules, and a built-in theorem prover drives the execution of Prolog queries. The TOY program’s internal organization into source files reflects the three different levels (see Figure 1) on which text-based NLP applications can be based. In terms of that figure, processing at a “deeper” level necessitates all components of “shallower” levels. In this paper, we describe this infrastructure and how it was adapted to a variety of applications. Section 2 gives a brief overview of the infrastructure. Section 3 presents the applications based on it. 
This paper evaluates the accuracy of HPSG parsing in terms of the identiﬁcation of predicate-argument relations. We could directly compare the output of HPSG parsing with PropBank annotations, by assuming a unique mapping from HPSG semantic representation into PropBank annotation. Even though PropBank was not used for the training of a disambiguation model, an HPSG parser achieved the accuracy competitive with existing studies on the task of identifying PropBank annotations. 
We describe the rapid development of a preliminary Hebrew-to-English Machine Translation system under a transfer-based framework speciﬁcally designed for rapid MT prototyping for languages with limited linguistic resources. The task is particularly challenging due to two main reasons: the high lexical and morphological ambiguity of Hebrew and the dearth of available resources for the language. Existing, publicly available resources were adapted in novel ways to support the MT task. The methodology behind the system combines two separate modules: a transfer engine which produces a lattice of possible translation segments, and a decoder which searches and selects the most likely translation according to an English language model. We demonstrate that a small manually crafted set of transfer rules sufﬁces to produce legible translations. Performance results are evaluated using state of the art measures and are shown to be encouraging. 
We present a relatively large-scale initiative in high-quality MT based on semantic transfer, reviewing the motivation for this approach, general architecture and components involved, and preliminary experience from a ﬁrst round of system integration (to be accompanied by a hands-on system demonstration, if appropriate).  The translation problem is one whose solution must be searched incrementally.  There will be no dramatic event to signal the end of the search.  
The paper directly compares two versions of a medical speech translation system, one with a grammar based language model (GLM) recognizer and the other with a statistical language model (SLM) recognizer. We construct the GLM using a corpus-based method, so that both the GLM and the SLM can be derived from the same corpus; evaluation is carried out with respect to performance on the speech translation task. Despite using a very small training set for both the GLM and the SLM, the SLM delivers much better word error rates on unseen test material. Nonetheless, evaluating both systems on translation performance rather than word error rates, the GLM-based version of the system outperforms the SLM on the actual translation task.  
Pairing structural descriptions in MT, syntax-semantics interfaces and so on becomes more difﬁcult the more structurally different are the languages involved; there is, implicitly or explicitly, a process of ‘tree parsing’, where a structural description is split into component smaller trees for transfer rules to be applied. Recent work has looked at the construction of transfer rules, using both symbolic and statistical approaches, that require the pairing of groups of several contiguous nodes in structural descriptions. We look at the case where pairings of groups of non-contiguous nodes are necessary, and present an efﬁcient dynamic programming algorithm based on TAG and drawing on compiler theory for a decomposition into appropriate groupings. We then examine the formal properties of this algorithm, and show that it is linear in the number of nodes in the tree and has the same complexity as existing algorithms requiring only groupings of contiguous nodes. 
In this paper we introduce a fully automatic method to extend an existing rich bilingual valency dictionary by using information from multiple plain bilingual dictionaries. We evaluate our method using a translation regression test, and get an improvement of 7%.  
A difficult aspect of the translation of English text into American Sign Language (ASL) animation has been the production of ASL phenomena called “classifier predicates.” The complex way these expressions use the 3D space around the signer challenges traditional MT approaches. This paper presents new models for classifier predicates based on a 3D spatial representation and an animation planning formalism that facilitate a translation approach and are compatible with current linguistic accounts of these phenomena. This design can be incorporated into a multi-path architecture to build English-to-ASL MT systems capable of producing classifier predicates. 1. Introduction and Background Although Deaf students in the U.S. and Canada are taught written English, their inability to hear spoken English results in most Deaf U.S. high school graduates (18+ years old) reading at a fourth-grade (10 year old) level (Holt, 1991). Unfortunately, many Deaf accessibility aids, like television closed captioning or teletype telephones, assume the user has strong English literacy skills. Many Deaf people with English reading difficulty are fluent in American Sign Language (ASL), and so an English-to-ASL MT system can make information and services accessible when English captioning text is at too high a reading level or a live interpreter is unavailable. ASL is a natural language used by the half million Deaf people in the United States and Canada. The structure of ASL is quite different than English, and its visual modality allows it to use phenomena not seen in spoken language yet argued to be linguistic (Neidle et al., 2000; Liddell, 2003). In addition to using hands, facial expression, eye gaze, head tilt, and body posture to convey meaning, an ASL signer can use the surrounding space for communicative purposes. For example, signers can assign discourse entities locations in space and later refer to them by pointing to these locations. The locations are not meaningful topologically, i.e. positioning an entity to the left of another in space doesn’t mean it is to the left of the other in the real world. 1.1. Classifier Predicates: A Spatially Complex Phenomena Other ASL expressions are more complex in their use of space and position invisible objects around the signer to topologically indicate the layout of entities in a 3D scene being discussed. Constructions called “classifier predicates” allow signers to use their hands to position, move, trace, or re-orient an imaginary object in the space in front of them to indicate the location, movement, shape, contour, physical dimension, or some other property of a corresponding real world entity. This paper will focus on classifier predicates of movement and location (CPMLs) of entities in a 3D scene. CPMLs consist of a semantically meaningful handshape and a 3D arm movement path. A classifier handshape is chosen from a closed set based on characteristics of  the entity described (whether it is a vehicle, human, animal, etc.) and what aspect of the entity is described (position, motion, etc). A CPML is often preceded by a noun phrase indicating the entity whose locomotion will be depicted. (While not illustrated in this paper, a CPML’s 3D path can be linguistically conventional rather than visually representational (Liddell, 2003); such predicates can also be handled by the linguistic models and MT approach discussed in section 4.) For example, the sentence “the car parked between the cat and the house” can be expressed using a set of three CPMLs. After making the ASL sign CAT, a signer would move a hand in a “bent V” handshape (see Figure 1) forward and slightly downward to a point in space in front of his or her torso where an imaginary miniature cat could be envisioned. Next, after making the ASL sign HOUSE, the signer would make a similar motion with a “downward C” handshape to a place where a house could be envisioned. Finally, after signing CAR, the signer would place their dominant hand in a “sideways 3” handshape and trace a path in space to indicate the route taken by the vehicle. At the end of the motion (at a location between the ‘cat’ and the ‘house’), the signer would position the open palm of their non-dominant hand. The dominant hand would end its motion by coming to rest atop the platform produced by the non-dominant hand. Generally, “bent V” handshapes are the classifier for stationary animals, “downward C” for boxy objects, and “sideways 3” for vehicles. As the example suggests, translation into an ASL classifier predicate is complex because of the productive and space-depicting nature of these expressions. 1.2. Previous Direct and Transfer ASL MT Architectures Since ASL has no written form, there are currently insufficient parallel English-ASL corpora for stochastic MT approaches; so, implemented ASL MT systems have used non-stochastic direct and transfer MT architectures. Graphics software is used to animate 3D virtual human characters to perform the signing output; therefore, these systems convert English text into an animation control script directing the characters how to perform ASL. Direct systems have used word-tosign dictionaries to produce Signed English (a non-ASL English-like form of signing), and transfer systems have handled more divergences to produce actual (but limited) ASL output (Huenerfauth, 2003). An interlingual MT system has also been proposed (Veale et al., 1998). Because these systems employed only traditional lexical and grammatical resources and because they made no attempt to model the spatial arrangement of objects in the 3D scene being discussed, they were unable to produce classifier predicates from spatial English text. Omitting these phenomena from the coverage of an English-to-ASL MT system is unsatisfactory for several reasons: (1) many English concepts lack a fluent ASL translation without classifier predicates, (2) these phenomena are common in native signing (ASL signers produce classifiers 1 to 17 times per minute, depending on genre) (Morford and MacFarlane, 2003), and (3) English/ASL translation pairs involving classifier predicates are quite structurally divergent (and would thus be particularly useful to translate for a Deaf user with limited English literacy skills). Figure 1: ASL Classifier Predicate Handshapes: “Bent V,” “Downward C,” and “Sideways 3”  
The problem of evaluating machine translation (MT) systems is more challenging than it may ﬁrst appear, as diverse translations can often be considered equally correct. The task is even more difﬁcult when practical circumstances require that evaluation be done automatically over short texts, for instance, during incremental system development and error analysis. While several automatic metrics, such as BLEU, have been proposed and adopted for largescale MT system discrimination, they all fail to achieve satisfactory levels of correlation with human judgments at the sentence level. Here, a new class of metrics based on machine learning is introduced. A novel method involving classifying translations as machine or humanproduced rather than directly predicting numerical human judgments eliminates the need for labor-intensive user studies as a source of training data. The resulting metric, based on support vector machines, is shown to signiﬁcantly improve upon current automatic metrics, increasing correlation with human judgments at the sentence level halfway toward that achieved by an independent human evaluator. 
Automatic evaluation metrics for Machine Translation (MT) systems, such as BLEU and the related NIST metric, are becoming increasingly important in MT. This paper reports a novel method of calculating the confidence intervals for BLEU/NIST scores using bootstrapping. With this method, we can determine whether two MT systems are significantly different from each other. We study the effect of test set size and number of reference translations on the confidence intervals for these MT evaluation metrics. 1. Introduction Automatic evaluation for Machine Translation (MT) systems has become prominent with the development of data driven MT. The essential idea comes from the highly successful word error rate metric used by the speech recognition community. For MT evaluation this has been extended to multiple reference translations (Nießen et al. 2000), and allowing for differences in word order (Leusch et al. 2003). In (Papineni et al, 2002) the BLEU metric was proposed, which averages the precision for unigram, bigram and up to 4-grams and applies a length penalty for translations too short. A variant of BLEU has been developed by NIST, using the information gain of the ngrams. Additional modifications to BLEU-type metrics have been proposed to improve the correlation with human evaluation scores (Melamed 2003, Pepescu-Belis 2003). Both BLEU/NIST metrics require a test suite to evaluate the MT systems. A test suite consists of two parts: testing sentences in the source language and multiple human reference translations in the target language. To have enough coverage in the source language, a test suite usually has hundreds of sentences. In order to cover translation variations multiple human references are used, typically 4 or more. This makes building a test suite expensive. Therefore, the BLEU/NIST scores are usually based on one test suite. Thus, we have to ask ourselves a question: "Is this score reliable?" Or in other words, what is the confidence interval for a specific metric, a particular translation system, and a given test set. Fortunately, statistical testing theory has developed an appropriate tool to deal with this kind of situation, the so-called bootstrapping method. After a short introduction into the MT evaluation metrics we will describe this bootstrapping approach. We will then study in detail the effect of test set size and the number of reference translations on the width of the confidence interval. In the case study presented in this paper we will use results from the TIDES MT evaluation 2002, esp. from the so-called large data track Chinese-English translation systems.  2. MT Evaluation Metrics  2.1. IBM BLEU metric  The BLEU metric is based on the modified n-gram precision, which counts how many n-grams  of the candidate translation match with n-grams of the reference translation:  ∑  ∑ Countmatched (n − gram)  ∑ ∑ pn  =  Sent∈{Hyp} n− gram∈Sent Count(n −  gram)  (Eq. 1)  Sent∈{Hyp} n− gram∈Sent  To compute pn, one first counts the maximum number of times an n-gram occurs in any single reference translation. Next, one clips the total count of each candidate n-gram by its maximum reference count, adds these clipped counts up, and divides by the total (unclipped) number of candidate words.  To bring in the factor of “recall”, BLEU uses a “brevity penalty” to penalize candidates shorter  than their reference translations. For a candidate translation with length c, its brevity penalty is  defined as:  BP  =  1 e (1−r  /  c)  if if  c>r c≤r  (Eq. 2)  where r is the “best match length” among all reference translations.  The final BLEU score is then the geometric average of the modified n-gram precision multiplied  by  the  brevity  penalty:  BLEU  =  N BP • exp(∑ wn log  pn )  n=1  (Eq. 3)  Usually, N=4 and wn=1/N.  2.2. Modified BLEU metric  The BLEU metric focuses heavily on long n-grams. A low score on 4-grams will result  in an overall low score, even if unigram precision is high. This is due to the fact that the  geometric mean of the n-gram precision scores is used. We proposed a modified version  to the original BLEU metric called the “modified BLEU” (M-BLEU for short). In M-  BLEU, a more balanced contribution from the different n-grams is achieved using the  arithmetic  means  of  the  n-gram  precisions.  M  − BLEU  =  N ∑ BP • wn pn  (Eq.  4).  The  calculation  n =1  of the modified n-gram precision and the brevity penalty is the same as in BLEU.  2.3. NIST Mteval metric  The motivation of NIST MTeval scoring metric (NIST score in short) is to weight more heavily  those N-grams that are more informative. This would, in addition, help to combat possible  gaming of the scoring algorithm, since those N-grams that are most likely to (co-)occur would  add less to the score than less likely N-grams. With the information gain  we get:  Info(w1....wn  )  =  log2    the the  # #  of of  occurrences of occurrences of  w1...wn−1 w1...wn    (Eq. 5)  Averaged  Modified  n − gram  ∑ ∑ Pr ecision =  N      all  _  w1  ... wn  _  Info(w1...wn that _ co −occur  )      ∑ n=1   (1)      all _ w1 ...wn _ in _ hyp    (Eq. 6)  BP = exp{β log2[min( Lhyp ,1)]} Lref  (Eq. 7)  The brevity penalty is calculated as Eq. 7, where β is chosen to make the penalty=0.5 when Lhyp  = 2/3 * Lref. β =-4.22.  Despite the motivation to put more weights on those n-grams that are more “informative”, the NIST metric fails to do so especially for the high-order n-grams. Zhang et al. (2004) observed that 80% of the NIST score for a typical MT system came from the unigram matches. Some 5gram matches were given no credits because their information value is 0. A particular feature of the NIST metric is that the scores increase with test set size. The reason for this is that when the test set size increases, the number of different n-grams, and thereby the information gain for each n-gram also increases. This leads to problems when comparing NIST scores. For example a system with NIST score 10.3 over a test set of 100 documents is not necessarily better than a system with NIST score 8.9 over a test set of 80 documents.  2.4. Human judgment Human assessments were carried out by LDC for the test set used in the 2002 TIDES MT evaluation. Similar to the DARPA-94 MT evaluation (White 94), the human assessment was a holistic scoring by human evaluators on the basis of the somewhat vaguely specified parameters of fluency and adequacy. Human evaluators were asked to assign the fluency and adequacy scores for each sentence generated by MT systems. The scores range from 1 to 5, where 1 stands for “worst” and 5 for “best”. Each sentence was evaluated by at least two evaluators and we use the averaged value as the human judgment for that sentence. Averaged among all the translation sentences, the sum of the fluency and adequacy is the human judgment for that MT system.  3. Confidence Intervals based on Bootstrap Percentiles In statistical tests, we often use confidence interval to measure the precision of an estimated value. The interval represents the range of values, consistent with the data, which is believed to encompass the "true" value with high probability (usually 95%). The confidence interval is expressed in the same units as the estimate. Wider intervals indicate lower precision; narrow intervals, greater precision. The estimated range is calculated from a given set of sample data. Since building test suites is expensive, it is not practical to create a set of testing suites to generate a set of sample BLEU/NIST scores. Instead, we use the well-known bootstrapping technique to measure the confidence interval for BLEU/NIST. Bootstrapping is a data-based statistical method for statistical inference, which can be used to measure the confidence interval (Efron and Tibshirani, 1993).  3.1. Algorithm Suppose we have a test suite T0 to test several Machine Translation systems translating from Chinese to English. There are N Chinese testing segments in the suite and for each testing segment we have R human reference translations. A segment is typically a sentence, but it can also be a paragraph or a document. Let’s represent the i-th segment of T0 as an n-tuple ti=<si, ri1, ri2,..,riR>, where si is the i-th Chinese segment to be translated and ri1 to riR are the R human translations (references) for segment si. Create a new test suite T1 with N segments by sampling with replacement from T0. Since we sample with replacement, a segment in T0 may occur zero, once or more than once in T1. Repeat this process for B times, e.g. B=2000, and we have B new test suites: T1... TB. T1 to TB are artificial test suites (also called bootstrap samples) created by resampling T0. Evaluate the MT systems on each of these B test suite using any MT evaluation metric, like WER, BLEU, M-BLEU, NIST, or even human evaluation scores. We will then have B scores. As one may expect, these scores have a rough normal distribution. Figure 1 shows an example of the BLEU score distribution over 20000 resampled test suites for an MT system. From these B scores, find the middle 95% of the scores (i.e. the 2.5th percentile: scorelow and the 97.5th percentile scoreup). [scorelow, scoreup] is the 95% confidence interval for the used evaluation metric for this MT system (Figure 2).  Frequency  Historgram of 20000 BLEU Scores 500 450 400 350 300 250 200 150 100 50 0 BLEU Score Figure 1. The histogram of 20000 BLEU scores  Figure 2. Measuring the confidence intervals for a MT evaluation score  We evaluated 7 Chinese-English MT systems based on the June 2002 evaluation set. The testing  data has 100 documents (878 sentences) and 4 human translations are used as references. We  created 2000 bootstrapping samples for each system and report their median score and the 95%  relative confidence intervals in Table 1. Relative confidence interval is defined as   −   Median − Scorelow Median  %,  +  Scoreup − Median Median   %   Results are given in Table 1. We see that we need different relative improvements for different  metrics before we can claim to have made a statistically significant improvement in our machine  translation system. It seems that the focus on longer n-grams in the BLEU metric make it less  discriminative than the unigram centered NIST score. This is in line with the design principles of  the NIST metric, to have high sensitivity when comparing different systems. M-BLEU stands in  the middle. There seems also to be a tendency that better systems are more consistent in their  translations, leading to smaller confidence intervals.  Table 1. The relative confidence intervals for 7 MT systems with B=2000  NIST  BLEU  M-BLEU  System Median  Interval  Median  Interval Median Interval  A  7.191 [-1.69%, +1.69%] 0.184 [-4.35%, +4.41%] 0.125 [-2.48%, +2.48%]  B  6.194 [-2.68%, +2.63%] 0.165 [-5.44%, +5.32%] 0.113 [-3.18%, +2.91%]  C  6.954 [-1.83%, +1.87%] 0.180 [-4.55%, +4.66%] 0.120 [-2.75%, +2.75%]  D  6.527 [-1.74%, +1.78%] 0.145 [-4.98%, +5.25%] 0.108 [-2.51%, +2.60%]  E  4.941 [-2.23%, +2.10%] 0.076 [-6.48%, +6.88%] 0.072 [-2.90%, +2.62%]  F  7.487 [-1.82%, +1.75%] 0.240 [-3.99%, +3.74%] 0.147 [-2.65%, +2.45%]  G  7.165 [-1.72%, +1.66%] 0.184 [-4.67%, +4.35%] 0.124 [-2.58%, +2.42%]  3.2. Comparing Two MT Systems In a way similar to measuring the confidence intervals for an MT system’s BLEU/NIST score, we can use bootstrapping to measure the confidence intervals for the discrepancy between the two MT systems.  Create test suites T0, T1... TB, where T1 to TB are artificial test suites created by resampling T0. System X scored x0 on T0 and system Y scored y0. The discrepancy between system X and Y is δ0=x0-y0. Repeat this process on every B test suite and we have B discrepancy scores: δ1, δ2...δB. From these B discrepancy scores, find the middle 95% of the scores (i.e. the 2.5th percentile and the 97.5th percentile). That is the 95% confidence interval for the discrepancy between MT system X and Y. If the confidence interval does not overlap with zero, we can claim that the difference between system X and Y are statistically significant.  In Figure 3 we compared 7 Chinese-English MT systems according to their Human, BLEU, NIST and M-BLEU scores. In this figure, “>” means system X is significantly “better” than system Y, where as “<” means that system X is significantly “worse” than Y. If the discrepancy between X and Y is not significant, i.e. the confidence interval overlaps with zero, we use “~” to represent that the two systems are not significantly different.  Figure 3. Comparison among 7 Chinese-English MT systems by BLEU  3.3. Implementation To calculate the confidence intervals using bootstrapping, we need to translate and evaluate the MT systems on each of the B test suites. B needs to be large, say, 1,000 or even 10,000, to guarantee reliable results. For most MT systems, the translation for a segment is independent of the previous segments in the test suite. In other words, the translation of segment s should always be the same no matter which test suite it is part of. In that sense, we do not need to translate B test suites. Instead, we only need to resample the translations of T0 and their corresponding human references. We developed an efficient method for bootstrapping. After translating T0, all the ngram matching information for segments in T0 are collected and stored in an array. To simulate the translation results of the artificial test suites, we need only resample the information from this array and calculate the BLEU/NIST scores from the segment’s scores1. 4. Discussions Equipped with the bootstrapping method, we can now study in detail the effect of test set size and the number of reference translations on the width of the confidence interval. 4.1. How much testing data is needed? For the TIDES MT evaluations the test set contain typically 100 documents, where a document has about 7~9 sentences on average. Do we really need nearly 1,000 test sentences to make the evaluation meaningful? Figures 4 to 7 show the NIST, BLEU, M-BLEU, and human evaluation score (sum of fluency and adequacy scores) for 7 different translation systems when the test set size varies from 10 to 100 documents, corresponding to about 10-100% of the entire test set. Each time 10 random documents were added to the existing test set in this ablation study.  Figure 4. NIST Scores for 7 MT systems over Figure 5. BLEU Scores for 7 MT systems over  different size of testing data  different size of testing data  
Previous work on marker-based EBMT [Gough & Way, 2003, Way & Gough, 2004] suffered from problems such as data-sparseness and disparity between the training and test data. We have developed a largescale robust EBMT system. In a comparison with the systems listed in [Somers, 2003], ours is the third largest EBMT system and certainly the largest English-French EBMT system. Previous work used the on-line MT system Logomedia to translate source language material as a means of populating the system’s database where bitexts were unavailable. We derive our sententially aligned strings from a Sun Translation Memory (TM) and limit the integration of Logomedia to the derivation of our word-level lexicon. We also use Logomedia to provide a baseline comparison for our system and observe that we outperform Logomedia and previous marker-based EBMT systems in a number of tests. 
In this paper, we propose incorporating similar sentence retrieval in machine translation to improve the translation of hard-to-translate input sentences. If a given input sentence is hard to translate, a sentence similar to the input sentence is retrieved from a monolingual corpus of translatable sentences and then provided to the MT system instead of the original sentence. This method is advantageous in that it relies only on a monolingual corpus. The similarity between an input sentence and each sentence in the corpus is determined from the ratio of the common N-gram. We use two conditions to improve the retrieval precision and add a ﬁltering method to avoid inappropriate sentences. An experiment using a Japanese-to-English MT system in a travel conversation domain proves that our method improves the translation quality of hard-to-translate input sentences by 9.8 %. 
An improved method for extracting translation equivalents from bilingual comparable corpora according to contextual similarity was developed. This method has two main features. First, a seed bilingual lexicon—which is used to bridge contexts in different languages—is adapted to the corpora from which translation equivalents are to be extracted. Second, the contextual similarity is evaluated by using a combination of similarity measures defined in opposite directions. An experiment using Wall Street Journal and Nihon Keizai Shimbun corpora, together with the EDR bilingual dictionary, demonstrated that the method effectively improves the coverage of a bilingual lexicon; the accuracy of lists of candidate translation equivalents for frequently occurring unknown words was around 30%. 1. Introduction Wide-coverage bilingual lexicons are essential in machine translation and cross-language information retrieval; therefore, automatic extraction of translation equivalents from corpora has been an important research issue over the last decade. Technologies for extracting translation equivalents from parallel corpora have been established (Gale and Church 1991; Kupiec 1993; Dagan, et al. 1993; Fung 1995; Kitamura and Matsumoto 1996; Melamed 1997). However, the availability of large parallel corpora is extremely limited. Methods for extracting translation equivalents from a pair of weakly comparable corpora, i.e., corpora of the same domain in different languages, are therefore required. Rapp (1995) demonstrated the possibility of extracting translation equivalents from comparable corpora; the underlying assumption is that a word and its translation occur in similar contexts. Subsequently, several researchers developed a method of evaluating the similarity between contexts of words in different languages with the assistance of a seed bilingual lexicon. However, it has not yet been proved practicable. Kaji and Aizono (1996) demonstrated the effectiveness of the method on pairs consisting of a document and its translation, but not on comparable corpora in general definition. Fung and McKeown (1997) first applied the method to comparable corpora. However, their experiment was done under an impractical setting; namely, candidate translation equivalents were beforehand restricted to a small set of manually selected words. Note that many words other than manually selected ones can have similar contexts as a target word1. Fung and Yee (1998) proposed an improved method that takes into account the reliability of seed pairs of translation equivalents, but it was not evaluated quantitatively. Rapp (1999) achieved relatively high extraction precision. 
We present a series of models for doing statistical machine translation based on labeled semantic dependency graphs. We describe how these models were employed to augment an existing example-based MT system, and present results showing that doing so led to a significant improvement in translation quality as measured by the BLEU metric. 1. Introduction Much research of late has been devoted to the invention and implementation of statistical machine translation (hereafter: SMT) systems of the kind originally described in (Brown et al., 1993). What these systems have in common is that they try to predict the most likely target language string given an input string in the source language using statistical methods. While traditional SMT systems do not explicitly model structural linguistic properties of the languages they are translating, recent trends in both SMT and speech recognition research suggest that it may be worthwhile to pay more attention to these properties when creating language models. Language models and SMT systems that take syntactic information into account are becoming more numerous. A representative sample includes the work of Chelba (1997), Charniak (2001), Knight and Yamada (2001), Charniak, Knight and Yamada (2003), and Eisner (2003). The intuition is that syntax-based models ought to be able to capture long-distance dependencies that surface-string models are unable to capture because events that depend on each other are often closer together in the syntax tree than they are in the surface string, in the sense that the distance to a sibling or common parent may be significantly shorter than distance between the same events in the surface string. The system described in this paper is an attempt to take one step further in the same direction: if syntactic models are good because they move events that depend on each other closer together than they are in the surface string, then semantic models ought to be even better. A semantic representation of a sentence ought to bring related events into nearer proximity by explicitly representing semantic dependencies still latent in the syntactic representation. In this paper, we show how our existing example-based MT system using labeled semantic dependency graph translation mappings benefited from the application of SMT techniques while still preserving the benefits provided by rich hierarchical linguistic information. 2. Description of the baseline system and the statistical framework 2.1. The baseline system For an overview of the example-based system to which we’ve applied these statistical techniques, see (Richardson et al, 2001). In a nutshell, the system parses aligned source and target language training sentences using a bottom-up, multi-path chart parsing algorithm, aligns the resulting  dependency graphs and creates a set of mappings from source language dependency graph fragments to target language dependency graph fragments (hereafter referred to as DG mappings). The nodes in a given dependency graph represent the lemmas of all concept words in the corresponding sentence. A pair of nodes may be connected by a directed edge labeled by a semantic or deep-syntactic dependency relationship. Associated with each node in the graph are zero or more binary features which specify additional syntactic or semantic information about the lemma of the node to which they are attached. Corresponding source and target dependency graphs are aligned to one another with the aid of a learned translation dictionary. At runtime, the input string is parsed and converted to a dependency graph which is then matched against the database of learned dependency graph fragments. This process yields the set of all (possibly overlapping) mappings whose source dependency graph chunk matches a portion of the input dependency graph. Figure 1: System diagram The baseline system then employs a greedy heuristic decoder to decide which set of transfer mappings to use. The mappings are sorted by size, then by the number of binary features matched, and finally by frequency. The first compatible set of mappings found that covers the input dependency graph is selected. Any gaps (nodes in the input graph not covered by the learned mappings) are filled in using a learned translation dictionary. The target sides of the chosen mappings are then combined into a complete target language dependency graph and passed to a generation module which generates the target language surface string.  2.2. The statistical framework The statistical framework described in this paper is designed as a replacement for the heuristic decoding mentioned above. It attempts to answer the question: “Which of all of the possible combinations of transfer mappings whose source component is covered by a portion of the input dependency graph will yield the best target language dependency graph?” The rest of the system components (i.e. the alignment and partitioning of the training data and the analysis and generation components) did not have to be modified for the new system. The traditional noisy-channel SMT model attempts to find the highest-probability translation for a sentence T = arg max(P(T | S)) , (1) where S is the source language sentence and T the target language sentence. By Bayes’ rule, T = arg max(P(T | S)) = arg max(P(S | T )P(T )). (2) A target language model trained on monolingual target language data is used to compute an estimate of P(T), and channel models of varying complexity are built to compute and estimate P(S|T). In our system, individual candidate mappings and combinations of mappings are scored using a linearly interpolated combination of scores from several heterogeneous information sources. This “kitchen sink” approach to SMT is superficially similar to the system described in (Och and Ney, 2002), except that it works with dependency graph mappings instead of with surface strings. Formally, then, we are looking for the set of transfer mappings Tmax out of all sets of transfer mappings T such that: Μ ∑ Tmax = arg max{ λµ Scorem (T )} , (3) µ =1 where µ ε Μ are the individual models, each of which assigns a score to a set of transfer mappings. The sources consulted for our system include a probabilistic channel model, target language model, and simple fertility model. Additional information sources whose scores are interpolated with the traditional SMT models include mapping size and number of binary features matched. We train weights for the models using Powell’s algorithm to maximize the BLEU score on the output of the system (Papineni et al., 2001). While the general idea behind these approaches is not new, the innovation in both cases is to apply these techniques at the labeled dependency graph level rather than at the string level. We know of no other system to date that has used statistical techniques to maximize the probability of labeled dependency-graph-to-dependency-graph mappings for machine translation. 3. Models 3.1. Target language model Most SMT systems use surface string n-gram models for their target language model. This has a number of advantages, particularly in a string-to-string translation system. Perhaps most  importantly, since n-gram models have been used in speech recognition for quite some time, there is considerable literature devoted to the subject of smoothing and compressing them (c.f. Stolcke, 1998; Goodman, 2000). The models are simple to train and to use, and it is relatively easy to optimize the size of the models to find a good tradeoff between minimizing space requirements and getting the precision necessary for the given task. However, surface-string n-gram models have some limitations as well. Perhaps the greatest limitation is their primary independence assumption, that word wi depends only on words wi-1, wi2 … wn-1. While it’s sometimes true that a word can be accurately predicted from one or two of its immediate predecessors, a number of linguistic constructs place highly predictive words sufficiently far from the words they predict that they are excluded from the scope of the n-gram. (Think, for example, of extraposition or of extended adjective constructions). We attempt to circumvent this particular shortcoming by building an n-gram model based on dependency graphs instead of on surface strings. Our target language model is a 5-gram model based on a vertical Markovization1 of dependency graphs that have been converted to trees, in which relations are represented as first-class events (nodes) along with the lemmas they relate. We make the Markov independence assumption that the probability of a given tree is the product of the probability of each of the nodes given its (n-1) previous ancestors. Thus we compute the probability of the target dependency graph τ by the following formula:  |τ |  ∏ PµT (τ ) = P(ci | ci−1...ci−(n−1) , µT ) ,  (4)  i  where C are all of the nodes in τ, ci-1 denotes the parent of ci, and n is the order of the model. The  model is pruned by removing infrequently-occurring n-grams and smoothed using interpolated  absolute discounting.  Consider the following dependency graph:  The probability of the graph according to a trigram DG model would be: P(A | ROOT) * P(R1 | ROOT A) * P (R2 | ROOT A) * P (B | A R1) * P(C | A R2) * P(D | A R2) * P (LEAF | R1 B) * P(LEAF | R2 C) * P(LEAF | R2 D) 
When building a machine translation system, the embedded part-of-speech (PoS) tagger deserves special attention, since PoS ambiguities are one of the main sources of mistranslations, specially when related languages are involved. The standard statistical approach for PoS tagging are hidden Markov models (HMM) properly trained by collecting statistics from source-language texts. In the case of bidirectional machine translation systems, this kind of training is often individually performed on each PoS tagger without taking into account the other language, that is, the corresponding target language. But target-language information may help to improve performance. In this paper, a new method is proposed which trains both PoS taggers simultaneously using mutual interaction: at every iteration, the parameters of the HMM corresponding to one of the languages are reﬁned by using the statistical data supplied by the current HMM for the other language. Both models bootstrap by learning cooperatively in an unsupervised manner and require only monolingual texts; no aligned texts are needed. Preliminary results are promising and surpass those of traditional unsupervised approaches. 
Cities and the birth of Guilds in the Middle Ages ................................................................. 2 The Renaissance ................................................................................................................ 4 The Age of Nations ............................................................................................................. 5 From the Industrial Revolution to the present...................................................................... 5 The Age of Globalization ..................................................................................................... 6 Today: a time to act ...........................................................................................………………6  Translation: an industry crying out for standards Globalization means different things to different people. For translators, globalization is the age when people talk to people without borders and through more and more diverse media. The internet makes communication instant, free, ever-present, free-flowing - but the one thing computers cannot do is translate. This is one of Globalization’s bottlenecks, which places the translator in a crucial situation. In Europe, the demise of the USSR and of Yugoslavia has resulted in the appearance of new Nation-states. In the process of legitimizing their national identities, language is a key issue. The geopolitics of language is sensitive. Moreover, new nations have joined the European Union. Unlike the UN which chose to use a few central languages, the EU has decided to use all of them in many activities, even if some sectors remain restricted to a few pivotal languages (more info on this passionating subject can be obtained at http://europa.eu.int/comm/dgs/translation/enlargement/preparing en.htm). Every sovereign State can communicate in Brussels or read legislation using its language. This is seen as making globalization respectful of national identities. The increase of translation is a technical challenge. It is also a moral issue. Translators are what the Greek language called hermeneuts: they don’t convey only information, they serve as bridges, go-betweens, almost ambassadors between people of different cultures. The translator is not just another trader who exploits the lack of understanding among people to make a living, he has to remain a person who offers his diplomatic skills, his vast culture, to make people understand each other, work together. While some actors of the translation industry may see globalization as profitable manna for quick money making, we as responsible translators should also consider what our duties are. Globalization will increase the possibilities for translators to play an important role in history, something that they have always done, sometimes at the risk of their lives. The following paper wishes to offer a historical and moral perspective on the role a world guild of translators could play. The author is aware of the various existing organisations, such as the FIT (http://www.fit-ift.org/). who have already played a pioneering role in this respect, and extends them a heartfelt salute. As a consultant and a trainer, I am in daily contact with a profession in turmoil, faced with an exceedingly competitive and demanding localization industry, in a marketplace that has gone wildly global. My vision is that our current situation is not a problem, but an opportunity to leap-frog history and create something totally new. Should I fail in conveying this vision, I would be honoured if some of the ideas I present would spark discussions of some sort. Guilds: a historical perspective Cities and the birth of Guilds in the Middle Ages Guilds appeared in the Carolingian era, with the purpose of regrouping professions sharing a common interest, but which could not find identification or protection within the feudal order.  Because they represented a self-organized power coming from the people, they may be seen as some embryo of the future civil society. Guilds flourished in Northern Europe (England, the Netherlands, Northern France, Rhenania and soon everywhere) and mostly concerned merchants and traders in the beginning. Being involved in transnational activities, they entailed some multilingualism. Later, guilds come to unite craftsmen at the turn of the 12th century. They later evolve into corporations. Guilds remain connected to the birth of cities in Europe. The Middle Ages see the multiplication and the establishment of medium-size cities that are havens for all sorts of trades and crafts, organised around the Church, the market place, and local governance. The City is the cradle that fosters the birth of the guilds that naturally emerge to satisfy the needs of various newly-formed professions. Though the Church was initially hostile to fraternities, the Scholastic thought later acted to legitimize this form of social organization. Other civilizations seem to have known similar types of organization: there were corporations in the medieval Byzantium, guilds in the Muslim world, professional castes in India, and all kinds of brotherhoods in many civilizations. Most guilds place themselves under the spiritual guidance of a patron Saint or some other indisputable moral figure and call themselves fraternities. Guilds and later corporations are spontaneously formed by professionals bound by the same practice, and who: • choose and impose a code of conduct (define the morals); • enhance the profession’s skills and knowledge; • screen and train the next generation (apprentice, companion, master); • regulate the quality of the production; • establish solidarity among its members; • lobby for privileges, speak with one voice; • provide justice and arbitration for the profession; • organize conventions, sponsor charities, etc. Guilds are miniature societies where the individual has duties as well as rights. The fulfillment of duties endows the individual with rights. In keeping with the ages when they were born, guilds are pyramidal organisations that resemble the monarchic or feudal structures of the age. Nevertheless, one should note that guilds are ahead of their times, introducing strong elements of shared power, a more transparent organisation, elective systems, and the accountability of executive officers. They are a driving force toward the future. Guilds are born during the lower Middle Ages we commonly refer to as the Feudal Age. They are city-level organisation. Of course, when solid nations emerge in the late Middle Ages, guilds expand to the national level, although their roots are clearly at the level of cities. For the sake of our thesis, we shall simply observe that the age of guilds in Europe coincides with the time when translators become an important component of cultural life. Gradually, all the major works of the Antiquity are translated from Greek to Latin, Arab to Latin (for example in Toledo, Spain), and from Latin gradually into national languages.  Why did the national languages appear? Around this time, many monarchies appeared in Western Europe, which were the embryos of the future nation-States. When the idea of the Universal Christian Empire collapsed around 919, each nation tried to develop its autonomy and therefore to promote and codify its own vernacular language. The national language soon became a critical symbol of the central power. The monarchic societies started to adopt central administrations which needed treatises and a lot of paperwork. And all universal works had to be translated from Latin, Greek or Hebrew into national languages. Concepts had to be borrowed from around the world and adapted to the national framework. From this viewpoint, the appearance of guilds and the multiplication of translators don’t happen simultaneously by chance. A common underlying factor is man’s desire to be autonomous and self-organized, independently from the Emperor or the Pope. Guilds and translators pursued a similar goal: to make people less dependant on the powerful figures who can, who know, who decide. If people could organize their professions, read books in their own languages, they could become real owners and start to decide what to think, what to say, what to do, without being told from above. To translate was a courageous activity, which wrested the monopoly of meaning from the Clergy and empowered the common man.l The Renaissance The Protestant Reformation, which gave an incredible impetus to translation (Martin Luther is considered a genius in translation and the first great German writer) and Renaissance’s Humanism which gave impetus to corporations and broke the barriers of the feudal order, had a common interest. There is no established translation/interpretation profession, as a distinct, full-time activity in those days, save a few exceptions. Translators and interpreters, in any city or kingdom, are too few, far from reaching the critical mass required to form a guild. If there had been such a guild, it could have taken Saint Jerome (347-420) as its patron. Saint Jerome translated the Bible into Latin. By deliberately translating the Scriptures into a “vulgar” Latin understood by most of the people of his time, and not into the “classical” Latin practised by scholars, he had made a strong moral, if not political, statement, deciding that knowledge should be shared equally by all and not rest in the hands of an elite. He refused to have any meaning twisted to accommodate the theological preferences of the day and unflinchingly carried out his translation work; when difficulty arose, he went to great lengths to seek the real meaning of the texts. In Asia, a similar phenomenon takes place. The vassals of China gradually develop their cultural autonomy and the major Buddhist and Confucianist Scriptures are all translated from Chinese into Korean, Japanese, or Vietnamese. In South East Asia, monarchies appear in Cambodia, Siam, Burma, and Laos. These monarchies, in order to legitimize their power, have to translate major works from the Sanskrit-Pali language base into vernacular languages. 
Abstract This paper objects to the current consensus that machine translation (MT) systems are generally inferior to human translation (HT) in terms of translation quality. In our opinion, this belief is erroneous for many reasons, the both most important being a lack of formalism in comparison methods and a certain supineness to recover from past experience. As a side eﬀect, this paper will provide evidence for a much more favorable judgment of the performance of contemporary MT systems. We will present and discuss known methods of automatic MT evaluation, give real world examples of both machine and human translation and ﬁnally suggest an universal formal evaluation method to handle both human, as well as MT output in a comparable fashion. Keywords: MT, HT, Machine Translation, Human Translation, Evaluation Methods 
Dublin 9, Ireland. sharon.obrien@dcu.ie  1.  Background  In the early 1990s, translation service providers, in particular those who  worked for IT companies, began using translation memory tools in order to meet  the ever-growing demand for translation. Translation Memory tools proved to be  efficient in reducing the amount of re-work involved in translating documentation  and help systems for products that were continually updated by reducing the  amount of re-translation and cutting and pasting that were the norm prior to that  era.  Now it is 2004 and we still hear reports about the ever-growing demand  for translation. According to an IDC survey of the globalisation,  internationalisation and localisation market, the annual growth rate is 16.3%. The  localisation and translation services part of this market is growing at an annual  rate of 14.6% (Van der Meer: 2003). Clearly, this level of growth increases the  need for faster translation throughput. In addition, there is a growing demand for  "gist" translation, or translation for informational purposes only. For example,  members of various EU institutions frequently request gist translations of  documents in order to ascertain whether or not the document ought to be  translated professionally. Users of web sites in languages other than their mother  tongue also avail of gist translation. So, while translation memory tools play an  extremely important part in meeting the world-wide need for translation, demand  for automatic translation by computers (i.e. machine translation - MT) is also on  the increase.  It has been widely acknowledged that when the aim of machine  translation is to produce high-quality translation, then post-editing by human  translators is necessary. The time and effort required for post-editing can be reduced by implementing a number of strategies. For example, previously translated sentences can be leveraged from a translation memory first, thereby reducing the number of words that have to be machine translated and post-edited. Also, machine translation quality can be improved by adding terms to user-specific MT dictionaries. A third strategy is to use controlled language (CL) to improve the translatability of the source text (Mitamura et al. 1998; Nyberg et al. 2003; EAMT/CLAW 2003). This paper focuses on this concept of translatability and on how it correlates with post-editing effort. It draws on research that is ongoing at the time of writing.1 In section 2 the notions of translatability and translatability measurement are presented. In section 3, methods for measuring post-editing effort are explained. Section 4 describes the research methodology in more detail. Section 5 presents data derived from the work-in-progress where the correlations between post-editing and translatability are being explored. A summary and future work will be outlined in Section 6. 2. Source Text Translatability Several researchers list "translatability" as one of the main goals of controlled language (Wojcik and Hoard 1996; Reuther 1998; Means and Godden 1996). To date, five researchers have written specifically on translatability measurement in the domain of CL, i.e. (Gdaniec 1994; Bernth 1999a, 1999b; Bernth & McCord 2000; Underwood and Jongejan 2001; Bernth and Gdaniec 
The present paper adapts the teaching scenario explained then and develops a controlled translation course in the automotive field for senior translation students focusing on the translation needs of the automotive industry, as discussed during the 2002 TOPTEC Symposium and reported by O'Brien (2002). As shown below, three main areas are catered for in the course: effective terminology management, increasing use of hybrid automation models comprising TM and MT, and rising demand of translators specialized in the automotive domain. 1. Introduction Controlled Translation has slowly emerged in the past few years as a new business scenario which features the use of controlled languages, pre-editing, machine translation (MT), translation memory (TM) software, and post-editing processes in order to speed up and standardize the multilingual production of technical documentation. This new scenario is increasingly being adopted by industries such as telecommunications and software localization, and offers an excellent framework that meets the translation needs of the automotive industry. 
This paper presents a preliminary evaluation of a set of Controlled Language (CL) rules selected within the framework of a research project on Machine Translation (MT). The main objective of this project is to improve the machine translatability of an English corpus. This corpus has been obtained from a global Internet security technology company, Symantec. The corpus contains XML alert notifications that are generated from a SQL database. Due to the time constraints that are associated with this new type of communication medium, MT presents itself as a prospective candidate.  1. Introduction Symantec’s localisation department is currently facing the following challenge: finding an alternative way to translate a high volume of security-related contents generated from a SQL database. The traditional translation workflow does not appear to be the most suited to this task for two reasons. First of all, the information generated by the databases is perishable and must be rapidly delivered to worldwide subscribers via alert notifications. Translations should then be obtained as quickly as possible in the desired target languages (Japanese, French and German) so that the received information is not obsolete. Besides, a significant number of updates can be sent to the subscribers, sometimes on the very day of the initial notification. The main challenge is therefore to find a solution that can provide an extremely fast turnaround. This requirement is coupled with economic factors. If the traditional translation workflow were used, the cost of translating ephemeral information would be enormous. Automating the translation process by introducing MT was therefore considered as a prospective solution. A feasibility study is currently in progress in order to identify and understand the  processes that would be part of a future implementation. This paper will describe the preliminary findings that have been made with regard to some of these processes. Section 2 will focus on significant previous initiatives in the field of MT. In sections 3 and 4, the corpus and the test suite that have been designed to test the effectiveness of a selection of CL rules will be presented. In section 5, I will report on the translation results obtained with two commercial MT systems: Logomedia Translate Pro and Systran Premium 4.0, for which User Dictionaries (UDs) have been created. These results will be used to provide preliminary answers to the following two questions: • How significant is the effectiveness of CL rules in terms of post-editing effort? • Which CL rules have the best impact on the MT output? I will also comment on the two evaluation methods that were used to assess the MT output (an automatic evaluation method and a manual evaluation method). The conclusion will provide recommendations with regard to the selection of CL rules. It will also outline the future directions of this study.  
Introduction Over the past five years, VoxTec, a division of Marine Acoustics, Inc., teaming with SRI International, has developed and refined and fielded the Phraselator®, a handheld oneway voice-to-voice phrase-based language translator. The Defense Advanced Research Projects Agency (DARPA) and a DARPA Small Business Innovative Research (SBIR) grant have funded the research and development and initial production. VoxTec has evolved the concept from a PC based system to a handheld device. After the 9/11 terrorist attack on the World Trade Center, Phraselator development was accelerated. Initial deployment of the prototype Phraselators to U. S. Military forces operating in Afghanistan in support of Operation Enduring Freedom began in March, 2002. Lessons learned from Afghanistan were folded into the redesigned Phraselator P2. To date, over 2,000 Phraselators P2s have been manufactured and delivered to users around the world.  The Phraselator is  basically a user  independent  voice  actuated  phrase  matcher. When a user  speaks a known  phrase into the  microphone,  the  automatic  speech  recognizer  (ASR)  matches the input  phrase with a known  phrase in the phrase  database. An output  translated recording in the target language is then played through a speaker. With the  Phraselator, the user is able to provide information, give directions or orders, or ask  questions that have an easily conveyed response. While simple in concept, developing a reliable, robust and flexible system for military users has been the heart of this effort. The Phraselator is the hardware component of the phrase translation system (PTS) consists of the hardware, application and automatic speech recognition (ASR) software, and module management system (MMS). The current Phraselator P2 is a ruggedized, weather resistant device with superior audio input and output. A typical phrase module consists of 100 to 1000 phrases translated and recorded into one or more target languages. Phrases are grouped into categories for purposes of improving accuracy for large modules and for navigating phrase databases. The phrases convey the translated meaning of the input phrase and are not necessarily an exact translation. In fact, a short input phrase may be matched with a long output translated phrase; e.g.: Input: This is a computer translator. Output (translated): This is a computer translator. It translates my words into your words. It does not understand your language. The translated output is a recording of a human speaker. As such, the translation can be male or female, adult or child; be in the correct language and dialect for a particular region; have the appropriate intonation and emphasis; and accurately translate idiomatic or context appropriate phrases. Phrases are initially recorded as WAV files and then converted into MP3 format for Phraselator application. MP3 files require about one tenth the size of a WAV file. In MP3 format, 1,000 recorded phrases require about two to three megabytes of computer storage space. The Phraselator stores the phrases on removable Secure Digital (SD) cards. These cards range in capacity from 64MB to 1+GB. VoxTec's module management system (MMS) consists of: Module Builder™, a toolkit for rapidly building custom phrase modules; a content database of over 15,000 phrases in over 50 different languages; and Lingua Port which allows users to download modules from the database for auto installation onto their Phraselator. This paper will provide an overview of the Phraselator PTS history and development; highlight the capabilities and limitations of phrase-based translation; present the concept of operations, development considerations and technology behind the Phraselator; highlight current operational use by military forces in support of Operation Enduring Freedom and Iraqi Freedom; and present future Phraselator development and fielding in both military and commercial applications. Approaches to Speech Machine Translation Ideally, we all would like a machine that you can speak anything you want into it, and then it automatically speaks translation in another language. While this approach may  not be practical for reasons discussed below, an alternative practical, albeit limited, approach is the phrase translation approach. Machine Translation Approach Over the years text-to-text machine translation (MT) has been developed and are available. While not perfect, theses programs have been found useful for both the casual user and as an aid for professional translators. MT performance tends to degrade with sentence complexity, technical jargon, and idiomatic expressions. For true speech-tospeech machine translation (SMT) obtaining an accurate translation is compounded. For SMT the general steps are: • Human speech in the source language (SL) input via a microphone • Analog to digital conversion of the audio input • Acoustical model phonetically matches words with the sound of what you said. • Language model guesses the most likely word based on context and frequency. • Speech engine combines the acoustical model and language model information to guess the words and produce the SL text output. • MT engine translates SL text to target language (TL) text. • Text-to-speech (TTS) engine produces synthesized speech out. This process is reversed for translation from the TL back to the SL. While this approach is a long-range, desirous approach, it has several limitations: • Works for simple, non-complex phrases; the more complicated the phrases, the less effective the translation, • Speech-to-text (STT) and TTS engines support a limited number of languages and extension to new languages is difficult. • MT engines support a limited number of generally commercially viable languages. • Requires high quality speech audio input. • User needs to train the speech engine to his or her voice • Requires powerful processor and large amount of memory. While there are prototype systems using this approach, they are generally considered to not be reliable or robust enough for practical use at this time for true two-way voice machine translation and have the limitations stated above. Phrase Translation Approach A more reliable and robust approach to SMT is a phrase-based translation system. This is, in reality, not a translator; but a speech actuated phrase matcher. When a known phrase in the SL is spoken into the device it is matched with a recorded phrase in the TL and played through a speaker. For this approach, there two methods for matching the SL audio input with the output recording. The first is a system using analog pattern matching. This approach requires the user to speak and record every phrase in the phrase database prior to using the device. When the  using the device, the user speaks the phrase and it matches the analog voice pattern with one of the recorded patterns and then plays the appropriate translation recording. This is simple and requires little processing power. It is, however user dependent and requires the user to say the phrase exactly as he recorded it.  The second approach, used by the Phraselator, is to use the acoustical model and language model from the automatic speech recognizer (ASR) engine as described above and match the phonetic content of the input with the phonetic content of one of the phrases in the phrase list. This approach is generally user independent, allows for inexact input, and can be modified easily. It does, however, require more processing power.  Performance Progress  Over the past years, there have significant improvements in the overall performance of  the Phraselator PTS as compared to its predecessors and can be evaluated in a number of  areas. The below graph highlights the performance progress.  • Reliability and robustness: No hard crashes or lockups; fail soft and recover;  absolute control of command and phrase modes; operation in noisy or vocal  environment  • Phrase  translation  accuracy: 95% phrase  translation accuracy;  rapid  translation;  alternate phrasing  capability; ability to  translate  inexact  phrasing.  Early  versions of the PTS  software has translation  accuracies that were  often less than 80%.  This was particularly  true in noisy  environments.  • Ease of use and  training: New user  trained in less than one  hour; minimal number  of commands and  functions.  Early  versions of the PTS  were not particularly  intuitive of easy to use  by the untrained person.  • Speaker independence: capable of being used by different users with no or little  registration. The PC based MLT ran Dragon Dictate and Dragon Naturally  Speaking for speech recognition. The user needed to train the system for his voice. The current Phraselator is user independent and can be used by male or female with minimal adaptation (two to three phrases). • Rapid module build capability: module build in less than two weeks; automated phrase addition in the field. Early module were hand crafted. It could take up to two months to build a module. Words not in the dictionary needed to be built by an engineer. Using Module Builder, a module can now be built in a day. • Voice/audio interface: Closed notebook operation; all commands or prompts via audio or voice. Using earlier PC versions of the PTS required the user to use the graphic user interface when using it. The Phraselator now permits eyes free and hands free operation. • Ease of software installation and setup: automated installation of speech recognition and application software and modules in less than 15 minutes. Since early versions of the PTS ran the commercial Dragon speech-to-text software on a PC, loading the software and calibrating it for individual use took time. Phraselator software and modules load in less than a minute. • Software size, hardware requirements and portability: Can software be ported to systems with limited processing power and storage? The initial application software, speech-to-text software, and phrase modules could require 500MB of storage on a PC. The same size module on the handheld Phraselator requires less than 30MB. Phrase Translation System (PTS) Components The Phraselator is basically a user independent voice actuated phrase matcher. When a user speaks a known phrase into the microphone, the automatic speech recognizer (ASR) matches the input phrase with a known phrase in the phrase database. An output translated recording in the target language is then played through a speaker. With the Phraselator, the user is able to provide information, give directions or orders, or ask questions that have an easily conveyed response. While simple in concept, the key is the development of a reliable, robust, integrated and flexible system. Components of the Phraselator are discussed below. Phraselator P2 Hardware Leveraging off of the success of the Phraselator model 1100, the P2 was developed. It is in essence, a ruggedized PDA design optimized for audio performance and field use. It consists of a base unit which houses the display and core  electronics, and an audio plug with microphone, speaker, and audio circuitry. The general specifications are: • WinCE.NET 4.2 OS • Intel® 400 MHz XScale PXA 255 Processor • 64 MB Flash RAM on-board (256 MB max) • SD/MMC slot • Class D digital amplified two-watt speaker • Daylight and night readable TFT color touch screen • Battery charging/power management • Long-lasting lithium-polymer battery • AA battery capable • Audio in and out jacks • Standard mini USB connector Design Considerations Since the P2 was developed with military field use in mind there are a number of design considerations which may not be applicable to a device for commercial or consumer use. Audio Performance The quality of the audio input will have a major impact on recognition accuracy and translation speed. Particular attention must be paid to the microphone type and quality, microphone directionality, microphone enclosure, audio circuitry, button noise, and electronic noise. Size and Weight vs Ruggedness and Weather Resistance For field use the device needs to withstand shock and vibration and environmental effects such as rain, dust, and heat. The size and weight of the device are directly coupled to these. Battery Life For military operations battery life is a major consideration. In the Phraselator design, particular attention was paid to maximizing the energy capacity of the battery, selecting a low power processor, and optimizing power management. Also, the Phraselator accepts AA batteries. Modularity As mentioned, the P2 has a rugged base unit (RBU) and an audio plug. The modular design was selected to allow for a future upgrade path for microphone input and speaker design; and to allow other plug in devices such as GPS devices, communications plugs, and card readers.  Application Software When the user presses the push-to-talk (PTT) button and says one of the phrases in the phrase database, the client application receives voice commands and spoken phrases and plays back the appropriate translation after the user selects from a list of best-guesses. The application is configured by loading the language and phrase module(s) from a remote connection (either via serial port or modem) onto a removable SD card. The client application program itself can also be updated in the field via a telephone connection to the device. The application runs in less than 8MB. When the application is loaded, the user may select from one of the modules available on the SD card. Each phrase module consists of: (1) the recognition grammars, vocabularies, and pronunciation dictionaries to configure the recognizer for a complete task, including the commandand-control phrases, (2) the output prompt waveforms for playback in any number of languages, and (3) any additional information or data to make the client application work properly in the field for a particular application. For improved recognition accuracy or translation speed for large modules the user may select a category. Typically, modules of up to 500 phrases translate at near real time. As the number of phrases approaches 1,000 there is a noticeable delay in translation speed on the order of from one to two seconds, particularly if the audio input is poor or there is high vocal background noise. Also, as the phrase database increases in size, there is a more likely possibility that the Phraselator will have do discriminate between similar sounding phrases or words. When a category is selected, only the phrases in that category will be recognized, plus the phrases in the “basic phrases” category. Other features that enhance the overall usability and functionality include; • Audio or text verification of phrase prior to playing the recording. • Ability to switch languages via vocal commands or touchscreen. • Ability to switch modules via vocal commands or touchscreen. • Record capability that allows the user to record a response in the TL so that it can be played and translated at a later time. • Search on word feature. The user speaks a word into the Phraselator and all phrases with that particular word are displayed. The user may then play one of the phrases.  ASR SRI’s DynaSpeak Speech Recognition Engine features run-time configurable grammars and vocabularies, and state-of-the-art speaker-independent continuous-speech performance. DynaSpeak was scaled down and ported to the Phraselator. The DynaSpeak ASR engine uses up to 10 MB of DRAM to allow for higher-accuracy acoustic models and a large dictionary of over 10,000 words. Furthermore, DynaSpeak contains adaptation algorithms that improve recognition performance for speakers using the system for more than several minutes at a time. The engine recognizes American English input speech with grammars containing up to 5,000 unique phrases. Content or Modules A typical phrase module consists of 100 to 1000 phrases translated and recorded into one or more target languages. Phrases are grouped into categories for purposes of improving accuracy for large modules and for navigating phrase databases. The phrases convey the translated meaning of the input phrase and are not necessarily an exact translation. In fact, a short input phrase may be matched with a long output translated phrase. The modules are stored on SD cards in MP3 compressed audio format. 1,000 recorded phrases requires about 5MB of SD card storage. Thus, a typical 128MB SD card can store 20,000 or more recordings. Modules may be obtained in one of three ways: Existing modules may be obtained from VoxTec directly or via the web portal; VoxTec can build custom modules; or the user can build his own module using Module Builder software. Modules may be developed by subject matter experts (SME) in conjunction with VoxTec personnel. VoxTec works with the SMEs to develop phrases and then builds the modules with translations in the desired languages. VoxTec uses contract language translation support to build the modules. Alternatively, using the Module Builder software, the users can build their own modules. VoxTec’s module management system (MMS) consists of: Module Builder™, a toolkit for rapidly building custom phrase modules; a content database of over 15,000 phrases in over 50 different languages; and Lingua Port which allows users to download modules from the database to a PC for auto installation onto their Phraselator. Module Builder runs on a Win/Intel computer and greatly simplifies the process of building a custom module. There are two versions of Module Builder: a version for professional users, and a version for use in the field. Using Module Builder, the basic steps are: • Develop phrase list • Type or import list into Module Builder • Build words not in the ASR dictionary using a user friendly tool (see figure below).  • Do voice recordings for English and target languages. • Compile module. • Transfer module to SD card. When VoxTec builds a module it is very important that the translation conveys the correct meaning in the particular domain with the appropriate voice inflections. After the phrase list is developed, it is provided to a language translation service for text translation into the desired languages. Often it is necessary to specify the nature of the module, specific dialects or domains such as medical, maritime law enforcement; male to male, male to female, elder to junior translations; aggressive or passive translations. When doing the voice over recordings of the translations, the speaker has to act out the phrases as he or she records them. After the recordings are completed, they may be  played back to another speaker of that language to verify the meaning of the translation in context. Training and Support As the Phraselator PTS transitions from a research and development program to a fielded system, necessary training and support must be provided. VoxTec has developed training materials and provided training around the world. Development and Fielding There is a large potential market for the Phraselator: business and tourist travelers, U. S. Military forces overseas, flight attendants, law enforcement, humanitarian and medical assistance, and others. Users Phraselator users may be broadly classified as casual, professional, or dedicated.  Casual A tourist or traveler would use the Phraselator much like a traveler's phrasebook available from Fodors or Berlitz. For this user the Phraselator needs to be affordable, small, reliable, and easy to operate. The envisioned Phraselator would be packaged as a consumer item with about 2000 phrases translated into one to four languages. The user would be able add a limited number of additional phrases on the fly via audio prompts. Additional modules or translations would be available on flash cards or downloads. Professional Law enforcement, military, medical, travel industry, humanitarian and other professionals need a customized PTS. These users would need mission or job related customized modules in multiple languages. Also they would need the flexibility to change or add to the modules and add additional languages. For the professional user, the Phraselator hardware would be basically the same and VoxTec would market a custom module build and translation service. Additionally, VoxTec would provide on-site training and module development services. Dedicated Professional users who want total control of the module development and translations would use the basic Phraselator hardware and software. For this user the PTS module build and database management software toolkit would be marketed separately. Market Segments The market can be divided into the following segments. Travel Industry The Phraselator provides a means of asking questions at check-in, providing general information, and providing emergency to non-English speaking travelers. This is a medium size specialized market. Potential travel industry users include: • Aviation flight crews can use the Phraselator for providing safety and emergency information to passengers in a large number of languages. • Airline counter agents can use the Phraselator for asking check-in questions, for aiding passengers, and for providing general information. Homeland Security This is a medium sized market consisting of both professional and dedicated users. The initial potential market is in excess of 30,000 users that include: • Police and fire departments • Fire departments • Customs inspectors and border patrols • Coast Guard inspections and safety Humanitarian Assistance This is a small, specialized market. Users include humanitarian assistance teams, and medical teams.  Medical and Dental offices This market is potentially large. With populations shifts in the U. S. medical and dental professionals have to diagnose and treat non-English speaking patients. Military Military users make up another small, specialized market. Typical applications include peacekeeping operations, foreign military training, and ship boardings and inspections.  Tourist By far the largest potential market is the international tourist and business traveler going. Global spending on travel and tourism has more than doubled over the last decade as the standard of living for most people in the world has risen and more countries have become accessible to tourists.  Global spending on tourism exceeds $500 billion. Each year over 20 million Americans travel abroad spending in excess of $50 billion; and over 50 million visitors come to the U. S. spending over $100 million.  Market Size and Trends While the potential market size is large it is difficult to estimate. Preliminary surveys indicate broad and considerable interest in such a product such as the Phraselator. Below is a preliminary estimate of the market size.  Estimated PotentialMarket Size  Size  % users Total users  Business and Professional  Flight Attendants  132,000  5%  6,600  Airline Check-in counters  166,000  5%  8,300  Government and Law Enforcement  Police departments  704,000  5%  35,200  Fire departments  293,000  2%  5,860  Customs inspectors, Immigration  5,000  20%  1,000  Coast Guard inspections and safety  2,000  50%  1,000  Humanitarian Assistance and Medical  Disaster relief  5,000  20%  1,000  Medical diagnostics  2,000  50%  1,000  Medical and Dental  Physicians, PAs and nurses  2,500,000  1%  25,000  Dentists and hygienists  300,000  1%  3,000  Military  Peacekeeping operations  5,000  10%  500  Training  2,000  25%  500  Ship boardings and inspections  100  50%  50  Consumer  Tourist  20,000,000  1%  200,000  Language practice and training  2,000,000  
In this paper we introduce a technique for creating searchable translation memories. Linear B’s searchable translation memories allow a translator to type in a phrase and retrieve a ranked list of possible translations for that phrase, which is ordered based on the likelihood of the translations. The searchable translation memories use translation models similar to those used in statistical machine translation. In this paper we ﬁrst describe the technical details of how the TMs are indexed and how translations are assigned probabilities, and then evaluate a searchable TM using precision and recall metrics. 
Background The motivation for this contribution arose from the widening scope of work at our Language Service. Situated on the borders with Belgium and the Netherlands, the Research Centre is the largest interdisciplinary research establishment in Europe with a staff of 4300, including nearly 1300 research scientists. The main fields of research can be summarized under the five headings of matter, energy, information, life and environment.1 Our work mainly comprises the translation of scientific papers from German into English for publication in learned journals and, also increasingly, the linguistic revision and polishing of such articles. Although we are a German national research institution, and a member of the Helmholtz Association of National Research Centres, the working language of our scientific endeavours is overwhelmingly English. Translations into German are rarely required. In the course of the 
Keywords: CAT tools, CAT tools problems, Central and Eastern Europe, Central and Eastern European languages, Slavonic languages, Translation into Slavonic languages. This lecture has been prepared based upon Lido-Lang Technical Translations' experience in the use of CAT tools. Our office has been working with Trados since 2001. In 2002 we started using SDLX as a second CAT tool. Less frequently we deal with DejaVu, Transit and/or WordFast. I. Analysis of the present knowledge and usage situation of CAT tools in Eastern European countries. Unfortunately, no official statistics data is available with regard to the application of CAT tools in the translation industry. Therefore, our observations are rather subjective. However, we are convinced that our conclusions present the reality quite well. Translators As all Language Service Providers, we constantly receive the applications of freelancers from different countries, among them from Central and Eastern Europe - let's focus on these. One of the basic questions asked in our initial questionnaire refers to the experience in Computer-Aided Translation tools. Having analysed the responses obtained, we can formulate the following general conclusions: 1. Ca. 75% of responses are negative - candidates have very limited knowledge about CAT tools or they have heard about Trados (or CAT tools in general) but they do not use it yet. 2. Ca. 15% of candidates use Trados or Wordfast 3. Ca. 10% of candidates use Trados and SDLX and/or other tools So the percentage of negative responses is relatively very high. There are two most frequently mentioned reasons for that: a) Costs of CAT tools - average salaries in developed countries are much higher than in the 10 "new" EU member-countries, therefore prices of CAT tools are seen as very high in Central/Eastern Europe. Even if a translator has heard about CAT tools, he/she cannot afford to buy the licence. b) Confusion of CAT tools with automatic translating machines. Translators do not believe automatic translation can be of any help and are proud to say that they do everything themselves (!).  A. Nedoma, J. Nedoma:  Problems with CAT tools ...  page 2/9  Translation Agencies Among the translation agencies (in the "new" EU member-countries) that we have visited personally or contacted in another way, the percentage of offices familiar with Trados, SDLX, DejaVu, etc. is less than 20%. A completely different picture arises from the Internet survey - among the translation companies active in the Internet, the percentage of CAT tools users is ca. 60-70%. However, more detailed discussion with managers of such agencies shows that in many cases the managing staff does not have any knowledge about CAT tools. These managers simply know that e.g. Trados exists and that certain of "their" freelancers can work with Trados. It means that the company itself cannot add any value to the product delivered by the translator. Some agencies simply act as a "contact box" transmitting messages and forwarding files between the translator and the customer. Such a situation obliges us first to convince our freelancers of the merits of CAT tools and then to train them, to make them efficient in working with the new tools. On the one hand it costs us time and money. But on the other hand, such a policy creates a closer relationship between our company and "our" freelancers and gives us an opportunity to prepare our suppliers for more efficient work in state-of-the-art technologies.  II. Why CAT tools cause problems?  This chapter may - to some extent - seem very evident to a part of the audience. However, all the problems discussed below appeared while processing real projects for our Western European customers. Real questions asked by our clients and our explanations related to many aspects of processing projects convinced us that those problems are still not evident nor generally obvious. They still cause troubles to many of us - Language Service Providers. We all are already well aware of the fact that CAT tools became a MUST in the translation industry. Not only do these tools raise the efficiency and quality of work, but they also enable considerable reductions in project costs. Nevertheless, the application of CAT tools is linked with many problems that everyone should be aware of, before acceptance and during the entire process of performance of translation projects. We have classified these problems into four groups: • alphabetical problems • grammatical problems • linguistic problems • technical problems  1) Alphabetical problems "...please do not send Bulgarian in Cyrillic, I need it in Latin alphabet" - recently asked seriously by our respected customer. It is very important to make our clients aware that the languages of Central and Eastern Europe have many special characters in the "common" Latin alphabet. Several languages use non-Latin alphabets. And also in the "common" Cyrillic alphabet, there exist special  A. Nedoma, J. Nedoma: _____________ Problems with CAT tools ... __________________________ page 3/9 characters used in some Cyrillic-written languages. Therefore, it must be stated at once whether a client's requirements regarding alphabet are logical or not. Knowledge of alphabets and their special characters is also very important during the post-translation processing of files, to check that the special characters were not corrupted. Table 1 presents a list of languages with indications about their used alphabet(s) and special characters. 
1. Introduction It is estimated that worldwide there are 6000 to 7000 languages, with these disappearing at an alarming rate so that only some 600 of these can be viewed a ‘safe’ and likely to survive (Nettle and Romaine 2000). Even major European languages like Swedish and Danish are now fearful of survival (Allwood 2004). The response has been to raise projects to document these languages before they disappear altogether, for example the Endangered Languages Documentation Programme (ELDP) funded by the Lisbet Rausing Charitable Fund at the School of Oriental and African Studies, University of London (SOAS), or the projects funded by the Volkswagen Stiftung in Germany. Diversity of languages is seen as precious to humanity– the languages embed within them particular conceptualisations of the world and knowledge that we should not lose. This documentation and preservation view of languages must be contrasted with language activism which seeks to revitalise languages as human resources in active use and intimately associated with the identity of people (Eisenlohr, 2004). Technology such as broadcast television has been  ASLIB 2004  Hall – Localising Nations.  seen as a major cause of language extinction, and equally well technology is seen as a major instrument in language revitalisation. In this paper we will explore what technology is required to revitalise a language We will look in depth at the situation in South Asia, in particular focussing on Nepal as a case study, concluding that not only must there be the technology available to support the writing system for the language, as characterised by Unicode, but that the whole language itself must be supported through language engineering resources as would be seen as normal for European languages. 2. Languages and their Power We will start by looking at the languages of South Asia and how some of those languages dominate others. South Asia has well over 1 billion inhabitants speaking between 500 and 1000 distinct languages, falling into three major language groups with a few languages from other language groups. The largest is the Indo-European group to which most European languages also belong, and covers the major languages of Bangladesh, northern India, Pakistan, Afghanistan, as well as Nepal. To the south are the Dravidian languages, the best known of these being Tamil, an official language not just of India but also of Sri Lanka and Singapore. Sinhala, the language of the rest of Sri Lanka, is usually classified as Indo-European, but with a strong Dravidian influence from its long contact with Tamil. The remaining languages in the north of the region, in the Himalayas, are Tibeto-Burmese, which includes Dzongka the official language of Bhutan and covers most of the languages of South East Asia as well.  Pushtu  Kasmiri  Indo-European Urdu  Tibetan Tibeto-Burmese  Punjabi  Nepali Dzongka  SindhIi ndo-EHuinrodpi eanBangAlsasamese  Gujurati  Tibeto-  Oriya  Marathi  Burmese  Telugu  Kannada 
The Institute of Localisation Professionals (TILP) was established in 2002 as a non-profit organisation and in 2003 merged with the US-based Professional Association for Localization (PAL). TILP’s objective is to develop professional practices in localisation globally. TILP is owned by its individual members. It coordinates a number of regional chapters in Europe, North America, Latin America and Asia. The Certified Localisation Professional Programme (CLP) was launched by TILP in September 2004 and provides professional certification to individuals working in a variety of professions in localisation, among them project managers, engineers, testers, internationalisation specialists, and linguists. This article will outline the CLP programme and is aimed at course providers interested in offering TILP accredited courses, employers planning to make CLP certification a requirement for future employees, and individual professionals planning to develop their professional career. Background The localisation industry emerged in the mid nineteen eighties in Europe as the provider of services to US-based digital content developers who wanted to sell into Europe and, therefore, had to linguistically and culturally adapt their products to the requirements of the emerging European markets. Companies like Wordperfect and Lotus Development were among the first IT companies to set up large-scale localisation operations in Ireland, which soon responded with the establishment of a large number of dedicated service providers. Softrans, which later became Softrans-Berlitz, then Berlitz and, more recently, Bowne Global Solutions, set the trend. Its success encouraged many, more traditional translation agencies to follow and venture into the world of high-tech digital content adaptation. From the very beginning, the individuals driving the development of this industry were business people teaming up with translation and software engineering professionals. When they looked for employees for their newly established businesses, they could not rely on a pool of well-trained and educated localisers. There were no courses where potential localisers could train and acquire the necessary skills. Training took place largely on the job. Consequently, employees were recruited mainly from among translation professionals.  In the early days of localisation, at least some degree of language and translation skills were a must; an interest in computing was a requirement; engineering expertise was an added bonus. Surprisingly, it took the best part of ten years for professional training providers to realise that they were losing out on a real opportunity. It was not until the mid-nineties that some companies, like Softrans-Berlitz, ETP, the Localisation Institute, and a number of third-level institutes, like Austin Community College, the University of Washington and the University of Limerick, developed course programmes responding directly to the needs of the localisation industry. Today, the education and training of future localisation professionals takes place mainly in colleges and universities. While there are commercial course providers, these focus on shortterm, highly focused and costly training sessions rather than on extended and affordable course programmes. A rapidly growing number of these third-level institutes teaching localisation-related courses are now organised in the Localisation Training, Teaching and Research Network (LttN) operating under the umbrella of The Institute of Localisation Professionals (TILP). The latest addition to the localisation course scenario are eLearning courses, offered by a number of providers, the most successful probably being the courses now offered by the Localisation Research Centre (LRC) at the University of Limerick (www.localisation.ie). When efforts were initiated by the LRC in 1998 to bring experts from the localisation industry and the educational arena together to define the Certified Localisation Professional (CLP) programme, there was much scepticism. How could skills, training and certification be defined in an industry where the only constant is change, where each new project is different from the previous one, and where each employer has different job specifications and skills requirements? Benefits for the stakeholders The initial CLP project was developed at the end of the nineteen nineties in an EU-funded project coordinated by the LRC. Despite their initial scepticism, nineteen IT companies1, six training providers2 and six universities3 worked together over a period of two years on CLP which was coordinated by the LRC4 and funded by the European Union’s ADAPT initiative The main objectives of the CLP project were: 
Natural language expressions are underspecified and require enrichment to develop into full fledged propositions. Their sense-general semantics must be complemented with pragmatic inferences that have to be systematically figured out and pinned down in a principled way, so as to make them suitable inputs for NLP algorithms. This paper deals with the underspecified ipf1 aspect in Russian and introduces a semantic and pragmatic framework that might serve as the basis for a rule-guided derivation of its different readings.
This paper describes a specific semantic property underlying binary dependencies: co-composition. We propose a more general definition than that given by Pustejovsky, what we call {``}optional co-composition{''}. The aim of the paper is to explore the benefits of optional cocomposition in two disambiguation tasks: both word sense and structural disambiguation. Concerning the second task, some experiments were performed on large corpora.
This article describes two practical applications of weighted multi-tape automata (WMTAs) in Natural Language Processing, that demonstrate the augmented descriptive power of WMTAs compared to weighted 1-tape and 2-tape automata. The two examples concern the preservation of intermediate results in transduction cascades and the search for similar words in two languages. As a basis for these applications, the article proposes a number of operations on WMTAs. Among others, it (re-)defines multi-tape intersection, where a number of tapes of one WMTA are intersected with the same number of tapes of another WMTA. In the proposed approach, multi-tape intersection is not an atomic operation but rather a sequence of more elementary ones, which facilitates its implementation.
Tree-based data structures are commonly used by computational linguists for the documentation and analysis of morphological and syntactic data. In this paper we apply such structures to phonological data and demonstrate how such representations can have practical and beneficial applications in computational lexicography. To this end, we describe three integrated modules: the first defines a multilingual feature set within a tree-based structure using XML; the second module traverses this tree and generalises over the data contained within it, optimising the phonological data and highlighting feature implications. The third uses the information contained within the tree representation as a knowledge base for the generation of multiple feature-based syllable lexica.
Named entities and more generally Multiword Lexical Units (MWUs) are important for various applications. However, language independent methods for automatically extracting MWUs do not provide us with clean data. So, in this paper we propose a method for selecting possible named entities from automatically extracted MWUs, and later, a statistics-based language independent unsupervised approach is applied to possible named entities in order to cluster them according to their type. Statistical features used by our clustering process are described and motivated. The Model-Based Clustering Analysis (MBCA) software enabled us to obtain different clusters for proposed named entities. The method was applied to Bulgarian and English. For some clusters, precision is very high; other clusters still need further refinement. Based on the obtained clusters, it is also possible to classify new possible named entities.
The existence of a Dictionary in electronic form for Modern Greek (MG) is mandatory if one is to process MG at the morphological and syntactic levels since MG is a highly inflectional language with marked stress and a spelling system with many characteristics carried over from Ancient Greek. Moreover, such a tool becomes necessary if one is to create efficient and sophisticated NLP applications with substantial linguistic backing and coverage. The present paper will focus on the deployment of such an electronic dictionary for Modern Greek, which was built in two phases: first it was constructed to be the basis for a spelling correction schema and then it was reconstructed in order to become the platform for the deployment of a wider spectrum of NLP tools.
Tree Adjoining Grammars (TAG) are known not to be powerful enough to deal with scrambling in free word order languages. The TAG-variants proposed so far in order to account for scrambling are not entirely satisfying. Therefore, an alternative extension of TAG is introduced based on the notion of node sharing. Considering data from German and Korean, it is shown that this TAG-extension can adequately analyse scrambling data, also in combination with extraposition and topicalization.
It is important to evaluate Spoken Dialogue Translation Systems, but as we show by analyzing evaluation methods in the Verbmobil, C-STAR II, and the Nespole! projects, the current state of the art is not fully satisfactory. Subjective methods are too costly, and objective methods, although cheaper, don’t give good indications about usability. We propose some ideas to improve that situation. 1. Introduction MT evaluation is a hot topic since 1960 or so, it may have several goals [3]. Speech-to-speech translation has been an active research field since 1986. We focus here on spoken dialogue translation, started with the C-STAR I project (1990-93), and its evaluation, which has become an important issue. In the automatic text translation community, first evaluation techniques proposed were subjective and relied on human judgment about the translation quality [9]. In the subjective evaluation setting, an candidate target translation is compared with the original source utterance or a handcrafted translation reference. The main practical problem with subjective evaluation is that it is a time consuming task. Hence, the text translation community moved towards automatic, objective, evaluation techniques in order to overcome this problem. In the objective evaluation setting, system outputs are compared with an handcrafted translation reference and several paraphrases. However, these techniques are not as useful as expected t o judge quality and seem useless to judge usability. In this paper, we try to find better ways to evaluate Spoken Dialogue Translation Systems. For this, we analyze evaluations conducted within three projects: Verbmobil, C-STAR II and Nespole!. We also give an overview of the current objective evaluation metrics used by the community. We report and comment, as well, the first C-STAR III pilot evaluation that used those objective metrics on the BTEC corpus. In the last section of this paper we comment some limits of the current evaluation paradigms, and, in order to go further, we make some proposals. 2. Current SDTS evaluation methods 2.1. Evaluation within Verbmobil The Verbmobil system [27] is a large demonstrator providing English, German and Japanese mobile phone users with simultaneous dialog interpretation services for  appointment scheduling, travel planning and remote PC maintenance.  2.1.1. Data and protocol In 1999 [23], potential users with English and German as mother tongue were put in a realistic end-to-end situation about negotiating an appointment. A supervisor listened to the conversation and solved all technical problems. The users and the supervisor had each to fill a form describing their (un)successful interactions on all the topics (13 topics were proposed) they touched on. 45 dialogues were collected and transcribed to provide references for the evaluation. The forms were used to compute a dialogue success rate. For the linguistic evaluation both monolingual and bilingual data were considered. Monolingual inputs were evaluated according to their syntactic and semantic correctness, and possible misunderstandings. For the translations themselves, – mismatch (yes/no), – soundness (yes/no), and – quality (good, intermediate, poor) were evaluated.  2.1.2. Results  The average percentage of successful task completion reached 86.8%. If the results are weighted by the frequency of the attempted tasks in the dialogues, the success rate reaches 89.6%. As far as the quality of the translation is concerned, results have shown that the quality is more sensitive to the insertion of information elements than to their deletion. Thus, if at least 50% of information elements are preserved in translation, scores over “poor”. On the opposite, an insertion of more than 20% of information elements results in “poor” scoring over “good+intermediate”. In 2000 [23], a mass evaluation was conducted using 5069 German and 4136 English input turns. The percentage of ‘approximately correct’ translations was mapped against the word accuracy rate in speech recognition. The term ‘manual selection’ refers to a manual selection of the best translation produced by the different translation engines implemented in the system.  Word Accuracy Rate >50% >75% >80%  GER-ENG # of turns Automatic selection Manual selection  5069 57 88  3267 66 95  2723 68 97  ENG-GER # of turns Automatic selection Manual selection  4136 53 86  3254 58 92  2291 60 94  Table 1: results of the Verbmobil mass evaluation  95 1/8  2.1.3. Comments The evaluation conduced in 1999 gives a precise idea of the translation quality according to speech recognition errors, insertions, and deletions. Using a less fine-grained approach, the 2000 evaluation has also confirmed that, the better the speech recognition hypothesis the better the translation. We will see in section 3 that the NESPOLE! project reached almost the same scores for the same recognition qualities. The excellent results in manual selection announced great potentialities of the system. 2.2. Evaluation within C-STAR II The C-STAR II systems [2] provided English, German, Italian, French, Korean and Japanese videoconference users with simultaneous dialog interpretation services for the tourism domain (transportation and accommodation booking, and sightseeing). Both users were able to play a customer or a tourist agent. Evaluation was not a key point of the project. However, ATR conducted an original and interesting evaluation [21]. 2.2.1. Data and protocol The test set consists of 330 Japanese turns extracted from 23 dialogues. Manual transcriptions are used as references. The goal is to compare human translations (from Japanese to English) with the translations produced by the system. Knowing the TOEIC scores of the Japanese translators, the TOEIC score of the system is measured. Japanese subjects were asked to produce a written translation from each spoken turn. Evaluation sheets were produced with the Japanese transcription and, in random order, the system and human English translations. Native English speakers able to understand written Japanese were asked to evaluate the translations quality, using a 4-point scale (perfect, correct, acceptable, un-understandable), and to select the best one among them. 2.2.2. Results Evaluation results show that the system performs better than human translators with a TOEIC score between 300 and 400 and performs worse than humans with a score equal to 800. A regression analysis confirmed that the system TOEIC score is 707.6. The same experiment was conducted using the speech recognition hypothesis as the Japanese sentence. In this latter case, the TOEIC score of the system was 548 (a 150 points loss). For the same system, an automatic (objective) evaluation was conducted using a set of English paraphrases (14.4) for each Japanese turn. The TOEIC scores computed, using a regression method, were 682.9 for the references and 547.3 for the hypothesis. 2.2.3. Comments ATR subjective evaluation proved a strong correlation between a costly subjective evaluation and a cheaper objective evaluation both based on the TOEIC scale. The problem is that the TOEIC scoring is not directly related with the usefulness of a SDTS. 3. Evaluation within NESPOLE! The NESPOLE! system [10] provides English, German, French clients and Italian tourist agents with simultaneous dialog interpretation services for the  tourism domain over Internet. Two showcases were evaluated en 2001 and 2002 in order to evaluate the performances of the demonstrators and the progress accomplished. The lingware architecture used within the project is a pivot-based approach. Our pivot, called IF (Interchange Format), is based on domain actions (DAs) that consist of a speech act and concepts. In addition to the DA, an IF representation contains arguments. When analyzing a speech turn, several IFs can be produced to represent roughly each sentence. Each segment of a turn mapped to a unique IF is called an SDU (Semantic Dialogue Unit). 3.1. 2001’s showcase-1 on “restricted tourism” 3.1.1. Data and protocol Four dialogues (2 for summer vacations and 2 for winter vacations) were randomly picked-up from the first NESPOLE! data collection [5] for each language. For Italian, tourist agent turns were used. For English, French and German, client turns were used. Evaluation was done at two levels: (1) Hypos, which are the automatic transcriptions produced by the Automatic Speech Recognition (ASR) modules, and (2) Refs, which are the manual transcriptions of the same speech signals. Each turn was also manually split into Semantic Dialogue Units (SDU) in order to get a SDU-based (and not a turnbased) evaluation of the translation quality. ASR modules were evaluated using the Word Accuracy Rate (WAR) score. However, WAR does not allow to measure precisely how speech recognition errors influence translation quality. We also graded the Hypos as paraphrases of the Refs, at the SDU level, to measure the loss of semantic information due to recognition errors. We performed monolingual evaluation (where the generated output language was the same as the input language), as well as crosslingual evaluations. For crosslingual evaluations, translation from English German and French to Italian was evaluated on client utterances, and translation from Italian to each of the three languages was evaluated on agent utterances. For each set, we used three human graders with bilingual abilities. Each SDU was graded as either “Perfect" (the meaning is translated correctly and output is fluent), “OK" (the meaning is translated almost correctly but output may be disfluent), or “Bad" (the meaning is not properly translated). We calculated the percentage of SDUs in each of these three categories. “Perfect" and “OK" were also merged into a larger category of “Acceptable" translations. Average percentages were calculated for each dialogue, each grader, and separately for client and agent utterances. Combined averages for all graders and for all dialogues were then computed for each language pair. 3.1.2. Results Table 2 combines all the results (in %) for acceptable translations using average score. Majority is reported when computed. 3.1.3. Comments Performances of the ASR modules for producing Hypos a s paraphrases are almost the same regardless the WAR. The results indicate acceptable monolingual translations (clients and agent turns) in a range of 40-48%. of SDUs o n Hypos. On Refs, the scores are, not surprisingly, better (46-  96 2/8  61%). For crosslingual translation towards Italian (on clients turns only), there is a performance drop (higher o n Refs than on Hypos) compared with the monolingual systems. It shows that either the Italian generator does not handle properly some IFs produced by the French, English and German analyzers (problem of coverage) or that there in an intercoder agreement problem across sites. The same problem occurs for crosslingual translation from Italian (on agent turns only). The performance drop is higher than on client turns. The same reasons explain the phenomenon. However, the problem of coverage is probably dominant in this case. For the French generator, we could indeed check that the latter is true. As references simulate a 100% speech recognition success rate, the translation scores on R e f s for the four monolingual end-to-end systems must be considered as upper bounds for the scores on hypothesis. However, we found out that the behaviour of the systems is not a linear function of the hypothesis as paraphrase rate. If it had been the case, for French, the percentage of acceptable translations on Hypos would have been 35% for 65% of Hypos as paraphrases. The actual score (41%) is 6 points higher than “expectation”. Figures are almost the same for all the four monolingual systems. For the three crosslingual systems towards Italian (client turns), the situation is the same. We observed a 5 points increase. The analyzers in the monolingual and crosslingual systems towards Italian are the same for each source language. Thus, we may say that those scores are better than expected thanks to the analyzers “robustness”. When checking the scores for the three crosslingual systems from Italian (1 Italian analyzer and 3 generators), we get unclear results: the French and German generators do not reach expectation on hypothesis by 1 or 2 points but the English generator scores over expectation by 5 points. We can only conclude that the generators of French and German are not as robust as those of English and Italian. Maybe this due to the fact that the IF is, in a way based on English and mostly defined by CMU and IRST.  ASR  WAR  Hypos as paraphrases Majority vote  Mono-lingual trans.  on Refs/Hypos Majority vote  Cross-lingual trans.  on Refs/Hypos Majority vote  on Refs/Hypos Majority vote  71 65 64 F-F c 54/41 51/38 F-I c 44/34 38/31 I-F a 40/27 38/26  62 66 — E-E c 48/45 — E-I c 55/43 — I-E a 47/37 46/35  64 68 — G-G c 46/40 — G-I c 32/27 — I-G a 47/31 45/20  77 70 — I-I a 61/48 60/44  Table 2: results of the NESPOLE! first showcase evaluation  3.2. 2002’s showcase-2a “Extended Tourism”  The second showcase evaluation methodology has been designed to overcome some problems of our first evaluation.  3.2.1. Data and protocol For each language, two unseen dialogues were picked up from the second NESPOLE! data collection [16]. The dialogues focused on additional scenarios such as tours of  castles and lakes. The evaluation data sets were of the same kind as those used in the first showcase evaluation. This evaluation also includes a comparison of the Showcase-1 components and the Showcase-2a components. The Showcase-1 components were frozen and saved after the Showcase-1 evaluation. The Showcase-1 components were then run on the Showcase-2a evaluation data in order to have a comparison of the two systems o n the same data. In this evaluation, the Showcase-1 system was only run on transcribed input. In this evaluation, we departed from our previous grading methodology in several ways. First, the 3-point scale (perfect, OK, bad) was replaced with a 4-point scale, based only on meaning preservation, taking neither fluency nor grammatical accuracy into account. Second, whereas we previously reported average scores across graders for each SDU, we calculated majority scores as well as averages. The majority votes are generally close to the averages, except where there is an outlier (a grader who was exceptionally harsh or lenient), but this problem did not occurred. Third, the graders for this evaluation were last-year students in a school for translators. Previously, graders had no special training in translation, and the groups were less homogeneous in terms of education and of second language knowledge than this year.  3.2.2. Study on graders agreement To establish the stability and coherence of our evaluation scheme, it is important to have a good measure of how well different human graders agree on scoring the same output, and also how consistent the graders are over time. Graders were first all trained on the same data set. They were given grading instructions and a grading training set. They graded this training set and we discussed their grading in order to finally have them agree on the same score for each SDU in the set. Graders were also given two copies of the same grading check file they had to grade before and after they graded the actual test sets. This allowed to check: (1) their mutual agreement on this check set before and after the actual grading task, (2) their consistency over time. In order to evaluate intercoder agreement before and after the task, we made 3 categories: (1) the 3 graders fully agree on the same grade, (2) the 3 graders agree on 2 grades that fall in the same final category (there is a majority vote), (3) the graders do not agree on the same final category (there is not majority vote). We got the following figures.  (1) Agreement (2) Majority (3) no majority  Before  71  28  
Recent work on training of log-linear interpolation models for statistical machine translation reported performance improvements by optimizing parameters with respect to translation quality, rather than to likelihood oriented criteria. This work presents an alternative and more direct training procedure for log-linear interpolation models. In addition, we point out the subtle interaction between log-linear models and the beam search algorithm. Experimental results are reported on two Chinese-English evaluation sets, C-Star 2003 and Nist 2003, by using a statistical phrase-based model derived from Model 4. By optimizing parameters with respect to the BLUE score, performance relative improvements by 9.6% and 2.8% were achieved, respectively.  1. Introduction Log-linear interpolation models, which can be formally derived within the maximum entropy framework [1], have been only recently applied to statistical machine translation (SMT) [2]. In addition, and similarly to what proposed for speech recognition [3], optimization of interpolation parameters can directly address translation quality, rather than the usual maximum likelihood criterion [4]. This paper goes along the direction of [4], and proposes an alternative and more direct training procedure, but computationally more intensive. Moreover, a subtle relationship between the parameter optimization and the beam search algorithm is pointed out, which might have an important impact on the choice of optimal parameters.  2. Log-Linear Model for SMT Given a source string f and a target string e, the framework of maximum entropy [5] provides a mean to directly address the posterior probability Pr(e | f ). By introducing the hidden alignment variable a, the usual SMT optimization criterion is expressed by:  e∗ = arg max Pr(e, a | f ) e a  ≈ arg max Pr(e, a | f )  (1)  e,a  The conditional distribution Pr(e, a | f ) is deter- mined through suitable real valued features functions hi(e, f , a), i = 1 . . . M , and takes the parametric form:  pλ(e, a | f ) =  exp{ i λihi(e, f , a)} e,a exp{ i λihi(e, f , a)}  (2)  The maximum entropy criterion suggests to com- pute values λi, which maximize the log-likelihood over a training sample T :  λ∗ = arg max  log pλ(e, a | f ) (3)  λ  (e,f ,a)∈T  An interesting log-linear model results if the following feature functions derived from Model 4 [6] are used:  h1(e, f , a) = log Pr(e) h2(e, f , a) = log Pr(φ | e) h3(e, f , a) = log Pr(τ | e, φ) h4(e, f , a) = log Pr(π | e, φ, τ ),  which explain f and a for e in terms of fertilities φ, tablets τ and permutations π. In fact, after simple manipulations, the usual decoding criterion for Model 4 results, with the addition of four scaling factors:  e∗ ≈ arg max Q(e, a; λ)  (4)  e,a  = arg max Pr(e)λ1 · Pr(φ | e)λ2 · e,a  Pr(τ | e, φ)λ3 · Pr(π | e, φ, τ )λ4 (5)  To tackle the optimization problem of eq. (5), a search algorithm can be devised which incrementally extends partial translation hypotheses (e˜, a˜) of the source string, until an optimal complete translation is found. A translation is said partial if its corresponding alignment a˜ does not cover all positions in f . The complexity of the search algorithm mainly depends on the number of possible translations and of target positions to be considered for each source word. To avoid exponential complexity, constraints on both factors are generally introduced. Moreover, the so-called pruning of hypotheses  103  is deployed, too. Hence, at each step (or target string length), only a beam with the most “promising” hypotheses is considered for extension. The following are two very popular pruning methods, which are usually applied to partial translations of the same length and/or covering the same source positions:  • threshold pruning: partial hypotheses (e˜, a˜) whose score Q(·) is smaller than the (local) optimum score Q∗ times a given factor T , i.e.  Q(e˜, a˜; λ) Q∗  <  T  ,  (6)  are eliminated;  • histogram pruning: hypotheses not among the top N best scoring ones are pruned.  3. Minimum Error Training  In place of the criterion (3), [4] recently proposed to esti- mate parameters by minimizing the number of translation errors. We assume that a function ED(λ) is available, which measures the translation errors made by running a model deﬁned by parameter values λ on a development set D. Hence, parameters are searched by:  λ∗ = arg min ED(λ)  (7)  λ  Unlike the log-likelihood criterion (3), the objective function ED(·) might have many local minima. Hence, ﬁnding an optimal solution can be very hard. In this work, we use the simplex method [7], an algorithm for multivariate function minimization which requires relatively few function evaluations. The same algorithm was already applied for the same task in [8] and for training log-linear language models in [1].  3.1. Interaction with Beam Search  The optimization process, besides tuning the parameters  of the statistical model, may also interfere with the beam  search. The reason is in the following property of the  scoring function (4):  Q(e˜, a˜; αλ) = Q(e˜, a˜; λ)α  (8)  for any positive real number α. As a consequence, the  threshold criterion (6) is affected by any change of the  parameter vector λ which corresponds to a scaling trans-  formation. For instance, a contraction of the parameter vector by a factor α = 0.5 would implicitly determine  the search to prune hypotheses according to the more re-  laxed constraint:  Q(e˜, a˜; λ) Q∗  <  T2  (9)  Hence, we can expect that any optimization algorithm would be easily attracted by parameter values which relax the pruning threshold, and reduce the error rate at the expense of more computations.  3.2. Simplex Initialization To remove the impact of the pruning threshold, the simplex method is started from a parameter conﬁguration inducing a loose threshold, so that any further widening of it does not give tangible effect on performance. A potential problem of this approach could be its high computational cost. In our implementation, the optimization remains feasible because the cost of search is also bounded by the histogram pruning. Moreover, optimization of parameters is no more inﬂuenced by the beam search, given that there is no relationship between the histogram pruning and the parameters. Another possibility could be to normalize the parameter vector (like in [2]), or to ﬁx one important parameter and let vary only the others. 4. Experiments 4.1. Baseline System The core of the translation system is a statistical model, based on the IBM Model 4 and extended to deal with phrases rather than with single words [9]. The corresponding log-linear model is similar to that shown in eq. (5) with the addition of two terms, which explicitly scale the fertility and distortion probabilities of the null word. Search is performed by a decoder based on dynamic programming. Both in training and testing, sentences are pre-processed in order to reduce data sparseness. Pre-processing includes: Chinese word segmentation, separation of words from punctuation, handling of acronyms and abbreviations, number extraction, case normalization, etc. In the following, we will refer to the baseline system when uniform parameters are assumed, which can be possibly scaled up or down to modify the beam-search pruning. 4.2. Data We evaluated our approach on two Chinese-English translation tasks: the Nist 2003 MT evaluation task1, large-data case-insensitive conditions, and the C-Star 2003 evaluation campaign 2. The ﬁrst task concerns with translation of new agencies, while the second task concerns with basic traveling expressions [10]. Test sentences are provided with 4 and 16 human translations, respectively. Tables 1 and 2 report detailed statistics about the used training and test data. For parameter optimization, the Nist 2002 MT evaluation data and 1,000 sentences extracted from the C-Star training data were used, respectively. It is worth noticing that the C-Star 2003 test set has been used as development set for the IWSLT-2004 evalu- 1www.nist.gov/speech/tests/mt 2www.c-star.org  104  Table 1: Statistics of training data.  
Feature selection is critical to the performance of maximumentropy-based statistical concept-based spoken language translation. The source language spoken message is first parsed into a structured conceptual tree, and then generated into the target language based on maximum entropy modeling. To improve feature selection in this maximum entropy approach, a new concept-word feature is proposed, which exploits both concept-level and word-level information. It thus enables the design of concise yet informative concept sets and easies both annotation and parsing efforts. The concept generation error rate is reduced by over 90% on training set and 7% on test set in our speech translation corpus within limited domains. To alleviate data sparseness problem, multiple feature sets are proposed and employed, which achieves 10%-14% further error rate reduction. Improvements are also achieved in our experiments on speech-to-speech translation.  !S!  QUERY SUBJECT WELLNESS  PLACE  PLACE PREPPH BODY-PART  is he bleeding any wehereelse besides his abdomen  !S!  PLACE  SUBJECT WELLNESS  PREPPH BODY-PART PLACE  QUERY  12345367389  34333  1. INTRODUCTION Automatic spoken language translation is crucial to speech-tospeech (S2S) translation systems that facilitate communication between people who speak different languages. While substantial progress has been made over the past decades in research areas of speech recognition and machine translation, multilingual natural speech translation remains a grand challenge for human speech and language technologies [1,2,3,4]. Compared to written-text messages, most conversational spoken messages are conveyed through casual spontaneous speech with strong disfluencies and imperfect syntax. In addition, the output from speech recognizers often contains recognition errors and no punctuations, which brings serious challenges to robust and accurate translation. In our prior work [5], we presented a statistical spoken language translation framework based on tree-structured semantic/syntactic representations, or concepts, as illustrated in Figure 1. In this example, the source English sentence and the corresponding Chinese translation are represented by a set of concepts – {PLACE, SUBJECT, WELLNESS, QUERY, PREPPH, BODY-PART}. Some of the concepts (such as PLACE, WELLNESS and BODY-PART) are semantic representations while some of the concepts (such as PREPPH) are syntactic representations. There are also concepts (such as SUBJECT and QUERY) that represent both semantic and syntactic information. Note that although the source and target sentences share the same set of concepts, the tree structures are significantly different from each  Figure 1. Example of Concept-based English-to-Chinese Translation other because of the well-known distinct nature of these two languages (i.e., English and Chinese). The above concept tree is comparable to interlingua [1] - a language-independent representation of intended meanings that is commonly used in modern spoken language translation systems. In our approach, the intended meanings are represented by a set of language-independent concepts (same as conventional interlingua approach) organized in a language-dependent treestructure (different from conventional interlingua method). The process of this concept-based translation may be further divided into two cascaded sub-processes: a) the generation of conceptual tree structure, and b) the generation of words within each concept, in the target language. While the total number of concepts may usually be limited to alleviate data sparseness impacts (especially for new domains), there are no constraints on the structures of the conceptual trees. Therefore, compared to traditional interlingua-based speech translation approaches, our conceptualtree-based approach could achieve more flexible meaning preservation with wider coverage and, hence, higher robustness and accuracy on translation tasks in limited domains, at the cost of additional challenges in the appropriate transformation of conceptual trees between source and target languages. Two principal challenges remain open in the design of conceptbased speech translation systems. One challenge is the design and selection of language-independent concepts, which usually depends on the domain in which the translation system is used.  115  This is a lengthy, tedious but very important task. The concepts have to be not only broad enough to cover all intended meanings in the source sentence but also informative so that a target sentence can be generated with right word sense and in a grammatically correct manner. The size of the concept set is also important as too many concepts may result in data sparseness for training, while too few concepts could degrade the translation accuracy. Another challenge is the generation of concepts in the target language via a natural concept generation (NCG) process. The purpose of NCG is to generate the correct concept structure in the target language corresponding to the concept structure in the source language. As explained before, the concept structures are language-dependent. Errors in concept generation could greatly distort or even ruin the meaning to be expressed in the target language, particularly in conversational speech translations where in most cases only a few concepts are conveyed in the messages to be translated. Therefore, accurate and robust NCG is viewed as an essential step towards high-performance conceptbased spoken language translation. While NCG approaches can be rule-based or statistical, we prefer the latter because of its trainability, scalability and portability. One such approach based on maximum-entropy (ME) criterion was presented in our previous work [5]. It was then improved in [6] and [7] by the employment of a series of algorithms such as forward-backward modeling and confidence measurement. One critical problem remain in our ME-based translation approach is feature selection. In theory, the principle of maximum entropy does not directly concern itself with the issue of feature selection [8]. It merely provides a framework to combine constraints of both source and target language into a translation model. In reality, however, the feature selection problem is crucial to the performance of ME-based approaches, since the universe of possible constraints (or features) is typically in thousands or even millions for natural language processing. Some of these impacts on ME-based speech translation were preliminarily described in our previous work [6]. In this paper, to address the above concerns, we analyze and discuss in greater detail the feature selection issue in the design of ME-based statistical concept-based speech translation systems. In particular, a novel feature is proposed to use the combination of concept and word information to achieve higher NCG accuracy while minimize the total number of distinct concepts and hence greatly reduce the concept annotation and natural language understanding effort. A multiple feature selection algorithm is further employed to handle data sparseness issues. Experiments with these new algorithms are performed and analyzed on both the NCG accuracy and the overall speech translation performance. 2. BASELINE STATISTICAL NATURAL CONCEPT GENERATION USING MAXIMIZING ENTROPY MODELS  A. Statistical Concept-based S2S Translation  ASR  NLU  NCG  NWG  TTS  Statistical Concept-based MT  Figure 2 IBM MASTOR (Multilingual Automatic Speechto-Speech TranslatOR) System  Figure 2 shows a general framework of our MASTOR speech translation system for applications in limited domains. A cascaded scheme of large-vocabulary conversational automatic speech recognition (ASR), statistical concept-based machine translation and concatenative text-to-speech (TTS) synthesis is applied by using state-of-the-art speech and language processing techniques. While each of these three functional units is crucial to the overall speech-to-speech translation quality, we are only concerned with the performance of statistical concept-based translation here. The baseline statistical concept-based translation further consists of three cascaded functional components: natural language understanding (NLU), natural concept generation (NCG) and natural word generation (NWG). In our MASTOR system, the NLU function is performed via a decision-tree-based statistical semantic parser pre-trained on an annotated text corpus [9]. The NWG process generates words in the target language based on the generated structural concepts from NCG as well as a tag-based word-to-word multilingual dictionary [10]. Although these two components are very important to our statistical interlinguabased translation, they are, again, beyond the scope of this paper. The NCG process generates a set of structural concepts in the target language according to a concept-based semantic parse tree derived from the NLU process in the source language. The accuracy of the NCG process has a great impact on the final translation performance as any errors of inserted, missing, replaced or mistakenly ordered concepts may cause severe understanding problems or loss of meaning during multilingual speech communication. Therefore, highly accurate NCG is essential to our goal of meaning preservation in conversational speech translation. In this paper, we focus on improving the ME-based statistical NCG method, as explained next.  B. ME-based Statistical NCG on Sequence Level The baseline statistical NCG algorithm on sequence level was proposed in [5] as an extension from the “NLG2” algorithm described in [11]. During natural concept sequence generation, the concept sequences in the target language are generated sequentially according to the output of NLU parser. Each new concept is generated based on the local n-grams of the up-to-date generated concept sequence and the subset of the input concept sequence that has not yet appeared in the generated sequence. Let us assume that the source language concept sequence pro- { } duced from NLU parser is C = c1, c2 ,1, cM . Let us further { } assume that a concept sequence S = s1, s2 ,1, sn containing n concepts has already been generated in target language. In  116  order to generate the next new concept sn+1 , the conditional probability of a concept candidate is defined and computed as  ( ) ∑∏∏ p s cm, sn, sn−1 =  α ( ) g fk ,s,cm ,sn ,sn−1 k k α ( ) g fk ,s,cm ,sn ,sn−1 k  ,  (1)  s∈V k  where s is the concept candidate to be generated, sn and sn−1 are the previous two concepts in S . V is the set of all possible concepts that can be generated. ( ) 2 f2k = s+k1, c k , s0k , s−k1 is the k-th feature. The selection of fk will be discussed in the next section. 2 αk is a probability weight corresponding to each feature fk . The value of αk is always positive and is optimized over a train- ing corpus by maximizing the overall logarithmic likelihood, i.e.,  [ ( )] L  ∑ ∑ ∑ α k = arg max  log p s cm , sn , sn−1 , (2)  α  l=1 s∈ql m  where Q = {ql ,1 ≤ l ≤ L} is the total set of concept sequences. The optimization process can be accomplished via the Improved Iterative Scaling algorithm using maximum entropy criterion described in [11].  g is a binary test function defined as  C: QUERY S:  SUBJECT WELLNESS PLACE  C: QUERY S: PLACE  SUBJECT WELLNESS  C: QUERY S: PLACE  WELLNESS SUBJECT  C: QUERY S: PLACE  SUBJECT WELLNESS  C: S: PLACE  SUBJECT WELLNESS QUERY  Figure 3. Example of Concept Sequence Generation during translation of English sentence “is he bleeding anywhere else besides his abdomen” as illustrated in Figure 1.  2  ( ) g  2 fk , s, cm , sn , sn−1  =  1   0  if fk = (s, cm , sn , sn−1 ) otherwise  (3)  2  where fk represents the co-occurrence of the generated concept  s and its context information of cm , sn and sn−1 .  Using (1), (2) and (3), sn+1 is generated by selecting the concept candidate with highest probability, i.e.,  ∏ ( ) sn+1  =  arg max M   s∈V  m=1  p  s cm ,sn, sn−1      .  (4)  { } For an input concept sequence C = c1, c2 ,1, cM , the gen- eration procedure is performed as follows:  !S!  QUERY SUBJECT WELLNESS  PLACE  PLACE PREPPH BODY-PART  !S!  QUERY SUBJECT WELLNESS  PLACE  PREPPH BODY-PART PLACE  !S!  PLACE  SUBJECT WELLNESS QUERY  PREPPH BODY-PART PLACE  !S!  PLACE  SUBJECT WELLNESS  PREPPH BODY-PART PLACE  QUERY  Figure 4. Example of Structural Concept Generation during translation of English sentence “is he bleeding anywhere else besides his abdomen” as illustrated in Figure 1 and Figure 3.  117  1) Set s0 = s−1 = START, where “ START” is a pre-defined concept representing the start of the sequence; Set n = 0; Define initial set of generation sequence S = φ ; 2) For each n, generate sn+1 according to equation (3) and set S = {s1,1, sn }+1 ; 3) If sn+1 ∈ C , set C = C − sn+1 (remove sn+1 from C); ac- cordingly, let M ← M −1; 4) If M ≥ 1 or n + 1 ≤ N , repeat 2) and 3); Otherwise, stop and output generated concept sequence S. Since the number of concepts generated in S could be different from the number of concepts in the input sequence in the source language, only a maximum number (denoted as N) of concepts may be generated. In our experiments, N = 11 . An example of primary-level (or main level) concept sequence generation is depicted in Figure 3 when translating the English sentence in Figure 1 into Chinese. C. Structural Concept Sequence Generation The algorithms described above only deal with the concept generation issue of a single sequence. To tackle the generation problem of multiple sequences at different structural levels, a recursive structural concept sequence generation algorithm is proposed in [2,3] as follows: 1) Traverse the semantic parse tree in a bottom-up left-to-right manner; 2) For each un-processed concept sequence in the parse tree, generate an optimal concept sequence in the target language based on the procedure described in sub-section 2.B; after each concept sequence is processed, mark the root-node of this sequence as visited; 3) Repeat step 2) until all parse braches in the source language are processed; 4) Replace nodes with their corresponding output sequence to form a complete concept tree for the output sentence. An example of structural concept sequence generation is depicted in Figure 4 when translating the English sentence in Figure 1 and Figure 3 into Chinese. 3. FEATURE SELECTION IN MAXIMUMENTROPY-BASED STATISTICAL NCG A. Problem Statement and Baseline Features Earlier we introduced two basic challenges in the design of statistical maximum-entropy-based models for natural concept generation: 1) finding appropriate facts or features about the observed data; 2) optimally incorporate these features into the target models. In the previous section, we solved the second problem by using maximum-entropy principle in equations (1-4). In  this section, we will attack the first challenge and improve natural concept generation performance by augmenting feature dimensions and combining various feature sets, as explained next.  ( ) Wf2ke(4)  begin = s+k1  with ,ck ,  the basic four-dimensional s0k , s−k1 defined in equation  feature (1) and  set (2),  which was first proposed in [5]. In this feature set, the order of  concepts in the input sequence is discarded to alleviate perform-  ance degradation caused by sparse training data. However, there  exist many cases in which the same set of concepts need to be  generated into two different concept sequences depending on the  order of the input sequence. For these typical concept sequences,  generation errors are inevitable with the features of the specific  form no matter how the statistical model is optimized.  ( ) To tackle this problem, w2e proposed in [6] an augmented five-  dimensional feature as  f  (5) k  =  s+k1, c0k , c+k1, s0k , s−k1  , where  c0k and c+k1 are two adjacent concepts in the source concept  sequence C . Accordingly, the conditional probability of a con-  cept candidate and the probability weights are modified as  ( ) gk  ¡ f  ( k  5  )  ,  s  ,  c  m  ,  c  m+1  ,  s  n  ,  s  n  −1  ∏α k  ( ) ∑∏α ( ) p s cm,cm+1, sn, sn−1 = k  gk  ¡ f  ( k  5  )  ,  s  ,  c  m  ,  c  m+1  ,  s  n  ,  s  n  −1  k  , (5)  s∈V k  ∑∑ ∑ [ ( )] L αk = arg max  M −1 log p s cm , cm+1, sn , sn−1 . (6)  α  l=1 s∈ql m=1  Since the above features and target sequences,  rf2ek(p5r)esaerentsexcotrnaccetepdt  orders from  in both source pre-annotated  parallel corpora during ME-based model training. Particularly, the optimization of (6) is performed upon a parallel tree-bank { } QQ = ul ,vl 1 ≤ l ≤ L , where ul and vl are the concept  sequences in source and target language, respectively. For each ( ) feature s, cm , cm+1, sn , sn−1 during ME model training,  cm and cm+1 are derived from ul , while sn and sn+1 are de-  rived from vl . This augmented feature strengthens the link be-  tween sequences in source and target languages, and can thereby improve NCG accuracy as reported in [6].  B. Conciseness versus Informativity of Concepts So far we tried to extract features on the concept level. However, as explained earlier, the definition and detection of concept itself is a very challenging task. On the one hand, the concepts are defined as concise as possible, since the smaller the number of total distinct concepts, the less the effort will be endeavored in the labor-extensive and time-consuming annotation procedure, and the higher the accuracy and robustness will be of the statistical natural language understanding algorithms. On the other hand, the concepts should be as informative as possible, because the concept generation accuracy will largely rely on the sufficient information provided by each concept.  118  WHQ AUX SUBJECT  What  did you  ACTION  TIME  eat  yesterday  SUBJECT  TIME  ACTION WHQ  
The PolyphraZ tool is under construction in the framework of the TraCorpEx project (Translation of Corpora of Examples), for the management of parallel multilingual corpora (coding, format, correspondence). It is a software platform allowing the preparation and handling of parallel corpora (languages, codings...), parallel presentation, and addition of new languages t o existing corpora by calling several MT systems, and letting human translators produce the final reference translations by using a web-based editor. It integrates the computation of some objective evaluation metrics (NIST, BLUE), and enables subjective evaluations thanks t o parallel presentations, and formating based on distance computations between sentences (at several levels). In the future, PolyphraZ should also support versioning and provide feedbacks to developers of the MT systems used: unknown words, badly translated words, and comparative presentations of the outputs of the various systems. Introduction We work on several parallel corpora such as the BTEC corpus and the Tanaka corpus, but we miss effective tools for the management of these corpora, such as a web platform allowing import, export, preparation (coding, formats...) and processing (translation, revision, edition…) of multilingual corpora. The BTEC comprises about 162320 sentences (about 4000 standard "translator's pages"1) in Japanese, Chinese, English and Korean, less in other CSTAR languages. Diffusion of this corpus is restricted to ATR partners in CSTAR (consortium for speech translation advanced research). Our practical goal is to produce a French version of the BTEC, with the same quality of the sentences. Tools such as Excel, TextEdit or BBEdit do not allow sharing such corpora on the Web, nor editing and visualizing parallel sentences. During a stay at ATR, the second author translated the complete BTEC, submitting 163 files of 1000 sentences to Systran Premium v.4, adequately parametrized, and revised the first 1000 sentences, equivalent to 24 standard translator pages in 6:08 hours, or 15 mn per page, under TextEdit, a standard text editor, manually aligning the source and target files. In a later experiment, he did the same on 510 sentences while three other French native speakers translated them by hand, at a rate of 1h per page each (the usual figure in professional translation). That shows that using MT output really speeds up the 
This paper deals with the task of statistical machine translation of spontaneous speech using a limited amount of training data. We propose a method for selecting relevant additional training data from other sources that may come from other domains. We present two ways to solve the data sparseness problem by including morphological information into the EM training of word alignments. We show that the use of part-of-speech information for harmonizing word order between source and target sentences yields signiﬁcant improvements in the BLEU score. 1. Introduction When developing a system to automatically translate spontaneous speech, we regard the following aspects as important: • Usually, only a limited corpus of bilingual sentence pairs is available for training. • Rule-based transfer machine translation methods are hardly applicable, since the utterances are often spontaneous and colloquial and may not represent wellformed sentences. Furthermore, in case of automatically recognized speech, the input sentence may contain recognition errors which may completely destroy the sentence structure. • The training data sparsely covers only a limited vocabulary and a very limited number of possible cases of non-monotonous translations. In this paper, we present some methods for mitigating these problems. We follow a statistical approach to machine translation in which we estimate translation model parameters from a training corpus of bilingual sentence pairs. In section 2 we will brieﬂy describe the source-channel approach to the statistical word alignment model and the alignment template system for machine translation. There are different aspects of the data sparseness problem. First, it may be that it is difﬁcult to obtain enough bilingual sentence-aligned training data for a speciﬁc language; this is not within the scope of this work. Another problem is obtaining additional bilingual data for speciﬁc domain or  genre like medical texts or travel conversations. In Section 3 we will describe a method for extending the training corpus with relevant bilingual data from larger (more general) corpora. The next problem we may face is the limited coverage of the vocabulary, when many words appear only once in the training corpus. This is especially true for highly inﬂected languages. Section 4 will present a possibility to use morphological information like word base forms to improve automatic word alignments for such languages. Some research in this direction has been performed in [1]; they proposed hierarchical lexicon models containing base forms and partof-speech tags for the translation from German into English. In our work, we will use such lexicon models directly in the alignment training, whereas [1] created the models from the ﬁnal (Viterbi) alignment obtained after the standard training procedure. Finally, when only a small bilingual corpus is available for training, not enough examples of word or phrase reordering are learned, and non-monotonous translations can not be produced or have a very poor quality. In Section 5 we will propose a method for re-ordering the source sentences in training and in testing using part-of-speech (POS) tags. We will try to reduce the differences in word order between the source and the corresponding target sentence. This strategy monotonizes the translation process. As a result, good estimates of model parameters become possible even with scarce training data. We will present experimental results in which we apply all of these methods to two machine translation tasks. These are the Nespole! [2] German-English corpus and the German-English Verbmobil corpus. We achieve substantial improvements with some of the presented techniques. 2. Statistical Machine Translation 2.1. Word Alignment In statistical machine translation, we are given a source language sentence f1J = f1 . . . fj . . . fJ , which is to be translated into a target language sentence eI1 = e1 . . . ei . . . eI . Among all possible target language sentences, we choose the  139  sentence with the highest probability:  eˆI1 = argmax P r(eI1|f1J )  (1)  eI1  = argmax P r(eI1) · P r(f1J |eI1)  (2)  eI1  The decomposition into two knowledge sources in Equation 2 is known as the source-channel approach to statistical machine translation [3]. It allows an independent modeling of the target language model P r(eI1) and translation model P r(f1J |eI1). The target language model describes the wellformedness of the target language sentence. The translation model links the source language sentence to the target language sentence. The word alignment A is introduced into the translation model as a hidden variable:  P r(f1J |eI1) =  P r(f1J , A|eI1)  (3)  A  Usually, restricted alignments are used in the sense that each source word is aligned to at most one target word. Thus, an alignment A is a mapping from source sentence positions to target sentence positions A = a1...aj...aJ , (aj ∈ {0, . . . , I}). The alignment aJ1 may contain alignments aj = 0 with the ‘empty’ word e0 to account for source sentence words that are not aligned to any target word at all. A detailed comparison of the commonly used translation models IBM-1 to IBM-5 [4], as well as the Hidden-Markov alignment model (HMM) [5] can be found in [6]. All these models include parameters p(f |e) for the single-word based lexicon. They differ in the alignment model. All of the model parameters are trained iteratively with the EM-Algorithm.  to the maximum entropy principle, e.g. using the Generalized Iterative Scaling (GIS) algorithm. Alternatively, one can train them with respect to the ﬁnal translation quality measured by some error criterion [8]. We follow the alignment template translation approach of [9], where a phrase tranlation model is used as one of the main features. The key elements of this translation approach are the alignment templates. These are pairs of source and target language phrases together with an alignment within the phrases. The phrases are extracted from the automatically estimated word alignments. The alignment templates are build at the level of word classes, which improves their generalization capability. Besides the alignment template translation model probabilities, we use additional feature functions. These are the word translation model and two language models: a wordbased trigram model and a class-based ﬁve-gram model. Furthermore, we use two heuristics, namely the word penalty and alignment template penalty feature functions. To model the alignment template reorderings, we use a feature function that penalizes reorderings linear in the jump width. We use a dynamic programming beam search algorithm to generate the translation hypothesis with maximum probability. This search algorithm allows for arbitrary reorderings at the level of alignment templates. Within the alignment templates, the word order is learned in training and kept ﬁx during the search process. This is only a brief description of the alignment template approach. For further details, see [9, 7].  2.2. Translation: Alignment Template Approach  3. Acquiring Additional Training Data  The argmax operation in Eq. 2 denotes the search problem, i.e. the generation of the output sentence in the target language. We have to maximize over all possible target language sentences. For the search, we choose an alternative to the classical source-channel approach and model the posterior probability P r(eI1|f1J ) directly. Using a log-linear model [7], we obtain:  M  P r(eI1|f1J ) = Z(f1J ) · exp  λmhm(eI1, f1J )  m=1  Here, Z(f1J ) denotes the appropriate normalization constant, hm are the feature functions and λm are the corresponding scaling factors. We thus arrive at the decision rule:  eˆI1 = argmax eI1  M λmhm(eI1, f1J ) m=1  This approach has the advantage that additional models or feature functions can be easily integrated into the overall system. The model scaling factors λM 1 are trained according  When only a small corpus of sentence pairs is available for training of the statistical translation models, it may be reasonable to include additional bilingual training data from other sources. Since this additional data may come from another domain and substantially differ from the original training corpus, a method for selecting relevant sentences is desirable. In our experiments, we use a relevance measure of ngram coverage. To this end, we compute the set C of ngrams occurring in the source part of the initial training corpus (n = 1, 2, 3, 4). Then, for each candidate source sentence in the additional corpus, we compute a score based on the occurrence of the n-grams from C in that sentence. The score is deﬁned as the geometric mean of n-gram precisions and is therefore similar to the BLEU score used in machine translation evaluation [10]. Such score provides a quantitative measure of how “out-of-domain” or “in-domain” the additional training data may be. We add only those sentence pairs to the initial training corpus, for which this score is sufﬁciently high.  140  4. Morphological Information for Word Alignments  4.1. Lexicon Smoothing  Existing statistical translation systems usually treat different derivations of the same base form as they were independent of each other. In our approach, the dependencies between such derivations are taken into account during the EM training of the statistical alignment models. Typically, the statistical lexicon model p(f |e) is based only on the full forms of the words. For highly inﬂected languages like German this might cause problems because the coverage of the lexicon might be low. In particular, the coverage problem arises in alignment trainings with small amount of data. The information that multiple full-form words share the same base form is not used in the lexicon model. To take this information into account, we smooth the lexicon model with a backing-off lexicon that is based on word base forms. The smoothing method we apply is well known from language modeling [11]. It is absolute discounting with interpolation:  max {N (f, e) − d, 0}  p(f |e) =  + α(e) · β(f |e)  N (e)  Here, e denotes the generalization, i.e. the base form, of the word e. The nonnegative value d is the discounting parameter, α(e) is a normalization constant and β(f, e) is the normalized backing-off distribution. The formula for α(e) is:      
This paper describes the speech recognition module of the speech-to-speech translation system being currently developed at ATR. It is a multi-lingual large vocabulary continuous speech recognition system supporting Japanese, English and Chinese languages. A corpusbased statistical approach was adopted for the system design. The database we collected consists of more than 600 000 sentences covering broad range of travel related conversations in each of the three languages. The recognition system is based on language-dependent acoustic and language models, and pronunciation dictionaries. The models are built using the latest training methods developed at ATR as the Minimum Description Length Successive State Splitting (MDL-SSS) and Multi-dimensional Composite N-gram techniques. The speciﬁcs of each language are taken into account in order to achieve high recognition performance. The speech recognition system is under constant improvement and enhancement, and although the models for the different languages are at different development stages, the recent evaluation experiments showed that the recognition performance is above 92% for every language. 1. Introduction Speech-to-Speech (S2S) translation is a pipe dream for human-beings, which enables communication with people speaking in different languages. Since our world is becoming borderless day by day, the importance of S2S translation technology has been increasing. ATR had started the S2S translation research in order to overcome this language barrier problem in 1986. So far, we have been working in speech recognition, machine translation, speech synthesis and integration for a S2S translation system. We are currently in the third term beginning from 2000. The target of this term is to develop technologies to make the S2S system work in real environments. Speech recognition system should be robust enough to recognize speech in noisy environments with various speaking styles. The machine translation system needs to be do-  main portable and good to translate wide variety of topics. The speech synthesis is requested to realize more natural and expressional speech quality. In this project all the researchers including speech processing researchers and natural language researchers are working collaboratively and closely for realization of S2S translation system. For the S2S system the speech recognition system should recognize speaker independent continuous spontaneous conversational speech. Back in 1986, the state of the art technology of speech recognition is only able to recognize speaker dependent connected words of small vocabulary. Due to a lot of efforts so far based on statistical modeling technologies like HMMs and N-grams and large amounts of speech and text corpus, now recognition of speaker independent continuous conversational speech is going to be available. The thing to consider especially developing speech recognition for S2S translation system is nothing but speech recognition system to be multi-lingual. In speech recognition, it is well known that probabilities by acoustic modeling by HMMs and language modeling by N-gram are jointly used to search an optimal word sequence in decoding. Parameters of HMMs and Ngrams are estimated using a large amount of speech data and text data. There is always trade-off between number of parameters for the model and number of training data. It always takes lots of time to ﬁnd a best number of parameters suitable for the data amount. This paper uses new acoustic modeling procedure to create variable state assignment of HMM states based on successive state splitting and information criteria. The successive state splitting algorithm, SSS, is a procedure to split a existing state and assign variable number of states for one phoneme unit considering likelihood increase. Furthermore, MDL-SSS, which is SSS algorithm based on minimum description length, is proposed in order to stop growth of number of states according the training data amount. In multi-lingual situations the available amounts of training data are often different and this makes optimization process complicated. The MDL-SSS is proposed to solve this problem. The same problem also  147  exists in language modeling. There are also an out-ofvocabulary problem and a new words problem. For this problems we introduce class-based N-gram and composite bigram to approximate trigram. It is also true that size of training text corpus amount is always different depending on language. An efﬁcient estimation algorithm always help to get good estimate of language model parameters. In this paper, we describe designs and algorithms of our multi-lingual speech recognition system for S2S translation system. Currently our target languages are Japanese, English and Chinese. This paper also introduces language dependent parts and their speech recognition evaluation results of each language as well.  2. Speech-to-Speech Translation Background  The goal of the automatic speech-to-speech translation is to generate a speech signal in one (target) language that conveys the linguistic information contained in a given speech signal of another (source) language. A statistical approach to the speech-to-speech translation problem gives the following formal solution:  ST∗  =  arg max P (ST |SS) ST  (1)  where SS and ST are the speech signals in the source and target languages. As direct evaluation of the conditional probability P (ST |SS) is intractable, it can be factorized as:  P (ST |SS) =  P (ST , TT , TS|SS)  TT ,TS  =  P (ST |TT , TS, SS)P (TT |TS, SS)P (TS|SS)  TT ,TS  ≈  P (ST |TT )P (TT |TS)P (TS|SS)  (2)  TT ,TS  where TS and TT are the text transcriptions of the source and target speech signals. Then, the maximization of P (ST |SS) can be further simpliﬁed to:  max P (ST |SS ) ST  =  max ST  P  (ST  |TT∗  )  max TT  P  (TT  |TS∗  )  max P (TS|SS)  (3)  TS  where TT∗ and TS∗ are arguments maximizing the second and third terms. This equation suggests that the S2S  translation problem can be decomposed into three inde-  pendent parts: P (TS|SS) which represents speech recognition, P (TT |TS) that is text-to-text translation model, and P (ST |TT ) which corresponds to speech synthesis.  For the speech recognition problem, suppose the in-  put speech signal SS is represented by a sequence of fea-  ture vectors y:  y = y1 · · · yt  (4)  Then, the speech recognition goal is to ﬁnd a word sequence w = w1 · · · wn that maximizes P (w|y):  wˆ = argmax P (w|y) w = argmax P (w1 · · · wn|y1 · · · yt) (5) w This can be rewritten by Bayes theorem as:  P (w|y)  
This paper gives an overview of the evaluation campaign results of the IWSLT041 workshop, which is organized by the C-STAR2 consortium to investigate novel speech translation technologies and their evaluation. The objectives of this workshop is to provide a framework for the applicability validation of existing machine translation evaluation methodologies to evaluate speech translation technologies. The workshop also strives to ﬁnd new directions in how to improve current methods. 1. Introduction The drastic increase in demands for the capability to assist trans-lingual conversations, triggered by IT technologies such as the Internet and the expansion of borderless communities such as the increased number of EU countries, has accelerated research activities on speech-to-speech translation technology. Many research projects have been designed to advance this technology, such as VERBMOBIL, C-STAR, NESPOLE!, and BABYLON. These projects, except for CSTAR, have mainly focused on the construction of a prototype system for several language pairs. On the contrary, one of C-STAR’s ongoing projects is the joint development of a speech corpus that can handle a common task in multiple languages. As a ﬁrst result of this activity, a JapaneseEnglish speech corpus comprising tourism-related sentences, originally compiled by ATR, has been translated into the native languages of the C-STAR members. The corpus serves as a primary source for developing and evaluating broadcoverage speech translation technologies [1]. This corpus is used in the research and development of multi-lingual speech-to-speech translation systems on a “common use” basis. For the effective and efﬁcient research and development of speech-to-speech translation systems, the evaluation of current translation quality is very important. In particular, the system developments done by using a common corpus, like C-STAR project, require careful evaluation of the prominent 1International Workshop on Spoken Language Translation, http://www.slt.atr.jp/IWSLT2004 2Consortium for Speech Translation Advanced Research, http://www.cstar.org/  translation techniques. Therefore, there is strong demand for the establishment of evaluation metrics for multilingual speech-to-speech translation systems. For this purpose, the Evaluation Campaign 2004 was carried out using parts of the multilingual corpus (cf. Section 2.1). The task was to translate 500 Chinese or Japanese sentences into English. Depending on the amount of permitted training data, three different language resource conditions (Small Data Track, Additional Data Track, Unrestricted Data Track) were distinguished. The translation quality was measured using both human assessments (subjective evaluation) and automatic scoring techniques (automatic evaluation). The evaluation results of the submitted MT systems are summarized in Section 3. The corpus supplied for this year’s conference, the reference translations, the output of the participating MT systems, and the evaluation results will be made publicly available after the workshop. These resources can be used as a benchmark for future research on MT systems and MT evaluation methodologies. We hope that IWSLT2004 will become the ﬁrst step toward establishing standard metrics and a standard corpus for speech-to-speech multi-lingual translation technology. 2. Evaluation Campaign 2004 The Evaluation Campaign 2004 was carried out using parts of the multilingual corpus jointly developed by the C-STAR partners (cf. Section 2.1). The task was to translate 500 Chinese or Japanese sentences into English. Depending on the amount of permitted training data, three different language resource conditions (Small Data Track, Additional Data Track, Unrestricted Data Track) were distinguished (cf. Section 2.2). Each participant was allowed to register only one MT system in each of the data tracks but could submit multiple translation results (runs) for the same track. In total, 14 institutions took part in this year’s workshop, submitting 20 MT systems for the Chinese-to-English (CE) and 8 MT systems for the Japanese-to-English (JE) translation tasks. The translation quality was measured using both human assessments (subjective evaluation) and automatic scoring  
This paper introduces ATR’s project named Corpus-Centered Computation (C3), which aims at developing a translation technology suitable for spoken language translation. C3 places corpora at the center of its technology. Translation knowledge is extracted from corpora, translation quality is gauged by referring to corpora, the best translation among multiple-engine outputs is selected based on corpora, and the corpora themselves are paraphrased or ﬁltered by automated processes to improve the data quality on which translation engines are based. In particular, this paper reports the hybridization architecture of different machine translation systems, our technologies, their performance on the IWSLT04 task, and paraphrasing methods. 1. Introduction There are two main strategies used in corpus-based translation: 1. Example-Based Machine Translation (EBMT) [1]: EBMT uses the corpus directly. EBMT retrieves the translation examples that are best matched to an input expression and then adjusts the examples to obtain the translation. 2. Statistical Machine Translation (SMT) [2]: SMT learns statistical models for translation from corpora and dictionaries and then searches for the best translation at run-time according to the statistical models for language and translation. By using the IWSLT04 task, this paper describes two endeavors that are independent at this moment: (a) a hybridization of EBMT and statistical models, and (b) a new approach for SMT, phrase-based HMM. (a) is used in the “unrestricted” Japanese-to-English track (Section 2), and (b) is used in “supplied” Japanese-to-English and Chinese-toEnglish tracks (Section 3). In addition, paraphrasing technologies, which are not used in the IWSLT04 task but boost translation performance, are also introduced in Section 4.  2. Hybrid MT System (Unrestricted J-to-E Track) No complete translation system has emerged nor is likely to emerge in the foreseeable future. Every approach to translation has its own way of acquiring translation knowledge and using the knowledge. Each system generates its peculiar errors in attempting translation. As a result, translation performance differs sentence-by-sentence, system-bysystem. There is the possibility of boosting translation performance through exploitation of multiple translations generated by different systems. Among several possible architectures to integrate multiple translation engines (Section 2.5), we demonstrate the acrchitecture below (Sections from 2.1 to 2.4) as one effective approach. 2.1. A Hybridization: Multiple EBMTs Followed By A Selector Based On SMT Models It is important to integrate “different” types of element machine translation systems in order to boost the overall performance by having them compensate each other. We propose an architecture in which multiple EBMT engines work in parallel and their outputs are passed to a post-process that selects the best candidate according to SMT models. Most EBMT systems employ phrases or sentences as the translation unit so that they can translate while taking a wider perspective in order to handle case relations, idiomatic expressions, sentence structure, and so on. However, when there is ambiguity in translation, EBMT selects the best translation mainly by the similarity between the input and the source part of the example. EBMT’s validation of its translation is ﬂawed. On the other hand, SMT employing IBM models translates an input sentence by a combination of word transfer and word re-ordering. Therefore, when it is applied to a language pair in which the word order is much different (e.g. English and Japanese), it is difﬁcult to ﬁnd a globally optimal solution due to the enormous search space. However, SMT can sort translations in the order of their quality according to its statistical models. We show two different EBMT systems here, brieﬂy explain each system, and then compare them. Finally, we ex-  13  plain the selector used to determine the best from multiple translations based on SMT models. 2.2. Two EBMTs 2.2.1. D3, DP-based EBMT Sumita [3] proposed D3 (Dp-match Driven transDucer), which exploits DP-matching between word sequences. Let’s illustrate the process with a simple sample below. Suppose we are translating a Japanese sentence into English. The Japanese input sentence (1-j) is translated into the English sentence (1-e) by utilizing the English sentence (2-e), whose source sentence (2-j) is similar to (1-j). The common parts are unchanged, and the different portions, shown in bold face, are substituted by consulting a bilingual dictionary. ;;; A Japanese input (1-j) iro/ga/ki/ni/iri/masen ;;; the most similar example in corpus (2-j) dezain/ga/ki/ni/iri/masen (2-e) I do not like the design. ;;; the English output (1-e) I do not like the color. We retrieve the most similar source sentence of examples from a bilingual corpus. For this, we use DP-matching, which tells us the edit distance between word sequences while giving us the matched portions between the input and the example. The edit distance is calculated as follows. The count of the inserted words, the count of the deleted words, and the semantic distance of the substituted words are summed. Then, this total is normalized by the sum of the lengths of the input and the source part of translation example. The semantic distance between two substituted words is calculated by using the hierachy of a thesaurus[4]. Our language resources in addition to a bilingual corpus are a bilingual dictionary, which is used for generating target sentences, and thesauri of both languages, which are used for incorporating the semantic distance between words into the distance between word sequences. Furthermore, lexical resources are also used for word alignment. 2.2.2. HPAT, Grammar-based EBMT The second EBMT is different from the ﬁrst EBMT in that it parses bitexts of a parallel coupus with grammars for both source and target languages. Imamura [5] proposed a new phrase alignment approach called Hierarchical Phrase Alignment (HPA). First, two sentences are tagged and parsed independently. This operation obtains two syntactic trees. Next, words are linked by the word alignment program. Then, HPA retrieves equivalent phrases that satisfy two conditions: 1) words in the pair correspond with no deﬁciency and no excess; 2) the phrases are  of the same syntactic category. Imamura [6] subsequently proposed HPA-based transla- tion (HPAT). HPAed bilingual trees include all information necessary to automatically generate transfer patterns. Translation is done according to transfer patterns using the TDMT engine [7]. First, the source part of transfer patterns are utilized, and source structure is obtained. Second, structural changes are performed by mapping source patterns to target patterns. Finally, lexical items are inserted by referring to a bilingual dictionary, and then a conventional generation is performed. Finally, Imamura [8] proposed a feedback cleaning method that utilizes automatic evaluation to remove incorrect/redundant translation rules. BLEU was utilized to measure translation quality for the feedback process, and the hillclimbing algorithm was applied in searching for the combinatorial optimization. Utilizing the features of this task, incorrect/redundant rules were removed from the initial solution, which contains all rules acquired from the training corpus. Our experiments showed a considerable improvement in MT quality. 2.2.3. Comparison of Two EBMTs As can be seen in Section 2.2.1, Section 2.2.2, and Table 1, the main difference between the two EBMT systems is in their use of grammars.  Table 1: Resources used for two EBMTs in IWSLT04 unresticted Japanese-to-English track.  bilingual corpus bilingual dictionary thesaurus grammar  D3 travel domain (20K) in-house in-house N.A.  HPAT travel domain (20K) in-house in-house in-house  D3 achieves a good quality, when there is a similar translation example in the parallel corpus, otherwise D3 may fail to produce a good translation. On the contrary, HPAT produces a modest quality translation for most of the inputs (Table 2).  Table 2: Features of the two EBMTs.  Unit Coverage Quality  D3 sentence narrow good  HPAT grammatical unit wide modest  This is conﬁrmed by the subjective evaluation of quality in Table 3. Here, we show MT’s quality by using ﬁve ranks, S, A, B, C, and D 1, from good quality to poor qual- 1The ﬁve grades are deﬁned as follows: (S) Splendid: ﬂuent like a naitive speaker; (A) Perfect: no problem with either information or grammar; (B)  14  ity. This is judged by English native-speakers who are also familiar with Japanese. The evaluator investigates bilingual information, i.e., the source sentence and its MT output. This is an overall score that considers both adequacy and ﬂuency, which are particular scores used in the IWSLT evaluation campaign. The IWSLT evaluator makes a monolingual evaluation, i.e., a reference translation made in advance by a professional translator and MT output, and judges the adequacy and ﬂuency of the MT translation. Table 3: ATR’s Overall Subjective Evaluation - percentages of S, A, B, C, and D ranks. D3 HPAT S 57.00 38.60 A 13.00 21.20 B 7.60 17.60 C 5.80 6.00 D 16.60 16.60 The portion of translations with rank “S” for D3 is very large, while the portions of translations with ranks “A,” “B,” and “C” are relatively small. Thus, the slope is very steep, while the slope of HPAT is gentle. 2.3. SMT-based Selector We proposed an SMT-based method of automatically selecting the best translation among outputs generated by multiple machine translation (MT) systems [9]. Conventional approaches to the selection problem include a method that automatically selects the output to which the highest probability is assigned according to a language model (LM). [10] These existing methods have two problems. First, they do not check whether information on source sentences is adequately translated into MT outputs, although they do check the ﬂuency of MT outputs. Second, they do not take the statistical behavior of assigned scores into consideration. The proposed approach scores MT outputs by using not only the language but also a translation model (TM). To conduct a statistical test later, this scoring is done by using each of multiple pairs of language and translation models. The method, then, checks whether the average TM∗LM score of an MT output is signiﬁcantly higher than that of another MT output. This check uses a multiple comparison test based on the Kruskal-Wallis test [11]. 2.4. Results 2.4.1. Selecting Effect As shown in Table 4, all of the metrics taken together show that the proposed selector outperforms both element trans- Good: easy to understand, with either some unimportant information missing or ﬂawed grammar; (C) Fair: broken, but understandable with effort; (D) Unacceptable: important information has been translated incorrectly.  lation systems; for example, mWER is decreased by 2.55 (about 7.5% reduction) from 28.86 to 26.31.  Table 4: Objective Evaluation.  BLEU NIST GTM mWER mPER  D3 60.36 10.35 77.70 28.86 26.07  HPAT 49.33 9.78 76.88 37.18 31.06  SELECT 63.06 10.72 79.67 26.31 23.33  DIFF. +3.00 +0.37 +1.97 -2.55 -2.97  Table 5: ATR’s Overall Subjective Evaluation - cumulative percentages of S, A, B, C, and D ranks.  S S,A S,A,B S,A,B,C D  D3 57.00 70.00 77.60 83.40 16.60  HPAT 38.60 59.80 77.40 83.40 16.60  SELECT 59.80 73.00 82.40 87.80 12.20  DIFF. +2.80 +3.00 +4.80 +4.40 -4.40  Next, the relationship between translation quality of element systems and gain by the selector was analyzed. Table 5 shows that the proposed selector reduces the number of low-quality translations (ranked “D”) while it increases the number of high-quality translations (ranked “S” to “B”).  2.4.2. Performance vs. Corpus Size Since the methods are corpus-based, the quantity of the corpus determines the system performance.  Table 6: mWER vs. Corpus size.  Training corpus IWSLT-supplied (2K) (20K) DIFF.  D3 45.71 28.86 -16.85  HPAT 47.28 37.18 -10.10  The corpus used in this experiment is ten times larger than the supplied corpus, and the drastic reduction in mWER has been demonstrated (Table 6). However, the quality with the small corpus is not so bad in the subjective evaluation shown in Table 7. We conjecture that adequacy is not low even with the supplied corpus, and the translation become similar to native English, that is, its ﬂuency improves as the size of corpus increases.  2.5. Discussion Related works have proposed ways to merge MT outputs from multiple MT systems [12] in order to output better translations. When the source language and the target language have similar sentence structures, this merging ap-  15  Table 7: ATR’s Overall Subjective Evaluation - IWSLT supplied corpus.  S S,A S,A,B S,A,B,C D  D3 34.80 47.40 62.60 73.40 26.60  HPAT 25.20 44.20 70.40 80.40 19.60  SELECT 34.00 50.60 72.20 81.80 18.20  proach is very attractive. On the other hand, when the source language and the target language have different sentence structures, such as English and Japanese, we often have translations whose structures are different from each other for a single input sentences. Thus, the authors regard the merging approach as less suitable than the approach of selecting. Hybridization can be implemented in several arichitectures, for example, SMT followed by EBMT, SMT and EBMT in parallel, and so on. Which archtecture is best is still an interesting open question. In addition to the merging and selecting approaches, a modiﬁcation approach can be taken. For example, Marcu [14] proposed a method in which initial translations are constructed by combining bilingual phrases from translation memory, which is followed by modifying the translations by greedy decoding [15]. Watanabe et al. [16] proposed a decoding algorithm in which translations that are similar to the input sentence are retrieved from bilingual corpora and then modiﬁed by greedy decoding.  3. Phrase-based HMM SMT System (Supplied J-to-E and C-to-E Tracks)  This section describes an innovative approach to statistical  translation modeling, namely the phrase-based HMM trans-  lation model. The model directly structures the phrase-based  translation approach in a Hidden Markov structure and pro-  poses an efﬁcient way to estimate and induce phrase transla-  tion pairs in a uniform fashion.  In the statistical approach to machine translation, orig-  inally proposed in [2], the problem of translating a source  text in a foreign language, f , into a target language, for in-  stance English, e is formulated as the maximization problem  of  eˆ = argmax P (e|f )  (1)  e  The noisy channel modeling of the above problem resulted  in  eˆ = argmax P (f |e)P (e)  (2)  e  Many previous efforts in the phrase-based approach to statistical machine translation basically approximated the former term, P (f |e), as the products of sequence of phrase  translations with additional constraints [17, 18, 19]:  P (f |e) ≈ P (¯fi|e¯ai )  (3)  i  where ¯fi is the ith phrase of the phrase-segmented sentence ¯f1m for f , and ai is the phrase alignment for the phrasesegmented texts. 2 Instead, we introduced two new hidden variables, ¯f and  e¯, to explicitly capture the phrase translation relationship:  P (f |e) = P (f , ¯f , e¯|e)  (4)  ¯f ,e¯  The term P (f , ¯f , e¯|e) is further decomposed into three terms:  P (f , ¯f , e¯|e) = P (f |¯f , e¯, e)P (¯f |e¯, e)P (e¯|e) (5)  The ﬁrst term of Equation 5 represents the probability that a segmented input sentence ¯f can be reordered and generated as the input text of f . The second term indicates the translation probability of the two phrase sequences of e¯ and ¯f . The last term is the likelihood of the phrase-segmented text e¯ generated from e. We call these terms the Phrase Segmentation Model, the Phrase Translation Model, and the Phrase Ngram Model, respectively.  3.1. Phrase Ngram Model  The phrase ngram model is approximated as:  P (e¯|e) ≈ P (e¯i|e¯i−1)  (6)  i  P (e¯i|e¯i−1) is treated as the bigram constraints of adjacent translated phrases e¯i and e¯i−1. The phrase ngram model can be easily estimated with  the Forward-Backward algorithm by expanding all possible phrase segmentations of e into a lattice structure E¯ as shown  in Figure 1. Each node in the lattice represents a particular phrase E¯ i in a sentence e connected by edges with associated probability of P (E¯ i|E¯ i ). The estimation procedure can be roughly summarized as  follows.  1. Initialize the probability table.  2. For each sentence e in the training corpus, estimate the posterior probabilities P (E¯ i, E¯ i |e) on the lattice using the Forward-Backward algorithm.  3. Estimate the prior probabilities based on the maxi-  mum likelihood estimation by using the estimated pos-  terior probabilities as the frequency of the occurrence  of words:  P (E¯ i|E¯ i ) =  e P (E¯ i, E¯ i |e) e E¯ i P (E¯ i, E¯ i |e)  (7)  4. Iterate steps 2 and 3 until a termination condition is satisﬁed.  2A phrase is simply a consecutive sequence of words and is not always linguistically coherent.  16  P (E¯ 2|E¯ 1) E¯ 2  E¯ 4  E¯ 1  E¯ 3  E¯ 6  E¯ 5  Figure 1: Phrase Ngram Model  3.2. Phrase Segmentation Model  According to the generative modeling represented in Equation 5, the term P (f |¯f , e¯, e) can be regarded as the distortion probability of how a phrase segmented sentence ¯f will be reordered to form the source sentence f . Instead, we model this as the likelihood of a particular phrase segment ¯fj observed in f :  P (f |¯f , e¯, e) ∝ P (¯f |f )  (8)  ≈  P (¯fj|f )  (9)  j  The segmentation model is realized as the unigram posterior probability of the phrase ngram model presented in Section 3.1. To brieﬂy summarize, the unigram posterior probability can be efﬁciently computed by the Forward-Backward algorithm using the lattice structure F¯ for f :  P (F¯ j|f ) =  P (F¯ j, f ) F¯j P (F¯ j , f )  (10)  The phrase segmentation model can be viewed as the prior term to assign a certain weight to a particular phrase given a source text. If we restrict the phrase length to 1, i.e. each phrase consisting of only one word, then the phrase segmentation model will assign 1 to all phrases.  3.3. Phrase Translation Model  The phrase translation model is approximated so that the phrase translation can be captured as the product of the individual phrase translations.  P (¯f |e¯, e) ≈ P (¯fj|e¯aj )  (11)  j  where the ai represents phrase alignment as seen in word alignment based translation model, such as the IBM Models.  3.4. Phrase-based HMM Statistical Translation  Combining all of the submodels – the phrase ngram model, the phrase segmentation model, and the phrase translation model – Equation 4 can be rewritten as  P (f |e) ≈  P (¯fj|f )P (¯fj |e¯i)P (e¯i|e¯i )  (12)  e¯,¯f j,i  If the phrase segmented sentences e¯ and ¯f are expanded into the corresponding lattice structures of E¯ and F¯ , then  E¯ 2  E¯ 4  E¯ 1  E¯ 3  P (E¯ 3|E¯ 1)  P (F¯ 2|E¯ 3)  P (F¯ 2|f )  F¯ 2  E¯ 6 E¯ 5 F¯ 6  F¯ 1  F¯ 3  F¯ 4  F¯ 5  Figure 2: Phrase-based HMM Statistical Translation Model  Equation 12 can be regarded as a Hidden Markov Model in which each source phrase F¯ j in the lattice F¯ is treated as an observation emitted from a state E¯ i, a target phrase, in the lattice E¯ , as shown in Figure 2. The use of the phrase-based HMM structure has already been proposed in [20] in the context of aligning documents and abstracts. In their approach, jump probabilities were explicitly encoded as the state transitions that roughly corresponded to the alignment probabilities in the context of the word-based statistical translation model. The use of the explicit jump or alignment probabilities served for the completeness of the translation modeling at the cost of the enormous search space needed to train the phrase-based HMM structure. In our approach, the state transitions are governed by the phrase ngram model, bigram of phrase connection probabilities, but this method ignores phrase alignment probabilities. Therefore, the phrase-based HMM translation model is a deﬁcient model. However its simplicity contributes to the faster estimation of parameters.  3.5. Parameter Estimation  The parameters for the phrase-based HMM translation model can be efﬁciently estimated by using the Forward-Backward algorithm brieﬂy described in Section 3.1.  For the Forward-Backward procedure, we deﬁne two  auxiliary  variables,  α(e  i2 i1  ,  fjj12  )  and  β(eii21 , fjj12 ).  α(eii21 , fjj12 )  represents the forward estimates of the probability of the  phrase eii21 translated into fjj12 after the emission of the  all  phrase  combinations  presented  in  e  i1 1  −1  .  Similarly,  β(eii21 , fjj12 ) represents the backward estimates of the proba-  bility of the phrase eii21 translated into fjj12 considering the all  right  phrase  combinations  of  e  l i2  +1  .  Therefore, the Forward-Backward algorithm can be for-  17  mulated to solve the recursions  i1 −2  α(eii21 , fjj12 ) =  α(eii1−1, fjj12 )  i  =1  f j2 j1  ∩fjj12  =∅  ×P (eii21 |eii1−1)P (fjj12 |eii21 )P (fjj12 |f ) (13)  l  β(eii21 , fjj12 ) =  β(eii2+1, fjj12 )  i =i2+2 fjj12 ∩fjj12 =∅  ×P (eii2+1|eii21 )P (fjj12 |eii2+1)P (fjj12 |f )  (14)  To overcome the problem of local convergence often observed in the EM algorithm [21], we use the lexicon model from the GIZA++ [22] training as the initial parameters for the phrase translation model. In addition, the phrase ngram model and the phrase segmentation models are individually trained over the monolingual corpus and remained ﬁxed during the HMM iterations.  3.6. Phrase Segment Induction  Equations 13 and 14 involve summation over all possible contexts, either in its left-hand-side or right-hand-side on the lattice structure of E¯ , and the summation over all possible segmentation over F¯ . Since the computation is still enormous, even with the help of dynamic programming, we restrict the possible segmentation to those phrase translation pairs induced before the estimation. The phrase pairs are induced by ﬁrst considering all possible bilingual phrase pairs in a training corpus using the product of two phrase translation probabilities:  P (e¯|f¯)P (f¯|e¯) =  count(e¯, f¯)2 f¯ count(e¯, f¯) e¯ count(e¯, f¯) (15)  where count(e¯, f¯) is the cooccurrence frequency of the two phrases e¯ and f¯. The basic idea of Equation 15 is to capture the bilingual correspondence while considering two directions. Additional phrases were exhaustively induced based on the intersection/union of the viterbi word alignments of the two directional models, P (e|f ) and P (f |e), computed by GIZA++ [17]. After the extraction of phrase translation pairs, their monolingual phrase lexicons were extracted and used as the possible segmentation for the source and target sentences.  3.7. Decoder  The decision rule to compute the best translation is based on the log-linear combinations of all subcomponents of translation models as presented in [23].  eˆ  =  argmax e  
We compare the performance of several SYSTRAN systems on the BTEC corpus. Two language pairs: Chinese to English and Japanese to English are used. Whenever it is possible the system will be used “off the shelf” and then tuned. The first system we use is freely available on the web. The second system, SYSTRAN Premium, is commercial. It is used in two ways: (1) choosing and ordering available original dictionaries and setting parameters, (2) same + user dictionaries. As far as the evaluation is concerned, we competed in the unlimited data track. 1. Introduction We first give our motivations for participating in this campaign with a commercial system. Then we briefly describe the system tested (SYSTRAN® 5.0) and the ways it can be parametrized. The bulk of this paper describes the evaluation procedure. We finish by presenting and analyzing the results. 2. Rationale MT evaluation is a hot topic since 1960 or so. The literature on evaluation may even be larger than that on MT techniques proper. MT evaluation may have several goals: – help buyers buy MT (or CAT) system best suited to their needs, – help funders decide on which technology to support, and – help developers measure various aspects of their systems, and measure progress. The MT evaluation campaign organized by the C-STAR III consortium falls in the latter category. Its aim is to measure the “quality” of various MT systems developed for speech-tospeech translation when applied to the BTEC corpus [7]. Another goal is to compare the MT systems developed by the C-STAR partner not only between them, but also with other systems, notably commercial systems. In the past, we often got the impression that, in similar campaigns, the commercial system used as a “baseline” were tested in quite biased ways. From what was reported, the experimenters submitted the input texts to free MT web servers, and evaluated the results. But that method is quite unfair, which makes all the conclusions scientifically invalid. For example, long ago, the CANDIDE system trained intensively on the Hansard corpus, was compared with an offthe-shelf version of SYTSRAN without any tuning. SYSTRAN clearly won, but the margin might have been far bigger (or perhaps not, this should have been studied!), if SYSTRAN had  been tuned to this totally unseen corpus, at the level of dictionaries, of course, but perhaps also of grammars. Another example is given by MSR [5] on comparison between their French-English system, highly tuned to their documents (actually, the transfer component was 100% induced from 150000 pairs of sentences and their associated “logical forms” or deep syntactic trees). They used also SYSTRAN, this time slightly tuning it by giving priority to SYSTRAN dictionaries containing computer related terms1. However, they apparently did not invest time to produce a user dictionary containing the MicroSoft computer science dictionary. Technical terminology varies a lot from firm to firm and even from product to product. What is then the value of the conclusion that their system was (slightly) better than SYSTRAN? And when they tried to do the same comparison on the Hansard, SYSTRAN (“general”) won. As members of C-STAR III not engaged in developing J-E or C-E systems (although we worked on prototypes in the 80’s), we felt it was interesting to take part in this evaluation campaign to establish a “most faithful baseline” for a commercial system. Which commercial system(s) to use? We choose SYSTRAN because: – it offers the two pairs C-E and J-E, – these pairs have been recently slightly improved (although more work has been done on E-C and E-J), – SYSTRAN agreed to give us free access to the latest version (v5), in its Premium packaging (Windows interface, with many tunable parameters, and the possibility to create a user dictionary), – it can be considered as a kind of “medium” baseline, when compared with the other commercial MT systems for C-E and J-E (some are far worse, and some far better, e.g. ATLAS-II for E-J and ALT/JE for J-E). There is another worry with the current evaluation campaigns: objective evaluation is performed using “reference” human translations, but the measures, based on n-gram co-occurrence counts, correlate only (very) weakly with human judgment [9], and that human judgment is too often not task oriented. As a result: – the evaluations produce tables of figures with no decisive, clear interpretation about intrinsic quality (relative to some precise goal); – real translation results are never shown side by side for subjective evaluation by the readers themselves; – no task-oriented measure is computed. To compute such a measure, one should measure the time it takes a human to produce a “reference translation” from an MT output, as done by [8]. 
We present a bidirectional Example-Based Machine Translation (EBMT) system for Chinese—English. The prerequisite is a bilingual aligned corpus of Chinese—English sentences, and we describe the example extraction efforts purely based on word alignment. The whole system is designed to be language independent and as automatic as possible for construction. We present initial experiments which show that our algorithm can successfully generate better translations for the domain in question than the baseline rule based system.  2. Auto Word Alignment Based EBMT To meet the concerns mentioned above, we propose an EBMT method based solely on the word alignment information of the Chinese English parallel corpus. Basically speaking, the whole process of system construction can be fully automatic as long as a translation dictionary and a word aligned bilingual corpus is provided. Figure 1 describes the architecture of the whole system.  1. Introduction The Olympic Games will be held in Beijing, China in 2008. It is clear that a great deal of translation will be required from Chinese to English, and vice versa. This paper describes our bi-directional Example-Based Machine Translation (EBMT) system for Chinese—English, which is purely based on the word alignment information of a given bilingual corpus. Among the vast issues for a translation system, the followings are emphasized in our system design: (1) Automatic construction: Manual knowledge composition is not desired for system building except for some public existed knowledge basses (e.g. dictionary). The whole process of translation knowledge acquisition should be as automatic as possible. (2) Sub-sentential translation example focus: Linguistically there are infinite sentences. A translation system is desired to capture the translation correspondences under the sentence level, hoping to recombine them for proper translations. (3) Linguistic light approach: Current deep linguistic analysis tools (like parser et al) are not reliable enough. So, if necessary, we would just choose shallow linguistic analyser which causes somewhat less information loss. (4) Adaptability: Since Olympics demands multi languages besides English and Chinese, the method is kept language independent as possible so that the system success (if it did!) could be readily extended to the translations between Chinese and other languages. For the same reason, domain specific advantages are not exploited in the the system.  Figure 1: AWA based EBMT architecture. Roughly speaking, the whole system can be understood as tow parts: the training process (left half of Fig.1) and the translation process (right half). The training process automatically extracts the translation example base from a word aligned Chinese English bilingual corpus. The key component is the word alignment based example extraction algorithm. The translation process will pre-process the input sentence (tokenization, numerical processing, Chinese word segmentation et al), feed it into the translation module and display the output. The translation engine will search the best translation via example selection, translation disambiguation and surface generation. The following of this section will introduce the main parts of the system in detail.  * Supported by the High Technology Research and Development Program of China (2002AA117010-09) and National Natural Science Foundation of China (60375019)  27  2.1. Word Alignment Based Example Extraction 
 2. Phrase-Based Models  This paper describes the system we built for the Chinese to English track of the IWSLT 2004 evaluation campaign. A one month effort was devoted to this exercise, starting from scratch and making use as much as possible of freely available packages. We show that a decent phrase-based translation engine can be built within this short time frame. 1. Introduction Machine Translation is a very active ﬁeld nowadays strongly anchored into a paradigm of performance. Evaluation exercises such as those conducted within the TIDES project are pushing system designers to constantly improve their systems. Currently, many of the top-performing systems are phrase-based statistical machine translation (SMT) engines. The fact that SMT systems are among the best ones in those evaluation exercises is not surprizing considering the peculiarities of the translation tasks considered. The popularity of phrase-based models (PBMs) in SMT is neither a surprise, since PBMs allow to a certain extent to cope with local word reordering across languages, as well as to account for local context modelling. [1] also credit PBMs for being somehow tolerant to tokenization errors, an interesting characteristic when dealing with languages such as Chinese, the source language under consideration in this study. This effervescent activity comes with some bonus. Several freely available valuable packages (e.g. Giza++ [2], Pharaoh [3], SRILM [4]) make possible the fast development of a phrase-based translation engine. Other packages allow to quickly evaluate a system according to a gold standard (e.g. MTEVAL http://www.nist.gov/speech/ tests/mt/mt2001/resource and GTM http:// nlp.cs.nyu.edu/GTM). This paper reports on the one month effort we spent building a system for the Chinese-toEnglish track of the IWSLT workshop, relying intensively on these packages. 31  Very recently, several authors [5, 6] proposed at the same time an astonishingly simple but powerful model which we designate hereafter as Flat Phrase-Based model (FPBM). A FPBM is simply a collection of pairs of sequences of words with one or several scores (or probabilities) attached to them. The main difference between a FPBM and an alignment template (AT) model [7] being that the former does not attempt to model internal reordering of phrases. Thus, FPBMs as such do not have generalization capabilities. Zens and Ney [8] give an experimental comparison of both models on three different test sets. On the German-English Verbmobil task, the AT engine outperforms the PB engine they tested, while on the other tasks — the Spanish-English Xerox task, and the French-English Hansards task — they observed the opposite. Toma`s et al. [9] recently revisited the AT model and report that combining it with a FPBM brings some improvements. The recipe given in [5, 6] for acquiring a FPBM is simple: use a word-alignment to identify in an heuristic way an alignment relation at the so-called phrasal level. Both articles propose relative frequency counts to score each pair of phrases. Several authors noted that the relative frequency estimator is particularly inappropriate to the task, since many phrases (and especially long ones) are seen only few times, sometimes only once. [5, 10] proposed to score a pair of phrases according to an IBM-like word-based model, the lexicon probabilities p(f |n) being learned by relative frequency over the word alignment set. This idea has also been tested by Vogel et al. [10]. Also, in [8], the authors propose to score a pair of phrases according to a smoothed probabilistic word bilingual lexicon. And Vogel et al. [1] demonstrated experimentally that rating phrases according to an informationbased score yields noticeable improvements. Other variations to the recipe mentioned above have been extensively investigated by the CMU team and summarized in [10]. They investigated variations on the way the word alignment is produced, considering for instance a bilingual bracketing alignment [11], an alignment technique also tried at the same workshop by [12]. Zhang et al. [13] proposed an alternative way to collect  phrases without requiring word alignment. They rely instead on point-wise mutual information between source and target words to identify phrases in both languages. This has a clear advantage over methods that rely on a monolingual segmentation step, followed by a bilingual mapping one, as for instance the one described in [14]. 3. Our system We developed a translation engine around the freely available package Pharaoh [3]. This package is provided with a binary ﬁle, as well as a carefully written user manual. The core of this decoder is a beam search engine optimizing a noisy channel model, as described in equation 1, where sI1 = s1, . . . , sI stands for the best sequence of I phrases that fully cover the source sentence s.  eˆ = argmaxe p(c|e)plm(e)λlm ω|e|×λω = argmaxe,I pφ(cI1|eI1)plm(e)λlm ω|e|×λω  (1)  Here, an independence assumption is further assumed between phrases, and the transfer model pφ is formulated as in equation 2, where φ is a FPBM, and d is a simple distortion model depending on ai the starting position of the foreign phrase ci, and bi−1 the ending position of the native phrase ei−1 (see [5] for more).  I  p(cI1|eI1) = φ(ci|ei)λφ d(ai − bi−1)λd  (2)  i=1  What really matters from the user point of view of this package, is the fact that the decoder takes as input:  • a pair of FPBMs, one for each direction, the direct model (in our case φ(e|c)) being used for nbest-list rescoring, a functionality of Pharaoh we did not use in this study.  • a target language model (English), in the format output by the SRILM package [4],  • a set of weights applied in a log-linear fashion to the different models, namely: λφ, the weight given to the transfer model; λlm, the weight given to the language model, λω, the word-penalty weight and λd, the weight given to the distortion model.  We trained the language models of the target part of the training corpus (20 000 English sentences) with the SRILM package1. In order to feed the phrase extractor, we ﬁrst wordaligned the training bitext making use of the Giza++ package. Since [5] shown that the degree of the IBM model from which the viterbi alignment is computed was not playing a crucial role, we used the viterbi approximation computed by Giza++ for the IBM model 3 (training IBM model 4 is more 1We did not investigate the many smoothing options this package handles, but applied the setting recommended in [15]. 32  demanding, since we need to train the word classes that are conditioning the distortion probabilities of this model). The only thing we had really to implement was a prescription to get the FPBMs required by Pharaoh. This is described in the following section.  4. Phrase extraction We tried two kinds of strategies to compute the FPBMs. The ﬁrst one, is directly following the approach described in [5, 6] and is detailed in section 4.1. The second one is a simple string-based approach described in section 4.2.  4.1. Word-alignment-based extractor  A nowadays standard practice among the PBM practitioners consists in aligning the bitext at the word level making use of word-alignment models trained in both directions (here C→E and E→C). This double alignment process makes senses since the underlying alignment model (most often an IBM model [16]) is not symmetrical. Two sets of links between words are then distinguished. We call P (for Precision) the set of links that are present in both alignment directions, and R (for Recall) the links that are present in at least one alignment (C→E or E→C). Note that P ⊆ R. The word alignment retained is constituted of the P-links, as well as some R-links in the neighborhood of P. We implemented a variant of this approach which is strongly inspired by [5, 6]. Although the principle is very straightforward, we did not ﬁnd in those articles a precise enough description of the algorithm. So, for the sake of completeness, we report in algorithm 1 the pseudo-code of the variant that we implemented. Our algorithm works in 4 steps. First, the P-links are considered (line 6), then extended by considering R-links (lines 9-21). Third, independent boxes are collected (lines 24-33). An independent box ((x1, x2), (y1, y2)) represents a region in the alignment matrix where none of the source words Sxx12 is aligned to a word not belonging to Tyy12 and vice-versa:  ∀x ∈ [x1, x2], ∀y : (x, y), y ∈ [y1, y2] ∀y ∈ [y1, y2], ∀x : (x, y), x ∈ [x1, x2]  (3)  where is an alignment relation made explicit by step 1 and 2 of the algorithm. The fourth and last step of the algorithm (lines 36-42) consists in electing pairs of phrases, any sequence of adjacent (on the source side) boxes. The pseudo-code of our variant makes use of a data structure T [x] (resp. T [y]) which stands for the target (resp. source) positions associated to the source (resp. target) position x (resp. y):  T [x] = {y : (x, y)}, ∀x ∈ [1, |S|] T [y] = {x : (x, y)}, ∀y ∈ [1, |T |]  (4)  We also need a few functions to simplify the description of the algorithm. The ﬁrst function maintains the T structure during step 1 and 2 of the projection algorithm.  function add(x, y)  T [x] ← T [x] ∪ {y} T [y] ← T [y] ∪ {x}  The second one, called during the extension stage veriﬁes that (x, y) is a valid link to extend on:  function neighbor(x, y) if (x, y) ∈ R, ∈P then if T [x] = {} or T [y] = {} then a ← a ∪ (x, y)  The third function collects the pairs of phrases after checking some few length properties and is called during step 4 of the algorithm: function add(x1, x2, y1, y2) x ← x2 − x1 + 1 y ← y2 − y1 + 1 if x ∈ [minLength, maxLength] then if y ∈ [minLength, maxLength] then if (max(x, y)/min(x, y)) ≤ ratio then res ← res ∪ (Sxx12 , Tyy12 ) 4.2. String-based extractor The second phrase extractor performs simple string operations. It is intended to capture obvious redundancies at the sentence and phrasal level in the training corpus. It is based on the simplifying assumption that if two strings are in relation of translation and if part of them also are, then we can induce a speciﬁc translation relation between the other parts. This is the idea formulated in the algorithm 2. In practice, we factor out the preﬁx and sufﬁx test carried out in lines 10 and 11 of Algorithm 2 by sorting the training corpus using as sort key: a) the Chinese sentence, b) the English sentence, c) the inverted Chinese sentence and d) the inverted English sentence. Iterating from the top to the bottom of these lists, whenever a line contains it’s preceding line, the preceding line is subtracted and the new pair of phrases added to the training corpus. The process was stopped when the productivity of the algorithm decreased, producing about 60 000 new pairs of phrases.  5. Phrase ranking We examined two ways of scoring the pairs of phrases (s, t). Both are estimates of the conditional probability p(t|s). The ﬁrst estimator is relative frequency (equation 5) which, as mentioned earlier, largely overestimates the probability of rare phrases. Table 1 reports the frequency distributions of the pair of phrases observed for different settings on the training corpus (20 000 pairs of sentences). Approximatively 90% of the observed pairs appear only once in the training corpus, and around 70% of the parameters are set to unity by the relative frequency estimator. An alternative is to resort to IBM model 1 [16] to score a pair. This is done by computing equation 6. 33  Algorithm 1 A Koehn-Tilmann-like variant for phrase ex-  traction Require: P, R, minLength, maxLength, ratio  Ensure: res contains all the pairs of phrases  1: Initialization  2: res ← {}  3: for all x : 1 → |S| do T [x] ← {}  4: for all y : 1 → |T | do T [y] ← {}  5:  6: Step1: P-projection  7: for all (x, y) ∈ P do add(x, y)  8:  9: Step2: Extension  10: for p : 1 → 2 do  11: repeat  12:  a ← {}  13:  for s : 1 → |S| do  14:  for all t ∈ T [s] do  15:  if p = 2 then  16:  neighbor(x-1,y-1); neighbor(x+1,y-1);  17:  neighbor(x-1,y+1); neighbor(x+1,y-1);  18:  else  19:  neighbor(x-1,y); neighbor(x+1,y);  20:  neighbor(x,y-1); neighbor(x,y+1);  21: for all (x, y) ∈ a do add(x, y)  22: until |a| = 0  23:  24: Step3: Collect independent boxes  25: b ← {}  26: for x : 1 → |S| do  27: X ← {x}; Y ← {}  28: repeat  29:  Xm ← X; Ym ← Y  30: for all x ∈ X do Y ← Y ∪ T [x]  31:  if Y != Ym then  32:  for all y ∈ Y do X ← X ∪ T [y]  33: until X = Xm and Y = Ym  34:  b ← b∪  (min{x : x ∈ X}, max{x : x ∈ X}), (min{y : y ∈ Y }, max{y : y ∈ Y })  35: x ← max{x : x ∈ X} + 1  36:  37: Step4: Combine boxes  38: for i : 1 → |b| do  39: let ((xmi , xMi ), (ymi , yMi )) = bi 40: add(xmi , xMi , ymi , yMi ) 41: for j : i + 1 → |b| do  42:  let ((xmj , xMj ), (ymj , yMj )) = bj  43:  if xMi + 1 = xmj then  44:  add(xmi , xMj , ymi , yMj )  Algorithm 2 A String-based phrase extractor  Require: T = {(Ei, Ci), i ∈ [1, |T |]}, a training corpus Ensure: res contains the pair of phrases  1: Initialization  2: res ← {}  3: for i : 1 → |T | do  4: res ← res ∪ (Ei, Ci)  5:  6: Applying compositionality  7: repeat  8: if (E1, C1) ∈ res then  9:  if (E2, C2) ∈ res then  10:  if C2 = C1α or C1 = C2α then  11:  if E2 = E1β or E1 = E2β then  12:  res ← res ∪ (β, α)  13: until convergence of res  15 to 16 English reference translations2. It was available four weeks before the ofﬁcial test and was used to gain some expertise on the phrase-based models.  Table 2: Main characteristics of the corpora used in this study.  corpus TRAIN TRAIN-A TRAIN-Q CSTAR TEST  |pair| 20 000 11 884 8 116 506 500  Chinese tokens words  182 904 112 000 70 904  7 643 6 456 4 024  3 515 870 3 794 893  English tokens words  188 935 7 181  116 343 6 008  72 592 3 900  –  –  –  –  |(t, s)|  prel(t|s) = |t|  (5)  |T |  pibm(t|s) = (|S| + 1)−|T |  p(ti|sj) (6)  i=1 j∈[0,|S|]  6. Corpora During this exercise, we only used the corpora made available by the organizers, the characteristic of which are reported in Table 2. No pre-processing was done to try to reinforce the parallelism between the two languages. Neither did we try to account for class of tokens such as numbers or dates. We did not change either the tokenization provided, but did convert the English into lowercase. Punctuation marks were left as is in the corpora, but removed after translation, as required by the organizers. The TRAIN corpus was split into TRAIN-Q and TRAIN-A corpus, gathering interrogative and afﬁrmative sentences respectively. See section 7.5 for the motivations behind this split. The CSTAR corpus contains 506 Chinese sentences with  Table 1: Frequency distribution of pairs of phrases observed in the training corpus for different values of minLength and maxLength. A ratio of 2.0 was applied. %f1, %f2 and %f3+ stand for the percentage of parameters (pairs of phrases) seen 1, 2 or at least 3 times in the TRAIN corpus. %p = 1 stands for the percentage of parameters that have a relative frequency of 1.  min max |model| %f1 %f2 %f3+ %p = 1  
We discuss phrase-based statistical machine translation performance enhancing techniques which have proven effective for Japanese-to-English and Chinese-toEnglish translation of BTEC corpus. We also address some issues that arise in conversational speech translation quality evaluations. 1. Introduction IBM spoken language translation system is based on a statistical translation model introduced in [1]. We adopt a phrase translation model as the baseline, for which the unit of translation is a phrase consisting of one or more words, [2], [3], [4], [5], [6]. The baseline system is augmented by the morphological analysis detailed in [7] for an improved word alignment and phrase selection. System performance is significantly improved by phrase selection from recall oriented word alignments (see Section 2 for the definition) and filtering. Re-ordering of source language sentence into the target language word order, [21], [22], further improves phrase selection and word order accuracy. Non-monotone decoding and language model probability computation for every word in a target phrase enhances the translation quality over monotone decoding and language model probability computation only for words at phrase boundaries. In Section 2, we give an overview of the baseline system. In Section 3, we discuss translation quality enhancing techniques along with experimental results. In Section 4, we address some issues in conversational speech translation evaluation. We discuss future work in Section 5. We use the term block (b) to denote a phrase translation pair consisting of a source phrase ( f ) and a target phrase ( e ). We use the symbol Pr(·) to denote general probability distribution and p(·) to denote model-based probability distribution. 2. Baseline System Overview Our baseline phrase translation system described in [Tillmann 2003] consists of three major components: word alignment, block selection, and decoding.  2.1. Word Alignment We obtain word alignment between the source and the target language sentences by successive application of IBM Model 1 viterbi alignment for initialization and iterative HMM-based alignment, [8], for refinement. We align a parallel corpus bi-directionally: one from the source language to the target language (A1: f → e) and the other from the target language to the source language (A2: e → f), where f denotes a source word position and e a target word position. We define precision (AP) and recall (AR) oriented alignments as follows: AP = A1 ∩ A2 AR = A1 U A2 AP is the intersection of A1 and A2, a high precision alignment. AR is the union of A1 and A2, a high recall alignment. The set of all source word positions covered by some word links in AP are denoted as col(AP). 2.2. Block Selection Starting from a high precision word alignment AP, we obtain blocks according to (i) a projection algorithm and (ii) a block extension algorithm. Projection Algorithm: We first project source intervals [f´, f], where f´, f ∈ col(AP). We compute the minimum target index e´ and maximum target index e for the word links that fall into the interval [f´, f]: [f´, f] → [ min e´, max e] e′ ∈ Pf ([ f ′, f ]) e ∈ Pf ([ f ′, f ]) Pf(·) projects source intervals into target intervals. The pair ([f´, f], [e´, e]) defines a block alignment link a. The block consisting of the target and source words at the link positions is denoted as b. Target and source words in a block are subject to the contiguity condition. Extension Algorithm: We expand the alignment links to include alignment points in the neighborhood of the high precision alignment AP and lie within the high recall alignment AR. The extensions are carried out iteratively until no new alignment links from AR are added. Among the candidate blocks obtained according to  39  the projection and extension algorithm, blocks  satisfying the following three conditions are kept for use  in translation:  i.  Source phrase ( f ) length ≤ 10 morphemes1  ii.  Target phrase ( e ) length ≤ 10 morphemes  iii. Block (b) frequency > 1  2.3. Decoding  Two types of model parameters, block unigram model and word trigram language model, are used in the baseline decoder. Block unigram probability is defined in (1), where n is the number of distinct blocks: count (b) (1) p(b) = n ∑ count (bi) i=0 Word trigram probability is computed at target phrase boundaries only, skipping over words within a target phrase in case the target phrase length ≥ 2. Trigram language model probability between adjacent target phrases is computed, as in (2).  (2) p(e i | e i − 1) = p(e1| e h , e h − 1)  ei is the current target phrase, e i − 1 is the previous (one or more) target phrase in the hypothesis. e1 is the first word of ei 2 , eh the last target word in the hypothesis and eh-1 the second to the last target word in the hypothesis. The task of the decoder is to find the block sequence that maximizes the product of the unigram block probability and the trigram language model probability without reordering. In decoder implementation, we use a DP-based beam search procedure. We start with an initial empty hypothesis. We maximize over all block segmentations b1, n, where n is the number of blocks covering the input sentence, with the source phrases yielding a segmentation of the input sentence, generating the target sentence simultaneously. The decoder processes the input sentence ‘cardinality synchronously’, i.e. all partial hypotheses active at a given point cover the same number of input words. We prune out weaker hypotheses based on the cost (for block unigram probability and trigram language model probability) they incurred so far. The cheapest final hypothesis − the hypothesis with the highest probability − with no untranslated source words is the translation output.  
This system is an experiment of examples based approach. It is based on a corpus containing 220 thousand sentence pairs with word alignment. The system contains four parts: matching and search, fragment matching, fragment assembling, evaluation and post processing. We use word alignment information to find and combine fragments. 1. Introduction This system is our first experiment of example based approach. It is based on corpus with word alignment. The corpus contains 220 thousands of news, literal, dictionaries, dialog sentence pairs. All sentence pairs are POS tagged and word aligned. 2. System Architecture The system has two parts: corpus and program. The corpus includes 220 thousand sentences pairs and a 460 thousand words and phrases dictionary. The program has four parts: matching and search, fragment matching, fragment assembling, evaluation and post processing. 2.1. Matching and searching Search for the most similar sentences from corpus. 2.2. Fragment matching Find out all matching and non-matching fragment of example sentence and corresponding fragment of translation of example sentence. 2.3. Fragment assembling Assemble the fragment into a full sentence. 2.4. Evaluation Evaluation the result of translation and determine keep or discard a non-aligned part. 2.5. Post processing Process spaces, cases and punctuations. 3. Corpus This system uses a corpus having 220 thousand sentences pair. It contains news, literal, dictionaries, dialog, etc. The sentences in corpus are POS tagged and word aligned.  3.1. POS tag The source language of the corpus is Chinese, and target language is English. For Chinese POS tagging, we use the ICTCLS 2.0. It’s developed by ICT. It uses Multi Layer HMM. In former test of Chinese High Technology Development and Research, it got 97.58% of accuracy and could process 31.5Kb characters per second. 3.2. Word alignment Word alignment is the base of our system. The arithmetic of word alignment is based on dictionary. It uses large scale bilingual dictionary, Word-Net and other human-readable dictionary. It is inspired by Ker’s method [4]. This method mainly depends on similarity measured by bilingual dictionary, relative distortion information and Part-of-Speech information to align words. By setting alignment window it acquires many-to-many word alignments. On a test set of 650 translation sentence pairs of Chinese and English, in which Chinese sentence has 24.8 words in average and English 34.5, the word alignment system gets a result of recall 62.9 ％ at the precision of 84.0%. Our algorithm is improved on Ker’s in these aspects: (1) The computation of relative distortion of Ker is improved, and the initial alignment anchors chosen by dictionary-based word similarity is added to improve alignment. (2) Proposed a concept of ‘alignment window’. By setting alignment window in the aligning process, many-to-many word alignments can be found. 3.3. Dictionary The system uses a 460 thousand words and phrases bilingual dictionary. The dictionary is the base of word alignment and translation. 4. Matching and searching This step is searching for the most similar sentence pairs from example base. There are two problems: how to measure the similarity of two sentences and how to find out the most similar sentence from a large scale corpus. 4.1. Measure of similarity The measurement of similarity determines whether or not find out the most fitted example for translation. Because our system is based on word, we must find out the longest match fragment for translation. We use follow formula to measure the similarity, m = ∑ w( pos(i)) * match(i) * w2(i)  47  The first item w is the weight of POS, different POS has different weight. In our system, verbs have the largest weight and stop words and named entities have the lowest weight. The second item match is 1 if the corresponding words are matched and 0 if not. The last item w2 is the measure of concatenation of words. If there are more concatenated words matched, w2 value is larger. In our system the value of w2 calculated as follow, w2 = l 2 l is the length of matching string.  string “was washed away” is matched (labeled as 1), then “bridge” (labeled as 2) is not matched, but there is a noun “web” is funded, so “bridge” is matched. In this example, there is only one matched fragment, because (1) and (2) are concatenated. 5.2. Target fragment After finding out the matched fragments, we must find out their corresponding fragment in target example. The word alignment is the guild of this step.  Figure 1: An example of similarity. Figure 1 give out a example of similarity, for source sentence S, there are two most similar sentences S1 and S2, and the similarity of S1 is 0.1651 and the similarity of S2 is 0.1547. For the similarity calculation is bi-direction, the largest value is 2.0 and the lowest values is 0. 4.2. Searching This step is the most time consuming step of the system. Actually, the efficiency of the step determines the efficiency of translation. In our system, an index is created for very words appeared in corpus. So, we can find all sentences which contain certain words. In the system, we search the example base for each word in source sentence orderly, and the searching results are joined into a set. Through some experiments, we know some words are not helpful for finding out the most similar example and consume much long time. So, we exclude highest frequency words from searching. After searching, the most similar examples are selected as final candidate set. 5. Fragment matching and assembling The key of EBMT system is how to split sentence into fragments and how to assemble the fragments into sentence. The familiar methods are based on parsing or word. We adopt word-based method. 5.1. Fragment matching This step is finding out all matching or non-matching fragments from source sentence and example sentence. First, we find out all matched words, and then find all mismatched words but have same part of speech. Figure 2: An example of fragment matching. Figure 2 shows a procedure of matching. S is the source sentence, and S1 is the examples. First, the matching word  Figure 3: Word alignment. Figure 3 shows an example of word alignment. The upper sentence is source example, and lower sentence is corresponding target sentence. There are 3 pairs, (“qiao”, bridge), (“chongzou”, washed) and (“chongzou”, away). As an example, we assume that “qiao” and “bei chongzou le” are different matched fragments. The first fragment “qiao” has one aligned pair, then “The bridge was” is the corresponding target fragment. The second fragment “bei chongzou le” has two aligned pairs, then “was washed away” is the corresponding target fragment. In this example, the word “was” is contained in both target fragments. It will be processed in later step. 5.3. Fragment assembling After finding out the target fragments, there are several matched target fragments and non-matched target fragments. For non-matched fragments, it must be searched in candidate set again for most similar examples. If can not find more example, it will be translated using dictionary. Actually, word alignment determines the positions of target fragments. 
Focus of this paper is the system for statistical machine translation developed at ITC-irst. It has been employed in the evaluation campaign of the International Workshop on Spoken Language Translation 2004 in all the three data set conditions of the Chinese-English track. Both the statistical model underlying the system and the system architecture are presented. Moreover, details are given on how the submitted runs have been produced. 1. Introduction This paper reports on the participation of ITC-irst to the evaluation campaign organized by the International Workshop on Spoken Language Translation (IWSLT) 2004. The Statistical Machine Translation (SMT) system developed at ITC-irst was applied to all the three data set conditions of the ChineseEnglish track. The ITC-irst SMT system implements an extension of the IBM Model 4 as a log-linear interpolation of statistical models, which estimate probabilities in terms of phrases. The use of phrases rather than words has recently emerged as a mean to cope with the limited context that Model 4 exploits to guess word translation (lexicon model) and word positions (distortion model) [1, 2, 3, 4, 5, 6, 7]. While parameters of the models are estimated exploiting statistics of phrase pairs extracted from word alignments, the weights of the interpolation are optimized through a training procedure which directly aims at minimizing translation errors on a development set. Decoding is implemented in terms of a dynamic programming algorithm. The paper is organized as follows. Next section details the statistical model underlying the system. Sections 3 and 4 brieﬂy describe the search and the segmentation algorithms, respectively. Section 5 gives an overview of the system architecture. Finally, in Section 6 experimental set-ups of the evaluation campaign runs and results are presented and discussed.  2. Statistical Machine Translation The advantages of the statistical translation approach are advocated by the many papers on the subject, which followed its ﬁrst introduction. Of course, there have been also attempts to overcome some of its shortcomings, e.g. the use of limited context within the foreign string to guess word translations and word positions. Recently, several research labs have reported improvements in translation accuracy by shifting from word- to phrase-based SMT. In particular, statistical phrasebased translation models have recently emerged, which rely on statistics of phrase pairs. Phrase pairs statistics can be automatically extracted from word-aligned parallel corpora [5]. In the following subsections, we introduce the SMT framework and the Model 4. Then, we brieﬂy describe a method for extracting phrase pairs. Finally, a novel phrase-based translation framework is presented which is tightly related to Model 4. 2.1. Log-linear Model As originally proposed by [8], the most likely translation of a foreign source sentence f into English is obtained by searching for the sentence with the highest posterior probability:  e∗ = arg max Pr(e | f )  (1)  e  Usually, the hidden variable a is introduced:  e∗ = arg max Pr(e, a | f )  (2)  e  a  which represents an alignment from source to target positions. The framework of maximum entropy [9] provides a mean to directly estimate the posterior probability Pr(e, a | f ). It is determined through suitable real valued feature functions hi(e, f , a), i = 1 . . . M , and takes the parametric form:  pλ(e, a | f ) =  exp{ i λihi(e, f , a)} e,a exp{ i λihi(e, f , a)}  (3)  51  The maximum entropy solution corresponds to values λi that maximize the log-likelihood over a training sample T :  λ∗  =  arg  max λ  (e,f ,a)∈T  log  pλ(e,  a  |  f  )  (4)  Unfortunately, a closed-form solution of (4) does not exist. An iterative procedure converging to the solution was proposed by [10]; an improved version is given in [11]. If the following feature functions are chosen [12]:  h1(e, f , a) = log Pr(e) h2(e, f , a) = log Pr(f , a | e) exploiting eq. (3), eq. (2) can be rewritten as:  e∗ = arg max Pr(e)λ1 Pr(f , a | e)λ2 (5) e a where λi’s represent scaling factors of factors. In eq. (5), English strings e are ranked on the basis of the weighted product of the language model probability Pr(e), usually computed through an n-gram language model [13], and the marginal of the translation probability Pr(f , a | e). In [8, 14] six translation models (Model 1 to 6) of increasing complexity are introduced. These alignment models are usually estimated through the Expectation Maximization algorithm [15], or approximations of it, by exploiting a suitable parallel corpus of translation pairs. For computational reasons, the optimal translation of f is computed with the approximated search criterion:  e∗ ≈ arg max Pr(e)λ1 max Pr(f , a | e)λ2  (6)  e  a  2.2. Model 4  Given the string e = e1, . . . , el, a string f and an alignment a are generated as follows: (i) a non-negative integer φi, called fertility, is generated for each word ei and for the null word e0; (ii) for each ei, a list τi, called tablet, of φi source words and a list πi, called permutation, of φi source positions are generated; (iii) ﬁnally, if the generated permutations cover all the available source positions exactly once then the process succeeds, otherwise it fails. Fertilities ﬁx the number of source words to be aligned to each target word, and the total length of the foreign string. Moreover, as permutations of Model 4 are constrained to assign positions in ascending order, it can be shown that if the process succeeds in generating a triple (φl0, τ0l, π0l ), then there is exactly one corresponding pair (f , a), and vice- versa. This property justiﬁes the following decomposition of Model 4:  pθ(f , a | e) = p(φl0, τ0l, π0l | el0)  (7)  = p(φ, τ , π | e)  (8)  = p(φ | e) · p(τ | φ, e) · p(π | φ, τ , e) (9)  where  l  l  p(φ | e) =  p(φi | ei) p(φ0 | φi) (10)  i=1  i=1  l  p(τ | φ, e) =  p(τi | φi, ei)  (11)  i=0  p(π | φ, τ , e)  =  
For the translation of text and speech, statistical methods on one side and interlingua based methods on the other have been used successfully. However, the former requires programming grammars for each language, plus the design of an interlingua, while the latter requires the collection of a large parallel corpus for every language pair. To alleviate these problems, we propose an approach that combines the advantages from both worlds. The proposed approach makes use of English or enriched English as an interlingua and can cascade data-driven translation systems into and from this interlingua. We show that enriching English with linguistic information that is automatically derived i only on English data performs better than pure cascaded systems. 1. Introduction In recent years, a number of translation approaches have been proposed to provide for reliable meaningful translation of text and of speech from one language to another. These include direct approaches (statistical, example-based), transfer and interlingua based approaches. Translation performance is usually the one (if not the most important) consideration for the evaluation of these systems, but has to be balanced by considerations of robustness and portability as additional important dimensions to the translation problem. The translation of speech, in particular, is faced with both of these additional challenges: the input speech and its recognition is fragmentary, ill-formed and errorful, and speech translation systems are frequently required to handle multiple language pairs and language directions to allow for successful cross-lingual dialogs between humans. To accommodate these additional constraints a popular approach has been the interlingua approach to translation. Here, an intermediate representation of meaning is chosen to express the key idea or intent of the speaker. An input sentence is parsed in terms of its key semantic content, represented in an interlingua structure, and from there an equivalent sentence is generated in another language. The use of an interlingua has several advantages. First, adding a new languages to an existing system is simplified, since a new language has to translate only into and out of the interlingua, and we do not require separate translators for every other language pair the system supports. Second, the translation step extracts only the key intentions from a speaker's utterance, thereby handling colloquial expressions, and reducing the sensitivity to redundancies, and disfluencies in spoken language. Third, the system can generate paraphrases from the interlingua back into one's own language to provide meaningful feedback and verification of the translation, before it is delivered into another language.  The advantages, however, come at a price: The extraction of the key content from a disfluent input sentence requires the development of semantic grammars that extract key information into frames, concepts and slots. Both, the design of a suitable, unambiguous and language independent interlingua as well as the development of grammars that map sentences to meaning are domain-dependent and have to be repeated for each topic or domain. Their development is labor intensive and requires both linguistic expertise and command of the language at hand. As an attempt to solve these problems, automatic learning is proposed to alleviate the manual development work. The most popular approaches at present are statistical and example-based methods. Both extract direct mappings from input to output language using large parallel corpora between these languages. Statistical machine translation permits automatic statistical learning to build a translator rather than manual programming. But a system has to be developed for each language pair. Each translator, in turn, requires a large parallel corpus for training. While parallel corpora are generally available for large common languages, it is rare to find large parallel corpora for more unusual language pairs (say, PaschtuCatalan) and domains. In this paper, we therefore develop an alternative strategy: the use of general English and a linguistically enriched English as interlingua. Here we avoid the manual design of an interlingua, and the writing of grammars for analysis and generation; but we also avoid the need for large parallel corpora for every language pair. Moreover, English as interlingua can be ‘enriched’ by linguistic information extracted in a data-driven fashion automatically and monolingually in English, where plenty of data exists. 2. Description of the EDTRL System In this section we describe the Error Driven Translation Rule Learning (EDTRL) translation system. The EDTRL system uses enriched English as an interlingua to translate from a source language into a target language going through described special interlingua as an intermediary step. This approach tries to combine the advantages of a system with an explicit interlingua and the advantages of a pure data-driven system. Thereby it becomes possible to add a new language to a given system with n languages very fast by only adding 2 components instead of n-1. The use of enriched English as an interlingua eliminates the need for an explicit, handcrafted interlingua specification and removes the domain limitation which is typical for interlingua-based translation systems. An additional benefit of this combination is the reduction of the ‘Parallel Data Sparseness Problem’. For most nonEnglish language pairs the amount of parallel text corpora is much smaller than the parallel text corpora from each of  61  these languages paired with English. Using English as an interlingua can therefore increase the amount of available training data.  2.1. Basic Design Ideas of EDTRL The EDTRL system is based on statistical transfer rules which are automatically learned from bilingual corpora. While the system can learn transfer rules from two non English languages and acts like a direct data-driven translation system, it is designed to use augmented, formalized English as Interlingua. Thus one language of the parallel corpus has to be English, and has to be standardized and annotated with additional linguistic information. The annotation and standardization process only depends on the English part of the parallel corpus and is consequently independent of the source and target language of the system. Annotations made on the English side are projected through the word nad phrase alignment models onto the source and target language. Some mapping errors are introduced by transferring the structural knowledge from English to some other language, but often that can be compensated through the higher quality and quantity of the available structured knowledge in English compared to most other languages. To allow for the use of a translation system in real-wordapplications a small footprint in memory and space as well as a fast translation process are important. To achieve these goals we decided instead to keep the whole statistical translation model to generate statistical rules from the model. Even these generated rules are to many to build a small and fast system, therefore we keep only a subset of all rules generated during the training and use an evaluation test set to determine the most significant and important rules. This allows us to find the best compromise between size and performance for each application domain. The use of probabilistic translation rules makes it easy to add new rules and even exceptions of existing rules. It also allows tracking translation errors and correcting them if necessary. This ability also leads to an interactive learning modus, where the user can teach the system and optimize its behavior.  2.1.1. Standardized and Simplified English  The standardization step tries to map alternative expressions  with similar or equal meanings to the most common used  alternative. Furthermore the sentence structure is simplified  [SE]. E.g. more complex rarely used tenses are replaced by  easier ones:  He had spoken.  He spoke.  He would be speaking.  He would speak.  These kinds of simplifications of course remove information,  but often such fine nuances are of little value to the quality of  the translation given the current state of the MT systems. In  most cases the translation profits from the transformations  through more reliable alignments and better utilization of the  training data.  Even humans can benefit from Simplified English in  some technical domains [AECMA]. Sometimes English  utterances have some freedom in word order without  changing the main meaning of the utterance. To obtain a  consistent word order some simple rules are applied:  E.g. please give me …  give me … please  2.1.2. Preserve translation alternatives The translation errors from the intermediate English to the target language can be reduced if not only the best hypothesis, but additional information from the search is used. We examined the following methods: n-best list of complete translations: The translation system produces up to n alternative translation hypotheses and passes them to the second translation step. The number of hypotheses has to be kept small to guarantee fast overall decoding, thereby allowing only for little variability. n-best word or phrase alternatives to the best hypothesis: This method selects the single best hypothesis from the first translation step, but augments it by adding alternative words or phrases, which have high translation probabilities. Full lattice: In order not to fix one translation hypothesis as the basis for constructing these alternatives, we can also pass on full translation lattices. Using a lattice as input for the second translation step has been shown as the most profitable way to use translation alternatives to improve the translation quality. 2.1.3. Additional knowledge sources Besides translation alternatives, further information on the structure and the semantic content of a sentence can be helpful. Therefore we incorporated the following additional knowledge sources into our system to provide information for the translation process: Morphological Analyzer: Starting from the WordNet ontology [WordNet] we built a system to analyse an English word form and determine its base form and derivation rule. The analyzer contains a set of common transformation rules and an even larger list of exceptions from these rules. In the current implementation, each word is analysed without using its context or information from former sentences. The precision for finding the base class is over 95% while the determination of the derivation rules is not yet that good. Sense Guesser: The sense guesser tries to find the sense of a word. Many words have different meanings depending on the context in which they occur. E.g. table can have the senses ‘desk’ or ‘chart’. Often the context of the word can be used for disambiguation. In our example, the context ‘in the’ assigns table to the chart-class, while ‘on the’ assigns it to the desk-class. We used the sense hierarchy from WordNet. Synonym Generator: WordNet also lists synonyms for words, all within the well structured and linked hierarchy. Both Sense Guesser and Synonym Generator only use open word classes like nouns, verbs, adjectives, and adverbs. Part-of-Speech Tagger: a statistical Part-of-Speech tagger was used to provide POS-tags. The tagger uses the tag set which is described in [Brill 1995] and was trained on the tagged Brown Corpus. Named Entity Tagger: a prototype of some handwritten rules allows us to find named entities, which often should be treated in a special way. Further knowledge sources like sentence type, active or passive voice, politeness, domain or category could also be added.  62  2.1.4. Probabilities and Confidence Measures Besides the information from the different sources also a probability or confidence measure for each of the knowledge sources and alternatives is added to the Interlingua. Therefore words and phrases carry attributes with possibilities and their possible alternatives. All this combined additional information forms the interlingua (intermediate representation) for the EDTRL system. For translating into English the interlingua can easily be transformed into plain English by stripping off all additional information and using the most likely alternative. For translating from English into some other language the additional information can be added directly, i.e. transforming plain English into the annotated form which is then used as the interlingua. 2.2. Training and Translating In the ideal case the learning process only needs parallel texts and optional dictionaries to and from English, because all other knowledge sources operate on English and are independent from the input and output language. However available direct parallel texts or dictionaries from the source to the target language could be incorporated into the system. 2.2.1. Statistical Alignment In a first step a word alignment (IBM1 or modified IBM2) is performed. In a second step a phrase alignment based on the word alignment is executed, which simultaneously joins similar regions on the word alignment matrix and splits the matrix into smaller parts. For these splitting and joining operations normalized probabilities from the word alignment and the language models are used. The phrase alignment generates a collection of partitions of the word alignment matrix and their probabilities. 2.2.2. Weight Functions for the Alignment To enhance the quality of the statistical alignment, weight functions are introduced, which change the weights of a sentence alignment in a special manner according to a heuristic concept. Different weight functions are examined. A) The Weight Position Factor takes into account, that in parallel sentences the source word positions are not independent from the corresponding translation word positions. Often they lie next to the diagonal of the alignment matrix. The following formula can give them a higher weight. k £¤¥ ¡¢ WordAPos − WordBPos • #WordsA #WordsB B) The Length Penalty consider the assumption that longer utterance often results in less accurate alignments and so they are punished using the following expression k log(len) C) Parallel utterances of significant different length often produce alignments of minor quality. Therefore the Matching Length Factor prefers utterances with almost the same length. k # LenA+# LenB 2⋅ max(#LenA,# LenB)  D) The Frequency Weight keep in mind, that alignments between words with similar frequency are typically more accurate than between words with very different frequency counts. k #WordsA+#WordsB 2⋅ max(#WordsA, #WordsB) Each function is parameterized and its parameters are estimated on a validation set. The Weight Position Factor gives the far best improvement for the alignment quality, compared to a manually alignment. It reduces the alignment error by 13.1% while the other weight functions give an improvement from 1.5% to 3%. A combination of all four alignment functions reduces the error by 14.6%. Besides the four weight functions many other functions are imaginable and can be examined.  2.2.3. Rule Generation and Selection On the basis of the phrase alignment, optional dictionaries and the semantic and morphologic knowledge translation rules are generated. Optimal rules should be accurate (not introducing errors in other translation contexts) and should not be too specific, so that they can be applied ¦ frequently. Rules are of the form: Cond1 | Cond2 | … Templ1 | Templ2 | … where Cond can be a word or phrase containing attribute classes and Templ is a template which has to be instantiated during the translation process. Both Cond and Templ carry probabilities. Most attribute classes are part of a hierarchy. This allows enforcing a match by walking up the tree to a more common representation while at the same time decreasing the rule score. A set of meta-rules controls the construction process. Every time a translation rule contradicts with the training data, the rule is split and new attributes are added to resolve the error. In order not to get too many rules, each rule is checked for its efficiency on a validation set.  2.2.4. The Translation Process  The translation process tries to match and instantiate rules along the input utterance. This results in a search tree which needs to be pruned if it grows too large in size. A beamsearch then gets the best hypothesis weighted by a trigram language model. Both directions, to and from the interlingua, are very similar, which is shown in the following simple example. In each direction explicit language knowledge is only used for the English part.  A) Translation: Chinese -> IL (Tagged English): § ¨Input: ©      ! ¨ ©"   # Rules:  <1> <2>  I’ve <VB> a <Disease>  $%# from someone 0.6 infection <NN> 0.3 | transmission <NN> 0.1 |  & # infect <VB> 0.2 | catch <VB> 0.1 cold <Disease> 0.3 | rheum <Body Substance> 0.2 |  to catch cold <VB,Change> 0.4  Instantiation of the first rule: => I think I've caught a cold from someone  63  B) IL (Tagged English) -> Chinese (or Spanish) :  
In this paper we describe the components of our statistical machine translation system used for the spoken language translation evaluation campaign. This system is based on phrase-to-phrase translations extracted from a bilingual corpus. A new phrase alignment approaches will be introduced, which ﬁnds the target phrase by optimizing the overall word-to-word alignment for the sentence pair under the constraint that words within the source phrase are only aligned to words within the target phrase. The system will be used for Chinese-to-English translations under the small, additional and unlimited data conditions, and for the small Japanese-to-English translation track. 1. Introduction Statistical machine translation (SMT) is currently the most promising approach, esp. to large vocabulary text translation. In the spirit of the Candide system developed in the early 90s at IBM [Brown et al. 1993], a number of statistical machine translation systems have been presented in the last few years [Wang and Waibel 1998], [Och and Ney 2000], [Yamada and Knight 2000]. These systems share the basic underlying principles of applying a translation model to capture the lexical and word reordering relationships between two languages, complemented by a target language model to drive the search process through translation model hypotheses. Their primary differences lie in the structure and source of their translation models. Whereas the original IBM system was based on purely word-based translation models, current SMT systems try to incorporate more complex structure. The statistical machine translation system developed in the Interactive Systems Laboratories (ISL) uses phrase-to-phrase translations as the primary building blocks to capture local context information, leading to better lexical choice and more reliable local reordering. A new approach to extract phrase translation pairs from bilingual data has been developed, which is not using the  Viterbi alignment, but is based on optimizing a constraint word-to-word alignment for the entire sentence pair. This is described in Section 2.1. Finding good phrase translation pairs is very important. But as a source phrase can have alternative translations, it is also necessary to assign meaningful probabilities to those alternatives. Typically, longer phrases are seen only a few times. Probabilities estimated from relative frequencies are therefore not reliable. We therefore calculate phrase translation probabilities based on the word-to-word translation probabilities, as described in Section 2.3. Section 3 outlines the architecture of the decoder that combines the translation and language model to generate complete translations. The BTEC corpus is a very limited domain corpus and therefore many test sentences are close to one or several sentences seen in the training data. We implemented and tested a simple translation memory component, which will be described in Section 4. Finally, in Section 5 we present a series of experiments in the Chinese-to-English and Japanese-toEnglish translation tasks. The Basic Travel Expression Corpus (BTEC) is used as domain-speciﬁc data [Takezawa et al. 2002]. Different data conditions are explored: small in-domain data only, using additional outof-domain data, and using a larger in-domain corpus. 2. The Models 2.1. Phrase Alignment The ISL translation system uses word-to-word and phrase-to-phrase translations, extracted from the bilingual corpus. Different phrase alignment methods have been explored in the past, like extracting phrase translation pairs from the Viterbi path of a word alignment, or simultaneously splitting source and target sentence into phrases and aligning them in an integrated way [Zhang 2003]. For the experiments reported in this paper a new phrase alignment method was explored.  65  2.2. Phrase Alignment via Constrained Sentence Alignment Assume we are searching for a good translation for one source phrase f˜ = f1...fk, and that we ﬁnd a sentence in the bilingual corpus, which contains this phrase. We are now interested in ﬁnding a sequence of words e˜ = e1...el in the target sentence, which is an optimal translation of the source phrase. Any sequence of words in the target sentence is a translation candidate, but most of them will not be considered translations of the source phrase at all, whereas some can be considered as partially correct translations, and a small number of candidates will be considered acceptable or good translations. We want to ﬁnd these good candidates. The IBM1 word alignment model aligns each source word to all target words with varying probabilities. Typically, only one or two words will have a high alignment probability, which for the IBM1 model is just the lexicon probability. We now modify the IBM1 alignment model by not summing the lexicon probabilities of all target words, but by restricting this summation in the following way: • for words inside the source phrase we sum only over the probabilities for words inside the target phrase candidate, and for words outside of the source phrase we sum only over the probabilities for the words outside the target phrase candidates; • the position alignment probability, which for the standard IBM1 alignment is 1/I, where I is the number of words in the target sentence, is modiﬁed to 1/(l) inside the source phrase and to 1/(I − l) outside the source phrase.  words and multiplying along the target words:  i1 −1  pi1,i2 (e|f ) =  p(ei |fj )×  i=1 j∈/(j1...j2)  i2 j2  I  p(ei |fj )  p(ei |fj )  i=i1 j=j1  i=i2+1 j∈/(j1...j2)  To ﬁnd the optimal target phrase we interpolate both alignment probabilities and take the pair (i1, i2) which gives the highest probability.  (i1, i2) = argmax{(1 − c)p(i1,i2)(f |e) + cp(i1,i2)(f |e)} i1 ,i2 Actually, we take not only the best translation candidate, but all candidates, which are within a given margin to the best one. All candidates are then used in the decoder, when also the language model is available to score the translations. The phrase pairs can be either extracted from the bilingual corpus at decoding time or stored and reused during system tuning. It should also be mentioned that single source words are treated in the same way, i.e. just as phrases of length 1. The target translation can then be one or several words.  2.3. Phrase Translation Probabilities Most phrase pairs (f˜, e˜) = (fj1 ...fj2 , ei1 ...ei2 ) are seen only a few times, even in very large corpora. Therefore, probabilities based on occurrence counts have little discriminative power. In our system we calculate phrase translation probabilities based on a statistical lexicon, i.e. on the word translation probabilities (p(f, e):  p(f˜|e˜) =  p(fj |ei ).  ji  More formally, we calculate the constrained alignment probability:  j1 −1  pi1,i2 (f |e) =  p(fj |ei )×  j=1 i∈/(i1..i2)  j2 i2  J  p(fj |ei )  p(fj |ei )  j=j1 i=i1  j=j2+1 i∈/(i1..i2)  and optimize over the target side boundaries i1 and i2.  (i1, i2) = argmax{pi1,i2 (f |e)} i1,i2  It is well know that ’looking from both sides’ is better than calculating the alignment only in one direction, as the word alignment models are asymmetric with respect to aligning one to many words. Similar to pi1,i2 (f |e) we can calculate pi1,i2 (e|f ), now summing over the source  2.4. The Language Model The language model used in the decoder is a standard 3gram language model. We use the SRI language model toolkit [SRI-LM Toolkit] to build language models of different sizes, using the target side of the bilingual data only or using additional monolingual data.  2.5. Position Alignment Model  Different languages have different word order. In the  standard word alignment models this is captured by word  position models, e.g. absolute positions p(i|j, I, J) in  IBM2 alignment model or relative positions p(i|iprev, I)  in the HMM alignment model [Vogel et al. 1996]. We  use a simpliﬁed relative position model in our SMT de-  coder.  p(i|iprev ,  I)  =  e−  |i−iprev c  |  (1)  with a suitably chosen constant c. This constant is essentially a scaling factor for the model when combining it with the other models in the decoder.  66  2.6. Sentence Length Model Source sentence and target sentence are typically of different length. However, when using a large bilingual corpus to collect the sentence length statistics, it becomes clear that the probability distribution p(I|J), where J is the number of words in the source sentence and I is the number of words in the target sentence, is rather ﬂat and therefore does not seem to be very helpful. On the other side we observe that the language model typically prefers shorter translation. To compensate for this we use a simple sentence length model, which gives a constant bonus for each word generated. Putting a higher weight on the sentence length model contribution to the overall translation score results in generating translations, which are on average longer. 3. Decoding Statistical machine translation is based on the noisy channel approach:  allows for ﬂexible pruning, as the language model history, the translated position and the number of generate words can be used individually and in combination in pruning. Details have been described in [Vogel et al. 2003] and especially in [Vogel 2003]. 4. A Translation Memory Component 
This paper describes a Multi-Engine based Chinese-toEnglish spoken language translation system. The design and implementation of the system is given in detail. Three different translation engines are employed in the system and a very simple way is proposed to select the best translation from all the outputs generated by them. The evaluation results from IWSLT2004 are reported and analyzed in detail. The results prove that the Multi-Engine based system is practical. 1. Introduction Spoken language translation (SLT) technologies attempt to cross the language barriers between people with different native languages who each want to engage in conversation by using their mother-tongue. The importance of these technologies is increasing because there are many more opportunities for cross-language communication in face-toface and telephone conversation, especially in the domain of tourism. Our work described in this paper is focused on the translation from Chinese spoken language to English, an important part of the multi-lingual information service system oriented to the 28th Beijing Olympic Games. The remainder of this paper is organized as follows: Section 2 gives the related work of Multi-Engine based spoken language translations; Section 3 describes the overview of our system in detail, and then, Section 4 presents the evaluation results and finally Section 5 gives the conclusion. 2. Related work Many approaches have been proposed and practiced in the SLT research, such as template based method, statistical method and so on. Different translation methods are integrated together to get better results for that each machine translation method has its own strengths and weaknesses. At present, there are two main approaches to use different translation methods in one system. The great difference  between the two approaches lies in that the output of the first one is combined by the best translation parts generated by various engines but the output of the second approach is a whole result selected from the results generated by each engine [1, 2, 3, 4]. The center idea of the first one is that: different translation engines run together and each engine generates part rather than whole results of an input sentence. The part results created by one engine may be used by the other engines when they generate new part results. Finally a selector or searcher selects part results from all the created part results to compose a new result for the whole input sentence as the system output. This method is very complex and the builder must be familiar with each translation method. The centre idea of the second one is that: different translation engines run independently and get their own result for a complete input sentence, and then a selector selects the best result as the system output. This method does not create new result. It is comparatively easy to be implemented and the builder need not be familiar with every translation method. So it is easy to integrate new translation engines. Our system integrates three different translation engines which are developed independently and each can run separately. The three engines are: template-based translator (TBMT), inter-lingua based translator (IBMT) and statistic translator (SMT). The three engines all generate a result for a complete input sentence and they can not communicate with each other inside the system, so we select the second approach to accomplish our system. 3. Overview of system Based on the motivation mentioned above, our system is constructed by three different components: 1) the preprocessor; 2) a center controller; 3) translation engines (TBMT, IBMT and SMT). Our architecture is some different from that mentioned above because of the particularity of the  73  approach to select results. Fig. 1 shows the architecture of our system. Every component is described in detail next.  in  out  Preprocessor  Center controller  SMT  TBMT  out IBMT  out  Figure 1: System architecture  3.1. Preprocessor The preprocessor is designed to process the input sentences before translation. It completes the following functions: 1) To delete all repeated words except some special adverbs like “ （very）”, “ （very）” etc, this is very important for translation, especially for the template based translator because the repeated words in spoken language may influence the match between the input sentence and the templates. 2) To recognize and analyze the numerals and numeral phrases (QP) in the input sentences and translate the Chinese numerals into Arabic numerals. 3) To recognize and understand the time words and time phrases (TP) and translate them into English expression. 3.2. Center controller In our system, the basic function of center controller is to select the system output from the results translated by different translator engines. There is some difference between the selector’s function proposed by [3]. The selector in [3] works after all the translation engines separately have a result, but the center controller in our system works when any translator finished its work and it controls the whole program. According to our investigation, the results from the TBMT are generally of higher accuracy than the results from the SMT but its coverage is much smaller than that of the statistic method because no proper template can be found for a lot of sentences. The IBMT also has high accuracy but  now it is only oriented to the hotel reservation domain which is only a small subset of tourism domain. So its contribution to the whole system is very limited. Our center controller is designed very simple. If a sentence is inputted to be translated, the work processes of our system are given as follows: First, the sentence is translated by the template based engine (TBMT). If almost the whole sentence (more than 80% in length computed by words) matches a template, the controller outputs the results as the system output and ended the whole translation process. Otherwise the sentence is sent to the inter-lingua based translator (IBMT). If a result is created here, the center controller outputs the result and finishes the whole process. Otherwise the sentence is passed to the statistic translator and the statistic translator translates the sentence and the result is output by the center controller. 3.3. Translation Engines  3.3.1. Template based translators  The template-based machine translator (TBMT) is the first translation engine in our system. It uses flexible expression format to describe the template condition. The template is designed as:  C1C2 LCn ⇒ T  (1)  Where, n is an integer ( n ≥ i ≥ 1 ), Ci is a component which expresses a condition that the input utterance of source language has to meet. The utterance may be a Chinese word, a variable signed as N (noun), V (verb), A (adj) or other symbols, and also can be TP or QP recognized by the processor. T is the output result corresponding to the input and it also contains the variables, TP and QP appeared on the left side. The formula (1) means that if an input sentence of source language meets the conditions expressed by the left side, it will be translated to the target language expressed by the right side T . On the right side, the symbols TP, QP, N, V and other variants are replaced with their corresponding target language expression. For details of the template-based translation translator, please refer to [5].  74  3.3.2. Interlingua-based translation method  The inter-lingua based machine translator (IBMT) works  after the template based translator in our system. IF  (Interchange Format) developed by C-STAR (Consortium  for Speech Translation Advanced Research international) is  used as our inter-lingua. An IF consists of four parts: speaker,  speech-act, concept, arguments. For example, the Chinese  sentence “  ” whose responding  English is “I would like to make a hotel reservation in  Beijing” is expressed by the following IF: c: give-  information+ disposition +reservation +accommodation  (disposition=(desire,who=I),reservationspec=(reservation,id  entifiability=no),accommodation-spec=hotel,location=name-  beijing ). For detailed information about IF, refer to [6].  In the inter-lingua based translator, there are two key  components: a spoken Chinese analyzer and an IF based  English generator. The analyzer translates Chinese sentences  to IF and it is based on the combination of statistical and rule  method. The analyzer first analyzes the sentence into  semantic chunks using rule-based method, and then analyzes  the sentences using HMM-based statistical method. The  approach is of the merits of rule-based method to analyze the  deep semantic structure of sentence and it may keep the  good robustness of statistical method. For detailed  information about the analyzer, please refer to [7].The  generator is used to generate English sentences from IF. Our  generator employs a hybrid approach in combination of  template-based and feature-based generation methods.  Templates containing variables are used to generate English  from those fixed expressions in IF, and for the flexible  expressions the feature-based generation method is used.  The combination of the two methods makes the generator  have a good tradeoff between efficiency and flexibility. For  detailed information about the generator, please refer to [8]. 
In this paper, we describe the RWTH statistical machine translation (SMT) system which is based on log-linear model combination. All knowledge sources are treated as feature functions which depend on the source language sentence, the target language sentence and possible hidden variables. The main feature of our approach are the alignment templates which take shallow phrase structures into account: a phrase level alignment between phrases and a word level alignment between single words within the phrases. Thereby, we directly consider word contexts and local reorderings. In order to incorporate additional models (the IBM-1 statistical lexicon model, a word deletion model, and higher order language models), we perform n-best list rescoring. Participating in the International Workshop on Spoken Language Translation (IWSLT 2004), we evaluate our system on the Basic Travel Expression Corpus (BTEC) Chinese-to-English and Japanese-to-English tasks.  1. Introduction  The goal of machine translation is the translation of a text given in some source language into a target language. We are given a source string f1J = f1...fj...fJ , which is to be translated into a target string eI1 = e1...ei...eI . Among all possible target strings, we will choose the string with the highest probability:  n  o  eˆI1 = argmax P r(eI1|f1J )  eI1  n  o  = argmax P r(eI1) · P r(f1J |eI1)  (1)  eI1  The decomposition into two knowledge sources in Equation 1 is known as the source-channel approach to statistical machine translation [1]. It allows an independent modeling of target language model P r(eI1) and translation model P r(f1J |eI1). The target language model describes the well-formedness of the target language sentence. The translation model links the source language sentence to the target language sentence. It can be further decomposed into alignment and lexicon model. An alternative to the classical source-channel approach is the direct modeling of the posterior probability P r(eI1|f1J ). Using a log-linear model [2], we obtain:  P r(eI1|f1J )  =  pλM 1  (eI1 ů  exp  |f1J PM  ) λm  hm  (eI1  ,  f1J  ÿ )  =  P  ům=PM1  exp  ÿ  λmhm(e  I 1  ,  f1J )  e  I 1  m=1  The  hm  denote  the  feature  functions. (  As  a  decision  rule, )  we  obtain:  X M  eˆI1 = argmax  λmhm(eI1, f1J )  (2)  eI1  m=1  This approach is a generalization of the source-channel approach. It has the advantage that additional models or feature functions can be easily integrated into the overall system. The overall architecture of the log-linear model combination is summarized in Figure 1. The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language. We have to maximize over all possible target language sentences. In a way similar to [3], we train the model scaling factors λM 1 with respect to the ﬁnal translation quality measured by some error criterion, e.g. the NIST score [4], the BLEU score [5] or the word error rate (WER) [6]. The remainder of the paper is organized as follows: in section 2, we will outline the RWTH statistical machine translation system which introduces the alignment templates [7, 2]. We will describe the training and search procedure of our approach. For the Japanese-English task, we will show that reordering constraints improve translation quality compared to an unconstrained search. We will describe the additional features we integrate into our system. Section 3 will present experimental details and will show the translation results obtained for the Chinese-to-English and Japanese-toEnglish evaluation tasks. Finally, section 4 will conclude.  2. The RWTH SMT System A general deﬁciency of single-word based approaches is that contextual information is not taken into account because they are only able to model correspondences between single words. A countermeasure is to consider word phrases rather than single words as the basis for the translation models. In other words, a whole group of adjacent words in the source sentence may be aligned with a whole group of adjacent words in the target language. As a result the context of words has a greater inﬂuence and local reorderings can be learned implicitly..  2.1. Word level alignments The main feature of our translation model are the alignment templates. An alignment templates z is a triple (f˜, E˜, A˜) which describes the alignment A˜ between a source class sequence F˜ and a target class sequence E˜. The classes used in F˜ and E˜ are automatically trained bilingual classes using the method described in [8]. The use of classes instead of words themselves has the advantage of a better generalization. E.g., if a class ”town” is used in both source and target language and alignment templates are learned for special towns, it is possible to generalize these alignment templates to all towns.  79  Source Language Text  ? Preprocessing      ?   Global Search    ¡ eˆI1 = argmax  PM  ¿ λmhm(eI1, f1J)  eI1  m=1    ? Postprocessing      ? Target Language Text  λ1 · h1(eI1, f1J) λ2 · h2(eI1, f1J) ppp λM · hM(eI1, f1J)  Figure 1: Architecture of the translation approach based on log-linear model combination.  . visit a for hangzhou to go also will they Figure 2: Example of a word aligned sentence pair and some possible alignment templates. An alignment template is applicable to a sequence of source words if the alignment template classes and the classes of the source words are equal, and it constrains the target words to correspond to the target class sequence. For the selection of words from classes we use a statistical model for p(f˜|z, e˜) based on the lexicon probabilities of a statistical lexicon p(f |e). Figure 2 shows an example of a word aligned sentence pair. The word alignment is represented with the black boxes. The ﬁgure also includes some of the possible alignment templates, represented  .  as the larger, unﬁlled rectangles. Note that the extraction algorithm would extract many more alignment templates from this sentence pair. In this example, the system input was the sequence of Chinese characters without any word segmentation.  2.2. Phrase level alignments  In order to describe the phrase level alignments in a formal way, we  ﬁrst decompose both the source sentence f1J and the target sentence eI1 into a sequence of phrases (k = 1, . . . , K). For the alignment aK 1 between the word phrases, we obtain the following equation:  X  P r(f1J |eI1) =  P r(aK 1 , f1J |eI1)  X aK 1  =  P r(aK 1 |eI1) · P r(f1J |aK 1 , eI1)  aK 1  Further, we introduce the alignment templates as hidden variables  for the translation of the K phrases:  X  P r(f1J |eI1) =  P r(aK 1 |eI1) ·  aK 1 ,z1K  P r(z1K |aK 1 , eI1) · P r(f1J |z1K , aK 1 , eI1) (3)  Hence, we obtain three different probability distributions: the phrase alignment probability P r(aK 1 |eI1), the probability to apply an alignment template P r(z1K |aK 1 , eI1), and the phrase translation probability P r(f1J |z1K , aK 1 , eI1). The phrase translation probability is discussed in section 2.1. For a detailed description of modeling, training and search, see [7].  80  2.3. Feature functions To use the three component models of Equation 3 in a log-linear approach, we deﬁne three different feature functions taking the logarithm for each component of the translation model instead of one feature function for the whole translation model p(f1J |eI1). The feature functions have then not only a dependence on f1J and eI1 but also on z1K , aK 1 . Yet, we are not limited to train only the alignment model scaling factors, the RWTH SMT system consists of the following base models: • a phrase translation model, • a phrase alignment model, • a word translation model, • a word-based trigram language model, • a class-based ﬁve-gram language model, and • a word penalty model. These features allow a straightforward integration into the used dynamic programming search algorithm [7]. In addition, we extract nbest candidate translations using A∗ search [9] and perform rescoring, for which we make use of the following extended models: • the IBM-1 lexicon model as suggested by [10], • a deletion model: for each source word, we check wether there exists a target translation with a probability higher than a given threshold. If not, this word is considered as deletion and the feature simply counts the number of deletions, • additional language models: applying the SRI Language Modeling Toolkit [11], we train n-gram language models of increasing order. We combine these different features in a log-linear model [2].  2.4. Optimization of model scaling factors  As training criterion, we use the maximum class posterior probability criterion:  (  )  λˆM 1  =  argmax λM 1  X S log pλM 1 (es|fs) s=1  (4)  on a parallel training corpus of sentence pairs (fs, es), s = 1, . . . , S. This criterion allows for only one reference transla-  tion, but for our tasks there exist multiple reference translations. Hence, we change the criterion to allow Rs reference translations es,1, . . . , es,Rs for the sentence es:  λˆM 1  =  ( X S argmax  λM 1  s=1  
This paper introduces TALP, a speech-to-speech statistical machine translation system developed at the TALP Research Center (Barcelona, Spain). TALP generates translations by searching for the best scoring path through a Finite-State Transducers (FSTs), which models an Xgram of the bilingual language deﬁned by tuples. A detailed description of the system and the core processes to train it from a parallel corpus are presented. Results on the Chinese-English supplied task of the Int. Workshop on Spoken Language Translation (IWSLT’04) Evaluation Campaign are shown and discussed. 1. Overview of the system TALP (Traduccio´ Automa`tica del Llenguatge Parlat) is a speech-to-speech statistical machine translation system developed at the TALP Research Center (Barcelona, Spain) during the last years. It implements an integrated architecture by joining speech recognition and translation in one single step. Mathematically, the system produces a translation by maximizing the joint probability between source and target languages, which is equivalent to a language model of an special language with bilingual units (called tuples). TALP implements this tuple language model by means of a Finite-State Transducer (FST) considering an Xgram memory, that is, a variablelength N-gram model which adapts its length to evidence in the data. Xgrams have proved good results in speech recognition tasks in the past [1]. Given such a bilingual FST, the search for a translation becomes the search for the best-scoring path among the transducer’s edges. This search can be performed by dynamic programming, using well-known decoding techniques from the speech recognition domain. This way, the Viterbi algorithm and a beam search can be used forwards taking only source-language words into account (ﬁrst part of each tuple), reading words in the target language during trace-back to produce the translation. Using This work has been partially supported by the Spanish Government under grant TIC2002-04447-C02 (ALIADO project), the European Union under grant FP6-506738 (TC-STAR project) and the Dep. of Universities, Research and Information Society (Generalitat de Catalunya).  Figure 1: A translation FST from Spanish to English the same structure and search method, acoustic models can be omitted to perform text translation tasks only. This translation FST is learned automatically from a parallel corpus in three main steps (and an optional preprocessing). First, an automatic word alignment is produced. Currently this is done by the freely-available GIZA++ software [2], implementing well-known IBM and HMM translation models [3, 4]. From this alignment, a tuple extraction algorithm generates the set tuples that induces a sequential segmentation of both source and target sentences. These tuples must respect word order in both languages, as this is necessary for the transducer to produce a correct-order translated output. Finally, Xgrams are learned using standard language modeling techniques. Previous publications on this system include [5] and [6]. The organization of the paper is as follows. Section 2 offers an overview of the system architecture, whereas sections 2 and 3 deepen into details on translation generation and training issues. Section 4 presents the experimental framework used to evaluate the system, whose results are discussed in section 5. Finally, section 6 concludes and outlines future research lines. 2. Translation generation Statistical machine translation is based on the assumption that every sentence e in the target language is a possible translation of a given sentence f in the source language. The main difference between two possible translations of a given sentence is a probability assigned to each, which is to be learned from a bilingual text corpus. This probability can be modeled by a joint probability model of  85  source and target languages. In this case, solving the translation problem is ﬁnding the sentence in the target language that maximises equation 1. This probability can be approximated by an Xgram of a joint or bilingual language model, learned from a set of tuples, as expressed in equation 2.  eˆ = arg max{p(e, f )} = · · · =  (1)  e  N  arg max{ e  p((e, f )n|(e, f )n−1, ..., (e, f )n−X+1)}  n=1  (2)  where:  (e, f )n = (ein ...ein+In , tjn ...tjn+Jn )  TALP implements this Xgram language model by means of ﬁnite-state transducer whose edges are labelled with tuples (as shown in ﬁgure 1). That is, each edge has a label that relates one or more words in the sourcelanguage to zero, one or more words in the target language. This way, some edges may have just one word in the source language whereas others may have more, and both be valid as long as the ﬁrst word is equal to the input. Bearing this in mind, all well-known ASR decoding techniques can be used to ﬁnd the best-scoring translation of a given sentence, once the transducer is built. In a speech-to-speech translation framework, input data is the speech signal, so the objective of translation becomes the search for the sentence e in the target language the following equation:  eˆ = arg max{p(e, f ) · p(x|f )}  (3)  e  where we introduce the acoustic model p(x|f ) in the optimisation (x being the input acoustic signal). Therefore, by following this transducer-based approach, the same training and search techniques can be used to tackle both text and speech translation tasks. The following section goes into the details on how the FST is learned from a parallel textual corpus. The current architecture of the TALP translator performs the search for the best translation in a monotonous fashion. Any reordering of the target words is restricted to the short region deﬁned by the tuple. That is, it can only be produced inside a tuple, which can contain crossed alignment relationships. This poses a strong limitation to the system, specially when dealing with pairs of languages with long reorderings in word alignment, such as Chinese and English. Several reordering techniques have been tested with the FST architecture, none of them providing signiﬁcant results (for Spanish-English case, see [6]).  Parallel corpus Word Alignment  Pre-processing  Tuples extraction  Embedded words dict.  Xgram estimation  Figure 2: Training stages from a parallel corpus to a translation FST 3. Training Usual language model techniques can be used to learn the tuples language model (X-gram) once a given parallel corpus has been transformed into a set of tuples for each sentence. In order to do so, the training of the system comprises three basic stages (and an optional preprocessing), which are shown in the ﬂow diagram of ﬁgure 2. These steps are described in the following subsections. 3.1. Preprocessing The pre-processing stage is aimed at categorising words in order to reduce output vocabulary, helping the alignment stage to increase accuracy, without reducing input ﬂexibility. Some basic word groups can be categorised, namely personal names, names of cities, towns or countries (manually), and dates, times of the day and numbers (automatically). With the X-gram software, these categories or word groups can be easily modelled by smaller ﬁnite-state transducers that translate each of their possible alternative values. However, this preprocessing is an optional and language-dependent stage, according to the availability of resources. In the frame of a Chinese-English translation task, only a small preprocessing has been performed. As evaluation is performed without punctuation marks, we experimented training without punctuation, but this was discarded as results were equal or worse than leaving punctuation until a ﬁnal output post-processing. On the other hand, a special segmentation of the training corpus was performed. Whenever a pair of ChineseEnglish sentences shared the same number and type of punctuation marks (considering ’.’ ’,’ and ’?’), these were  86  split according to the position of punctuation. That causes the train corpus to have more and shorter sentences. 3.2. Word alignment Assuming that the input parallel text in sentence aligned, we perform a standard statistical word alignment stage by using GIZA++, a freely-available software which implements the so-called IBM alignment models presented in [3] as well as the HMM-based alignment model [4], producing the Viterbi alignment as an approximation to the most probable one. Due to the asymmetric nature of the resulting alignment (linking one word in the source language to one or more words in the target language), several symmetrization strategies can be used (such as the union or the intersection between alignments in both directions). In our case, both the union and the intersection are performed and can be also used to generate the set of tuples, like the source-to-target (s2t) and target-to-source (t2s) alignments. 3.3. Tuples extraction Once the alignment is produced, the tuples extraction unit has to build units so that the order of the sentence in both languages is not violated, a necessary requirement when dealing with ﬁnite-state translation transducers, as already exposed in [7], because otherwise the transducer would learn order-incorrect sentences. Given a sentence pair and a corresponding word alignment, the sequential set of tuples contains those pairs of m source words and n target words satisfying these constraints: 1. It induces a monotonous segmentation of the pair of sentences. 2. Words are consecutive along both source and target sides of the tuple. 3. No word on either side of the tuple is aligned to a word out of the tuple. 4. Each tuple cannot be decomposed into smaller phrases without violating the previous constraint. Note that this set is unique under these conditions [8]. The only ambiguity appears when a target word is aligned to NULL, in which case we append it to the next tuple (if exists, else to the previous). An example of the tuple extraction process is drawn in ﬁgure 3. When extracting tuples with more than one word in each language (as the third tuple in ﬁgure 3), a certain local reordering of the target is necessarily encoded. While helping the system to avoid local reordering mistakes, this strategy can suffer from an information loss, as the source words appearing in this tuple may not have any  src: F  f1  f2  f3  f4  f5  f6  trg: E tuples: T  e1  e2  (f1f2 , e1)  e3  e4  (f3 , e2e3e4)  e5  e6  (f4f5f6 , e5e6)  Figure 3: Tuples extraction from an aligned sentence pair  translation if they do not appear elsewhere alone in a tuple. We call these words embedded, as their translation appears only embedded in a longer phrase. To avoid this, we build up a dictionary of translations for embedded words from the most accurate word alignment available. For a certain embedded word fj and a given word alignment, we look for the target words ei...ei+K that are most frequently aligned to fj with these two conditions: 1. Target words ei...ei+K are consecutive in the target sentence. 2. Target words are aligned only to fj or to null. This way, we build up a statistical dictionary independently of the non-monotonicity of the word alignment. The entries of the dictionary are used as unigrams in the bilingual model estimated by the FST. To create the dictionary, all four aforementioned word alignments have been tested for several translation tasks, and the intersection has consistently given better results, even though its translations are always one-word. This strategy is useful though not robust enough yet. By building up the dictionary, we are able to produce a word-by-word translations for some embedded words whenever the sequence in the test sentence is not equal to any training tuple. However, information on embedded N-grams is not extracted at the moment. This has growing importance when dealing with very different pairs of languages, in terms of word ordering, as with a ChineseEnglish task. In section 4.3 the impact of this technique is evaluated in practice. 3.4. Xgram estimation Finally, given the parallel corpus described in a set of tuples for each sentence, a Finite-State Transducer containing Xgram probabilities is learned. Usually, a maximum length of 3 is used for memory, to avoid over-ﬁtting to training data. A back-off strategy is follow and a pruning of the resulting automaton can be performed. Two parameters are used for this: on the one hand, the minimum number of times a certain history (Xgram) must occur to be considered. And on the other hand, two different nodes sharing the same recent history are merged if the  87  divergence between their output probability distributions is smaller than a certain threshold (see details in [1]). Given the usual sparseness problems when dealing with parallel corpora, the ﬁrst parameter is not used (set to 1), whereas the latter (hereafter referred to ’f’) performs a slight pruning. 4. Experiment and results The presented system has been evaluated in the framework of the International Workshop on Spoken Language Translation (IWSLT’04), a Satellite Workshop of the Interspeech - ICSLP. In the workshop, an Evaluation Campaign has been conducted for two translation directions, namely Chinese-to-English and Japanese-toEnglish. Moreover, two different tracks per direction have been proposed, namely using only the supplied corpus (supplied) and allowing the use of any additional data for training purposes (unrestricted). Besides, an intermediate track allowing the use of the supplied corpus plus certain linguistic resources available from LDC has been proposed for the Chinese-English task. TALP has participated only in the Chinese-to-English supplied track, the reason being that we believe the Japanese-to-English task to be even more demanding in terms of reordering. As our system lacks any direct treatment of long reorderings, we found that the Chinese-toEnglish task brought up enough challenges for research. Next, we present a brief description of the supplied corpus, the evaluation measures used in the track and the results achieved by two different TALP runs. 4.1. Chinese-to-English IWSLT’04 supplied corpus Table 1 shows the main statistics of the supplied data, namely number of sentences, words, vocabulary, and maximum and average sentence lengths for each language, respectively. The difference between ’Train set’ and ’Segmented train set’ is the segmentation discussed in section 3.1. A development set of 506 sentences was also supplied, together with 16 reference English translations. There are 160 unseen words in the development set and 104 unseen words in the test set. 4.2. Evaluation measures The output of the system is evaluated using automatic and manual evaluation measures. For the automatic evaluation, 16 man-made English reference translations of the test corpus are used. The evaluation measures include BLEU score, NIST score, mWER, mPER and GTM (general text matcher). As for human assessment, each translated sentence is evaluated by three human judges, according to ”ﬂuency” and ”adequacy” of the translation. While ﬂuency indicates how the evaluation segment sounds to a native speaker of English, from ’Incomprehensible’ (1) to  supplied sent. words  Train set  Chinese 20,000 182,904  English  188,935  Segmented train set  Chinese 22,205 —-  English  —-  Development set  Chinese 506 3,515  Test set  Chinese 500 3,794  voc. 7,643 8,191 ——- 870 893  Lmax 69 75 62 58 24 62  Lavg 9.1 9.4 8.2 8.5 6.9 7.5  Table 1: Chi-Eng supplied corpus statistics  ’Flawless English’ (5), adequacy judges how much of the information is carried by the translation, from ’None of it’ (1) to ’All of the information’ (5). 4.3. Development work Several different conﬁgurations were tested for the development set. Their results are shown in Table 2, where ’aU’ and ’a2’ refer to using the union and the s2t alignment, respectively. The term ’seg’ refers to training with the segmented version of the corpus, whereas ’f’ refers setting the pruning parameter to 0.2, instead of leaving it to 0 (see section 3.4). As about the effect of using a dictionary of embedded words (see section 3.3), an evaluation without it has been performed, leading to the results shown with term ’-D’. In general, these results show a slight variation in performance for both alignments, but with a remarkable descent in terms of NIST score.  runs aU aU,seg aU,seg,f aU,seg,-D a2 a2,seg a2,seg,f a2,seg,-D  BLEU 0.244 0.251 0.255 0.264 0.319 0.318 0.314 0.315  NIST 5.169 5.187 5.210 4.741 3.789 3.871 3.678 3.706  WER 0.615 0.607 0.603 0.606 0.614 0.606 0.607 0.607  PER 0.529 0.521 0.518 0.524 0.552 0.546 0.548 0.547  GTM 0.591 0.595 0.594 0.592 0.573 0.573 0.570 0.571  Table 2: Automatic evaluation results (development set)  All runs using the union alignment leave 7 sentences untranslated (empty), whereas runs using the s2t alignment leave 16, 18, 19 and 19 sentences each. As we can see, the greatest difference between all the results lies in the original word alignment used to extract the bilingual tuples. The segmented version of the corpus provides a slight but consistent improvement, helping the training to produce more accurate alignments and shorter tuples. As about pruning, it seems that this technique does not make  88  much of a change, but it turns the algorithm a bit more efﬁcient. 4.4. Test set results For the reasons presented above, we have selected conﬁgurations ’aU,seg,f’ (run A) and ’a2,seg,f’ (run B) as ﬁrst-best and second-best for the test set. The difference in their original alignment makes a big difference in the ﬁnal translation transducer, as we can see in the statistics shown in Table 3, where the total number of tuples, tuples vocabulary size, average tuples length (adding source and target words) and number of embedded words are shown.  runs tuples vcb length embed  A 97,338 27,039 3.9 B 140,896 29,344 2.9  4741 1545  Table 3: Statistics of two different runs  Usually, the union alignment leads to much longer tuples, which in turn increases the number of embedded words, whose translation is ’solved’ by the dictionary built up with the intersection. On the contrary, using the s2t alignment we increase the number of total tuples (by decreasing their length), reducing at the same time the number of embedded words. However, we appreciate an important increase in the percentage of tuples translating to NULL (up to 28%, in contrast to 7.5% for the union), an undesirable consequence of following the asymmetric alignment. This could be avoided by taking a hard decision as to where to align these tuples (whether to the previous or the next tuple), but we do not believe this to be of much gain compared to using the union alignment. Finally, many of the new unigrams that are created through the dictionary when using the union alignment, already exist in the FST using the s2t alignment, but they are linked to NULL, which is inappropriate when no history can help in decoding. Table 4 presents the results obtained by these two runs evaluating against automatic measures.  runs BLEU NIST WER PER GTM A 0.279 6.778 0.556 0.465 0.647 B 0.331 5.391 0.550 0.490 0.620 Table 4: Automatic evaluation results for two runs Run A has produced no output in 5/500 sentences. Run B has produced no output in 11/500 sentences. Results show a surprising behaviour: while NIST score, PER and GTM clearly prefer run A, the BLEU metric gives a much better score to run B, being the WER practically identical in both cases. All in all, we believe run A to be slightly better and more consistent with human  translation, being more based on a phrase translation approach. It seems that BLEU does not seem to penalise the ’shortening’ effect of run B output. In fact, the average output sentence length for run A is 6.01 words, whereas for run B is only 5.18, a clear consequence of the high percentage of tuples translating to NULL. Table 5 presents the TALP results of the manual evaluation for run A. As expected given the lack of a reordering scheme in the statistical machine translator proposed, the ﬂuency score does not even achieve a ’3’, meaning ’Non-native English’. However, the adequacy score is quite good as it means the ’Much of the information’ is being translated in the output. run ﬂuency adequacy A 2.792 3.022 Table 5: Manual evaluation results for run A  5. Analysis and discussion Among the various conﬁgurations tested, the biggest difference lies in the word alignment used to extract tuples. However, all of them share a very important limitation of the current architecture. This refers to word reordering, which is strictly limited to the local reordering inside a tuple, making the approach inappropriate for pairs of languages with a very different word order. In the ChineseEnglish case, the system is unable to perform long reorderings, which leads to an important loss in the ﬂuency of the output translation. On the other hand, the addition of a dictionary when using the union alignment ensures that most of content words are translated, assuring that ’most of the information’ is included. This is a typical problem of statistical machine translation systems, which tend to make stupid syntactic or morphological mistakes while still providing a ’fair’ message translation. Some examples of translation and one reference for the development set are shown in Table 6.  Translation: Reference: Translation: Reference:  that what time start ? what time does it start ? stomach very hurts . i have a severe pain in my stomach .  Table 6: Samples of translations and reference (dev set)  Finally, we would like to point out the seemingly inconsistent results of automatic evaluation measures, which demand further research towards ﬁnding more robust ways to measure translation performance.  89  6. Conclusion and further work The statistical machine translation TALP system has been presented in detail. Description and training details from a parallel corpus have been shown. An evaluation in the framework of Chinese-English supplied task of the IWSLT’04 workshop has been performed. Results have been discussed, addressing the limitations of the system that are highlighted by this challenging translation task. Future work to improve the system should necessarily tackle the problem of embedded N-grams. One way of treating them would be to extract their translation in a dictionary as it is currently done with embedded words. This would lead to a phrase-based-like approach, but with a reduced set of phrases compared to current approaches. Moreover, a generalization of the extracted tuples is necessary, for example using classiﬁcation algorithms or clustering. This could give the system the power of translation unseen tuples adequately, by using the context of ’similar’ seen tuples. And last but not least, techniques to overcome the reordering limitation must be researched, even if that means some big structural change in the translation model based on FST, which proves currently inadequate for pairs of language with different word ordering. 7. Acknowledgements The authors want to thank Josep Maria Crego and Jose´ A. R. Fonollosa (members of the TALP Research Center) for their contribution to this work. 8. References [1] A. Bonafonte and J. Marin˜o, “Language modeling using X-grams,” Proc. of the 4th Int. Conf. on Spoken Language Processing, ICSLP’96, pp. 394–397, October 1996. [2] Giza++ software, “http://www-i6.informatik.rwthaachen.de/˜och/software/giza++.html.” [3] P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer, “The mathematics of statistical machine translation,” Computational Linguistics, vol. 19, no. 2, pp. 263–311, 1993. 
 1. The Background Grammar As for our team’s “philosophical” background and attitude, we would like to verify that computational linguistics is worth returning from the nowadays wide-spread attitude characterized by “shallow parsing” (which is held to save expenses) [15] to the pure theoretical (generative) linguistic basis.1 Our crucial argument relies on a double (parallel computational and linguistic) chance available in the early years of the new century: to use simultaneously, on one hand, a significantly greater number of huge patterns than earlier due to the immense increase in memory capacity [30], and to work out a formal grammar, on the other hand, showing the distribution of capacity advantageous in modern computer science (in harmony with the development mentioned above): “minimal processing – maximal database”. This latter chance has something to do with the sweeping lexicalist turn [13, 14, 17, 28, 31] in generative linguistics, which used to be chiefly “processoriented” (i.e. syntax-centered) in its first period [16]; the current attitude can be characterized by two mottoes of Joshi’s [18], the father of mildly contextsensitive grammars [26]: “Complicate Locally, Simplify Globally”, and “Grammar ≈ Lexicon”. What we propose as the ideal background grammar, in harmony with this favorable tendency, is a new sort of generative grammar, GASG (“Generative /Generalized Argument Structure Grammar”, defined 
1. Introduction Approaches to machine translation from French to Arabic are rare. Certainly, translation memory systems do exist (as for example the commercial system An-Nakel Al-Arabi from French to Arabic from CIMOS (Paris)1) but the limitations of such systems are now well known to all specialists working in the domain. In this paper we present an approach which we have implemented specially for the machine translation of specialty languages from French to Arabic. At the outset of the project, existing approaches (see for example [1], [2]) were examined (direct, transfer, pivot, statistical) but it became apparent that each presented limits and it became increasingly evident that it was necessary to devise an approach specific to this particular language couple. The fact that we have here two languages that are very distant linguistically means that, from the linguistic point of view, certain linguistic operations which are specific to this couple will not necessarily be found in other couples, whether or not one or other of French or Arabic is present as either source or target. The approach that we describe is thus unique for the couple French to Arabic; and it follows that it is not valid for the inverse couple Arabic to French. Reversing the 
 We report on the results of an experiment aimed at enabling a machine translation system to select the appropriate strategy for dealing with words and phrases which have different translations depending on whether they are used as proper names or common nouns in the source text. We used the ANNIE named entity recognition system to identify named entities in the source text and pass them to MT systems in the form of "do-not-translate" lists. A consistent gain of about 20% in translation accuracy was achieved for all tested systems. The results suggest that successful translation strategy selection is dependent on accurate segmentation and disambiguation of the source text – aspects which could be significantly improved by named entity recognition. We further suggest an automatic method for distinguishing and lexical differences in MT output that could have applications in automated MT evaluation for morphologically rich languages.  1. Introduction Language communities develop certain acceptable practices and norms for translating different types of concepts, expressions and texts from other languages and cultures. These practices are described as translation methods, translation strategies and translation procedures. (Vinay and Darbelnet, 1958, 1995). Translation methods relate to whole texts, while strategies and (finer-grained) procedures relate to sentences and smaller units (Newmark, 1988:81). The choice of a translation strategy often depends on the type of a translated unit. For example, for certain types of proper names the optimal translation strategy is transference, i.e., a “donot-translate” or “transliterate” strategy, while the majority of common nouns are translated with other strategies: literal translation, transposition, modulation, etc. (Newmark, 1988: 81-88). This implies that recognising different types of units in the source text is a necessary condition for optimising the choice of translation strategy and, ultimately, for improving the quality of the target text. The problem of selecting translation strategies for words that may be used as proper names or common nouns in the source language is related to a more general problem of word sense disambiguation (WSD) – one of the most serious problems for Machine Translation technology. Dealing with “proper vs common disambiguation” (PCD) often requires combining  different knowledge sources, in a similar way to WSD (Stevenson and Wilks, 2001). But the cross-level nature of this problem also suggests that improvement in MT quality could be achieved through improving related aspects of the source-text analysis, such as Named Entity (NE) recognition (Babych and Hartley, 2003; Somers, 2003:524). For the purposes of this discussion, we assimilate proper nouns to NEs and investigate NE recognition as a possible solution to the PCD problem insofar as it might enable the selection of the correct strategy. Accurate NE recognition is important for the general quality of MT for the following reasons: 1. The translation of the same token may be different depending on whether the token is a common noun or part of an NE, e.g. in Russian if a common name is a part of an organization name, a “do-not-translate” or “transliterate” strategy should be used instead of a default translation strategy: (1) Original: …the Los Angeles office of the Hay Group, a management consulting firm. MT output1: …Лос-Анджелесский офис Группы Сена, управление консультантская фирма. ('… the Los Angeles office of the group of the hay [i.e., the grass, cut and dried for fodder], management consulting firm ') 
 Linear B Ltd. Technology Transfer Centre King’s Buildings Edinburgh EH9 3JL josh@linearb.co.uk  Abstract. In this paper we introduce Linear B's statistical machine translation system. We describe how Linear B's phrase-based translation models are learned from a parallel corpus, and show how the quality of the translations produced by our system can be improved over time through editing. There are two levels at which our translations can be edited. The first is through a simple correction of the text that is produced by our system. The second is through a mechanism which allows an advanced user to examine the sentences that a particular translation was learned from. The learning process can be improved by correcting which phrases in the sentence should be considered translations of each other.  .  1. Introduction Statistical machine translation was first proposed in Brown et al (1988). Since statistical machine translation systems are created by automatically analyzing a corpus of example translations they have a number of advantages over systems that are built using more traditional approaches to MT: • They make few linguistic assumptions and can therefore be applied to nearly any language pair, given a sufficiently large corpus. • They can be developed in a matter of weeks or days, whereas systems that are hand-crafted by linguists and lexicographers can take years. • They can be improved with little additional effort as more data becomes available. More recent advances in phrase-based approaches to statistical translation (Koehn et al (2003), Marcu and Wong (2002), Och et al (1999)) have led to a dramatic increase in the quality of the translation systems. Phrase-based translation systems produce higher-quality translation since they use longer segments of human translated text. Using longer segments of human translated text reduces problems associated with literal word-for-word translations. For example, multi-word expressions such as idioms are better translated. Linear B is a commercial provider of statistical machine translation systems. This paper  describes Linear B's advances to phrase-based machine translation that allow translation quality to be improved through editing translations that are produced by our system. There are two levels at which our translations can be edited: • The first is through a simple correction of the text that is produced by our system. Our system improves by dynamically learning the correct translations of new phrases. These new phrases are extracted from the corrected sentence pair using the existing translation models, and can be used immediately for subsequent translations. • The second is through a mechanism that allows an advanced user to inspect which phrases the system's translation was composed from. If a particular phrase was mistranslated, the user can examine the set of sentence pairs that a particular translation was learned from, and make corrections. These features mean that our system is capable of improving with use and adapting to be more appropriate for a new domain. This has two main implications: our systems get better as our customers use them, and our systems have the potential to be trained using example translations from one domain (such as government documents, which have abundant translations) and gradually adapted to a new domain. The remainder of the paper is as follows: Section 2 describes how our phrase-based models of  26  translation are learned from archived translations, and gives example output produced by a system trained on data from the Canadian parliament. Section 3 shows how our system dynamically integrates edited output by extracting the translations of new phrases, and weighting the corrected translations more heavily than existing translations in the model. Section 4 described the advanced editing technique that allows a user to inspect the sentence pairs which a faulty translation was learned from, and correct the statistical models by explicitly showing the system which phrases ought to be learned from those sentence pairs instead. 2. Phrase-based Statistical Translation The goal of statistical machine translation is to be able to choose that English sentence, e, that is the most probable translation of a given sentence, f, in a foreign language. Rather than choosing e* that directly maximizes the conditional probability p(e|f), Bayes' rule is generally applied: e* = argmaxe p(e) p(f|e) The effect of applying Bayes' rule is to divide the task into estimating two probabilities: a language model probability p(e) which can be estimated using a monolingual corpus, and a translation model probability p(f|e) which is estimated using a bilingual, sentence-aligned corpus. Here we examine different ways of calculating the translation model probability.  2.1 Word Alignments Brown et al (1993) define a series of translation models, which are commonly referred to as IBM Models 1 to 5. The IBM Models formulate translation essentially as a word-level operation. The probability that a foreign sentence is the translation of an English sentence is calculating by summing over the probabilities of all possible wordlevel alignments, a, between the sentences: p(f|e) = ∑a p(f,a|e) Thus they decompose the problem of determining whether a sentence is a good translation of another into the problem of determining whether there is a sensible mapping between the words in the sentences. Figure 1 illustrates a probable word-level alignment between a sentence pair in the Canadian Hansard bilingual corpus. Brown et al formulate alignment probability p(f,a|e) in terms of distortion, fertility, and spurious word probabilities in addition to word-for-word translation probabilities. The act of translation in the Brown et al approach is one of string rewriting. In string rewriting each word in a source sentence is replaced by zero or more words in the target language. Then, a number of ``spurious'' target words might be inserted with no direct connection to the original source words. Finally the words are then reordered in some fashion to form the translation. Problems with the Brown et al approach to translation include: • It doesn't have a direct way of translating phrases; instead fertility probabilities are used to replicate words and translate them individually. • Using small units such as words means that a lot of word reordering has to happen. But the distortion probability is a poor explanation of word order.  Figure 1. A word-level alignment for a sentence pair that occurs in our training data  2.2 Phrase Alignments Phrase-based translation, by contrast, uses larger segments of human translated text. Phrase-based translation does away with fertility and spurious word probabilities. While it does have some notion of distortion, this is less pertinent since local reorderings such as adjective-noun alternation can be easily captured in phrases. The main part of phrase-  27  based translation models is the estimation of phrasal translation probabilities. In general, the probability of an English phrase e translating as a French f is calculated as the number of times that the English phrase was aligned with the French phrase in the training corpus, divided by the total number of times that the French phrase occurred: p(f|e) = count(f,e) / ∑f count(f,e) The trick is how to go about extracting the counts for phrase alignments from a training corpus. Many methods for calculating phrase alignments use word-level alignments as a starting point.1 There are various heuristics for extracting phrase alignments from word alignments, some are described in Koehn (2003), Tillmann (2003), and Vogel et al (2003). The online version of this paper gives a graphical illustration of the method of extracting incrementally larger phrases2 from word alignments described in Och and Ney (2003). Counts are collected over phrases extracted from word alignments of all sentence pairs in the training corpus. These counts are then used to calculate phrasal translation probabilities. The act of translating with phrase-based translation model involves breaking an input sentence into all possible substrings, looking up all translations that were aligned with each substring in the training corpus, and then searching through all possible translations to find the best translation of source sentence. 2.3 Example Translation This section gives an example translation which was produced by our system when it was trained on a collection of example translations from the Canadian Parliament. For the source passage L’honorable Leonard J. Gustafson: Honorables sénateurs, tandis que la guerre en Irak entre dans sa troisième semaine, nous ne devons pas oublier qu’il faut prendre des mesures pour 
 1. Background We describe ongoing efforts towards and challenges in adapting and using an Example-Based Machine Translation (EBMT) system in the context of a transnational digital government project (Cavalli-Sforza, et al., 2003; Su et al., under review). The project represents an unusual collaboration between universities, government agencies, and an international organization aimed at applying information technology (IT) to a problem of international concern: detecting and monitoring activities related to the transnational movement of illicit drugs. The process is coordinated by the Organization of American States (OAS). The work is performed by a team of researchers from seven universities (U. of Belize, Pontificia Universidad Católica Madre y Maestra in the Dominican Republic, Carnegie Mellon U., North Carolina State U., U. of Colorado, U. of Florida, U. of Massachusetts) and experts from agencies in the three participating countries: the OAS's InterAmerican Observatory on Drugs and Office of Science and Technology in the U.S., the National Drug Abuse Control Council (NDACC) of Belize's Ministry of Health, and the National Drug Council of the Dominican Republic. The motivation for this project and the choice of partner countries and institutions stems, in part, from an NSF-funded workshop for exploratory research on transnational digital government (May 9-11, 2001, Belize City, Belize). The specific choice of governmental domain at which to target our research activity – the  collection, notification, and sharing of information regarding movement of people across borders – was the result of a collective decision in the early phases of the project. It speaks directly to one of the indicators (“Displacement”) used by the Multilateral Evaluation Mechanism (MEM), a multinational effort that involves the collection and analysis of data by, and from, several government agencies and non-government organizations within each country. The MEM is managed by OAS’s Inter-American Drug Abuse Control Commission (CICAD) with participation by 34 OAS member states . 1 
 Abstract. This paper presents a syntactic analysis of a fragment of Amharic noun phrases. Amharic noun phrases possess some peculiar characteristics especially relating to the syntax of the definite article. The definite article shows some morphological and distributional characteristics distinct from languages such as English and German, which makes it difficult to directly apply the methods proposed for these languages to the analysis of Amharic noun phrases. In this paper, we provide a formal analysis of Amharic noun phrases by combining the methods of the different approaches suggested for analyzing noun phrases. The result shows that the affixial treatment of the Amharic definite article better explains the facts regarding the syntax of Amharic noun phrases.  1. Introduction Amharic belongs to the Semitic language family and is one of the most widely spoken languages in Ethiopia. Amharic has its own script that is borrowed from Ge'ez, another Ethiopian Semitic language (Leslau, 1995:1). Amharic has a complex morphology. Thus the need for automatic processing of Amharic words is getting increasing attention, which is reflected in the recent rise in the number of Amharic language processing applications (Nega, 1999; Abiyot, 2000; Mesfin, 2001; Worku, 1997). Regarding the syntax of Amharic, except for a prototype experiment done on a Probabilistic Context Free Grammar for Amharic by Atelach (2003), very little work has been done to formalize the syntax of Amharic especially using unification based grammar formalisms. On the other hand, syntactic analysis forms the core of most Natural Language Processing applications such as Machine Translation. 
Abstract. The notion of a translator's workstation has been widely discussed at various points in the history of translation and computers, and a number of tools and language resources have been proposed for inclusion in it, ranging from general purpose text-editing facilities, to tools designed specifically for translators, such as translation memory and terminology management software. This paper reports on the progress of a project that has been initiated to investigate which of the many available tools and language resources translators today are actually incorporating into their workstations, and which they deem to be useful in supporting their work. Specifically, in this paper, the findings of a survey of UK translators are presented, focussing specifically on the levels of uptake of a wide range of tools and language resources. To date, some 400 responses to this survey have been received, logged and analysed.  Perhaps more than other professionals,  translators are feeling the long-term changes brought about by the information age.  The snowballing acceleration of available information, the increase in intercultural encounters,  and the continuing virtualisation of private and business life  have resulted in drastic and lasting changes in the way translators work.  (Austermühl 2001:1)  in their coverage of translators' working practices,  1. Introduction  were undertaken prior to, or in the very early days  The notion of a translator's workstation, comprising a number of computer-based aids to support translators in their work has been widely discussed in the past few decades in the literature of translation and computers (see for example Kay 1980/1997; Melby 1982 and 1992; Hutchins 1998; and Somers 2003). Still other authors have recently provided detailed and comprehensive overviews of the burgeoning array of tools and language resources available to the translator today (see for example Austermühl 2001), ranging from word processing facilities, through dictionary look-up tools, systems for creating and managing terminology collections, to translation memory and machine translation. In addition to these discussions of the computer-based facilities that exist to support translators, a number of studies have been conducted to determine how translators actually work and which of the available tools and language resources they add to their workstations and incorporate into their translation workflow.  of, both the 'Internet boom' in the commercial world and the commercial availability of tools such as translation memory (see for example Smith and Tyldesley 1986; Fulford, Höge and Ahmad 1990). Consequently, they can give little indication of the use being made of these facilities by translators, and are thus inevitably now somewhat dated. Still other studies have focused on a narrow range of tools in the translation environment, such as the uptake of machine translation (see for example Brace, Vasconcellos, and Miller 1995), or have been concentrated on tool usage within an individual organisational setting: see for example the review of technology usage at the European Commission (Blatt 1998). A further study of translators in various European countries (reported in Reuther 1999), has considered the 'language engineering' and 'language technology' requirements of translators working in a variety of contexts, but does not really provide any detailed insights into what is actually in use in the translator community.  Some of these surveys, although comprehensive  53  In the light of the lack of empirical data regarding actual translation practice, a three-year project (funded by the EPSRC1) has been established, the aim of which is to explore the tools and language resources UK translators today are incorporating into their workstations, and to identify the strategies they employ for integrating them into their workflow, as well as the impacts those computer-based aids are having on their working environments. The focus of the project is on freelance translators and small translation businesses as these today represent a significant proportion of the translator community in the UK (Fraser 2000). Part of the first phase of the project comprised a survey of translators based in the UK, the key objective of which was to determine the uptake of a range of tools and language resources. To date, some 400 responses to this survey have been received, logged and analysed. The initial findings of this survey are presented in this paper. The paper concludes with a discussion of the implications of the findings for translation tool developers, translator trainers, and researchers, as well as for working translators. Before presenting the survey findings, an overview is provided of the conceptual framework for the research project as a whole. 2. Conceptual Framework It was recognised from the outset of this research project that an investigation into the uptake of computer-based aids by translators would entail an interdisciplinary literature search in order to gain adequate coverage of the wide range of factors to be considered in such a study. To this end, the following domains were identified as key 'informant domains' for the project: first, language and translation (including translation principles and methods, and language technologies); second, information and communications technology (ICT) and information systems; and third, small business management. Within these informant domains, the areas being studied include: ¾ Language and translation: Translator working practices and working environments; Computer-based aids available for translators, and the categorisation of these aids; Translation workflow models. 
 F.Gaspari@postgrad.umist.ac.uk  Abstract. On-line machine translation (MT) services are becoming increasingly popular among Internet users. In particular, over the last few years there has been a dramatic increase in the number of monolingual web-sites that rely on Internet-based MT systems to disseminate their contents in a variety of languages, which seems to be one of the most interesting areas in the current use of MT technology. This paper is based on preliminary observations of these recent developments and reports on how on-line MT services are actually integrated into a sample of monolingual web-sites only available in English, attempting to evaluate the success of the strategies used to incorporate webbased MT technology for dissemination purposes. The discussion suggests in conclusion that the overall lack of a user-oriented approach and the limited consideration of issues of user-friendliness make the integration of on-line MT into the sample of monolingual web-sites largely ineffective.  1. Introduction 1.1 Background This paper provides an overview of how a sample of 36 monolingual web-sites incorporate on-line machine translation (MT) technology to disseminate their contents in multiple languages. The investigation focuses on web-sites that originally present information only in English, and explicitly encourage non-English-speaking visitors to translate their web-pages via links to on-line MT services. These monolingual web-sites rely on web-based MT technology for dissemination purposes, i.e. to make their contents available in other languages to a much wider population of Internet users with different linguistic backgrounds. The use of on-line MT technology integrated into monolingual web-sites is becoming increasingly popular (see e.g. Yang & Lange 2003: 192-193), even though exposing Internet users to raw MT output in order to disseminate information in multiple languages certainly entails controversial issues: “This is a sensitive area, since the imperfections of MT may distort the message”  (Yang & Lange 2003: 201). It seems particularly interesting that MT technology today is heavily used for dissemination purposes on the Internet, contradicting the widely held belief that unedited MT output is only suitable for internal use or gisting purposes, and should not be used for outbound circulation. 1.2 Purposes of the evaluation This paper reviews some of the most common patterns in the way in which monolingual web-sites embed on-line MT technology, and attempts a brief evaluation of how successful these strategies are from the point of view of Internet users. In this respect, this research looks in particular at some of the perceived weaknesses in the approach towards the use of on-line MT services for dissemination purposes, and contrasts them with examples of good user-centred design emerging from the sample of web-sites. Due to the preliminary nature of this report and to space constraints, only some of the criteria that are relevant to the successful integration of online MT technology into monolingual web-sites are reviewed and evaluated here. The investigation lays particular emphasis on user-friendliness and  62  interaction design issues, and the paper sketches a picture of some of the broad trends that emerge from looking at the chosen sample of web-sites. 2. Web-sites and factors considered for the evaluation 2.1 Choice of the sample of web-sites The web-sites that have been included in the sample considered for this investigation have been selected by the author after submitting a few simple queries to the search engine Google1, by means of textual search strings and relevant keywords2. This procedure restricted the scope of the enquiry to web-sites with textual contents originally written in English that explicitly encourage their visitors to use on-line MT, in order to obtain on-the-fly translations into other preferred languages. The multiple queries submitted to Google returned several thousands of hits in total, thus indicating that many monolingual web-sites in English currently incorporate on-line MT to disseminate information. A few dozens of these web-sites were briefly checked by the author, and subsequently a sample subset of 36 of them was extracted, so as to have an equal number for each designated main category (i.e. 12 for each of the three types considered here – see next section 2.2 below for details). The selection criteria for this further choice were arbitrarily set by the author, being however only aimed at representing different types of web-sites and providing a reasonably comprehensive overview of the different strategies used (more or less successfully) to integrate online MT technology into monolingual web-sites. The resulting sample of 36 web-sites provides a small-scale realistic picture of how on-line MT is used by monolingual web-sites, exemplifying a number of general user-friendliness and interaction  
1. Introduction Research in Machine Translation (MT) has explored many different methods over the years, including rule-based, statistical and example-based models as well as hybrid and multi-engine approaches. Certain MT systems have been developed for particular sublanguage domains. Furthermore, since 1996, there has been a growing interest in controlled languages and their application in MT as demonstrated by the series of CLAW workshops on controlled language applications. These have sparked the development of both monolingual and multilingual guidelines and applications using controlled language (CL) for many languages. Natural language grammars can be restricted in such a way that ambiguity and complexity is lessened or eliminated completely. Controlled languages are subsets of natural languages whose grammars and dictionaries have been restricted for this purpose. As well as aiding human comprehension of texts, CLs can also be used for improving the computational processing of text and potential benefits have been claimed for the integration of controlled languages with translation tools. Until quite recently, however, the area of Controlled Translation has been largely ignored. Only a limited number of rule-based MT (RBMT)  systems have been used to translate controlled language documentation, including Caterpillar’s CTE and CMU’s KANT system [Mitamura & Nyberg, 1995], and General Motors CASL and LantMark [Means & Godden, 1996]. However, such systems can be very complex and expensive to develop for controlled translation, as it is difficult to fine-tune such general-purpose systems to derive specific, restricted applications. It is widely recognised that the use of traditional RBMT systems can lead to the well known ‘knowledge- acquisition bottleneck’. It is also acknowledged that the use of corpus-based MT technology can overcome this problem. It is difficult, therefore, to comprehend why more work has not been done in the development of ExampleBased MT (EBMT) systems for controlled language applications, especially when one considers that the quality of EBMT systems depends heavily on the quality of the reference translations in the system database—the more these are controlled, the better the expected quality of translation output by the system. Recently [Gough & Way, 2003] presented the first attempt at controlled translation using EBMT. In this paper, they attempted to control the output translations by incorporating in the system's memories target language strings written according to Sun Microsystems’ controlled language  73  guidelines. In this paper we improve on their method of extracting sub-sentential alignments. In rerunning their experiments with our new method, we succeed in populating the system’s databases with considerably more sub-sentential fragments and demonstrate a considerable increase in translation quality. As is acknowledged in [Gough & Way, 2003], it is more usual to propose the use of CL as a means of controlling the input texts rather than the output translations. In this paper, therefore, we use our improved methodology on their training and test data to control the processing of the source language. In assessing the results of [Gough & Way, 2003] and our improvements for French-English, we compare our novel results for English-French using manual and automatic evaluation metrics, and comment on the relative success of controlling source and target texts in controlled translation using EBMT. We also compare the results achieved with an array of automatic evaluation metrics. Finally, it has been claimed in the literature [Carl, 2003; Schäler et al., 2003] that EBMT systems should fare better than RBMT systems when confronted with controlled data. To provide some experimental backup to these insights, we provide results for the good on-line system Logomedia, and compare these with the results obtained for our system. The remainder of the paper is organised as follows: in section 2, we describe relevant previous research in the area of controlled translation. In section 3, we present our EBMT system and the methodology used to derive controlled translations. In section 4, we use both automatic and manual evaluation metrics to assess our system based on the results of different experiments. Finally, we conclude, summarise our contribution to the area of controlled translation in particular, and to EBMT in general. 2. Controlled Translation Recent research [Carl, 2003; Schäler et al., 2003] has addressed the theme of controlled translation and outlined some theoretical requirements for the development of MT systems for use with CLs. With respect to controlled translation in a transfer-based system, there are three stages of processing: it is necessary to exert control over the source language, the transfer routines as well as the generation component. With the absence of control at any one of these stages, one cannot necessarily expect to produce a high-quality controlled translation. While the theoretical issues of controlled translation have been addressed to a certain extent, the lack of sententially aligned texts conforming to  sets of controlled language specifications is a major obstacle in the development of applications for controlled translation. Although controlled language specifications do exist for English (e.g. CTE or CASL) and French (e.g. GIFAS Rationalised French [Barthe, 1998]), there is no controlled bitext in existence for any language. Moreover, the difficulty surrounding this task comes to light when we consider that there is no guarantee that enforcing different sets of controlled language specifications on both source and target documents would ensure the production of a necessary and sufficient translation. However, some efforts have been made to automate this process. For example, [Hartley et al., 2001; Power et al., 2003] approach this task with respect to multilingual natural language generation. Users are prompted by the system to build up a text in one language in a technical domain. Although they need to be an expert in the specific domain, no foreign language knowledge is required. Instead, multiple expressions of the same underlying input in various languages is facilitated. While this task may be tedious, the strings will conform exactly to a strictly defined controlled language. [Bernth, 2003] seeks to constrain the output so as to facilitate speech-to-speech translation. Bernth explores parse trees to identify undesirable constructions and rewrite them with suitable substituted target text. This method is, however, unavailable to us, as the corpus we use does not contain such detailed structural representations. The transfer-driven MT system of [Yamada et al., 2000] constrains transfer rules to control the generation of the correct forms of politeness in Japanese given English input. More relevant to our approach is the previous work in the area of controlled translation using EBMT. [Gough & Way, 2003] use a corpus of Sun documentation written according to CL guidelines to constrain the translations of ‘unconstrained’ input. They translate the controlled English text using the on-line system Logomedia, selected as it was deemed to be the better of the three on-line MT systems tested in [Way & Gough, 2003]. It is acknowledged in [Gough & Way, 2003] that while this may not be controlled translation per se according to the definitions of [Carl, 2003; Schäler et al., 2003], they justify this approach given the lack of availability of both controlled input and output. In this paper, we extend the work of [Gough & Way, 2003] in two ways: firstly, we apply a number of improvements to their method of deriving sub-sentential resources which lead to enhancements  
This paper introduces a new approach to translation memories. The proposed translation technology uses linguistic analysis (morphology and parsing) to determine similarity between two source-language segments, and attempts to assemble a sensible transltion using translations of source-language chunks if the entire source segment was not found. This is achieved by integrating a rule-based machine translation (RBMT) engine. The drawback of this approach is language-dependence; however, proper grammar acquisition methods are being developed to speed up grammar preparation for further language pairs. The paper discusses the basic processes of the proposed TM. Then the underlying machine translation engine is described. This is followed by an outline of the inegrated translation support tool where the proposed TM architecture is to become a core component. We argue that both the recall and the precision of a language-aware translation memory is higher than those of character-based systems. To this end, the paper introduces an evaluation scheme currently being prepared for use for testing the TM, followed by some estimated figures, the latter providing arguments in favour of the proposed TM scheme.  1. Introduction Commercially available translation memories do not involve linguistic knowledge. Instead, they determine similarity between translation units on the basis of pure mathematical distance calculations, with the majority of the algorithms using fuzzy indexes. Although most such systems recognize and handle non-translatable passages (such as dates, numbers, some abbreviations), and incorporate terminology management modules as well, existing translation memory systems do not systematically treat and recognize morpho-syntactic or even syntactic similarities, if the translation units themselves are too different in terms of characters. This paper presents a proposed system addressing these particular problems, one that attempts to assemble translations from sub-sentence units stored in its database, both in the source and the target languages. The proposed language-aware translation memory program is being implemented as a languagedependent translation support tool, first developed for the English-Hungarian language pair. The development process aims at surpassing the recall and precision of existing systems, resulting in a tool that offers translations more often, with the suggestions being closer to the desired translation. In the first version, the translation is assembled from stored translations of noun phrases and the morphosyntactic skeleton of the source unit. The morpho-  syntactic skeleton is a sequence of lemmas and morpho-syntactic parses of the words in the source unit, with a symbolic NP slot at the place of each noun phrase. This scheme may result ambiguities – an undesired phenomenon in CAT systems –, but the best translation will be a far closer approximation of the desired one than in existing systems. Therefore, in the devised user interface, the user will be able to select the best translation from multiple suggestions. However, in the subsequent revision phase – when the human translator transforms the suggestion into the desired translation –, fewer corrections will be necessary. In order to exploit the advantages of rule-based machine translation (RBMT), the system incorporates a parser/translator module, named MetaMorpho, developed by the authors’ team. Thus the suggestions are produced from stored translations and core linguistic patterns used by the MT module. If the translation memory is used extensively, new translation patterns will be created in the thousands or the tens of thousands, which, given the close connection between the TM engine and the MT module, can be recycled to enhance the quality of the MT part as well. Section 2 of this paper discusses the basic processes of the proposed TM system, showing the characteristics of the architecture and the mechanisms. This section also describes the procedure of insert-  82  ing a new translation unit into the translation memory database. Section 3 provides details on the linguistic matching of source and target segments, as well as incoming source segments and stored translation units. Section 4 describes the MetaMorpho translation mechanism, which is an efficient blend of an example-based and a transfer-based architecture. Section 5 provides implementation-specific details of the development project, describing the integration of the proposed translation memory tool into full-scale translation tools. Section 6 concludes the paper by describing a proposed evaluation scheme for the translation memory, and providing arguments in favour of the proposed translation memory scheme. 2. The basic processes of the proposed TM scheme Translation memory systems maintain a database of existing translations. Such databases are practically sentence-aligned but unannotated parallel corpora. The success of a TM system depends entirely on the lookup structure associated with the parallel corpus. In commercial TM applications, the lookup structure is a fuzzy index, which helps the system find source segments not entirely identical to the current source segment (i.e. on which the translator is currently working). The similarity measure is based on the character codes, and does not take into account the linguistic properties of either the stored segments or the current segment. Another problem of commercial TMs is that they handle the translations on an ‘as-is’ basis: if a source segment is found at whatever level of similarity to the current one, the stored translation is inserted to the current target text, leaving it to the translator to adjust the translation to the contents of the current source segment. In this scheme, no smaller unit than a single segment (usually a sentence) can be looked for in the database. 2.1. Integration of RBMT The proposed TM scheme is being built around a rule-based machine translation module named MetaMorpho. Provided the appropriate grammar lexicon, the MetaMorpho module is able to determine the structure of the source segment, and produce an automatically generated translation. The key benefit of this module – the one exploited in the translation memory scheme – is that the atomic unit of a grammar is a lexicalized syntactic pattern. Therefore, the grammar does not consist of abstract rules, but mainly syntactic patterns that hold the properties of an idiomatic or otherwise lexically 
Abstract. . The paper proposes a model for translation between syntactically similar languages of acceding countries. This model is based on the presupposition that the translation of related languages should exploit the relatedness by using as simple methods and tools as possible. In the first part the paper discusses the properties of some “new” languages, the second part describes a simple translation model which has already been tested on several pairs of syntactically similar languages..  1. Introduction The historical event of EU enlargement scheduled for the 1st May of this year brings many new challenges. Apart from political and economical ones there are also linguistic challenges. The enlargement introduces ten new languages, increasing thus the number of official languages of EU to 21 and the number of language pairs to 210. It is quite clear that such a sudden huge increase of the number of language pairs requires solutions exploiting all possible advantages which might make the enormous translation task a little bit easier. In this paper we would like to propose a model for translation between those languages of acceding countries which are more or less related. This model is based on the presupposition that the translation of related languages should exploit the relatedness by using as simple methods and tools as possible. It also advocates the idea that although for every acceding country it is extremely important to translate from “big” EU languages (English, German, French), it would be useful to shift the research focus to a machine translation among related “new” languages. It might quickly solve at least some problems of translation inside enlarged EU and to help to bridge a gap until fullfledged MT systems are developed. 2. The classification of relatedness Natural languages are grouped into language families. Usually, languages from the same language family are more similar than languages from different language families. Although the division of languages into language families is not  a perfect criterion for classification, it provides a raw hierarchy. Let us distinguish the following levels of proximity of languages with examples of language pairs from the Central and East Europe: • Variants of one `underlying' language (e.g., Serbian and Croatian, Upper and Lower Sorbian). • Very closely related languages (e.g., Czech and Slovak or Upper Sorbian) are very similar in morphology, syntax and lexics. Semantic ambiguities are rare. We suppose that no syntactic parser is needed in MT systems for such languages. • Closely related languages (e.g., Czech and Polish or Russian) are similar in morphology and lexics, although some semantic ambiguities occur. Syntactic constructions are not fully compatible, e.g., counterparts of analytic constructions are synthetic and vice versa or different lexems are used, cf. Czech byl jsem [I was] with Pol. byłem, Czech byl zničen [has been destroyed] (aux. verb být) with Polish został zniszczony (aux. verb zostać instead of być). Partial transfer is needed to perform MT among these languages. • Related languages (e.g., Czech and Lithuanian or Latvian) are not as similar as closely related languages, although there are still many similarities (because of a common origin and/or strong mutual influence). The morphological system is similar and there are many one-to-one correspondences in the lexics. Although the syntax is very similar, there are differences (as in the previous case) and, moreover, there are some constructions that do  90  not have direct counterparts in the other language. For example, Lithuanian halfparticiples can be expressed by Czech transgressives (e.g., Czech odešel nerozloučiv se [he left without saying good bye]-> išėjo neatsisveikinęs). On the other hand, Lithuanian gerunds have no direct counterpart in Czech (the transgressive has the same semantic function, but its use is restricted grammatically). They have to be expressed by other means, usually through nominalization or embedded sentence (e.g., Czech při výbuchu bomby zahynul člověk [a man has died by the bomb explosion] -> sprogus bombai žuvo žmogus). The remaining two categories in decreasing order of similarity (Languages with common origin and Languages of different origin) are irrelevant from the point of view of MT using simplified methods. If we look more closely on the set of "new" languages with regard to their mutual relatedness (or linguistic similarity), we must notice that although three languages are more or less isolated in the community (Hungarian (Finno-Ugric), Turkish and Maltese (Semitic)), the remaining languages are either related mutually or related to some of the languages of current EU members. The latter case is Estonian (Finno-Ugric), which is closely related to Finnish. The largest subgroup, namely six “new” languages, belong to the BaltoSlavic family: Czech, Polish, Slovak (West Slavic), Slovenian (South Slavic) and Latvian and Lithuanian (East Baltic). Moreover, Russian, an East Slavic language, is used widely in Latvia and Estonia and should therefore be also taken into account. 3. Typology of language similarity As the term similarity of languages (we intentionally replace the notion of relatedness by this notion due to the fact that from the point of view of our translation model it is more important than relatedness) is very vague, it is necessary to classify the similarity into several categories: • typological • morphological • syntactic • lexical. 3.1 Typological similarity The first type of similarity is the most important one. Due to the fact that in the following text we propose to use only shallow syntactic analysis which does not take into account syntactic relationship among larger constituents, it is clear that any difference in the constituent order on the  sentence level will have dire consequences for the translation quality. The shallow approach would definitely be unsuccessful for translation between languages of different typology. Let us take Czech and Lithuanian as an example of the language pair, which supports this claim. These languages have rich inflection and very high degree of word order freedom, thus we can suppose that it won’t be necessary to change the word order at the level of inner participants in most cases. On the other hand, both languages differ a lot in the lexics and morphology. For example, both Example 1 and Example 2 mean approximately Dad read a/the book. The difference between these two sentences is in their information structure. Example 1 should be translated as Dad read a book, whereas Example 2 means in fact The book has been read by Dad1. In the first sentence, the noun book is not contextually bound (it belongs to the focus), in the latter one it belongs to the topic. The category of voice differs in both sentences due to a strict word order in English, although in both Czech equivalents, active voice is used2. We can see that in the Lithuanian translation, the word order is exactly the same.  Example 1:  Czech: Otec  četl  Lith.: Tėvas  skaitė  Father(nom.) read(3sg., past)  knihu. knygą. book(acc.)  Example 2:  Czech: Knihu  četl  Lith.: Knygą  skaitė  Book(acc.) read(3sg., past)  otec. tėvas. father(nom.)  3.2 Lexical similarity  The lexical similarity does not mean that the vocabulary has to have the same origin, i.e., that words have to be created from the same (protostem). What is important for shallow MT (and for MT in general), is the semantic correspondence (preferably one-to-one relation between meanings of both words). Similar morphological systems simplify the transfer. For example, Slavonic languages (except  
The Translatica system originates from the PolEng project developed in 1996-2002 at Adam Mickiewicz University (AMU) in Poznań. PolEng developed a transfer-based system translating texts from Polish into English, with the lexicon based on Internet texts. Translatica expands PolEng capabilities by translation in the reverse direction and the usage of broader lexicon obtained from the contents of OPEP. The main features of PolEng inherited by Translatica are:  bottom-up parsing based on the CYK algorithm  phrasal structure representation  Perl-like formalism of transfer and synthesis rules. It is worth noting that the same formalism for description is used for both directions The formalism for the description of grammars is a kind of CFG. The authors have tried to use available resources to describe the English grammar, e.g. AGFL (2002)but it turned out that they would hardly comply with the elaborated translation engine (see Graliński (2002) for details on the engine). In order to use the engine it was necessary for us to compose grammar rules ourselves. According to the agreement between the authors of PolEng and the authors of OPEP, the lexicon for the English-to-Polish direction should be based on the OPEP contents. The paper reports the work that was done in 2003 in order to adopt OPEP contents to Translatica – the PolEng successor.  2. Building a lexicon for an MT system with the Polish language Two approaches for building an MT lexicon are mainly discussed in the literature. In Pinkham and Smets (2002) the authors distinguish between "HanC systems" – based on "Hand-crafted Dictionary" and the "Lead systems" – based on "Learned Dictionary". Systems of the first type use traditional bilingual dictionaries to determine the transfer between word senses, whereas in "Lead systems" the transfer part of the dictionary is trained on bilingual text corpora. The authors of the publication show the advantages of the "Lead approach" – particularly for new pairs of languages and limited time for development. By contrast, the experiments of Ilaraza, Mayor, and Sarasola (2001) demonstrate the advantages of building an MT lexicon on the basis of large traditional dictionaries. The authors compare translation produced by the system that uses raw bilingual dictionary to tha given by the system whose dictionary is a merge of a Basque lexical database and the Morris bilingual English-Basque dictionary. The authors state that the output of the latter system is distinctly better. Another dichotomy is mentioned by Baldwin, Hutchinson and Bond (1999). On the basis of English-Japanese translation the authors compare the systems that store entries as source/target language pairs to those which consider both languages separately. In the source/target approach "a word has as many senses as it has translation equivalents". In the latter approach sense distinctions are specific for each language, which in the authors' opinion is "more cognitively justifiable". The main argument for the "monolingual" approach in MT is that the decision on the sense disambiguation may be postponed until the process of generation whereas in the "bilingual approach" the semantic constraints on the source side are used for disambiguation in both source analysis and transfer. Another argument  98  against "bilingual" dictionaries is their unidirectionality. One of the main criteria for choosing the type of lexicon for an MT application is the availability of resources. Because of the scarcity of aligned Polish-English bi-texts the "Lead approach" has lost its main benefit for our purposes: rapid deployment – in order to use the approach it would be necessary to build aligned corpora (the same reason has determined the choice of the transfer method used for the translation, rather than the corpusbased one). On the other hand, our group have had to free disposal quite large traditional dictionaries: OPEP English-Polish dictionary and a few Polish dictionaries mentioned in section 4. The situation called for the "HanC approach". The decision whether the entries should be bilingual or monolingual was to large extend determined by the conditions of the agreement between the PolEng group and PWN. The dictionary publishers (as well as the authors of the system) liked the system to use the linguistic knowledge included in the dictionary material to the maximum extend. The system dictionary should mirror the OPEP material as closely as possible. This called for the bilingual description. The uni-directionality was not a counterargument either as the Polishto-English part had already been developed. 3. Main goals The main goals posed to the conversion process were:  to lose as little information as possible from OPEP  to extract and formalize syntactic and semantic information given in OPEP  to supplement data with all information necessary for transferbased machine translation – in accordance with the Translatica algorithm. 4. Resources Before the start of the work, the group consolidated the following resources: 1. Lexical resources at free disposal:  OPEP in the electronic form, XML format  PolEng lexicon (Polish-to-English)  Polish lexicon of inflected forms delivered by PWN – the  lexicon developed by the organization of Polish Scrabble players, further referred to as the scrabble dictionary  other dictionaries of Polish published by PWN, e.g. Bańko (2000)  lists of entries (e.g. proper nouns) from PWN encyclopedias. The PolEng lexicon comprises some syntactical information that could prove useful for the other direction; the scrabble dictionary handles Polish inflection exhaustively; Bańko (2000) includes some information on syntactical features of entries, in the form well suited for computer processing. 2. Lexical resources at limited disposal (via Internet), e.g.  Meriam-Webster Dictionary (www.mw.com)  Internet English-Polish dictionary (www.dict.pl) These and other dictionaries available on-line helped lexicographers understand the meaning of some entries or suggest alternative equivalents 3. Text corpora concordancers:  British National Corpus (http://sara.natcorp.ox.ac.uk)  Collins Cobuild Corpus (http://www.cobuild.collins.co.uk/for m.html)  WordCorp (http://www.webcorp.org.uk/index.ht ml)  PolEng Internet corpus – the corpus of Polish Internet texts collected while working with the Polish-English translation. Consulting such tools helped to verify syntactic features of words – such information is not given exhaustively in OPEP. WordNet has proved to play a key role in assigning semantic values. The semantic hierarchy used in Translatica is a subtree of the WordNet lattice. 4. Translation tools:  grammar description  syntactic-semantic parser  tools for transfer and synthesis. Translation tools impose specific constraints on the type of information that should be stored in the dictionary. 5. Translatica dictionary formalism  99  The dictionary formalism of the Translatica system assumes one lexicon for each direction (source/target approach). For example, the description of an entry in the English-to-Polish direction gives the constraints under which a word (or a phrase) is translated into appropriate equivalents. 5. Basic problem – time limitations As is shown in section 6, full and detailed conversion of a single entry consumes a lot of man-work (apart from computer work). The group could afford 27 man-months to accomplish the task of conversion (3 lexicographers, 9 months). The available time was not sufficient to manually elaborate each entry of OPEP (even after automatic preprocessing). The group assumed the following approach: • function words should be described almost from scratch • out of 55 900 entries in OPEP, ca 20 000 most frequent ones (according to BNC) should be conversed automatically and then elaborated manually • the rest of the dictionary should be conversed only automatically • errors resulting from manual and automatic conversion should be corrected semi-automatically. 6. Processing of the dictionary The process of dictionary conversion involved the following stages:  automatic conversion of dictionary information  automatic morphological description  manual description/verification of 20 000 most frequent lexemes  semi-automatic correction of errors  manual correction of errors found while testing the translation system. 6.1. Automatic conversion of dictionary information In Mayfield and McNamee (2002) the authors present an interesting idea that aims at simplifying the conversion of bilingual dictionaries from human-readable to computerreadable form. They have created a language called ABET (APL Bidict Extraction Tool) that allows for automating "the processes that are the same across most extraction tasks". At  the time the paper was written the authors had converted 50 MB of on-line "bidicts" of varying formats and the longest ABET script they needed consisted of mere twenty-four lines. Although we think that a language like ABET may prove beneficial in dealing with more than one dictionary of a simple format we do not think that the idea would work for traditional off-line dictionaries that include deep linguistic knowledge, rarely given in a systematic way. In our attempt to convert the dictionary we have come across so many specific "little problems" that it is hard to imagine for us that any generalizing language could be of much help. The script used for the conversion job was written in Perl. The section lists the major problems in the conversion (and does not mention those "little nuisances" which are particularly not amenable to description in a language of a higher level). The task consisted in the following steps:  
±Dept. of Mathematics U. of Maryland, College Park, Maryland ctate@math.umd.edu  C. R. Voss* *Multilingual Computing Group Army Research Lab Adelphi, Maryland voss@arl.army.mil  Abstract. How can recent advances in automating the evaluation of machine translation (MT) engines be applied to automate the evaluation of more complex embedded MT systems? In this paper, we describe initial evaluation testing of FALCon, an embedded MT system where hard-copy documents are scanned into bit-mapped images, converted into online text files via optical character recognition (OCR) software, and then translated by an MT engine from one natural language into another. Our challenge with the ongoing support of FALCon systems is to evaluate when the replacement of a particular component with a new or upgraded product will yield significant improvements over the baseline system performance. In this paper we address the following questions: (i) how can we automate the baseline end-to-end evaluation of this system? and (ii) what is the relation between the accuracy of the individual components and their end-to-end accuracy, that could be used to model the system performance?  1. Introduction How can recent advances in automating the evaluation of machine translation (MT) engines1 be applied to automate the evaluation of more complex embedded MT systems2? Consider, for example, FALCon, an embedded system whose process flow appears in Figure 1. First the hard-copy document pages are scanned into bit-mapped images, then converted into online text files via optical character recognition (OCR) software, and then translated by an MT engine from one natural language into another [3,4]. Our challenge with the ongoing support of FALCon systems in the field is to evaluate when the replacement of a particular component with a new or upgraded product will yield significant improvements in system performance. In this paper we address the following questions: (i) how can we automate the baseline end-to-end evaluation of this system? and (ii) what is the relation between the accuracy of the individual components and their end-to-end 
 Abstract. We describe a Machine Translation (MT) approach that is specifically designed to enable rapid development of MT for languages with limited amounts of online resources. Our approach assumes the availability of a small number of bi-lingual speakers of the two languages, but these need not be linguistic experts. The bi-lingual speakers create a comparatively small corpus of word aligned phrases and sentences (on the order of magnitude of a few thousand sentence pairs) using a specially designed elicitation tool. From this data, the learning module of our system automatically infers hierarchical syntactic transfer rules, which encode how syntactic constituent structures in the source language transfer to the target language. The collection of transfer rules is then used in our run-time system to translate previously unseen source language text into the target language. We describe the general principles underlying our approach, and present results from an experiment, where we developed a basic Hindi-to-English MT system over the course of two months, using extremely limited resources.  1. Introduction Corpus-based Machine Translation (MT) approaches such as Statistical Machine Translation (SMT) (Brown et al, 1990), (Brown et al, 1993), (Vogel and Tribble, 2002), (Yamada and Knight, 2001), (Papineni et al, 1998), (Och and Ney, 2002) and Example-based Machine Translation (EBMT) (Brown, 1997), (Sato and Nagao, 1990) have received much attention in recent years, and have significantly improved the state-of-the-art of Machine Translation for a number of different language pairs. These approaches are attractive because they are fully automated, and require orders of magnitude less human labor than traditional rulebased MT approaches. However, to achieve reasonable levels of translation performance, the corpus-based methods require very large volumes of sentence-aligned parallel text for the two languages – on the order of magnitude of a million words or more. Such resources are currently available for only a small number of language pairs. While the amount of online resources for many languages will undoubtedly grow over time, many of the languages spoken by smaller ethnic groups and populations in  the world will not have such resources within the foreseeable future. Corpus-based MT approaches will therefore not be effective for such languages for some time to come. Our MT research group at Carnegie Mellon, under DARPA and NSF funding, has been working on a new MT approach that is specifically designed to enable rapid development of MT for languages with limited amounts of online resources. Our approach assumes the availability of a small number of bi-lingual speakers of the two languages, but these need not be linguistic experts. The bi-lingual speakers create a comparatively small corpus of word aligned phrases and sentences (on the order of magnitude of a few thousand sentence pairs) using a specially designed elicitation tool. From this data, the learning module of our system automatically infers hierarchical syntactic transfer rules, which encode how constituent structures in the source language transfer to the target language. The collection of transfer rules is then used in our runtime system to translate previously unseen source language text into the target language. We refer to this system as the “Trainable Transfer-based MT System”, or in short the XFER system.  116  In this paper, we describe the general principles underlying our approach, and the current state of development of our research system. We then describe an extensive experiment we conducted to assess the promise of our approach for rapid rampup of MT for languages with limited resources: a Hindi-to-English XFER MT system was developed over the course of two months, using extremely limited resources on the Hindi side. We compared the performance of our XFER system with our inhouse SMT and EBMT systems, under this limited data scenario. The results of the experiment indicate that under these extremely limited training data conditions, when tested on unseen data, the XFER system significantly outperforms both EBMT and SMT. We are currently in the middle of yet another two-month rapid-development application of our XFER approach, where we are developing a Hebrew-to-English XFER MT system. Preliminary results from this experiment will be reported at the workshop. 2. Trainable Transfer-based MT Overview The fundamental principles behind the design of our XFER approach for MT are: (1) that it is possible to automatically learn syntactic transfer rules from limited amounts of word-aligned data; (2) that such data can be elicited from non-expert bilingual speakers of the pair of languages; and (3) that the rules learned are useful for machine translation between the two languages. We assume that one of the two languages involved is a “major” language (such as English or Spanish) for which significant amounts of linguistic resources and knowledge are available. The XFER system consists of four main sub-systems: elicitation of a word aligned parallel corpus; automatic learning of transfer rules; the run time transfer system; and a statistical decoder for selection of a final translation output from a large lattice of alternative translation fragments produced by the transfer system. The architectural design of the XFER system in a configuration in which translation is performed from a limited-resource language to a major language is shown in Figure 1.  Figure 1. Architecture of the XFER MT System and its Major Components Figure 2. The Elicitation Tool as Used to Translate and Align an English Sentence to Hindi. 3. Elicitation of Word-Aligned Parallel Data The purpose of the elicitation sub-system is to collect a high quality, word aligned parallel corpus. A specially designed user interface was developed to allow bilingual speakers to easily translate sentences from a corpus of the major language (i.e. English) into their native language (i.e. Hindi), and to graphically annotate the word alignments between the two sentences. Figure 2 contains a snapshot of the elicitation tool, as used in the translation and alignment of an English sentence to Hindi. The informant must be bilingual and literate in the language of elicitation and the language being elicited, but does not need to have knowledge of linguistics or computational linguistics. The word-aligned elicited corpus is the primary source of data from which transfer rules are inferred by our system. In order to support effective rule learning, we designed a “controlled” English elicitation corpus. The design of this corpus was based on elicitation principles from field linguistics, and the variety of phrases and sentences attempts to cover a wide variety of linguistic phenomena that the minor language may or may not possess. The elicitation process is organized along “minimal pairs”, which allows us to identify whether the minor languages possesses specific linguistic  117  phenomena (such as gender, number, agreement, etc.). The sentences in the corpus are ordered in groups corresponding to constituent types of increasing levels of complexity. The ordering supports the goal of learning compositional syntactic transfer rules. For example, simple noun phrases are elicited before prepositional phrases and simple sentences, so that during rule learning, the system can detect cases where transfer rules for NPs can serve as components within higher-level transfer rules for PPs and sentence structures. The current controlled elicitation corpus contains about 2000 phrases and sentences. It is by design very limited in vocabulary. A more detailed description of the elicitation corpus, the elicitation process and the interface tool used for elicitation can be found in (Probst et al, 2001), (Probst and Levin, 2002). 4. Automatic Transfer Rule Learning The rule learning system takes the elicited, wordaligned data as input. Based on this information, it then infers syntactic transfer rules. The learning system also learns the composition of transfer rules. In the compositionality learning stage, the learning system identifies cases where transfer rules for “lower-level” constituents (such as NPs) can serve as components within “higher-level” transfer rules (such as PPs and sentence structures). This process generalizes the applicability of the learned transfer rules and captures the compositional makeup of syntactic correspondences between the two languages. The output of the rule learning system is a set of transfer rules that then serve as a transfer grammar in the run-time system. The transfer rules are comprehensive in the sense that they include all information that is necessary for parsing, transfer, and generation. In this regard, they differ from “traditional” transfer rules that exclude parsing and generation information. Despite this difference, we will refer to them as transfer rules. The design of the transfer rule formalism itself was guided by the consideration that the rules must be simple enough to be learned by an automatic process, but also powerful enough to allow manually-crafted rule additions and changes to improve the automatically learned rules. The following list summarizes the components of a transfer rule. In general, the x-side of a transfer rules refers to the source language (SL), whereas the y-side refers to the target language (TL).  Figure 3. An Example Transfer Rule along with its Components 1. Type information: This identifies the type of the transfer rule and in most cases corresponds to a syntactic constituent type. Sentence rules are of type “S”, noun phrase rules of type “NP”, etc. The formalism also allows for SL and TL type information to be different. 2. Part-of speech/constituent information: For both SL and TL, we list a linear sequence of components that constitute an instance of the rule type. These can be viewed as the “righthand sides” of context-free grammar rules for both source and target language grammars. The elements of the list can be lexical categories, lexical items, and/or phrasal categories. 3. Alignments: Explicit annotations in the rule describe how the set of source language components in the rule align and transfer to the set of target language components. Zero alignments and many-to-many alignments are allowed. 4. X-side constraints: The x-side constraints provide information about features and their values in the source language sentence. These constraints are used at run-time to determine whether a transfer rule applies to a given input sentence. 5. Y-side constraints: The y-side constraints are similar in concept to the x-side constraints, but they pertain to the target language. At run-time, y-side constraints serve to guide and constrain the generation of the target language sentence. 6. XY-constraints: The xy-constraints provide information about which feature values transfer from the source into the target language. Specific TL words can obtain feature values from the source language sentence. Figure 3 shows an example transfer rule along with all its components.  118  Learning from elicited data proceeds in three stages: the first phase, Seed Generation, produces initial “guesses” at transfer rules. The rules that result from Seed Generation are “flat” in that they specify a sequence of parts of speech, and do not contain any non-terminal or phrasal nodes. The second phase, Compositionality Learning, adds structure using previously learned rules. For instance, it learns that sequences such as “Det N PostP” and “Det Adj N PostP” can be re-written more generally as “NP PostP”, as an expansion of PP in Hindi. This generalization process can be done automatically based on the flat version of the rule, and a set of previously learned transfer rules for NPs. The first two stages of rule learning result in a collection of structural transfer rules that are context-free – they do not contain any unification constraints that limit their applicability. Each of the rules is associated with a collection of elicited examples from which the rule was created. The rules can thus be augmented with a collection of unification constraints, based on specific features that are extracted from the elicited examples. The constraints can then limit the applicability of the rules, so that a rule may succeed only for inputs that satisfy the same unification constraints as the phrases from which the rule was learned. A constraint relaxation technique known as “Seeded Version Space Learning” attempts to increase the generality of the rules by identifying unification constraints that can be relaxed without introducing translation errors. While the first two steps of rule learning are currently well developed, the learning of appropriately generalized unification constraints is still in a preliminary stage of investigation. Detailed descriptions of the rule learning process can be found in (Probst et al, 2003). 5. The Runtime Transfer System At run time, the translation module translates a source language sentence into a target language sentence. The output of the run-time system is a lattice of translation alternatives. The alternatives arise from syntactic ambiguity, lexical ambiguity, multiple synonymous choices for lexical items in the dictionary, and multiple competing hypotheses from the rule learner. The runtime translation system incorporates the three main processes involved in transfer-based MT: parsing of the SL input, transfer of the parsed constituents of the SL to their corresponding structured constituents on the TL side, and generation of the TL output. All three of these processes are performed based on the  transfer grammar – the comprehensive set of transfer rules that are loaded into the runtime system. In the first stage, parsing is performed based solely on the “x” side of the transfer rules. The implemented parsing algorithm is for the most part a standard bottom-up Chart Parser, such as described in (Allen, 1995). A chart is populated with all constituent structures that were created in the course of parsing the SL input with the sourceside portion of the transfer grammar. Transfer and generation are performed in an integrated second stage. A dual TL chart is constructed by applying transfer and generation operations on each and every constituent entry in the SL parse chart. The transfer rules associated with each entry in the SL chart are used in order to determine the corresponding constituent structure on the TL side. At the word level, lexical transfer rules are accessed in order to seed the individual lexical choices for the TL word-level entries in the TL chart. Finally, the set of generated TL output strings that corresponds to the collection of all TL chart entries is collected into a TL lattice, which is then passed on for decoding. A more detailed description of the runtime transfer-based translation sub-system can be found in (Peterson, 2002). 6. Target Language Decoding In the final stage, a statistical decoder is used in order to select a single target language translation output from a lattice that represents the complete set of translation units that were created for all substrings of the input sentence. The translation units in the lattice are organized according the positional start and end indices of the input fragment to which they correspond. The lattice typically contains translation units of various sizes for different contiguous fragments of input. These translation units often overlap. The lattice also includes multiple word-to-word (or word-to-phrase) translations, reflecting the ambiguity in selection of individual word translations. The task of the statistical decoder is to select a linear sequence of adjoining but nonoverlapping translation units that maximizes the probability of the target language string given the source language string. The probability model that is used calculates this probability as a product of two factors: a translation model for the translation units and a language model for the target language. The probability assigned to translation units is based on a trained word-to-word probability model. A standard trigram model is used for the target language model.  119  The decoding search algorithm considers all possible sequences in the lattice and calculates the product of the language model probability and the translation model probability for the resulting sequence of target words. It then selects the sequence which has the highest overall probability. As part of the decoding search, the decoder can also perform a limited amount of re-ordering of translation units in the lattice, when such reordering results in a better fit to the target language model. 7. Construction of the Hindi-to-English System As part of a DARPA “Surprise Language Exercise”, we quickly developed a Hindi-to-English MT system based on our XFER approach over a twomonth period. The training and development data for the system consisted entirely of phrases and sentences that were translated and aligned by Hindi speakers using our elicitation tool. Two very different corpora were used for elicitation: our “controlled” typological elicitation corpus and a set of NP and PP phrases that we extracted from the Brown Corpus section of the Penn Treebank. We estimated the total amount of human effort required in collecting, translating and aligning the elicited phrases based on a sample. The estimated time spent on translating and aligning a file (of 200 phrases) was about 8 hours. Translation took about 75% of the time, and alignment about 25%. We estimate the total time spent to be about 700 hours of human labor. We acquired a transfer grammar for Hindito-English transfer by applying our automatic learning module to the corpus of word-aligned data. The learned grammar consists of a total of 327 rules. In a second round of experiments, we assigned probabilities to the rules based on the frequency of the rule (i.e. how many training examples produce a certain rule). We then pruned rules with low probability, resulting in a grammar of a mere 16 rules. As a point of comparison, we also developed a small manual transfer grammar. The manual grammar was developed by two non-Hindi-speaking members of our project, assisted by a Hindi language expert. Our grammar of manually written rules has 70 transfer rules. The grammar includes a rather large verb paradigm, with 58 verb sequence rules, ten recursive noun phrase rules and two prepositional phrase rules. Figure 4 shows an example of recursive NP and PP transfer rules.  Figure 4. Recursive NP and PP Transfer Rules for Hindi to English Translation In addition to the transfer grammar, the XFER system requires a word-level translation lexicon. The Hindi-to-English lexicon we constructed contains entries from a variety of sources. One source for lexical translation pairs is the elicited corpus itself. The translations pairs can simply be read off from the alignments that were manually provided by Hindi speakers. Because the alignments did not need to be 1-to-1, the resulting lexical translation pairs can have strings of more than one word one either the Hindi or English side or both. Another source for lexical entries was an English-Hindi dictionary provided by the Linguistic Data Consortium (LDC). Two local Hindi experts “cleaned up” a portion of this lexicon, by editing the list of English translations provided for the Hindi words, and leaving only those that were “best bets” for being reliable, all-purpose translations of the Hindi word. The full LDC lexicon was first sorted by Hindi word frequency (estimated from Hindi monolingual text) and the cleanup was performed on the most frequent 12% of the Hindi words in the lexicon. The “clean” portion of the LDC lexicon was then used for the limited-data experiment. This consisted of 2725 Hindi words, which corresponded to about 10,000 translation pairs. This effort took about 3 days of manual labor. To create an additional resource for high-quality translation pairs, we used monolingual Hindi text to extract the 500 most frequent bigrams. These bigrams were then translated into English by an expert in about 2 days. Some judgment was applied in selecting bigrams that could be translated reliably out of context. Finally, our lexicon contains a number of manually written phrase-level rules. The system we put together also included a morphological analysis module for Hindi input. The morphology module used is the IIIT Morpher (IIIT  120  Morphology Module). Given a fully inflected word in Hindi, Morpher outputs the root and other features such as gender, number, and tense. To integrate the IIIT Morpher with our system, we installed it as a server. 8. Hindi-to-English Translation Evaluation The evaluation of our XFER-based Hindi-to-English MT system compares the performance of this system with an SMT system and EBMT system that were trained on the exact same training data as our XFER system. The limited training data consists of: • 17,589 word-aligned phrases and sentences from the elicited data collection. This includes both our translated and aligned controlled elicitation corpus, and also the translated and aligned uncontrolled corpus of noun phrases and prepositional phrases extracted from the Penn Treebank. • A Small Hindi-to-English Lexicon: 23,612 “clean” translation pairs from the LDC dictionary. • A small amount of manually acquired lexical resources (as described above). The limited data setup includes no additional parallel Hindi-English text. The total amount of bilingual training data was estimated to amount to about 50,000 words. A small, previously unseen, Hindi text was selected as a test-set for this experiment. The testset chosen was a section of the data collected at Johns Hopkins University during the later stages of the DARPA Hindi exercise, using a web-based interface. The section chosen consists of 258 sentences, for which four English reference translations are available. The following systems were evaluated in the experiment: 1. Three versions of the Hindi-to-English XFER system: 1a. XFER with No Grammar: the XFER system with no syntactic transfer rules (i.e. only lexical phrase-to-phrase matches and word-toword lexical transfer rules, with and without morphology). 1b. XFER with Learned Grammar: The XFER system with automatically learned syntactic transfer rules. 1c. XFER with Manual Grammar: The XFER system with the manually developed syntactic transfer rules. 2. SMT: The CMU Statistical MT (SMT) system (Vogel et al, 2003), trained on the limited-data parallel text resources.  3. EBMT: The CMU Example-based MT (EBMT) system (Brown, 1997), trained on the limiteddata parallel text resources. 4. MEMT: A “multi-engine” version that combines the lattices produced by the SMT system, and the XFER system with manual grammar. The decoder then selects an output from the joint lattice. Performance of the systems was measured using the NIST scoring metric (Doddington, 2002), as well as the BLEU score (Papineni et al, 2002). In order to validate the statistical significance of the differences in NIST and BLEU scores, we applied a commonly used sampling technique over the test set: we randomly draw 258 sentences independently from the set of 258 test sentences (thus sentences can appear zero, once, or more in the newly drawn set). We then calculate scores for all systems on the randomly drawn set (rather than the original set). This process was repeated 10,000 times. Median scores and 95% confidence intervals were calculated based on the set of scores. The results for the various systems tested can be seen in Table 1 below. Figure 5 shows the NIST score results with different reordering windows within the decoder.  Table 1. System Performance Results for the Various Translation Approaches  System EBMT SMT XFER no gra XFER learn gra XFER man gra MEMT  BLEU 0.058 0.102 (+/- 0.016) 0.109 (+/- 0.015) 0.112 (+/- 0.016) 0.135 (+/- 0.018) 0.136 (+/- 0.018)  NIST 4.22 4.70 (+/- 0.20) 5.29 (+/- 0.19) 5.32 (+/- 0.19) 5.59 (+/- 0.20) 5.65 (+/- 0.21)  NIST Score  6 5.8 5.6 5.4 5.2 5 4.8 4.6 4.4 4.2 4 0  NIST scores  
There is abundant evidence that language technologies can only be developed using large bodies of language resources (LRs) for language modelling, as test beds, for evaluation, example bases, and terminology source. The need for LRs applies both for research and for commercial applications. Not only raw data, but also ‘derived’ LRs, e.g. annotated corpora, lexica and grammars, as well as tools for manipulating data form part of the material of interest. The production of such LRs also enables the linguistic cultural heritage of a community or nation to be preserved in an age of digital access and storage. This is the reason the NEMLAR project was started. There is a strong interest in supporting the Arabic language, in the region, in Europe and elsewhere. The project runs 20032005. The NEMLAR project covers recognised European centres and recognised partners in 6 non-EU Mediterranean countries, namely Jordan, Morocco, Egypt, Lebanon, Tunisia, West Bank and Gaza Strip. 2. NEMLAR goals The goal of the NEMLAR (Network for EuroMediterranean LAnguage Resources) project is to create a network of qualified Euro-Mediterranean partners to specify and support the development of high priority LRs for Arabic and other local languages in a systematic, standards-driven, collaborative learning context. The project will focus on identifying the state of the art of LRs in the region, assessing priority requirements  through consultations with language industry and communication players, and establishing a basic LR kit for the major forms of the region's predominant language - Arabic, and other local wide-spoken languages where appropriate. This knowledge base has appeared in its first version (Nikkhou et al. 2004). 3. Survey: key players, language resources and industrial needs It is a key part of this project to provide knowledge about the language technology players, projects (ongoing activities), products etc. So a 'mapping' is made covering all Mediterranean countries participating in the project, resulting in a knowledge base with details of all universities, research institutions and companies, as well as ongoing projects, and existing products, - with relation to Language Resources (LRs). This knowledge base is ready in its first instance. It covers 35 institutional players in the region, and some 20 individual players. It will further develop during the lifetime of the project, but we believe to have identified the most important players already. Of the 35 institutional players, 22 are based in Arabic speaking countries, obviously in particular in our partner countries. E.g. there are 8 entities in Egypt, 4 in Lebanon, 3 in Jordan and 3 in Palestine. Kuwait houses the Sakhr company with subsidiaries in many countries, incl. Egypt. In Europe, companies such as Systran and France Télécom also take an interest in Arabic language processing.  124  3.1 Tools and LRs Existing Arabic tools and LRs in the region, in Europe or elsewhere have been identified, and the first version of the survey report describes stateof-the-art of LRs for the languages of the region. It should be stressed that the survey has focused on resources in the region, not in the whole world.  Table 1: Number of tools  Arabic NLP technologies and tools 31  Speech processing technologies  11  Text processing technologies  11  Here NLP tools are modules that normally are parts of systems, e.g. morphological analyzer, POS tagger, language identifier, term finder etc. Also classified as NLP tools are research results that have not yet been commercialised, e.g. grammar checker, grapheme recognition for OCR. It has been encouraging to see that e.g. POS taggers do exist, and not only at the universities, but also as products. Morphological analyzers exist at the universities and also as a component of commercial products, e.g. machine translation. It is important to make morphological analyzers available in a source format, so that researchers can further elaborate on the morphological analysis and can combine this analysis with other components in their efforts to gain new insights and develop ideas for new and better language modules. Syntactic analyzers exist in some universities, and as an important part of e.g. MT systems. Overall, it seems at present that there is no large scale grammar and parser freely available for researchers. It is foreseeable that commercially developed syntactic analyzers cannot be made available, so we believe that some interest should go into investigating the existence of syntactic analyzers and the possibilities of developing them further. Speech processing technologies cover Arabic text-to-speech, speech recognition, speaker recognition etc. Arabic text-to-speech exists in several versions as products. Arabic text-to-speech is of good quality which can be easily compared to similar tools for other languages. It would be important to identify open source modules which can be used by researchers for further improvement and research.  At present we have identified the following amount of language resources:  Table 2: Number of language resources  Speech databases  22  Lexical databases  29  Text corpora  24  Multimodal resources  
 1. Introduction Ambiguity is the most considerable problem in natural language processing systems, among which machine translation systems suffer this problem in a high degree. The problem of ambiguity in translating texts by the machine is different from one by the human. Human is a complex machine so that he can choose the suitable target equivalent(s) of any source language forms, sometimes without becoming aware of the irrelevant alternatives, based on his understanding in the context. He can also automatically consider a group of the words, rather than individual word to understand the meaning of a sentence, even if the words of the group are not relevant. But a machine cannot think at all. Since only written texts are presented to the computer, it can not mechanically use the relevant text. However, this problem has been somehow solved by the field of discourse analysis which is not within the scope of this study. Nowadays the application of statistical approaches and studying statistical-based methods in natural language processing as well as in machine translation has been rapidly increasing. Statistical linguistics basically relies on the studying of various linguistic units occurrences frequencies including word-forms, lexemes, morphemes, letters, etc. in a sample corpus, and solving various linguistic problems as ambiguity  with reference to these certain frequencies and calculating the probability of them. Statisticsbased approaches factor out the need for computational mechanisms and high linguistic knowledge for solving linguistic problems. So computational cost of a statistics-based approach is much lower than a knowledge-based or a rulebased approach (Su and Chang, 1990). A statisticsbased system needs a large database or corpus to guarantee its reliability, however, with availability of many tagged and untagged text corpora, acquiring linguistic knowledge from a large sample corpus is no longer an impossible task (Garside et al, 1987) . Lexical ambiguity refers to a case in which either a lexical unit belongs to different lexical categories with different senses, or a lexical unit for which there are more than one sense while these different senses fall into the same lexical category (Mosavi miangah, 2000). Our concern in this study is solving the second type of lexical ambiguity, that is, those lexical ambiguities in which the different senses of a word fall into the same lexical category using statistical information about different equivalents of English ambiguous words in the target language, Persian. By statistical information we mean calculating the occurrences or co-occurrences frequencies of the ambiguous word equivalents in the target language and  129  selecting the most appropriate equivalent for every ambiguous word using a statistical model. Several statistics-based methods for word sense disambiguation have been recently developed using a large tagged or untagged corpus. However, most of these systems are dealing with lexical ambiguity of the first type mentioned above, that is, those lexical units which belong to different lexical categories. A method for word sense disambiguation which was described by Marshall is known as CLAWS system. The main feature of this system is using a collocational probabilities matrix showing the relative likelihood of co-occurrences of all ordered units previously tagged by WORDTAG program. CLAWS uses a large proportion of the tagged Brown corpus to generate statistics of each tag frequency and also the frequency of any two tags adjacent to each other (Marshall, 1983). Another system known as VOLSONGA has been designed to overcome the nonpolynomial complexity of CLAWS. This system does not use tag triple and idioms, and by this reason it is not necessary to manually construct special lists. VOLSUNGA considers only two successive tags in each time of calculation and so reduces the algorithm from exponential complexity to linear. It only requires a tagged corpus based on which establishes its tables of probabilities (De rose, S. J. 1988) . Considering the two methods of disambiguation mentioned above we can see that they use a previously tagged corpus to disambiguate those lexical units which have more than one lexical category. After determining the suitable category for a word by these methods, in the case that it has more than one meaning in the limits of its category, the problem of ambiguity still remains. A rather novel method for disambiguation of multiple-meaning words presented by Dagan and Itai tries to select the most probable sense of a word using frequencies of the related word combinations in a second language corpus. In this method the word combinations fall in the limits of the syntactic tuples in the second language. However, first of all the system identifies syntactic relations between words using a source language parser and maps the alternative interpretations of  these relations to the target language using a bilingual lexicon (Dagan and Itai, 1994). Koehn and Knight (2000) have also developed a novel method in which they use only unrelated monolingual corpora and a lexicon. By estimating word translation probabilities using the EM (expectation maximization) algorithm, they extended upon target language modeling. They propose the use of syntactic relations such as subject-verb, verb-object, adjective-noun to disambiguate word translations. However, Koehn and Knight focus in their experiment on nouns to simplify their experimental setup. They combine the notion of translation probabilities with the use of context. The method presented by this study is somehow similar to the two methods mentioned above with some kinds of manipulations in order to conform to the properties of Persian as the target language. We consider the co-occurrences of the multiple-meaning words in a monolingual corpus of the target language, namely, Persian. Calculating the frequencies of these words in the corpus we can select the most probable sense for these multiple meaning words. However, instead of considering syntactic tuples in the target language corpus we consider only certain cooccurrences words in that corpus without having a syntactic analysis for corpus. In this method there is no need to analyze the second language corpus from the syntactic point of view. The only task of our algorithm for gaining the required statistical information is determining the nearest noun, pronoun, adjective or verb to our ambiguous word whether it is a noun, a verb, an adjective or an adverb. Table 1. describes the conventions in detail. However, with applying this method for the pair of English and Persian languages only a small portion of ambiguous words in English can be correctly translated into Persian. For some others even the use of syntactic tuples and syntactic relations between the ambiguous word and other words in the corpus has not been satisfactorily successful. So, the most appropriate and convenient way to disambiguate the multiplemeaning words seems to specify the domain or field of texts to which such words belong. In the following lines we will discuss the matter in detail.  130  ambiguous word possible combinations respectively  example  if  noun (N)  1) adjective + N 2) noun + N 3) N + noun 4) verb + N 5) N + verb  sandy bank river bank bank robber He borrows from the bank. The bank works 24 hours a day.  if adjective (Aj)  1) Aj+ noun 2) noun/pronoun +Aj  old man He is old.  if verb (V)  1) V + noun/pronoun 2) noun/pronoun + V  He minds the baby. Do you mind the baby  if adverb (Av)  1) verb + Av 2) Av + verb  He has not yet seen it. Yet, you should go further.  Table 1. Possible combinations (co-occurrences) of an ambiguous word with the other parts of speech in source language  1. 1. Persian background Persian is a member of synthetic language family. It means that in Persian a new word is created by adding prefix, suffix, infix or another noun, adjective, preposition or verb to the beginning or the end of the word. In these cases the basic form of the word or verb stem usually is not broken (Mosavi miangah, 2001). Grammatical word order in Persian is shown as SOV, although a rather free word order is also possible but not grammatically acceptable. In this language every verb has two stems, present stem and past stem, and different inflectional forms of a verb is constructed using either the present or past stem. Persian uses Arabic alphabet. Texts are written from right to left. Short vowels (a, e, o) are usually not written; only the long vowels (y, u, a:) are represented in the text. Persian morphology is an affixal system consisting mainly of suffixes and a few prefixes. The nominal paradigm consists of a relatively small number of affixes. The verbal inflectional system is quite regular and can be obtained by the combination of prefixes, stems, inflections and auxiliaries. The elements within a noun phrase are linked by the enclitic particle called ezafe. This morpheme is usually an unwritten vowel, but it could also have an orthographic realization in certain phonological environments. Adjectives follow the same morphological patterns as nouns.  They can also appear with comparative and superlative morphemes. Certain adverbs, mainly manner adverbs, can behave like adjectives and can appear with all the adjectival affixes. The inflectional system for the Persian verbs consists of simple forms and compound forms; the latter are forms that require an auxiliary verb. The simple forms are divided into two groups according to the stem they use in their formation, present or past. The citation form for the verb is the infinitive (Megerdoomian, 2000). 2. Linguistic model Our model has been implemented within the framework of Marchuk's theory of machine translation called as "the theory of translation equivalencies" or " translational correspondencies". It is based on an assumption that translation per se (as opposed to the interpretation of the source text context) may be and should be performed using only the means offered by the systems of the languages involved (Marchuk, 1988, and Miram, 1998). In order to carry out the experiment, first of all we need an automatic bilingual dictionary of English to Persian to be able to distinguish all possible translations of each word especially those of the ambiguous words which are our aim in this study. For determining the correct equivalent of each ambiguous word in the certain sentences in  131  the source language, namely, English our algorithm searches each of its alternative translations (within a single part of speech) in the monolingual corpus of the target language, namely, Persian and calculate the frequencies of their occurrence along with their nearest linguistic units referring to the table 1 separately. Then by the help of a simulation model the most probable alternative for every English multiple-meaning word are selected as the most appropriate Persian equivalent for that word. Consider, for example the following English sentence extracted from the textbook "psychology applied to teaching" "Tentative analysis of the behavior has been provided an acceptable perception of learning process by which we can overcome many problems of the primary students." (Biehler, 1974) In above sentence the words underlined have more than one equivalents in Persian although in English they may not be known as ambiguous words. In the following lines we illustrate Persian translation of this English sentence including all alternatives for each of its ambiguous word. Tajziyeye azmayeshiye raftar darke ghabeleghabuli az farayande yadgiri/ amuzesh/danesh be dast dadeh ast/tahiyeh kardeh ast, ke be vasileye an ma mitavanim bar besyari az moshkelate daneshamuzane/ danesgjuyane ebtedaii/avaliye ghalabeh konim. The verb "provide" has two equivalents in Persian. For selecting the most suitable one we should compare its co-occurrence frequency with its complement which is the nearest noun by which it follows, here, "perception". Referring to our Persian corpus we extract the following cooccurrences for the word combination "to provide perception": be dast dadane dark 14 times, and tahiyeh kardane dark 0 times. Naturally we prefer the first case for the best suitable equivalent for the verb "provide". To find the most appropriate equivalent for the word "learning" in Persian we should calculate the frequency of its alternative cooccurrences with the nearest noun (here, process) in a monolingual corpus of its related field (here, psychology and learning). Referring to our monolingual Persian corpus in this field we see that the noun phrase farayande yadgiri has been appeared 240 times on our corpus, the noun phrase farayande amuzesh has been appeared 20 times and farayande danesh 0 times. Using statistical  model we prefer yadgiri to amuzesh as the more appropriate translation for the word "learning"1 The English noun phrase "primary students" also has four alternative translations in Persian as daneshamuzane ebtedaii, daneshamuzane avaliye, daneshjuyane ebtedaii and daneshjuyane avaliye. Considering our Persian corpus we can see that the first combination has been appeared 150 times, the second 15 times, the third 0 times and finally the last one 8 times in our corpus. Using our statistical model we can choose the alternative daneshamuzane ebtedaii as the best equivalent translation of English noun phrase "primary students". 2. 1. Processing steps In the following lines it has been tried to describe the processing steps of this experiment in detail. To begin with, it is necessary to compile a bilingual lexicon from English to Persian including all possible translations of each English word into Persian. This kind of lexicon or dictionary is already available in the form of the automatic dictionary of English to Persian. As every one knows, many English words belong to different parts of speech and in our dictionary there is one Persian equivalent for each part of speech of the given words unless it has more than one equivalent in a single part of speech. The latter case is the target word for disambiguation in this experiment. Determination of the word part of speech is supposed to be carried out by a syntactic parser and some context-frame rules (Mosavi miangah, 2002). Thus, the reminding ambiguities which fall within the scope of a single part of speech have to be resolved by the present procedure. After distinguishing the word for which we want to find the suitable target equivalent, the next stage naturally is to collect a Persian monolingual corpus in which we can find different equivalents of the mentioned word accompanied by some certain nouns, pronouns, adjectives or verbs with different frequencies. In this stage we try to collect a separate Persian corpus for each field of knowledge and then we can refer to that special corpus of the target language considering the subject matter of our source text. To gain statistical data from the Persian corpus we are mainly concerned with the 
1. Introduction The compromise between a natural language grammar’s expressive power and the performance of the resulting system is a crucial issue for the developer of any MT application. By opting for a high-performance approach, such as finite state technology, one inevitably has to sacrifice descriptive power, whilst theoretically motivated formalisms such as HPSG or LFG rarely have implementations that are efficient enough for commercial use. The MetaMorpho formalism, and Moose, our parser implementation, is an attempt at a reasonable compromise. 2. The MetaMorpho formalism 2.1 Bottom-up parsing A MetaMorpho rule is illustrated in Figure 1. The grammar operates with rule pairs that consist of one rewrite rule used during bottom-up parsing and one or more corresponding transfer rules that  are applied during top-down generation. The set of parse rules effectively forms a phrase-structure (context free) grammar where each symbol has a list of typed features. Features can either select from a set of symbolic values, i.e., {SG, PL}, or contain a string, such as the lexical form of a structure’s head; however, the formalism does not allow for embedded feature structures or functions. The right-hand side of the parse rule can state conditions for any of its symbols’ values. At this level, all conditions and assignments must be stated explicitly: there are no designated head or foot features, nor is there any mechanism to automatically percolate features when a unification takes place. 2.2 Kills Each rule can state any number of overrides or kills on other rules: if the “killer” rule fires over a specific range of the input, it blocks the “killed” one over the same range and any productions of that rule are removed from the chart. This  LHS symbol LHS assignment Rule title Unique ID RHS conditions listed in brackets RHS symbol  *VP=meet+DOBJ:0401311343 EN.VP[TV.conj] = TV(lex=”meet”, pass=NO) + DOBJ HU.VP(focus=NO, EN.DOBJ.reqfocus=YES) = DOBJ[case=INS] + TV[lex=”találkozik”, VP.tense] HU.VP = TV[lex=”találkozik”, VP.tense] + DOBJ[case=INS]  Transfer (gen) rules  LHS condition  Gen RHS sumbol RHS assignment  Figure 1. Overview of a MetaMorpho rule illustrating the transitive subcategorization of “meet.” Several features have been omitted for the sake of clarity.  138  mechanism of overrides extends the expressive power of the grammar beyond that of pure context-free formalisms. Kills are extensively used for syntactic disambiguation: if a PP, for instance, figures in two alternative parses as a free adjunct and as the complement of a VP, respectively, the latter analysis will override the first. 2.3 Top-down generation When the whole input is processed and no applicable rules remain, generation proceeds topdown from the root symbols by firing the transfer rule corresponding to the parse rule that created the edge at parse time – a solution we term immediate transfer as it uses no separate transfer mechanism or target transformations. Transfer rules can have conditions in the left-hand side, and in the case of multiple transfer rules, the first one whose conditions are satisfied is fired. This mechanism allows for a local rearrangement of the parse tree’s subtrees. In order to handle more complicated word order changes, however, a stronger means of rearrangement is provided also. A subtree can be memorized in a feature when a unification takes  places at parse time, and this feature’s value can be percolated up the parse tree and down the transfer tree just like any other feature. A phrase swallowed at any level in the source side can thus be expanded at a completely different location in the transfer tree. The power and simplicity of subtree memorization and random insertion can be demonstrated with the translation of English possessive structures into Hungarian: the friend of the man translates into a-DET férfi-N-NOM/man barátja-N-POS/friend, i.e., the order of NP’s is reversed. Through the interplay of only two rules (the place of memorization at N-bar level and insertion at NP-level), a possessive structure of any length is translated recursively in reverse order into Hungarian. Figure 2 illustrates two such rules and a more complex possessive structure translated in this manner. 3. The English-Hungarian grammar Our grammar, developed in the MetaMorpho formalism, currently has above 130 thousand rules, the majority of which are lexicalized items. The system uses no separate dictionary: what  *NN=NX+of+NP:0109261534-1 EN.NN[lnpf=YES, Left_np<-NP, NX.num] = NX + PREP(lex="of") + NP HU.NN = NX[NN.case, NN.postp, ownernum=SG, ownerpers=EN.NP.pers]  *NPX=the+NM:0205291509-11  EN.NPX[NM.num]  = DET(dettype=DEF) + NM  HU.NPX(EN.NM.lnpf=NO) = DET + NM[NPX.case, NPX.postp]  HU.NPX(EN.NM.lnpf=YES) = NP#1{EN.NM.Left_np}[case=DAT] + DET + NM[NPX.case, NPX.postp]  NP 345 NPX 344 DET lex="the" NM 341 NN 339 NX 4 N lex="friend" PREP lex="of" NP 311 NPX 310 DET lex="the" NM 307 NN 305 NX 99 N lex="wife" PREP lex="of" NP 297 NPX 293 PRONX 270 PRON lex="I, case=GEN NM 290 NN 288 NX 287 N lex="neighbour"  NP 591{345} NPX 592{344} NP 593{311} NPX 596{310} NP 597{297} NPX 600{293} DET dettype=DEF NM 602{290} NN 603{288} NX 604{287} N lex="szomszéd" DET dettype=DEF NM 599{307} NN 606{305} NX 607{99} N lex="feleség" DET dettype=DEF NM 595{341} NN 609{339} NX 610{4} N lex="barát"  Figure 2. The source and target trees for “the friend of the wife of my neighbour”, translated as “a szomszédomnak a feleségének a barátja,” and a simplified version of the two rules involved in the reverse order translation  139  would traditionally be entries in a lexicon are  the ones shown here. Therefore, for the sake of  integrated in the form of rules.  human readability and maintainability, lexical  As the grammar uses no feature structures,  items are coded in a simpler form where all non-  complex lexical information such as the subcats of  lexical information is omitted. The actual rules are  a verb are not described in terms of features but  then generated off-line from their simplified  by creating separate rules, in this case with a VP  source, and a large amount of linguistic  in their left-hand side, for each subcat frame. The 
Localisation (new versions of screens, help text and documentation), while not cheap, is relatively well understood, with many companies providing expertise and tools. The problem of multilingual user support is much more complex, with few off-the-shelf solutions available. LTC-Communicator, a software product from the Language Technology Centre Ltd, offers an innovative and cost-effective response to this growing need.  
Several major telecommunications companies have made significant investment in either controlled language and/or machine translation over the past 10 years.
In this paper we describe the FAME interlingual speech-to- speech translation System for Spanish, Catalan and English which is intended to assist users in the reservation of a hotel room when calling or visiting abroad. The System has been developed as an extension of the existing NESPOLE! translation system [4] which translates between English, German, Italian and French. After a brief introduction we describe the Spanish and Catalan System components including speech recognition, transcription to IF mapping, IF to text generation and speech synthesis. We also present a task-oriented evaluation method used to inform about system development and some preliminary results.
An adaptable statistical or hybrid MT system relies heavily on the quality of word-level alignments of real-world data. Statistical alignment approaches provide a reasonable initial estimate for word alignment. However, they cannot handle certain types of linguistic phenomena such as long-distance dependencies and structural differences between languages. We address this issue in Multi-Align, a new framework for incremental testing of different alignment algorithms and their combinations. Our design allows users to tune their systems to the properties of a particular genre/domain while still benefiting from general linguistic knowledge associated with a language pair. We demonstrate that a combination of statistical and linguistically-informed alignments can resolve translation divergences during the alignment process.
The Burrows-Wheeler Transform (BWT) was originally developed for data compression, but can also be applied to indexing text. In this paper, an adaptation of the BWT to word-based indexing of the training corpus for an example-based machine translation (EBMT) system is presented. The adapted BWT embeds the necessary information to retrieve matched training instances without requiring any additional space and can be instantiated in a compressed form which reduces disk space and memory requirements by about 40{\%} while still remaining searchable without decompression. Both the speed advantage from O(log N) lookups compared to the O(N) lookups in the inverted-file index which had previously been used and the structure of the index itself act as enablers for additional capabilities and run-time speed. Because the BWT groups all instances of any n-gram together, it can be used to quickly enumerate the most-frequent n-grams, for which translations can be precomputed and stored, resulting in an order-of-magnitude speedup at run time.
Because of its clarity and its simplified way of writing, controlled language (CL) is being paid increasing attention by NLP (natural language processing) researchers, such as in machine translation. The users of controlled languages are of two types, firstly the authors of documents written in the controlled language and secondly the end-user readers of the documents. As a subset of natural language, controlled language restricts vocabulary, grammar, and style for the purpose of reducing or eliminating both ambiguity and complexity. The use of controlled language can help decrease the complexity of natural language to a certain degree and thus improve the translation quality, especially for the partial or total automatic translation of non-general purpose texts, such as technical documents, manuals, instructions and medical reports. Our focus is on the machine translation of medical protocols applied in the field of zoonosis. In this article we will briefly introduce why controlled language is preferred in our research work, what kind of benefits it will bring to our work and how we could make use of this existing technique to facilitate our translation tool.
German has a richer system of inflectional morphology than English, which causes problems for current approaches to statistical word alignment. Using Giza++ as a reference implementation of the IBM Model 1, an HMMbased alignment and IBM Model 4, we measure the impact of normalizing inflectional morphology on German-English statistical word alignment. We demonstrate that normalizing inflectional morphology improves the perplexity of models and reduces alignment errors.
Spoken Translation, Inc. (STI) of Berkeley, CA has developed a commercial system for interactive speech-to-speech machine translation designed for both high accuracy and broad linguistic and topical coverage. Planned use is in situations requiring both of these features, for example in helping Spanish-speaking patients to communicate with English-speaking doctors, nurses, and other health-care staff.
Existing automated MT evaluation methods often require expert human translations. These are produced for every language pair evaluated and, due to this expense, subsequent evaluations tend to rely on the same texts, which do not necessarily reflect real MT use. In contrast, we are designing an automated MT evaluation system, intended for use by post-editors, purchasers and developers, that requires nothing but the raw MT output. Furthermore, our research is based on texts that reflect corporate use of MT. This paper describes our first step in system design: a hierarchical classification scheme of fluency errors in English MT output, to enable us to identify error types and frequencies, and guide the selection of errors for automated detection. We present results from the statistical analysis of 20,000 words of MT output, manually annotated using our classification scheme, and describe correlations between error frequencies and human scores for fluency and adequacy.
This paper presents an empirical evaluation of the main usability factors that play a significant role in the interaction with on-line Machine Translation (MT) services. The investigation is carried out from the point of view of typical users with an emphasis on their real needs, and focuses on a set of key usability criteria that have an impact on the successful deployment of Internet-based MT technology. A small-scale evaluation of the performance of five popular web-based MT systems against the selected usability criteria shows that different approaches to interaction design can dramatically affect the level of user satisfaction. There are strong indications that the results of this study can be fed back into the development of on-line MT services to enhance their design, thus ensuring that they meet the requirements and expectations of a wide range of Internet users.
This paper describes some difficulties associated with the translation of numbers (scalars) used for counting, measuring, or selecting items or properties. A set of problematic issues is described, and the presence of these difficulties is quantified by examining a set of texts and translations. An approach to a solution is suggested.
Feedback from field deployments of machine translation (MT) is instructive but hard to obtain, especially in the case of soldiers deployed in mobile and stressful environments. We first consider the process of acquiring feedback: the difficulty of getting and interpreting it, the kinds of information that have been used in place of or as predictors of direct feedback, and the validity and completeness of that information. We then look at how to better forecast the utility of MT in deployments so that feedback from the field is focused on aspects that can be fixed or enhanced rather than on overall failure or viability of the technology. We draw examples from document and speech translation.
The public demonstration of a Russian-English machine translation system in New York in January 1954 {--} a collaboration of IBM and Georgetown University {--} caused a great deal of public interest and much controversy. Although a small-scale experiment of just 250 words and six {`}grammar{'} rules it raised expectations of automatic systems capable of high quality translation in the near future. This paper describes the system, its background, its impact and its implications.
We describe Pharaoh, a freely available decoder for phrase-based statistical machine translation models. The decoder is the implement at ion of an efficient dynamic programming search algorithm with lattice generation and XML markup for external components.
Lingvistica is developing a family of MT systems for Dutch to and from English, German, and French. PARS/H, a Dutch to and from English system, is a fully commercial product, while PARS/HD, for Dutch to and from German MT, and PARS/HF, for Dutch to and from French, are under way. The PARS/Dutch family of MT systems is based on the rule-based Lingvistica{'}s Dutch morphological-syntactic analyzer and synthesizer dealing with vowel and consonant alterations in Dutch words, as well as Dutch syntactic analysis and synthesis. Besides, a German analyzer and synthesizer have been developed, and a similar French one is being constructed. Representative Dutch and German grammatical dictionaries have been created, comprising Dutch and German words and their complete morphological descriptions: class and subclass characteristics, alteration features, and morphological declension/conjugation paradigms. The PARS/H dictionary editor provides simple dictionary updating. Numerous specialist dictionaries are being and have been created. The user interface integrates PARS/H with MS Word and MS Internet Explorer, fully preserving the corresponding formats. Integrating with MS Excel and many other applications is under way.
A year ago we were faced with a challenge: rapidly develop a machine translation (MT) system for written Pashto with limited resources. We had three full-time native speakers (one with a Ph.D. in general linguistics, and translation experience) and one part-time descriptive linguist with a typological-functional background. In addition, we had a legacy MT software system, which neither the speakers nor the linguist was familiar with, although we had the opportunity to occasionally confer with experienced system users. There were also dated published grammars of varying (usually inadequate) quality available.
Recent research has shown that a balanced harmonic mean (F1 measure) of unigram precision and recall outperforms the widely used BLEU and NIST metrics for Machine Translation evaluation in terms of correlation with human judgments of translation quality. We show that significantly better correlations can be achieved by placing more weight on recall than on precision. While this may seem unexpected, since BLEU and NIST focus on n-gram precision and disregard recall, our experiments show that correlation with human judgments is highest when almost all of the weight is assigned to recall. We also show that stemming is significantly beneficial not just to simpler unigram precision and recall based metrics, but also to BLEU and NIST.
Named entities make up a bulk of documents. Extracting named entities is crucial to various applications of natural language processing. Although efforts to identify named entities within monolingual documents are numerous, extracting bilingual named entities has not been investigated extensively owing to the complexity of the task. In this paper, we describe a statistical phrase translation model and a statistical transliteration model. Under the proposed models, a new method is proposed to align bilingual named entities in parallel corpora. Experimental results indicate that a satisfactory precision rate can be achieved. To enhance the performance, we also describe how to improve the proposed method by incorporating approximate matching and person name recognition. Experimental results show that performance is significantly improved with the enhancement.
We describe the use of a translation memory in the context of a reconstruction of a landmark application of machine translation, the Canadian English to French weather report translation system. This system, which has been in operation for more than 20 years, was developed using a classical symbolic approach. We describe our experiment in developing an alternative approach based on the analysis of hundreds of thousands of weather reports. We show that it is possible to obtain excellent translations using translation memory techniques and we analyze the kinds of translation errors that are induced by this approach.
The Keyword Translator is a part of the Question Analyzer module in the JAVELIN Question-Answering system; it translates the keywords, which are used to query documents and extract answers, from one language to another. Much work has been in the area of query translation for CLIR or MLIR, however, many have focused on methods using hard-to-obtain and domain-specific resources, and evaluation is often based on retrieval performance rather than translation correctness. In this paper we will describe methods combining easily accessible, general-purpose MT systems to improve keyword translation correctness. We also describe methods that utilize the question sentence available to a question-answering system to improve translation correctness. We will show that using multiple MT systems and the question sentence to translate keywords from English to Mandarin Chinese can significantly improve keyword translation correctness.
Named-entities in free text represent a challenge to text analysis in Machine Translation and Cross Language Information Retrieval. These phrases are often transliterated into another language with a different sound inventory and writing system. Named-entities found in free text are often not listed in bilingual dictionaries. Although it is possible to identify and translate named-entities on the fly without a list of proper names and transliterations, an extensive list of existing transliterations certainly will ensure high precision rate. We use a seed list of proper names and transliterations to train a Machine Transliteration Model. With the model it is possible to extract proper names and their transliterations in monolingual or parallel corpora with high precision and recall rates.
This paper compares a manually written MT grammar and a grammar learned automatically from an English-Spanish elicitation corpus with the ultimate purpose of automatically refining the translation rules. The experiment described here shows that the kind of automatic refinement operations required to correct a translation not only varies depending on the type of error, but also on the type of grammar. This paper describes the two types of grammars and gives a detailed error analysis of their output, indicating what kinds of refinements are required in each case.
TransType2 is a novel kind of interactive MT in which the system and the user collaborate in drafting a target text, the system{'}s contribution taking the form of predictions that extend what the translator has already typed in. TT2 is also an international research project in which end-users are represented by two translation firms. We describe the contribution of these translators to the project, from their input to the system{'}s functional specifications to their participation in quarterly user trials. We also present the results of the latest round of user trials.
This paper describes an evaluation experiment about a Japanese-Uighur machine translation system which consists of verbal suffix processing, case suffix processing, phonetic change processing, and a Japanese-Uighur dictionary including about 20,000 words. Japanese and Uighur have many syntactical and language structural similarities, including word order, existence and same functions of case suffixes and verbal suffixes, morphological structure, etc. For these reasons, we can consider that we can translate Japanese into Uighur in such a manner as word-by-word aligning after morphological analysis of the input sentences without complicated syntactical analysis. From the point of view of practical usage, we have chosen three articles about environmental issue appeared in Nippon Keizai Shinbun, and conducted a translation experiment on the articles with our MT system, for clarifying our argument. Here, we have counted the correctness of phrases in the Output sentences to be evaluating criteria. As a results of the experiment, 84.8{\%} of precision has been achieved.
We describe an approach to creating a small but diverse corpus in English that can be used to elicit information about any target language. The focus of the corpus is on structural information. The resulting bilingual corpus can then be used for natural language processing tasks such as inferring transfer mappings for Machine Translation. The corpus is sufficiently small that a bilingual user can translate and word-align it within a matter of hours. We describe how the corpus is created and how its structural diversity is ensured. We then argue that it is not necessary to introduce a large amount of redundancy into the corpus. This is shown by creating an increasingly redundant corpus and observing that the information gained converges as redundancy increases.
This paper describes an intelligibility snap-judgment test. In this exercise, participants are shown a series of human translations and machine translations and are asked to determine whether the author was human or machine. The experiment shows that snap judgments on intelligibility are made successfully and that system rankings on snap judgments are consistent with more detailed intelligibility measures. In addition to demonstrating a quick intelligibility judgment, representing on a few minutes time of each participant, it details the types of errors which led to the snap judgments.
MT systems that use only superficial representations, including the current generation of statistical MT systems, have been successful and useful. However, they will experience a plateau in quality, much like other {``}silver bullet{''} approaches to MT. We pursue work on the development of interlingual representations for use in symbolic or hybrid MT systems. In this paper, we describe the creation of an interlingua and the development of a corpus of semantically annotated text, to be validated in six languages and evaluated in several ways. We have established a distributed, well-functioning research methodology, designed a preliminary interlingua notation, created annotation manuals and tools, developed a test collection in six languages with associated English translations, annotated some 150 translations, and designed and applied various annotation metrics. We describe the data sets being annotated and the interlingual (IL) representation language which uses two ontologies and a systematic theta-role list. We present the annotation tools built and outline the annotation process. Following this, we describe our evaluation methodology and conclude with a summary of issues that have arisen.
At AMTA 2002, we reported on a pilot project to machine translate Microsoft{'}s Product Support Knowledge Base into Spanish. The successful pilot has since resulted in the permanent deployment of both Spanish and Japanese versions of the knowledge base, as well as ongoing pilot projects for French and German. The translated articles in each case have been produced by MSR-MT, Microsoft Research{'}s data-driven MT system, which has been trained on well over a million bilingual sentence pairs for each target language from previously translated materials contained in translation memories and glossaries. This paper describes our experience in deploying this system and the (positive) customer response to the availability of machine translated articles, as well as other uses of MSR-MT either planned or underway at Microsoft.
At AMTA-2002 we presented a deployed application of Machine Translation (MT) at Ford Motor Company in the domain of vehicle assembly process planning. This application uses an MT system developed by SYSTRAN to translate Ford{'}s manufacturing process build instructions from English to Spanish, German, Dutch and Portuguese. Our MT system has already translated over 2 million instructions into these target languages and is an integral part of our manufacturing process planning to support Ford{'}s assembly plants in Europe, Mexico and South America. A major component of the MT system development was the creation of a set of technical glossaries for the correct translation of automotive and Ford-specific terminology. Due to the dynamic nature of the automobile industry we need to keep these technical glossaries current as our terminology frequently changes due to the introduction of new manufacturing technologies, vehicles and vehicle features. In addition, our end-users need to be able to test and modify translations and see these results deployed in a timely manner. In this paper we will discuss the tools and business process that we have developed in conjunction with SYSTRAN in order to maintain and customize our MT system and improve its performance in the face of an ever-changing business environment.
In conventional word alignment methods, some employ statistical models or statistical measures, which need large-scale bilingual sentence-aligned training corpora. Others employ dictionaries to guide alignment selection. However, these methods achieve unsatisfactory alignment results when performing word alignment on a small-scale domain-specific bilingual corpus without terminological lexicons. This paper proposes an approach to improve word alignment in a specific domain, in which only a small-scale domain-specific corpus is available, by adapting the word alignment information in the general domain to the specific domain. This approach first trains two statistical word alignment models with the large-scale corpus in the general domain and the small-scale corpus in the specific domain respectively, and then improves the domain-specific word alignment with these two models. Experimental results show a significant improvement in terms of both alignment precision and recall, achieving a relative error rate reduction of 21.96{\%} as compared with state-of-the-art technologies.
In this paper, a Japanese-Chinese Machine Translation (MT) system using the so-called Super-Function (SF) approach is presented. A SF is a functional relation mapping sentences from one language to another. The core of the system uses the SF approach to translate without going through syntactic and semantic analysis as many MT systems usually do. Our work focuses on business users for whom MT often is a great help if they need an immediate idea of the content of texts like e-mail messages, reports, web pages, or business letters. In this paper, we aim at performing MT between Japanese and Chinese to translate business letters by the SF based technique.
