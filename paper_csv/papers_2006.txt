The study of Japanese topic marker “WA” has attracted much attention, especially in comparative  studies with the nominative case marker “GA.” For instance, it has been assumed that WA-marked  constituents represent old information while GA-marked ones new information, or WA marks the  subjects of kind/individual-level predicates while GA the subjects of stage-level predicates. Consider  the examples in (1):  (1) a. Uma-wa kusa-o  taber-u.  horse-TOP grass-ACC feed.on-PRES  ‘Horses feed on grass.’  (kind/individual-level sentence)  b. Uma-ga kusa-o  tabe-teir-u.  horse-NOM grass-ACC eat-PROG-PRES  ‘A horse is eating grass.’  (stage-level sentence)  (2) a. Uma-ga  kusa-o  taber-u.  horse-NOM grass-ACC feed.on-PRES  ‘Horses feed on grass.’  (kind/individual-level sentence)  b. Uma-wa  kusa-o  tabe-teir-u.  horse-TOP grass-ACC eat-PROG-PRES  ‘The horse is eating grass.’  (stage-level sentence)  When the subject of the kind/individual-level predicate taberu 'feed on' is marked with the nominative  case GA in a matrix clause, as in (2a), an exhaustive listing reading is forced in some marked context,  and hence construed as (a part of) FOCUS. When the referent of the subject of a stage-level predicate  is already given in discourse, it can (and should) be marked with the topic marker (see Kuno (1973),  Shirai (1985), Uechi (1996), Yabushita (2005) for discussion). These observations often lead to the  Diesing style analysis of individual/stage-level subjects in Japanese in which the subjects of  generic/individual-level predicates should be marked with the topic marker WA, whereas those of  stage-level predicates with the nominative marker GA as default. In this kind of analysis, the  difference in marking of subjects is reduced to structural differences in syntax. The subjects of generic  sentences are projected onto [Spec, IP] position, while those of episodic sentences are projected onto  198  [Spec, VP] position.  But their account immediately runs into a problem because we easily find many counterexamples to  this simple generalization about WA/GA-distinction in terms of genericity of sentences. Observe some  of the examples shown in (3).  (3)a. Kodomo-demo [uma-ga/?uma-wa  usa-o taberu-koto]-o sit-teir-u.  Children-even [horse-NOM/horse-TOP grass-ACC eat-COMP]-ACC know-PRES  'Even children know that horses feed on grass.'  b. [Uma-ga/*Uma-wa taber-u] kusa-o osie-te-kudasai.  horse-NOM/ -TOP eat-PRES] grass-ACC teach-PRES please  ‘Please tell me the kind of grass which horses eat.’  c. Nani-ga/*Nani-wa kusa-o  taber-u-no?  what-NOM/ -TOP grass-ACC eat-PRES-Q  ‘What eats the grass?’  Topicalization does not apply to elements in the complement and relative clauses in general and  nominative-marking of the subjects is strongly preferred or almost obligatory, as in (3a) and (3b).  Even if the external argument of an individual-level predicate shows up in the matrix sentence, it must  be marked with GA when it is a question word, as illustrated in (3c) (because the question words should  be interpreted as 'focus' though the presuppositional question words like dono doobutu ‘which animal’  can be marked with WA with a contrastive reading). The Diesing style account cannot offer any  explanation about the facts observed in (3) in which the subjects of individual-level predicates must be  marked with nominative without exhaustive listing connotation).  There are more important properties with the topic marker WA. WA can follow PPs or clauses as  well as N(P)s, as illustrated in (4) if the referents of these phrases have been evoked in the previous  discourse or situations.  (4) a. Oosaka-ni-wa gaikoku-jin-ga takusan kurasi-teir-u.  Oosaka-IN-TOP foreigners-NOM many  living-PROG-PRES.  ‘In Oosaka, many foreigners are living.’  b. *Oosaka-ni-ga  gaikoku-jin-ga takusan kurasi-teir-u.  Oosaka-IN-NOM foreigners-NOM many living-PROG-PRES.  (5) [Taroo-ga ringo-o mui-ta –no]-WA  sono naihu-(de)-da.1  Taroo-NOM apple-ACC peel-PAST-COMP]-TOP that knife-(WITH)-BE-PRES  ‘It is with this knife that Taroo peeled apples.’  In this paper, we will discuss topicalization involving a wide range of grammatical constituents in  terms of strong compositionality in the theory of grammar. I will demonstrate that the main function of  topic marker WA is to divide a sentence into the two parts, each standing for a restrictor (i.e., topic) and  a nuclear scope (i.e.,comment) in the sense of Hajicova, Partee and Sgall (1998), and will give a unified  account for nonstandard constituency these constructions show and a direct correspondences between  syntactic segmentation and information structure.  1. TOPIC-COMMENT Articulation  Before showing the concrete derivations of topic-comment structures, let us assume that all-focus sentences (topicless neutral description in Kuno 1972) are default and that the topic marker WA functions as a ‘defocusing’ operator in the sense of Hendriks (2002). The sentences in (3) shows that there are some contexts in which the subjects of kind/individual-level predicates, which are usually marked with WA, must be marked with GA without exhaustiveness reading. In the Japanese traditional grammar, WA has been assumed to be a KAKARI-JOSHI, which means that expressions marked with it  
This paper discusses the implementation of a knowledge-rich approach to automatic acquisition of grammatical information. Our study is based on Word Sketch Engine (Kilgarriff and Tudgell 2002). The original claims of WSE are two folded: that linguistic generalizations can be automatically extracted from a corpus with simple collocation information provided that the corpus is large enough; and that such a methodology is easily adaptable for a new language. Our work on Chinese Sketch Engine attests to the claim the WSE is adaptable for a new language. More critically, we show that the quality of grammatical information provided has a directly bearing on the result of grammatical information acquisition. We show that when provided with a knowledge rich lexical grammar, both the quantity and quality of the extracted knowledge improves substantially over the results with simple PS rules. 
280  Dialect Area Hebei Henan Heilongjiang Hubei Hunan Jilin Jiangsu Jiangxi Liaonin Neimenggu Ningxia Shandong Shanxi( ) Shannxi() Shanghai Sichuan Tianjin Xizang(Tibet) Xinjiang Yunnan Zhejiang  Total  Female 7 14 24 4 6 10 13 3 21 2 2 20 5 9 6 9 4 1 4 1 7 212  Male 5 8 14 8 4 7 5 0 11 3 0 8 0 5 2 3 9 0 2 2 2 126  The 10 monophthongs in the test are listed in Table 2. These 10 monophthongs of Mandarin are selected according to reference [4], and it is not completely same with the monophthong catalogue used by other researchers. Usually, ê[ε] is only used as the mood word “ ( )”,and its voice is unstable. In phonetics, there is dispute of whether the vowel should be put into the Mandarin speech frame. In this paper, considering the special application background, ê[ε] is dealt with as an independent monophthong. The software used to measure the formants and is SFSwin. The LPC technique is used in SFSwin with a formant trajectory estimation dynamic program to get the optimum formants estimation[5].  Table 2. Monophthongs in the test  No.  tag  International phonetic symbol  Chinese character with the monophthong  
In view of this problem, Elbers (1989) argues that if comprehension does not qualify as a likely source of input for analysis, production does. His output-as-input hypothesis is based on a three-phase, inductive acquisition model, which takes the child’s own productions as primary input for analysis. In Phase A, the child takes in incompletely analyzed fragments from adult input, and starts using these in her or his own production. In Phase B, own productions are analyzed and hypotheses concerning relationships between forms and meanings are derived. In Phase C, these hypotheses are tested against novel adult input. It will be proved by the result of this paper that adult’s input does influence greatly on children’s language acquisition. Specifically, the output-as-input hypothesis explains best children’s TS rules acquisition in Mandarin. 2.2 Expectations Based on the literature reviews, expectation of children’s acquisition of tone and tone sandhi is described as follow. (Li & Thompson’s four stages are adopted) Stage I (1,0): before segments acquisition, T1 and T4 are acquired. Specifically, T1 appears earlier than T4 for the reason that level tone is acquired earlier than contour tone. Stage II/one-word stage (1,6): T2 and T3 appear in the sequence of T2>T3. However, substitution between these two tones occurs. Stage III (2/3 word stage) (2,4): Confusion between T2 and T3 remains. All four tone sandhi appear with the order: TS3>TS2/TS4>TS1. Stage IV (sentence stage) (3,0): Both TS2 and TS1 appear stably and confusion between T2 and T3 no longer exists. Though no previous studies mention the acquisition of TS3 and TS4, it will be discussed in the paper. The expectation for TS3 is that TS3 should appear at stage III earlier than the appearance of TS2 because TS3 only involves early-acquired T4. And the expectation for TS4 is that TS4 should appear when children start combining these two lexemes with other lexemes, on early period of stage III. Therefore, the expectation of tone acquisition is T1>T4>T2>T3 as Xu suggested. And the expectation of TS acquisition is TS3 >TS2/TS4 > TS1, and all at stage III. As for the role of adult’s input, a close examination will be made during the observation. If Baker & Nelson’s immediate recasts are correct, it is expected that subjects would more or less modify their production after given immediate recasts. If Elber’s output-as-input hypothesis is correct, it is expected that subjects would improve only those they choose to produce after given immediate recasts. And if Innatism theory is correct, it is expected that adult’s input does not help acquisition. 3. Methodology Three children aged respectively (1;5), (2;2) and (3;0) were observed. They have Mandarin as their first language, and have very little influence of any second language. The period of data collection is two months, from April to May, 2006. Data collection on tone acquisition was done by 6-hour-videotaping when subjects were playing with their parents. Error-counting on recorded tokens was done to decide at which acquisition level the 289  subject is. As for the procedure of TS acquisition, we engaged the subjects in looking at pictures. The adult pointed to the picture and asked the child what it was, and tried to elicit a naming response from the child. All the collected tokens in picture-naming session were analyzed targeting to answer research question (1) ‘is there an ordering in the acquisition of four TS rules under the influence of tone acquisition?’ After each utterance, the adult would provide a recast as correct model and asked the subject to repeat. This repetition is to evaluate the influence of adult’s input targeting to answer the second research question, ‘how does adult input help acquisition of TS rules?’  4. Result/discussion  Table 1 presents the correct usage of tones and TS rules of three subjects.  Table 1 Number and Accuracy rate of three subjects  Age (1,5)  Age (2,2)  Correct/Production tokens Correct/Production tokens  T1  30/37 (81.08%)  185/191 (96.86%)  T2  43/74 (58.11%)  225/249 (90.36%)  T3  30/73 (41.09%)  181/363 (49.86%)  T4  27/44 (61.36%)  432/442 (97.74%)  TS1  0  2/6 (33.33%)  TS2  0  4/6 (66.67%)  TS3  0  6/6 (100%)  TS4  0  5/6 (83.33%)  Age (3,0) Correct/Production tokens 356/356 (100%) 388/388 (100%) 167/167 (100%) 464/464 (100%) 6/6 (100%) 6/6 (100%) 6/6 (100%) 6/6 (100%)  The (1,5) subject, who is at Stage II, is expected to produce stably T1 and T4, and T2 and T3 appear in the sequence of T2>T3. Substitution between these two tones occurs. However, data collection from this subject is difficult. All tokens in the videotaping and testing sessions are elicited, for this child rarely spontaneously utters any speech. Therefore, table 1 shows the accuracy rate of the (1,5) subject’s repeated production. Accuracy percentage of tone production of (1,5) subject is shown as T1>T4>T2>T3. T3 is often pronounced wrongly. And in these wrong pronunciation, the subject always substitutes T2 for T3, such as hand ‘手’ as [şou2] and yes ‘好’ as [hau2]. It is only shown once in our observation that the subject substitutes T3 for T2 in 魚 as [yu3]. This asymmetry tells that T3 is more difficult than T2, and disagrees with previous papers saying that T2 and T3 substitute each other, for a clearly greater difficulty of T3 for the (1;5) subject. It is interesting to find out that the accuracy rate of T4 is only slightly higher than the accuracy rate of T2. Based on the expectation, this (1;5) subject is at stage II where T1 and T4 should appear quite stably while T2 and T3 much less stably. However, the result suggests that T4 may be acquired roughly at the time when T2 is acquired, for both tones are slanting tone. However, further studies are acquired to explore into this preliminary observation. In addition, the observation of this (1;5) subject also clearly tells us that the tonal system is acquired earlier than the acquisition of phonemes. When the subject repeats, most the tone is produced even though the phonetic configuration is not correct. For example, 鞋 ‘shoes’ as [ie2], and 樹 ‘trees’ as [u4]. It is shown that the tonal system is acquired earlier than the phoneme acquisition. The observation of (2;2) child clearly provides an answer to the question of the order of tone sandhi acquisition under the influence of tone acquisition. As can be seen, in the 6-hour-videotaping, all four tones appear. T1 and T4 are stable, while T3 is relatively unstable. Based on the percentage of accuracy, we may order a difficulty scale which supports the previous studies on tone acquisition, stating that T1 and T4 are easier to acquire than T2 and T3; particularly, T3 is the most difficult one, shown by a low percentage of accuracy. The expectation for (3,0) subject, who is at stage IV, is that confusion between T2 and T3 no longer exists. Observation of 3-year-old subject accords with the expectation. The subject produces four tones  290  without any confusion both in testing and videotaping sessions. We then conclude that the (3;0) subject has completed acquiring tones in Mandarin. Let’s move to the TS acquisition. For the (1,5) subject, who is at one word stage, his mother tries to elicit the repetition of some frequently used terms, such as 謝謝 ‘thank-you’, 再見 ‘goodbye’, 好吃 ‘good taste’, and 手錶 ‘watch’. It is found that only TS3 is applied in 謝謝 ‘thank you’ and 再見 ‘goodbye’. Though the data is limited, the appearance of TS3 accords with the expectation that TS3 is the first TS rule to be acquired. At the age of (2;2), it is observed that all four TS rules appear with different proficiency. TS3 (T4 Æ high falling tone/ _ T4) appears most stably. The next stably-applied rule is TS4, then TS2, and the least stably TS1. The accuracy percentage appearing in testing sessions is supported by the accuracy percentage of subject’s applying tone sandhi rules in the videotaping session. Table 2 shows the percentage of accuracy in the videotaping session of (2,2) subject.  Table 2 Percentage of accuracy in applying TS rules in videotaping session of (2,2) subject  TS1  TS2  TS3  TS4  Total appearance  47  86  101  35  Correct appliance  16  65  96  35  Percentage of accuracy 34.04% 75.58% 95.05%  100%  Table 2 supports Table 1 in the fact that it is clearly seen that TS3, which has high accuracy rate in testing session, also has high accuracy rate in the videotaping session. For example, in the production of bisyllabic or trisyllabic T4 phrases, such as 樹上 ‘in the tree’, 去睡覺 ‘go to bed’, the subject applies TS3 without mistakes. As for TS1, the accuracy percentages both in the testing session and the videotaping session are the lowest. Errors, such as 給我 ‘give me’ as [ge1 wo3] and 給你 ‘give you’ as [ge1 ni1] are observed despite the subject can correctly pronounce the isolated morpheme. This shows that the subject is already aware that a special attention should be given to the adjacent dipping tones; however, for the reason that appliance of the Third Tone Sandhi Rule has not been fully acquired, the subject fails to produce the correct tones. Even so, the subject shows her own ways to avoid the adjacency of two third tones. The expectation for (3,0) subject at Stage IV is that all TS rules are applied correctly. Indeed, the subject applies all four TS rules correctly when needed. The stably acquired tonal system is supported by the videotaping session, where the accuracy rate for four TS rules are nearly 100%. We then conclude that the (3;0) subject has completed acquiring tonal system in Mandarin. In conclusion, from the observation of three subjects, the results generally accord with our expectations. Specifically, the tone acquisition order and TS rules acquisition order are the expected T1>T4>T2>T3 and TS3/TS4>TS2>TS1, which shows that the order of TS acquisition is influenced by tone acquisition. It is also shown that not until the age of three does the subject complete acquiring tonal system in Mandarin. Though within the short period of two months of observation, we cannot tell the specific duration the subject acquires the tone or tone sandhi, the difficulty sequence in acquisition observed from (1,5) and (2,2) subjects provides the clue. T1 and T4 are easier than T2, which is easier than T3. As for TS acquisition, TS3 and TS4 are easier than TS2, which is easier than TS1. After answering research questions (a), in the next session, adults’ input will be discussed. 4.4 Influence of adult’s Input Since the three-year-old subject has acquired the tonal system completely, no repetition task is necessary, we shall only discuss the influence of adult’s inputs on (1,5) and (2,2) subjects. For the reason that all data from (1,5) child is from elicitation, it is clear that parents’ input influences the production. For example, the first repeated production of 好吃 ‘good taste’ is [hau1 tşh1], and the second repeated production is the correct tones [hau3 tşh1]. However, the imitation is not an easy task. In the production of 手錶 ‘watch’, the first repetition is [şou1][biau2], and the second repetition is [şou3][biau2], and the third repetition is [şou1][biau3]. The subject seemed to be able to focus on only one sound at one time.  291  As for the (2,2) subject, parents input also greatly influences her tonal production. When she said 給 我 ‘give me’ as [gai1 wo3] for the first time, but [gai2 wo3] after her mother’s input, and 給你 ‘give you’ as [gai1 ni1] for the first time, but [gai2 ni3] after adults’ input. Not only the sandhi tones are modified, but also tones are modified. For example, 椅 ‘chair’ is pronounced with low level tone for the first time, but correctly pronounced after repetition. Though adult’s input seems to have immediate effect, we want to know if, or specifically how, adult’s input really help the acquisition. Therefore a follow-up observation of (2;2) subject three weeks after the testing session is made to see if the subjects can apply correct TS rules after given recasts in testing session. Interestingly, there is a close correlation of successful acquisition examined three weeks after the testing session and simultaneous repetition right after the recasts in testing session. It is observed that immediate input has effects only when the child’s spontaneous-repetition after the input occurs. To analyze the relationship between acquisition and simultaneous repetition, subjects’ input utterances are classified as spontaneous or non-spontaneous and in terms of whether they were responded to or not. Table 3 shows the result.  Table 3 Children’s responses to input  TS1  TS2  TS3  TS4  Spontaneous reaction  6  5  0  0  Non-spontaneous reaction  0  
 U-00000000 - U-0000007F: 7  0xxxxxxx  U-00000080 - U-00000FFF: 12  11xxxxxx 10xxxxxx  U-00001000 - U-0003FFFF: 18  11xxxxxx 11xxxxxx 10xxxxxx  U-00040000 - U-00FFFFFF: 24  11xxxxxx 11xxxxxx 11xxxxxx 10xxxxxx  U-01000000 - U-3FFFFFFF: 30  11xxxxxx 11xxxxxx 11xxxxxx 11xxxxxx 10xxxxxx  U-40000000 - U-7FFFFFFF: 36  11xxxxxx 11xxxxxx 11xxxxxx 11xxxxxx 11xxxxxx 10xxxxxx  Take following code sequence as an example. 0xxxxxxx11xxxxxx 10xxxxxx 0xxxxxxx 11xxxxxx 11xxxxxx 10xxxxxx  354  Scanning the sequence from left to right, the first byte with mark bit 0 in the first bit is an ASCII code, the second byte is with mark “11”, so this is the first byte of a multi-byte code, the third byte is with mark “10”, so this is the end of the multi-byte code. The fourth byte is an ASCII code. The last 3 bytes is a 3 byte code. A character code in a 11-10 form of UTF-8 code sequence starts with a byte marked with “11”, ended with a byte marked with “10”, or starts with a byte marked with “0” and ended this byte. It is easy to make the transformation programs by the data structure as shown. Based on Multilevel Mark Theory (1, 2), this form can be simplified further; mark 11 can be simplified as mark 1, and mark 10 can be simplified as mark 0. 3. 1-0 transformation between UCS and 1-0 form of UTF-8  3.1 1-0 transformation form of UTF-8  11-10 form of UTF-8 can be simplified as 1-0 form of UTF-8, as shown in Table 2. UTF-16 or UTF32 is not compatible with existing UNIX implementations. The main problem is some control character of ASCII, such as null bytes and/or the ASCII slash ("/"). To make these character encodings in compatibility with historical file systems; the control character in the last byte of multi-byte encoding sequence should be delete. If the 7 bit value of last byte is < 20, i.e. the third bit is 0 (counted from left); then, the third bit is set to 1. The original information of third bit is put in the rightest bit in the byte just left to the rightest byte, as shown in table 2. In 1-0 form of UTF-8, at most 5 bytes is needed to transform UTF-32, and the implication mechanism is much simpler. It is easy to distinguish the character codes in the 1-0 form of UTF-8 code sequence. Take following code sequence as an example. 0xxxxxxx1xxxxxxx 0xxxxxxx 0xxxxxxx 1xxxxxxx 1xxxxxxx 0xxxxxxx Scanning the sequence from left to right, the first byte with mark bit 0 in the first bit, it is an ASCII code, the second byte is with mark 1, so this is a multi-byte code, the third byte is with mark 0, so this is the end of the multi-byte code. The fourth byte is an ASCII code. The last 3 bytes is a 3 byte code.  Table 2. Illustrating The 1-0 form of UTF-8  UCS  Free 7 bit bits value  U-00000000 - U-0000007F: 7  U-00000080 - U-00001FFF: 13  <20 >=20  U-00002000 - U-000FFFFF: 20  <20 >=20  U-00100000 - U-07FFFFFF: 27  <20 >=20  U-08000000 - U-FFFFFFFF: 34  <20 >=20  1-0 UTF-8 sequence 0xxxxxxx 1xxxxxx0 001xxxxx 1xxxxxx1 0xxxxxxx 1xxxxxxx 1xxxxxx0 001xxxxx 1xxxxxxx 1xxxxxx1 0xxxxxxx 1xxxxxxx 1xxxxxxx 1xxxxxx0 001xxxxx 1xxxxxxx 1xxxxxxx 1xxxxxx1 0xxxxxxx 1xxxxxxx 1xxxxxxx 1xxxxxxx 1xxxxxx0 001xxxxx 1xxxxxxx 1xxxxxxx 1xxxxxxx 1xxxxxx1 0xxxxxxx  355  A character code in a 1-0 form of UTF-8 code sequence starts with a byte marked with “1”, followed with byte marked with “1”, and ended with a byte marked with “0”, or starts with a byte marked with “0” and ended this byte.  3.2 Transformation between UCS and 1-0 form of UTF-8 The transformation between UCS and 11-10 form of UTF-8 is easy and similar to the transformation between UCS and 1-0 form of UTF-8, so the former is omitted.  Table 3. Concatenation preparation for 1-0 form of UTF-8  UCS U-00000000 - U-0000007F: U-00000080 - U-00001FFF: U-00002000 - U-000FFFFF: U-00100000 - U-07FFFFFF: U-08000000 - U-FFFFFFFF:  1-0 UTF-8 sequence 0xxxxxxx 1yyyyyyy0 0xxxxxxx 1zzzzzzz 1yyyyyyy0 0xxxxxxx 1uuuuuuu 1zzzzzzz 1yyyyyyy0 0xxxxxxx 1qqvvvvv 1xxxxxxx 1zzzzzzz 1yyyyyyy0 0xxxxxxx  Table 4. Demonstration Concatenation for 1-0 form of UTF-8  UCS U-00000000 - U-0000007F U-00000080 - U-00001FFF U-00002000 - U-000FFFFF U-00100000 - U-07FFFFFF U-08000000 - U-FFFFFFFF  Concatenation of UTF-8 sequence 0xxxxxxx 000yyyyy yx y0xxxxx 00000000 000zzzzz zzzyyyyy yx y0xxxxx 00000uuu uuuuzzzz zzzyyyyy yx y0xxxxx vvvvvuuu uuuuzzzz zzzyyyyy yx y0xxxxx  The UCS value is easy to be concatenated as shown in table 3 and table 4. The bits marked by q in table 3 are not used in the transformation. y0 in the UTF-8 sequence above refers to the rightest bit in the byte left to the rightest byte of a character code, y0=0 as the 7 bit value of the rightest byte <20, otherwise, y0=1. It is easy to implement the transformation programs by table 3 and table 4.  4. Transformation between Local Code and 1-0 Form of UTF-8 The transformation between local codes and UTF-8 are frequently used, this used to take two steps, the local code (or UTF-8) is first transformed into UCS, and then transformed to UTF-8 (or local code). For 1-0 Form of UTF-8, the transformation between local codes and UTF-8 can be done by one step. This makes computer time and space greatly saved. 4.1 Transformation for 1 byte local codes Because the first bit of each byte of 1-0 Form of UTF-8 is taken as mark bit; to avoid losing information in the transformation between local codes and 1-0 Form of UTF-8, codes of a local code system should be divided into one or more code classes. For 1 byte character code, if it is 7 bit coded characters, i.e. the first bit of the character code is always 0 or 1; then, the code system can be represented by one class  356  codes, such as: ASCII, ISO-8859-1, ISO-8859-2, and etc. A byte sequence of 1-0 Form of UTF-8 for 1 byte local codes consists of 2 bytes, as shown in Table 5.  Table 5. Illustration the transformation between 1 byte local codes and 1-0 Form of UTF-8  Related local codes  ASCII ISO-8859-1 ISO-8859-2 / ISO-8859-16  0xxxxxxx 1xxxxxxx 1xxxxxxx / 1xxxxxxx  1-0 UTF-8 sequence  Code class byte 10000000 10000001  Character code 0xxxxxxx 0xxxxxxx  10000010  0xxxxxxx  /  /  10010000  0xxxxxxx  In a code string, if ACSCII code is as default code for 1 byte code, then the class byte of ACSCII can be omitted, as shown in the following example. 0xxxxxxx100000010xxxxxxx0xxxxxxx100000100xxxxxxx0xxxxxxx Scanning the string from left to right, the first code is an ASCII code, 1 byte long; the second is an ISO-8859-1 code, 2 byte long; the third is also an ASCII code; the fourth is an ISO-8859-2 code, 2 byte long; the fifth code is also an ASCII code.  4.2 Transformation for multiple byte local codes  GB18030 includes three kind codes, 1 byte, 2 bytes and 4 byte codes, as shown in Table 6. The 2 byte codes 0f GB18030 is the same as GBK. According to the second byte of GBK, 0xA0~ 0xFE, 0x40 0x7E and 0x80 -0x9F; GBK can be divided into 3 code classes: GBK-0, GBK-1, and GBK-2. GB2312 is included in code class: GBK-0. KSC 5601 can also be represented by one class codes. According to the second byte: 0x40~ 0x7E and 0xA1~0xFE, Big5 can be represented by 2 class codes: Big5-1 and Big5-2. 1 byte code of GB18030 can be divided into 2 parts, ASCII and 0x80; 0x80 can be represented by 1 class of code.  Table 6. GB18030 code space  Code space  Code number  
This paper discusses a planner of the semantics of utterances, whose essential design is an epistemic theorem prover. The planner was designed for the purpose of planning communicative actions, whose eﬀects are famously unknowable and unobservable by the doer/speaker, and depend on the beliefs of and inferences made by the recipient/hearer. The fully implemented model can achieve goals that do not match action eﬀects, but that are rather entailed by them, which it does by reasoning about how to act: state-space planning is interwoven with theorem proving in such a way that a theorem prover uses the eﬀects of actions as hypotheses. The planner is able to model problematic conversational situations, including felicitous and infelicitous instances of bluﬃng, lying, sarcasm, and stating the obvious. 1 
Dependency Tree Semantics (DTS) is an underspeciﬁed formalism for representing quantiﬁer scope ambiguities in natural language. DTS features a direct interface with a Dependency grammar and an incremental, constraint-based disambiguation mechanism. In this paper, we discuss the meaning of quantiﬁer dependency in DTS by translating its well formed structures into formulae of a Second Order Logic augmented with Mostowskian generalized quantiﬁers. 
Abstract Natural language interfaces to spatial databases have not received a lot of attention in computational linguistics, in spite of the potential value of such systems for users of Geographical Information Systems (GISs). This paper presents a controlled language for GIS queries, solves some of the semantic problems for spatial inference in this language, and introduces a system that implements this controlled language as a novel interface for GIS. 
93430 Villetaneuse – FRANCE nouiouaf@lipn.univ-paris13.fr  Pascal Nicolas LERIA University of Angers 2, bd Lavoisier F-49045 Angers cedex pascal.nicolas@univ-angers.fr  1. Motivation The traditional tri-partition syntax/semantics/pragmatics is commonly used in most of the computer systems that aim at the simulation of the human understanding of Natural Language (NL). This conception does not reflect the flexible and creative manner that humans use in reality to interpret texts. Generally speaking, formal NL semantics is referential i.e. it assumes that it is possible to create a static discourse universe and to equate the objects of this universe to the (static) meanings of words. The meaning of a sentence is then built from the meanings of the words in a compositional process and the semantic interpretation of a sentence is reduced to its logical interpretation based on the truth conditions. The very difficult task of adapting the meaning of a sentence to its context is often left to the pragmatic level, and this task requires to use a huge amount of common sense knowledge about the domain. This approach is seriously challenged (see for example [4][14]). It has been showed that the above tri-partition is very artificial because linguistic as well as extra-linguistic knowledge interact in the same global process to provide the necessary elements for understanding. Linguistic phenomena such as polysemy, plurals, metaphors and shifts in meaning create real difficulties to the referential approach of the NL semantics discussed above. As an alternative solution to these problems, [4] proposes an inferential approach to the NL semantics in which words trigger inferences depending on the context of their apparition. In the same spirit we claim that understanding a NL text is a reasoning process based on our knowledge about the norms1 of its domain i.e. what we generally expect to happen in normal situations. But what kind of reasoning is needed for natural language semantics? The answer to this question is based on the remark that texts seldom provide normal details that are assumed to be known to the reader. Instead, they focus on abnormal situations or at least on events that cannot be inferred by default from the text by an ordinary reader. A central issue in the human understanding of NL is the ability to infer systematically and easily an amount of implicit information necessary to answer indirect questions about the text. The consequences resulting from truth-based entailments are logically valid but they are poor and quite limited. Those obtained by a norm-based approach are defeasible: they are admitted as long as the text does not mention explicit elements that contradict them. However they provide richer information and enable a deeper understanding of the text. That is why the norm-based reasoning must be non-monotonic. In addition to this central question, the representation language must take into account a number of modalities (including the temporal aspect) that are very useful to answer different questions on NL texts. The next section gives a general logical framework to represent in a first order language the necessary knowledge about a domain and allows non-monotonic reasoning. Section 3 shows how to implement our representation language fragment in the formalism of Answer Set Programming by transforming them into extended logic programs. In section 4, we discuss the use of our language in 
Viale del Politecnico 1 Rome, Italy pennacchiotti@info.uniroma2.it  Patrick Pantel Information Sciences Institute University of Southern California 4676 Admiralty Way Marina del Rey, CA 90292 pantel@isi.edu  Abstract In this paper, we present Espresso, a weakly-supervised iterative algorithm combined with a web-based knowledge expansion technique, for extracting binary semantic relations. Given a small set of seed instances for a particular relation, the system learns lexical patterns, applies them to extract new instances, and then uses the Web to filter and expand the instances. Preliminary experiments show that Espresso extracts highly precise lists of a wide variety of semantic relations when compared with two state of the art systems. 1. Introduction Recent attention to knowledge-rich problems such as question answering [18] and textual entailment [10] has encouraged Natural Language Processing (NLP) researchers to develop algorithms for automatically harvesting shallow semantic resources. With seemingly endless amounts of textual data at our disposal, we have a tremendous opportunity to automatically grow semantic term banks and ontological resources. Methods must be accurate, adaptable and scalable to the varying sizes of domain corpora (e.g., textbooks vs. World Wide Web), and independent or weakly dependent on human supervision. In this paper we present Espresso, a novel bootstrapping algorithm for automatically harvesting semantic relations, aiming at effectively supporting NLP applications, emphasizing two major points that have been partially neglected by previous systems: generality and weak supervision. From the one side, Espresso is intended as a general-purpose system able to extract a wide variety of binary semantic relations, from the classical is-a and part-of relations, to more specific and domain oriented ones like chemical reactants in a chemistry domain and position succession in political texts. The system architecture is designed with generality in mind, avoiding any relation-specific inference technique. Indeed, for each semantic relation, the system builds specific lexical patterns inferred from textual corpora. From the other side, Espresso requires only weak human supervision. In order to start the extraction process, a user provides only a small set of seed instances of a target relation (e.g. Italy-country and Canada-country for the is-a relation.) In our experience, a handful of seed instances, in general, is sufficient for large corpora while for smaller corpora, a slightly larger set is required. To guarantee weakest supervision, Espresso combines its bootstrapping approach with a web-based knowledge expansion technique and linguistic analysis, exploiting the seeds as much as possible.  2. Relevant Work To date, most research on lexical relation harvesting has focused on is-a and part-of relations. Approaches fall into two main categories: pattern- and clustering-based. Most common are pattern-based approaches. Hearst [12] pioneered using patterns to extract hyponym (is-a) relations. Manually building three lexico-syntactic patterns, Hearst sketched a bootstrapping algorithm to learn more patterns from instances, which has served as the model for most subsequent pattern-based algorithms. Berland and Charniak [1] propose a system for part-of relation extraction, based on the Hearst approach [12]. Seed instances are used to infer linguistic patterns that, in turn, are used to extract new instances, ranked according to various statistical measures. While this study introduces statistical measures to evaluate instance reliability, it remains vulnerable to data sparseness and has the limitation of taking into consideration only one-word terms. Improving upon Berland and Charniak [1], Girju et al. [11] employ machine learning algorithms and WordNet [8] to disambiguate part-of generic patterns, like [whole-NP’s partNP]. This study is the first extensive attempt to solve the problem of generic relational patterns, that is, those expressive patterns that have high recall while suffering low precision, as they subsume a large set of instances. In order to discard incorrect instances, Girju et al. learn WordNet-based selectional restrictions, like [whole-NP(scene#4)’s part-NP(movie#1)]. While making huge grounds on improving precision/recall, the system requires heavy supervision through manual semantic annotations. Ravichandran and Hovy [20] focus on efficiency issues for scaling relation extraction to terabytes of data. A simple and effective algorithm is proposed to infer surface patterns from a small set of instance seeds by extracting all substrings relating seeds in corpus sentences. The frequencies of the substrings in the corpus are then used to retain the best patterns. The approach gives good results on specific relations such as birthdates, however it has low precision on generic ones like is-a and part-of. Pantel et al. [17] proposed a similar, highly scalable approach, based on an edit-distance technique, to learn lexico-POS patterns, showing both good performances and efficiency. Espresso uses a similar approach to infer patterns, but we then apply refining techniques to deal with various types of relations. Other pattern-based algorithms include Riloff and Shepherd [21], who used a semi-automatic method for discovering similar words using a few seed examples by using pattern-based techniques and human supervision, KnowItAll [7] that performs large-scale extraction of facts from the Web, Mann [15] and Fleischman et al. [9] who used part of speech patterns to extract a subset of is-a relations involving proper nouns, and Downey et al. [6] who formalized the problem of relation extraction in a coherent and effective combinatorial model that is shown to outperform previous probabilistic frameworks. Clustering approaches to relation extraction are less common and have insofar been applied only to is-a extraction. These methods employ clustering algorithms to group words according to their meanings in text, label the clusters using its members’ lexical or syntactic dependencies, and then extract an is-a relation between each cluster member and the cluster label. Caraballo [3] proposed the first attempt, which used conjunction and apposition features to build noun clusters. Recently, Pantel and Ravichandran [16] extended this approach by making use of all syntactic dependency features for each noun. The advantage of clustering approaches is that they permit algorithms to identify is-a relations that do not explicitly appear in text, however they generally fail to produce coherent clusters from fewer than 100 million words; hence they are unreliable for small corpora.  3. The Espresso Algorithm The Espresso algorithm is based on a similar framework to the one adopted in [12]. For a specific semantic binary relation (e.g., is-a), the algorithm requires as input a small set of seed instances Is and a corpus C. An instance is a pair of terms x and y governed by the relation at hand (e.g., Pablo Picasso is-a artist). Starting from these seeds, the algorithm begins a four-phase loop. In the first phase, the algorithm infers a set of patterns P that captures as many of the seed instances as possible in C. In the second phase, we define a reliability measure to select the best set of patterns P'⊆P. In phase three, the patterns in P' are used to extract a set of instances I. Finally, in phase four, Espresso scores each instance and then selects the best instances I' as input seeds for the next iteration. The algorithm terminates when a predefined stopping condition is met (for our preliminary experiments, the stopping condition is set according to the size of the corpus). For each induced pattern p and instance i, the information theoretic scores, rπ(p) and rι(i) respectively, aim to express their reliability. Below, Sections 3.2–3.5 describe in detail these different phases of Espresso. 3.1. Term definition Before one can extract relation instances from a corpus, it is necessary to define a tokenization procedure for extracting terms. Terms are commonly defined as surface representations of stable and key domain concepts [19]. Defining regular expressions over POS-tagged corpora is the most commonly used technique to both define and extract terms. We adopt a slightly modified version of the term definition given in [13], as it is one of the most commonly used in the literature: ((Adj|Noun)+|((Adj|Noun)*(NounPrep)?)(Adj|Noun)*)Noun We operationally extend the definition of Adj to include present and past participles as most noun phrases composed of them are usually intended as terms (e.g., boiling point). Thus, unlike many approaches for automatic relation extraction, we allow complex multi-word terms as anchor points. Hence, we can capture relations between complex terms, such as “record of a criminal conviction” part-of “FBI report”. 3.2. Phase 1: Pattern discovery The pattern discovery phase takes as input a set of instances I' and produces as output a set of lexical patterns P. For the first iteration I' = Is, the set of initial seeds. In order to induce P, we apply a slight modification to the approach presented in [20]. For each input instance i = {x, y}, we first retrieve all sentences Sx,y containing the two terms x and y. Sentences are then generalized into a set of new sentences SGx,y by replacing all terminological expressions by a terminological label (TR). For example: “Because/IN HF/NNP is/VBZ a/DT weak/JJ acid/NN and/CC x is/VBZ a/DT y” is generalized as: “Because/IN TR is/VBZ a/DT TR and/CC x is/VBZ a/DT y” All substrings linking terms x and y are then extracted from the set SGx,y, and overall frequencies are computed. The most frequent substrings then represent the set of new patterns P, where the frequency cutoff is experimentally set. Term generalization is particularly useful for small corpora, where generalization is vital to ease the data sparseness. However, the generalized patterns are naturally less precise. Hence, when dealing with bigger corpora, the  system allows the use of Sx,y∪SGx,y in order to extract substrings. For our experiments, we used the set SGx,y .  3.3. Phase 2: Pattern filtering  In this phase, Espresso selects among the patterns P those that are most reliable. Intuitively, a  reliable pattern is one that is both highly precise and one that extracts many instances. The  recall of a pattern p can be approximated by the fraction of input instances in I' that are  extracted by p. Since it is difficult at run-time to estimate the precision of a pattern, we are  weary of keeping patterns that generate many instances (i.e., patterns that generate high recall  but potentially disastrous precision). We thus prefer patterns that are highly associated with  the input patterns I'. Pointwise mutual information [4] is a commonly used metric for  measuring the strength of association between two events x and y:  pmi(x,  y)  =  log  P(x, y) P(x)P(y)  We define the reliability of a pattern p, rπ(p), as its average strength of association across each input instance i in I', weighted by the reliability of each instance i:  ∑ rπ (p) =  ⎜⎛ i∈I′ ⎜⎝  pmi(i, p) max pmi I′  ∗ rι (i)⎟⎟⎠⎞  where rι(i) is the reliability of instance i (defined in Section 3.5) and maxpmi is the maximum pointwise mutual information between all patterns and all instances. rπ(p) ranges from [0,1]. The reliability of the manually supplied seed instances are rι(i) = 1. The pointwise mutual information between instance i = {x, y} and pattern p is estimated using the following formula: pmi(i, p) = log x, p, y x,*, y *, p,*  where |x, p, y| is the frequency of pattern p instantiated with terms x and y and where the asterisk (*) represents a wildcard. A well-known problem is that pointwise mutual information is biased towards infrequent events. To address this, we multiply pmi(i, p) with the discounting factor suggested in [16]. The set of highest n scoring patterns P', according to rπ(p), are then selected and retained for the next phase, where n is the number of patterns of the previous iteration incremented by 1. In general, we expect that the set of patterns is formed by those of the previous iteration plus a new one. Yet, new statistical evidence can lead the algorithm to discard a pattern that was previously discovered. Moreover, to further discourage too generic patterns that might have low precision, a threshold t is set for the number of instances that a pattern retrieves. Patterns firing more than t instances are then discarded, no matter what their score is. In this paper, we experimentally set t to a value dependent on the size of the corpus. In future work, this parameter can be learned using a development corpus. Our reliability measure ensures that overly generic patterns, which may potentially have very low precision, are discarded. However, we are currently exploring a web-expansion algorithm that could both help detect generic patterns and also filter out their incorrect instances. We estimate the precision of the instance set generated by a new pattern p by looking at the number of these instances that are instantiated on the Web by previously accepted patterns.  Generic patterns will generate instances with higher Web counts than incorrect patterns. Then, the Web counts can also be used to filter out incorrect instances from the generic patterns’ instantiations. More details are discussed in Section 4.3.  3.4. Phase 3: Instance discovery In this phase, Espresso retrieves from the corpus the set of instances I that match any of the lexical patterns in P'. In small corpora, the number of extracted instances can be too low to guarantee sufficient statistical evidence for the pattern discovery phase of the next iteration. In such cases, the system enters a web expansion phase, in which new instances for the given patterns are retrieved from the Web, using the Google search engine. Specifically, for each instance i∈ I, the system creates a set of queries, using each pattern in P' with its y term instantiated with i’s y term. For example, given the instance “Italy ; country” and the pattern [Y such as X] , the resulting Google query will be “country such as *”. New instances are then created from the retrieved Web results (e.g. “Canada ; country”) and added to I. We are currently exploring filtering mechanisms to avoid retrieving too much noise. Moreover, to cope with data sparsity, a syntactic expansion phase is also carried out. A set of new instances is created for each instance i∈ I by extracting sub-terminological expressions from x corresponding to the syntactic head of terms. For example, expanding the relation “new record of a criminal conviction” part-of “FBI report”, the following new instances are obtained: “new record” part-of “FBI report”, and “record” part-of “FBI report”.  3.5. Phase 4: Instance filtering  Estimating the reliability of an instance is similar to estimating the reliability of a pattern. Intuitively, a reliable instance is one that is highly associated with as many reliable patterns as possible (i.e., we have more confidence in an instance when multiple reliable patterns instantiate it.) Hence, analogous to our pattern reliability measure in Section 3.3, we define the reliability of an instance i, rι(i), as:  ∑ rι (i) =  p∈P′  pmi(i, p) max pmi P′  ∗ rπ  (p)  where rπ(p) is the reliability of pattern p (defined in Section 3.3) and maxpmi is the maximum pointwise mutual information between all patterns and all instances, as in Section 3.3. Espresso finally selects the highest scoring m instances, I', and retains them as input for the subsequent iteration. In this paper, we experimentally set m = 200.  4. Experimental Results  4.1. Experimental Setup In this section, we present a preliminary comparison of Espresso with two state of the art systems on the task of extracting various semantic relations.  4.1.1. Datasets We perform our experiments using the following two datasets:   TREC-9: This dataset consists of a sample of articles from the Aquaint (TREC-9) newswire text collection. The sample consists of 5,951,432 words extracted from the following data files: AP890101 – AP890131, AP890201 – AP890228, and AP890310 – AP890319.  CHEM: This small dataset of 313,590 words consists of a college level textbook of introductory chemistry [2]. We preprocess the corpora using the Alembic Workbench POS-tagger [5]. 4.1.2. Systems We compare the results of Espresso with the following two state of the art extraction systems:  RH02: This algorithm by Ravichandran and Hovy [20] learns lexical extraction patterns from a set of seed instances of a particular relation (see Section 2.)  PR04: This is-a extraction algorithm from Pantel and Ravichandran [16] first automatically induces concepts (clusters) from a raw corpus, names the concepts, and then extracts an is-a relation between each cluster member and its cluster label. For each cluster member, the system may generate multiple possible is-a relations, but in this evaluation we only keep the highest scoring one. To apply this algorithm, both datasets were first analyzed using the Minipar parser [14].  ESP: This is the algorithm described in this paper (details in Section 3). 4.1.3. Semantic Relations Espresso is designed to extract various semantic relations exemplified by a given small set of seed instances. For our preliminary evaluation, we consider the standard is-a and part-of relations as well as three novel relations:  succession: This relation indicates that one proper noun succeeds another in a position or title. For example, George Bush succeeded Bill Clinton and Pope Benedict XVI succeeded Pope John Paul II. We evaluate this relation on the TREC-9 corpus.  reaction: This relation occurs between chemical elements/molecules that can be combined in a chemical reaction. For example, hydrogen gas reacts-with oxygen gas and zinc reacts-with hydrochloric acid. We evaluate this relation on the CHEM corpus.  production: This relation occurs when a process or element/object produces a result. For example, ammonia produces nitric oxide. We evaluate this relation on the CHEM corpus. For each semantic relation, we manually extracted a set of seed examples. The seeds were used for both Espresso as well as RH021. Table 1 lists a sample of the seeds as well as sample outputs from Espresso. 4.2. Precision and Recall We implemented each of the three systems outlined in Section 4.1.2 and applied them to the TREC and CHEM datasets. For each output set, per relation, we evaluate the precision of the system by extracting a random sample of instances (50 for the TREC corpus and 20 for the 
Abstract Verbs or adjectives and their nominalizations and certain adverb adjective pairs can be argued to introduce the same concept. This can be shown through inference patterns, which can be explained if we assume Davidsonian eventualities underlying all predicates. We make a contribution to the underlying state discussion by investigating the advantages and disadvantages of Davidsonian versus Kimian states for statives such as copular predicates. Findings are implemented in our parser Delilah. 
Ghent University Steven. Schockaert,Martine. DeCock,Etienne. Kerre@ UGent. be Abstract We pursue two strategies for oﬄine data collection for a temporal question answering system that uses both quantitative methods and fuzzy methods to reason about time and events. The ﬁrst strategy extracts event descriptions from the structured year entries in the online encyclopedia Wikipedia, yielding clean quantitative temporal information about a range of events. The second strategy mines the web using patterns indicating temporal relations between events and times and between events. Web mining leverages the volume of data available on the web to ﬁnd qualitative temporal relations between known events and new, related events and to build fuzzy time spans for events for which we lack crisp metric temporal information. 
This short paper is focused on the formal semantic model: Universal Semantic Code (USC), which acquires a semantic lexicon from thesauruses and pairs it with formal meaning representation. The USC model postulates: Knowledge Inference (KI) is effective only on the basis of Semantic Knowledge Representation (SKR). The USC model represents formalized meanings of verbs and phrasal verbs as a main component of its semantic classification. USC algebra defines a formula for the verb, limited set of elements, relations between them, and a natural language interpretation of the formula. 
Semantic Networks (SN) are a knowledge representation paradigm especially suited for the meaning representation of natural language expressions. In order to clearly deﬁne their basic constructs, the relations and functions used in a semantic network must be given a logical characterization. The paper exempliﬁes this strategy for Multilayered Extended Semantic Networks (the so-called MultiNet paradigm). In particular, it is shown that the axioms characterizing the logical properties of the expressional means of an SN have to be classiﬁed according to different criteria which are connected with speciﬁc types of inference. 
Abstract This paper introduces the Alligator theorem prover for Dependent Type Systems (dts). We start with highlighting a number of properties of dts that make them speciﬁcally suited for computational semantics. We then brieﬂy introduce dts and our implementation. The paper concludes with an example of a dts proof that illustrates the suitability of dts for modelling anaphora resolution. 
Similarity measures for text have historically been an important tool for solving information retrieval problems. In this paper we consider extended similarity metrics for documents and other objects embedded in graphs, facilitated via a lazy graph walk. We provide a detailed instantiation of this framework for email data, where content, social networks and a timeline are integrated in a structural graph. The suggested framework is evaluated for the task of disambiguating names in email documents. We show that reranking schemes based on the graph-walk similarity measures often outperform baseline methods, and that further improvements can be obtained by use of appropriate learning methods. 
Classification techniques deploy supervised labeled instances to train classifiers for various classification problems. However labeled instances are limited, expensive, and time consuming to obtain, due to the need of experienced human annotators. Meanwhile large amount of unlabeled data is usually easy to obtain. Semi-supervised learning addresses the problem of utilizing unlabeled data along with supervised labeled data, to build better classifiers. In this paper we introduce a semi-supervised approach based on mutual reinforcement in graphs to obtain more labeled data to enhance the classifier accuracy. The approach has been used to supplement a maximum entropy model for semi-supervised training of the ACE Relation Detection and Characterization (RDC) task. ACE RDC is considered a hard task in information extraction due to lack of large amounts of training data and inconsistencies in the available data. The proposed approach provides 10% relative improvement over the state of the art supervised baseline system. 
We discuss several feature sets for novelty detection at the sentence level, using the data and procedure established in task 2 of the TREC 2004 novelty track. In particular, we investigate feature sets derived from graph representations of sentences and sets of sentences. We show that a highly connected graph produced by using sentence-level term distances and pointwise mutual information can serve as a source to extract features for novelty detection. We compare several feature sets based on such a graph representation. These feature sets allow us to increase the accuracy of an initial novelty classifier which is based on a bagof-word representation and KL divergence. The final result ties with the best system at TREC 2004. 
In many information retrieval and selection tasks it is valuable to score how much a text is about a certain entity and to compute how much the text discusses the entity with respect to a certain viewpoint. In this paper we are interested in giving an aboutness score to a text, when the input query is a person name and we want to measure the aboutness with respect to the biographical data of that person. We present a graph-based algorithm and compare its results with other approaches. 
We study how two graph algorithms apply to topic-driven summarization in the scope of Document Understanding Conferences. The DUC 2005 and 2006 tasks were to summarize into 250 words a collection of documents on a topic consisting of a few statements or questions. Our algorithms select sentences for extraction. We measure their performance on the DUC 2005 test data, using the Summary Content Units made available after the challenge. One algorithm matches a graph representing the entire topic against each sentence in the collection. The other algorithm checks, for pairs of openclass words in the topic, whether they can be connected in the syntactic graph of each sentence. Matching performs better than connecting words, but a combination of both methods works best. They also both favour longer sentences, which makes summaries more ﬂuent. 
In this paper we present a novel similarity between pairs of co-indexed trees to automatically learn textual entailment classiﬁers. We deﬁned a kernel function based on this similarity along with a more classical intra-pair similarity. Experiments show an improvement of 4.4 absolute percent points over state-of-the-art methods. 
In this paper we present a graph-based approach to question answering. The method assumes a graph representation of question sentences and text sentences. Question answering rules are automatically learnt from a training corpus of questions and answer sentences with the answer annotated. The method is independent from the graph representation formalism chosen. A particular example is presented that uses a speciﬁc graph representation of the logical contents of sentences. 
We present a graph-based semi-supervised learning algorithm to address the sentiment analysis task of rating inference. Given a set of documents (e.g., movie reviews) and accompanying ratings (e.g., “4 stars”), the task calls for inferring numerical ratings for unlabeled documents based on the perceived sentiment expressed by their text. In particular, we are interested in the situation where labeled data is scarce. We place this task in the semi-supervised setting and demonstrate that considering unlabeled reviews in the learning process can improve ratinginference performance. We do so by creating a graph on both labeled and unlabeled data to encode certain assumptions for this task. We then solve an optimization problem to obtain a smooth rating function over the whole graph. When only limited labeled data is available, this method achieves signiﬁcantly better predictive accuracy over other methods that ignore the unlabeled examples during training. 
This paper describes a new approach for estimating term weights in a text classiﬁcation task. The approach uses term cooccurrence as a measure of dependency between word features. A random walk model is applied on a graph encoding words and co-occurrence dependencies, resulting in scores that represent a quantiﬁcation of how a particular word feature contributes to a given context. We argue that by modeling feature weights using these scores, as opposed to the traditional frequency-based scores, we can achieve better results in a text classiﬁcation task. Experiments performed on four standard classiﬁcation datasets show that the new random-walk based approach outperforms the traditional term frequency approach to feature weighting. 
Document indexing and representation of term-document relations are very important for document clustering and retrieval. In this paper, we combine a graph-based dimensionality reduction method with a corpus-based association measure within the Generalized Latent Semantic Analysis framework. We evaluate the graph-based GLSA on the document clustering task. 
Synonyms extraction is a difﬁcult task to achieve and evaluate. Some studies have tried to exploit general dictionaries for that purpose, seeing them as graphs where words are related by the deﬁnition they appear in, in a complex network of an arguably semantic nature. The advantage of using a general dictionary lies in the coverage, and the availability of such resources, in general and also in specialised domains. We present here a method exploiting such a graph structure to compute a distance between words. This distance is used to isolate candidate synonyms for a given word. We present an evaluation of the relevance of the candidates on a sample of the lexicon. 
We introduce Chinese Whispers, a randomized graph-clustering algorithm, which is time-linear in the number of edges. After a detailed definition of the algorithm and a discussion of its strengths and weaknesses, the performance of Chinese Whispers is measured on Natural Language Processing (NLP) problems as diverse as language separation, acquisition of syntactic word classes and word sense disambiguation. At this, the fact is employed that the small-world property holds for many graphs in NLP. 
We present a graph-matching algorithm for semantic relation assignment. The algorithm is part of an interactive text analysis system. The system automatically extracts pairs of syntactic units from a text and assigns a semantic relation to each pair. This is an incremental learning algorithm, in which previously processed pairs and user feedback guide the process. After each assignment, the system adds to its database a syntactic-semantic graph centered on the main element of each pair of units. A graph consists of the main unit and all syntactic units with which it is syntactically connected. An edge contains information both about syntax and about semantic relations for use in further processing. Syntactic-semantic graph matching is used to produce a list of candidate assignments for 63.75% of the pairs analysed, and in 57% of situations the correct relations is one of the system’s suggestions; in 19.6% of situations it suggests only the correct relation. 
Ve´ronis (2004) has recently proposed an innovative unsupervised algorithm for word sense disambiguation based on small-world graphs called HyperLex. This paper explores two sides of the algorithm. First, we extend Ve´ronis’ work by optimizing the free parameters (on a set of words which is different to the target set). Second, given that the empirical comparison among unsupervised systems (and with respect to supervised systems) is seldom made, we used hand-tagged corpora to map the induced senses to a standard lexicon (WordNet) and a publicly available gold standard (Senseval 3 English Lexical Sample). Our results for nouns show that thanks to the optimization of parameters and the mapping method, HyperLex obtains results close to supervised systems using the same kind of bag-ofwords features. Given the information loss inherent in any mapping step and the fact that the parameters were tuned for another set of words, these are very interesting results. 
Comparing word contexts is a key component of many NLP tasks, but rarely is it used in conjunction with additional ontological knowledge. One problem is that the amount of overhead required can be high. In this paper, we provide a graphical method which easily combines an ontology with contextual information. We take advantage of the intrinsic graphical structure of an ontology for representing a context. In addition, we turn the ontology into a metric space, such that subgraphs within it, which represent contexts, can be compared. We develop two variants of our graphical method for comparing contexts. Our analysis indicates that our method performs the comparison efﬁciently and offers a competitive alternative to non-graphical methods. 
We describe a highly interactive system for bidirectional, broad-coverage spoken language communication in the healthcare area. The paper briefly reviews the system's interactive foundations, and then goes on to discuss in greater depth issues of practical usability. We present our Translation Shortcuts facility, which minimizes the need for interactive verification of sentences after they have been vetted once, considerably speeds throughput while maintaining accuracy, and allows use by minimally literate patients for whom any mode of text entry might be difficult. We also discuss facilities for multimodal input, in which handwriting, touch screen, and keyboard interfaces are offered as alternatives to speech input when appropriate. In order to deal with issues related to sheer physical awkwardness, we briefly mention facilities for hands-free or eyes-free operation of the system. Finally, we point toward several directions for future improvement of the system. 
We present a task-level evaluation of the French to English version of MedSLT, a medium-vocabulary unidirectional controlled language medical speech translation system designed for doctor-patient diagnosis interviews. Our main goal was to establish task performance levels of novice users and compare them to expert users. Tests were carried out on eight medical students with no previous exposure to the system, with each student using the system for a total of three sessions. By the end of the third session, all the students were able to use the system confidently, with an average task completion time of about 4 minutes. 
 2 Background  S-MINDS is a speech translation engine, which allows an English speaker to communicate with a non-English speaker easily within a question-and-answer, interview-style format. It can handle limited dialogs such as medical triage or hospital admissions. We have built and tested an English-Korean system for doing medical triage with a translation accuracy of 79.8% (for English) and 78.3% (for Korean) for all non-rejected utterances. We will give an overview of the system building process and the quantitative and qualitatively system performance. 
This position paper looks critically at a number of aspects of current research into spoken language translation (SLT) in the medical domain. We ﬁrst discuss the user proﬁle for medical SLT, criticizing designs which assume that the doctor will necessarily need or want to control the technology. If patients are to be users on an equal standing, more attention must be paid to usability issues. We focus brieﬂy on the issue of feedback in SLT systems, pointing out the difﬁculties of relying on text-based paraphrases. We consider the delicate issue of evaluating medical SLT systems, noting that some of the standard and much-used evaluation techniques for all aspects of the SLT chain might not be suitable for use with real users, even if they are role-playing. Finally, we discuss the idea that the “pathway to healthcare” involves much more than a face-to-face interview with a medical professional, and that different technologies including but not restricted to SLT will be appropriate along this pathway.  (SLT) of doctor–patient dialogues is an obvious and timely and attractive application of language technology. As Bouillon et al. (2005) state, the task is both useful and manageable, particularly as interactions are highly constrained, and the domain can be divided into smaller domains based on symptom types. In this position paper, we wish to discuss a number of aspects of this research area, and suggest that we should broaden our horizons to look beyond the central doctor–patient consultation to consider the variety of interactions on the pathway to healthcare, and beyond the conﬁnes of SLT as an appropriate technology for patient–provider communication. In particular we want to stress the importance of the users – both practitioners and patients – in the design, especially considering computer- and conventional literacy. We will argue that the pathway to healthcare involves a range of communicative activities requiring different language skills and implying different technologies, not restricted to SLT. We will comment on the different situations which have been targeted by research in this ﬁeld so far, and the impact of different target languages on research, and how the differing avilability of resources and software inﬂuences research. We also need to consider more carefully the design of the feedback and veriﬁcation elements of systems, and the need for realistic evaluations.  
We describe a highly interactive system for bidirectional, broad-coverage spoken language communication in the healthcare area. The paper briefly reviews the system's interactive foundations, and then goes on to discuss in greater depth our Translation Shortcuts facility, which minimizes the need for interactive verification of sentences after they have been vetted. This facility also considerably speeds throughput while maintaining accuracy, and allows use by minimally literate patients for whom any mode of text entry might be difficult. 
MedSLT is a unidirectional medical speech translation system intended for use in doctor-patient diagnosis dialogues, which provides coverage of several different language pairs and subdomains. Vocabulary ranges from about 350 to 1000 surface words, depending on the language and subdomain. We will demo both the system itself and the development environment, which uses a combination of rule-based and data-driven methods to construct efﬁcient recognisers, generators and transfer rule sets from small corpora.  based language models, and translation uses a rulebased interlingual framework. The system, including the development environment, is built on top of Regulus (Regulus, 2006), an Open Source platform for developing grammar-based speech applications, which in turn sits on top of the Nuance Toolkit. The demo will show how MedSLT can be used to carry out non-trivial diagnostic dialogues. In particular, we will demonstrate how an integrated intelligent help system counteracts the brittleness inherent in rule-based processing, and rapidly leads new users towards the supported system coverage. We will also demo the development environment, and show how grammars and sets of transfer rules can be efﬁciently constructed from small corpora of a few hundred to a thousand examples.  
Sehda’s 2-way speech translation system, S-MINDS, interprets between provider and patient in routine medical interactions with very high accuracy. Optimizing the system for new tasks or languages requires very little data. New developments include a hybrid translation approach that allows participants to say complex or out-ofdomain utterances, the expansion of hands-free functionality, and the ability to deliver the most urgent expressions instantaneously. 
In this paper, we are proposing a multi­ lingual prototype that can effectively col­ lect, record and document medical data in a domain specific environment. The aim of this project is to develop an electronic support system that can be used to assist asthma management in an emergency de­ partment. 
In this paper, we describe the IBM MASTOR, a speech-to-speech translation system that can translate spontaneous free-form speech in real-time on both laptop and hand-held PDAs. Challenges include speech recognition and machine translation in adverse environments, lack of training data and linguistic resources for under-studied languages, and the need to rapidly develop capabilities for new languages. Another challenge is designing algorithms and building models in a scalable manner to perform well even on memory and CPU deficient hand-held computers. We describe our approaches, experience, and success in building working free-form S2S systems that can handle two language pairs (including a low-resource language). 1. INTRODUCTION Automatic speech-to-speech (S2S) translation breaks down communication barriers between people who do not share a common language and hence enable instant oral cross-lingual communication for many critical applications such as emergency medical care. The development of an accurate, efficient and robust S2S translation system poses a lot of challenges. This is especially true for colloquial speech and resource deficient languages. The IBM MASTOR speech-to-speech translation system has been developed for the DARPA CAST and Transtac programs whose mission is to develop technologies that enable rapid deployment of real-time S2S translation of low-resource languages on portable devices. It originated from the IBM MARS S2S system handling the air travel reservation domain described in [1], which was later significantly improved in all components, including ASR, MT and TTS, and later evolved into the MASTOR multilingual S2S system that covers much broader domains such as medical treatment and force protection [2,3]. More recently, we have further broadened our experience and efforts to very rapidly develop systems for under-studied languages, such as regional dialects of Arabic. The intent of this program is to provide language support to military, medical and humanitarian personnel during operations in foreign territories, by deciphering possibly critical language communications with a two-way real-time speech-to-speech translation system designed for specific tasks such as medical triage and force protection. The initial data collection effort for the project has shown that the domain of force protection and medical triage is, though limited, rather broad. In fact, the definition of domain coverage is tough when the speech from responding foreign language speakers are concerned, as their responses are less constrained and may include out-of-domain words and concepts. Moreover, flexible casual or colloquial speaking style inevitably appears in the human-to-human conversational communications. Therefore, the project is a great challenge that calls for major research efforts. * Thanks to DARPA for funding  Among all the challenges for speech recognition and translation for under-studied languages, there are two main issues: 1) Lack of appropriate amount of speech data that represent the domain of interest and the oral language spoken by the target speakers, resulting in difficulties in accurate estimation of statistical models for speech recognition and translation. 2) Lack of linguistic knowledge realization in spelling standards, transcriptions, lexicons and dictionaries, or annotated corpora. Therefore, various different approaches have to be explored. 
 SD translation schema speciﬁes translation  (synchronous grammar)  (string relation)  A syntax-directed translator ﬁrst parses the source-language input into a parse-  induces  implements  tree, and then recursively converts the tree into a string in the target-language. We  SD translator (source parser + recursive converter)  model this conversion by an extended treeto-string transducer that have multi-level trees on the source-side, which gives our  Figure 1: The relationship among SD concepts, adapted from (Aho and Ullman, 1972).  system more expressive power and ﬂexibility. We also deﬁne a direct probability model and use a linear-time dynamic programming algorithm to search for the best derivation. The model is then extended to the general log-linear framework in order to rescore with other features like n-gram language models. We   S        NP(↓1)  VP   VB(↓2) NP(↓3)  ,  VB(↓2)  S NP(↓1)          NP(↓3)     Figure 2: An example of complex reordering represented as an STSG rule, which is beyond any SCFG.  devise a simple-yet-effective algorithm to  generate non-duplicate k-best translations for n-gram rescoring. Initial experimen-  from input string to output string. In this context, an SD translator consists of two components, a source-  tal results on English-to-Chinese transla-  language parser and a recursive converter which is  tion are presented.  usually modeled as a top-down tree-to-string trans-  ducer (Ge´cseg and Steinby, 1984). The relationship  
This paper presents a series of efﬁcient dynamic-programming (DP) based algorithms for phrase-based decoding and alignment computation in statistical machine translation (SMT). The DP-based decoding algorithms are analyzed in terms of shortest path-ﬁnding algorithms, where the similarity to DP-based decoding algorithms in speech recognition is demonstrated. The paper contains the following original contributions: 1) the DP-based decoding algorithm in (Tillmann and Ney, 2003) is extended in a formal way to handle phrases and a novel pruning strategy with increased translation speed is presented 2) a novel alignment algorithm is presented that computes a phrase alignment efﬁciently in the case that it is consistent with an underlying word alignment. Under certain restrictions, both algorithms handle MT-related problems efﬁciently that are generally NP complete (Knight, 1999). 
This paper presents a discriminative parser that does not use a generative model in any way, yet whose accuracy still surpasses a generative baseline. The parser performs feature selection incrementally during training, as opposed to a priori, which enables it to work well with minimal linguistic cleverness. The main challenge in building this parser was ﬁtting the training data into memory. We introduce gradient sampling, which increased training speed 100-fold. Our implementation is freely available at http://nlp.cs.nyu.edu/parser/. 
We present a classiﬁcation-based word prediction model based on IGTREE, a decision-tree induction algorithm with favorable scaling abilities and a functional equivalence to n-gram models with backoff smoothing. Through a ﬁrst series of experiments, in which we train on Reuters newswire text and test either on the same type of data or on general or ﬁctional text, we demonstrate that the system exhibits log-linear increases in prediction accuracy with increasing numbers of training examples. Trained on 30 million words of newswire text, prediction accuracies range between 12.6% on ﬁctional text and 42.2% on newswire text. In a second series of experiments we compare all-words prediction with confusable prediction, i.e., the same task, but specialized to predicting among limited sets of words. Confusable prediction yields high accuracies on nine example confusable sets in all genres of text. The confusable approach outperforms the all-words-prediction approach, but with more data the difference decreases. 
We consider the problem of identifying among many candidates a single best solution which jointly maximizes several domain-speciﬁc target functions. Assuming that the candidate solutions can be generated incrementally, we model the error in prediction due to the incompleteness of partial solutions as a normally distributed random variable. Using this model, we derive a probabilistic search algorithm that aims at ﬁnding the best solution without the necessity to complete and rank all candidate solutions. We do not assume a Viterbi-type decoding, allowing a wider range of target functions. We evaluate the proposed algorithm on the problem of best parse identiﬁcation, combining simple heuristic with more complex machine-learning based target functions. We show that the search algorithm is capable of identifying candidates with a very high score without completing a signiﬁcant proportion of the candidate solutions. 
Markov logic is a highly expressive language recently introduced to specify the connectivity of a Markov network using ﬁrst-order logic. While Markov logic is capable of constructing arbitrary ﬁrst-order formulae over the data, the complexity of these formulae is often limited in practice because of the size and connectivity of the resulting network. In this paper, we present approximate inference and estimation methods that incrementally instantiate portions of the network as needed to enable ﬁrstorder existential and universal quantiﬁers in Markov logic networks. When applied to the problem of identity uncertainty, this approach results in a conditional probabilistic model that can reason about objects, combining the expressivity of recently introduced BLOG models with the predictive power of conditional training. We validate our algorithms on the tasks of citation matching and author disambiguation. 
Integrating information from different stages of an NLP processing pipeline can yield significant error reduction. We demonstrate how re-ranking can improve name tagging in a Chinese information extraction system by incorporating information from relation extraction, event extraction, and coreference. We evaluate three stateof-the-art re-ranking algorithms (MaxEntRank, SVMRank, and p-Norm Push Ranking), and show the benefit of multi-stage re-ranking for cross-sentence and crossdocument inference. 
Subject ellipsis is one of the characteristics of informal English. The investigation of subject ellipsis in corpora thus reveals an abundance of pragmatic and extralinguistic information associated with subject ellipsis that enhances natural language understanding. In essence, the presence of subject ellipsis conveys an ‘informal’ conversation involving 1) an informal ‘Topic’ as well as familiar/close ‘Participants’, 2) specific ‘Connotations’ that are different from the corresponding full sentences: interruptive (ending discourse coherence), polite, intimate, friendly, and less determinate implicatures. This paper also construes linguistic environments that trigger the use of subject ellipsis and resolve subject ellipsis. 
In this paper we discuss issues related to speeding up parsing with wide-coverage uniﬁcation grammars. We demonstrate that state-of-the-art optimisation techniques based on backbone parsing before uniﬁcation do not provide a general solution, because they depend on speciﬁc properties of the grammar formalism that do not hold for all uniﬁcation based grammars. As an alternative, we describe an optimisation technique that combines ambiguity packing at the constituent structure level with pruning based on local features. 
We describe the WHY2-ATLAS intelligent tutoring system for qualitative physics that interacts with students via natural language dialogue. We focus on the issue of analyzing and responding to multisentential explanations. We explore an approach that combines a statistical classiﬁer, multiple semantic parsers and a formal reasoner for achieving a deeper understanding of these explanations in order to provide appropriate feedback on them. 
 have both syntactic and semantic incompatibilities,  This paper investigates how to extend coverage of a domain independent lexicon tailored for natural language understanding. We introduce two algorithms for adding lexical entries from VERBNET to the lexicon of the TRIPS spoken dialogue system. We report results on the efﬁciency of the method, discussing in particular precision versus coverage issues and implications for mapping to other lexical databases.  and compare two techniques for aligning semantic classes and the syntax-semantics mappings between them. The resulting lexicon is to be used in precise interpretation tasks, so its consistency and accuracy are a high priority. Thus, though it is possible to generate lexical entries automatically (Kwon and Hovy, 2006; Swift, 2005), we use a semi-automatic method in which an expert hand-checks the automatically generated entries before adding them to the lexicon. Therefore, our goal is to maximise the number of new useful entries added to the lexicon while min-  
Non-statistical natural language understanding components need world knowledge of the domain for which they are applied in a machine-readable form. This knowledge can be represented by manually created ontologies. However, as soon as new concepts, instances or relations are involved in the domain, the manually created ontology lacks necessary information, i.e. it becomes obsolete and/or incomplete. This means its “world model” will be insufﬁcient to understand the user. The scalability of a natural language understanding system, therefore, essentially depends on its capability to be up to date. The approach presented herein applies the information provided by the user in a dialog system to acquire the knowledge needed to understand him or her adequately. Furthermore, it takes the position that the type of incremental ontology learning as proposed herein constitutes a viable approach to enhance the scalability of natural language systems. 
 One such question answering system (Narayanan  Metaphors are ubiquitous in language and developing methods to identify and deal with metaphors is an open problem in Natural Language Processing (NLP). In this paper we describe results from using a maximum entropy (ME) classiﬁer to identify metaphors. Using the Wall Street Journal (WSJ) corpus, we annotated all the verbal targets associated with a set of frames which includes frames of spatial motion, manipulation, and health. One surprising ﬁnding was that over 90% of annotated targets from these frames are used metaphorically, underscoring the importance of processing ﬁgurative language. We then used this labeled data and  and Harabagiu, 2004) takes PropBank/FrameNet annotations as input, uses the PropBank targets to indicate which actions are being described with which arguments and produces an answer using probabilistic models of actions as the tools of inference. Initiating these action models is called simulation. Such action models provide deep inferential capabilities for embodied domains. They can also, when provided with appropriate metaphoric mappings, be extended to cover metaphoric language (Narayanan, 1997). Exploiting the inferential capabilities of such action models over the broadest domain requires a system to determine whether a verb is being used literally or metaphorically. Such a system could then activate the necessary metaphoric mappings and initiate the appropriate simulation.  each verbal target’s PropBank annotation to train a maximum entropy classiﬁer to make this literal vs. metaphoric distinction. Using the classiﬁer, we reduce the ﬁnal error in the test set by 5% over the verb-speciﬁc majority class baseline and 31% over the corpus-wide majority class baseline.  2 Metaphor Work in Cognitive Semantics (Lakoff and Johnson, 1980; Johnson, 1987; Langacker, 1987; Lakoff, 1994) suggests that the structure of abstract actions (such as states, causes, purposes, and means) are characterized cognitively in terms of image schemas which are schematized recurring patterns from the embodied domains of force, motion, and space.  
While a great effort has concerned the development of fully integrated modular understanding systems, few researches have focused on the problem of unifying existing linguistic formalisms with cognitive processing models. The Situated Constructional Interpretation Model is one of these attempts. In this model, the notion of “construction” has been adapted in order to be able to mimic the behavior of Production Systems. The Construction Grammar approach establishes a model of the relations between linguistic forms and meaning, by the mean of constructions. The latter can be considered as pairings from a topologically structured space to an unstructured space, in some way a special kind of production rules. 
This paper describes our ongoing work in and thoughts on developing a grammar learning system based on a construction grammar formalism. Necessary modules are presented and first results and challenges in formalizing the grammar are shown up. Furthermore, we point out the major reasons why we chose construction grammar as the most fitting formalism for our purposes. Then our approach and ideas of learning new linguistic phenomena, ranging from holophrastic constructions to compositional ones, is presented. 
Fluid Construction Grammar (FCG) is a new linguistic formalism designed to explore in how far a construction grammar approach can be used for handling open-ended grounded dialogue, i.e. dialogue between or with autonomous embodied agents about the world as experienced through their sensory-motor apparatus. We seek scalable, open-ended language systems by giving agents both the ability to use existing conventions or ontologies, and to invent or learn new ones as the needs arise. This paper contains a brief introduction to the key ideas behind FCG and its current status. 
This paper investigates the usefulness of prosodic features in classifying rhetorical relations between utterances in meeting recordings. Five rhetorical relations of contrast, elaboration, summary, question and cause are explored. Three training methods - supervised, unsupervised, and combined - are compared, and classiﬁcation is carried out using support vector machines. The results of this pilot study are encouraging but mixed, with pairwise classiﬁcation achieving an average of 68% accuracy in discerning between relation pairs using only prosodic features, but multi-class classiﬁcation performing only slightly better than chance. 
In a context where information retrieval is extended to spoken “documents” including conversations, it will be important to provide users with the ability to seek informational content, rather than socially motivated small talk that appears in many conversational sources. In this paper we present a preliminary study aimed at automatically identifying “irrelevance” in the domain of telephone conversations. We apply a standard machine learning algorithm to build a classiﬁer that detects offtopic sections with better-than-chance accuracy and that begins to provide insight into the relative importance of features for identifying utterances as on topic or not.  In this paper we investigate one approach for automatically identifying “irrelevance” in the domain of telephone conversations. Our initial data consist of conversations in which each utterance is labeled as being on topic or not. We apply inductive classiﬁer learning algorithms to identify useful features and build classiﬁers to automatically label utterances. We begin in Section 2 by hypothesizing features that might be useful for the identiﬁcation of irrelevant regions, as indicated by research on the linguistics of conversational speech and, in particular, small talk. Next we present our data and discuss our annotation methodology. We follow this with a description of the complete set of features and machine learning algorithms investigated. Section 6 presents our results, including a comparison of the learned classiﬁers and an analysis of the relative utility of various features.  
This paper examines language similarity in messages over time in an online community of adolescents from around the world using three computational measures: Spearman’s Correlation Coefficient, Zipping and Latent Semantic Analysis. Results suggest that the participants’ language diverges over a six-week period, and that divergence is not mediated by demographic variables such as leadership status or gender. This divergence may represent the introduction of more unique words over time, and is influenced by a continual change in subtopics over time, as well as community-wide historical events that introduce new vocabulary at later time periods. Our results highlight both the possibilities and shortcomings of using document similarity measures to assess convergence in language use. 
Our goal is to automatically detect the functional roles that meeting participants play, as well as the expertise they bring to meetings. To perform this task, we build decision tree classiﬁers that use a combination of simple speech features (speech lengths and spoken keywords) extracted from the participants’ speech in meetings. We show that this algorithm results in a role detection accuracy of 83% on unseen test data, where the random baseline is 33.3%. We also introduce a simple aggregation mechanism that combines evidence of the participants’ expertise from multiple meetings. We show that this aggregation mechanism improves the role detection accuracy from 66.7% (when aggregating over a single meeting) to 83% (when aggregating over 5 meetings). 
 2 Background  We investigated automatic action item detection from transcripts of multi-party meetings. Unlike previous work (Gruenstein et al., 2005), we use a new hierarchical annotation scheme based on the roles utterances play in the action item assignment process, and propose an approach to automatic detection that promises improved classiﬁcation accuracy while enabling the extraction of useful information for summarization and reporting. 
In email conversational analysis, it is often useful to trace the the intents behind each message exchange. In this paper, we consider classiﬁcation of email messages as to whether or not they contain certain intents or email-acts, such as “propose a meeting” or “commit to a task”. We demonstrate that exploiting the contextual information in the messages can noticeably improve email-act classiﬁcation. More speciﬁcally, we describe a combination of n-gram sequence features with careful message preprocessing that is highly effective for this task. Compared to a previous study (Cohen et al., 2004), this representation reduces the classiﬁcation error rates by 26.4% on average. Finally, we introduce Ciranda: a new open source toolkit for email speech act prediction. 
We introduce a novel topic segmentation approach that combines evidence of topic shifts from lexical cohesion with linguistic evidence such as syntactically distinct features of segment initial and final contributions. Our evaluation shows that this hybrid approach outperforms state-of-the-art algorithms even when applied to loosely structured, spontaneous dialogue. Further analysis reveals that using dialogue exchanges versus dialogue contributions improves topic segmentation quality. 
We present a system for analyzing conversational data. The system includes state-ofthe-art natural language processing components that have been modified to accommodate the unique nature of conversational data. In addition, we leverage the added richness of conversational data by analyzing various aspects of the participants and their relationships to each other. Our tool provides users with the ability to easily identify topics or persons of interest, including who talked to whom, when, entities that were discussed, etc. Using this tool, one can also isolate more complex networks of information: individuals who may have discussed the same topics but never talked to each other. The tool includes a UI that plots information over time, and a semantic graph that highlights relationships of interest. 
This paper presents a pragmatic approach to Discourse Representation Theory (DRT) in an attempt to address the pragmatic limitations of DRT (Werth 1999; Simons 2003). To achieve a more pragmatic DRT model, this paper extends standard DRT framework to incorporate more pragmatic elements such as representing agents’ cognitive states and the complex process through which agents recognize utterances employing the linguistic content in forming mental representations of other agent’s cognitive states. The paper gives focus to the usually ignored link in DRT literature between speaker beliefs and the linguistic content, and between the linguistic content and hearer’s beliefs. 
This research is aimed at understanding the dynamics of collaborative multi-party discourse across multiple communication modalities. Before we can truly make significant strides in devising collaborative communication systems, there is a need to understand how typical users utilize computationally supported communications mechanisms such as email, instant messaging, video conferencing, chat rooms, etc., both singularly and in conjunction with traditional means of communication such as face-to-face meetings, telephone calls and postal mail. Attempting to understand an individual’s communications profile with access to only a single modality is challenging at best and often futile. Here, we discuss the development of RACE – Retrospective Analysis of Communications Events – a test-bed prototype to investigate issues relating to multi-modal multi-party discourse. We also examine future avenues of research for further enhancing our prototype and investigating this area. 
Most current definitional question answering systems apply one-size-fits-all lexicosyntactic patterns to identify definitions. By analyzing a large set of online definitions, this study shows that the semantic types of definienda constrain both lexical semantics and lexicosyntactic patterns of the definientia. For example, “heart” has the semantic type [Body Part, Organ, or Organ Component] and its definition (e.g., “heart locates between the lungs”) incorporates semantic-typedependent lexicosyntactic patterns (e.g., “TERM locates …”) and terms (e.g., “lung” has the same semantic type [Body Part, Organ, or Organ Component]). In contrast, “AIDS” has a different semantic type [Disease or Syndrome]; its definition (e.g., “An infectious disease caused by human immunodeficiency virus”) consists of different lexicosyntactic patterns (e.g., “…causes by…”) and terms (e.g., “infectious disease” has the semantic type [Disease or Syndrome]). The semantic types are defined in the widely used biomedical knowledge resource, the Unified Medical Language System (UMLS). 
This paper describes a natural language query engine that enables users to search for entities, relationships, and events that are extracted from biological literature. The query interpretation is guided by a domain ontology, which provides a mapping between linguistic structures and domain conceptual relations. We focus on the usability of the natural language interface to users who are used to keywordbased information retrieval. Preliminary evaluation of our approach using the GENIA corpus and ontology shows promising results. 
The ﬁeld of molecular biology is growing at an astounding rate and research ﬁndings are being deposited into public databases, such as Swiss-Prot. Many of the over 200,000 protein entries in Swiss-Prot 49.1 lack annotations such as subcellular localization or function, but the vast majority have references to journal abstracts describing related research. These abstracts represent a huge amount of information that could be used to generate annotations for proteins automatically. Training classiﬁers to perform text categorization on abstracts is one way to accomplish this task. We present a method for improving text classiﬁcation for biological journal abstracts by generating additional text features using the knowledge represented in a biological concept hierarchy (the Gene Ontology). The structure of the ontology, as well as the synonyms recorded in it, are leveraged by our simple technique to signiﬁcantly improve the F-measure of subcellular localization text classiﬁers by as much as 0.078 and we achieve F-measures as high as 0.935. 
With the rising influence of the Gene Ontology, new approaches have emerged where the similarity between genes or gene products is obtained by comparing Gene Ontology code annotations associated with them. So far, these approaches have solely relied on the knowledge encoded in the Gene Ontology and the gene annotations associated with the Gene Ontology database. The goal of this paper is to demonstrate that improvements to these approaches can be obtained by integrating textual evidence extracted from relevant biomedical literature. 
We introduce a new approach to named entity classification which we term a Priority Model. We also describe the construction of a semantic database called SemCat consisting of a large number of semantically categorized names relevant to biomedicine. We used SemCat as training data to investigate name classification techniques. We generated a statistical language model and probabilistic contextfree grammars for gene and protein name classification, and compared the results with the new model. For all three methods, we used a variable order Markov model to predict the nature of strings not represented in the training data. The Priority Model achieves an F-measure of 0.958-0.960, consistently higher than the statistical language model and probabilistic context-free grammar. 
The identiﬁcation of genes in biomedical text typically consists of two stages: identifying gene mentions and normalization of gene names. We have created an automated process that takes the output of named entity recognition (NER) systems designed to identify genes and normalizes them to standard referents. The system identiﬁes human gene synonyms from online databases to generate an extensive synonym lexicon. The lexicon is then compared to a list of candidate gene mentions using various string transformations that can be applied and chained in a ﬂexible order, followed by exact string matching or approximate string matching. Using a gold standard of MEDLINE abstracts manually tagged and normalized for mentions of human genes, a combined tagging and normalization system achieved 0.669 F-measure (0.718 precision and 0.626 recall) at the mention level, and 0.901 F-measure (0.957 precision and 0.857 recall) at the document level for documents used for tagger training. 
University of Texas at Austin  University of Texas at Austin  
In this paper, we construct a biomedical semantic role labeling (SRL) system that can be used to facilitate relation extraction. First, we construct a proposition bank on top of the popular biomedical GENIA treebank following the PropBank annotation scheme. We only annotate the predicate-argument structures (PAS’s) of thirty frequently used biomedical predicates and their corresponding arguments. Second, we use our proposition bank to train a biomedical SRL system, which uses a maximum entropy (ME) model. Thirdly, we automatically generate argument-type templates which can be used to improve classification of biomedical argument types. Our experimental results show that a newswire SRL system that achieves an F-score of 86.29% in the newswire domain can maintain an F-score of 64.64% when ported to the biomedical domain. By using our annotated biomedical corpus, we can increase that F-score by 22.9%. Adding automatically generated template features further increases overall F-score by 0.47% and adjunct arguments (AM) Fscore by 1.57%, respectively.  
The ability to accurately model the content structure of text is important for many natural language processing applications. This paper describes experiments with generative models for analyzing the discourse structure of medical abstracts, which generally follow the pattern of “introduction”, “methods”, “results”, and “conclusions”. We demonstrate that Hidden Markov Models are capable of accurately capturing the structure of such texts, and can achieve classiﬁcation accuracy comparable to that of discriminative techniques. In addition, generative approaches provide advantages that may make them preferable to discriminative techniques such as Support Vector Machines under certain conditions. Our work makes two contributions: at the application level, we report good performance on an interesting task in an important domain; more generally, our results contribute to an ongoing discussion regarding the tradeoffs between generative and discriminative techniques. 
A picture is worth a thousand words. Biomedical researchers tend to incorporate a significant number of images (i.e., figures or tables) in their publications to report experimental results, to present research models, and to display examples of biomedical objects. Unfortunately, this wealth of information remains virtually inaccessible without automatic systems to organize these images. We explored supervised machine-learning systems using Support Vector Machines to automatically classify images into six representative categories based on text, image, and the fusion of both. Our experiments show a significant improvement in the average Fscore of the fusion classifier (73.66%) as compared with classifiers just based on image (50.74%) or text features (68.54%). 
We present a small set of attachment heuristics for postnominal PPs occurring in full-text articles related to enzymes. A detailed analysis of the results suggests their utility for extraction of relations expressed by nominalizations (often with several attached PPs). The system achieves 82% accuracy on a manually annotated test corpus of over 3000 PPs from varied biomedical texts. 
Resolving anaphora is an important step in the identiﬁcation of named entities such as genes and proteins in biomedical scientiﬁc articles. The goal of this work is to resolve associative and coreferential anaphoric expressions making use of the rich domain resources (such as databases and ontologies) available for the biomedical area, instead of annotated training data. The results are comparable to extant state-of-the-art supervised methods in the same domain. The system is integrated into an interactive tool designed to assist FlyBase curators by aiding the identiﬁcation of the salient entities in a given paper as a ﬁrst step in the aggregation of information about them. 
We describe BioLiterate, a prototype software system which infers relationships involving relationships between genes, proteins and malignancies from research abstracts, and has initially been tested in the domain of the molecular genetics of oncology. The architecture uses a natural language processing module to extract entities, dependencies and simple semantic relationships from texts, and then feeds these features into a probabilistic reasoning module which combines the semantic relationships extracted by the NLP module to form new semantic relationships. One application of this system is the discovery of relationships that are not contained in any individual abstract but are implicit in the combined knowledge contained in two or more abstracts. 
Nested Named Entities (nested NEs), one containing another, are commonly seen in biomedical text, e.g., accounting for 16.7% of all named entities in GENIA corpus. While many works have been done in recognizing non-nested NEs, nested NEs have been largely neglected. In this work, we treat the task as a binary classification problem and solve it using Support Vector Machines. For each token in nested NEs, we use two schemes to set its class label: labeling as the outmost entity or the inner entity. Our preliminary results show that while the outmost labeling tends to work better in recognizing the outmost entities, the inner labeling recognizes the inner NEs better. This result should be useful for recognition of nested NEs. 
We propose a novel approach to the identiﬁcation of biomedical terms in research publications using the Perceptron HMM algorithm. Each important term is identiﬁed and classiﬁed into a biomedical concept class. Our proposed system achieves a 68.6% F-measure based on 2,000 training Medline abstracts and 404 unseen testing Medline abstracts. The system achieves performance that is close to the state-of-the-art using only a small feature set. The Perceptron HMM algorithm provides an easy way to incorporate many potentially interdependent features. 
We describe a pilot project in semiautomatically refactoring a biomedical corpus. The total time expended was just over three person-weeks, suggesting that this is a cost-efﬁcient process. The refactored corpus is available for download at http://bionlp.sourceforge.net. 
In modern biology, digitization of biosystematics publications is an important task. Extraction of taxonomic names from such documents is one of its major issues. This is because these names identify the various genera and species. This article reports on our experiences with learning techniques for this particular task. We say why established Named-Entity Recognition techniques are somewhat difficult to use in our context. One reason is that we have only very little training data available. Our experiments show that a combining approach that relies on regular expressions, heuristics, and word-level language recognition achieves very high precision and recall and allows to cope with those difficulties. 
We demonstrate that bootstrapping a gene name recognizer for FlyBase curation from automatically annotated noisy text is more effective than fully supervised training of the recognizer on more general manually annotated biomedical text. We present a new test set for this task based on an annotation scheme which distinguishes gene names from gene mentions, enabling a more consistent annotation. Evaluating our recognizer using this test set indicates that performance on unseen genes is its main weakness. We evaluate extensions to the technique used to generate training data designed to ameliorate this problem. 
This paper presents a fully automated linguistic approach to measuring distance between phonemes across languages. In this approach, a phoneme is represented by a feature matrix where feature categories are fixed, hierarchically related and binary-valued; feature categorization explicitly addresses allophonic variation and feature values are weighted based on their relative prominence derived from lexical frequency measurements. The relative weight of feature values is factored into phonetic distance calculation. Two phonological distances are statistically derived from lexical frequency measurements. The phonetic distance is combined with the phonological distances to produce a single metric that quantifies cross-language phoneme distance. The performances of target-language phoneme HMMs constructed solely with source language HMMs, first selected by the combined phonetic and phonological metric and then by a data-driven, acoustics distance-based method, are compared in context-independent automatic speech recognition (ASR) experiments. Results show that this approach consistently performs equivalently to the acoustics-based approach, confirming its effectiveness in estimating cross-language similarity between phonemes in an ASR environment.  
This paper presents an unsupervised batch learner for the quantity-insensitive stress systems described in Gordon (2002). Unlike previous stress learning models, the learner presented here is neither cue based (Dresher and Kaye, 1990), nor reliant on a priori Optimality-theoretic constraints (Tesar, 1998). Instead our learner exploits a property called neighborhooddistinctness, which is shared by all of the target patterns. Some consequences of this approach include a natural explanation for the occurrence of binary and ternary rhythmic patterns, the lack of higher n-ary rhythms, and the fact that, in these systems, stress always falls within a certain window of word edges. 
Within the information-theoretical framework described by (Rissanen, 1989; de Marcken, 1996; Goldsmith, 2001), pointers are used to avoid repetition of phonological material. Work with which we are familiar has assumed that there is only one way in which items could be pointed to. The purpose of this paper is to describe and compare several different methods, each of which satisﬁes MDL’s basic requirements, but which have different consequences for the treatment of linguistic phenomena. In particular, we assess the conditions under which these different ways of pointing yield more compact descriptions of the data, both from a theoretical and an empirical perspective. 
In performing morpho-phonological sequence processing tasks, such as letterphoneme conversion or morphological analysis, it is typically not enough to base the output sequence on local decisions that map local-context input windows to single output tokens. We present a global sequence-processing method that repairs inconsistent local decisions. The approach is based on local predictions of overlapping trigrams of output tokens, which open up a space of possible sequences; a data-driven constraint satisfaction inference step then searches for the optimal output sequence. We demonstrate signiﬁcant improvements in terms of word accuracy on English and Dutch letter-phoneme conversion and morphological segmentation, and we provide qualitative analyses of error types prevented by the constraint satisfaction inference method. 
This paper proposes an unsupervised learning algorithm for Optimality Theoretic grammars, which learns a complete constraint ranking and a lexicon given only unstructured surface forms and morphological relations. The learning algorithm, which is based on the ExpectationMaximization algorithm, gradually maximizes the likelihood of the observed forms by adjusting the parameters of a probabilistic constraint grammar and a probabilistic lexicon. The paper presents the algorithm’s results on three constructed language systems with different types of hidden structure: voicing neutralization, stress, and abstract vowels. In all cases the algorithm learns the correct constraint ranking and lexicon. The paper argues that the algorithm’s ability to identify correct, restrictive grammars is due in part to its explicit reliance on the Optimality Theoretic notion of Richness of the Base. 
For a language with limited resources, a dictionary may be one of the few available electronic resources. To make effective use of the dictionary for translation, however, users must be able to access it using the root form of morphologically deformed variant found in the text. Stemming and data driven methods, however, are not suitable when data is sparse. We present algorithms for discovering morphemes from limited, noisy data obtained by scanning a hard copy dictionary. Our approach is based on the novel application of the longest common substring and string edit distance metrics. Results show that these algorithms can in fact segment words into roots and afﬁxes from the limited data contained in a dictionary, and extract afﬁxes. This in turn allows non native speakers to perform multilingual tasks for applications where response must be rapid, and their knowledge is limited. In addition, this analysis can feed other NLP tools requiring lexicons.  of data for statistical methods. New approaches that can deal with limited, and perhaps noisy, data are necessary for these languages. Printed dictionaries often exist for languages before large amounts of electronic text, and provide a variety of information in a structured format. In this paper, we propose Morphology Induction from Noisy Data (MIND), a natural language morphology induction framework that operates on from information in dictionaries, speciﬁcally headwords and examples of usage. We use string searching algorithms to morphologically segment words and identify preﬁxes, sufﬁxes, circumﬁxes, and inﬁxes in noisy and limited data. We present our preliminary results on two data sources (Cebuano and Turkish), give a detailed analysis of results, and compare them to a state-of-the-art morphology learner. We employ the automatically induced afﬁxes in a simple word segmentation process, decreasing the error rate of incorrectly segmented words by 35.41%. The next section discusses prior work on morphology learning. In Section 3 and 4, we describe our approach and MIND framework in detail. Section 6 explains the experiments and presents results. We conclude with future work. 2 Related Work  
This paper introduces the probabilistic paradigm, a probabilistic, declarative model of morphological structure. We describe an algorithm that recursively applies Latent Dirichlet Allocation with an orthogonality constraint to discover morphological paradigms as the latent classes within a suffix-stem matrix. We apply the algorithm to data preprocessed in several different ways, and show that when suffixes are distinguished for part of speech and allomorphs or gender/conjugational variants are merged, the model is able to correctly learn morphological paradigms for English and Spanish. We compare our system with Linguistica (Goldsmith 2001), and discuss the advantages of the probabilistic paradigm over Linguistica’s signature representation. 
We present a novel approach to the unsupervised detection of afﬁxes, that is, to extract a set of salient preﬁxes and sufﬁxes from an unlabeled corpus of a language. The underlying theory makes no assumptions on whether the language uses a lot of morphology or not, whether it is preﬁxing or sufﬁxing, or whether afﬁxes are long or short. It does however make the assumption that 1. salient afﬁxes have to be frequent, i.e occur much more often that random segments of the same length, and that 2. words essentially are variable length sequences of random characters, e.g a character should not occur in far too many words than random without a reason, such as being part of a very frequent afﬁx. The afﬁx extraction algorithm uses only information from ﬂuctation of frequencies, runs in linear time, and is free from thresholds and untransparent iterations. We demonstrate the usefulness of the approach with example case studies on typologically distant languages. 
Evaluation of machine translation output is an important but difﬁcult task. Over the last years, a variety of automatic evaluation measures have been studied, some of them like Word Error Rate (WER), Position Independent Word Error Rate (PER) and BLEU and NIST scores have become widely used tools for comparing different systems as well as for evaluating improvements within one system. However, these measures do not give any details about the nature of translation errors. Therefore some analysis of the generated output is needed in order to identify the main problems and to focus the research efforts. On the other hand, human evaluation is a time consuming and expensive task. In this paper, we investigate methods for using of morpho-syntactic information for automatic evaluation: standard error measures WER and PER are calculated on distinct word classes and forms in order to get a better idea about the nature of translation errors and possibilities for improvements. 
This paper presents some very preliminary results for and problems in developing a statistical machine translation system from English to Turkish. Starting with a baseline word model trained from about 20K aligned sentences, we explore various ways of exploiting morphological structure to improve upon the baseline system. As Turkish is a language with complex agglutinative word structures, we experiment with morphologically segmented and disambiguated versions of the parallel texts in order to also uncover relations between morphemes and function words in one language with morphemes and functions words in the other, in addition to relations between open class content words. Morphological segmentation on the Turkish side also conﬂates the statistics from allomorphs so that sparseness can be alleviated to a certain extent. We ﬁnd that this approach coupled with a simple grouping of most frequent morphemes and function words on both sides improve the BLEU score from the baseline of 0.0752 to 0.0913 with the small training data. We close with a discussion on why one should not expect distortion parameters to model word-local morpheme ordering and that a new approach to handling complex morphotactics is needed.  
The Arabic language has far richer systems of inﬂection and derivation than English which has very little morphology. This morphology difference causes a large gap between the vocabulary sizes in any given parallel training corpus. Segmentation of inﬂected Arabic words is a way to smooth its highly morphological nature. In this paper, we describe some statistically and linguistically motivated methods for Arabic word segmentation. Then, we show the efﬁciency of proposed methods on the Arabic-English BTEC and NIST tasks. 
Many syntactic models in machine translation are channels that transform one tree into another, or synchronous grammars that generate trees in parallel. We present a new model of the translation process: quasi-synchronous grammar (QG). Given a source-language parse tree T1, a QG deﬁnes a monolingual grammar that generates translations of T1. The trees T2 allowed by this monolingual grammar are inspired by pieces of substructure in T1 and aligned to T1 at those points. We describe experiments learning quasi-synchronous context-free grammars from bitext. As with other monolingual language models, we evaluate the crossentropy of QGs on unseen text and show that a better ﬁt to bilingual data is achieved by allowing greater syntactic divergence. When evaluated on a word alignment task, QG matches standard baselines.  
We investigate why weights from generative models underperform heuristic estimates in phrasebased machine translation. We ﬁrst propose a simple generative, phrase-based model and verify that its estimates are inferior to those given by surface statistics. The performance gap stems primarily from the addition of a hidden segmentation variable, which increases the capacity for overﬁtting during maximum likelihood training with EM. In particular, while word level models beneﬁt greatly from re-estimation, phrase-level models do not: the crucial difference is that distinct word alignments cannot all be correct, while distinct segmentations can. Alternate segmentations rather than alternate alignments compete, resulting in increased determinization of the phrase table, decreased generalization, and decreased ﬁnal BLEU score. We also show that interpolation of the two methods can result in a modest increase in BLEU score. 
In this article, we present a translation system which builds translations by gluing together Tree-Phrases, i.e. associations between simple syntactic dependency treelets in a source language and their corresponding phrases in a target language. The Tree-Phrases we use in this study are syntactically informed and present the advantage of gathering source and target material whose words do not have to be adjacent. We show that the phrase-based translation engine we implemented beneﬁts from Tree-Phrases. 
We present discriminative reordering models for phrase-based statistical machine translation. The models are trained using the maximum entropy principle. We use several types of features: based on words, based on word classes, based on the local context. We evaluate the overall performance of the reordering models as well as the contribution of the individual feature types on a word-aligned corpus. Additionally, we show improved translation performance using these reordering models compared to a state-of-the-art baseline system. 
In this paper we propose a generalization of the Stack-based decoding paradigm for Statistical Machine Translation. The well known single and multi-stack decoding algorithms deﬁned in the literature have been integrated within a new formalism which also deﬁnes a new family of stackbased decoders. These decoders allows a tradeoff to be made between the advantages of using only one or multiple stacks. The key point of the new formalism consists in parameterizeing the number of stacks to be used during the decoding process, and providing an efﬁcient method to decide in which stack each partial hypothesis generated is to be insertedduring the search process. Experimental results are also reported for a search algorithm for phrase-based statistical translation models. 
Word posterior probabilities are a common approach for conﬁdence estimation in automatic speech recognition and machine translation. We will generalize this idea and introduce n-gram posterior probabilities and show how these can be used to improve translation quality. Additionally, we will introduce a sentence length model based on posterior probabilities. We will show signiﬁcant improvements on the Chinese-English NIST task. The absolute improvements of the BLEU score is between 1.1% and 1.6%. 
In statistical machine translation, large numbers of parallel sentences are required to train the model parameters. However, plenty of the bilingual language resources available on web are aligned only at the document level. To exploit this data, we have to extract the bilingual sentences from these documents. The common method is to break the documents into segments using predeﬁned anchor words, then these segments are aligned. This approach is not error free, incorrect alignments may decrease the translation quality. We present an alternative approach to extract the parallel sentences by partitioning a bilingual document into two pairs. This process is performed recursively until all the sub-pairs are short enough. In experiments on the Chinese-English FBIS data, our method was capable of producing translation results comparable to those of a state-of-the-art sentence aligner. Using a combination of the two approaches leads to better translation performance. 
In this paper we present a novel method for deriving paraphrases during automatic MT evaluation using only the source and reference texts, which are necessary for the evaluation, and word and phrase alignment software. Using target language paraphrases produced through word and phrase alignment a number of alternative reference sentences are constructed automatically for each candidate translation. The method produces lexical and lowlevel syntactic paraphrases that are relevant to the domain in hand, does not use external knowledge resources, and can be combined with a variety of automatic MT evaluation system. 
State of the art in statistical machine translation is currently represented by phrasebased models, which typically incorporate a large number of probabilities of phrase-pairs and word n-grams. In this work, we investigate data compression methods for efﬁciently encoding n-gram and phrase-pair probabilities, that are usually encoded in 32-bit ﬂoating point numbers. We measured the impact of compression on translation quality through a phrase-based decoder trained on two distinct tasks: the translation of European Parliament speeches from Spanish to English, and the translation of news agencies from Chinese to English. We show that with a very simple quantization scheme all probabilities can be encoded in just 4 bits with a relative loss in BLEU score on the two tasks by 1.0% and 1.6%, respectively. 
We present two translation systems experimented for the shared-task of “Workshop on Statistical Machine Translation,” a phrase-based model and a hierarchical phrase-based model. The former uses a phrasal unit for translation, whereas the latter is conceptualized as a synchronousCFG in which phrases are hierarchically combined using non-terminals. Experiments showed that the hierarchical phrasebased model performed very comparable to the phrase-based model. We also report a phrase/rule extraction technique diﬀerentiating tokenization of corpora. 
Complex Language Models cannot be easily integrated in the ﬁrst pass decoding of a Statistical Machine Translation system – the decoder queries the LM a very large number of times; the search process in the decoding builds the hypotheses incrementally and cannot make use of LMs that analyze the whole sentence. We present in this paper the Language Computer’s system for WMT06 that employs LMpowered reranking on hypotheses generated by phrase-based SMT systems 
The joint probability model proposed by Marcu and Wong (2002) provides a strong probabilistic framework for phrase-based statistical machine translation (SMT). The model’s usefulness is, however, limited by the computational complexity of estimating parameters at the phrase level. We present the ﬁrst model to use word alignments for constraining the space of phrasal alignments searched during Expectation Maximization (EM) training. Constraining the joint model improves performance, showing results that are very close to stateof-the-art phrase-based models. It also allows it to scale up to larger corpora and therefore be more widely applicable. 
The Microsoft Research translation system is a syntactically informed phrasal SMT system that uses a phrase translation model based on dependency treelets and a global reordering model based on the source dependency tree. These models are combined with several other knowledge sources in a log-linear manner. The weights of the individual components in the loglinear model are set by an automatic parametertuning method. We give a brief overview of the components of the system and discuss our experience with the Europarl data translating from English to Spanish. 1. Introduction The dependency treelet translation system developed at MSR is a statistical MT system that takes advantage of linguistic tools, namely a source language dependency parser, as well as a word alignment component. [1] To train a translation system, we require a sentence-aligned parallel corpus. First the source side is parsed to obtain dependency trees. Next the corpus is word-aligned, and the source dependencies are projected onto the target sentences using the word alignments. From the aligned dependency corpus we extract all treelet translation pairs, and train an order model and a bi-lexical dependency model. To translate, we parse the input sentence, and employ a decoder to find a combination and ordering of treelet translation pairs that cover the source tree and are optimal according to a set of models. In a now-common generalization of the classic noisy-channel framework, we use a loglinear combination of models [2], as in below:  { } translation S , F ,Λ=argmax T  ∑λ f ∈F f  f S ,T   Such an approach toward translation scoring has proven very effective in practice, as it allows a translation system to incorporate information from a variety of probabilistic or non-probabilistic sources. The weights Λ = { λf } are selected by discriminatively training against held out data.  2. System Details  A brief word on notation: s and t represent source  and target lexical nodes; S and T represent source  and target trees; s and t represent source and target  treelets (connected subgraphs of the dependency  tree). The expression ∀t∈ T refers to all the  lexical items in the target language tree T and |T|  refers to the count of lexical items in T. We use  subscripts th  to  indicate  selected  words:  Tn  represents  the n lexical item in an in-order traversal of T.  2.1. Training We use the broad coverage dependency parser NLPWIN [3] to obtain source language dependency trees, and we use GIZA++ [4] to produce word alignments. The GIZA++ training regimen and parameters are tuned to optimize BLEU [5] scores on held-out data. Using the word alignments, we follow a set of dependency tree projection heuristics [1] to construct target dependency trees, producing a word-aligned parallel dependency tree corpus. Treelet translation pairs are extracted by enumerating all source treelets (to a maximum size) aligned to a target treelet.  2.2. Decoding  We use a tree-based decoder, inspired by dynamic programming. It searches for an approximation of  158 Proceedings of the Workshop on Statistical Machine Translation, pages 158–161, New York City, June 2006. c 2006 Association for Computational Linguistics  we ­2 should ­1 follow the ­2  Rio ­1  agenda +1  hemos ­1 de +1 cumplir el ­1 programa +1 de ­1 Río +1  Figure 1: Aligned dependency tree pair, annotated with headrelative positions  the n-best translations of each subtree of the input dependency tree. Translation candidates are composed from treelet translation pairs extracted from the training corpus. This process is described in more detail in [1].  2.3. Models  2.3.1. Channel models  We employ several channel models: a direct maximum likelihood estimate of the probability of target given source, as well as an estimate of source given target and target given source using the word-based IBM Model 1 [6]. For MLE, we use absolute discounting to smooth the probabilities:  P  MLE    t∣s  =  c    s c  ,   t s  −λ ,*   Here, c represents the count of instances of the treelet pair 〈s, t〉 in the training corpus, and λ is determined empirically. For Model 1 probabilities we compute the sum over all possible alignments of the treelet without normalizing for length. The calculation of source given target is presented below; target given source is calculated symmetrically. PM1 t∣s =∏ ∑ P  t∣s  t∈t s∈s  2.3.2. Bilingual n-gram channel models Traditional phrasal SMT systems are beset by a number of theoretical problems, such as the ad hoc estimation of phrasal probability, the failure to model the partition probability, and the tenuous connection between the phrases and the underlying word-based alignment model. In string-based SMT systems, these problems are outweighed by the key role played by phrases in capturing “local” order. In the absence of good global ordering models, this has led to an 159  inexorable push towards longer and longer phrases, resulting in serious practical problems of scale, without, in the end, obviating the need for a real global ordering story. In [13] we discuss these issues in greater detail and also present our approach to this problem. Briefly, we take as our basic unit the Minimal Translation Unit (MTU) which we define as a set of source and target word pairs such that there are no word alignment links between distinct MTUs, and no smaller MTUs can be extracted without violating the previous constraint. In other words, these are the minimal non-compositional phrases. We then build models based on n-grams of MTUs in source string, target string and source dependency tree order. These bilingual n-gram models in combination with our global ordering model allow us to use shorter phrases without any loss in quality, or alternately to improve quality while keeping phrase size constant. As an example, consider the aligned sentence pair in Figure 1. There are seven MTUs: m1 = <we should / hemos> m2 = <NULL / de> m3 = <follow / cumplir> m4 = <the / el> m5 = <Rio / Rio> m6 = <agenda / programa> m7 = <NULL / de> We can then predict the probability of each MTU in the context of (a) the previous MTUs in source order, (b) the previous MTUs in target order, or (c) the ancestor MTUs in the tree. We consider all of these traversal orders, each acting as a separate feature function in the log linear combination. For source and target traversal order we use a trigram model, and a bigram model for tree order. 2.3.3. Target language models We use both a surface level trigram language model and a dependency-based bigram language model [7], similar to the bilexical dependency modes used in some English Treebank parsers (e.g. [8]). ∏∣T∣ Psurf  T = Ptrisurf  T i∣T i−2 , T i−1  i=1 ∣T ∣ ∏ Pbilex  T = Pbidep  T i∣ parent  T i  i=1 Ptrisurf is a Kneser-Ney smoothed trigram language model trained on the target side of the training corpus, and Pbilex is a Kneser-Ney smoothed  bigram language model trained on target language dependencies extracted from the aligned parallel dependency tree corpus.  2.3.4. Order model  The order model assigns a probability to the position (pos) of each target node relative to its head based on information in both the source and target trees:  P  order order  T  ∣S  ,T  = ∏ t ∈T  P  pos  t  ,  parent  t   ∣S  ,T    Here, position is modeled in terms of closeness to the head in the dependency tree. The closest premodifier of a given head has position -1; the closest post-modifier has a position 1. Figure 1 shows an example dependency tree pair annotated with head-relative positions. We use a small set of features reflecting local information in the dependency tree to model P(pos (t,parent(t)) | S, T): • Lexical items of t and parent(t), the parent of t in the dependency tree. • Lexical items of the source nodes aligned to t and head(t). • Part-of-speech ("cat") of the source nodes aligned to the head and modifier. • Head-relative position of the source node aligned to the source modifier. These features along with the target feature are gathered from the word-aligned parallel dependency tree corpus and used to train a statistical model. In previous versions of the system, we trained a decision tree model [9]. In the current version, we explored log-linear models. In addition to providing a different way of combining information from multiple features, log-linear models allow us to model the similarity among different classes (target positions), which is advantageous for our task. We implemented a method for automatic selection of features and feature conjunctions in the log-linear model. The method greedily selects feature conjunction templates that maximize the accuracy on a development set. Our feature selection study showed that the part-of-speech labels of the source nodes aligned to the head and the modifier and the head-relative position of the source node corresponding to the modifier were the most important features. It was useful to concatenate the part-of-speech of the source head with every feature. This effectively achieves learning of separate movement models for each  160  source head category. Lexical information on the pairs of head and dependent in the source and target was also very useful. To model the similarity among different target classes and to achieve pooling of data across similar classes, we added multiple features of the target position. These features let our model know, for example, that position -5 looks more like position -6 than like position 3. We added a feature “positive”/“negative” which is shared by all positive/negative positions. We also added a feature looking at the displacement of a position in the target from the corresponding position in the source and features which group the target positions into bins. These features of the target position are combined with features of the input. This model was trained on the provided parallel corpus. As described in Section 2.1 we parsed the source sentences, and projected target dependencies. Each head-modifier pair in the resulting target trees constituted a training instance for the order model. The score computed by the log-linear order model is used as a single feature in the overall loglinear combination of models (see Section 1), whose parameters were optimized using MaxBLEU [2]. This order model replaced the decision tree-based model described in [1]. We compared the decision tree model to the log-linear model on predicting the position of a modifier using reference parallel sentences, independent of the full MT system. The decision tree achieved per decision accuracy of 69% whereas the log-linear model achieved per 
Considering data obtained from a corpus of database QA dialogues, we address the nature of the discourse structure needed to resolve the several kinds of contextual phenomena found in our corpus. We look at the thematic relations holding between questions and the preceding context and discuss to which extent thematic relatedness plays a role in discourse structure. 
QACIAD (Question Answering Challenge for Information Access Dialogue) is an evaluation framework for measuring interactive question answering (QA) technologies. It assumes that users interactively collect information using a QA system for writing a report on a given topic and evaluates, among other things, the capabilities needed under such circumstances. This paper reports an experiment for examining the assumptions made by QACIAD. In this experiment, dialogues under the situation that QACIAD assumes are collected using WoZ (Wizard of Oz) simulating, which is frequently used for collecting dialogue data for designing speech dialogue systems, and then analyzed. The results indicate that the setting of QACIAD is real and appropriate and that one of the important capabilities for future interactive QA systems is providing cooperative and helpful responses. 
The automatic QA system described in this paper uses a reference interview model to allow the user to guide and contribute to the QA process. A set of system capabilities was designed and implemented that defines how the user’s contributions can help improve the system. These include tools, called the Query Template Builder and the Knowledge Base Builder, that tailor the document processing and QA system to a particular domain by allowing a Subject Matter Expert to contribute to the query representation and to the domain knowledge. During the QA process, the system can interact with the user to improve query terminology by using Spell Checking, Answer Type verification, Expansions and Acronym Clarifications. The system also has capabilities that depend upon, and expand the user’s history of interaction with the system, including a User Profile, Reference Resolution, and Question Similarity modules 
This paper describes a new methodology for enhancing the quality and relevance of suggestions provided to users of interactive Q/A systems. We show that by using Conditional Random Fields to combine relevance feedback gathered from users along with information derived from discourse structure and coherence, we can accurately identify irrelevant suggestions with nearly 90% F-measure. 
Contextual question answering (QA), in which users’ information needs are satisﬁed through an interactive QA dialogue, has recently attracted more research attention. One challenge of engaging dialogue into QA systems is to determine whether a question is relevant to the previous interaction context. We refer to this task as relevancy recognition. In this paper we propose a data driven approach for the task of relevancy recognition and evaluate it on two data sets: the TREC data and the HandQA data. The results show that we achieve better performance than a previous rule-based algorithm. A detailed evaluation analysis is presented.  
In this paper, we propose ellipsis handling method for follow-up questions in Information Access Dialogue (IAD) task of NTCIR QAC3. In this method, our system classiﬁes ellipsis patterns of question sentences into three types and recognizes elliptical elements using ellipsis handling algorithm for each type. In the evaluation using Formal Run and Reference Run data, there were several cases which our algorithm could not handle ellipsis correctly. According to the analysis of evaluation results, the main reason of low performance was lack of word information for recognition of referential elements. If our system can recognize word meanings correctly, some errors will not occur and ellipsis handling works well. 
We describe a large-scale evaluation of four interactive question answering system with real users. The purpose of the evaluation was to develop evaluation methods and metrics for interactive QA systems. We present our evaluation method as a case study, and discuss the design and administration of the evaluation components and the effectiveness of several evaluation techniques with respect to their validity and discriminatory power. Our goal is to provide a roadmap to others for conducting evaluations of their own systems, and to put forward a research agenda for interactive QA evaluation. 
In this presentation, I will look back at 10 years of CoNLL conferences and the state of the art of machine learning of language that is evident from this decade of research. My conclusion, intended to provoke discussion, will be that we currently lack a clear motivation or “mission” to survive as a discipline. I will suggest that a new mission for the ﬁeld could be found in a renewed interest for theoretical work (which learning algorithms have a bias that matches the properties of language?, what is the psycholinguistic relevance of learner design issues?), in more sophisticated comparative methodology, and in solving the problem of transfer, reusability, and adaptation of learned knowledge. 
Previous results have shown disappointing performance when porting a parser trained on one domain to another domain where only a small amount of data is available. We propose the use of data-deﬁned kernels as a way to exploit statistics from a source domain while still specializing a parser to a target domain. A probabilistic model trained on the source domain (and possibly also the target domain) is used to deﬁne a kernel, which is then used in a large margin classiﬁer trained only on the target domain. With a SVM classiﬁer and a neural network probabilistic model, this method achieves improved performance over the probabilistic model alone. 
While most work on parsing with PCFGs has focused on local correlations between tree conﬁgurations, we attempt to model non-local correlations using a ﬁnite mixture of PCFGs. A mixture grammar ﬁt with the EM algorithm shows improvement over a single PCFG, both in parsing accuracy and in test data likelihood. We argue that this improvement comes from the learning of specialized grammars that capture non-local correlations. 
We present an improved approach for learning dependency parsers from treebank data. Our technique is based on two ideas for improving large margin training in the context of dependency parsing. First, we incorporate local constraints that enforce the correctness of each individual link, rather than just scoring the global parse tree. Second, to cope with sparse data, we smooth the lexical parameters according to their underlying word similarities using Laplacian Regularization. To demonstrate the beneﬁts of our approach, we consider the problem of parsing Chinese treebank data using only lexical features, that is, without part-of-speech tags or grammatical categories. We achieve state of the art performance, improving upon current large margin approaches. 
We explore a novel computational approach to identifying “constructions” or “multi-word expressions” (MWEs) in an annotated corpus. In this approach, MWEs have no special status, but emerge in a general procedure for ﬁnding the best statistical grammar to describe the training corpus. The statistical grammar formalism used is that of stochastic tree substitution grammars (STSGs), such as used in Data-Oriented Parsing. We present an algorithm for calculating the expected frequencies of arbitrary subtrees given the parameters of an STSG, and a method for estimating the parameters of an STSG given observed frequencies in a tree bank. We report quantitative results on the ATIS corpus of phrase-structure annotated sentences, and give examples of the MWEs extracted from this corpus. 
We demonstrate an original and successful approach for both resolving and generating deﬁnite anaphora. We propose and evaluate unsupervised models for extracting hypernym relations by mining cooccurrence data of deﬁnite NPs and potential antecedents in an unlabeled corpus. The algorithm outperforms a standard WordNet-based approach to resolving and generating deﬁnite anaphora. It also substantially outperforms recent related work using pattern-based extraction of such hypernym relations for coreference resolution. 
This paper investigates an isolated setting of the lexical substitution task of replacing words with their synonyms. In particular, we examine this problem in the setting of subtitle generation and evaluate state of the art scoring methods that predict the validity of a given substitution. The paper evaluates two context independent models and two contextual models. The major ﬁndings suggest that distributional similarity provides a useful complementary estimate for the likelihood that two Wordnet synonyms are indeed substitutable, while proper modeling of contextual constraints is still a challenging task for future research. 
We present a method for recognizing semantic role arguments using a kernel on weighted marked ordered labeled trees (the WMOLT kernel). We extend the kernels on marked ordered labeled trees (Kazama and Torisawa, 2005) so that the mark can be weighted according to its importance. We improve the accuracy by giving more weights on subtrees that contain the predicate and the argument nodes with this ability. Although Kazama and Torisawa (2005) presented fast training with tree kernels, the slow classiﬁcation during runtime remained to be solved. In this paper, we give a solution that uses an efﬁcient DP updating procedure applicable in argument recognition. We demonstrate that the WMOLT kernel improves the accuracy, and our speed-up method makes the recognition more than 40 times faster than the naive classiﬁcation. 
Recent work on Semantic Role Labeling (SRL) has shown that to achieve high accuracy a joint inference on the whole predicate argument structure should be applied. In this paper, we used syntactic subtrees that span potential argument structures of the target predicate in tree kernel functions. This allows Support Vector Machines to discern between correct and incorrect predicate structures and to re-rank them based on the joint probability of their arguments. Experiments on the PropBank data show that both classiﬁcation and re-ranking based on tree kernels can improve SRL systems. 
This paper investigates whether human associations to verbs as collected in a web experiment can help us to identify salient verb features for semantic verb classes. Assuming that the associations model aspects of verb meaning, we apply a clustering to the verbs, as based on the associations, and validate the resulting verb classes against standard approaches to semantic verb classes, i.e. GermaNet and FrameNet. Then, various clusterings of the same verbs are performed on the basis of standard corpus-based types, and evaluated against the association-based clustering as well as GermaNet and FrameNet classes. We hypothesise that the corpusbased clusterings are better if the instantiations of the feature types show more overlap with the verb associations, and that the associations therefore help to identify salient feature types. 
This paper presents a new application of the recently proposed machine learning method Alternating Structure Optimization (ASO), to word sense disambiguation (WSD). Given a set of WSD problems and their respective labeled examples, we seek to improve overall performance on that set by using all the labeled examples (irrespective of target words) for the entire set in learning a disambiguator for each individual problem. Thus, in effect, on each individual problem (e.g., disambiguation of “art”) we beneﬁt from training examples for other problems (e.g., disambiguation of “bar”, “canal”, and so forth). We empirically study the effective use of ASO for this purpose in the multitask and semi-supervised learning conﬁgurations. Our performance results rival or exceed those of the previous best systems on several Senseval lexical sample task data sets. 
We propose a generalization of the supervised DOP model to unsupervised learning. This new model, which we call U-DOP, initially assigns all possible unlabeled binary trees to a set of sentences and next uses all subtrees from (a large subset of) these binary trees to compute the most probable parse trees. We show how U-DOP can be implemented by a PCFG-reduction technique and report competitive results on English (WSJ), German (NEGRA) and Chinese (CTB) data. To the best of our knowledge, this is the first paper which accurately bootstraps structure for Wall Street Journal sentences up to 40 words obtaining roughly the same accuracy as a binarized supervised PCFG. We show that previous approaches to unsupervised parsing have shortcomings in that they either constrain the lexical or the structural context, or both. 
We investigate the appropriateness of using a technique based on support vector machines for identifying thematic structure of text streams. The thematic segmentation task is modeled as a binaryclassiﬁcation problem, where the different classes correspond to the presence or the absence of a thematic boundary. Experiments are conducted with this approach by using features based on word distributions through text. We provide empirical evidence that our approach is robust, by showing good performance on three different data sets. In particular, substantial improvement is obtained over previously published results of worddistribution based systems when evaluation is done on a corpus of recorded and transcribed multi-party dialogs. 
In this paper we investigate a new problem of identifying the perspective from which a document is written. By perspective we mean a point of view, for example, from the perspective of Democrats or Republicans. Can computers learn to identify the perspective of a document? Not every sentence is written strongly from a perspective. Can computers learn to identify which sentences strongly convey a particular perspective? We develop statistical models to capture how perspectives are expressed at the document and sentence levels, and evaluate the proposed models on articles about the Israeli-Palestinian conﬂict. The results show that the proposed models successfully learn how perspectives are reﬂected in word usage and can identify the perspective of a document with high accuracy. 
Distributional approaches to grammar induction are typically inefﬁcient, enumerating large numbers of candidate constituents. In this paper, we describe a simpliﬁed model of distributional analysis which uses heuristics to reduce the number of candidate constituents under consideration. We apply this model to a large corpus of over 400000 words of written English, and evaluate the results using EVALB. We show that the performance of this approach is limited, providing a detailed analysis of learned structure and a comparison with actual constituentcontext distributions. This motivates a more structured approach, using a process of attachment to form constituents from their distributional components. Our ﬁndings suggest that distributional methods do not generalize enough to learn syntax effectively from raw text, but that attachment methods are more successful. 
We present a simple context-free grammatical inference algorithm, and prove that it is capable of learning an interesting subclass of context-free languages. We also demonstrate that an implementation of this algorithm is capable of learning auxiliary fronting in polar interrogatives (AFIPI) in English. This has been one of the most important test cases in language acquisition over the last few decades. We demonstrate that learning can proceed even in the complete absence of examples of particular constructions, and thus that debates about the frequency of occurrence of such constructions are irrelevant. We discuss the implications of this on the type of innate learning biases that must be hypothesized to explain ﬁrst language acquisition. 
Much work on information extraction has successfully used gazetteers to recognise uncommon entities that cannot be reliably identiﬁed from local context alone. Approaches to such tasks often involve the use of maximum entropy-style models, where gazetteers usually appear as highly informative features in the model. Although such features can improve model accuracy, they can also introduce hidden negative effects. In this paper we describe and analyse these effects and suggest ways in which they may be overcome. In particular, we show that by quarantining gazetteer features and training them in a separate model, then decoding using a logarithmic opinion pool (Smith et al., 2005), we may achieve much higher accuracy. Finally, we suggest ways in which other features with gazetteer feature-like behaviour may be identiﬁed. 
We present a novel context pattern induction method for information extraction, speciﬁcally named entity extraction. Using this method, we extended several classes of seed entity lists into much larger high-precision lists. Using token membership in these extended lists as additional features, we improved the accuracy of a conditional random ﬁeld-based named entity tagger. In contrast, features derived from the seed lists decreased extractor accuracy. 
M. Civit, Ma A. Mart´ı, B. Navarro, N. Buﬁ, B. Ferna´ndez, and R. Marcos. 2003. Issues in the syntactic annotation of Cast3LB. In Proc. of the 4th Intern. Workshop on Linguistically Interpreteted Corpora (LINC). 
Erik Tjong Kim Sang  ILK / Computational Linguistics and AI  Informatics Institute  Tilburg University, P.O. Box 90153,  University of Amsterdam, Kruislaan 403  NL-5000 LE Tilburg, The Netherlands  NL-1098 SJ Amsterdam, The Netherlands  {S.V.M.Canisius,A.M.Bogers,  erikt@science.uva.nl  Antal.vdnBosch,J.Geertzen}@uvt.nl  
We present a new machine learning framework for multi-lingual dependency parsing. The framework uses a linear, pipeline based, bottom-up parsing algorithm, with a look ahead local search that serves to make the local predictions more robust. As shown, the performance of the ﬁrst generation of this algorithm is promising. 
In this paper, we present a framework for multi-lingual dependency parsing. Our bottom-up deterministic parser adopts Nivre’s algorithm (Nivre, 2004) with a preprocessor. Support Vector Machines (SVMs) are utilized to determine the word dependency attachments. Then, a maximum entropy method (MaxEnt) is used for determining the label of the dependency relation. To improve the performance of the parser, we construct a tagger based on SVMs to find neighboring attachment as a preprocessor. Experimental evaluation shows that the proposed extension improves the parsing accuracy of our base parser in 9 languages. (Hajič et al., 2004; Simov et al., 2005; Simov and Osenova, 2003; Chen et al., 2003; Böhmová et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; Džeroski et al., 2006; Civit and Martí, 2002; Nilsson et al., 2005; Oflazer et al., 2003; Atalay et al., 2003). 
 2 Parsing Methodology  We use SVM classiﬁers to predict the next action of a deterministic parser that builds labeled projective dependency graphs in an incremental fashion. Non-projective dependencies are captured indirectly by projectivizing the training data for the classiﬁers and applying an inverse transformation to the output of the parser. We present evaluation results and an error analysis focusing on Swedish and Turkish. 
Our approach to dependency parsing is based on the linear model of McDonald et al.(McDonald et al., 2005b). Instead of solving the linear model using the Maximum Spanning Tree algorithm we propose an incremental Integer Linear Programming formulation of the problem that allows us to enforce linguistic constraints. Our results show only marginal improvements over the non-constrained parser. In addition to the fact that many parses did not violate any constraints in the ﬁrst place this can be attributed to three reasons: 1) the next best solution that fulﬁls the constraints yields equal or less accuracy, 2) noisy POS tags and 3) occasionally our inference algorithm was too slow and decoding timed out. 
Unlexicalized probabilistic context-free parsing is a general and ﬂexible approach that sometimes reaches competitive results in multilingual dependency parsing even if a minimum of language-speciﬁc information is supplied. Furthermore, integrating parser results (good at long dependencies) and tagger results (good at short range dependencies, and more easily adaptable to treebank peculiarities) gives competitive results in all languages. 
In this paper, we propose a three-step multilingual dependency parser, which generalizes an efficient parsing algorithm at first phase, a root parser and postprocessor at the second and third stages. The main focus of our work is to provide an efficient parser that is practical to use with combining only lexical and part-ofspeech features toward language independent parsing. The experimental results show that our method outperforms Maltparser in 13 languages. We expect that such an efficient model is applicable for most languages. 
This paper presents an approach to dependency parsing which can utilize any standard machine learning (classiﬁcation) algorithm. A decision list learner was used in this work. The training data provided in the form of a treebank is converted to a format in which each instance represents information about one word pair, and the classiﬁcation indicates the existence, direction, and type of the link between the words of the pair. Several distinct models are built to identify the links between word pairs at different distances. These models are applied sequentially to give the dependency parse of a sentence, favoring shorter links. An analysis of the errors, attribute selection, and comparison of different languages is presented. 
We analyze four different types of document networks with respect to their small world characteristics. These characteristics allow distinguishing wiki-based systems from citation and more traditional text-based networks augmented by hyperlinks. The study provides evidence that a more appropriate network model is needed which better reﬂects the speciﬁcs of wiki systems. It puts emphasize on their topological differences as a result of wikirelated linking compared to other textbased networks. 
This discussion document concerns the challenges to assessments of reliability posed by wikis and the potential for language processing techniques for aiding readers to decide whether to trust particular text. 
Preliminary surveys show that the lan­ guage of blogs is not restricted to the more informal levels of expression. In­ stead blogs may include many kinds of written language: from simple personal notes to literary prose or poetry. The pa­ per presents a sample of Italian blogs and comments on the results of the search of literary forms in two Web corpora using search engine queries. 
This paper is a presentation of a doctoral research in progress focused on a new genre: online encyclopaedias. The introduction to Wikipedia and Encyclopaedia Britannica Online will be followed by a presentation of wiki as a new textual genre. Wikipedia analysis will focus firstly on the investigation of the “WikiLanguage”, the language used in official encyclopaedic articles. Secondly, the “WikiSpeak”, the spoken-written language used by Wikipedians in their backstage and informal community, will be taken into account. The initial findings of this research seem to suggest that, the language of the Wikipedia’s coauthored articles is formal and standardized in a way similar to that found in Encyclopaedia Britannica Online. By contrast, the WikiSpeak, as a new variety of NetSpeak Jargon, can be considered as a creative domain, an independent and individual expression of linguistic freedom of selfrepresentation, characterizing the wiki Computer Mediated Discourse Community. 1. Introduction The encyclopaedia's structure, either hierarchical or alphabetically ordered, with its evolving nature is particularly adaptable to a disk-based or online format. All major printed encyclopaedias have moved to this method of delivery. Online E-ncyclopedias can include multimedia (such as video, sound clips and animated illustrations) unavailable in the printed format. They can make use of hypertext crossreferences between conceptually related items and, furthermore, they offer the additional advantage of being dynamic: new and frequently updated information can be presented almost  immediately, rather than waiting for the next release of a static format (as with a paper or disk publication). This research is based particularly on a contrastive linguistic analysis of Wikipedia and Encyclopaedia Britannica Online. The latter is considered one of the greatest examples of general encyclopaedias in the English speaking world. It contains 120,000 articles which are commonly considered accurate, reliable and well-written. Brief article summaries can be viewed for free on the net, while the full text is available only for individuals with monthly or yearly subscription. On the other hand, Wikipedia is a collaborative authoring project on the web, a repository of encyclopaedic knowledge, an example of a collaborative hypermedium focused on a common project. It is one of the most popular reference websites receiving around 50 million hits per day. It is a social edemocracy environment, designed with the goal of creating a free encyclopedia containing information on all subjects written collaboratively by volunteers. At the time of writing this paper the project has produced over two and half million articles and has been officially recognized as the largest international online community. It consists of 200 independent language editions and the English version is the biggest one with more than 962,995 articles (up to January 2006). 2. Wiki as new textual genre With reference to the extensive empirical studies of Susan Herring on CMC, wikis and blogs considered as spaces belonging to the second web generation, can be regarded as adding new peculiarities to the existing synchronous and asynchronous tools of the first CMC generation (such as e-mail, mailing list, forum and chat). It is well known in media studies that “the medium is the message” as McLuhan (1964) pointed out in the sixties, and in fact the medium adds unique properties to  16  the web genre in terms of production, function, and reception which cannot be ignored. Wikis are co-authoring tools which allow collective collaboration. They can be, simultaneously, a repository of information and an asynchronous tool of communication and discussion across the web (see Wikipedia). All wikis have integrated search engines for locating content and are open to anyone since they are considered a public space, even though they can be protected against unauthentic users. Their main aim is to create documents. Wikis, unlike traditionally designed web sites, encourage “topical writing” by using wiki links and creating a wide network of interconnected pages. The interlinking process becomes simpler to type by just putting the word(s) in square brackets. It simultaneously creates a new topic title (a WikiWord), a new writing space for that topic and a link to that space. Once created, a topic will be available anywhere on the wiki as whenever the WikiWord is typed, it will link to the writing space of that topic (Morgan, 2006). The writer, the supreme authority in print, is considered the one who transmits content through paper pages, to passive readers, whose role is merely to decode and interpret their message. The electronic writing space, being hypertextual and extremely flexible, changes the landscape. Writers can create multiple structures from the same topics (hierarchy, web, spiral, etc.) and readers can enter, browse and leave text at many points. In the hypertext, the author creates different paths for the reader, although there is neither a canonical path nor a defined page order to follow. The new active readers making their choices, become coauthors of the hypertext (Bolter, 1991). This idea is more pronounced on a wiki than elsewhere, because in an open wiki the reader can (if allowed) really interrupt the process, rewriting, changing, erasing and modifying the original text or creating new topics. Traditional writing creates a gap between writer and reader. Wiki technology mediates the gap because the two actors assume interchangeable roles in this new open eenvironment. To conclude, wiki text is never static as it is considered revisable, a-temporal as nodes continually change through the collaborative writing process, creating a never ending evolving network of topics. Thus, knowledge becomes webbed, contextualized though it remains temporary as it can always  be changed or vandalized. Luckily, the original version can always, and easily, be recovered by SysOps1, through page histories2 (Morgan, 2006). Wikis offer two different writing modes. The first one is known as “document mode”. When it is used, contributors create documents collaboratively and can leave their additions to articles. Multiple authors can edit and update the content of documents which gradually become representations of contributors’ shared knowledge (Leuf and Cunningham, 2001). Wikis have two states, “Read” and “Edit”. “Read state” is by default. In this case, wiki pages look just like normal webpages. When the user wants to edit a page, he/she must only access the “edit state”. “Document mode” is expository, extensive, monological, formal, refined and less creative than “thread mode”. It is in third person and unsigned. “Document mode” demonstrates that knowledge is collective and that the ideas, not the writers, are the main focus. Writers contribute to “document mode” refactoring, reorganizing, incorporating and synthesizing “thread mode” comments in encyclopaedic articles and changing the first to third person (Morgan, 2006). The second wiki writing mode is “thread mode”. Contributors carry out discussions by posting signed messages in the discussion page connected to the main article. Others reply to the original message and so a group of threaded messages evolves (Morgan, 2006). “Thread mode” is dialogical, open, collective, dynamic and informal. It develops organically, without a predictive structure. It expresses public thinking, presents multiple positions and is exploratory. Entries are phrased in first person and are signed. Rather then replying to a discussion entry, the writer can refactor the page to incorporate suggestions made, then delete the comment. “Thread mode” demonstrates that knowledge is the result of constructivist collaboration and not a lonely production. 
We present results of our experiments with the application of machine learning on binary blog classification, i.e. determining whether a given web page is a blog page. We have gathered a corpus in excess of half a million blog or blog-like pages and pre-classified them using a simple baseline. We investigate which algorithms attain the best results for our classification problem and experiment with resampling techniques, with the aim of utilising our large dataset to improve upon our baseline. We show that the application of off-the-shelf machine learning technology to perform binary blog classification offers substantial improvement over our baseline. Further gains can sometimes be achieved using resampling techniques, but these improvements are relatively small compared to the initial gain. 
The study presented in this paper explores the current state of genre evolution on the web through web users’ perception. More precisely, it explores the perception of genres when users are faced not only with prototypical genre exemplars but also with hybrid or individualized web pages, and interpret the subjects’ perception in term of genre evolution Although this exploration is partial (23 labels to be assigned to 25 web pages), it offers an interesting section of the genre repertoire on the web. This study can be also seen as a confirmatory study, because it confirms that a number of recent web genres, unprecedented in the paper world (such as home page, FAQs, and blog) can be recognized by the subjects; others have not fully emerged and many web users are not familiar with their new genre labels; finally some web pages show a high level of ambiguity and web users largely disagree on assigning labels to them. 
In this paper we discuss the notions of hypertext, blog, wiki and cognitive mapping in order to ﬁnd a solution to the main problems of processing text data stored in these forms. We propose the structure and architecture of Novelle as a new environment to compose texts. Its ﬂexible model allows the collaboration for contents and a detailed description of ownership. Data are stored in a XML repository, so as to use the capabilities of this language. To develop quickly and efﬁciently we choose AJAX technology over the Ruby on Rails framework. 
The problem in processing Chinese chat text originates from the anomalous characteristics and dynamic nature of such a text genre. That is, it uses ill-edited terms and anomalous writing styles in chat text, and the anomaly is created and discarded very quickly. To handle this problem, one solution is to re-train the recognizer periodically. This costs a lot of manpower in producing the timely chat text corpus. The new approaches are proposed in this paper to detect the anomaly within dynamic Chinese chat text by incorporating standard Chinese corpora and chat corpus. We first model standard language text using standard Chinese corpora and apply these models to detect anomalous chat text. To improve detection quality, we construct anomalous chat language model using one static chat text corpus and incorporate this model into the standard language models. Our approaches calculate confidence and entropy for the input text and apply threshold values to help make the decisions. The experiments prove that performance equivalent to the best ones produced by the approaches in existence can be achieved stably with our approaches. 
This paper describes a method to automatically create and maintain gazetteers for Named Entity Recognition (NER). This method extracts the necessary information from linguistic resources. Our approach is based on the analysis of an on-line encyclopedia entries by using a noun hierarchy and optionally a PoS tagger. An important motivation is to reach a high level of language independence. This restricts the techniques that can be used but makes the method useful for languages with few resources. The evaluation carried out proves that this approach can be successfully used to build NER gazetteers for location (F 78%) and person (F 68%) categories. 
We investigate whether the Wikipedia corpus is amenable to multilingual analysis that aims at generating parallel corpora. We present the results of the application of two simple heuristics for the identiﬁcation of similar text across multiple languages in Wikipedia. Despite the simplicity of the methods, evaluation carried out on a sample of Wikipedia pages shows encouraging results. 
This paper presents a proposal for iCLEF 2006, the interactive track of the CLEF cross-language evaluation campaign. In the past, iCLEF has addressed applications such as information retrieval and question answering. However, for 2006 the focus has turned to text-based image retrieval from Flickr. We describe Flickr, the challenges this kind of collection presents to cross-language researchers, and suggest initial iCLEF tasks. 
This paper describes our approach to representing and querying multi-dimensional, possibly overlapping text annotations, as used in our question answering (QA) system. We use a system extending XQuery, the W3C-standard XML query language, with new axes that allow one to jump easily between different annotations of the same data. The new axes are formulated in terms of (partial) overlap and containment. All annotations are made using stand-off XML in a single document, which can be efﬁciently queried using the XQuery extension. The system is scalable to gigabytes of XML annotations. We show examples of the system in QA scenarios. 
Publishers of biomedical journals increasingly use XML as the underlying document format. We present a modular text-processing pipeline that inserts XML markup into such documents in every processing step, leading to multi-dimensional markup. The markup introduced is used to identify and disambiguate named entities of several semantic types (protein/gene, Gene Ontology terms, drugs and species) and to communicate data from one module to the next. Each module independently adds, changes or removes markup, which allows for modularization and a flexible setup of the processing pipeline. We also describe how the cascaded approach is embedded in a large-scale XML-based application (EBIMed) used for on-line access to biomedical literature. We discuss the lessons learnt so far, as well as the open problems that need to be resolved. In particular, we argue that the pragmatic and tailored solutions allow for reduction in the need for overlapping annotations — although not completely without cost. 
In this paper we discuss technical issues arising from the interdependence between tokenisation and XML-based annotation tools, in particular those which use standoff annotation in the form of pointers to word tokens. It is common practice for an XML-based annotation tool to use word tokens as the target units for annotating such things as named entities because it provides appropriate units for stand-off annotation. Furthermore, these units can be easily selected, swept out or snapped to by the annotators and certain classes of annotation mistakes can be prevented by building a tool that does not permit selection of a substring which does not entirely span one or more XML elements. There is a downside to this method of annotation, however, in that it assumes that for any given data set, in whatever domain, the optimal tokenisation is known before any annotation is performed. If mistakes are made in the initial tokenisation and the word boundaries conﬂict with the annotators’ desired actions, then either the annotation is inaccurate or expensive retokenisation and reannotation will be required. Here we describe the methods we have developed to address this problem. We also describe experiments which explore the effects of different granularities of tokenisation on NER tagger performance. 
The NITE Query Language (NQL) has been used successfully for analysis of a number of heavily cross-annotated data sets, and users especially value its elegance and ﬂexibility. However, when using the current implementation, many of the more complicated queries that users have formulated must be run in batch mode. For a re-implementation, we require the query processor to be capable of handling large amounts of data at once, and work quickly enough for on-line data analysis even when used on complete corpora. Early results suggest that the most promising implementation strategy is one that involves the use of XQuery on a multiple ﬁle data representation that uses the structure of individual XML ﬁles to mirror tree structures in the data, with redundancy where a data node has multiple parents in the underlying data object model. 
This paper presents the compilation of the CroCo Corpus, an English-German translation corpus. Corpus design, annotation and alignment are described in detail. In order to guarantee the searchability and exchangeability of the corpus, XML stand-off mark-up is used as representation format for the multi-layer annotation. On this basis it is shown how the corpus can be queried using XQuery. Furthermore, the generalisation of results in terms of linguistic and translational research questions is briefly discussed. 
XML documents annotated by diﬀerent NLP tools accommodate multidimensional markup in a single hierarchy. To query such documents one has to account for diﬀerent possible nesting structures of the annotations and the original markup of a document. We propose an expressive pattern language with extended semantics of the sequence pattern, supporting negation, permutation and regular patterns that is especially appropriate for querying XML annotated documents with multi-dimensional markup. The concept of fuzzy matching allows matching of sequences that contain textual fragments and known XML elements independently of how concurrent annotations and original markup are merged. We extend the usual notion of sequence as a sequence of siblings allowing matching of sequence elements on the diﬀerent levels of nesting and abstract so from the hierarchy of the XML document. Extended sequence semantics in combination with other language patterns allows more powerful and expressive queries than queries based on regular patterns. 
We describe the way we adapted a text analysis tool for annotating with the Linguistic Description Scheme of MPEG-7 text related to and extracted from multimedia content. Practically applied in the DIRECT-INFO EC R&D project we show how such linguistic annotation contributes to semantic annotation of multimodal analysis systems, demonstrating also the use of the XML schema of MPEG-7 for supporting cross-media semantic content annotation. 
We present ANNIS, a linguistic database that aims at facilitating the process of exploiting richly annotated language data by naive users. We describe the role of the database in our research project and the project requirements, with a special focus on aspects of multilevel annotation. We then illustrate the usability of the database by illustrative examples. We also address current challenges and next steps. 
The NITE XML Toolkit (NXT) is open source software for working with multimodal, spoken, or text language corpora. It is speciﬁcally designed to support the tasks of human annotators and analysts of heavily cross-annotated data sets, and has been used successfully on a range of projects with varying needs. In this text to accompany a demonstration, we describe NXT along with four uses on different corpora that together span its most novel features. The examples involve the AMI and ICSI Meeting Corpora; a study of multimodal reference; a syntactic analysis of Genesis in classical Hebrew; and discourse annotation of Switchboard dialogues. 
We demonstrate work in progress1 using the Nite XML Toolkit on a corpus of multimodal dialogues with an MP3 player collected in a Wizard-of-Oz (WOZ) experiments and annotated with a rich feature set at several layers. We designed an NXT data model, converted experiment log ﬁle data and manual transcriptions into NXT, and are building annotation tools using NXT libraries. 
We present an XML annotation format (MEANING Annotation Format, MAF) specifically designed to represent and integrate different levels of linguistic annotations and a tool that provides flexible access to them (MEANING Browser). We describe our experience in integrating linguistic annotations coming from different sources, and the solutions we adopted to implement efficient access to corpora annotated with the Meaning Format. 
We present the Heart of Gold middleware by demonstrating three XMLbased integration scenarios where multidimensional markup produced online by multilingual natural language processing (NLP) components is combined to deliver rich, robust linguistic markup for use in NLP-based applications like information extraction, question answering and semantic web. The scenarios include (1) robust deep-shallow integration, (2) shallow processing cascades, and (3) treebank storage of multi-dimensionally annotated texts. 
The paper discusses two topics: ﬁrstly an approach of using multiple layers of annotation is sketched out. Regarding the XML representation this approach is similar to standoff annotation. A second topic is the use of heterogeneous linguistic resources (e.g., XML annotated documents, taggers, lexical nets) as a source for semiautomatic multi-dimensional markup to resolve typical linguistic issues, dealing with anaphora resolution as a case study.1 
The American National Corpus and its annotations are represented in a stand-off XML format compliant with the specifications of ISO TC37 SC4 WG1’s Linguistic Annotation Framework. Because few systems that enable search and access of the corpus currently support stand-off markup, the project has developed a SAX like parser that generates ANC data with annotations in-line, in a variety of output formats. 
This paper describes the usage of XML for representing cross-language phrase alignments in parallel treebanks. We have developed a TreeAligner as a tool for interactively inserting and correcting such alignments as an independent level of treebank annotation. 
We present a standoff annotation framework for the integration of NLP components, currently implemented in the context of the DELPH-IN tools1. This provides a ﬂexible standoff pointer scheme suitable for various types of data, a lattice encodes structural ambiguity, intraannotation relationships are encoded, and annotations are decorated with structured content. We provide an XML serialization for intercomponent communication. 
Recent literature on text-tagging reported successful results by applying Maximum Entropy (ME) models. In general, ME taggers rely on carefully selected binary features, which try to capture discriminant information from the training data. This paper introduces a standard setting of binary features, inspired by the literature on named-entity recognition and text chunking, and derives corresponding realvalued features based on smoothed logprobabilities. The resulting ME models have orders of magnitude fewer parameters. Effective use of training data to estimate features and parameters is achieved by integrating a leaving-one-out method into the standard ME training algorithm. Experimental results on two tagging tasks show statistically signiﬁcant performance gains after augmenting standard binaryfeature models with real-valued features. 
We present a new method for performing sequence labelling based on the idea of using a machine-learning classiﬁer to generate several possible output sequences, and then applying an inference procedure to select the best sequence among those. Most sequence labelling methods following a similar approach require the base classiﬁer to make probabilistic predictions. In contrast, our method can be used with virtually any type of classiﬁer. This is illustrated by implementing a sequence classiﬁer on top of a (nonprobabilistic) memory-based learner. In a series of experiments, this method is shown to outperform two other methods; one naive baseline approach, and another more sophisticated method. 
We propose a simple solution to the sequence labeling problem based on an extension of weighted decomposition kernels. We additionally introduce a multiinstance kernel approach for representing lexical word sense information. These new ideas have been preliminarily tested on named entity recognition and PP attachment disambiguation. We ﬁnally suggest how these techniques could be potentially merged using a declarative formalism that may provide a basis for the integration of multiple sources of information when using kernel-based learning in NLP. 
In this paper we present a multiclassiﬁer approach for multilabel document classiﬁcation problems, where a set of k-NN classiﬁers is used to predict the category of text documents based on different training subsampling databases. These databases are obtained from the original training database by random subsampling. In order to combine the predictions generated by the multiclassiﬁer, Bayesian voting is applied. Through all the classiﬁcation process, a reduced dimension vector representation obtained by Singular Value Decomposition (SVD) is used for training and testing documents. The good results of our experiments give an indication of the potentiality of the proposed approach. 
We report on our work to build a discourse parser (SemDP) that uses semantic features of sentences. We use an Inductive Logic Programming (ILP) System to exploit rich verb semantics of clauses to induce rules for discourse parsing. We demonstrate that ILP can be used to learn from highly structured natural language data and that the performance of a discourse parsing model that only uses semantic information is comparable to that of the state of the art syntactic discourse parsers. 
We investigate methods that add syntactically motivated features to a statistical machine translation system in a reranking framework. The goal is to analyze whether shallow parsing techniques help in identifying ungrammatical hypotheses. We show that improvements are possible by utilizing supertagging, lightweight dependency analysis, a link grammar parser and a maximum-entropy based chunk parser. Adding features to n-best lists and discriminatively training the system on a development set increases the BLEU score up to 0.7% on the test set. 
Recent work on the design of automatic systems for semantic role labeling has shown that feature engineering is a complex task from a modeling and implementation point of view. Tree kernels alleviate such complexity as kernel functions generate features automatically and require less software development for data extraction. In this paper, we study several tree kernel approaches for both boundary detection and argument classiﬁcation. The comparative experiments on Support Vector Machines with such kernels on the CoNLL 2005 dataset show that very simple tree manipulations trigger automatic feature engineering that highly improves accuracy and efﬁciency in both phases. Moreover, the use of different classiﬁers for internal and pre-terminal nodes maintains the same accuracy and highly improves efﬁciency. 
In this paper we present a family of kernel functions, named Syntagmatic Kernels, which can be used to model syntagmatic relations. Syntagmatic relations hold among words that are typically collocated in a sequential order, and thus they can be acquired by analyzing word sequences. In particular, Syntagmatic Kernels are deﬁned by applying a Word Sequence Kernel to the local contexts of the words to be analyzed. In addition, this approach allows us to deﬁne a semi supervised learning schema where external lexical knowledge is plugged into the supervised learning process. Lexical knowledge is acquired from both unlabeled data and hand-made lexical resources, such as WordNet. We evaluated the syntagmatic kernel on two standard Word Sense Disambiguation tasks (i.e. English and Italian lexical-sample tasks of Senseval-3), where the syntagmatic information plays a crucial role. We compared the Syntagmatic Kernel with the standard approach, showing promising improvements in performance. 
This paper describes an approach to learning concept deﬁnitions which operates on fully parsed text. A subcorpus of the Dutch version of Wikipedia was searched for sentences which have the syntactic properties of deﬁnitions. Next, we experimented with various text classiﬁcation techniques to distinguish actual deﬁnitions from other sentences. A maximum entropy classiﬁer which incorporates features referring to the position of the sentence in the document as well as various syntactic features, gives the best results. 
This paper describes experiments in using machine learning for relation disambiguation. There have been succesfuld experiments in combining machine learning and ontologies, or light-weight ontologies such as WordNet, for word sense disambiguation. However, what we are trying to do, is to disambiguate complex concepts consisting of two simpler concepts and the relation that holds between them. The motivation behind the approach is to expand existing methods for content based information retrieval. The experiments have been performed using an annotated extract of a corpus, consisting of prepositions surrounded by noun phrases, where the prepositions denote the relation we are trying disambiguate. The results show an unexploited opportunity of including prepositions and the relations they denote, e.g. in content based information retrieval. 
This article describes a robust semantic parser that uses a broad knowledge base created by interconnecting three major resources: FrameNet, VerbNet and PropBank. The FrameNet corpus contains the examples annotated with semantic roles whereas the VerbNet lexicon provides the knowledge about the syntactic behavior of the verbs. We connect VerbNet and FrameNet by mapping the FrameNet frames to the VerbNet Intersective Levin classes. The PropBank corpus, which is tightly connected to the VerbNet lexicon, is used to increase the verb coverage and also to test the effectiveness of our approach. The results indicate that our model is an interesting step towards the design of free-text semantic parsers. 
Names are important in many societies, even in technologically oriented ones which use ID systems or other ways to identify individual people. Names such as personal surnames are the most important as they are used in many processes, such as identifying of people, record linkage and for genealogical research as well. For Thai names this situation is a bit different in that in Thai the first names are most important. Even phone books and directories are sorted according to the first names. Here we present a system for constructing Thai names from basic syllables. Typically Thai names convey a meaning. For this we use an ontology of names to capture the meaning of the variants which are based on the Thai naming methodology and rules.  important referrer to a person even if there are numbering systems like ID numbers because such systems are not universal. Names are often queried in a different way than they were entered. Names represent complex lexical structures which have to be handled systematically for data entry, storage and retrieval in order to get sufficient recall or precision the retrieval process in. In this paper we present a first account of our findings on constructing Thai names with the help of an ontology of names as well as a working methodology for the naming process in Thai culture. This paper is organized as follows: Section 2 contains a description of names and their elements. In Section 3 we outline the concept of ontology of names. In Section 4 we present the construction process and system for Thai names. We apply this system together with an ontology to construct names with an appropriate meaning. Section 5 shows the conclusions of our study and further work which has to be performed.  
In this paper, we introduce a WordNetbased measure of semantic relatedness by combining the structure and content of WordNet with co–occurrence information derived from raw text. We use the co–occurrence information along with the WordNet deﬁnitions to build gloss vectors corresponding to each concept in WordNet. Numeric scores of relatedness are assigned to a pair of concepts by measuring the cosine of the angle between their respective gloss vectors. We show that this measure compares favorably to other measures with respect to human judgments of semantic relatedness, and that it performs well when used in a word sense disambiguation algorithm that relies on semantic relatedness. This measure is ﬂexible in that it can make comparisons between any two concepts without regard to their part of speech. In addition, it can be adapted to different domains, since any plain text corpus can be used to derive the co–occurrence information. 
As text data becomes plentiful, unsupervised methods for Word Sense Disambiguation (WSD) become more viable. A problem encountered in applying WSD methods is finding the exact number of senses an ambiguity has in a training corpus collected in an automated manner. That number is not known a priori; rather it needs to be determined based on the data itself. We address that problem using cluster stopping methods. Such techniques have not previously applied to WSD. We implement the methods of Calinski and Harabasz (1975) and Hartigan (1975) and our adaptation of the Gap statistic (Tibshirani, Walter and Hastie, 2001). For evaluation, we use the WSD Test Set from the National Library of Medicine, whose sense inventory is the Unified Medical Language System. The best accuracy for selecting the correct number of clusters is 0.60 with the C&H method. Our error analysis shows that the cluster stopping methods make finergrained sense distinctions by creating additional clusters. The highest F-scores (82.89), indicative of the quality of cluster membership assignment, are comparable to the baseline majority sense (82.63) and point to a path towards accuracy improvement via additional cluster pruning. The importance and significance of the current work is in applying cluster stopping rules to WSD. 
The granularity of word senses in current general purpose sense inventories is often too ﬁne-grained, with narrow sense distinctions that are irrelevant for many NLP applications. This has particularly been a problem with WordNet which is widely used for word sense disambiguation (WSD). There have been several attempts to group WordNet senses given a number of different information sources in order to reduce granularity. We propose relating senses as a matter of degree to permit a softer notion of relationships between senses compared to ﬁxed groupings so that granularity can be varied according to the needs of the application. We compare two such approaches with a gold-standard produced by humans for this work. We also contrast this goldstandard and another used in previous research with the automatic methods for relating senses for use with back-off methods for WSD. 
The correct identiﬁcation of metonymies is not normally a problem for most people. For computers, things are different, however. In Natural Language Processing, metonymy recognition is therefore usually addressed with complex algorithms that rely on hundreds of labelled training examples. This paper investigates two approaches to metonymy recognition that dispense with this complexity, albeit in different ways. The ﬁrst, an unsupervised approach to Word Sense Discrimination, does not require any labelled training instances. The second, Memory-Based Learning, replaces the complexity of current algorithms by a ‘lazy’ learning phase. While the ﬁrst approach is often able to identify a metonymical and a literal cluster in the data, it is the second in particular that produces state-of-the-art results. 
Although it is generally agreed that Word Sense Disambiguation (WSD) is an application dependent task, the great majority of the efforts has aimed at the development of WSD systems without considering their application. We argue that this strategy is not appropriate, since some aspects, such as the sense repository and the disambiguation process itself, vary according to the application. Taking Machine Translation (MT) as application and focusing on the sense repository, we present evidence for this argument by examining WSD in English-Portuguese MT of eight sample verbs. By showing that the traditional monolingual WSD strategies are not suitable for multilingual applications, we intend to motivate the development of WSD methods for particular applications. 
This paper presents an analysis of semantic association norms for German nouns. In contrast to prior studies, we not only collected associations elicited by written representations of target objects but also by their pictorial representations. In a ﬁrst analysis, we identiﬁed systematic differences in the type and distribution of associate responses for the two presentation forms. In a second analysis, we applied a soft cluster analysis to the collected targetresponse pairs. We subsequently used the clustering to predict noun ambiguity and to discriminate senses in our target nouns. 
In this paper we present a system for translating named entities from Basque to Spanish based on comparable corpora. For that purpose we have tried two approaches: one based on Basque linguistic features, and a language-independent tool. For both tools we have used BasqueSpanish comparable corpora, a bilingual dictionary and the web as resources. 
This paper studies a strategy for identifying and using multi-word expressions in Statistical Machine Translation. The performance of the proposed strategy for various types of multi-word expressions (like nouns or verbs) is evaluated in terms of alignment quality as well as translation accuracy. Evaluations are performed by using real-life data, namely the European Parliament corpus. Results from translation tasks from English-to-Spanish and from Spanish-to-English are presented and discussed. 
In this paper, we report on our experiment to extract Chinese multiword expressions from corpus resources as part of a larger research effort to improve a machine translation (MT) system. For existing MT systems, the issue of multiword expression (MWE) identification and accurate interpretation from source to target language remains an unsolved problem. Our initial test on the Chineseto-English translation functions of Systran and CCID’s Huan-Yu-Tong MT systems reveal that, where MWEs are involved, MT tools suffer in terms of both comprehensibility and adequacy of the translated texts. For MT systems to become of further practical use, they need to be enhanced with MWE processing capability. As part of our study towards this goal, we test and evaluate a statistical tool, which was developed for English, for identifying and extracting Chinese MWEs. In our evaluation, the tool achieved precisions ranging from 61.16% to 93.96% for different types of MWEs. Such results demonstrate that it is feasible to automatically identify many Chinese MWEs using our tool, although it needs further improvement. 
The Japanese language has various types of compound functional expressions, which are very important for recognizing the syntactic structures of Japanese sentences and for understanding their semantic contents. In this paper, we formalize the task of identifying Japanese compound functional expressions in a text as a chunking problem. We apply a machine learning technique to this task, where we employ that of Support Vector Machines (SVMs). We show that the proposed method signiﬁcantly outperforms existing Japanese text processing tools. 
For NLP applications that require some sort of semantic interpretation it would be helpful to know what expressions exhibit an idiomatic meaning and what expressions exhibit a literal meaning. We investigate whether automatic word-alignment in existing parallel corpora facilitates the classiﬁcation of candidate expressions along a continuum ranging from literal and transparent expressions to idiomatic and opaque expressions. Our method relies on two criteria: (i) meaning predictability that is measured as semantic entropy and (ii), the overlap between the meaning of an expression and the meaning of its component words. We approximate the mentioned overlap as the proportion of default alignments. We obtain a signiﬁcant improvement over the baseline with both measures. 
This paper provides a speciﬁcation of requirements for collocation extraction systems, taking as an example the extraction of noun + verb collocations from German texts. A hybrid approach to the extraction of habitual collocations and idioms is presented, aiming at a detailed description of collocations and their morphosyntax for natural language generation systems as well as to support learner lexicography. 
Light verb constructions (LVCs), such as “make a call” in English, can be said to be complex predicates in which the verb plays only a functional role. LVCs pose challenges for natural language understanding, as their semantics differ from usual predicate structures. We extend the existing corpus-based measures for identifying LVCs between verb-object pairs in English, by proposing using new features that use mutual information and assess other syntactic properties. Our work also incorporates both existing and new LVC features into a machine learning approach. We experimentally show that using the proposed framework incorporating all features outperforms previous work by 17%. As machine learning techniques model the trends found in training data, we believe the proposed LVC detection framework and statistical features is easily extendable to other languages. 
This paper describes automatic treatment of multi-word expressions in a morphologically complex flective language – Estonian. It focuses on a special type of multi-word expressions – the verbal multi-word expressions that can function as predicates. Authors describe two language resources – a database of verbal multi-word expressions and a corpus where these items have been annotated manually. The analysis of the annotated corpus demonstrates that the Estonian verbal multi-word expressions alternate in several grammatical categories. Different types of the verbal multi-word expressions (opaque and transparent idioms, support verb constructions and collocations) behave differently in the corpus with regard to the freedom of alternation. The paper describes main types of these alternations and the methods for dealing with them automatically.  disambiguation, syntactic analysis, machine translation etc.), we need to devise algorithms to actually use them. This in turn requires knowledge about the behavior of VMWEs in real texts. Estonian language belongs to the Finnic group of the Finno-Ugric language family. Typologically Estonian is an agglutinating language but more fusional and analytic than the languages belonging to the northern branch of the Finnic languages. The word order is relatively free. One can find a detailed description of the grammatical system of Estonian in (Erelt 2003). In this paper we will focus on a special type of Estonian multi-word expressions, namely those that can function as a predicate in a clause. This paper is organized as follows. In section 2 we give a brief overview of the VMWEs in Estonian. Section 3 describes the database of the VMWEs and the corpus, where the VMWEs have been manually annotated. Here we will also present the statistics of the VMWEs in the corpus. In section 4 we discuss the variability of these expressions as registered in our corpus and the consequences of these variations for the automatic treatment of the VMWEs. And finally we will make our conclusions in section 5.  
This paper discusses an approach to modeling monolingual and bilingual dictionaries in the description logic species of the OWL Web Ontology Language (OWL DL). The central idea is that the model of a bilingual dictionary is a combination of the models of two monolingual dictionaries, in addition to an abstract translation model. The paper addresses the advantages of using OWL DL for the design of monolingual and bilingual dictionaries and proposes a generalized architecture for that purpose. Moreover, mechanisms for querying and checking the consistency of such models are presented, and it is concluded that DL provides means which are capable of adequately meeting the requirements on the design of multilingual dictionaries. 
Multiword units significantly contribute to the robustness of MT systems as they reduce the inevitable ambiguity inherent in word to word matching. The paper focuses on a relatively little studied kind of MW units which are partially fixed and partially productive. In fact, MW units will be shown to form a continuum between completely frozen expression where the lexical elements are specified at the level of particular word forms and those which are produced by syntactic rules defined in terms of general part of speech categories. The paper will argue for the use of local grammars proposed by Maurice Gross to capture the productive regularity of MW units and will illustrate a uniform implementation of them in the NooJ grammar development framework. 
In this paper we will present an evaluation of current state-of-the-art algorithms for Anaphora Resolution based on a segment of Susanne corpus (itself a portion of Brown Corpus), a much more comparable text type to what is usually required at an international level for such application domains as Question/Answering, Information Extraction, Text Understanding, Language Learning. The portion of text chosen has an adequate size which lends itself to significant statistical measurements: it is portion A, counting 35,000 tokens and some 1000 third person pronominal expressions. The algorithms will then be compared to our system, GETARUNS, which incorporates an AR algorithm at the end of a pipeline of interconnected modules that instantiate standard architectures for NLP. Fmeasure values reached by our system are significantly higher (75%) than the other ones. 
In this paper, we extend an existing statistical parsing model to produce richer output parse trees, annotated with PropBank semantic role labels. Our results show that the model can be robustly extended to produce more complex output parse trees without any loss in performance and suggest that joint inference of syntactic and semantic representations is a viable alternative to approaches based on a pipeline of local processing steps. 
We present in this paper a parser relying on a constraint-based formalism called Property Grammar. We show how constraints constitute an efficient solution in parsing non canonical material such as spoken language transcription or e-mails. This technique, provided that it is implemented with some control mechanisms, is very efficient. Some results are presented, from the French parsing evaluation campaign EASy. 
Covering as many phenomena as possible is a traditional goal of parser development, but the broader a grammar is made, the blunter it may become, as rare constructions inﬂuence the behaviour on simple sentences that were already solved correctly. We observe the effects of intentionally removing support for speciﬁc constructions from a broad-coverage grammar of German. We show that accuracy of analysing sentences from the NEGRA corpus can be improved not only for sentences that do not need the extra coverage, but even when including those that do. 
We present a novel method to identify effective surface text patterns using an internet search engine. Precision is only one of the criteria to identify the most effective patterns among the candidates found. Another aspect is frequency of occurrence. Also, a pattern has to relate diverse instances if it expresses a non-functional relation. The learned surface text patterns are applied in an ontology population algorithm, which not only learns new instances of classes but also new instancepairs of relations. We present some £rst experiments with these methods. 
This paper describes SIE (Simple Information Extraction), a modular information extraction system designed with the goal of being easily and quickly portable across tasks and domains. SIE is composed by a general purpose machine learning algorithm (SVM) combined with several customizable modules. A crucial role in the architecture is played by Instance Filtering, which allows to increase efﬁciency without reducing effectiveness. The results obtained by SIE on several standard data sets, representative of different tasks and domains, are reported. The experiments show that SIE achieves performance close to the best systems in all tasks, without using domain-speciﬁc knowledge. 
The requirement for large labelled training corpora is widely recognized as a key bottleneck in the use of learning algorithms for information extraction. We present TPLEX, a semi-supervised learning algorithm for information extraction that can acquire extraction patterns from a small amount of labelled text in conjunction with a large amount of unlabelled text. Compared to previous work, TPLEX has two novel features. First, the algorithm does not require redundancy in the fragments to be extracted, but only redundancy of the extraction patterns themselves. Second, most bootstrapping methods identify the highest quality fragments in the unlabelled data and then assume that they are as reliable as manually labelled data in subsequent iterations. In contrast, TPLEX’s scoring mechanism prevents errors from snowballing by recording the reliability of fragments extracted from unlabelled data. Our experiments with several benchmarks demonstrate that TPLEX is usually competitive with various fully-supervised algorithms when very little labelled training data is available. 
Semantic relationships between words comprised by thesauri are essential features for IR, text mining and information extraction systems. This paper introduces a new approach to identiﬁcation of semantic relations such as synonymy by a lexical graph. The graph is generated from a text corpus by embedding syntactically parsed sentences in the graph structure. The vertices of the graph are lexical items (words), their connection follows the syntactic structure of a sentence. The structure of the graph and distances between vertices can be utilized to deﬁne metrics for identiﬁcation of semantic relations. The approach has been evaluated on a test set of 200 German synonym sets. Inﬂuence of size of the text corpus, word generality and frequency has been investigated. Conducted experiments for synonyms demonstrate that the presented methods can be extended to other semantic relations. 
We present two methods for semiautomatic detection and correction of errors in textual databases. The ﬁrst method (horizontal correction) aims at correcting inconsistent values within a database record, while the second (vertical correction) focuses on values which were entered in the wrong column. Both methods are data-driven and language-independent. We utilise supervised machine learning, but the training data is obtained automatically from the database; no manual annotation is required. Our experiments show that a signiﬁcant proportion of errors can be detected by the two methods. Furthermore, both methods were found to lead to a precision that is high enough to make semi-automatic error correction feasible. 
In this paper we present a hybrid approach for the acquisition of syntacticosemantic patterns from raw text. Our approach co-trains a decision list learner whose feature space covers the set of all syntactico-semantic patterns with an Expectation Maximization clustering algorithm that uses the text words as attributes. We show that the combination of the two methods always outperforms the decision list learner alone. Furthermore, using a modular architecture we investigate several algorithms for pattern ranking, the most important component of the decision list learner. 
Most works on relation extraction assume considerable human effort for making an annotated corpus or for knowledge engineering. Generic patterns employed in KnowItAll achieve unsupervised, highprecision extraction, but often result in low recall. This paper compares two bootstrapping methods to expand recall that start with automatically extracted seeds by KnowItAll. The ﬁrst method is string pattern learning, which learns string contexts adjacent to a seed tuple. The second method learns less restrictive patterns that include bags of words and relation-speciﬁc named entity tags. Both methods improve the recall of the generic pattern method. In particular, the less restrictive pattern learning method can achieve a 250% increase in recall at 0.87 precision, compared to the generic pattern method. 
This paper introduces a semi-supervised learning framework for creating training material, namely active annotation. The main intuition is that an unsupervised method is used to initially annotate imperfectly the data and then the errors made are detected automatically and corrected by a human annotator. We applied active annotation to named entity recognition in the biomedical domain and encouraging results were obtained. The main advantages over the popular active learning framework are that no seed annotated data is needed and that the reusability of the data is maintained. In addition to the framework, an efﬁcient uncertainty estimation for Hidden Markov Models is presented. 
The paper examines how people’s judgements of proximity between two objects are inﬂuenced by the presence of a third object. In an experiment participants were presented with images containing three shapes in different relative positions, and asked to rate the acceptability of a locative expression such as ‘the circle is near the triangle’ as descriptions of those images. The results showed an interaction between the relative positions of objects and the linguistic roles that those objects play in the locative expression: proximity was a decreasing function of the distance between the head object in the expression and the prepositional clause object, and an increasing function the distance between the head and the third, distractor object. This ﬁnding leads us to a new account for the semantics of spatial prepositions such as near. 
This paper explicates how English and Polish select different aspects of a spatial situation to express coincidence or proximity. Spatial relations verbalized by different senses of at in English are reflected primarily by means of w, na, przy and u in Polish. First, at is presented as conceptualizing the relation of coincidence or proximity with the whole Landmark (LM) when it is conceived as a point. Then, the Polish prepositions are investigated – how they differ with respect to the specific part of the LM that is involved in a topological relation. Although the prepositions are available to the speakers of the respective languages to render different types of relations, their senses overlap in a complex way. 
This paper presents the current results of an ongoing research project on corpus distribution of prepositions and pronouns within Polish preposition-pronoun contractions. The goal of the project is to provide a quantitative description of Polish preposition-pronoun contractions taking into consideration morphosyntactic properties of their components. It is expected that the results will provide a basis for a revision of the traditionally assumed inﬂectional paradigms of Polish pronouns and, thus, for a possible remodeling of these paradigms. The results of corpus-based investigations of the distribution of prepositions within preposition-pronoun contractions can be used for grammar-theoretical and lexicographic purposes. 
 word order (i.e., pre- vs. postpositional use of the adposition).  This paper discusses the partitive-genitive case alternation of Finnish adpositions. This case alternation is explained in terms of bidirectional alignment of markedness in form and meaning. Marked PP meanings are assigned partitive case, unmarked ones genitive case.  
The proper interpretation of prepositions is an important issue for automatic natural language understanding. We present an approach towards PP interpretation as part of a natural language understanding system which has been successfully employed in various NLP tasks for information retrieval and question answering. Our approach is based on the so-called MultiNet paradigm, a knowledge representation formalism especially designed for the representation of natural language semantics. The paper describes how the information about the semantic interpretation of PPs is represented in the lexicon and in PP interpretation rules and how this information is used during semantic analysis. Moreover, we report on experiments that evaluate the impact of using this information about PP interpretation on the CLEF question answering task. 
In The Preposition Project (TPP), 13 prepositions have now been analyzed and considerable data made available. These prepositions, among the most common words in English, contain 211 senses. By analyzing the coverage of these senses, it is shown that TPP provides potentially greater breadth and depth than other inventories of the range of semantic roles. Specific inheritance mechanisms are developed within the preposition sense inventory and shown to be viable and provide a basis for the rationalization of the range of preposition meaning. In addition, this rationalization can be used for developing a data-driven mapping of a semantic role hierarchy. Based on these findings and methodology, the broad structure of a WordNet-like representation of preposition meaning, with self-contained disambiguation tests, is outlined. 
This paper describes ongoing work, aimed at producing a lexicon of prepositions, i.e. relations denoted by prepositions, to be used for information retrieval purposes. The work is ontology based, which for this project means that the ontological types of the arguments of the preposition are considered, rather than the word forms. Thus, sense distinctions are made based on ontological constraints on the arguments. 
Instruments are expressed in language by various means: prepositions, postpositions, afﬁxes including case marks, nonﬁnite verbs, etc. We consider here 12 languages from ﬁve families in order to be able to identify the different meaning components that structure instrumentality. 
This paper discusses the behaviour of German particle verbs formed by two-way prepositions in combination with pleonastic PPs including the verb particle as a preposition. These particle verbs have a characteristic feature: some of them license directional prepositional phrases in the accusative, some only allow for locative PPs in the dative, and some particle verbs can occur with PPs in the accusative and in the dative. Directional particle verbs together with directional PPs present an additional problem: the particle and the preposition in the PP seem to provide redundant information. The paper gives an overview of the semantic verb classes inﬂuencing this phenomenon, based on corpus data, and explains the underlying reasons for the behaviour of the particle verbs. We also show how the restrictions on particle verbs and pleonastic PPs can be expressed in a grammar theory like Lexical Functional Grammar (LFG). 
This paper presents a method for identifying token instances of verb particle constructions (VPCs) automatically, based on the output of the RASP parser. The proposed method pools together instances of VPCs and verb-PPs from the parser output and uses the sentential context of each such instance to differentiate VPCs from verb-PPs. We show our technique to perform at an F-score of 97.4% at identifying VPCs in Wall Street Journal and Brown Corpus data taken from the Penn Treebank. 
This paper deals with the prepositions which introduce an adjunct of duration, such as the English for and in. On the basis of both crosslingual and monolingual evidence these adjuncts are argued to be ambiguous between a ﬂoating and an anchored interpretation. To capture the distinction in formal terms I employ the framework of HEAD-DRIVEN PHRASE STRUCTURE GRAMMAR, enriched with a number of devices which are familiar from DISCOURSE REPRESENTATION THEORY. The resulting analysis is demonstrated to be relevant for machine translation, natural language generation and natural language understanding. 
The correct attachment of prepositional phrases (PPs) is a central disambiguation problem in parsing natural languages. This paper compares the baseline situation in English, German and Swedish based on manual PP attachments in various treebanks for these languages. We argue that cross-language comparisons of the disambiguation results in previous research is impossible because of the different selection procedures when building the training and test sets. We perform uniform treebank queries and show that English has the highest noun attachment rate followed by Swedish and German. We also show that the high rate in English is dominated by the preposition of. From our study we derive a list of criteria for proﬁling data sets for PP attachment experiments. 
The present study focuses on the lexical meanings of prepositions rather than on the thematic meanings because it is intended for use in an English-Bengali machine translation (MT) system, where the meaning of a lexical unit must be preserved in the target language, even though it may take a different syntactic form in the source and target languages. Bengali is the fifth language in the world in terms of the number of native speakers and is an important language in India. There is no concept of preposition in Bengali. English prepositions are translated to Bengali by attaching appropriate inflections to the head noun of the prepositional phrase (PP), i.e., the object of the preposition. The choice of the inflection depends on the spelling pattern of the translated Bengali head noun. Further postpositional words may also appear in the Bengali translation for some prepositions. The choice of the appropriate postpositional word depends on the WordNet synset information of the head noun. Idiomatic or metaphoric PPs are translated into Bengali by looking into a bilingual example base. The analysis presented here is general and applicable for translation from English to many other Indo-Aryan languages that handle prepositions using inflections and postpositions. 
This paper presents the automatic extension to other languages of TERSEO, a knowledge-based system for the recognition and normalization of temporal expressions originally developed for Spanish1. TERSEO was ﬁrst extended to English through the automatic translation of the temporal expressions. Then, an improved porting process was applied to Italian, where the automatic translation of the temporal expressions from English and from Spanish was combined with the extraction of new expressions from an Italian annotated corpus. Experimental results demonstrate how, while still adhering to the rule-based paradigm, the development of automatic rule translation procedures allowed us to minimize the effort required for porting to new languages. Relying on such procedures, and without any manual effort or previous knowledge of the target language, TERSEO recognizes and normalizes temporal expressions in Italian with good results (72% precision and 83% recall for recognition). 
The standard PCFG approach to parsing is quite successful on certain domains, but is relatively inﬂexible in the type of feature information we can include in its probabilistic model. In this work, we discuss preliminary work in developing a new probabilistic parsing model that allows us to easily incorporate many different types of features, including crosslingual information. We show how this model can be used to build a successful parser for a small handmade gold-standard corpus of 188 sentences (in 3 languages) from the Europarl corpus. 
We introduce the problem of explicit modeling of form relationships between words in different languages, focusing here on languages having an alphabetic writing system and affixal morphology. We present an algorithm that learns the cross-language correspondence between affixes and letter sequences. The algorithm does not assume prior knowledge of affixes in any of the languages, using only a simple single letter correspondence as seed. Results are given for the English-Spanish language pair. 
This paper describes a method of discriminating ambiguous names that relies upon features found in corpora of a more abundant language. In particular, we discriminate ambiguous names in Bulgarian, Romanian, and Spanish corpora using information derived from much larger quantities of English data. We also mix together occurrences of the ambiguous name found in English with the occurrences of the name in the language in which we are trying to discriminate. We refer to this as a language salad, and ﬁnd that it often results in even better performance than when only using English or the language itself as the source of information for discrimination. 
We describe a knowledge and resource light system for an automatic morphological analysis and tagging of Brazilian Portuguese.1 We avoid the use of labor intensive resources; particularly, large annotated corpora and lexicons. Instead, we use (i) an annotated corpus of Peninsular Spanish, a language related to Portuguese, (ii) an unannotated corpus of Portuguese, (iii) a description of Portuguese morphology on the level of a basic grammar book. We extend the similar work that we have done (Hana et al., 2004; Feldman et al., 2006) by proposing an alternative algorithm for cognate transfer that effectively projects the Spanish emission probabilities into Portuguese. Our experiments use minimal new human effort and show 21% error reduction over even emissions on a ﬁne-grained tagset. 
 We describe a method which uses one or more intermediary languages in order to automatically generate translation dictionaries. Such a method could potentially be used to efﬁciently create translation dictionaries for language groups which have as yet had little interaction. For any given word in the source language, our method involves ﬁrst translating into the intermediary language(s), then into the target language, back into the intermediary language(s) and ﬁnally back into the source language. The relationship between a word and the number of possible translations in another language is most often 1-to-many, and so at each stage, the number of possible translations grows exponentially. If we arrive back at the same starting point i.e. the same word in the source language, then we hypothesise that the meanings of the words in the chain have not diverged signiﬁcantly. Hence we backtrack through the link structure to the target language word and accept this as a suitable translation. We have tested our method by using English as an intermediary language to automatically generate a Spanish-to-German dictionary, and the results are encouraging.  
We present an unsupervised approach to Word Sense Disambiguation (WSD). We automatically acquire English sense examples using an English-Chinese bilingual dictionary, Chinese monolingual corpora and Chinese-English machine translation software. We then train machine learning classiﬁers on these sense examples and test them on two gold standard English WSD datasets, one for binary and the other for ﬁne-grained sense identiﬁcation. On binary disambiguation, performance of our unsupervised system has approached that of the state-of-the-art supervised ones. On multi-way disambiguation, it has achieved a very good result that is competitive to other state-of-the-art unsupervised systems. Given the fact that our approach does not rely on manually annotated resources, such as sense-tagged data or parallel corpora, the results are very promising. 
This paper presents the ﬁrst step to project POS tags and dependencies from English and French to Polish in aligned corpora. Both the English and French parts of the corpus are analysed with a POS tagger and a robust parser. The English/Polish bi-text and the French/Polish bi-text are then aligned at the word level with the GIZA++ package. The intersection of IBM-4 Viterbi alignments for both translation directions is used to project the annotations from English and French to Polish. The results show that the precision of direct projection vary according to the type of induced annotations as well as the source language. Moreover, the performances are likely to be improved by deﬁning regular conversion rules among POS tags and dependencies. 
 2 Background  A Question Answering (QA) system allows the user to ask questions in natural language and to obtain one or several answers. If compared with a classical IR engine like Google, what kind of key benefits QA bring to users and how to measure their distinctive performances. This is what we shall attempt here to determine, specially in providing a comparative weak and strong points table of each system, along with showing how QA systems, in particular our Qristal QA system, requires up two to six time less “user effort”. 
The aim of this paper is to investigate how much the effectiveness of a Question Answering (QA) system was affected by the performance of Machine Translation (MT) based question translation. Nearly 200 questions were selected from TREC QA tracks and ran through a question answering system. It was able to answer 42.6% of the questions correctly in a monolingual run. These questions were then translated manually from English into Arabic and back into English using an MT system, and then re-applied to the QA system. The system was able to answer 10.2% of the translated questions. An analysis of what sort of translation error affected which questions was conducted, concluding that factoid type questions are less prone to translation error than others. 
We describe on-going work in the development of a cross-language questionanswering framework for the open domain. An overview of the framework is being provided, some details on the important concepts of a flexible framework are presented and two cross-cutting aspects (cross-linguality and credibility) for question-answering systems are up for discussion. 
This article presents a bilingual question answering system, which is able to process questions and documents both in French and in English. Two cross-lingual strategies are described and evaluated. First, we study the contribution of biterms translation, and the inﬂuence of the completion of the translation dictionaries. Then, we propose a strategy for transferring the question analysis from one language to the other, and we study its inﬂuence on the performance of our system. 
In this paper, we describe the extension of an existing monolingual QA system for English-to-Chinese and English-toJapanese cross-lingual question answering (CLQA). We also attempt to characterize the influence of translation on CLQA performance through experimental evaluation and analysis. The paper also describes some language-specific issues for keyword translation in CLQA. 
Question Classiﬁcation is an important task in Question Answering Systems. This paper presents a Spanish Question Classiﬁer based on machine learning, automatic online translators and different language features. Our system works with English collections and bilingual questions (English/Spanish). We have tested two Spanish-English online translators to identify the lost of precision. We have made experiments using lexical, syntactic and semantic features to test which ones made a better performance. The obtained results show that our system makes good classiﬁcations, over a 80% in terms of accuracy using the original English questions and over a 65% using Spanish questions and machine translation systems. Our conclusion about the features is that a lexical, syntactic and semantic features combination obtains the best result. 
In this paper we extend the application of our statistical pattern classiﬁcation approach to question answering (QA) which has previously been applied successfully to English and Japanese to develop two prototype QA systems in Chinese and Swedish. We show what data is necessary to achieve this and also evaluate the performance of the two new systems using a translation of the TREC 2003 factoid QA task. While performance for Chinese and Swedish is found to be lower than that for the more developed English and Japanese systems we explain why this is the case and offer solutions for their improvement. All systems form the basis of our publicly accessible web-based multilingual QA system at http://asked.jp. 
A dialogue based Question Answering (QA) system for Railway information in Telugu has been described. Telugu is an important language in India belonging to the Dravidian family. The main component of our QA system is the Dialogue Manager (DM), to handle the dialogues between user and system. It is necessary in generating dialogue for clarifying partially understood questions, resolving Anaphora and Co-reference problems. Besides, different modules have been developed for processing the query and its translation into formal database query language statement(s). Based on the result from the database, a natural language answer is generated. The empirical results obtained on the current system are encouraging. Testing with a set of questions in Railway domain, the QA system showed 96.34% of precision and 83.96% of dialogue success rate. Such a question answering system can be effectively utilized when integrated with a speech input and speech output system. 
This paper describes how a question answering (QA) system developed for smallsized document collections of several million sentences was modiﬁed in order to work with a monolingual subset of the web. The basic QA system relies on complete sentence parsing, inferences, and semantic representation matching. The extensions and modiﬁcations needed for useful and quick answers from web documents are discussed. The main extension is a two-level approach that ﬁrst accesses a web search engine and downloads some of its document hits and then works similar to the basic QA system. Most modiﬁcations are restrictions like a maximal number of documents and a maximal length of investigated document parts; they ensure acceptable answer times. The resulting web QA system is evaluated on the German test collection from QA@CLEF 2004. Several parameter settings and strategies for accessing the web search engine are investigated. The main results are: precision-oriented extensions and experimentally derived parameter settings are needed to achieve similar performance on the web as on small-sized document collections that show higher homogeneity and quality of the contained texts; adapting a semantic QA system to the web is feasible, but answering a question is still expensive in terms of bandwidth and CPU time. 
This paper describes an approach to adapt an existing multilingual Open-Domain Question Answering (ODQA) system for factoid questions to a Restricted Domain, the Geographical Domain. The adaptation of this ODQA system involved the modiﬁcation of some components of our system such as: Question Processing, Passage Retrieval and Answer Extraction. The new system uses external resources like GNS Gazetteer for Named Entity (NE) Classiﬁcation and Wikipedia or Google in order to obtain relevant documents for this domain. The system focuses on a Geographical Scope: given a region, or country, and a language we can semi-automatically obtain multilingual geographical resources (e.g. gazetteers, trigger words, groups of place names, etc.) of this scope. The system has been trained and evaluated for Spanish in the scope of the Spanish Geography. The evaluation reveals that the use of scope-based Geographical resources is a good approach to deal with multilingual Geographical Domain Question Answering. 
The research community states that QA systems should support the integration of deeper modes of language understanding as well as more elaborated reasoning schemas in order to boost the performances of current QA systems as well as the quality and the relevance of the produced answers. Depending on the complexity of the question and the associated passages, more or less complex strategies can be used, such as : • deep semantic analysis of NL questions such as anaphora resolutions, • context and ambiguity detection, • responses to unanticipated questions or to resolve situations in which no answer is found in the data sources, • models for answer completeness, • dialogue and interactive QA scenario, • models for answer fusion from different sources, etc. We focus in this talk on the role of reasoning in a QA process by answering the following questions: what kind of reasoning capabilities can be used ? on what kind of resources can they be built on ? at what extend they can be used when developing realistic systems ? Based on the synthesis of the current state of the art, the second part of the talk describes a general typology of inference at different levels: deep semantic analysis of both NL questions and passages, textual 
The availability of robust and deep syntactic parsing can improve the performance of Question Answering systems. This is illustrated using examples from Joost, a Dutch QA system which has been used for both open (CLEF) and closed domain QA. 
We discuss how deep interpretation and generation can be integrated with a knowledge representation designed for question answering to build a tutorial dialogue system. We use a knowledge representation known to perform well in answering exam-type questions and show that to support tutorial dialogue it needs additional features, in particular, compositional representations for interpretation and structured explanation representations. 
In this paper, we propose a ﬁne grained classiﬁcation of english adjectives geared at modeling the distinct inference patterns licensed by each adjective class. We show how it can be implemented in description logic and illustrate the predictions made by a series of examples. The proposal has been implemented using Description logic as a semantic representation language and the prediction veriﬁed using the DL theorem prover RACER. Topics: Textual Entailment, Adjectival Semantics 
We consider a logicist approach to natural language understanding based on the translation of a quasi-logical form into a temporal logic, explicitly constructed for the representation of action and change, and the subsequent reasoning about this semantic structure in the context of a background knowledge theory using automated theorem proving techniques. The approach is substantiated through a proof-ofconcept question answering system implementation that uses a head-driven phrase structure grammar developed in the Linguistic Knowledge Builder to construct minimal recursion semantics structures which are translated into a Temporal Action Logic where both the SNARK automated theorem prover and the Allegro Prolog logic programming environment can be used for reasoning through an interchangeable compilation into ﬁrst-order logic or logic programs respectively. 
We demonstrate a proof-of-concept system that uses a shallow chunking-based technique for knowledge extraction from natural language text, in particular looking at the task of story understanding. This technique is extended with a reasoning engine that borrows techniques from dynamic ontology reﬁnement to discover the semantic similarity of stories and to merge them together. 
In this paper, we present an approach which aims at providing numerical answers in a question-answering system. These answers are generated in a cooperative way: they explain the variation of numerical values when several values, apparently incoherent, are extracted from the web as possible answers to a question. 
Most question answering and information retrieval systems are insensitive to different users’ needs and preferences, as well as their reading level. In (Quarteroni and Manandhar, 2006), we introduce a hybrid QA-IR system based on a a user model. In this paper we focus on how the system ﬁlters and re-ranks the search engine results for a query according to their reading difﬁculty, providing user-tailored answers. Keywords: question answering, information retrieval, user modelling, readability. 
Frequency dictionaries play an important role both in psycholinguistic experiment design and in language technology. The paper describes a new, freely available, web-based frequency dictionary of Hungarian that is being used for both purposes, and the language-independent techniques used for creating it. 0 Introduction In theoretical linguistics introspective grammaticality judgments are often seen as having methodological primacy over conclusions based on what is empirically found in corpora. No doubt the main reason for this is that linguistics often studies phenomena that are not well exempliﬁed in data. For example, in the entire corpus of written English there seems to be only one attested example, not coming from semantics papers, of Bach-Peters sentences, yet the grammaticality (and the preferred reading) of these constructions seems beyond reproach. But from the point of view of the theoretician who claims that quantiﬁer meanings can be computed by repeat substitution, even this one example is one too many, since no such theory can account for the clearly relevant (though barely attested) facts. In this paper we argue that ordinary corpus size has grown to the point that in some areas of theoretical linguistics, in particular for issues of inﬂectional morphology, the dichotomy between introspective judgments and empirical observations need no longer be maintained: in this area at least, it is now nearly possible to make the leap from zero observed frequency to zero theoretical probability i.e. ungrammaticality.  In many other areas, most notably syntax, this is still untrue, and here we argue that facts of derivational morphology are not yet entirely within the reach of empirical methods. Both for inﬂectional and derivational morphology we base our conclusions on recent work with a gigaword web-based corpus of Hungarian (Hala´csy et al 2004) which goes some way towards fulﬁlling the goals of the WaCky project (http://wacky.sslmit.unibo.it, see also Lu¨deling et al 2005) inasmuch as the infrastructure used in creating it is applicable to other medium-density languages as well. Section 1 describes the creation of the WFDH Web-based Frequency Dictionary of Hungarian from the raw corpus. The critical disambiguation step required for lemmatization is discussed in Section 2, and the theoretical implications are presented in Section 3. The rest of this Introduction is devoted to some terminological clariﬁcation and the presentation of the elementary probabilistic model used for psycholinguistic experiment design. 0.1 The range of data Here we will distinguish three kinds of corpora: small-, medium-, and large-range, based on the internal coherence of the component parts. A smallrange corpus is one that is stylistically homogeneous, generally the work of a single author. The largest corpora that we could consider small-range are thus the oeuvres of the most proliﬁc writers, rarely above 1m, and never above 10m words. A medium-range corpus is one that remains within the conﬁnes of a few text types, even if the authorship of individual documents can be discerned e.g. by detailed study of word usage. The LDC gigaword corpora, composed almost entirely of news (journalistic prose), are from this perspec-  
This paper studies issues related to the compilation of a bilingual lexicon for technical terms. In the task of estimating bilingual term correspondences of technical terms, it is usually rather difﬁcult to ﬁnd an existing corpus for the domain of such technical terms. In this paper, we adopt an approach of collecting a corpus for the domain of such technical terms from the Web. As a method of translation estimation for technical terms, we employ a compositional translation estimation technique. This paper focuses on quantitatively comparing variations of the components in the scoring functions of compositional translation estimation. Through experimental evaluation, we show that the domain/topic-speciﬁc corpus contributes toward improving the performance of the compositional translation estimation. 
This paper presents CUCWeb, a 166 million word corpus for Catalan built by crawling the Web. The corpus has been annotated with NLP tools and made available to language users through a ﬂexible web interface. The developed architecture is quite general, so that it can be used to create corpora for other languages. 
This paper presents a proposal to facilitate the use of the annotated web as corpus by alleviating the annotation bottleneck for corpus data drawn from the web. We describe a framework for large-scale distributed corpus annotation using peerto-peer (P2P) technology to meet this need. We also propose to annotate a large reference corpus in order to evaluate this framework. This will allow us to investigate the affordances offered by distributed techniques to ensure replicability of linguistic research based on web-derived corpora. 
When corporations, news media and advocacy organizations embrace networked information technology, intentionally or unintentionally, they influence democratic processes. To capture and understand the influence of publicly available electronic content, the US Election 2004 Web Monitor1 tracked the online coverage of US presidential candidates, and investigated how this coverage reflected their position on environmental issues. 
This paper presents a new approach and a software for collecting specialized corpora on the Web. This approach takes advantage of a very popular XML-based norm used on the Web for sharing content among websites: RSS (Really Simple Syndication). After a brief introduction to RSS, we explain the interest of this type of data sources in the framework of corpus development. Finally, we present Corporator, an Open Source software which was designed for collecting corpus from RSS feeds. 
This paper presents a general architecture and four algorithms that use Natural Language Processing for automatic ontology matching. The proposed approach is purely instance based, i.e., only the instance documents associated with the nodes of ontologies are taken into account. The four algorithms have been evaluated using real world test data, taken from the Google and LookSmart online directories. The results show that NLP techniques applied to instance documents help the system achieve higher performance. 
Some languages’ orthographic properties allow written data to be used for phonological research. This paper reports on an on-going project that uses a web-derived text corpus to study the phonology of Tagalog, a language for which large corpora are not otherwise available. Novel findings concerning the phenomenon of intervocalic tapping are discussed in detail, and an overview of other phonological phenomena in the language that can be investigated through written data is given. 
In this paper we present an approach to structure learning in the area of web documents. This is done in order to approach the goal of webgenre tagging in the area of web corpus linguistics. A central outcome of the paper is that purely structure oriented approaches to web document classiﬁcation provide an information gain which may be utilized in combined approaches of web content and structure analysis. 
This paper demonstrates how unsupervised techniques can be used to learn models of deep linguistic structure. Determining the semantic roles of a verb’s dependents is an important step in natural language understanding. We present a method for learning models of verb argument patterns directly from unannotated text. The learned models are similar to existing verb lexicons such as VerbNet and PropBank, but additionally include statistics about the linkings used by each verb. The method is based on a structured probabilistic model of the domain, and unsupervised learning is performed with the EM algorithm. The learned models can also be used discriminatively as semantic role labelers, and when evaluated relative to the PropBank annotation, the best learned model reduces 28% of the error between an informed baseline and an oracle upper bound. 
In this paper we introduce an empirical approach to the semantic interpretation of superlative adjectives. We present a corpus annotated for superlatives and propose an interpretation algorithm that uses a wide-coverage parser and produces semantic representations. We achieve Fscores between 0.84 and 0.91 for detecting attributive superlatives and an accuracy in the range of 0.69–0.84 for determining the correct comparison set. As far as we are aware, this is the ﬁrst automated approach to superlatives for open-domain texts and questions. 
We propose a supervised, two-phase framework to address the problem of paraphrase recognition (PR). Unlike most PR systems that focus on sentence similarity, our framework detects dissimilarities between sentences and makes its paraphrase judgment based on the signiﬁcance of such dissimilarities. The ability to differentiate signiﬁcant dissimilarities not only reveals what makes two sentences a nonparaphrase, but also helps to recall additional paraphrases that contain extra but insigniﬁcant information. Experimental results show that while being accurate at discerning non-paraphrasing dissimilarities, our implemented system is able to achieve higher paraphrase recall (93%), at an overall performance comparable to the alternatives. 
NLP systems for tasks such as question answering and information extraction typically rely on statistical parsers. But the efﬁcacy of such parsers can be surprisingly low, particularly for sentences drawn from heterogeneous corpora such as the Web. We have observed that incorrect parses often result in wildly implausible semantic interpretations of sentences, which can be detected automatically using semantic information obtained from the Web. Based on this observation, we introduce Web-based semantic ﬁltering—a novel, domain-independent method for automatically detecting and discarding incorrect parses. We measure the effectiveness of our ﬁltering system, called WOODWARD, on two test collections. On a set of TREC questions, it reduces error by 67%. On a set of more complex Penn Treebank sentences, the reduction in error rate was 20%. 
We propose a framework to derive the distance between concepts from distributional measures of word co-occurrences. We use the categories in a published thesaurus as coarse-grained concepts, allowing all possible distance values to be stored in a concept–concept matrix roughly .01% the size of that created by existing measures. We show that the newly proposed concept-distance measures outperform traditional distributional word-distance measures in the tasks of (1) ranking word pairs in order of semantic distance, and (2) correcting realword spelling errors. In the latter task, of all the WordNet-based measures, only that proposed by Jiang and Conrath outperforms the best distributional conceptdistance measures. 
We introduce SPMT, a new class of statistical Translation Models that use Syntactiﬁed target language Phrases. The SPMT models outperform a state of the art phrase-based baseline model by 2.64 Bleu points on the NIST 2003 Chinese-English test corpus and 0.28 points on a humanbased quality metric that ranks translations on a scale from 1 to 5. 
We discuss different strategies for smoothing the phrasetable in Statistical MT, and give results over a range of translation settings. We show that any type of smoothing is a better idea than the relativefrequency estimates that are often used. The best smoothing techniques yield consistent gains of approximately 1% (absolute) according to the BLEU metric. 
We investigate the impact of parse quality on a syntactically-informed statistical machine translation system applied to technical text. We vary parse quality by varying the amount of data used to train the parser. As the amount of data increases, parse quality improves, leading to improvements in machine translation output and results that signiﬁcantly outperform a state-of-the-art phrasal baseline. 
Reordering is currently one of the most important problems in statistical machine translation systems. This paper presents a novel strategy for dealing with it: statistical machine reordering (SMR). It consists in using the powerful techniques developed for statistical machine translation (SMT) to translate the source language (S) into a reordered source language (S’), which allows for an improved translation into the target language (T). The SMT task changes from S2T to S’2T which leads to a monotonized word alignment and shorter translation units. In addition, the use of classes in SMR helps to infer new word reorderings. Experiments are reported in the EsEn WMT06 tasks and the ZhEn IWSLT05 task and show signiﬁcant improvement in translation quality. 
In this paper, we present ParaEval, an automatic evaluation framework that uses paraphrases to improve the quality of machine translation evaluations. Previous work has focused on fixed n-gram evaluation metrics coupled with lexical identity matching. ParaEval addresses three important issues: support for paraphrase/synonym matching, recall measurement, and correlation with human judgments. We show that ParaEval correlates significantly better than BLEU with human assessment in measurements for both fluency and adequacy. 
In this paper we study the utility of discourse structure for spoken dialogue performance modeling. We experiment with various ways of exploiting the discourse structure: in isolation, as context information for other factors (correctness and certainty) and through trajectories in the discourse structure hierarchy. Our correlation and PARADISE results show that, while the discourse structure is not useful in isolation, using the discourse structure as context information for other factors or via trajectories produces highly predictive parameters for performance analysis. 
In this paper we address the issue of automatically assigning information status to discourse entities. Using an annotated corpus of conversational English and exploiting morpho-syntactic and lexical features, we train a decision tree to classify entities introduced by noun phrases as old, mediated, or new. We compare its performance with hand-crafted rules that are mainly based on morpho-syntactic features and closely relate to the guidelines that had been used for the manual annotation. The decision tree model achieves an overall accuracy of 79.5%, signiﬁcantly outperforming the hand-crafted algorithm (64.4%). We also experiment with binary classiﬁcations by collapsing in turn two of the three target classes into one and retraining the model. The highest accuracy achieved on binary classiﬁcation is 93.1%. 
Citation function is deﬁned as the author’s reason for citing a given paper (e.g. acknowledgement of the use of the cited method). The automatic recognition of the rhetorical function of citations in scientiﬁc text has many applications, from improvement of impact factor calculations to text summarisation and more informative citation indexers. We show that our annotation scheme for citation function is reliable, and present a supervised machine learning framework to automatically classify citation function, using both shallow and linguistically-inspired features. We ﬁnd, amongst other things, a strong relationship between citation function and sentiment classiﬁcation. 
This paper presents a comparative study of probabilistic treebank parsing of German, using the Negra and Tu¨Ba-D/Z treebanks. Experiments with the Stanford parser, which uses a factored PCFG and dependency model, show that, contrary to previous claims for other parsers, lexicalization of PCFG models boosts parsing performance for both treebanks. The experiments also show that there is a big difference in parsing performance, when trained on the Negra and on the Tu¨BaD/Z treebanks. Parser performance for the models trained on Tu¨Ba-D/Z are comparable to parsing results for English with the Stanford parser, when trained on the Penn treebank. This comparison at least suggests that German is not harder to parse than its West-Germanic neighbor language English. 
Discriminative learning methods are widely used in natural language processing. These methods work best when their training and test data are drawn from the same distribution. For many NLP tasks, however, we are confronted with new domains in which labeled data is scarce or non-existent. In such cases, we seek to adapt existing models from a resourcerich source domain to a resource-poor target domain. We introduce structural correspondence learning to automatically induce correspondences among features from different domains. We test our technique on part of speech tagging and show performance gains for varying amounts of source and target training data, as well as improvements in target domain parsing accuracy using our improved tagger. 
Integer Linear Programming has recently been used for decoding in a number of probabilistic models in order to enforce global constraints. However, in certain applications, such as non-projective dependency parsing and machine translation, the complete formulation of the decoding problem as an integer linear program renders solving intractable. We present an approach which solves the problem incrementally, thus we avoid creating intractable integer linear programs. This approach is applied to Dutch dependency parsing and we show how the addition of linguistically motivated constraints can yield a signiﬁcant improvement over stateof-the-art. 
This paper describes our attempt at NomBank-based automatic Semantic Role Labeling (SRL). NomBank is a project at New York University to annotate the argument structures for common nouns in the Penn Treebank II corpus. We treat the NomBank SRL task as a classiﬁcation problem and explore the possibility of adapting features previously shown useful in PropBank-based SRL systems. Various NomBank-speciﬁc features are explored. On test section 23, our best system achieves F1 score of 72.73 (69.14) when correct (automatic) syntactic parse trees are used. To our knowledge, this is the ﬁrst reported automatic NomBank SRL system. 
Complex tasks like question answering need to be able to identify events in text and the relations among those events. We show that this event identification task and a related task, identifying the semantic class of these events, can both be formulated as classification problems in a word-chunking paradigm. We introduce a variety of linguistically motivated features for this task and then train a system that is able to identify events with a precision of 82% and a recall of 71%. We then show a variety of analyses of this model, and their implications for the event identification task. 
This paper describes an extremely lexicalized probabilistic model for fast and accurate HPSG parsing. In this model, the probabilities of parse trees are deﬁned with only the probabilities of selecting lexical entries. The proposed model is very simple, and experiments revealed that the implemented parser runs around four times faster than the previous model and that the proposed model has a high accuracy comparable to that of the previous model for probabilistic HPSG, which is deﬁned over phrase structures. We also developed a hybrid of our probabilistic model and the conventional phrasestructure-based model. The hybrid model is not only signiﬁcantly faster but also signiﬁcantly more accurate by two points of precision and recall compared to the previous model. 
We propose a conditional random ﬁeldbased method for supertagging, and apply it to the task of learning new lexical items for HPSG-based precision grammars of English and Japanese. Using a pseudo-likelihood approximation we are able to scale our model to hundreds of supertags and tens-of-thousands of training sentences. We show that it is possible to achieve start-of-the-art results for both languages using maximally language-independent lexical features. Further, we explore the performance of the models at the type- and token-level, demonstrating their superior performance when compared to a unigram-based baseline and a transformation-based learning approach. 
Semantic lexical matching is a prominent subtask within text understanding applications. Yet, it is rarely evaluated in a direct manner. This paper proposes a definition for lexical reference which captures the common goals of lexical matching. Based on this deﬁnition we created and analyzed a test dataset that was utilized to directly evaluate, compare and improve lexical matching models. We suggest that such decomposition of the global semantic matching task is critical in order to fully understand and improve individual components. 
We consider the problem of constructing a directed acyclic graph that encodes temporal relations found in a text. The unit of our analysis is a temporal segment, a fragment of text that maintains temporal coherence. The strength of our approach lies in its ability to simultaneously optimize pairwise ordering preferences and global constraints on the graph topology. Our learning method achieves 83% F-measure in temporal segmentation and 84% accuracy in inferring temporal relations between two segments. 
In this paper, we present a weakly supervised learning approach for spoken language understanding in domain-specific dialogue systems. We model the task of spoken language understanding as a successive classification problem. The first classifier (topic classifier) is used to identify the topic of an input utterance. With the restriction of the recognized target topic, the second classifier (semantic classifier) is trained to extract the corresponding slot-value pairs. It is mainly data-driven and requires only minimally annotated corpus for training whilst retaining the understanding robustness and deepness for spoken language. Most importantly, it allows the employment of weakly supervised strategies for training the two classifiers. We first apply the training strategy of combining active learning and self-training (Tur et al., 2005) for topic classifier. Also, we propose a practical method for bootstrapping the topic-dependent semantic classifiers from a small amount of labeled sentences. Experiments have been conducted in the context of Chinese public transportation information inquiry domain. The experimental results demonstrate the effectiveness of our proposed SLU framework and show the possibility to reduce human labeling efforts significantly. 
We analyze humorous spoken conversations from a classic comedy television show, FRIENDS, by examining acousticprosodic and linguistic features and their utility in automatic humor recognition. Using a simple annotation scheme, we automatically label speaker turns in our corpus that are followed by laughs as humorous and the rest as non-humorous. Our humor-prosody analysis reveals signiﬁcant differences in prosodic characteristics (such as pitch, tempo, energy etc.) of humorous and non-humorous speech, even when accounted for the gender and speaker differences. Humor recognition was carried out using standard supervised learning classiﬁers, and shows promising results signiﬁcantly above the baseline. 
In this paper we describe a novel distributed language model for N -best list re-ranking. The model is based on the client/server paradigm where each server hosts a portion of the data and provides information to the client. This model allows for using an arbitrarily large corpus in a very efﬁcient way. It also provides a natural platform for relevance weighting and selection. We applied this model on a 2.97 billion-word corpus and re-ranked the N -best list from Hiero, a state-of-theart phrase-based system. Using BLEU as a metric, the re-ranked translation achieves a relative improvement of 4.8%, signiﬁcantly better than the model-best translation. 
We develop admissible A* search heuristics for synchronous parsing with Inversion Transduction Grammar, and present results both for bitext alignment and for machine translation decoding. We also combine the dynamic programming hook trick with A* search for decoding. These techniques make it possible to ﬁnd optimal alignments much more quickly, and make it possible to ﬁnd optimal translations for the ﬁrst time. Even in the presence of pruning, we are able to achieve higher BLEU scores with the same amount of computation. 
This paper proposes a statistical, treeto-tree model for producing translations. Two main contributions are as follows: (1) a method for the extraction of syntactic structures with alignment information from a parallel corpus of translations, and (2) use of a discriminative, featurebased model for prediction of these targetlanguage syntactic structures—which we call aligned extended projections, or AEPs. An evaluation of the method on translation from German to English shows similar performance to the phrase-based model of Koehn et al. (2003). 
For transliterating foreign words into Chinese, the pronunciation of a source word is spelled out with Kanji characters. Because Kanji comprises ideograms, an individual pronunciation may be represented by more than one character. However, because different Kanji characters convey different meanings and impressions, characters must be selected carefully. In this paper, we propose a transliteration method that models both pronunciation and impression, whereas existing methods do not model impression. Given a source word and impression keywords related to the source word, our method derives possible transliteration candidates and sorts them according to their probability. We evaluate our method experimentally. 
In this paper we investigate unsupervised name transliteration using comparable corpora, corpora where texts in the two languages deal in some of the same topics — and therefore share references to named entities — but are not translations of each other. We present two distinct methods for transliteration, one approach using an unsupervised phonetic transliteration method, and the other using the temporal distribution of candidate pairs. Each of these approaches works quite well, but by combining the approaches one can achieve even better results. We believe that the novelty of our approach lies in the phonetic-based scoring method, which is based on a combination of carefully crafted phonetic features, and empirical results from the pronunciation errors of second-language learners of English. Unlike previous approaches to transliteration, this method can in principle work with any pair of languages in the absence of a training dictionary, provided one has an estimate of the pronunciation of words in text. 
The increasing ﬂow of information between languages has led to a rise in the frequency of non-native or loan words, where terms of one language appear transliterated in another. Dealing with such out of vocabulary words is essential for successful cross-lingual information retrieval. For example, techniques such as stemming should not be applied indiscriminately to all words in a collection, and so before any stemming, foreign words need to be identiﬁed. In this paper, we investigate three approaches for the identiﬁcation of foreign words in Arabic text: lexicons, language patterns, and n-grams and present that results show that lexicon-based approaches outperform the other techniques. 
In this paper we propose a machinelearning approach to paragraph boundary identiﬁcation which utilizes linguistically motivated features. We investigate the relation between paragraph boundaries and discourse cues, pronominalization and information structure. We test our algorithm on German data and report improvements over three baselines including a reimplementation of Sporleder & Lapata’s (2006) work on paragraph segmentation. An analysis of the features’ contribution suggests an interpretation of what paragraph boundaries indicate and what they depend on. 
In this paper we describe a coreference resolution method that employs a classiﬁcation and a clusterization phase. In a novel way, the clusterization is produced as a graph cutting algorithm, in which nodes of the graph correspond to the mentions of the text, whereas the edges of the graph constitute the conﬁdences derived from the coreference classiﬁcation. In experiments, the graph cutting algorithm for coreference resolution, called BESTCUT, achieves state-of-the-art performance. 
This paper presents a method of automatically constructing information extraction patterns on predicate-argument structures (PASs) obtained by full parsing from a smaller training corpus. Because PASs represent generalized structures for syntactical variants, patterns on PASs are expected to be more generalized than those on surface words. In addition, patterns are divided into components to improve recall and we introduce a Support Vector Machine to learn a prediction model using pattern matching results. In this paper, we present experimental results and analyze them on how well protein-protein interactions were extracted from MEDLINE abstracts. The results demonstrated that our method improved accuracy compared to a machine learning approach using surface word/part-of-speech patterns. 
How can proteins fold so quickly into their unique native structures? We show here that there is a natural analogy between parsing and the protein folding problem, and demonstrate that CKY can ﬁnd the native structures of a simpliﬁed lattice model of proteins with high accuracy. 
In this work we learn clusters of contextual annotations for non-terminals in the Penn Treebank. Perhaps the best way to think about this problem is to contrast our work with that of Klein and Manning (2003). That research used treetransformations to create various grammars with different contextual annotations on the non-terminals. These grammars were then used in conjunction with a CKY parser. The authors explored the space of different annotation combinations by hand. Here we try to automate the process — to learn the “right” combination automatically. Our results are not quite as good as those carefully created by hand, but they are close (84.8 vs 85.7). 
This paper presents a corpus-based account of structural priming in human sentence processing, focusing on the role that syntactic representations play in such an account. We estimate the strength of structural priming effects from a corpus of spontaneous spoken dialogue, annotated syntactically with Combinatory Categorial Grammar (CCG) derivations. This methodology allows us to test a range of predictions that CCG makes about priming. In particular, we present evidence for priming between lexical and syntactic categories encoding partially satisﬁed subcategorization frames, and we show that priming effects exist both for incremental and normal-form CCG derivations. 
We study unsupervised methods for learning reﬁnements of the nonterminals in a treebank. Following Matsuzaki et al. (2005) and Prescher (2005), we may for example split NP without supervision into NP[0] and NP[1], which behave differently. We ﬁrst propose to learn a PCFG that adds such features to nonterminals in such a way that they respect patterns of linguistic feature passing: each node’s nonterminal features are either identical to, or independent of, those of its parent. This linguistic constraint reduces runtime and the number of parameters to be learned. However, it did not yield improvements when training on the Penn Treebank. An orthogonal strategy was more successful: to improve the performance of the EM learner by treebank preprocessing and by annealing methods that split nonterminals selectively. Using these methods, we can maintain high parsing accuracy while dramatically reducing the model size. 
We investigate whether one can determine from the transcripts of U.S. Congressional ﬂoor debates whether the speeches represent support of or opposition to proposed legislation. To address this problem, we exploit the fact that these speeches occur as part of a discussion; this allows us to use sources of information regarding relationships between discourse segments, such as whether a given utterance indicates agreement with the opinion expressed by another. We ﬁnd that the incorporation of such information yields substantial improvements over classifying speeches in isolation. 
Combining ﬁne-grained opinion information to produce opinion summaries is important for sentiment analysis applications. Toward that end, we tackle the problem of source coreference resolution – linking together source mentions that refer to the same entity. The partially supervised nature of the problem leads us to deﬁne and approach it as the novel problem of partially supervised clustering. We propose and evaluate a new algorithm for the task of source coreference resolution that outperforms competitive baselines. 
Ranking documents or sentences according to both topic and sentiment relevance should serve a critical function in helping users when topics and sentiment polarities of the targeted text are not explicitly given, as is often the case on the web. In this paper, we propose several sentiment information retrieval models in the framework of probabilistic language models, assuming that a user both inputs query terms expressing a certain topic and also speciﬁes a sentiment polarity of interest in some manner. We combine sentiment relevance models and topic relevance models with model parameters estimated from training data, considering the topic dependence of the sentiment. Our experiments prove that our models are effective. 
This paper proposes an unsupervised lexicon building method for the detection of polar clauses, which convey positive or negative aspects in a speciﬁc domain. The lexical entries to be acquired are called polar atoms, the minimum human-understandable syntactic structures that specify the polarity of clauses. As a clue to obtain candidate polar atoms, we use context coherency, the tendency for same polarities to appear successively in contexts. Using the overall density and precision of coherency in the corpus, the statistical estimation picks up appropriate polar atoms among candidates, without any manual tuning of the threshold values. The experimental results show that the precision of polarity assignment with the automatically acquired lexicon was 94% on average, and our method is robust for corpora in diverse domains and for the size of the initial lexicon. 
We describe a probabilistic approach to content selection for meeting summarization. We use skipchain Conditional Random Fields (CRF) to model non-local pragmatic dependencies between paired utterances such as QUESTION-ANSWER that typically appear together in summaries, and show that these models outperform linear-chain CRFs and Bayesian models in the task. We also discuss different approaches for ranking all utterances in a sequence using CRFs. Our best performing system achieves 91.3% of human performance when evaluated with the Pyramid evaluation metric, which represents a 3.9% absolute increase compared to our most competitive non-sequential classiﬁer. 
Adapting language models across styles and topics, such as for lecture transcription, involves combining generic style models with topic-specific content relevant to the target document. In this work, we investigate the use of the Hidden Markov Model with Latent Dirichlet Allocation (HMM-LDA) to obtain syntactic state and semantic topic assignments to word instances in the training corpus. From these context-dependent labels, we construct style and topic models that better model the target document, and extend the traditional bag-of-words topic models to n-grams. Experiments with static model interpolation yielded a perplexity and relative word error rate (WER) reduction of 7.1% and 2.1%, respectively, over an adapted trigram baseline. Adaptive interpolation of mixture components further reduced perplexity by 9.5% and WER by a modest 0.3%. 
This paper presents a corrective model for speech recognition of inﬂected languages. The model, based on a discriminative framework, incorporates word ngrams features as well as factored morphological features, providing error reduction over the model based solely on word n-gram features. Experiments on a large vocabulary task, namely the Czech portion of the MALACH corpus, demonstrate performance gain of about 1.1–1.5% absolute in word error rate, wherein morphological features contribute about a third of the improvement. A simple feature selection mechanism based on χ2 statistics is shown to be effective in reducing the number of features by about 70% without any loss in performance, making it feasible to explore yet larger feature spaces. 
We investigate the problem of learning a part-of-speech (POS) lexicon for a resource-poor language, dialectal Arabic. Developing a high-quality lexicon is often the ﬁrst step towards building a POS tagger, which is in turn the front-end to many NLP systems. We frame the lexicon acquisition problem as a transductive learning problem, and perform comparisons on three transductive algorithms: Transductive SVMs, Spectral Graph Transducers, and a novel Transductive Clustering method. We demonstrate that lexicon learning is an important task in resourcepoor domains and leads to signiﬁcant improvements in tagging accuracy for dialectal Arabic. 
This paper explores the use of a character segment based character correction model, language modeling, and shallow morphology for Arabic OCR error correction. Experimentation shows that character segment based correction is superior to single character correction and that language modeling boosts correction, by improving the ranking of candidate corrections, while shallow morphology had a small adverse effect. Further, given sufficiently large corpus to extract a dictionary and to train a language model, word based correction works well for a morphologically rich language such as Arabic. 
Supervised and semi-supervised sense disambiguation methods will mis-tag the instances of a target word if the senses of these instances are not deﬁned in sense inventories or there are no tagged instances for these senses in training data. Here we used a model order identiﬁcation method to avoid the misclassiﬁcation of the instances with undeﬁned senses by discovering new senses from mixed data (tagged and untagged corpora). This algorithm tries to obtain a natural partition of the mixed data by maximizing a stability criterion deﬁned on the classiﬁcation result from an extended label propagation algorithm over all the possible values of the number of senses (or sense number, model order). Experimental results on SENSEVAL-3 data indicate that it outperforms SVM, a one-class partially supervised classiﬁcation algorithm, and a clustering based model order identiﬁcation algorithm when the tagged data is incomplete. 
User-supplied reviews are widely and increasingly used to enhance ecommerce and other websites. Because reviews can be numerous and varying in quality, it is important to assess how helpful each review is. While review helpfulness is currently assessed manually, in this paper we consider the task of automatically assessing it. Experiments using SVM regression on a variety of features over Amazon.com product reviews show promising results, with rank correlations of up to 0.66. We found that the most useful features include the length of the review, its unigrams, and its product rating. 
We present an approach for the joint extraction of entities and relations in the context of opinion recognition and analysis. We identify two types of opinion-related entities — expressions of opinions and sources of opinions — along with the linking relation that exists between them. Inspired by Roth and Yih (2004), we employ an integer linear programming approach to solve the joint opinion recognition task, and show that global, constraint-based inference can signiﬁcantly boost the performance of both relation extraction and the extraction of opinion-related entities. Performance further improves when a semantic role labeling system is incorporated. The resulting system achieves F-measures of 79 and 69 for entity and relation extraction, respectively, improving substantially over prior results in the area. 
Lexical features are key to many approaches to sentiment analysis and opinion detection. A variety of representations have been used, including single words, multi-word Ngrams, phrases, and lexicosyntactic patterns. In this paper, we use a subsumption hierarchy to formally deﬁne different types of lexical features and their relationship to one another, both in terms of representational coverage and performance. We use the subsumption hierarchy in two ways: (1) as an analytic tool to automatically identify complex features that outperform simpler features, and (2) to reduce a feature set by removing unnecessary features. We show that reducing the feature set improves performance on three opinion classiﬁcation tasks, especially when combined with traditional feature selection. 
We extended language modeling approaches in information retrieval (IR) to combine collaborative ﬁltering (CF) and content-based ﬁltering (CBF). Our approach is based on the analogy between IR and CF, especially between CF and relevance feedback (RF). Both CF and RF exploit users’ preference/relevance judgments to recommend items. We ﬁrst introduce a multinomial model that combines CF and CBF in a language modeling framework. We then generalize the model to another multinomial model that approximates the Polya distribution. This generalized model outperforms the multinomial model by 3.4% for CBF and 17.4% for CF in recommending English Wikipedia articles. The performance of the generalized model for three different datasets was comparable to that of a state-of-theart item-based CF method. 
Random Indexing is a vector space technique that provides an efﬁcient and scalable approximation to distributional similarity problems. We present experiments showing Random Indexing to be poor at handling large volumes of data and evaluate the use of weighting functions for improving the performance of Random Indexing. We ﬁnd that Random Index is robust for small data sets, but performance degrades because of the inﬂuence high frequency attributes in large data sets. The use of appropriate weight functions improves this signiﬁcantly. 
Markov order-1 conditional random ﬁelds (CRFs) and semi-Markov CRFs are two popular models for sequence segmentation and labeling. Both models have advantages in terms of the type of features they most naturally represent. We propose a hybrid model that is capable of representing both types of features, and describe efﬁcient algorithms for its training and inference. We demonstrate that our hybrid model achieves error reductions of 18% and 25% over a standard order-1 CRF and a semi-Markov CRF (resp.) on the task of Chinese word segmentation. We also propose the use of a powerful feature for the semi-Markov CRF: the log conditional odds that a given token sequence constitutes a chunk according to a generative model, which reduces error by an additional 13%. Our best system achieves 96.8% F-measure, the highest reported score on this test set. 
Web extraction systems attempt to use the immense amount of unlabeled text in the Web in order to create large lists of entities and relations. Unlike traditional IE methods, the Web extraction systems do not label every mention of the target entity or relation, instead focusing on extracting as many different instances as possible while keeping the precision of the resulting list reasonably high. URES is a Web relation extraction system that learns powerful extraction patterns from unlabeled text, using short descriptions of the target relations and their attributes. The performance of URES is further enhanced by classifying its output instances using the properties of the extracted patterns. The features we use for classification and the trained classification model are independent from the target relation, which we demonstrate in a series of experiments. In this paper we show how the introduction of a simple rule based NER can boost the performance of URES on a variety of relations. We also compare the performance of URES to the performance of the stateof-the-art KnowItAll system, and to the performance of its pattern learning component, which uses a simpler and less powerful pattern language than URES. 
We present an investigation of recently proposed character and word sequence kernels for the task of authorship attribution based on relatively short texts. Performance is compared with two corresponding probabilistic approaches based on Markov chains. Several conﬁgurations of the sequence kernels are studied on a relatively large dataset (50 authors), where each author covered several topics. Utilising Moffat smoothing, the two probabilistic approaches obtain similar performance, which in turn is comparable to that of character sequence kernels and is better than that of word sequence kernels. The results further suggest that when using a realistic setup that takes into account the case of texts which are not written by any hypothesised authors, the amount of training material has more inﬂuence on discrimination performance than the amount of test material. Moreover, we show that the recently proposed author unmasking approach is less useful when dealing with short texts. 
Entity annotation involves attaching a label such as ‘name’ or ‘organization’ to a sequence of tokens in a document. All the current rule-based and machine learningbased approaches for this task operate at the document level. We present a new and generic approach to entity annotation which uses the inverse index typically created for rapid key-word based searching of a document collection. We deﬁne a set of operations on the inverse index that allows us to create annotations deﬁned by cascading regular expressions. The entity annotations for an entire document corpus can be created purely of the index with no need to access the original documents. Experiments on two publicly available data sets show very signiﬁcant performance improvements over the documentbased annotators. 
Information Extraction (IE) is the task of extracting knowledge from unstructured text. We present a novel unsupervised approach for information extraction based on graph mutual reinforcement. The proposed approach does not require any seed patterns or examples. Instead, it depends on redundancy in large data sets and graph based mutual reinforcement to induce generalized “extraction patterns”. The proposed approach has been used to acquire extraction patterns for the ACE (Automatic Content Extraction) Relation Detection and Characterization (RDC) task. ACE RDC is considered a hard task in information extraction due to the absence of large amounts of training data and inconsistencies in the available data. The proposed approach achieves superior performance which could be compared to supervised techniques with reasonable training data. 
When a machine learning-based named entity recognition system is employed in a new domain, its performance usually degrades. In this paper, we provide an empirical study on the impact of training data size and domain information on the performance stability of named entity recognition models. We present an informative sample selection method for building high quality and stable named entity recognition models across domains. Experimental results show that the performance of the named entity recognition model is enhanced signiﬁcantly after being trained with these informative samples. 
In this paper we describe and evaluate several statistical models for the task of realization ranking, i.e. the problem of discriminating between competing surface realizations generated for a given input semantics. Three models (and several vari- ants) are trained and tested: an Ò-gram language model, a discriminative maximum entropy model using structural information (and incorporating the language model as a separate feature), and ﬁnally an SVM ranker trained on the same feature set. The resulting hybrid tactical generator is part of a larger, semantic transfer MT system. 
In this paper, we propose a sentence ordering algorithm using a semi-supervised sentence classification and historical ordering strategy. The classification is based on the manifold structure underlying sentences, addressing the problem of limited labeled data. The historical ordering helps to ensure topic continuity and avoid topic bias. Experiments demonstrate that the method is effective. 1. Introduction Sentence ordering has been a concern in text planning and concept-to-text generation (Reiter et al., 2000). Recently, it has also drawn attention in multi-document summarization (Barzilay et al., 2002; Lapata, 2003; Bollegala et al., 2005). Since summary sentences generally come from different sources in multi-document summarization, an optimal ordering is crucial to make summaries coherent and readable. In general, the strategies for sentence ordering in multi-document summarization fall in two categories. One is chronological ordering (Barzilay et al., 2002; Bollegala et al., 2005), which is based on time-related features of the documents. However, such temporal features may be not available in all cases. Furthermore, temporal inference in texts is still a problem, in spite of some progress in automatic disambiguation of temporal information (Filatova  et al., 2001). Another strategy is majority ordering (MO) (McKeown et al., 2001; Barzilay et al., 2002), in which each summary sentence is mapped to a theme, i.e., a set of similar sentences in the documents, and the order of these sentences determines that for summary sentences. To do that, a directed theme graph is built, in which if a theme A occurs behind another theme B in a document, B is linked to A no matter how far away they are located. However, this may lead to wrong theme correlations, since B’s occurrence may rely on a third theme C and have nothing to do with A. In addition, when outputting theme orders, MO uses a kind of heuristic that chooses a theme based on its in-out edge difference in the directed theme graph. This may cause topic disruption, since the next choice may have no link with previous choices. Lapata (2003) proposed a probabilistic ordering (PO) method for text structuring, which can be adapted to majority ordering if the training texts are those documents to be summarized. The primary evidence for the ordering are informative features of sentences, including words and their grammatical dependence relations, which needs reliable parsing of the text. Unlike in MO, selection of the next sentence here is based on the most recent one. However, this may lead to topic bias: i.e. too many sentences on the same topic. In this paper, we propose a historical ordering (HO) strategy, in which the selection of the next sentence is based on the whole history of selection, not just the most recent choice. This  526 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 526–533, Sydney, July 2006. c 2006 Association for Computational Linguistics  strategy helps to ensure continuity of topics but to avoid topic bias at the same time. To do that, we need to map summary sentences to those in documents. We formalize this as a kind of classification problem, with summary sentences as class labels. Since there are very few (only one) labeled examples for each class, we adopt a kind of semi-supervised classification method, which makes use of the manifold structure underlying the sentences to do the classification. A common assumption behind this learning paradigm is that the manifold structure among the data, revealed by higher density, provides a global comparison between data points (Szummer et al., 2001; Zhu et al., 2003; Zhou et al., 2003). Under such an assumption, even one labeled example is enough for classification, if only the structure is determined. The remainder of the paper is organized as follows. In section 2, we give an overview of the proposed method. In section 3~5, we talk about the method including sentence networks, classification and ordering. In section 6, we present experiments and evaluations. Finally in section 7, we give some conclusions and future work. 2. Overview Fig. 1 gives the overall structure of the proposed method, which includes three modules: construction of sentence networks, sentence classification and sentence ordering. Sentence network construction Sentence classification Summary sentence ordering Fig. 1. Algorithm Overview The first step is to build a sentence neighborhood  network with weights on edges, which can serve as the basis for a Markov random walk (Tishby et al., 2000). The neighborhood is based on similarity between sentences, and weights on edges can be seen as transition probabilities for the random walk. From this network, we can derive new representations for sentences. The second step is to make a classification of sentences, with each summary sentence as a class label. Since only one labeled example exists for each class, we use a semi-supervised method based on a Markov random walk to reveal the manifold structure for the classification. The third step is to order summary sentences according to the original positions of their partners in the same class. During this process, the next selection of a sentence is based on the whole history of selection, i.e., the association of the sentence with all those already selected.  3. Sentence Network Construction  Suppose S is the set of all sentences in the documents and a summary (a summary sentence may be not a document sentence), let S={s1, s2, …, sN} with a distance metric d(si,sj), the distance between two sentences si and sj, which is based on the Jensen-Shannon divergence (Lin, 1991). We construct a graph with sentences as points by sorting the distances among the points in an ascending order and repeatedly connecting two points according to the order until a connected graph is obtained. Then, we assign a weight wi,j, as in (1), to each edge based on the distance.  1) wi, j = exp(−d (si , s j ) / δ )  The weights are symmetric, wi,i=1 and wi,j=0 for  all non-neighbors (δ is set as 0.6 in this work). 2)  is the one-step transition probability p(si, sj) from  si to sj based on weights of neighbors.  ∑ 2 )  p(si , s j ) =  wi, j wi,k  k  527  × Let M be the N N matrix and Mi,j= p(si, sj), then Mt is the tth Markov random walk matrix, whose i, j-th entry is the probability pt(si, sj) of the transition from si to sj after t steps. In this way, each sentence sj is associated with a vector of conditional probabilities pt(si, sj), i=1, …, N, which form a new manifold-based representation for sj. With such representations, sentences are close whenever they have a similar distribution over the starting points. Notice that the representations depend on the step parameter t (Tishby et al., 2000). With smaller values of t, unlabeled points may be not connected with labeled ones; with bigger values of t, the points may be indistinguishable. So, an appropriate t should be estimated. 4. Sentence Classification Suppose s1, s2, …, sL are summary sentences and their labels are c1, c2, …, cL respectively. In our ≤ ≤ case, each summary sentence is assigned with a unique class label ci, 1 i L. This also means that for each class ci, there is only one labeled example, i.e., the summary sentence, si. Let S={(s1, c1), (s2, c2), …, (sL, cL), sL+1,…, sN}, then the task of sentence classification is to infer the labels for unlabeled sentences, sL+1,…, sN. Through the classification, we can get similar sentences for each summary sentence. To do that, ≤ ≤ ≤ ≤ we assume that each sentence has a distribution p(ck|si), 1 k L, 1 i N, and these probabilities are to be estimated from the data. Seeing a sentence as a sample from the t step Markov random walk in the sentence graph, we have the following interpretation of p(ck|si). ∑ 3) p(ck | si ) = p(ck | s j ) pt ( j,i) j This means that the probability of si belonging to ck is dependent on the probabilities of those sentences belonging to ck which will transit to si after t steps and their transition probabilities. With the conditional log-likelihood of labeled sentences 4) as the estimation criterion, we can  use the EM algorithm to estimate p(ck|si), in which the E-step and M-step are 5) and 6)  respectively.  L  L  N  ∑ ∑ ∑ 4) log p(ck | sk ) = log p(ck | s j )pt ( j, k)  k =1  k =1  j =1  L ∑ 5) p(si | sk , ck ) = p(ck | si ) pt (i, k) / p(ck | si ) pt (i, k) k =1  ∑ 6) p(ck | si ) = p(si | sk , ck ) / p(si | sk , ck ) 1≤k ≤L  The final class ci for si is given in 7).  7) ci = arg max ck p(ck | si )  p(ci|si) is called the membership probability of si. After classification, each sentence is assigned a  label according to 7).  One key problem in this setting is to estimate  the parameter t. A possible strategy for that is by  cross validation, but it needs a large amount of  labeled data. Here, following Szummer et al.,  2001, we use marginal difference of probabilities  of sentences falling in different classes as the  estimation criterion, which is given in 8).  ∑ ∑ 8)m(S)  =  (L max  s∈S  1≤k ≤L  p(ck  |  s) − p(ck 1≤k ≤L  |  s))  To maximize 8), we can get an appropriate value  for the parameter t, which means that a better t  should make sentences belong to some classes  more prominently. Notice that the classes  represented by summary sentences may be  incomplete for all the sentences occurring in the  documents, so some sentences will belong to the  classes without obviously different probabilities.  To avoid such sentences in the estimation of t, we  only choose the top (40%) sentences in a class  based on their membership probabilities.  5. Sentence Ordering  After sentence classification, we get a class of similar sentences for each summary sentence, which is also a member of the class. With these sentence classes, we create a directed class graph based on the order of their member sentences in documents. In the graph, each sentence class is a  528  node, and there exists a directed edge ei,j from one node ci to another cj if and only if there is si in ci immediately appearing before sj in cj in the documents (the sentences not in classes are  neglected). The weight of ei,j, Fi,j, captures the frequency of such occurrence. We add one  additional node denoting an initial class c0, and it links to each class with a directed edge e0,j, the weight F0,j of which is the frequency of the member sentences of the class appearing at the  beginning of the documents.  Suppose the input is the class graph G=<C, E>,  where C = {c1, c2, …, cL} is the set of the classes, E={ei,j|1≤i, j≤L} is the set of the directed edges, and o is the ordering of the classes. Fig. 2 gives  the ordering algorithm.  --------------------------------------------------  ← i)  ck  ←  max ci ∈C  F0,i  ii) o o ck  iii) For all ci in C, F0,i ← F0,i + Fk,i  ≠ iv) Remove ck from C and ek,j and ei,k from E; v) Repeat i)-iv) while C { c0}  vi) Return the order o.  --------------------------------------------------------  Fig. 2 Ordering algorithm  In the algorithm, there are two main steps. Step i) selects the class whose member sentences occur most frequently immediately after those in c0. Step iii) updates the weights of the edges e0,i. In fact, it can be seen as merge of the original c0 and ck, and in this sense the updated c0 represents the history of selections. In contrast to the MO algorithm, the ordering algorithm here (HO) uses immediate back-front co-occurrence, while the MO algorithm uses relative back-front locations. On the other hand, the selection of a class is dependent on previous selections in HO, while in MO, the selection of a class is mainly dependent on its in-out edge difference. In contrast to the PO algorithm, the selection of a class in HO is dependent on all previous selections, while in PO, the selection is only  related to the most recent one. As an example, Fig. 3 gives an initial class graph. The output orderings by PO and HO are [c1, c3, c4, c2] and [c1, c3, c2, c4] respectively. The difference lies in whether to select c4 or c2 after selection of c3. PO selects c4 since it only considers the most recent selection, while HO selects c2 because it considers all previous selections including c1.  5 c2  c0 9 c1  6 c3  
This paper presents an empirical evaluation of the quality of publicly available large-scale knowledge resources. The study includes a wide range of manually and automatically derived large-scale knowledge resources. In order to establish a fair and neutral comparison, the quality of each knowledge resource is indirectly evaluated using the same method on a Word Sense Disambiguation task. The evaluation framework selected has been the Senseval-3 English Lexical Sample Task. The study empirically demonstrates that automatically acquired knowledge resources surpass both in terms of precision and recall the knowledge resources derived manually, and that the combination of the knowledge contained in these resources is very close to the most frequent sense classiﬁer. As far as we know, this is the ﬁrst time that such a quality assessment has been performed showing a clear picture of the current state-of-the-art of publicly available wide coverage semantic resources. 
Word clustering is important for automatic thesaurus construction, text classiﬁcation, and word sense disambiguation. Recently, several studies have reported using the web as a corpus. This paper proposes an unsupervised algorithm for word clustering based on a word similarity measure by web counts. Each pair of words is queried to a search engine, which produces a co-occurrence matrix. By calculating the similarity of words, a word cooccurrence graph is obtained. A new kind of graph clustering algorithm called Newman clustering is applied for efﬁciently identifying word clusters. Evaluations are made on two sets of word groups derived from a web directory and WordNet. 
Co-occurrence analysis has been used to determine related words or terms in many NLP-related applications such as query expansion in Information Retrieval (IR). However, related words are usually determined with respect to a single word, without relevant information for its application context. For example, the word “programming” may be considered to be strongly related to “Java”, and applied inappropriately to expand a query on “Java travel”. To solve this problem, we propose to add another context word in the relation to specify the appropriate context of the relation, leading to term relations of the form “(Java, travel) → Indonesia”. The extracted relations are used for query expansion in IR. Our experiments on several TREC collections show that this new type of context-dependent relations performs much better than the traditional co-occurrence relations. 1. Introduction A query usually is a poor expression of an information need. This is not only due to its short length (usually a few words), but also due to the inability of users to provide the best terms to describe their information need. At best, one can expect that some, but not all, relevant terms are used in the query. Query expansion thus aims to improve query expression by adding related terms to the query. However, the effect of query expansion is strongly determined by the term relations used (Peat and Willett, 1991). For example, even if “programming” is strongly related to “Java”, if this relation is used to expand a query on “Java travel”, the retrieval result will likely deteriorate because the irrelevant term “programming” is introduced,  leading to the retrieval of irrelevant documents about “programming”. A number of attempts have been made to deal with the problem of selecting appropriate expansion terms. For example, Wordnet has been used in (Voorhees, 1994) to determine the expansion terms. However, the experiments did not show improvement on retrieval effectiveness. Many experiments have been carried out using associative relations extracted from term cooccurrences; but they showed variable results (Peat and Willett, 1991). In (Qiu and Frei, 1993), it is observed that one of the reasons is that one tried to determine expansion terms according to each original query term separately, which may introduce much noise. Therefore, they proposed to determine the expansion terms by summing up the relations of a candidate expansion term to each of the query terms. In so doing, a candidate expansion term is preferred if it has a strong relationship with many of the query terms. However, it is still difficult to prevent the expansion process from adding “programming” to a query on “Java travel” because of its very strong relation with “Java”. The approach used in (Qiu and Frei, 1993) indeed tries to correct a handicap inherent in the relations: as term relations are created between two single words such as “Java → programming”, no information is available to help determine the appropriate context to apply it. The approach used in (Qiu and Frei, 1993) can simply alleviate the problem without solving it radically. In this paper, we argue that the solution lies in the relations themselves. They have to contain more information to help determine the appropriate context to apply them. We thus propose a way to add some context information into the relations: we introduce an additional word into the condition part of the relation, such as “(Java, computer) → programming”, which  551 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 551–559, Sydney, July 2006. c 2006 Association for Computational Linguistics  means “programming” is related to “(Java, computer)” together. In so doing, we would be able to prevent from extracting and applying a relation such as “(Java, travel) → programming”. In this paper, we will test the extracted relations in query expansion for IR. We choose to implement query expansion within the language modeling (LM) framework because of its flexibility and high performance. The experiments on several TREC collections will show that our query expansion approach can bring large improvements in retrieval effectiveness. In the following sections, we will first review some of the relevant approaches on query expansion and term relation extraction. Then we will describe our general IR models and the extraction of term relations. The experimental results will be reported and finally some conclusions will be drawn. 2. Query Expansion and Term Relations It has been found that a key factor that determines the effect of query expansion is the selection of appropriate expansion terms (Peat and Willett, 1991). To determine expansion terms, one possible resource is thesauri constructed manually, such as Wordnet. Thesauri contain manually validated relations between terms, which can be used to suggest related terms. (Voorhees, 1994) carried out a series of experiments on selecting related terms (e.g. synonyms, hyonyms, etc.) from Wordnet. However, the experiments did not show that this can improve retrieval effectiveness. Some of the reasons are as follows: Although Wordnet contains many relations validated by human experts, the coverage is far from complete for the purposes of IR: not only linguistically motivated relations, but also association relations, are useful in IR. Another problem is the lack of information about the appropriate context to apply relations. For example, Wordnet contains two synsets for “computer”, one for the sense of “machine” and another for “human expert”. It is difficult to automatically select the correct synset to expand the word “computer” even if we know that the query’s area is computer science. Another often used resource is associative relations extracted from co-occurrences: two terms that co-occur frequently are thought to be associated to each other (Jing and Croft, 1994). However, co-occurrence relations are noisy:  Frequently co-occurring terms are not necessarily related. On the other hand, they can also miss true relations. The most important problem is still that of ambiguity: when one term is associated with another, it may be related for one sense and not for other possible senses. It is then difficult to determine when the relation applies. In most of the previous studies, relations extracted are restricted between one word and another. This limitation makes the relations ambiguous, and their utilization in query expansion often introduces undesired terms. We believe that the key to make a relation less ambiguous is to add some contextual information. In an attempt to select better expansion terms, (Qiu and Frei, 1993) proposed the following approach to select expansion terms: terms are selected according to their relation to the whole query, which is calculated as the sum of their relations to each of the query terms. Therefore, a term that is related to several query terms will be favored. In a similar vein, (Bai et al. 2005) also try to determine the relationship of a word to a group of words by combining its relationships to each of the words in the group. This can indeed select better expansion terms. The consideration of other query terms produces a weak contextual effect. However, this effect is limited due to the nature of the relations extracted, in which a term depends on only one other term. Much of the noise in the sets will remain after selection. For a query composed of several words, what we would really like to have is a set of terms that are related to all the words taken together (and not separately). By combining words in the condition part such as “(Java, travel)” or “(base, bat)”, each word will serve as a context to the other in order to constrain the related terms. In these cases, we would expect that “hotel”, “island” or “Indonesia” would co-occur much more often with “(Java, travel)” than “programming”, and “ball”, “catcher” etc. cooccur much more often with “(base, bat)” than “animal” or “foundation”. One naturally would suggest that compound terms can be used for this purpose. However, for many queries, it is difficult to form a legitimate compound term. Even if we can detect one occurrence of a compound, we may miss others that use its variants. For example, if “Java travel” is used as a query, we will likely be able to consider it as a compound term. The same compound (or its variant) would be difficult to  552  detect in a document talking about traveling to Java: the two words may appear at some distance or not in some specific syntactic structure as required in (Lin, 1997). This will lead to the problem of mismatching between document and query. In fact, compound terms are not the only way to add contextual information to a word. By putting two words together (without forming a compound term), we usually obtain a more precise sense for each word. For example, from “Java travel”, we can guess that the intended meaning is likely related to “traveling to Java Island”. People will not interpret this combination in the sense of “Java programming”. In the same way, people would not consider “animal” to be a related term to “base, bat”. These examples show that in a combination of words, each word indeed serves to specify a context to interpret another word. It then suggests the following approach: we can adjunct some additional word(s) in the condition part of a relation, such as “(Java, travel) → Indonesia”, which means “Indonesia” is related to “(Java, travel)” together. It is expected that one would not obtain “(Java, travel) → programming”. Owing to the context effect explained above, we will call the relations with multiple words in the condition part context-dependent relations. In order to limit the computation complexity, we will only consider adding one additional word into relations. The proposed approach follows the same principle as (Yarowsky, 1995), which tried to determine the appropriate word sense according to one relevant context word. However, the requirement for query expansion is less than word sense disambiguation: we do not need to know the exact word sense to make expansion. We only need to determine the relevant expansion terms. Therefore, there is no need to determine manually a set of seeds before the learning process takes place. To some extent, the proposed approach is also related to (Schütze and Pedersen, 1997), which calculate term similarity according to the words appearing in the same context, or to second-order co-occurrences. However, a key difference is that (Schütze and Pedersen, 1997) consider only separate context words, while we consider multiple context words together. Once term relations are determined, they will be used in query expansion. The basic IR process  will be implemented in a language modeling framework. This framework is chosen for its flexibility to integrate term relations. Indeed, the LM framework has proven to be capable of integrating term relations and query expansion (Bai et al., 2005; Berger and Lafferty, 1999; Zhai and Lafferty, 2001). However, none of the above studies has investigated the extraction of strong context-dependent relations from text collections. In the next section, we will describe the general LM framework and our query expansion models. Then the extraction of term relation will be explained.  3. Context-Dependent Query Expansion in Language Models  The basic IR approach based on LM (Ponte and Croft, 1998) determines the score of relevance of a document D by its probability to generate the query Q. By assuming independence between query terms, we have:  ∏ ∑ P(Q | D) = P(wi | D) ∝ log P(wi | D)  wi ∈Q  wi ∈Q  where P(wi | D) denotes the probability of a word in the language model of the document D. As no ambiguity will arise, we will use D to mean both the language model of the document and the document itself (similarly for a query model and a query Q).  Another score function is based on KLdivergence or cross entropy between the document model and the query model:  ∑ score (D, Q) = P(wi | Q) log P(wi | D) wi ∈V  where V is the vocabulary. Although we have both document and query models in the above formulation, usually only the document model is smoothed, while the query model uses Maximum Likelihood Estimation (MLE) PML(wi | Q) . Then we have:  ∑ score(D, Q) = PML (wi | Q) log P(wi | D) wi ∈Q  However, it is obvious that a distance (KLdivergence) measured between a short query of a few words and a document cannot be precise. A better expression would contain all the related terms. The construction of a better query expression is the very motivation for query expansion in traditional IR systems. It is the same in LM for IR: to create a better query expression (model) to be able to measure the distance to a  553  document in a more precise way. The key to creating the new model is the integration of term relations.  3.1 LM for Query Expansion  Term relations have been used in several recent language models in IR. (Berger and Lafferty, 1999) proposed a translation model that expands the document model. The same approach can also be used to expand the query model. Following (Berger and Lafferty, 1999), we arrive at the first expansion model as follows, which has also been used in (Bai et al., 2005):  Model 1: Context-independent query expansion model (CIQE)  ∑ ∑ PR (wi | Q) = PR (wi , q j | Q) = PR (wi | q j )PML (q j | Q)  q j ∈V  q j ∈Q  In this model, each original query term qj is expanded by related terms wi. The relations between them are determined by PR (wi | q j ) . We  will explain how this probability is defined in  Section 3.2. However, we can already see here  that wi is determined solely by one of the query term qj. So, we call this model “contextindependent query expansion model” (CIQE).  The above expanded query model enables us  to obtain new related expansion terms, to which  we also have to add the original query. This can  be obtained through the following smoothing:  P(wi | Q) = λ1 PML (wi | Q) + ∑ (1 − λ1 ) PR (wi | q j )PML (q j | Q) (1) q j ∈Q  where λ1 is a smoothing parameter. However, if the query model is expanded on all the vocabulary (V), the query evaluation will be very time consuming because the query and the document have to be compared on every word (dimension). In practice, we observe that only a small number of terms have strong relations with a given term, and the terms having weak relations usually are not truly related. So we can limit the expansion terms only to the strongly related ones. By doing this, we can also expect to filter out some noise and considerably reduce the retrieval time. Suppose that we have selected a set E of strong expansion terms. Then we have:  ∑ score(D, Q) = P(wi | Q) log P(wi | D)  wi ∈V  ∑ ≈  P(wi | Q) log P(wi | D)  wi ∈E ∪Q  This query expansion method uses the same principle as (Qiu and Frei, 1993), but in a LM setting: the selected expansion terms are those that are strongly related to all the query terms (this is what the summation means). The approach used in (Bai et al., 2005) is slightly different: A context vector is first built for each word; then a context vector for a group of words (e.g. a multi-word query) is composed from the context vectors of the words of the group; finally related terms to the group of words are determined according to the similarity of their context vectors to that of the group. This last step uses second-order co-occurrences similarly to (Schütze and Pedersen, 1997). In both (Qiu and Frei, 1993) and (Bai et al., 2005), the terms related to a group of words are determined from their relations to each of the words in the group, while the latter relations are extracted separately. Irrelevant expansion terms can be retained. As we showed earlier, in many cases, when one additional word is used with another word, the sense of each of them can usually be better determined. This additional word may be sufficient to interpret correctly many multi-word user queries. Therefore, our goal is to extract stronger context-dependent relations of the form (qj qk) → wi, or to build a probability function PR (wi | qjqk ) . Once this function is determined, it can be integrated into a new language model as follows.  Model 2: Context-dependent query expansion model (CDQE)  ∑ PR (wi | Q) =  PR (wi | q jqk )P(q jqk | Q)  q j ,qk ∈V  ∑ ≈  PR (wi | q jqk )P(q jqk | Q)  q j ,qk ∈Q  As PR(wi | q jqk ) is a relation with two terms as  condition, we will also call it a biterm relation. The name “biterm” is due to (Srikanth and Srihari, 2002), which means two terms cooccurring within some distance. Similarly, PR (wi | q j ) will be called unigram relation. The corresponding query models will be called biterm relation model and unigram relation model. As in general LM, the biterm relation model can be smoothed with a unigram model. Then we have the following score function:  PR (wi | Q) = λ2 PML (wi | Q) +  ∑ (1 − λ2 )  PR (wi | q j qk )P(q j q k | Q) (2)  q j ,qk ∈Q  554  where λ2 is another smoothing parameter.  3.2 Extraction of Term Relations  The key problem now is to obtain the relations we need: PR (wi | wj ) and PR (wi | wjwk ) . For the first probability, as in many previous studies, we exploit term co-occurrences. PR (wi | wj ) could be built as a traditional bigram model. However, this is not a good approach for IR because two related terms do not necessarily co-occur side by side. They often appear at some distance. Therefore, this model is indeed a biterm model (Srikanth and Srihari, 2002), i.e., we allow two terms be separated within some distance. We use the following formula to determine this probability:  ∑ PR (wi | w j ) =  c(wi , w j ) c(wl , wj )  wl  where c(wi, wj ) is the frequency of co-occurrence  of the biterm (wi , w j ) , i.e. two terms in the same  window of fixed size across the collection. In our case, we set the window size at 10 (because this size turned out to be reasonable in our pilot experiments). For P(wi | wjwk ) , we further extend the biterm to triterm, and we use the frequency of cooccurrences of three terms c(wi, wj , wk ) within the same windows in the document collection:  ∑ PR(wi | wjwk ) =  c(wi , wj , wk ) c(wl , wj , wk )  wl  The number of relations determined in this  way can be very large. The upper bound for  P(wi | wj ) and P(wi | w j wk ) are respectively O(|V|2) and O(|V|3). However, many relations  have very low probabilities and are often noise.  As we only consider a subset of strong expansion  terms, the relations with low probability are  almost never used. Therefore, we set two filtering  criteria:  • The biterm in the condition of a relation should be higher than a threshold (10 in our case);  • The probability of a relation should be higher than another threshold (0.0001 in our case).  • One more filtering criterion is mutual information (MI), which reflects the relatedness of two terms in their combination (w j , wk ) . To keep a relation P(wi | w j wk ) , we  require (wj , wk ) be a meaningful combination. We use the following pointwise MI (Church and Hanks 1989):  MI  (w  j  ,  wk  )  =  log  P(w j , wk ) P(w j )P(wk  )  We only keep meaningful combinations such that MI(wj , wk ) > 0 .  By these filtering criteria, we are able to reduce considerably the number of biterms and triterms. For example, on a collection of about 200MB, with a vocabulary size of about 148K, we selected only about 2.7M useful biterms and about 137M triterms, which remain tractable.  3.3 Probability of Biterms  In LM used in IR, each query term is attributed the same weight. This is equivalent to a uniform probability distribution, i.e.:  P(qi  |  Q)  =  |  
We propose a general method for reranker construction which targets choosing the candidate with the least expected loss, rather than the most probable candidate. Different approaches to expected loss approximation are considered, including estimating from the probabilistic model used to generate the candidates, estimating from a discriminative model trained to rerank the candidates, and learning to approximate the expected loss. The proposed methods are applied to the parse reranking task, with various baseline models, achieving signiﬁcant improvement both over the probabilistic models and the discriminative rerankers. When a neural network parser is used as the probabilistic model and the Voted Perceptron algorithm with data-deﬁned kernels as the learning algorithm, the loss minimization model achieves 90.0% labeled constituents F1 score on the standard WSJ parsing task. 
We present an unsupervised learning approach to disambiguate various relations between name entities by use of various lexical and syntactic features from the contexts. It works by calculating eigenvectors of an adjacency graph’s Laplacian to recover a submanifold of data from a high dimensionality space and then performing cluster number estimation on the eigenvectors. This method can address two difﬁculties encoutered in Hasegawa et al. (2004)’s hierarchical clustering: no consideration of manifold structure in data, and requirement to provide cluster number by users. Experiment results on ACE corpora show that this spectral clustering based approach outperforms Hasegawa et al. (2004)’s hierarchical clustering method and a plain k-means clustering method. 
In this paper we show that generative models are competitive with and sometimes superior to discriminative models, when both kinds of models are allowed to learn structures that are optimal for discrimination. In particular, we compare Bayesian Networks and Conditional loglinear models on two NLP tasks. We observe that when the structure of the generative model encodes very strong independence assumptions (a la Naive Bayes), a discriminative model is superior, but when the generative model is allowed to weaken these independence assumptions via learning a more complex structure, it can achieve very similar or better performance than a corresponding discriminative model. In addition, as structure learning for generative models is far more efﬁcient, they may be preferable for some tasks. 
This paper explores the use of two graph algorithms for unsupervised induction and tagging of nominal word senses based on corpora. Our main contribution is the optimization of the free parameters of those algorithms and its evaluation against publicly available gold standards. We present a thorough evaluation comprising supervised and unsupervised modes, and both lexical-sample and all-words tasks. The results show that, in spite of the information loss inherent to mapping the induced senses to the gold-standard, the optimization of parameters based on a small sample of nouns carries over to all nouns, performing close to supervised systems in the lexical sample task and yielding the second-best WSD systems for the Senseval-3 all-words task. 
In this paper we approach word sense disambiguation and information extraction as a uniﬁed tagging problem. The task consists of annotating text with the tagset deﬁned by the 41 Wordnet supersense classes for nouns and verbs. Since the tagset is directly related to Wordnet synsets, the tagger returns partial word sense disambiguation. Furthermore, since the noun tags include the standard named entity detection classes – person, location, organization, time, etc. – the tagger, as a by-product, returns extended named entity information. We cast the problem of supersense tagging as a sequential labeling task and investigate it empirically with a discriminatively-trained Hidden Markov Model. Experimental evaluation on the main sense-annotated datasets available, i.e., Semcor and Senseval, shows considerable improvements over the best known “ﬁrst-sense” baseline. 
Named-entity recognition systems extract entities such as people, organizations, and locations from unstructured text. Rather than extract these mentions in isolation, this paper presents a record extraction system that assembles mentions into records (i.e. database tuples). We construct a probabilistic model of the compatibility between ﬁeld values, then employ graph partitioning algorithms to cluster ﬁelds into cohesive records. We also investigate compatibility functions over sets of ﬁelds, rather than simply pairs of ﬁelds, to examine how higher representational power can impact performance. We apply our techniques to the task of extracting contact records from faculty and student homepages, demonstrating a 53% error reduction over baseline approaches. 
We present two discriminative methods for name transliteration. The methods correspond to local and global modeling approaches in modeling structured output spaces. Both methods do not require alignment of names in different languages – their features are computed directly from the names themselves. We perform an experimental evaluation of the methods for name transliteration from three languages (Arabic, Korean, and Russian) into English, and compare the methods experimentally to a state-of-theart joint probabilistic modeling approach. We find that the discriminative methods outperform probabilistic modeling, with the global discriminative modeling approach achieving the best performance in all languages. 
The end-to-end performance of natural language processing systems for compound tasks, such as question answering and textual entailment, is often hampered by use of a greedy 1-best pipeline architecture, which causes errors to propagate and compound at each stage. We present a novel architecture, which models these pipelines as Bayesian networks, with each low level task corresponding to a variable in the network, and then we perform approximate inference to ﬁnd the best labeling. Our approach is extremely simple to apply but gains the beneﬁts of sampling the entire distribution over labels at each stage in the pipeline. We apply our method to two tasks – semantic role labeling and recognizing textual entailment – and achieve useful performance gains from the superior pipeline architecture. 
This paper discusses a novel probabilistic synchronous TAG formalism, synchronous Tree Substitution Grammar with sister adjunction (TSG+SA). We use it to parse a language for which there is no training data, by leveraging off a second, related language for which there is abundant training data. The grammar for the resource-rich side is automatically extracted from a treebank; the grammar on the resource-poor side and the synchronization are created by handwritten rules. Our approach thus represents a combination of grammar-based and empirical natural language processing. We discuss the approach using the example of Levantine Arabic and Standard Arabic. 
We present an operational framework allowing to express a large scale Tree Adjoining Grammar (TAG) by using higher level operational constraints on tree descriptions. These constraints ﬁrst meant to guarantee the well formedness of the grammatical units may also be viewed as a way to put model theoretic syntax at work through an efﬁcient ofﬂine grammatical compilation process. Our strategy preserves TAG formal properties, hence ensures a reasonable processing efﬁciency. 
We present an initial investigation into the use of a metagrammar for explicitly sharing abstract grammatical speciﬁcations among languages. We deﬁne a single class hierarchy for a metagrammar which allows us to automatically generate grammars for different languages from a single compact metagrammar hierarchy. We use as our linguistic example the verbsecond phenomenon, which shows considerable variation while retaining a basic property, namely the fact that the verb can appear in one of two positions in the clause. 
 In this paper, we argue that in it-clefts as in It was Ohno who won, the cleft pronoun (it) and the cleft clause (who won) form a discontinuous syntactic constituent, and a semantic unit as a deﬁnite description, presenting arguments from Percus (1997) and Hedberg (2000). We propose a syntax of it-clefts using Tree-Local MultiComponent Tree Adjoining Grammar and a compositional semantics on the proposed syntax using Synchronous Tree Adjoining Grammar.  
In relative clauses, the wh relative pronoun can be embedded in a larger phrase, as in a boy [whose brother] Mary hit. In such examples, we say that the larger phrase has pied-piped along with the whword. In this paper, using a similar syntactic analysis for wh pied-piping as in Han (2002) and further developed in Kallmeyer and Schefﬂer (2004), I propose a compositional semantics for relative clauses based on Synchronous Tree Adjoining Grammar. It will be shown that (i) the elementary tree representing the logical form of a wh-word provides a generalized quantiﬁer, and (ii) the semantic composition of the pied-piped material and the wh-word is achieved through adjoining in the semantics of the former onto the latter. 
 This paper discusses interactions between negative concord and restructuring/clause union in Palestinian Arabic. Analyses formulated in Tree Adjoining Grammar and Combinatorial Categorial Grammar are compared, with the conclusion that a perspicuous analysis of the the intricacies of the data requires aspects of both formalisms; in particular, the TAG notion of the extended domain of locality and the CCG notion of ﬂexible constituency.  
Several grammars have been proposed for modeling RNA pseudoknotted structure. In this paper, we focus on multiple context-free grammars (MCFGs), which are natural extension of context-free grammars and can represent pseudoknots, and extend a speciﬁc subclass of MCFGs to a probabilistic model called SMCFG. We present a polynomial time parsing algorithm for ﬁnding the most probable derivation tree and a probability parameter estimation algorithm. Furthermore, we show some experimental results of pseudoknot prediction using SMCFG algorithm. 
This paper presents an LTAG account for binding of reﬂexives and reciprocals in English. For these anaphors, a multi-component lexical entry is proposed, whose ﬁrst component is a degenerate NP-tree that adjoins into the anaphor’s binder. This establishes the local structural relationship needed to ensure coreference and agreement. The analysis also allows a parallel treatment of reﬂexives and reciprocals, which is desirable because their behavior is very similar. In order to account for non-local binding phenomena, as in raising and ECM cases, we employ ﬂexible composition, constrained by a subject intervention constraint between the two components of the anaphor’s lexical entry. Finally, the paper discusses further data such as extraction and picture-NP examples. 
Relative quantiﬁer scope in German depends, in contrast to English, very much on word order. The scope possibilities of a quantiﬁer are determined by its surface position, its base position and the type of the quantiﬁer. In this paper we propose a multicomponent analysis for German quantiﬁers computing the scope of the quantiﬁer, in particular its minimal nuclear scope, depending on the syntactic conﬁguration it occurs in. 
 Our paper aims at capturing the distribution of negative polarity items (NPIs) within lexicalized Tree Adjoining Grammar (LTAG). The condition under which an NPI can occur in a sentence is for it to be in the scope of a negation with no quantiﬁers scopally intervening. We model this restriction within a recent framework for LTAG semantics based on semantic uniﬁcation. The proposed analysis provides features that signal the presence of a negation in the semantics and that specify its scope. We extend our analysis to modelling the interaction of NPI licensing and neg raising constructions.  
This paper presents a LTAG-based analysis of gapping and VP ellipsis, which proposes that resolution of the elided material is part of a general disambiguation procedure, which is also responsible for resolution of underspecified representations of scope. 
Surface realisation from ﬂat semantic formulae is known to be exponential in the length of the input. In this paper, we argue that TAG naturally supports the integration of three main ways of reducing complexity: polarity ﬁltering, delayed adjunction and empty semantic items elimination. We support these claims by presenting some preliminary results of the TAG-based surface realiser GenI. 
In this paper, a generic system that generates parsers from parsing schemata is applied to the particular case of the XTAG English grammar. In order to be able to generate XTAG parsers, some transformations are made to the grammar, and TAG parsing schemata are extended with feature structure uniﬁcation support and a simple tree ﬁltering mechanism. The generated implementations allow us to study the performance of different TAG parsers when working with a large-scale, widecoverage grammar. 
This paper compares two approaches to computational semantics, namely semantic uniﬁcation in Lexicalized Tree Adjoining Grammars (LTAG) and Lexical Resource Semantics (LRS) in HPSG. There are striking similarities between the frameworks that make them comparable in many respects. We will exemplify the differences and similarities by looking at several phenomena. We will show, ﬁrst of all, that many intuitions about the mechanisms of semantic computations can be implemented in similar ways in both frameworks. Secondly, we will identify some aspects in which the frameworks intrinsically differ due to more general differences between the approaches to formal grammar adopted by LTAG and HPSG. 
In this paper, we introduce SEMTAG, a toolbox for TAG-based parsing and generation. This environment supports the development of wide-coverage grammars and differs from existing environments for TAG such as XTAG, (XTAG-ResearchGroup, 2001) in that it includes a semantic dimension. SEMTAG is open-source and freely available. 
The ability to represent cross-serial dependencies is one of the central features of Tree Adjoining Grammar (TAG). The class of dependency structures representable by lexicalized TAG derivations can be captured by two graph-theoretic properties: a bound on the gap degree of the structures, and a constraint called well-nestedness. In this paper, we compare formalisms from two strands of extensions to TAG in the context of the question, how they behave with respect to these constraints. In particular, we show that multi-component TAG does not necessarily retain the well-nestedness constraint, while this constraint is inherent to Coupled Context-Free Grammar (Hotz and Pitsch, 1996). 
Semantic role labeling (SRL) methods typically use features from syntactic parse trees. We propose a novel method that uses Lexicalized Tree-Adjoining Grammar (LTAG) based features for this task. We convert parse trees into LTAG derivation trees where the semantic roles are treated as hidden information learned by supervised learning on annotated data derived from PropBank. We extracted various features from the LTAG derivation trees and trained a discriminative decision list model to predict semantic roles. We present our results on the full CoNLL 2005 SRL task. 
In this paper, we present a system which can extract syntactic feature structures from a Korean Treebank (Sejong Treebank) to develop a Feature-based Lexicalized Tree Adjoining Grammars. 
Coordination of phrases of different syntactic categories has posed a problem for generative systems based only on syntactic categories. Although some prefer to treat them as exceptional cases that should require some extra mechanism (as for elliptical constructions), or to allow for unrestricted cross-category coordination, they can be naturally derived in a grammatic functional generative approach. In this paper we explore the ideia on how mixing syntactic categories and grammatical functions in the label set of a Tree Adjoining Grammar allows us to develop grammars that elegantly handle both the cases of same- and cross-category coordination in an uniform way. 
This paper presents informally an Earley algorithm for TAG which behaves as the algorithm given by (Schabes and Joshi, 1988). This algorithm is a specialization to TAG of a more general algorithm dedicated to second order ACGs. As second order ACGs allows to encode Linear Context Free Rewriting Systems (LCFRS) (de Groote and Pogodalla, 2004), the presentation of this algorithm gives a rough presentation of the formal tools which can be used to design efﬁcient algorithms for LCFRS. Furthermore, as these tools allow to parse linear λ-terms, they can be used as a basis for developping algorithms for generation. 
In this paper, we introduce a generic approach to elliptic coordination modeling through the parsing of Ltag grammars. We show that erased lexical items can be replaced during parsing by informations gathered in the other member of the coordinate structure and used as a guide at the derivation level. Moreover, we show how this approach can be indeed implemented as a light extension of the LTAG formalism throuh a so-called “fusion” operation and by the use of tree schemata during parsing in order to obtain a dependency graph. 
Russian and Polish lack ’unbounded’ syntactic dependencies that fall into the primary empirical domain of TAG-Adjoining, namely, long-distance movement/ﬁller-gap dependencies across a tensed clause boundary. A theory that incorporates Adjoining as a recursive structure building device provides a novel and straightforward account of this gap, whereas existing theories of syntactic locality, e.g. of the standard Minimalist kind, face difﬁculties explaining the phenomenon. These languages thus supply direct linguistic evidence for Adjoining. 
 In this paper, structures involving the raising verb seem, are examined. Speciﬁcally, it is shown that previously-proposed elementary trees for seem with an experiencer argument are inadequate, based upon syntactic testing. In Storoshenko (2006), new articulated structures for the seem predicate are proposed, modelled upon the treatment of ditransitive verbs. This paper recapitulates and further motivates the ditransitive-style analysis, while illustrating its potential value in issues surrounding extraction and the raising construction in TAG.  
As the language generation community explores the possibility of an evaluation program for language generation, it behooves us to examine our experience in evaluation of other systems that produce text as output. Large scale evaluation of summarization systems and of question answering systems has been carried out for several years now. Summarization and question answering systems produce text output given text as input, while language generation produces text from a semantic representation. Given that the output has the same properties, we can learn from the mistakes and the understandings gained in earlier evaluations. In this invited talk, I will discuss what we have learned in the large scale summarization evaluations carried out in the Document Understanding Conferences (DUC) from 2001 to present, and in the large scale question answering evaluations carried out in TREC (e.g., the deﬁnition pilot) as well as the new large scale evaluations being carried out in the DARPA GALE (Global Autonomous Language Environment) program. DUC was developed and run by NIST and provides a forum for regular evaluation of summarization systems. NIST oversees the gathering of data, including both input documents and gold standard summaries, some of which is done by NIST and some of which is done by LDC. Each year, some 30 to 50 document sets were gathered as test data and somewhere between two to nine summaries were written for each of the input sets. NIST has carried out both manual and automatic evaluation by comparing system output against the gold standard summaries written by humans. The results are made public at the annual conference. In the most recent years, the number of participants has  
We describe a generation-oriented workbench for the Performance Grammar (PG) formalism, highlighting the treatment of certain word order and movement constraints in Dutch and German. PG enables a simple and uniform treatment of a heterogeneous collection of linear order phenomena in the domain of verb constructions (variably known as Cross-serial Dependencies, Verb Raising, Clause Union, Extraposition, Third Construction, Particle Hopping, etc.). The central data structures enabling this feature are clausal “topologies”: one-dimensional arrays associated with clauses, whose cells (“slots”) provide landing sites for the constituents of the clause. Movement operations are enabled by unification of lateral slots of topologies at adjacent levels of the clause hierarchy. The PGW generator assists the grammar developer in testing whether the implemented syntactic knowledge allows all and only the well-formed permutations of constituents. 
This paper presents a novel algorithm for efﬁciently generating paraphrases from disjunctive logical forms. The algorithm is couched in the framework of Combinatory Categorial Grammar (CCG) and has been implemented as an extension to the OpenCCG surface realizer. The algorithm makes use of packed representations similar to those initially proposed by Shemtov (1997), generalizing the approach in a more straightforward way than in the algorithm ultimately adopted therein. 
We describe an implemented generator for a spoken dialogue system that follows the ‘overgeneration and ranking’ approach. We ﬁnd that overgeneration based on bottom-up chart generation is wellsuited to a) model phenomena such as alignment and variation in dialogue, and b) address robustness issues in the face of imperfect generation input. We report evaluation results of a ﬁrst user study involving 20 subjects. 
It would be useful to enable dialogue agents to project, through linguistic means, their individuality or personality. Equally, each member of a pair of agents ought to adjust its language (to a greater or lesser extent) to match that of its interlocutor. We describe CRAG, which generates dialogues between pairs of agents, who are linguistically distinguishable, but able to align. CRAG-2 makes use of OPENCCG and an over-generation and ranking approach, guided by a set of language models covering both personality and alignment. We illustrate with examples of output, and brieﬂy note results from user studies with the earlier CRAG-1, indicating how CRAG-2 will be further evaluated. Related work is discussed, along with current limitations and future directions. 
Human text is characterised by the individual lexical choices of a speciﬁc author. Signiﬁcant variations exist between authors. In contrast, natural language generation systems normally produce uniform texts. In this paper we apply distributional similarity measures to help verb choice in a natural language generation system which tries to generate text similar to individual author. By using a distributional similarity (DS) measure on corpora collected from a recipe domain, we get the most likely verbs for individual authors. The accuracy of matching verb pairs produced by distributional similarity is higher than using the synonym outputs of verbs from WordNet. Furthermore, the combination of the two methods provides the best accuracy. 
This paper describes adjective-to-verb paraphrasing in Japanese. In this paraphrasing, generated verbs require additional sufﬁxes according to their difference in meaning. To determine proper sufﬁxes for a given adjective-verb pair, we have examined the verbal features involved in the theory of Lexical Conceptual Structure. 
Algorithms that generate expressions to identify a referent are mostly tailored towards objects which are in some sense conceived as holistic entities, describing them in terms of their properties and relations to other objects. This approach may prove not fully adequate when referring to components of structured objects, specifically for abstract objects in formal domains, where scope and relative positions are essential features. In this paper, we adapt the standard Dale and Reiter algorithm to specifics of such references as observed in a corpus about mathematical proofs. Extensions incorporated include an incremental specialization of property values for metonymic references, local and global positions reflecting group formations and implicature-based scope preferences to justify unique identification of the intended referent. The approach is primarily relevant for domains where abstract formal objects are prominent, but some of its features are also useful to extend the expressive repertoire of reference generation algorithms in other domains. 
It is often desirable that referring expressions be chosen in such a way that their referents are easy to identify. In this paper, we investigate to what extent identiﬁcation becomes easier by the addition of logically redundant properties.We focus on hierarchically structured domains, whose content is not fully known to the reader when the referring expression is uttered. Introduction Common sense suggests that speakers and writers who want to get their message across should make their utterances easy to understand. Broadly speaking, this view is conﬁrmed by empirical research (Deutsch 1976, Mangold 1986, Levelt 1989, Sonnenschein 1984, Clark 1992, Cremers 1996, Arts 2004, Paraboni and van Deemter 2002, van der Sluis, 2005). The present paper follows in the footsteps of Paraboni and van Deemter (2002) by focussing on hierarchically structured domains and asking whether any beneﬁts are obtained when an algorithm for the generation of referring expressions (GRE) builds logical redundancy into the descriptions that it generates. Where Paraboni and van Deemter (2002) reported on the results of a simple experiment in which subjects were asked to say which description they preferred in a given context, the present paper describes a much more elaborate experiment, measuring how difﬁcult it is for subjects to ﬁnd the referent of a description. 
The natural language generation literature provides many algorithms for the generation of referring expressions. In this paper, we explore the question of whether these algorithms actually produce the kinds of expressions that people produce. We compare the output of three existing algorithms against a data set consisting of human-generated referring expressions, and identify a number of signiﬁcant differences between what people do and what these algorithms do. On the basis of these observations, we suggest some ways forward that attempt to address these differences. 
Past work of generating referring expressions mainly utilized attributes of objects and binary relations between objects in order to distinguish the target object from others. However, such an approach does not work well when there is no distinctive attribute among objects. To overcome this limitation, this paper proposes a novel generation method utilizing perceptual groups of objects and n-ary relations among them. The evaluation using 18 subjects showed that the proposed method could effectively generate proper referring expressions. 
We report on a study examining the generation of noun phrases within a spoken dialog agent for a navigation domain. The task is to provide real-time instructions that direct the user to move between a series of destinations within a large interior space. A subtask within sentence planning is determining what form to choose for noun phrases. This choice is driven by both the discourse history and spatial context features derived from the directionfollower’s position, e.g. his view angle, distance from the target referent and the number of similar items in view. The algorithm was developed as a decision tree and its output was evaluated by a group of human judges who rated 62.6% of the expressions generated by the system to be as good as or better than the language originally produced by human dialog partners. 
Existing algorithms for the Generation of Referring Expressions (GRE) aim at generating descriptions that allow a hearer to identify its intended referent uniquely; the length of the expression is also considered, usually as a secondary issue. We explore the possibility of making the trade-off between these two factors more explicit, via a general cost function which scores these two aspects separately. We sketch some more complex phenomena which might be amenable to this treatment. 
This paper presents a method of querying databases by means of a natural languagelike interface which offers the advantage of minimal conﬁguration necessary for porting the system. The method allows us to ﬁrst automatically infer the set of possible queries that can apply to a given database, automatically generate a lexicon and grammar rules for expressing these queries, and then provide users with an interface that allows them to pose these queries in natural language without the well-known limitations of most natural language interfaces to databases. The way the queries are inferred and constructed means that semantic translation is performed with perfect reliability. 
In this paper, we present a questionanswering system on the Web which aims at generating intelligent answers to numerical questions. These answers are generated in a cooperative way: besides a direct answer, comments are generated to explain to the user the variation of numerical data extracted from the Web. We present the content determination and realisation tasks. We also present some elements of evaluation with respect to end-users. 
We report the results of a pilot study on generating Multiple-Choice Test Items from medical text and discuss the main tasks involved in this process and how our system was evaluated by domain experts. 
This paper presents the design of a discourse generator that plans the content and organization of lay-oriented genetic counseling documents containing arguments, and an experiment to evaluate the arguments. Due to the separation of domain, argument, and genre-specific concerns and the methodology used for acquiring a domain model, this approach should be applicable to argument generation in other domains. 
One thing was clear: opinions abounded, most of them strong ones. Shared-task evaluation had been ﬁrmly put on the NLG agenda. So, we thought, what better than to create a larger, 
In this position paper, we argue that a common task and corpus are not the only ways to evaluate Natural Language Generation (NLG) systems. It might be, in fact, too narrow a view on evaluation and thus not be the best way to evaluate these systems. The aim of a common task and corpus is to allow for a comparative evaluation of systems, looking at the systems’ performances. It is thus a “systemoriented” view of evaluation. We argue here that, if we are to take a system oriented view of evaluation, the community might be better served by enlarging the view of evaluation, defining common dimensions and metrics to evaluate systems and approaches. We also argue that end-user (or usability) evaluations form another important aspect of a system’s evaluation and should not be forgotten. 
This paper discusses the construction of a corpus for the evaluation of algorithms that generate referring expressions. It is argued that such an evaluation task requires a semantically transparent corpus, and controlled experiments are the best way to create such a resource. We address a number of issues that have arisen in an ongoing evaluation study, among which is the problem of judging the output of GRE algorithms against a human gold standard. 
We propose to organise a series of sharedtask NLG events, where participants are asked to build systems with similar input/output functionalities, and these systems are evaluated with a range of different evaluation techniques. The main purpose of these events is to allow us to compare different evaluation techniques, by correlating the results of different evaluations on the systems entered in the events. 
 In this paper we deal with learning and forgetting of speech commands in speech dialogue systems. We discuss two mathematical models for learning and four models for forgetting. Furthermore, we describe the experiments used to determine the learning and forgetting curve in our environment. Our ﬁndings are compared to the theoretical models and based on this we deduce which models best describe learning and forgetting in our automotive environment. The resulting models are used to develop an adaptive help system for a speech dialogue system. The system provides only relevant context speciﬁc information. 
We developed a multi-domain spoken dialogue system that can handle user requests across multiple domains. Such systems need to satisfy two requirements: extensibility and robustness against speech recognition errors. Extensibility is required to allow for the modiﬁcation and addition of domains independent of other domains. Robustness against speech recognition errors is required because such errors are inevitable in speech recognition. However, the systems should still behave appropriately, even when their inputs are erroneous. Our system was constructed on an extensible architecture and is equipped with a robust and extensible domain selection method. Domain selection was based on three choices: (I) the previous domain, (II) the domain in which the speech recognition result can be accepted with the highest recognition score, and (III) other domains. With the third choice we newly introduced, our system can prevent dialogues from continuously being stuck in an erroneous domain. Our experimental results, obtained with 10 subjects, showed that our method reduced the domain selection errors by 18.3%, compared to a conventional method. 
In this paper, we describe methods for building and evaluation of limited domain question-answering characters. Several classiﬁcation techniques are tested, including text classiﬁcation using support vector machines, language-model based retrieval, and cross-language information retrieval techniques, with the latter having the highest success rate. We also evaluated the effect of speech recognition errors on performance with users, ﬁnding that retrieval is robust until recognition reaches over 50% WER. 
We explore the relationship between question answering and constraint relaxation in spoken dialog systems. We develop dialogue strategies for selecting and presenting information succinctly. In particular, we describe methods for dealing with the results of database queries in informationseeking dialogs. Our goal is to structure the dialogue in such a way that the user is neither overwhelmed with information nor left uncertain as to how to reﬁne the query further. We present evaluation results obtained from a user study involving 20 subjects in a restaurant selection task. 
In this paper we present an approach to dialogue management that supports the generation of multifunctional utterances. It is based on the multidimensional dialogue act taxonomy and associated context model as developed in Dynamic Interpretation Theory (DIT). The multidimensional organisation of the taxonomy reﬂects that there are various aspects that dialogue participants have to deal with simultaneously during a dialogue. Besides performing some underlying task, a participant also has to pay attention to various aspects of the communication process itself, including social conventions. Therefore, a multi-agent approach is proposed, in which for each of the dimensions in the taxonomy a specialised dialogue act agent is designed, dedicated to the generation of dialogue acts from that particular dimension. These dialogue act agents operate in parallel on the information state of the system. For a simpliﬁed version of the taxonomy, a dialogue manager has been implemented and integrated into an interactive QA system. 
This paper investigates the problems facing modelling agents’ beliefs in Discourse Representation Theory (DRT) and presents a viable solution in the form of a dialogue-based DRT representation of beliefs. Integrating modelling dialogue interaction into DRT allows modelling agents’ beliefs, intentions and mutual beliefs. Furthermore, it is one of the aims of the paper to account for the important notion of agents’ varying degrees of belief in different contexts.1 
This paper presents an extension to the Reference Domain Theory (Salmon-Alt, 2001) in order to solve plural references. While this theory doesn’t take plural reference into account in its original form, this paper shows how several entities can be grouped together by building a new domain and how they can be accessed later on. We introduce the notion of super-domain, representing the access structure to all the plural referents of a given type. 
Integration of new utterances into context is a central task in any model for rational (human-machine) dialogues in natural language. In this paper, a pragmatics-ﬁrst approach to specifying the meaning of utterances in terms of plans is presented. A rational dialogue is driven by the reaction of dialogue participants on how they ﬁnd their expectations on changes in the environment satisﬁed by their observations of the outcome of performed actions. We present a computational model for this view on dialogues and illustrate it with examples from a real-world application. 
This paper investigates semantic and pragmatic presupposition in Discourse Representation Theory (DRT) and enhances the pragmatic perspective of presupposition in DRT. In doing so, it draws attention to the need to account for agent presupposition (i.e. both speaker and hearer presupposition) when dealing with pragmatic presupposition. Furthermore, this paper links this pragmatic conception of presupposition with the semantic one (sentence presupposition) through using ‘information checks’ which agents are hypothesized to employ when making and receiving utterances.1 
This paper presents an evaluation of indirect anaphor resolution which considers as lexical resource the semantic tagging provided by the PALAVRAS parser. We describe the semantic tagging process and a corpus experiment. 
We study the interplay of the discourse structure of a scientiﬁc argument with formal citations. One subproblem of this is to classify academic citations in scientiﬁc articles according to their rhetorical function, e.g., as a rival approach, as a part of the solution, or as a ﬂawed approach that justiﬁes the current research. Here, we introduce our annotation scheme with 12 categories, and present an agreement study. 
We present a dialogue manager for “Call for Fire” training dialogues. We describe the training environment, the domain, the features of its novel information statebased dialogue manager, the system it is a part of, and preliminary evaluation results. 
Identiﬁcation of action items in meeting recordings can provide immediate access to salient information in a medium notoriously difﬁcult to search and summarize. To this end, we use a maximum entropy model to automatically detect action itemrelated utterances from multi-party audio meeting recordings. We compare the effect of lexical, temporal, syntactic, semantic, and prosodic features on system performance. We show that on a corpus of action item annotations on the ICSI meeting recordings, characterized by high imbalance and low inter-annotator agreement, the system performs at an F measure of 31.92%. While this is low compared to better-studied tasks on more mature corpora, the relative usefulness of the features towards this task is indicative of their usefulness on more consistent annotations, as well as to related tasks. 
A problem in dialogue research is that of ﬁnding and managing expectations. Adjacency pair theory has widespread acceptance, but traditional classiﬁcation features (in particular, ‘previous-tag’ type features) do not exploit this information optimally. We suggest a method of dialogue segmentation that veriﬁes adjacency pairs and allows us to use dialogue-level information within the entire segment and not just the previous utterance. We also use the χ2 test for statistical signiﬁcance as ‘noise reduction’ to reﬁne a list of pairs. Together, these methods can be used to extend expectation beyond the traditional classiﬁcation features. 
In this paper, we explain a rapid development method of multimodal dialogue sys-tem using MIML (Multimodal Interaction Markup Language), which defines dialogue patterns between human and various types of interactive agents. The feature of this language is three-layered description of agent-based interactive systems which separates task level description, interaction description and device dependent realization. MIML has advantages in high-level interaction description, modality extensibility and compatibility with standardized technologies. 
In this paper we consider the problem of identifying and classifying discourse coherence relations. We report initial results over the recently released Discourse GraphBank (Wolf and Gibson, 2005). Our approach considers, and determines the contributions of, a variety of syntactic and lexico-semantic features. We achieve 81% accuracy on the task of discourse relation type classiﬁcation and 70% accuracy on relation identiﬁcation. 
We present a ﬁrst analysis of interannotator agreement for the DIT++ tagset of dialogue acts, a comprehensive, layered, multidimensional set of 86 tags. Within a dimension or a layer, subsets of tags are often hierarchically organised. We argue that especially for such highly structured annotation schemes the well-known kappa statistic is not an adequate measure of inter-annotator agreement. Instead, we propose a statistic that takes the structural properties of the tagset into account, and we discuss the application of this statistic in an annotation experiment. The experiment shows promising agreement scores for most dimensions in the tagset and provides useful insights into the usability of the annotation scheme, but also indicates that several additional factors inﬂuence annotator agreement. We ﬁnally suggest that the proposed approach for measuring agreement per dimension can be a good basis for measuring annotator agreement over the dimensions of a multidimensional annotation scheme. 
We present a probabilistic approach for the interpretation of arguments that casts the selection of an interpretation as a model selection task. In selecting the best model, our formalism balances conﬂicting factors: model complexity against data ﬁt, and structure complexity against belief reasonableness. We ﬁrst describe our basic formalism, which considers interpretations comprising inferential relations, and then show how our formalism is extended to suppositions that account for the beliefs in an argument, and justiﬁcations that account for the inferences in an interpretation. Our evaluations with users show that the interpretations produced by our system are acceptable, and that there is strong support for the postulated suppositions and justiﬁcations. 
We consider here the task of linear thematic segmentation of text documents, by using features based on word distributions in the text. For this task, a typical and often implicit assumption in previous studies is that a document has just one topic and therefore many algorithms have been tested and have shown encouraging results on artiﬁcial data sets, generated by putting together parts of different documents. We show that evaluation on synthetic data is potentially misleading and fails to give an accurate evaluation of the performance on real data. Moreover, we provide a critical review of existing evaluation metrics in the literature and we propose an improved evaluation metric. 
Dialog systems for mobile robots operating in the real world should enable mixedinitiative dialog style, handle multi-modal information involved in the communication and be relatively independent of the domain knowledge. Most dialog systems developed for mobile robots today, however, are often system-oriented and have limited capabilities. We present an agentbased dialog model that are specially designed for human-robot interaction and provide evidence for its efﬁciency with our implemented system. 
The goal of this paper is to show how to accomplish a more enjoyable and enthusiastic dialogue through the analysis of human-to-human conversational dialogues. We ﬁrst created a conversational dialogue corpus annotated with two types of tags: one type indicates the particular aspects of the utterance itself, while the other indicates the degree of enthusiasm. We then investigated the relationship between these tags. Our results indicate that affective and cooperative utterances are signiﬁcant to enthusiastic dialogue. 
This paper reports on an experiment in which we explore a new approach to the automatic measurement of multi-word expression (MWE) compositionality. We propose an algorithm which ranks MWEs by their compositionality relative to a semantic field taxonomy based on the Lancaster English semantic lexicon (Piao et al., 2005a). The semantic information provided by the lexicon is used for measuring the semantic distance between a MWE and its constituent words. The algorithm is evaluated both on 89 manually ranked MWEs and on McCarthy et al’s (2003) manually ranked phrasal verbs. We compared the output of our tool with human judgments using Spearman’s rank-order correlation coefficient. Our evaluation shows that the automatic ranking of the majority of our test data (86.52%) has strong to moderate correlation with the manual ranking while wide discrepancy is found for a small number of MWEs. Our algorithm also obtained a correlation of 0.3544 with manual ranking on McCarthy et al’s test data, which is comparable or better than most of the measures they tested. This experiment demonstrates that a semantic lexicon can assist in MWE compositionality measurement in addition to statistical algorithms. 
Making use of latent semantic analysis, we explore the hypothesis that local linguistic context can serve to identify multi-word expressions that have noncompositional meanings. We propose that vector-similarity between distribution vectors associated with an MWE as a whole and those associated with its constitutent parts can serve as a good measure of the degree to which the MWE is compositional. We present experiments that show that low (cosine) similarity does, in fact, correlate with non-compositionality. 
It is well known that multi-word expressions are problematic in natural language processing. In previous literature, it has been suggested that information about their degree of compositionality can be helpful in various applications but it has not been proven empirically. In this paper, we propose a framework in which information about the multi-word expressions can be used in the word-alignment task. We have shown that even simple features like point-wise mutual information are useful for word-alignment task in English-Hindi parallel corpora. The alignment error rate which we achieve (AER = 0.5040) is signiﬁcantly better (about 10% decrease in AER) than the alignment error rates of the state-of-art models (Och and Ney, 2003) (Best AER = 0.5518) on the English-Hindi dataset. 
Complex Predicates or CPs are multiword complexes functioning as single verbal units. CPs are particularly pervasive in Hindi and other IndoAryan languages, but an usage account driven by corpus-based identification of these constructs has not been possible since single-language systems based on rules and statistical approaches require reliable tools (POS taggers, parsers, etc.) that are unavailable for Hindi. This paper highlights the development of first such database based on the simple idea of projecting POS tags across an English-Hindi parallel corpus. The CP types considered include adjective-verb (AV), noun-verb (NV), adverb-verb (Adv-V), and verb-verb (VV) composites. CPs are hypothesized where a verb in English is projected onto a multi-word sequence in Hindi. While this process misses some CPs, those that are detected appear to be more reliable (83% precision, 46% recall). The resulting database lists usage instances of 1439 CPs in 4400 sentences. 
However large a hand-crafted widecoverage grammar is, there are always going to be words and constructions that are not included in it and are going to cause parse failure. Due to their heterogeneous and ﬂexible nature, Multiword Expressions (MWEs) provide an endless source of parse failures. As the number of such expressions in a speaker’s lexicon is equiparable to the number of single word units (Jackendoff, 1997), one major challenge for robust natural language processing systems is to be able to deal with MWEs. In this paper we propose to semi-automatically detect MWE candidates in texts using some error mining techniques and validating them using a combination of the World Wide Web as a corpus and some statistical measures. For the remaining candidates possible lexicosyntactic types are predicted, and they are subsequently added to the grammar as new lexical entries. This approach provides a signiﬁcant increase in the coverage of these expressions. 
Previous computational work on learning the semantic properties of verb-particle constructions (VPCs) has focused on their compositionality, and has left unaddressed the issue of which meaning of the component words is being used in a given VPC. We develop a feature space for use in classiﬁcation of the sense contributed by the particle in a VPC, and test this on VPCs using the particle up. The features that capture linguistic properties of VPCs that are relevant to the semantics of the particle outperform linguistically uninformed word co-occurrence features in our experiments on unseen test VPCs. 
We present two novel paraphrase tests for automatically predicting the inherent semantic relation of a given compound nominalisation as one of subject, direct object, or prepositional object. We compare these to the usual verb–argument paraphrase test using corpus statistics, and frequencies obtained by scraping the Google search engine interface. We also implemented a more robust statistical measure than maximum likelihood estimation — the conﬁdence interval. A signiﬁcant reduction in data sparseness was achieved, but this alone is insufﬁcient to provide a substantial performance improvement. 
In many theoretical and applied areas of computational linguistics researchers operate with a notion of linguistic distance or, conversely, linguistic similarity, which is the focus of the present workshop. While many CL areas make frequent use of such notions, it has received little focused attention, an honorable exception being Lebart & Rajman (2000). This workshop brings a number of these strands together, highlighting a number of common issues. 
This study investigates similarity judgments from two angles. First, we look at models suggested in the psychology and philosophy literature which capture the essence of concept similarity evaluation for humans. Second, we analyze the properties of many metrics which simulate such evaluation capabilities. The first angle reveals that non-experts can judge similarity and that their judgments need not be based on predefined traits. We use such conclusions to inform us on how gold standards for word sense disambiguation tasks could be established. From the second angle, we conclude that more attention should be paid to metric properties before assigning them to perform a particular task. 
Semantic relatedness is a special form of linguistic distance between words. Evaluating semantic relatedness measures is usually performed by comparison with human judgments. Previous test datasets had been created analytically and were limited in size. We propose a corpus-based system for automatically creating test datasets.1 Experiments with human subjects show that the resulting datasets cover all degrees of relatedness. As a result of the corpus-based approach, test datasets cover all types of lexical-semantic relations and contain domain-speciﬁc words naturally occurring in texts. 
We present results on the relation discovery task, which addresses some of the shortcomings of supervised relation extraction by applying minimally supervised methods. We describe a detailed experimental design that compares various conﬁgurations of conceptual representations and similarity measures across six different subsets of the ACE relation extraction data. Previous work on relation discovery used a semantic space based on a term-bydocument matrix. We ﬁnd that representations based on term co-occurrence perform signiﬁcantly better. We also observe further improvements when reducing the dimensionality of the term co-occurrence matrix using probabilistic topic models, though these are not signiﬁcant. 
We design and test a sentence comparison method using the framework of Robust Minimal Recursion Semantics which allows us to utilise the deep parse information produced by Jacy, a Japanese HPSG based parser and the lexical information available in our ontology. Our method was used for both paraphrase detection and also for answer sentence selection for question answering. In both tasks, results showed an improvement over Bag-of-Words, as well as providing extra information useful to the applications. 
We investigate the problem of measuring phonetic similarity, focusing on the identiﬁcation of cognates, words of the same origin in different languages. We compare representatives of two principal approaches to computing phonetic similarity: manually-designed metrics, and learning algorithms. In particular, we consider a stochastic transducer, a Pair HMM, several DBN models, and two constructed schemes. We test those approaches on the task of identifying cognates among Indoeuropean languages, both in the supervised and unsupervised context. Our results suggest that the averaged context DBN model and the Pair HMM achieve the highest accuracy given a large training set of positive examples. 
We examine various string distance measures for suitability in modeling dialect distance, especially its perception. We ﬁnd measures superior which do not normalize for word length, but which are are sensitive to order. We likewise ﬁnd evidence for the superiority of measures which incorporate a sensitivity to phonological context, realized in the form of n-grams— although we cannot identify which form of context (bigram, trigram, etc.) is best. However, we ﬁnd no clear beneﬁt in using gradual as opposed to binary segmental difference when calculating sequence distances. 
To determine how close two language models (e.g., n-grams models) are, we can use several distance measures. If we can represent the models as distributions, then the similarity is basically the similarity of distributions. And a number of measures are based on information theoretic approach. In this paper we present some experiments on using such similarity measures for an old Natural Language Processing (NLP) problem. One of the measures considered is perhaps a novel one, which we have called mutual cross entropy. Other measures are either well known or based on well known measures, but the results obtained with them vis-avis one-another might help in gaining an insight into how similarity measures work in practice. The ﬁrst step in processing a text is to identify the language and encoding of its contents. This is a practical problem since for many languages, there are no universally followed text encoding standards. The method we have used in this paper for language and encoding identiﬁcation uses pruned character n-grams, alone as well augmented with word n-grams. This method seems to give results comparable to other methods. 
This paper presents an approach to the question whether it is possible to construct a parser based on ideas from case-based reasoning. Such a parser would employ a partial analysis of the input sentence to select a (nearly) complete syntax tree and then adapt this tree to the input sentence. The experiments performed on German data from the Tu¨ba-D/Z treebank and the KaRoPars partial parser show that a wide range of levels of generality can be reached, depending on which types of information are used to determine the similarity between input sentence and training sentences. The results are such that it is possible to construct a case-based parser. The optimal setting out of those presented here need to be determined empirically. 
We compare vectors containing counts of trigrams of part-of-speech (POS) tags in order to obtain an aggregate measure of syntax difference. Since lexical syntactic categories reﬂect more abstract syntax as well, we argue that this procedure reﬂects more than just the basic syntactic categories. We tag the material automatically and analyze the frequency vectors for POS trigrams using a permutation test. A test analysis of a 305,000 word corpus containing the English of Finnish emigrants to Australia is promising in that the procedure proposed works well in distinguishing two different groups (adult vs. child emigrants) and also in highlighting syntactic deviations between the two groups. 
This paper outlines a measure of language similarity based on structural similarity of surface syntactic dependency trees. Unlike the more traditional string-based measures, this measure tries to reﬂect “deeper” correspondences among languages. The development of this measure has been inspired by the experience from MT of syntactically similar languages. This experience shows that the lexical similarity is less important than syntactic similarity. This claim is supported by a number of examples illustrating the problems which may arise when a measure of language similarity relies too much on a simple similarity of texts in diﬀerent languages. 
 2 Distance Measures  The results of experiments on the application of a variety of distance measures to a question-answering task are reported. Variants of tree-distance are considered, including whole-vs-sub tree, node weighting, wild cards and lexical emphasis. We derive string-distance as a special case of tree-distance and show that a particular parameterisation of tree-distance outperforms the string-distance measure. 
In this paper we propose two metrics to be used in various ﬁelds of computational linguistics area. Our construction is based on the supposition that in most of the natural languages the most important information is carried by the ﬁrst part of the unit. We introduce total rank distance and scaled total rank distance, we prove that they are metrics and investigate their max and expected values. Finally, a short application is presented: we investigate the similarity of Romance languages by computing the scaled total rank distance between the digram rankings of each language. 
Optimizing the production, maintenance and extension of lexical resources is one the crucial aspects impacting Natural Language Processing (NLP). A second aspect involves optimizing the process leading to their integration in applications. With this respect, we believe that the production of a consensual specification on multilingual lexicons can be a useful aid for the various NLP actors. Within ISO, one purpose of LMF (ISO24613) is to define a standard for lexicons that covers multilingual data. 
The role of lexical resources is often understated in NLP research. The complexity of Chinese, Japanese and Korean (CJK) poses special challenges to developers of NLP tools, especially in the area of word segmentation (WS), information retrieval (IR), named entity extraction (NER), and machine translation (MT). These difficulties are exacerbated by the lack of comprehensive lexical resources, especially for proper nouns, and the lack of a standardized orthography, especially in Japanese. This paper summarizes some of the major linguistic issues in the development NLP applications that are dependent on lexical resources, and discusses the central role such resources should play in enhancing the accuracy of NLP tools. 
In this paper we present an application fostering the integration and interoperability of computational lexicons, focusing on the particular case of mutual linking and cross-lingual enrichment of two wordnets, the ItalWordNet and Sinica BOW lexicons. This is intended as a case-study investigating the needs and requirements of semi-automatic integration and interoperability of lexical resources. 
Standard techniques used in multilingual terminology management fail to describe legal terminologies as they are bound to different legal systems and terms do not share a common meaning. In the LexALP project, we use a technique defined for general lexical databases to achieve cross language interoperability between languages of the Alpine Convention. In this paper we present the methodology and tools developed for the collection, description and harmonisation of the legal terminology of spatial planning and sustainable development in the four languages of the countries of the Alpine Space. 
In this paper we discuss the development of a multilingual collocation dictionary for translation purposes. By ‘collocation’ we mean not only set or fixed expressions including idioms, simple cooccurrences of items and metaphorical uses, but also translators’ paraphrases. We approach this problem from two directions. Firstly we identify certain linguistic phenomena and lexicographical requirements that need to be respected in the development of such dictionaries. The second and other direction concerns the development of such dictionaries in which linguistic phenomena and lexicographic attributes are themselves a means of access to the collocations. The linguistic phenomena and lexicographical requirements concern variously placing the sense of collocations rather than headwords or other access methods at the centre of interest, together with collocation synonymy and translation equivalence, polysemy and non-reversibility of the lexis, and other more lexicographic properties such as varieties of language and regionalisms, and types of translation. 
Although traditionally seen as a languageindependent task, collocation extraction relies nowadays more and more on the linguistic preprocessing of texts (e.g., lemmatization, POS tagging, chunking or parsing) prior to the application of statistical measures. This paper provides a language-oriented review of the existing extraction work. It points out several language-speciﬁc issues related to extraction and proposes a strategy for coping with them. It then describes a hybrid extraction system based on a multilingual parser. Finally, it presents a case-study on the performance of an association measure across a number of languages. 
We introduce a new type of lexical structure called lexical system, an interoperable model that can feed both monolingual and multilingual language resources. We begin with a formal characterization of lexical systems as “pure” directed graphs, solely made up of nodes corresponding to lexical entities and links. To illustrate our approach, we present data borrowed from a lexical system that has been generated from the French DiCo database. We later explain how the compilation of the original dictionary-like database into a net-like one has been made possible. Finally, we discuss the potential of the proposed lexical structure for designing multilingual lexical resources. 
Parallel corpus is a valuable resource used in various ﬁelds of multilingual natural language processing. One of the most signiﬁcant problems in using parallel corpora is the lack of their availability. Researchers have investigated approaches to collecting parallel texts from the Web. A basic component of these approaches is an algorithm that judges whether a pair of texts is parallel or not. In this paper, we propose an algorithm that accelerates this task without losing accuracy by preprocessing a bilingual dictionary as well as the collection of texts. This method achieved 250,000 pairs/sec throughput on a single CPU, with the best F1 score of 0.960 for the task of detecting 200 Japanese-English translation pairs out of 40, 000. The method is applicable to texts of any format, and not speciﬁc to HTML documents labeled with URLs. We report details of these preprocessing methods and the fast comparison algorithm. To the best of our knowledge, this is the ﬁrst reported experiment of extracting Japanese– English parallel texts from a large corpora based solely on linguistic content. 
 2 Background  An area of recent interest in crosslanguage information retrieval (CLIR) is the question of which parallel corpora might be best suited to tasks in CLIR, or even to what extent parallel corpora can be obtained or are necessary. One proposal, which in our opinion has been somewhat overlooked, is that the Bible holds a unique value as a multilingual corpus, being (among other things) widely available in a broad range of languages and having a high coverage of modern-day vocabulary. In this paper, we test empirically whether this claim is justified through a series of validation tests on various information retrieval tasks. Our results appear to indicate that our methodology may significantly outperform others recently proposed. 
Event detection and recognition is a complex task consisting of multiple sub-tasks of varying difﬁculty. In this paper, we present a simple, modular approach to event extraction that allows us to experiment with a variety of machine learning methods for these sub-tasks, as well as to evaluate the impact on performance these sub-tasks have on the overall task. 
Work on the interpretation of temporal expressions in text has generally been pursued in one of two paradigms: the formal semantics approach, where an attempt is made to provide a well-grounded theoretical basis for the interpretation of these expressions, and the more pragmaticallyfocused approach represented by the development of the TIMEX2 standard, with its origins in work in information extraction. The former emphasises formal elegance and consistency; the latter emphasises broad coverage for practical applications. In this paper, we report on the development of a framework that attempts to integrate insights from both perspectives, with the aim of achieving broad coverage of the domain in a well-grounded manner from a formal perspective. We focus in particular on the development of a compact notation for representing the semantics of underspeciﬁed temporal expressions that can be used to provide component-level evaluation of systems that interpret such expressions. 
The frequency of occurrence of words in natural languages exhibits a periodic and a non-periodic component when analysed as a time series. This work presents an unsupervised method of extracting periodicity information from text, enabling time series creation and filtering to be used in the creation of sophisticated language models that can discern between repetitive trends and non-repetitive writing patterns. The algorithm performs in O(n log n) time for input of length n. The temporal language model is used to create rules based on temporal-word associations inferred from the time series. The rules are used to guess automatically at likely document creation dates, based on the assumption that natural languages have unique signatures of changing word distributions over time. Experimental results on news items spanning a nine year period show that the proposed method and algorithms are accurate in discovering periodicity patterns and in dating documents automatically solely from their content. 
 occur is followed in (1), but overridden by means of a discourse relation, Explanation in (2).  Previous research on temporal anchoring and ordering has focused on the annotation and learning of temporal relations between events. These qualitative relations can be usefully supplemented with information about metric constraints, specifically as to how long events last. This paper describes the first steps in acquiring metric temporal constraints for events. The work is carried out in the context of the TimeML framework for marking up events and their temporal relations. This pilot study examines the feasibility of acquisition of metric temporal constraints from corpora. 
The extension to new languages is a well known bottleneck for rule-based systems. Considerable human effort, which typically consists in re-writing from scratch huge amounts of rules, is in fact required to transfer the knowledge available to the system from one language to a new one. Provided sufﬁcient annotated data, machine learning algorithms allow to minimize the costs of such knowledge transfer but, up to date, proved to be ineffective for some speciﬁc tasks. Among these, the recognition and normalization of temporal expressions still remains out of their reach. Focusing on this task, and still adhering to the rule-based framework, this paper presents a bunch of experiments on the automatic porting to Italian of a system originally developed for Spanish. Different automatic rule translation strategies are evaluated and discussed, providing a comprehensive overview of the challenge. 
In this paper, we demonstrate how to extend TimeML, a rich specification language for event and temporal expressions in text, with the implicit typical durations of events, temporal information in text that has hitherto been largely unexploited. Event duration information can be very important in applications in which the time course of events is to be extracted from text. For example, whether two events overlap or are in sequence often depends very much on their durations. 
Current research in developmental biology aims to link developmental genetic pathways with the processes going on at cellular and tissue level. Normal processes will only take place under speciﬁc sequential conditions at the level of the pathways. Disrupting or altering pathways may mean disrupted or altered development. This paper is part of a larger work exploring methods of detecting and extracting information on developmental events from free text and on their relations in space and time. 
Our research aim here is to build a CLIR system that works for a language pair with poor resources where the source language (e.g. Indonesian) has limited language resources. Our IndonesianJapanese CLIR system employs the existing Japanese IR system, and we focus our research on the IndonesianJapanese query translation. There are two problems in our limited resource query translation: the OOV problem and the translation ambiguity. The OOV problem is handled using target language’s resources (English-Japanese dictionary and Japanese proper name dictionary). The translation ambiguity is handled using a Japanese monolingual corpus in our translation filtering. We select the final translation set using the mutual information score and the TF×IDF score. The result on NTCIR 3 (NII-NACSIS Test Collection for IR Systems) Web Retrieval Task shows that the translation method achieved a higher IR score than the transitive machine translation (using Kataku (Indonesian-English) and Babelfish/ Excite (English-Japanese) engine) result. The best result achieved about 49% of the monolingual retrieval. 
Information Extraction, Summarization and Question Answering all manipulate natural language texts and should benefit from the use of NLP techniques. Statistical techniques have till now outperformed symbolic processing of unrestricted text. However, Information Extraction and Question Answering require by far more accurate results of what is currently produced by Bag-Of-Words approaches. Besides, we see that such tasks as Semantic Evaluation of Text Entailment or Similarity – as required by the RTE Challenge, impose a much stricter performance in semantic terms to tell true from false pairs. We will speak in favour of a hybrid system, a combination of statistical and symbolic processing with reference to a specific problem, that of Anaphora Resolution which looms large and deep in text processing. 1. Introduction Although full syntactic and semantic analysis of opendomain natural language text is beyond current technology, a number of papers have been recently published [1,2,3] showing that, by using probabilistic or symbolic methods, it is possible to obtain dependencybased representations of unlimited texts with good recall and precision. Consequently, we believe it should be possible to augment the manual-annotation-based approach with automatically built annotations by extracting a limited subset of semantic relations from unstructured text. In short, shallow/partial text understanding on the level of semantic relations, an extended label including Predicate-Argument Structures and other syntactically and semantically derivable head modifiers and adjuncts. This approach is promising because it attempts to address the well-known shortcomings of standard “bag-of-words” (BOWs) information retrieval/extraction techniques without requiring manual intervention: it develops current NLP technologies which make heavy use of statistically and FSA based approaches to syntactic parsing. GETARUNS [4,5,6], a text understanding system (TUS), developed in collaboration between the University of Venice and the University of Parma, can perform semantic analysis on the basis of syntactic parsing and, after performing anaphora resolution, builds a quasi  logical form with flat indexed Augmented Dependency Structures (ADSs). In addition, it uses a centering algorithm to individuate the topics or discourse centers which are weighted on the basis of a relevance score. This logical form can then be used to individuate the best sentence candidates to answer queries or provide appropriate information. This paper is organized as follows: in section 2 below we discuss why deep linguistic processing is needed in Information Retrieval and Information Extraction; in section 3 we present GETARUNS, the NLP system and the Upper Module of GETARUNS; in section 4 we describe two experiments with state-of-the-art benchmark corpora. 2 Ternary Expressions as PredicateArgument Structures Researchers like Lin, Katz and Litkowski have started to work in the direction of using NLP to populate a database of RDFs, thus creating the premises for the automatic creation of ontologies to be used in the IR/IE tasks. However, in no way RDFs and ternary expressions may constitute a formal tool sufficient to express the complexity of natural language texts. RDFs are assertions about the things (people, Webpages and whatever) they predicate about by asserting that they have certain properties with certain values. If we may agree with the fact that this is natural way of dealing with data handled by computers most frequently, it also a fact that this is not equivalent as being useful for natural language. The misconception seems to be deeply embedded in the nature of RDFs as a whole: they are directly comparable to attribute-value pairs and DAGs which are also the formalism used by most recent linguistic unification-based grammars. From the logical and semantic point of view RDFs also resemble very closely first order predicate logic constructs: but we must remember that FOPL is as such insufficient to describe natural language texts. Ternary expressions(T-expressions), <subject relation object>. Certain other parameters (adjectives, possessive nouns, prepositional phrases, etc.) are used to create additional T-expressions in which prepositions and several special words may serve as relations. For instance, the following simple sentence 
Assume that you are looking for information about a particular person. A search engine returns many pages for that person’s name. Some of these pages may be on other people with the same name. One method to reduce the ambiguity in the query and ﬁlter out the irrelevant pages, is by adding a phrase that uniquely identiﬁes the person we are interested in from his/her namesakes. We propose an unsupervised algorithm that extracts such phrases from the Web. We represent each document by a term-entity model and cluster the documents using a contextual similarity metric. We evaluate the algorithm on a dataset of ambiguous names. Our method outperforms baselines, achieving over 80% accuracy and signiﬁcantly reduces the ambiguity in a web search task. 
We consider the question of how information from the textual context of citations in scientiﬁc papers could improve indexing of the cited papers. We ﬁrst present examples which show that the context should in principle provide better and new index terms. We then discuss linguistic phenomena around citations and which type of processing would improve the automatic determination of the right context. We present a case study, studying the effect of combining the existing index terms of a paper with additional terms from papers citing that paper in our corpus. Finally, we discuss the need for experimentation for the practical validation of our claim. 
In this paper, we explore the use of structured content as semantic constraints for enhancing the performance of traditional term-based document retrieval in special domains. First, we describe a method for automatic extraction of semantic content in the form of attribute-value (AV) pairs from natural language texts based on domain models constructed from a semistructured web resource. Then, we explore the effect of combining a state-ofthe-art term-based IR system and a simple constraint-based search system that uses the extracted AV pairs. Our evaluation results have shown that such combination produces some improvement in IR performance over the term-based IR system on our test collection. 
A key task in an extraction system for query-oriented multi-document summarisation, necessary for computing relevance and redundancy, is modelling text semantics. In the Embra system, we use a representation derived from the singular value decomposition of a term co-occurrence matrix. We present methods to show the reliability of performance improvements. We ﬁnd that Embra performs better with dimensionality reduction. 
This paper presents experiments with the evaluation of automatically produced summaries of literary short stories. The summaries are tailored to a particular purpose of helping a reader decide whether she wants to read the story. The evaluation procedure includes extrinsic and intrinsic measures, as well as subjective and factual judgments about the summaries pronounced by human subjects. The experiments confirm the experience of summarizing more conventional genres: sentence overlap between human- and machine-made summaries is not a complete picture of the quality of a summary. In fact, in our case, sentence overlap does not correlate well with human judgment. We explain the evaluation procedures and discuss several challenges of evaluating summaries of works of fiction. 
This paper proposes methods to pre-process questions in the postings before a QA system can find answers in a discussion group in the Internet. Pre-processing includes garbage text removal and question segmentation. Garbage keywords are collected and different length thresholds are assigned to them for garbage text identification. Interrogative forms and question types are used to segment questions. The best performance on the test set achieves 92.57% accuracy in garbage text removal and 85.87% accuracy in question segmentation, respectively. 
Unlike open-domain factoid questions, clinical information needs arise within the rich context of patient treatment. This environment establishes a number of constraints on the design of systems aimed at physicians in real-world settings. In this paper, we describe a clinical question answering system that focuses on a class of commonly-occurring questions: “What is the best drug treatment for X?”, where X can be any disease. To evaluate our system, we built a test collection consisting of thirty randomly-selected diseases from an existing secondary source. Both an automatic and a manual evaluation demonstrate that our system compares favorably to PubMed, the search system most commonly-used by physicians today. 
This paper describes a novel framework for using scenario knowledge in opendomain Question Answering (Q/A) applications that uses a state-of-the-art textual entailment system (Hickl et al., 2006b) in order to discover textual information relevant to the set of topics associated with a scenario description. An intrinsic and an extrinsic evaluation of this method is presented in the context of an automatic Q/A system and results from several user scenarios are discussed. 
We present a comparative study of corpusbased methods for the automatic synthesis of email responses to help-desk requests. Our methods were developed by considering two operational dimensions: (1) information-gathering technique, and (2) granularity of the information. In particular, we investigate two techniques – retrieval and prediction – applied to information represented at two levels of granularity – sentence-level and document level. We also developed a hybrid method that combines prediction with retrieval. Our results show that the different approaches are applicable in different situations, addressing a combined 72% of the requests with either complete or partial responses. 
The Document Understanding Conference (DUC) 2005 evaluation had a single useroriented, question-focused summarization task, which was to synthesize from a set of 25-50 documents a well-organized, ﬂuent answer to a complex question. The evaluation shows that the best summarization systems have difﬁculty extracting relevant sentences in response to complex questions (as opposed to representative sentences that might be appropriate to a generic summary). The relatively generous allowance of 250 words for each answer also reveals how difﬁcult it is for current summarization systems to produce ﬂuent text from multiple documents. 
We describe an unusual data set of thousands of annotated images with interesting sense phenomena. Natural language image sense annotation involves increased semantic complexities compared to disambiguating word senses when annotating text. These issues are discussed and illustrated, including the distinction between word senses and iconographic senses. 
In this paper, we present a semiautomatic approach for annotating semantic information in biomedical texts. The information is used to construct a biomedical proposition bank called BioProp. Like PropBank in the newswire domain, BioProp contains annotations of predicate argument structures and semantic roles in a treebank schema. To construct BioProp, a semantic role labeling (SRL) system trained on PropBank is used to annotate BioProp. Incorrect tagging results are then corrected by human annotators. To suit the needs in the biomedical domain, we modify the PropBank annotation guidelines and characterize semantic roles as components of biological events. The method can substantially reduce annotation efforts, and we introduce a measure of an upper bound for the saving of annotation efforts. Thus far, the method has been applied experimentally to a 4,389-sentence treebank corpus for the construction of BioProp. Inter-annotator agreement measured by kappa statistic reaches .95 for combined decision of role identification and classification when all argument labels are considered. In addition, we show that, when trained on BioProp, our biomedical SRL system called BIOSMILE achieves an F-score of 87%.  
This work reports on three human tense annotation experiments for Chinese verbs in Chinese-to-English translation scenarios. The results show that inter-annotator agreement increases as the context of the verb under the annotation becomes increasingly speciﬁed, i.e. as the context moves from the situation in which the target English sentence is unknown to the situation in which the target lexicon and target syntactic structure are fully speciﬁed. The annotation scheme with a fully speciﬁed syntax and lexicon in the target English sentence yields a satisfactorily high agreement rate. The annotation results were then analyzed via an ANOVA analysis, a logistic regression model and a log-linear model. The analyses reveal that while both the overt and the latent linguistic factors seem to signiﬁcantly affect annotation agreement under different scenarios, the latent features are the real driving factors of tense annotation disagreement among multiple annotators. The analyses also ﬁnd the verb telicity feature, aspect marker presence and syntactic embedding structure to be strongly associated with tense, suggesting their utility in the automatic tense classiﬁcation task. 
The paper reports on a detailed quantitative analysis of distributional language data of both Italian and Czech, highlighting the relative contribution of a number of distributed grammatical factors to sentence-based identification of subjects and direct objects. The work uses a Maximum Entropy model of stochastic resolution of conflicting grammatical constraints and is demonstrably capable of putting explanatory theoretical accounts to the test of usage-based empirical verification. 
The languages that are most commonly subject to linguistic annotation on a large scale tend to be those with the largest populations or with recent histories of linguistic scholarship. In this paper we discuss the problems associated with lowerdensity languages in the context of the development of linguistically annotated resources. We frame our work with three key questions regarding the deﬁnition of lower-density languages; increasing available resources and reducing data requirements. A number of steps forward are identiﬁed for increasing the number lowerdensity language corpora with linguistic annotations. 
 place a low emphasis on NLP-friendly properties (Minimalism. Optimality Theory, etc.).  This report explores the question of compatibility between annotation projects including translating annotation formalisms to each other or to common forms. Compatibility issues are crucial for systems that use the results of multiple annotation projects. We hope that this report will begin a concerted effort in the field to track the compatibility of annotation schemes for part of speech tagging, time annotation, treebanking, role labeling and other phenomena. 1. Introduction Different corpus annotation projects are driven by different goals, are applied to different types of data (different genres, different languages, etc.) and are created by people with different intellectual backgrounds. As a result of these and other factors, different annotation efforts make different underlying theoretical assumptions. Thus, no annotation project is really theoryneutral, and in fact, none should be. It is the theoretical concerns which make it possible to write the specifications for an annotation project and which cause the resulting annotation to be consistent and thus usable for various natural language processing (NLP) applications. Of course the theories chosen for annotation projects tend to be theories that are useful for NLP. They place a high value on descriptive adequacy (they cover the data), they are formalized sufficiently for consistent annotation to be possible, and they tend to share major theoretical assumptions with other annotation efforts, e.g., the noun is the head of the noun phrase, the verb is the head of the sentence, etc. Thus the term theory-neutral is often used to mean something like NLP-friendly. Obviously, the annotation compatibility problem that we address here is much simpler than it would be if we had to consider theories which  As annotation projects are usually research efforts, the inherent theoretical differences may be viewed as part of a search for the truth and the enforcement of adherence to a given (potentially wrong) theory could hamper this search. In addition, annotation of particular phenomena may be simplified by making theoretical assumptions conducive to describing those phenomena. For example, relative pronouns (e.g., that in the NP the book that she read) may be viewed as pronouns in an anaphora annotation project, but as intermediate links to arguments for a study of predicate argument structure. On the other hand, many applications would benefit by merging the results of different annotation projects. Thus, differences between annotation projects may be viewed as obstacles. For example, combining two or more corpora annotated with the same information may improve a system (i.e., "there's no data like more data.") To accomplish this, it may be necessary to convert corpora annotated according to one set of specifications into a different system or to convert two annotation systems into a third system. For example, to obtain lots of part of speech data for English, it is advantageous to convert POS tags from several tagsets (see Section 2) into a common form. For more temporal data than is available in Timex3 format, one might have to convert Timex2 and Timex3 tags into a common form (See Section 5). Compromises that do not involve conversion can be flawed. For example, a machine learner may determine that feature A in framework 1 predicts feature A' in framework 2. However, the system may miss that features A and B in framework 1 actually both correspond to feature A', i.e., they are subtypes. In our view, directly modeling the parameters of compatibility would be preferable.  38 Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006, pages 38–53, Sydney, July 2006. c 2006 Association for Computational Linguistics  Some researchers have attempted to combine a number of different resource annotations into a single merged form. One motivation is that the merged representation may be more than the sum of its parts. It is likely that inconsistencies and errors (often induced by task-specific biases) can be identified and adjusted in the merging process; inferences may be drawn from how the component annotation systems interact; a complex annotation in a single framework may be easier for a system to process than several annotations in different frameworks; and a merged framework will help guide further annotation research (Pustojevsky, et. al. 2005). Another reason to merge is that a merged resource in language A may be similar to an existing resource in language B. Thus merging resources may present opportunities for constructing nearly parallel resources, which in turn could prove useful for a multilingual application. Merging PropBank (Kingsbury, and Palmer 2002) and NomBank (Meyers, et. al. 2004) would yield a predicate argument structure for nouns and verbs, carrying more similar information to the Praque Dependency TreeBank's TectoGrammatical structure (Hajicova and Ceplova, 2000) than either component. This report and an expanded online version http://nlp.cs.nyu.edu/wiki/corpuswg/Annotation Compatibility both describe how to find correspondences between annotation frameworks. This information can be used to combine various annotation resources in different ways, according to one’s research goals, and, perhaps, could lead to some standards for combining annotation. This report will outline some of our initial findings in this effort with an eye towards maintaining and updating the online version in the future. We hope this is a step towards making it easier for systems to use multiple annotation resources. 2. Part of Speech and Phrasal Categories On our website, we provide correspondences among a number of different part of speech tagsets in a version of the table from pp. 141-142 of Manning and Schütze (1999), modified to include the POS classes from CLAWS1 and ICE. Table 1 is a sample taken from this table for expository purposes (the full table is not provided due to space limitations). Traditionally,  part of speech represents a fairly coarse-grained division among types of words, usually distinguishing among: nouns, verbs, adjectives, adverbs, determiners and possibly a few other classes. While part of speech classifications may vary for particular words, especially closed class items, we have observed a larger problem. Most part of speech annotation projects incorporate other distinctions into part of speech classification. Furthermore, they incorporate different types of distinctions. As a result, conversion between one tagset and another is rarely one to one. It can, in fact, be many to many, e.g., BROWN does not distinguish the  Table 1: Part of Speech Compatibility  Extending Manning and Schütze 1999, pp. 141-142, to cover Claws1 and ICE -- Longer Version Online  Class  Wrds  Claws c5, Claws1  Brow n  PTB  ICE  HapAdj py, AJ0 bad  JJ  JJ  ADJ. ge  hap-  Adj, pier, comp wors  AJC  e  JJR  JJR  ADJ. comp  Adj, super  nicestworst  AJS  JJT  JJS  ADJ. sup  Adj, past eaten JJ part  ??  VBN ADJ. , JJ edp  Adj, pres part  calming  JJ  ??  VBG ADJ. , JJ ingp  slow-  Adv  ly, sweet  AV0  -ly  RB  RB  ADV. ge  Adv comp faster AV0  RBR  RBR  ADV. comp  Adv fastsuper est  AV0  RBT  RBS  ADV. sup  Adv up, Part off, out  AVP, RP, RI  RP  RP  ADV. {phras, ge}  Conj and, CJC,  coord or  CC  CC  CC  CONJUNC.  39  this,  Det  each, DT0, ano- DT  ther  Det. any, DT0, pron some DTI  Det pron these DT0, Plur those DTS  Det preq  quite  DT0, aBL  Det preq  all, half  DT0, ABN  airNoun craft, NN0 data  Noun sing  cat, pen  NN1  Noun plur  cats, pens  NN2  Noun prop sing  Paris, Mike  NP0  Verb. base take, pres live  VVB  Verb, infin Verb, past Verb, pres part Verb, pastpart  take, live took, lived taking, living taken , lived  VVI VVD VVG VVN  Verb, pres  takes ,  VVZ  coord  DT DT DT1 DT DTS DT ABL PDT ABN PDT NN NN  PRON. dem.si ng, PRON (recip) PRON. nonass, PRON. ass PRON. dem. plu ADV .intens PRON. univ, PRON. quant N.com. sing  infinitive form of a verb (VB in the Penn Treebank, V.X.infin in ICE) from the presenttense form (VBP in the Penn Treebank, V.X.pres in ICE) that has the same spelling (e.g., see in They see no reason to leave). In contrast, ICE distinguishes among several different subcategories of verb (cop, intr, cxtr, dimontr, ditr, montr and TRANS) and the Penn Treebank does not.1 In a hypothetical system which merges all the different POS tagsets, it would be advantageous to factor out different types of features (similar to ICE), but include all the distinctions made by all the tag sets. For example, if a token give is tagged as VBP in the Penn Treebank, VBP would be converted into VERB.anysubc.pres. If another token give was tagged VB in Brown, VB would be converted to VERB.anysubc{infin,n3pres} (n3pres = not-3rdperson and present tense). This allows systems to acquire the maximum information from corpora, tagged by different research groups.  CKIP Chinese-Treebank (CCTB) and Penn  NN  NN  N.com. Chinese Treebank (PCTB) are two important  sing  resources for Treebank-derived Chinese NLP  tasks (CKIP, 1995; Xia et al., 2000; Xu et al.,  NNS  NNS  N.com. plu  2002; Li et al., 2004). CCTB is developed in traditional Chinese (BIG5-encoded) at the  Academia Sinica, Taiwan (Chen et al., 1999;  Chen et al., 2003). CCTB uses the Information-  NP  NNP  N.prop .sing  based Case Grammar (ICG) framework to express both syntactic and semantic descriptions.  The present version CCTB3 (Version 3) provides  VB  V.X. VBP {pres,  61,087 Chinese sentences, 361,834 words and 6 files that are bracketed and post-edited by  imp} humans based on a 5-million-word tagged Sinica  Corpus (CKIP, 1995). CKIP POS tagging is a  VB  VB  V.X. infin  hierarchical system. The first POS layers include eight main syntactic categories, i.e. N (noun), V  VBD  VBD  V.X. past  (verb), D (adverb), A (adjective), C (conjunction), I (interjection), T (particles) and P (preposition). In CCTB, there are 6 non-terminal  phrasal categories: S (a complete tree headed by  VBG  VBG  V.X. ingp  a predicate), VP (a phrase headed by a predicate), NP (a phrase beaded by an N), GP (a  phrase headed by locational noun or adjunct), PP  VBN  VBN  V.X. edp  VBZ  VBZ  V.X. pres  
This paper applies the categories from an opinion annotation scheme developed for monologue text to the genre of multiparty meetings. We describe modifications to the coding guidelines that were required to extend the categories to the new type of data, and present the results of an inter-annotator agreement study. As researchers have found with other types of annotations in speech data, interannotator agreement is higher when the annotators both read and listen to the data than when they only read the transcripts. Previous work exploited prosodic clues to perform automatic detection of speaker emotion (Liscombe et al. 2003). Our findings suggest that doing so to recognize opinion categories would be a promising line of work. 
Semantic information is important for precise word sense disambiguation system and the kind of semantic analysis used in sophisticated natural language processing such as machine translation, question answering, etc. There are at least two kinds of semantic information: lexical semantics for words and phrases and structural semantics for phrases and sentences. We have built a Japanese corpus of over three million words with both lexical and structural semantic information. In this paper, we focus on our method of annotating the lexical semantics, that is building a word sense tagged corpus and its properties. 
The PropBank primarily adds semantic role labels to the syntactic constituents in the parsed trees of the Treebank. The goal is for automatic semantic role labeling to be able to use the domain of locality of a predicate in order to find its arguments. In principle, this is exactly what is wanted, but in practice the PropBank annotators often make choices that do not actually conform to the Treebank parses. As a result, the syntactic features extracted by automatic semantic role labeling systems are often inconsistent and contradictory. This paper discusses in detail the types of mismatches between the syntactic bracketing and the semantic role labeling that can be found, and our plans for reconciling them. 
We present a comparison of two formalisms for representing natural language utterances, namely deep syntactical Tectogrammatical Layer of Functional Generative Description (FGD) and a semantic formalism, MultiNet. We discuss the possible position of MultiNet in the FGD framework and present a preliminary mapping of representational means of these two formalisms. 
As the interest in annotated corpora is spreading, there is increasing concern with using existing language technology for corpus processing. In this paper we explore the idea of using natural language generation systems for corpus annotation. Resources for generation systems often focus on areas of linguistic variability that are under-represented in analysis-directed approaches. Therefore, making use of generation resources promises some signiﬁcant extensions in the kinds of annotation information that can be captured. We focus here on exploring the use of the KPML (Komet-Penman MultiLingual) generation system for corpus annotation. We describe the kinds of linguistic information covered in KPML and show the steps involved in creating a standard XML corpus representation from KPML’s generation output. 
This paper presents the English valency lexicon EngValLex, built within the Functional Generative Description framework. The form of the lexicon, as well as the process of its semi-automatic creation is described. The lexicon describes valency for verbs and also includes links to other lexical sources, namely PropBank. Basic statistics about the lexicon are given. The lexicon will be later used for annotation of the Wall Street Journal section of the Penn Treebank in Praguian formalisms. 
This paper describes a pattern-based method to automatically enrich a core ontology with the definitions of a domain glossary. We show an application of our methodology to the cultural heritage domain, using the CIDOC CRM core ontology. To enrich the CIDOC, we use available resources such as the AAT art and architecture glossary, WordNet, the Dmoz taxonomy for named entities, and others. 
In this paper, we outline the development of a system that automatically constructs ontologies by extracting knowledge from dictionary deﬁnition sentences using Robust Minimal Recursion Semantics (RMRS). Combining deep and shallow parsing resource through the common formalism of RMRS allows us to extract ontological relations in greater quantity and quality than possible with any of the methods independently. Using this method, we construct ontologies from two different Japanese lexicons and one English lexicon. We then link them to existing, handcrafted ontologies, aligning them at the word-sense level. This alignment provides a representative evaluation of the quality of the relations being extracted. We present the results of this ontology construction and discuss how our system was designed to handle multiple lexicons and languages. 
One of the challenging tasks in the context of the Semantic Web is to automatically extract instances of binary relations from Web documents – for example all pairs of a person and the corresponding birthdate. In this paper, we present LEILA, a system that can extract instances of arbitrary given binary relations from natural language Web documents – without human interaction. Different from previous approaches, LEILA uses a deep syntactic analysis. This results in consistent improvements over comparable systems (such as e.g. Snowball or TextToOnto). 
In this paper we propose and investigate Ontology Population from Textual Mentions (OPTM), a sub-task of Ontology Population from text where we assume that mentions for several kinds of entities (e.g. PERSON, O R G A N I Z A T I O N , LO C A T I O N , GEOPOLITICAL_ ENTITY) are already extracted from a document collection. On the one hand, OPTM simplifies the general Ontology Population task, limiting the input textual material; on the other hand, it introduces challenging extensions to Ontology Population restricted to named entities, being open to a wider spectrum of linguistic phenomena. We describe a manually created benchmark for OPTM and discuss several factors which determine the difficulty of the task. 
In this paper we develop an automatic classiﬁer for a very large set of labels, the WordNet synsets. We employ Conditional Random Fields (CRFs) because of their ﬂexibility to include a wide variety of nonindependent features. Training CRFs on a big number of labels proved a problem because of the large training cost. By taking into account the hypernym/hyponym relation between synsets in WordNet, we reduced the complexity of training from O(T M 2N G) to O(T (logM )2N G) with only a limited loss in accuracy. 
Learning taxonomy for technical terms is difficult and tedious task, especially when new terms should be included. The goal of this paper is to assign taxonomic relations among technical terms. We propose new approach to the problem that relies on term specificity and similarity measures. Term specificity and similarity are necessary conditions for taxonomy learning, because highly specific terms tend to locate in deep levels and semantically similar terms are close to each other in taxonomy. We analyzed various features used in previous researches in view of term specificity and similarity, and applied optimal features for term specificity and similarity to our method. 
In this paper, we describe a rote extractor that learns patterns for ﬁnding semantic relations in unrestricted text, with new procedures for pattern generalisation and scoring. An improved method for estimating the precision of the extracted patterns is presented. We show that our method approximates the precision values as evaluated by hand much better than the procedure traditionally used in rote extractors. 
We present an approach for extracting relations from texts that exploits linguistic and empirical strategies, by means of a pipeline method involving a parser, partof-speech tagger, named entity recognition system, pattern-based classification and word sense disambiguation models, and resources such as ontology, knowledge base and lexical databases. The relations extracted can be used for various tasks, including semantic web annotation and ontology learning. We suggest that the use of knowledge intensive strategies to process the input text and corpusbased techniques to deal with unpredicted cases and ambiguity problems allows to accurately discover the relevant relations between pairs of entities in that text. 
One of the central assumptions of Optimality Theory is the hypothesis of strict domination among constraints. A few studies have suggested that this hypothesis is too strong and should be abandoned in favor of a weaker cumulativity hypothesis. If this suggestion is correct, we should be able to find evidence for cumulativity in the comprehension of Gapping sentences, which lack explicit syntactic clues in the form of the presence of a finite verb. On the basis of a comparison between several computational models of constraint evaluation, we conclude that the comprehension of Gapping sentences does not yield compelling evidence against the strict domination hypothesis. 
We provide two different methods for bounding search when parsing with freer word-order languages. Both of these can be thought of as exploiting alternative sources of constraints not commonly used in CFGs, in order to make up for the lack of more rigid word-order and the standard algorithms that use the assumption of rigid word-order implicitly. This work is preliminary in that it has not yet been evaluated on a large-scale grammar/corpus for a freer word-order language. 
The constraint-oriented approaches to language processing step back from the generative theory and make it possible, in theory, to deal with all types of linguistic relationships (e.g. dependency, linear precedence or immediate dominance) with the same importance when parsing an input utterance. Yet in practice, all implemented constraint-oriented parsing strategies still need to discriminate between “important” and “not-so-important” types of relations during the parsing process. In this paper we introduce a new constraint-oriented parsing strategy based on Property Grammars, which overcomes this drawback and grants the same importance to all types of relations. 
The literature investigating the notion of presupposition in Discourse Representation Theory (DRT) has mainly been dubbed as being semantic (Simons 2003). This paper investigates the linguistic application of pragmatic-based constraints to the ‘semantic’ notion of presupposition in DRT. By applying pragmatic-based constraints to presuppositional phenomenon, we aim to defend DRT against the accusation that DRT’s interpretation of presuppositional phenomenon is essentially ‘semantic’ and push this interpretation further towards the pragmatic side of the semantic/pragmatic interface.1 
This article presents a novel syntactic parser architecture, in which a linguistic formalism can be enriched with all sorts of constraints, included extra-linguistic ones, thanks to the seamless coupling of the formalism with a programming language. 
In spite of its potential for bidirectionality, Extensible Dependency Grammar (XDG) has so far been used almost exclusively for parsing. This paper represents one of the ﬁrst steps towards an XDG-based integrated generation architecture by tackling what is arguably the most basic among generation tasks: lexicalization. Herein we present a constraint-based account of disjunction in lexicalization, i.e. a way to enable an XDG grammar to generate all paraphrases — along the lexicalization axis, of course — realizing a given input semantics. Our model is (i) efﬁcient, yielding strong propagation, (ii) modular and (iii) favourable to synergy inasmuch as it allows collaboration between modules, notably semantics and syntax. We focus on constraints ensuring wellformedness and completeness and avoiding over-redundancy. 
This paper presents a method for identifying an opinion with its holder and topic, given a sentence from online news media texts. We introduce an approach of exploiting the semantic structure of a sentence, anchored to an opinion bearing verb or adjective. This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using data from FrameNet. We decompose our task into three phases: identifying an opinion-bearing word, labeling semantic roles related to the word in the sentence, and then finding the holder and the topic of the opinion word among the labeled semantic roles. For a broader coverage, we also employ a clustering technique to predict the most probable frame for a word which is not defined in FrameNet. Our experimental results show that our system performs significantly better than the baseline. 
We target the problem of linking source mentions that belong to the same entity (source coreference resolution), which is needed for creating opinion summaries. In this paper we describe how source coreference resolution can be transformed into standard noun phrase coreference resolution, apply a state-of-the-art coreference resolution approach to the transformed data, and evaluate on an available corpus of manually annotated opinions. 
On the World Wide Web, the volume of subjective information, such as opinions and reviews, has been increasing rapidly. The trends and rules latent in a large set of subjective descriptions can potentially be useful for decision-making purposes. In this paper, we propose a method for summarizing subjective descriptions, speciﬁcally opinions in Japanese. We visualize the pro and con arguments for a target topic, such as “Should Japan introduce the summertime system?” Users can summarize the arguments about the topic in order to choose a more reasonable standpoint for decision making. We evaluate our system, called “OpinionReader”, experimentally. 
Recent advances in text analysis have led to finer-grained semantic analysis, including automatic sentiment analysis— the task of measuring documents, or chunks of text, based on emotive categories, such as positive or negative. However, considerably less progress has been made on efficient ways of exploring these measurements. This paper discusses approaches for visualizing the affective content of documents and describes an interactive capability for exploring emotion in a large document collection. 
An emerging task in text understanding and generation is to categorize information as fact or opinion and to further attribute it to the appropriate source. Corpus annotation schemes aim to encode such distinctions for NLP applications concerned with such tasks, such as information extraction, question answering, summarization, and generation. We describe an annotation scheme for marking the attribution of abstract objects such as propositions, facts and eventualities associated with discourse relations and their arguments annotated in the Penn Discourse TreeBank. The scheme aims to capture the source and degrees of factuality of the abstract objects. Key aspects of the scheme are annotation of the text spans signalling the attribution, and annotation of features recording the source, type, scopal polarity, and determinacy of attribution. 
This paper presents a method for searching the web for sentences expressing opinions. To retrieve an appropriate number of opinions that users may want to read, declaratively subjective clues are used to judge whether a sentence expresses an opinion. We collected declaratively subjective clues in opinionexpressing sentences from Japanese web pages retrieved with opinion search queries. These clues were expanded with the semantic categories of the words in the sentences and were used as feature parameters in a Support Vector Machine to classify the sentences. Our experimental results using retrieved web pages on various topics showed that the opinion expressing sentences identified by the proposed method are congruent with sentences judged by humans to express opinions. 
We report progress on adding affectdetection to a program for virtual dramatic improvisation, monitored by a human director. We have developed an affect-detection module to control an automated virtual actor and to contribute to the automation of directorial functions. The work also involves basic research into how affect is conveyed through metaphor. The project contributes to the application of sentiment and subjectivity analysis to the creation of emotionally believable synthetic agents for interactive narrative environments. 
In this paper, we present the results of experiments aiming to validate a twodimensional typology of affective states as a suitable basis for affective classiﬁcation of texts. Using a corpus of English weblog posts, annotated for mood by their authors, we trained support vector machine binary classiﬁers to distinguish texts on the basis of their afﬁliation with one region of the space. We then report on experiments which go a step further, using four-class classiﬁers based on automated scoring of texts for each dimension of the typology. Our results indicate that it is possible to extend the standard binary sentiment analysis (positive/negative) approach to a two dimensional model (positive/negative; active/passive), and provide some evidence to support a more ﬁne-grained classiﬁcation along these two axes. 
The automatic extraction of trend information from text documents such as newspaper articles would be useful for exploring and examining trends. To enable this, we used data sets provided by a workshop on multimodal summarization for trend information (the MuST Workshop) to construct an automatic trend exploration system. This system ﬁrst extracts units, temporals, and item expressions from newspaper articles, then it extracts sets of expressions as trend information, and ﬁnally it arranges the sets and displays them in graphs. For example, when documents concerning the politics are given, the system extracts “%” and “Cabinet approval rating” as a unit and an item expression including temporal expressions. It next extracts values related to “%”. Finally, it makes a graph where temporal expressions are used for the horizontal axis and the value of percentage is shown on the vertical axis. This graph indicates the trend of Cabinet approval rating and is useful for investigating Cabinet approval rating. Graphs are obviously easy to recognize and useful for understanding information described in documents. In experiments, when we judged the extraction of a correct  graph as the top output to be correct, the system accuracy was 0.2500 in evaluation A and 0.3334 in evaluation B. (In evaluation A, a graph where 75% or more of the points were correct was judged to be correct; in evaluation B, a graph where 50% or more of the points were correct was judged to be correct.) When we judged the extraction of a correct graph in the top ﬁve outputs to be correct, accuracy rose to 0.4167 in evaluation A and 0.6250 in evaluation B. Our system is convenient and effective because it can output a graph that includes trend information at these levels of accuracy when given only a set of documents as input. 
Several recently reported techniques for the automatic acquisition of Information Extraction (IE) systems have used dependency trees as the basis of their extraction pattern representation. These approaches have used a variety of pattern models (schemes for representing IE patterns based on particular parts of the dependency analysis). An appropriate model should be expressive enough to represent the information which is to be extracted from text without being overly complicated. Four previously reported pattern models are evaluated using existing IE evaluation corpora and three dependency parsers. It was found that one model, linked chains, could represent around 95% of the information of interest without generating an unwieldy number of possible patterns. 
This paper deals with the use of computational linguistic analysis techniques for information access and ontology learning within the legal domain. We present a rule-based approach for extracting and analysing definitions from parsed text and evaluate it on a corpus of about 6000 German court decisions. The results are applied to improve the quality of a text based ontology learning method on this corpus.1 
This paper presents a novel approach to the semi-supervised learning of Information Extraction patterns. The method makes use of more complex patterns than previous approaches and determines their similarity using a measure inspired by recent work using kernel methods (Culotta and Sorensen, 2004). Experiments show that the proposed similarity measure outperforms a previously reported measure based on cosine similarity when used to perform binary relation extraction. 
Lexical Chains are powerful representations of documents. In particular, they have successfully been used in the ﬁeld of Automatic Text Summarization. However, until now, Lexical Chaining algorithms have only been proposed for English. In this paper, we propose a greedy Language-Independent algorithm that automatically extracts Lexical Chains from texts. For that purpose, we build a hierarchical lexico-semantic knowledge base from a collection of texts by using the Pole-Based Overlapping Clustering Algorithm. As a consequence, our methodology can be applied to any language and proposes a solution to languagedependent Lexical Chainers. 
We present two semi-supervised learning techniques to improve a state-of-the-art multi-lingual name tagger. For English and Chinese, the overall system obtains 1.7% - 2.1% improvement in F-measure, representing a 13.5% - 17.4% relative reduction in the spurious, missing, and incorrect tags. We also conclude that simply relying upon large corpora is not in itself sufficient: we must pay attention to unlabeled data selection too. We describe effective measures to automatically select documents and sentences. 
An unsupervised learning method, based on corpus linguistics and special language terminology, is described that can extract time-varying information from text streams. The method is shown to be ‘language-independent’ in that its use leads to sets of regular-expressions that can be used to extract the information in typologically distinct languages like English and Arabic. The method uses the information related to the distribution of Ngrams, for automatically extracting ‘meaning bearing’ patterns of usage in a training corpus. The analysis of an English news wire corpus (1,720,142 tokens) and Arabic news wire corpus (1,720,154 tokens) show encouraging results. 
Many information extraction (IE) systems rely on manually annotated training data to learn patterns or rules for extracting information about events. Manually annotating data is expensive, however, and a new data set must be annotated for each domain. So most IE training sets are relatively small. Consequently, IE patterns learned from annotated training sets often have limited coverage. In this paper, we explore the idea of using the Web to automatically identify domain-speciﬁc IE patterns that were not seen in the training data. We use IE patterns learned from the MUC-4 training set as anchors to identify domain-speciﬁc web pages and then learn new IE patterns from them. We compute the semantic afﬁnity of each new pattern to automatically infer the type of information that it will extract. Experiments on the MUC-4 test set show that these new IE patterns improved recall with only a small precision loss. 
Thesauruses are useful resources for NLP; however, manual construction of thesaurus is time consuming and suffers low coverage. Automatic thesaurus construction is developed to solve the problem. Conventional way to automatically construct thesaurus is by finding similar words based on context vector models and then organizing similar words into thesaurus structure. But the context vector methods suffer from the problems of vast feature dimensions and data sparseness. Latent Semantic Index (LSI) was commonly used to overcome the problems. In this paper, we propose a feature clustering method to overcome the same problems. The experimental results show that it performs better than the LSI models and do enhance contextual information for infrequent words. 
This paper reports on an initial and necessary step toward the construction of a Pan-Chinese lexical resource. We investigated the regional variation of lexical items in two specific domains, finance and sports; and explored how much of such variation is covered in existing Chinese synonym dictionaries, in particular the Tongyici Cilin. The domain-specific lexical items were obtained from subsections of a synchronous Chinese corpus, LIVAC. Results showed that 20-40% of the words from various subcorpora are unique to the individual communities, and as much as 70% of such unique items are not yet covered in the Tongyici Cilin. The results suggested great potential for building a Pan-Chinese lexical resource for Chinese language processing. Our next step would be to explore automatic means for extracting related lexical items from the corpus, and to incorporate them into existing semantic classifications. 
An HMM-based Single Character Recovery (SCR) Model is proposed in this paper to extract a large set of “atomic abbreviation pairs”from a large text corpus. By an “atomic abbreviation pair,”it refers to an abbreviated word and its root word (i.e., unabbreviated form) in which the abbreviation is a single Chinese character. This task is interesting since the abbreviation process for Chinese compound words seems to be “compositional”; in other words, one can often decode an abbreviated word, such as “台大”(Taiwan University), character-by-character back to its root form. With a large atomic abbreviation dictionary, one may be able to recover multiple-character abbreviations more easily. With only a few training iterations, the acquisition accuracy of the proposed SCR model achieves 62% and 50 % precision for training set and test set, respectively, from the ASWSC-2001 corpus. 
This paper presents a semantic model for Chinese garden-path sentences. Based on the Sentence Degeneration model of HNC theory, a garden-path can arise from two types of ambiguities: SD type ambiguity and NP allocated ambiguity. This paper provides an approach to process garden-paths, in which ambiguity detection and analysis take the place of revision. The performance of the approach is evaluated on a small manually annotated test set. The results show that our algorithm can analyze Chinese garden-path sentences effectively. 
Coreference resolution is the process of identifying expressions that refer to the same entity. This paper presents a clustering algorithm for unsupervised Chinese coreference resolution. We investigate why Chinese coreference is hard and demonstrate that techniques used in coreference resolution for English can be extended to Chinese. The proposed system exploits clustering as it has advantages over traditional classification methods, such as the fact that no training data is required and it is easily extended to accommodate additional features. We conduct a set of experiments to investigate how noun phrase identification and feature selection can contribute to coreference resolution performance. Our system is evaluated on an annotated version of the TDT3 corpus using the MUC-7 scorer, and obtains comparable performance. We believe that this is the first attempt at an unsupervised approach to Chinese noun phrase coreference resolution. 
On the task of determining the tense to use when translating a Chinese verb into English, current systems do not perform as well as human translators. The main focus of the present paper is to identify features that human translators use, but which are not currently automatically extractable. The goal is twofold: to test a particular hypothesis about what additional information human translators might be using, and as a pilot to determine where to focus effort on developing automatic extraction methods for features that are somewhat beyond the reach of current feature extraction. The paper shows that incorporating several latent features into the tense classiﬁer boosts the tense classiﬁer’s performance, and a tense classiﬁer using only the latent features outperforms one using only the surface features. Our ﬁndings conﬁrm the utility of the latent features in automatic tense classiﬁcation, explaining the gap between automatic classiﬁcation systems and the human brain. 
Sentence retrieval plays a very important role in question answering system. In this paper, we present a novel cluster-based language model for sentence retrieval in Chinese question answering which is motivated in part by sentence clustering and language model. Sentence clustering is used to group sentences into clusters. Language model is used to properly represent sentences, which is combined with sentences model, cluster/topic model and collection model. For sentence clustering, we propose two approaches that are OneSentence-Multi-Topics and OneSentence-One-Topic respectively. From the experimental results on 807 Chinese testing questions, we can conclude that the proposed cluster-based language model outperforms over the standard language model for sentence retrieval in Chinese question answering. 
The role of lexical resources is often understated in NLP research. The complexity of Chinese, Japanese and Korean (CJK) poses special challenges to developers of NLP tools, especially in the area of word segmentation (WS), information retrieval (IR), named entity extraction (NER), and machine translation (MT). These difficulties are exacerbated by the lack of comprehensive lexical resources, especially for proper nouns, and the lack of a standardized orthography, especially in Japanese. This paper summarizes some of the major linguistic issues in the development NLP applications that are dependent on lexical resources, and discusses the central role such resources should play in enhancing the accuracy of NLP tools. 
This paper describes a hybrid model and the corresponding algorithm combining support vector machines (SVMs) with statistical methods to improve the performance of SVMs for the task of Chinese Named Entity Recognition (NER). In this algorithm, a threshold of the distance from the test sample to the hyperplane of SVMs in feature space is used to separate SVMs region and statistical method region. If the distance is greater than the given threshold, the test sample is classified using SVMs; otherwise, the statistical model is used. By integrating the advantages of two methods, the hybrid model achieves 93.18% F-measure for Chinese person names and 91.49% Fmeasure for Chinese location names. 
The BA-construction refers to a special grammatical structure in Mandarin Chinese. It is an extremely important syntactic structure in Chinese, which is frequently used in daily life. The study of the BA-construction has attracted the attention of almost all linguists who are interested in this language. Yet it is a quite complex and difficult linguistic phenomenon and it is hard to analyze it satisfactorily to cope with the syntactic structure(s) of another language which does not possess this kind of construction (e.g. in machine translation). This paper discusses a few methods on how some of the English imperative sentences are realized by the Chinese BA-construction which is mandatory in transferring certain source language (SL) information into target language (TL) in an experimental machine translation (MT) system. We also introduce the basic syntactic structures of the BAconstruction and explain how we formalize and control these structures to satisfy our need. Some features related to the BA-construction, such as obligatoriness versus the optionality, the semantics as well as the properties of the elements preceding and following the BA are also discussed. Finally we suggest that by constraining the variations of the formalized patterns of the BA-  construction, a better MT could be reached. 
In this paper, we propose a hybrid approach to chunking Chinese base noun phrases (base NPs), which combines SVM (Support Vector Machine) model and CRF (Conditional Random Field) model. In order to compare the result respectively from two chunkers, we use the discriminative post-processing method, whose measure criterion is the conditional probability generated from the CRF chunker. With respect to the special structures of Chinese base NP and complete analyses of the first two results, we also customize some appropriate grammar rules to avoid ambiguities and prune errors. According to our overall experiments, the method achieves a higher accuracy in the final results. 
Functional chunks are defined as a series of non-overlapping, non-nested segments of text in a sentence, representing the implicit grammatical relations between the sentence-level predicates and their arguments. Its top-down scheme and complexity of internal constitutions bring in a new challenge for automatic parser. In this paper, a new parsing model is proposed to formulate the complete chunking problem as a series of boundary detection sub tasks. Each of these sub tasks is only in charge of detecting one type of the chunk boundaries. As each sub task could be modeled as a binary classification problem, a lot of machine learning techniques could be applied. In our experiments, we only focus on the subject-predicate (SP) and predicateobject (PO) boundary detection sub tasks. By applying SVM algorithm to these sub tasks, we have achieved the best F-Score of 76.56% and 82.26% respectively. 
 tains the conclusion and outlines of our future work in this field.  The main purpose of this paper is the exploitation and application of an audio and video bimodal corpus of the Chinese language in broadcasting. It deals with the designation of the size and structure of speech samples according to radio and television program features. Secondly, it discusses annotation method of broadcast speech with achievements made and suggested future improvements. Finally, it presents an attempt to describe the distribution of annotated items in our corpus. 
The Third International Chinese Language Processing Bakeoff was held in Spring 2006 to assess the state of the art in two important tasks: word segmentation and named entity recognition. Twenty-nine groups submitted result sets in the two tasks across two tracks and a total of ﬁve corpora. We found strong results in both tasks as well as continuing challenges. 
We present a Chinese Named Entity Recognition (NER) system submitted to the close track of Sighan Bakeoff2006. We deﬁne some additional features via doing statistics in training corpus. Our system incorporates basic features and additional features based on Conditional Random Fields (CRFs). In order to correct inconsistently results, we perform the postprocessing procedure according to n-best results given by the CRFs model. Our ﬁnal system achieved a F-score of 85.14 at MSRA, 89.03 at CityU, and 76.27 at LDC. 
This paper presents two word segmentation (WS) systems and a named entity recognition (NER) system in France Telecom R&D Beijing. The one system of WS is for open tracks based on ngram language model and another one is for closed tracks based on maximum entropy approach. The NER system uses a hybrid algorithm based on Class-based language model and rule-based knowledge. These systems are all augmented with a set of post-processors. 
This paper describes a Chinese word segmentation system that is based on majority voting among three models: a forward maximum matching model, a conditional random ﬁeld (CRF) model using maximum subword-based tagging, and a CRF model using minimum subwordbased tagging. In addition, it contains a post-processing component to deal with inconsistencies. Testing on the closed track of CityU, MSRA and UPUC corpora in the third SIGHAN Chinese Word Segmentation Bakeoff, the system achieves a F-score of 0.961, 0.953 and 0.919, respectively. 
This paper describes a Chinese word segmentor (CWS) for the third International Chinese Language Processing Bakeoff (SIGHAN Bakeoff 2006). We participate in the word segmentation task at the Microsoft Research (MSR) closed testing track. Our CWS is based on backward maximum matching with word support model (WSM) and contextual-based Chinese unknown word identification. From the scored results and our experimental results, it shows WSM can improve our previous CWS, which was reported at the SIGHAN Bakeoff 2005, about 1% of F-measure. 
This paper addresses two major problems in closed task of Chinese word segmentation (CWS): tagging sentences interspersed with non-Chinese words, and long named entity (NE) identification. To resolve the former, we apply Kmeans clustering to identify non-Chinese characters, and then adopt a two-tagger architecture: one for Chinese text and the other for non-Chinese text. For the latter problem, we apply postprocessing to our CWS output using automatically generated templates. The experiment results show that, when non-Chinese characters are sparse in the training corpus, our two-tagger method significantly improves the segmentation of sentences containing non-Chinese words. Identification of long NEs and long words is also enhanced by template-based postprocessing. In the closed task of SIGHAN 2006 CWS, our system achieved F-scores of 0.957, 0.972, and 0.955 on the CKIP, CTU, and MSR corpora respectively. 
This paper presents the Chinese word segmentation systems developed by Speech and Hearing Research Group of National Laboratory on Machine Perception (NLMP) at Peking University, which were evaluated in the third International Chinese Word Segmentation Bakeoff held by SIGHAN. The Chinese character-based maximum entropy model, which switches the word segmentation task to a classiﬁcation task, is adopted in system developing. To integrate more linguistics information, an n-gram language model as well as several post processing strategies are also employed. Both the closed and open tracks regarding to all four corpora MSRA, UPUC, CITYU, CKIP are involved in our systems’ evaluation, and good performance are achieved. Especially, in the closed track on MSRA, our system ranks 1st. 
In sequence labeling tasks, applying different machine learning models and feature sets usually leads to different results. In this paper, we exploit two ensemble methods in order to integrate multiple results generated under different conditions. One method is based on majority vote, while the other is a memory-based approach that integrates maximum entropy and conditional random field classifiers. Our results indicate that the memory-based method can outperform the individual classifiers, but the majority vote method cannot. 
This paper describes our word segmentation system and named entity recognition (NER) system for participating in the third SIGHAN Bakeoff. Both of them are based on character tagging, but use different tag sets and different features. Evaluation results show that our word segmentation system achieved 93.3% and 94.7% F-score in UPUC and MSRA open tests, and our NER system got 70.84% and 81.32% F-score in LDC and MSRA open tests. 
We report an experiment in which a highperformance boosting based NER model originally designed for multiple European languages is instead applied to the Chinese named entity recognition task of the third SIGHAN Chinese language processing bakeoff. Using a simple characterbased model along with a set of features that are easily obtained from the Chinese input strings, the system described employs boosting, a promising and theoretically well-founded machine learning method to combine a set of weak classiﬁers together into a ﬁnal system. Even though we did no other Chinese-speciﬁc tuning, and used only one-third of the MSRA and CityU corpora to train the system, reasonable results are obtained. Our evaluation results show that 75.07 and 80.51 overall F-measures were obtained on MSRA and CityU test sets respectively. 
This paper briefly describes our system in the third SIGHAN bakeoff on Chinese word segmentation and named entity recognition. This is done via a word chunking strategy using a context-dependent Mutual Information Independence Model. Evaluation shows that our system performs well on all the word segmentation closed tracks and achieves very good scalability across different corpora. It also shows that the use of the same strategy in named entity recognition shows promising performance given the fact that we only spend less than three days in total on extending the system in word segmentation to incorporate named entity recognition, including training and formal testing. 
We have participated in three open tracks of Chinese word segmentation and named entity recognition tasks of SIGHAN Bakeoff3. We take a probabilistic feature based Maximum Entropy (ME) model as our basic frame to combine multiple sources of knowledge. Our named entity recognizer achieved the highest F measure for MSRA, and word segmenter achieved the medium F measure for MSRA. We find effective combining of the external multi-knowledge is crucial to improve performance of word segmentation and named entity recognition. 
Most of the Chinese word segmentation systems utilizes monolingual dictionary and are used for monolingual processing. For the tasks of machine translation (MT) and cross-language information retrieval (CLIR), another translation dictionary may be used to transfer the words of documents from the source languages to target languages. The inconsistencies resulting from the two types of dictionaries (segmentation dictionary and transfer dictionary) may produce some problems for MT and CLIR. This paper shows the effectiveness of the external resources (bilingual dictionary and word list) for Chinese word segmentations. 
We describe the application of the LingPipe toolkit to Chinese word segmentation and named entity recognition for the 3rd SIGHAN bakeoff. 
This paper describes the work on Chinese named entity recognition performed by Yahoo team at the third International Chinese Language Processing Bakeoff. We used two conditional probabilistic models for this task, including conditional random fields (CRFs) and maximum entropy models. In particular, we trained two conditional random field recognizers and one maximum entropy recognizer for identifying names of people, places, and organizations in unsegmented Chinese texts. Our best performance is 86.2% F-score on MSRA dataset, and 88.53% on CITYU dataset. 
In this paper, a language tagging template named POC-NLW (position of a character within an n-length word) is presented. Based on this template, a twostage statistical model for Chinese word segmentation is constructed. In this method, the basic word segmentation is based on n-gram language model, and a Hidden Markov tagger based on the POC-NLW template is used to implement the out-of-vocabulary (OOV) word identification. The system participated in the MSRA_Close and UPUC_Close word segmentation tracks at SIGHAN Bakeoff 2006. Results returned by this bakeoff are reported here. 
This paper mainly describes a Chinese named entity recognition (NER) system NER@ISCAS, which integrates text, part-of-speech and a small-vocabularycharacter-lists feature for MSRA NER open track under the framework of Conditional Random Fields (CRFs) model. The techniques used for the close NER and word segmentation tracks are also presented. 
We extended the work of Low, Ng, and Guo (2005) to create a Chinese word segmentation system based upon a maximum entropy statistical model. This system was entered into the Third International Chinese Language Processing Bakeoff and evaluated on all four corpora in their respective open tracks. Our system achieved the highest F-score for the UPUC corpus, and the second, third, and seventh highest for CKIP, CITYU, and MSRA respectively. Later testing with the gold-standard data revealed that while the additions we made to Low et al.’s system helped our results for the 2005 data with which we experimented during development, a number of them actually hurt our scores for this year’s corpora. 
 Named entities (NE), and New words (NW). Figure 1 demonstrates our system structure.  This paper presents our work for participation in the Third International Chinese Word Segmentation Bakeoff. We apply several processing approaches according to the corresponding sub-tasks, which are exhibited in real natural language. In our system, Trigram model with smoothing algorithm is the core module in word segmentation, and Maximum Entropy model is the basic model in Named Entity Recognition task. The experiment indicates that this system achieves Fmeasure 96.8% in MSRA open test in the third SIGHAN-2006 bakeoff. 
This document analyses the bakeoff results from NetEase Co. in the SIGHAN5 Word Segmentation Task and Named Entity Recognition Task. The NetEase WS system is designed to facilitate research in natural language processing and information retrieval. It supports Chinese and English word segmentation, Chinese named entity recognition, Chinese part of speech tagging and phrase conglutination. Evaluation result shows our WS system has a passable precision in word segmentation except for the unknown words recognition. 
This paper describes an n-gram based reinforcement approach to the closed track of word segmentation in the third Chinese word segmentation bakeoff. Character n-gram features of unigram, bigram, and trigram are extracted from the training corpus and its frequencies are counted. We investigated a step-by-step methodology by using the n-gram statistics. In the first step, relatively definite segmentations are fixed by the tight threshold value. The remaining tags are decided by considering the left or right space tags that are already fixed in the first step. Definite and loose segmentation are performed simply based on the bigram and trigram statistics. In order to overcome the data sparseness problem of bigram data, unigram is used for the smoothing. 
In this paper, we described our Chinese word segmentation system for the 3rd SIGHAN Chinese Language Processing Bakeoff Word Segmentation Task. Our system deal with the Chinese character sequence by using the Maximum Entropy model, which is fully automatically generated from the training data by analyzing the character sequences from the training corpus. We analyze its performance on both closed and open tracks on Microsoft Research (MSRA) and University of Pennsylvania and University of Colorado (UPUC) corpus. It is shown that we can get the results just acceptable without using dictionary. The conclusion is also presented. 
Chinese word segmentation and Part-ofSpeech (POS) tagging have been commonly considered as two separated tasks. In this paper, we present a system that performs Chinese word segmentation and POS tagging simultaneously. We train a segmenter and a tagger model separately based on linear-chain Conditional Random Fields (CRF), using lexical, morphological and semantic features. We propose an approximated joint decoding method by reranking the N-best segmenter output, based POS tagging information. Experimental results on SIGHAN Bakeoff dataset and Penn Chinese Treebank show that our reranking method signiﬁcantly improve both segmentation and POS tagging accuracies. 
Asian languages are far from most western-style in their non-separate word sequence especially Chinese. The preliminary step of Asian-like language processing is to find the word boundaries between words. In this paper, we present a general purpose model for both Chinese word segmentation and named entity recognition. This model was built on the word sequence classification with probability model, i.e., conditional random fields (CRF). We used a simple feature set for CRF which achieves satisfactory classification result on the two tasks. Our model achieved 91.00 in F rate in UPUCTreebank data, and 78.71 for NER task. 
Chinese named entity recognition is one of the difficult and challenging tasks of NLP. In this paper, we present a Chinese named entity recognition system using a multi-phase model. First, we segment the text with a character-level CRF model. Then we apply three word-level CRF models to the labeling person names, location names and organization names in the segmentation results, respectively. Our systems participated in the NER tests on open and closed tracks of Microsoft Research (MSRA). The actual evaluation results show that our system performs well on both the open tracks and closed tracks. 
We participated in the Third International Chinese Word Segmentation Bakeoff. Speciﬁcally, we evaluated our Chinese word segmenter NEUCipSeg in the close track, on all four corpora, namely Academis Sinica (AS), City University of Hong Kong (CITYU), Microsoft Research (MSRA), and University of Pennsylvania/University of Colorado (UPENN). Based on Support Vector Machines (SVMs), a basic segmenter is designed regarding Chinese word segmentation as a problem of character-based tagging. Moreover, we proposed postprocessing rules specially taking into account the properties of results brought out by the basic segmenter. Our system achieved good ranks in all four corpora. 
                                                                                             !                     "               "        #      #                                ! $     %&    '   ()))*                         !                                        ! +                            #          !                                  #           #           #            !     "                                "                        "      ! Proceedings of the 2006 Australasian Language Technology Workshop (ALTW2006), pages 1–1. 
Efﬁcient wide-coverage parsing is integral to large-scale NLP applications. Unfortunately, parsers for linguistically motivated formalisms, e.g. HPSG and TAG, are often too inefﬁcient for these applications. This paper describes two modiﬁcations to the standard CKY chart parsing algorithm used in the Clark and Curran (2006) Combinatory Categorial Grammar (CCG) parser. The ﬁrst modiﬁcation extends the tight integration of the supertagger and parser, so that individual supertags can be added to the chart, which is then repaired rather than rebuilt. The second modiﬁcation adds constraints to the chart that restrict which constituents can combine. Parsing speed is improved by 30–35% without a signiﬁcant accuracy penalty and a small increase in coverage when both of these modiﬁcations are used. 
Supervised word sense disambiguation has proven incredibly difﬁcult. Despite signiﬁcant effort, there has been little success at using contextual features to accurately assign the sense of a word. Instead, few systems are able to outperform the default sense baseline of selecting the highest ranked WordNet sense. In this paper, we suggest that the situation is even worse than it might ﬁrst appear: the highest ranked WordNet sense is not even the best default sense classiﬁer. We evaluate several default sense heuristics, using supersenses and SemCor frequencies to achieve signiﬁcant improvements on the WordNet ranking strategy. 
We present a set of experiments involving sentence classiﬁcation, addressing issues of representation and feature selection, and we compare our ﬁndings with similar results from work on the more general text classiﬁcation task. The domain of our investigation is an email-based help-desk corpus. Our investigations compare the use of various popular classiﬁcation algorithms with various popular feature selection methods. The results highlight similarities between sentence and text classiﬁcation, such as the superiority of Support Vector Machines, as well as differences, such as a lesser extent of the usefulness of features selection on sentence classiﬁcation, and a detrimental effect of common preprocessing techniques (stop-word removal and lemmatization). 
NLTK, the Natural Language Toolkit, is an open source project whose goals include providing students with software and language resources that will help them to learn basic NLP. Until now, the program modules in NLTK have covered such topics as tagging, chunking, and parsing, but have not incorporated any aspect of semantic interpretation. This paper describes recent work on building a new semantics package for NLTK. This currently allows semantic representations to be built compositionally as a part of sentence parsing, and for the representations to be evaluated by a model checker. We present the main components of this work, and consider comparisons between the Python implementation and the Prolog approach developed by Blackburn and Bos (2005). 
The driving vision for our work is to provide intelligent, automated assistance to users in understanding the status of their email conversations. Our approach is to create tools that enable the detection and connection of speech acts across email messages. We thus require a mechanism for tagging email utterances with some indication of their dialogic function. However, existing dialog act taxonomies as used in computational linguistics tend to be too task- or application-speciﬁc for the wide range of acts we ﬁnd represented in email conversation. The Verbal Response Modes (VRM) taxonomy of speech acts, widely applied for discourse analysis in linguistics and psychology, is distinguished from other speech act taxonomies by its construction from crosscutting principles of classiﬁcation, which ensure universal applicability across any domain of discourse. The taxonomy categorises on two dimensions, characterised as literal meaning and pragmatic meaning. In this paper, we describe a statistical classiﬁer that automatically identiﬁes the literal meaning category of utterances using the VRM classiﬁcation. We achieve an accuracy of 60.8% using linguistic features derived from VRM’s human annotation guidelines. Accuracy is improved to 79.8% using additional features. 
The current situation for Word Sense Disambiguation (WSD) is somewhat stuck due to lack of training data. We present in this paper a novel disambiguation algorithm that improves previous systems based on acquisition of examples by incorporating local context information. With a basic conﬁguration, our method is able to obtain state-of-the-art performance. We complemented this work by evaluating other well-known methods in the same dataset, and analysing the comparative results per word. We observed that each algorithm performed better for different types of words, and each of them failed for some particular words. We proposed then a simple unsupervised voting scheme that improved signiﬁcantly over single systems, achieving the best unsupervised performance on both the Senseval 2 and Senseval 3 lexical sample datasets. 
Current text-based question answering (QA) systems usually contain a named entity recogniser (NER) as a core component. Named entity recognition has traditionally been developed as a component for information extraction systems, and current techniques are focused on this end use. However, no formal assessment has been done on the characteristics of a NER within the task of question answering. In this paper we present a NER that aims at higher recall by allowing multiple entity labels to strings. The NER is embedded in a question answering system and the overall QA system performance is compared to that of one with a traditional variation of the NER that only allows single entity labels. It is shown that the added noise produced introduced by the additional labels is offset by the higher recall gained, therefore enabling the QA system to have a better chance to ﬁnd the answer. 
We present a system for named entity recognition (ner) in astronomy journal articles. We have developed this system on a ne corpus comprising approximately 200,000 words of text from astronomy articles. These have been manually annotated with ∼40 entity types of interest to astronomers. We report on the challenges involved in extracting the corpus, deﬁning entity classes and annotating scientiﬁc text. We investigate which features of an existing state-of-the-art Maximum Entropy approach perform well on astronomy text. Our system achieves an F-score of 87.8%. 
We examine standard deep lexical acquisition features in automatically predicting the gender of noun types and tokens by bootstrapping from a small annotated corpus. Using a knowledge-poor approach to simulate prediction in unseen languages, we observe results comparable to morphological analysers trained specifically on our target languages of German and French. These results describe further scope in analysing other properties in languages displaying a more challenging morphosyntax, in order to create language resources in a language-independent manner. 
Automatic mapping of key concepts from clinical notes to a terminology is an important task to achieve for extraction of the clinical information locked in clinical notes and patient reports. The present paper describes a system that automatically maps free text into a medical reference terminology. The algorithm utilises Natural Language Processing (NLP) techniques to enhance a lexical token matcher. In addition, this algorithm is able to identify negative concepts as well as performing term qualification. The algorithm has been implemented as a web based service running at a hospital to process real-time data and demonstrated that it worked within acceptable time limits and accuracy limits for them. However broader acceptability of the algorithm will require comprehensive evaluations. 
Relevance feedback has already proven its usefulness in probabilistic information retrieval (IR). In this research we explore whether a pseudo relevance feedback technique on IR can improve the Question Answering task (QA). The basis of our exploration is the use of relevant named entities from the top retrieved documents as clues of relevance. We discuss two interesting ﬁndings from these experiments: the reasons the results were not improved, and the fact that today’s metrics of IR evaluation on QA do not reﬂect the results obtained by a QA system. 
The necessity of a gradient approach to salience ranking of referents introduced in a discourse is evaluated by looking at (unbound) pronoun resolution preferences when there are competing non-salient referents. The study uses a sentencecompletion technique in which participants had to resolve pronouns (“John sprayed the paint on the wall and then it ...”). Results suggest that a gradient salience model is necessary. Syntactic and semantic prominence effects on pronoun resolution were also compared with results showing that semantic prominence (i.e., agent > patient) determined the salience ranking of competing referents. 
Proﬁciency in a second language is of vital importance for many people. Today’s access to corpora of text, including the Web, allows new techniques for improving language skill. Our project’s aim is the development of techniques for presenting the user with suitable web text, to allow optimal language acquisition via reading. Some text found on the Web may be of a suitable level of difﬁculty but appropriate techniques need to be devised for locating it, as well as methods for rapid retrieval. Our experiments described here compare the range of difﬁculty of text found on the Web to that found in traditional hard-copy texts for English as a Second Language (ESL) learners, using standard readability measures. The results show that the ESL text readability range fall within the range for Web text. This suggests that an on-line text retrieval engine based on readability can be of use to language learners. However, web pages pose their own difﬁculty, since those with scores representing high readability are often of limited use. Therefore readability measurement techniques need to be modiﬁed for the Web domain. 
Direct questions such as “Who saw Mary?” intuitively request for a certain type of answer, for instance a noun phrase “John” or a quantiﬁed noun phrase such as “A man”. Following the structured meaning approach to questions, we propose an analysis of wh-questions in typelogical grammar that incorporates the requirement for a certain type of answer into the type assigned to wh-phrases. Interestingly, the syntactic and semantic decomposition leads to a derivability pattern between instances of wh-phrases. With this pattern we can explain the difference between wh-pronouns (‘who’) and wh-determiners (‘which’), and derive whquestions that require multiple answers. 
This paper describes a mechanism for identifying errors made by a student during a computer-aided language learning dialogue. The mechanism generates a set of ‘perturbations’ of the student’s original typed utterance, each of which embodies a hypothesis about an error made by the student. Perturbations are then passed through the system’s ordinary utterance interpretation pipeline, along with the student’s original utterance. An utterance disambiguation algorithm selects the best interpretation, performing error correction as a side-effect. 
As research in text-to-text paraphrase generation progresses, it has the potential to improve the quality of generated text. However, the use of paraphrase generation methods creates a secondary problem. We must ensure that generated novel sentences are not inconsistent with the text from which it was generated. We propose a machine learning approach be used to ﬁlter out inconsistent novel sentences, or False Paraphrases. To train such a ﬁlter, we use the Microsoft Research Paraphrase corpus and investigate whether features based on syntactic dependencies can aid us in this task. Like Finch et al. (2005), we obtain a classiﬁcation accuracy of 75.6%, the best known performance for this corpus. We also examine the strengths and weaknesses of dependency based features and conclude that they may be useful in more accurately classifying cases of False Paraphrase. 
This paper investigates whether multisemantic-role (MSR) based selectional preferences can be used to improve the performance of supervised verb sense disambiguation. Unlike conventional selectional preferences which are extracted from parse trees based on hand-crafted rules, and only include the direct subject or the direct object of the verbs, the MSR based selectional preferences to be presented in this paper are extracted from the output of a state-of-the-art semantic role labeler and incorporate a much richer set of semantic roles. The performance of the MSR based selectional preferences is evaluated on two distinct datasets: the verbs from the lexical sample task of SENSEVAL-2, and the verbs from a movie script corpus. We show that the MSR based features can indeed improve the performance of verb sense disambiguation. 
 Many natural language processes have some degree of preprocessing of data: tokenisation, stemming and so on. In the domain of Statistical Machine Translation it has been shown that word reordering as a preprocessing step can help the translation process.  Recently, hand-written rules for reordering in  German–English translation have shown good  results, but this is clearly a labour-intensive and  language pair-speciﬁc approach. Two possible  sources of the observed improvement are that (1)  the reordering explicitly matches the syntax of  the source language more closely to that of the  target language, or that (2) it ﬁts the data bet-  ter to the mechanisms of phrasal SMT; but it is  not clear which. In this paper, we apply a gen-  eral principle based on dependency distance min-  imisation to produce reorderings. Our language-  independent approach achieves half of the im-  provement of a reimplementation of the hand-  crafted approach, and suggests that reason (2) is a  possible explanation for why that reordering ap-  proach works.  Help you I can, yes.  Jedi Master Yoda  
Instant messaging dialogue is real-time, text-based computer-mediated communication conducted over the Internet. Messages sent over instant messaging can be encoded We propose a method of using dialogue acts to predict utterances in taskoriented dialogue. Dialogue acts provide a semantic representation of utterances in a dialogue. An evaluation using a dialogue simulation program shows that our proposed method of predicting responses provides useful suggestions for almost all response types. 
Search engine interfaces come in a range of variations from the familiar text-based approach to the more experimental graphical systems. It is rare however that psychological or human factors research is undertaken to properly evaluate or optimize the systems, and to the extent this has been done the results have tended to contradict some of the assumptions that have driven search engine design. Our research is focussed on a model in which at least 100 hits are selected from a corpus of documents based on a set of query words and displayed graphically. Matrix manipulation techniques in the SVD/LSA family are used to identify significant dimensions and display documents according to a subset of these dimensions. The research questions we are investigating in this context relate to the computational methods (how to rescale the data), the linguistic information (how to characterize a document), and the visual attributes (which linguistic dimensions to display using which attributes). 
XML information retrieval (XML-IR) systems respond to user queries with results more speciﬁc than documents. XML-IR queries contain both content and structural requirements traditionally expressed in a formal language. However, an intuitive alternative is natural language queries (NLQs). Here, we discuss three approaches for handling NLQs in an XMLIR system that are comparable to, and even outperform formal language queries. 
This research aims to extract detailed clinical proﬁles, such as signs and symptoms, and important laboratory test results of the patient from descriptions of the diagnostic and treatment procedures in journal articles. This paper proposes a novel mark-up tag set to cover a wide variety of semantics in the description of clinical case studies in the clinical literature. A manually annotated corpus which consists of 75 clinical reports with 5,117 sentences has been created and a sentence classiﬁcation system is reported as the preliminary attempt to exploit the fast growing online repositories of clinical case reports. 
This paper introduces a method for the semi-automatic generation of grammar test items by applying Natural Language Processing (NLP) techniques. Based on manually-designed patterns, sentences gathered from the Web are transformed into tests on grammaticality. The method involves representing test writing knowledge as test patterns, acquiring authentic sentences on the Web, and applying generation strategies to transform sentences into items. At runtime, sentences are converted into two types of TOEFL-style question: multiplechoice and error detection. We also describe a prototype system FAST (Free Assessment of Structural Tests). Evaluation on a set of generated questions indicates that the proposed method performs satisfactory quality. Our methodology provides a promising approach and offers significant potential for computer assisted language learning and assessment. 
This paper describes a novel approach for the automatic generation and evaluation of a trivial dialogue phrases database. A trivial dialogue phrase is deﬁned as an expression used by a chatbot program as the answer of a user input. A transfer-like genetic algorithm (GA) method is used to generating the trivial dialogue phrases for the creation of a natural language generation (NLG) knowledge base. The automatic evaluation of a generated phrase is performed by producing n-grams and retrieving their frequencies from the World Wide Web (WWW). Preliminary experiments show very positive results. 
This demo presents LeXFlow, a workflow management system for crossfertilization of computational lexicons. Borrowing from techniques used in the domain of document workflows, we model the activity of lexicon management as a set of workflow types, where lexical entries move across agents in the process of being dynamically updated. A prototype of LeXFlow has been implemented with extensive use of XML technologies (XSLT, XPath, XForms, SVG) and open-source tools (Cocoon, Tomcat, MySQL). LeXFlow is a web-based application that enables the cooperative and distributed management of computational lexicons. 
In this paper we present Valido, a tool that supports the difﬁcult task of validating sense choices produced by a set of annotators. The validator can analyse the semantic graphs resulting from each sense choice and decide which sense is more coherent with respect to the structure of the adopted lexicon. We describe the interface and report an evaluation of the tool in the validation of manual sense annotations. 
We present a practical HPSG parser for English, an intelligent search engine to retrieve MEDLINE abstracts that represent biomedical events and an efﬁcient MEDLINE search tool helping users to ﬁnd information about biomedical entities such as genes, proteins, and the interactions between them. 
The main aim of the MIMA (Mining Information for Management and Acquisition) Search System is to achieve ‘structuring knowledge’ to accelerate knowledge exploitation in the domains of science and technology. This system integrates natural language processing including ontology development, information retrieval, visualization, and database technology. The ‘structuring knowledge’ that we define indicates 1) knowledge storage, 2) (hierarchical) classification of knowledge, 3) analysis of knowledge, 4) visualization of knowledge. We aim at integrating different types of databases (papers and patents, technologies and innovations) and knowledge domains, and simultaneously retrieving different types of knowledge. Applications for the several targets such as syllabus structuring will also be mentioned. 
This paper describes FERRET, an interactive question-answering (Q/A) system designed to address the challenges of integrating automatic Q/A applications into real-world environments. FERRET utilizes a novel approach to Q/A – known as predictive questioning – which attempts to identify the questions (and answers) that users need by analyzing how a user interacts with a system while gathering information related to a particular scenario. 
We present a Korean question answering framework for restricted domains, called K-QARD. K-QARD is developed to achieve domain portability and robustness, and the framework is successfully applied to build question answering systems for several domains. 
The interpretation of temporal expressions in text is an important constituent task for many practical natural language processing tasks, including question-answering, information extraction and text summarisation. Although temporal expressions have long been studied in the research literature, it is only more recently, with the impetus provided by exercises like the ACE Program, that attention has been directed to broad-coverage, implemented systems. In this paper, we describe our approach to intermediate semantic representations in the interpretation of temporal expressions. 
In this interactive presentation, a Chinese named entity and relation identification system is demonstrated. The domainspecific system has a three-stage pipeline architecture which includes word segmentation and part-of-speech (POS) tagging, named entity recognition, and named entity relation identitfication. The experimental results have shown that the average F-measure for word segmentation and POS tagging after correcting errors achieves 92.86 and 90.01 separately. Moreover, the overall average F-measure for 6 kinds of name entities and 14 kinds of named entity relations is 83.08% and 70.46% respectively. 
This paper introduces a method for computational analysis of move structures in abstracts of research articles. In our approach, sentences in a given abstract are analyzed and labeled with a specific move in light of various rhetorical functions. The method involves automatically gathering a large number of abstracts from the Web and building a language model of abstract moves. We also present a prototype concordancer, CARE, which exploits the move-tagged abstracts for digital learning. This system provides a promising approach to Webbased computer-assisted academic writing. 
This interactive presentation describes LexNet, a graphical environment for graph-based NLP developed at the University of Michigan. LexNet includes LexRank (for text summarization), biased LexRank (for passage retrieval), and TUMBL (for binary classiﬁcation). All tools in the collection are based on random walks on lexical graphs, that is graphs where different NLP objects (e.g., sentences or phrases) are represented as nodes linked by edges proportional to the lexical similarity between the two nodes. We will demonstrate these tools on a variety of NLP tasks including summarization, question answering, and prepositional phrase attachment. 
This paper presents Archivus, a multimodal language-enabled meeting browsing and retrieval system. The prototype is in an early stage of development, and we are currently exploring the role of natural language for interacting in this relatively unfamiliar and complex domain. We brieﬂy describe the design and implementation status of the system, and then focus on how this system is used to elicit useful data for supporting hypotheses about multimodal interaction in the domain of meeting retrieval and for developing NLP modules for this speciﬁc domain. 
The LOGON MT demonstrator assembles independently valuable general-purpose NLP components into a machine translation pipeline that capitalizes on output quality. The demonstrator embodies an interesting combination of hand-built, symbolic resources and stochastic processes. 
The SAMMIE1 system is an in-car multimodal dialogue system for an MP3 application. It is used as a testing environment for our research in natural, intuitive mixed-initiative interaction, with particular emphasis on multimodal output planning and realization aimed to produce output adapted to the context, including the driver’s attention state w.r.t. the primary driving task. 
TwicPen is a terminology-assistance system for readers of printed (ie. off-line) material in foreign languages. It consists of a hand-held scanner and sophisticated parsing and translation software to provide readers a limited number of translations selected on the basis of a linguistic analysis of the whole scanned text fragment (a phrase, part of the sentence, etc.). The use of a morphological and syntactic parser makes it possible (i) to disambiguate to a large extent the word selected by the user (and hence to drastically reduce the noise in the response), and (ii) to handle expressions (compounds, collocations, idioms), often a major source of difﬁculty for non-native readers. The system exists for the following language-pairs: EnglishFrench, French-English, German-French and Italian-French. 
In this paper we describe the current state of a new Japanese lexical resource: the Hinoki treebank. The treebank is built from dictionary deﬁnition sentences, and uses an HPSG based Japanese grammar to encode both syntactic and semantic information. It is combined with an ontology based on the deﬁnition sentences to give a detailed sense level description of the most familiar 28,000 words of Japanese. 
The Natural Language Toolkit is a suite of program modules, data sets and tutorials supporting research and teaching in computational linguistics and natural language processing. NLTK is written in Python and distributed under the GPL open source license. Over the past year the toolkit has been rewritten, simplifying many linguistic data structures and taking advantage of recent enhancements in the Python language. This paper reports on the simpliﬁed toolkit and explains how it is used in teaching NLP. 
We present Outilex, a generalist linguistic platform for text processing. The platform includes several modules implementing the main operations for text processing and is designed to use large-coverage Language Resources. These resources (dictionaries, grammars, annotated texts) are formatted into XML, in accordance with current standards. Evaluations on efﬁciency are given. 
We describe the new release of the RASP (robust accurate statistical parsing) system, designed for syntactic annotation of free text. The new version includes a revised and more semantically-motivated output representation, an enhanced grammar and part-of-speech tagger lexicon, and a more ﬂexible and semi-supervised training method for the structural parse ranking model. We evaluate the released version on the WSJ using a relational evaluation scheme, and describe how the new release allows users to enhance performance using (in-domain) lexical information. 
Natural Language Generation (NLG) is a way to automatically realize a correct expression in response to a communicative goal. This technology is mainly explored in the fields of machine translation, report generation, dialog system etc. In this paper we have explored the NLG technique for another novel applicationassisting disabled children to take part in conversation. The limited physical ability and mental maturity of our intended users made the NLG approach different from others. We have taken a flexible approach where main emphasis is given on flexibility and usability of the system. The evaluation results show this technique can increase the communication rate of users during a conversation. 
An unsupervised part-of-speech (POS) tagging system that relies on graph clustering methods is described. Unlike in current state-of-the-art approaches, the kind and number of different tags is generated by the method itself. We compute and merge two partitionings of word graphs: one based on context similarity of high frequency words, another on log-likelihood statistics for words of lower frequencies. Using the resulting word clusters as a lexicon, a Viterbi POS tagger is trained, which is refined by a morphological component. The approach is evaluated on three different languages by measuring agreement with existing taggers. 
In this paper, we will present an efficient method to compute the co-occurrence counts of any pair of substring in a parallel corpus, and an algorithm that make use of these counts to create subsentential alignments on such a corpus. This algorithm has the advantage of being as general as possible regarding the segmentation of text. 
Most of the work on treebank-based statistical parsing exclusively uses the WallStreet-Journal part of the Penn treebank for evaluation purposes. Due to the presence of this quasi-standard, the question of to which degree parsing results depend on the properties of treebanks was often ignored. In this paper, we use two similar German treebanks, Tu¨Ba-D/Z and NeGra, and investigate the role that different annotation decisions play for parsing. For these purposes, we approximate the two treebanks by gradually taking out or inserting the corresponding annotation components and test the performance of a standard PCFG parser on all treebank versions. Our results give an indication of which structures are favorable for parsing and which ones are not. 
It has previously been assumed in the psycholinguistic literature that ﬁnite-state models of language are crucially limited in their explanatory power by the locality of the probability distribution and the narrow scope of information used by the model. We show that a simple computational model (a bigram part-of-speech tagger based on the design used by Corley and Crocker (2000)) makes correct predictions on processing difﬁculty observed in a wide range of empirical sentence processing data. We use two modes of evaluation: one that relies on comparison with a control sentence, paralleling practice in human studies; another that measures probability drop in the disambiguating region of the sentence. Both are surprisingly good indicators of the processing difﬁculty of garden-path sentences. The sentences tested are drawn from published sources and systematically explore ﬁve different types of ambiguity: previous studies have been narrower in scope and smaller in scale. We do not deny the limitations of ﬁnite-state models, but argue that our results show that their usefulness has been underestimated. 
In order to build a simulated robot that accepts instructions in unconstrained natural language, a corpus of 427 route instructions was collected from human subjects in the ofﬁce navigation domain. The instructions were segmented by the steps in the actual route and labeled with the action taken in each step. This ﬂat formulation reduced the problem to an IE/Segmentation task, to which we applied Conditional Random Fields. We compared the performance of CRFs with a set of hand-written rules. The result showed that CRFs perform better with a 73.7% success rate. 
We investigate independent and relevant event-based extractive mutli-document summarization approaches. In this paper, events are defined as event terms and associated event elements. With independent approach, we identify important contents by frequency of events. With relevant approach, we identify important contents by PageRank algorithm on the event map constructed from documents. Experimental results are encouraging. 
Both rhetorical structure and punctuation have been helpful in discourse processing. Based on a corpus annotation project, this paper reports the discursive usage of 6 Chinese punctuation marks in news commentary texts: Colon, Dash, Ellipsis, Exclamation Mark, Question Mark, and Semicolon. The rhetorical patterns of these marks are compared against patterns around cue phrases in general. Results show that these Chinese punctuation marks, though fewer in number than cue phrases, are easy to identify, have strong correlation with certain relations, and can be used as distinctive indicators of nuclearity in Chinese texts. 
Current parsing models are not immediately applicable for languages that exhibit strong interaction between morphology and syntax, e.g., Modern Hebrew (MH), Arabic and other Semitic languages. This work represents a ﬁrst attempt at modeling morphological-syntactic interaction in a generative probabilistic framework to allow for MH parsing. We show that morphological information selected in tandem with syntactic categories is instrumental for parsing Semitic languages. We further show that redundant morphological information helps syntactic disambiguation. 
We present a novel hybrid approach for Word Sense Disambiguation (WSD) which makes use of a relational formalism to represent instances and background knowledge. It is built using Inductive Logic Programming techniques to combine evidence coming from both sources during the learning process, producing a rule-based WSD model. We experimented with this approach to disambiguate 7 highly ambiguous verbs in EnglishPortuguese translation. Results showed that the approach is promising, achieving an average accuracy of 75%, which outperforms the other machine learning techniques investigated (66%). 
An open-domain spoken dialog system has to deal with the challenge of lacking lexical as well as conceptual knowledge. As the real world is constantly changing, it is not possible to store all necessary knowledge beforehand. Therefore, this knowledge has to be acquired during the run time of the system, with the help of the out-of-vocabulary information of a speech recognizer. As every word can have various meanings depending on the context in which it is uttered, additional context information is taken into account, when searching for the meaning of such a word. In this paper, I will present the incremental ontology learning framework On2L. The deﬁned tasks for the framework are: the hypernym extraction from Internet texts for unknown terms delivered by the speech recognizer; the mapping of those and their hypernyms into ontological concepts and instances; and the following integration of them into the system’s ontology. 
We analyze the concept of focus in speech and the relationship between focus and speech acts for prosodic generation. We determine how the speaker’s utterances are inﬂuenced by speaker’s intention. The relationship between speech acts and focus information is used to deﬁne which parts of the sentence serve as the focus parts. We propose the Focus to Emphasize Tones (FET) structure to analyze the focus components. We also design the FET grammar to analyze the intonation patterns and produce tone marks as a result of our analysis. We present a proof-of-the-concept working example to validate our proposal. More comprehensive evaluations are part of our current work. 
We present the implementation of a system which extracts not only lexicalized grammars but also feature-based lexicalized grammars from Korean Sejong Treebank. We report on some practical experiments where we extract TAG grammars and tree schemata. Above all, full-scale syntactic tags and well-formed morphological analysis in Sejong Treebank allow us to extract syntactic features. In addition, we modify Treebank for extracting lexicalized grammars and convert lexicalized grammars into tree schemata to resolve limited lexical coverage problem of extracted lexicalized grammars. 
In this paper, we compare the performance of a state-of-the-art statistical parser (Bikel, 2004) in parsing written and spoken language and in generating subcategorization cues from written and spoken language. Although Bikel’s parser achieves a higher accuracy for parsing written language, it achieves a higher accuracy when extracting subcategorization cues from spoken language. Additionally, we explore the utility of punctuation in helping parsing and extraction of subcategorization cues. Our experiments show that punctuation is of little help in parsing spoken language and extracting subcategorization cues from spoken language. This indicates that there is no need to add punctuation in transcribing spoken corpora simply in order to help parsers. 
We introduce a new multi-threaded parsing algorithm on uniﬁcation grammars designed speciﬁcally for multimodal interaction and noisy environments. By lifting some traditional constraints, namely those related to the ordering of constituents, we overcome several difﬁculties of other systems in this domain. We also present several criteria used in this model to constrain the search process using dynamically loadable scoring functions. Some early analyses of our implementation are discussed.  CLAVIUS provides a ﬂexible, and trainable new bi-directional parsing algorithm on multidimensional input spaces, and produces modalityindependent semantic interpretation with a low computational cost.  
In this paper, we describe the research using machine learning techniques to build a comma checker to be integrated in a grammar checker for Basque. After several experiments, and trained with a little corpus of 100,000 words, the sys­ tem guesses correctly not placing com­ mas with a precision of 96% and a re­ call of 98%. It also gets a precision of 70% and a recall of 49% in the task of placing commas. Finally, we have shown that these results can be im­ proved using a bigger and a more ho­ mogeneous corpus to train, that is, a bigger corpus written by one unique au­ thor. 
In this paper, we describe a rote extractor that learns patterns for ﬁnding semantic relationships in unrestricted text, with new procedures for pattern generalization and scoring. These include the use of partof-speech tags to guide the generalization, Named Entity categories inside the patterns, an edit-distance-based pattern generalization algorithm, and a pattern accuracy calculation procedure based on evaluating the patterns on several test corpora. In an evaluation with 14 entities, the system attains a precision higher than 50% for half of the relationships considered. 
We present a comparative study on Machine Translation Evaluation according to two different criteria: Human Likeness and Human Acceptability. We provide empirical evidence that there is a relationship between these two kinds of evaluation: Human Likeness implies Human Acceptability but the reverse is not true. From the point of view of automatic evaluation this implies that metrics based on Human Likeness are more reliable for system tuning. Our results also show that current evaluation metrics are not always able to distinguish between automatic and human translations. In order to improve the descriptive power of current metrics we propose the use of additional syntax-based metrics, and metric combinations inside the QARLA Framework. 
We investigate the eﬀect of corpus size in combining supervised and unsupervised learning for two types of attachment decisions: relative clause attachment and prepositional phrase attachment. The supervised component is Collins’ parser, trained on the Wall Street Journal. The unsupervised component gathers lexical statistics from an unannotated corpus of newswire text. We ﬁnd that the combined system only improves the performance of the parser for small training sets. Surprisingly, the size of the unannotated corpus has little eﬀect due to the noisiness of the lexical statistics acquired by unsupervised learning. 
 traditional MT is treated as a “black-box” with  little or minimal adaptation. One advantage of  Short Messaging Service (SMS) texts be-  this pre-translation normalization is that the di-  have quite differently from normal written  versity in different user groups and domains can  texts and have some very special phenom-  be modeled separately without accessing and  ena. To translate SMS texts, traditional  adapting the language model of the MT system  approaches model such irregularities di-  for each SMS application. Another advantage is  rectly in Machine Translation (MT). How-  that the normalization module can be easily util-  ever, such approaches suffer from  ized by other applications, such as SMS to  customization problem as tremendous ef-  voicemail and SMS-based information query.  fort is required to adapt the language  In this paper, we present a phrase-based statis-  model of the existing translation system to  tical model for SMS text normalization. The  handle SMS text style. We offer an alter-  normalization is visualized as a translation prob-  native approach to resolve such irregulari-  lem where messages in the SMS language are to  ties by normalizing SMS texts before MT.  be translated to normal English using a similar  In this paper, we view the task of SMS  phrase-based statistical MT method (Koehn et al.,  normalization as a translation problem  2003). We use IBM’s BLEU score (Papineni et  from the SMS language to the English language 1 and we propose to adapt a  al., 2002) to measure the performance of SMS text normalization. BLEU score computes the  phrase-based statistical MT model for the  similarity between two sentences using n-gram  task. Evaluation by 5-fold cross validation  statistics, which is widely-used in MT evalua-  on a parallel SMS normalized corpus of  tion. A set of parallel SMS messages, consisting  5000 sentences shows that our method can  of 5000 raw (un-normalized) SMS messages and  achieve 0.80702 in BLEU score against  their manually normalized references, is con-  the baseline BLEU score 0.6958. Another  structed for training and testing. Evaluation by 5-  experiment of translating SMS texts from  fold cross validation on this corpus shows that  English to Chinese on a separate SMS text  our method can achieve accuracy of 0.80702 in  corpus shows that, using SMS normaliza-  BLEU score compared to the baseline system of  tion as MT preprocessing can largely  0.6985. We also study the impact of our SMS  boost SMS translation performance from  text normalization on the task of SMS transla-  0.1926 to 0.3770 in BLEU score.  tion. The experiment of translating SMS texts  from English to Chinese on a corpus comprising  
We evaluate the accuracy of an unlexicalized statistical parser, trained on 4K treebanked sentences from balanced data and tested on the PARC DepBank. We demonstrate that a parser which is competitive in accuracy (without sacriﬁcing processing speed) can be quickly tuned without reliance on large in-domain manuallyconstructed treebanks. This makes it more practical to use statistical parsers in applications that need access to aspects of predicate-argument structure. The comparison of systems using DepBank is not straightforward, so we extend and validate DepBank and highlight a number of representation and scoring issues for relational evaluation schemes. 
We show that we can automatically classify semantically related phrases into 10 classes. Classiﬁcation robustness is improved by training with multiple sources of evidence, including within-document cooccurrence, HTML markup, syntactic relationships in sentences, substitutability in query logs, and string similarity. Our work provides a benchmark for automatic n-way classiﬁcation into WordNet’s semantic classes, both on a TREC news corpus and on a corpus of substitutable search query phrases. 
To enable conversational QA, it is important to examine key issues addressed in conversational systems in the context of question answering. In conversational systems, understanding user intent is critical to the success of interaction. Recent studies have also shown that the capability to automatically identify problematic situations during interaction can signiﬁcantly improve the system performance. Therefore, this paper investigates the new implications of user intent and problematic situations in the context of question answering. Our studies indicate that, in basic interactive QA, there are different types of user intent that are tied to different kinds of system performance (e.g., problematic/error free situations). Once users are motivated to ﬁnd speciﬁc information related to their information goals, the interaction context can provide useful cues for the system to automatically identify problematic situations and user intent. 
Pipeline computation, in which a task is decomposed into several stages that are solved sequentially, is a common computational strategy in natural language processing. The key problem of this model is that it results in error accumulation and suffers from its inability to correct mistakes in previous stages. We develop a framework for decisions made via in pipeline models, which addresses these difﬁculties, and presents and evaluates it in the context of bottom up dependency parsing for English. We show improvements in the accuracy of the inferred trees relative to existing models. Interestingly, the proposed algorithm shines especially when evaluated globally, at a sentence level, where our results are signiﬁcantly better than those of existing approaches. 
Named entity translation is indispensable in cross language information retrieval nowadays. We propose an approach of combining lexical information, web statistics, and inverse search based on Google to backward translate a Chinese named entity (NE) into English. Our system achieves a high Top-1 accuracy of 87.6%, which is a relatively good performance reported in this area until present. 
This paper presents an unsupervised learning approach to disambiguate various relations between name entities by use of various lexical and syntactic features from the contexts. It works by calculating eigenvectors of an adjacency graph’s Laplacian to recover a submanifold of data from a high dimensionality space and then performing cluster number estimation on the eigenvectors. Experiment results on ACE corpora show that this spectral clustering based approach outperforms the other clustering methods. 
In this paper, we describe an empirical study of Chinese chunking on a corpus, which is extracted from UPENN Chinese Treebank-4 (CTB4). First, we compare the performance of the state-of-the-art machine learning models. Then we propose two approaches in order to improve the performance of Chinese chunking. 1) We propose an approach to resolve the special problems of Chinese chunking. This approach extends the chunk tags for every problem by a tag-extension function. 2) We propose two novel voting methods based on the characteristics of chunking task. Compared with traditional voting methods, the proposed voting methods consider long distance information. The experimental results show that the SVMs model outperforms the other models and that our proposed approaches can improve performance signiﬁcantly. 
Word alignment methods can gain valuable guidance by ensuring that their alignments maintain cohesion with respect to the phrases speciﬁed by a monolingual dependency tree. However, this hard constraint can also rule out correct alignments, and its utility decreases as alignment models become more complex. We use a publicly available structured output SVM to create a max-margin syntactic aligner with a soft cohesion constraint. The resulting aligner is the ﬁrst, to our knowledge, to use a discriminative learning method to train an ITG bitext parser. 
There are some sorts of ‘Preposition + Noun’ combinations in Farsi that apparently a Prepositional Phrase almost behaves as Compound Prepositions. As they are not completely behaving as compounds, it is doubtful that the process of word formation is a morphological one. The analysis put forward by this paper proposes “incorporation” by which an No is incorporated to a Po constructing a compound preposition. In this way tagging prepositions and parsing texts in Natural Language Processing is defined in a proper manner. 
This paper explores techniques to take advantage of the fundamental difference in structure between hidden Markov models (HMM) and hierarchical hidden Markov models (HHMM). The HHMM structure allows repeated parts of the model to be merged together. A merged model takes advantage of the recurring patterns within the hierarchy, and the clusters that exist in some sequences of observations, in order to increase the extraction accuracy. This paper also presents a new technique for reconstructing grammar rules automatically. This work builds on the idea of combining a phrase extraction method with HHMM to expose patterns within English text. The reconstruction is then used to simplify the complex structure of an HHMM The models discussed here are evaluated by applying them to natural language tasks based on CoNLL-20041 and a sub-corpus of the Lancaster Treebank2. Keywords: information extraction, natural language, hidden Markov models. 
Cross-linguistic similarities are reﬂected by the speech sound systems of languages all over the world. In this work we try to model such similarities observed in the consonant inventories, through a complex bipartite network. We present a systematic study of some of the appealing features of these inventories with the help of the bipartite network. An important observation is that the occurrence of consonants follows a two regime power law distribution. We ﬁnd that the consonant inventory size distribution together with the principle of preferential attachment are the main reasons behind the emergence of such a two regime behavior. In order to further support our explanation we present a synthesis model for this network based on the general theory of preferential attachment. 
Data-driven grammatical function tag assignment has been studied for English using the Penn-II Treebank data. In this paper we address the question of whether such methods can be applied successfully to other languages and treebank resources. In addition to tag assignment accuracy and f-scores we also present results of a task-based evaluation. We use three machine-learning methods to assign Cast3LB function tags to sentences parsed with Bikel’s parser trained on the Cast3LB treebank. The best performing method, SVM, achieves an f-score of 86.87% on gold-standard trees and 66.67% on parser output - a statistically signiﬁcant improvement of 6.74% over the baseline. In a task-based evaluation we generate LFG functional-structures from the functiontag-enriched trees. On this task we achive an f-score of 75.67%, a statistically significant 3.4% improvement over the baseline. 
The ability to compress sentences while preserving their grammaticality and most of their meaning has recently received much attention. Our work views sentence compression as an optimisation problem. We develop an integer programming formulation and infer globally optimal compressions in the face of linguistically motivated constraints. We show that such a formulation allows for relatively simple and knowledge-lean compression models that do not require parallel corpora or largescale resources. The proposed approach yields results comparable and in some cases superior to state-of-the-art. 
We consider the problem of producing a multi-document summary given a collection of documents. Since most successful methods of multi-document summarization are still largely extractive, in this paper, we explore just how well an extractive method can perform. We introduce an “oracle” score, based on the probability distribution of unigrams in human summaries. We then demonstrate that with the oracle score, we can generate extracts which score, on average, better than the human summaries, when evaluated with ROUGE. In addition, we introduce an approximation to the oracle score which produces a system with the best known performance for the 2005 Document Understanding Conference (DUC) evaluation.  that unigram overlap is around 40%. (Teufel and van Halteren 2004) used a “factoid” agreement analysis of human summaries for a single document and concluded that a resulting consensus summary is stable only if 30–40 summaries are collected. In light of the strong evidence that nearly half of the terms in human-generated multi-document abstracts are not from the original documents, and that agreement of vocabulary among human abstracts is only about 40%, we pose two coupled questions about the quality of summaries that can be attained by document extraction: 1. Given the sets of unigrams used by four human summarizers, can we produce an extract summary that is statistically indistinguishable from the human abstracts when measured by current automatic evaluation methods such as ROUGE?  
We present an algorithm for automatically disambiguating noun-noun compounds by deducing the correct semantic relation between their constituent words. This algorithm uses a corpus of 2,500 compounds annotated with WordNet senses and covering 139 different semantic relations (we make this corpus available online for researchers interested in the semantics of noun-noun compounds). The algorithm takes as input the WordNet senses for the nouns in a compound, ﬁnds all parent senses (hypernyms) of those senses, and searches the corpus for other compounds containing any pair of those senses. The relation with the highest proportional cooccurrence with any sense pair is returned as the correct relation for the compound. This algorithm was tested using a ’leaveone-out’ procedure on the corpus of compounds. The algorithm identiﬁed the correct relations for compounds with high precision: in 92% of cases where a relation was found with a proportional cooccurrence of 1.0, it was the correct relation for the compound being disambiguated. Keywords: Noun-Noun Compounds, Conceputal Combination, Word Relations, WordNet 
Most approaches to event extraction focus on mentions anchored in verbs. However, many mentions of events surface as noun phrases. Detecting them can increase the recall of event extraction and provide the foundation for detecting relations between events. This paper describes a weaklysupervised method for detecting nominal event mentions that combines techniques from word sense disambiguation (WSD) and lexical acquisition to create a classiﬁer that labels noun phrases as denoting events or non-events. The classiﬁer uses bootstrapped probabilistic generative models of the contexts of events and non-events. The contexts are the lexically-anchored semantic dependency relations that the NPs appear in. Our method dramatically improves with bootstrapping, and comfortably outperforms lexical lookup methods which are based on very much larger handcrafted resources. 
This paper proposes a new approach for Multi-word Expression (MWE)extraction on the motivation of gene sequence alignment because textual sequence is similar to gene sequence in pattern analysis. Theory of Longest Common Subsequence (LCS) originates from computer science and has been established as afﬁne gap model in Bioinformatics. We perform this developed LCS technique combined with linguistic criteria in MWE extraction. In comparison with traditional n-gram method, which is the major technique for MWE extraction, LCS approach is applied with great efﬁciency and performance guarantee. Experimental results show that LCS-based approach achieves better results than n-gram. 
 In this paper we discuss the current methods in the representation of corpora annotated at multiple levels of linguistic organization (so-called multi-level or multi-layer corpora). Taking ﬁve approaches which are representative of the current practice in this area, we discuss the commonalities and differences between them focusing on the underlying data models. The goal of the paper is to identify the common concerns in multi-layer corpus representation and processing so as to lay a foundation for a unifying, modular data model. 
Most machine transliteration systems transliterate out of vocabulary (OOV) words through intermediate phonemic mapping. A framework has been presented that allows direct orthographical mapping between two languages that are of different origins employing different alphabet sets. A modified joint source–channel model along with a number of alternatives have been proposed. Aligned transliteration units along with their context are automatically derived from a bilingual training corpus to generate the collocational statistics. The transliteration units in Bengali words take the pattern C+M where C represents a vowel or a consonant or a conjunct and M represents the vowel modifier or matra. The English transliteration units are of the form C*V* where C represents a consonant and V represents a vowel. A Bengali-English machine transliteration system has been developed based on the proposed models. The system has been trained to transliterate person names from Bengali to English. It uses the linguistic knowledge of possible conjuncts and diphthongs in Bengali and their equivalents in English. The system has been evaluated and it has been observed that the modified joint source-channel model performs best with a Word Agreement Ratio of 69.3% and a Transliteration Unit Agreement Ratio of 89.8%.  
Using abundant Web resources to mine Chinese term translations can be applied in many fields such as reading/writing assistant, machine translation and crosslanguage information retrieval. In mining English translations of Chinese terms, how to obtain effective Web pages and evaluate translation candidates are two challenging issues. In this paper, the approach based on semantic prediction is first proposed to obtain effective Web pages. The proposed method predicts possible English meanings according to each constituent unit of Chinese term, and expands these English items using semantically relevant knowledge for searching. The refined related terms are extracted from top retrieved documents through feedback learning to construct a new query expansion for acquiring more effective Web pages. For obtaining a correct translation list, a translation evaluation method in the weighted sum of multi-features is presented to rank these candidates estimated from effective Web pages. Experimental results demonstrate that the proposed method has good performance in Chinese-English term translation acquisition, and achieves 82.9% accuracy. 
Recently, many Natural Language Processing (NLP) applications have improved the quality of their output by using various machine learning techniques to mine Information Extraction (IE) patterns for capturing information from the input text. Currently, to mine IE patterns one should know in advance the type of the information that should be captured by these patterns. In this work we propose a novel methodology for corpus analysis based on cross-examination of several document collections representing different instances of the same domain. We show that this methodology can be used for automatic domain template creation. As the problem of automatic domain template creation is rather new, there is no well-deﬁned procedure for the evaluation of the domain template quality. Thus, we propose a methodology for identifying what information should be present in the template. Using this information we evaluate the automatically created domain templates through the text snippets retrieved according to the created templates. 
This paper presents a detailed study of the integration of knowledge from both dependency parses and hierarchical word ontologies into a maximum-entropy-based tagging model that simultaneously labels words with both syntax and semantics. Our ﬁndings show that information from both these sources can lead to strong improvements in overall system accuracy: dependency knowledge improved performance over all classes of word, and knowledge of the position of a word in an ontological hierarchy increased accuracy for words not seen in the training data. The resulting tagger offers the highest reported tagging accuracy on this tagset to date. 
To study PP attachment disambiguation as a benchmark for empirical methods in natural language processing it has often been reduced to a binary decision problem (between verb or noun attachment) in a particular syntactic conﬁguration. A parser, however, must solve the more general task of deciding between more than two alternatives in many different contexts. We combine the attachment predictions made by a simple model of lexical attraction with a full-ﬂedged parser of German to determine the actual beneﬁt of the subtask to parsing. We show that the combination of data-driven and rule-based components can reduce the number of all parsing errors by 14% and raise the attachment accuracy for dependency parsing of German to an unprecedented 92%. 
We address the problem dealing with skewed data, and propose a method for estimating effective training stories for the topic tracking task. For a small number of labelled positive stories, we extract story pairs which consist of positive and its associated stories from bilingual comparable corpora. To overcome the problem of a large number of labelled negative stories, we classify them into some clusters. This is done by using k-means with EM. The results on the TDT corpora show the effectiveness of the method. 
We propose a robust method of automatically constructing a bilingual word sense dictionary from readily available monolingual ontologies by using estimation-maximization, without any annotated training data or manual tuning. We demonstrate our method on the English FrameNet and Chinese HowNet structures. Owing to the robustness of EM iterations in improving translation likelihoods, our word sense translation accuracies are very high, at 82% on average, for the 11 most ambiguous words in the English FrameNet with 5 senses or more. We also carried out a pilot study on using this automatically generated bilingual word sense dictionary to choose the best translation candidates and show the first significant evidence that frame semantics are useful for translation disambiguation. Translation disambiguation accuracy using frame semantics is 75%, compared to 15% by using dictionary glossing only. These results demonstrate the great potential for future application of bilingual frame semantics to machine translation tasks. 
We claim that existing speciﬁcation languages for tree based grammars fail to adequately support identiﬁer managment. We then show that XMG (eXtensible MetaGrammar) provides a sophisticated treatment of identiﬁers which is effective in supporting a linguist-friendly grammar design. 
One of the challenges in the automatic generation of referring expressions is to identify a set of domain entities coherently, that is, from the same conceptual perspective. We describe and evaluate an algorithm that generates a conceptually coherent description of a target set. The design of the algorithm is motivated by the results of psycholinguistic experiments. 
Semantic parsing is the task of mapping natural language sentences to complete formal meaning representations. The performance of semantic parsing can be potentially improved by using discriminative reranking, which explores arbitrary global features. In this paper, we investigate discriminative reranking upon a baseline semantic parser, SCISSOR, where the composition of meaning representations is guided by syntax. We examine if features used for syntactic parsing can be adapted for semantic parsing by creating similar semantic features based on the mapping between syntax and semantics. We report experimental results on two real applications, an interpreter for coaching instructions in robotic soccer and a naturallanguage database interface. The results show that reranking can improve the performance on the coaching interpreter, but not on the database interface. 
This paper deals with multilingual database generation from parallel corpora. The idea is to contribute to the enrichment of lexical databases for languages with few linguistic resources. Our approach is endogenous: it relies on the raw texts only, it does not require external linguistic resources such as stemmers or taggers. The system produces alignments for the 20 European languages of the ‘Acquis Communautaire’ Corpus. 
Synchronous Context-Free Grammars (SCFGs) have been successfully exploited as translation models in machine translation applications. When parsing with an SCFG, computational complexity grows exponentially with the length of the rules, in the worst case. In this paper we examine the problem of factorizing each rule of an input SCFG to a generatively equivalent set of rules, each having the smallest possible length. Our algorithm works in time O(n log n), for each rule of length n. This improves upon previous results and solves an open problem about recognizing permutations that can be factored. 
This paper studies the enrichment of Spanish WordNet with synset glosses automatically obtained from the English WordNet glosses using a phrase-based Statistical Machine Translation system. We construct the English-Spanish translation system from a parallel corpus of proceedings of the European Parliament, and study how to adapt statistical models to the domain of dictionary deﬁnitions. We build specialized language and translation models from a small set of parallel deﬁnitions and experiment with robust manners to combine them. A statistically signiﬁcant increase in performance is obtained. The best system is ﬁnally used to generate a deﬁnition for all Spanish synsets, which are currently ready for a manual revision. As a complementary issue, we analyze the impact of the amount of in-domain data needed to improve a system trained entirely on out-of-domain data. 
Parsing is a computationally intensive task due to the combinatorial explosion seen in chart parsing algorithms that explore possible parse trees. In this paper, we propose a method to limit the combinatorial explosion by restricting the CYK chart parsing algorithm based on the output of a chunk parser. When tested on the three parsers presented in (Collins, 1999), we observed an approximate three–fold speedup with only an average decrease of 0.17% in both precision and recall. 
Example-based parsing has already been proposed in literature. In particular, attempts are being made to develop techniques for language pairs where the source and target languages are different, e.g. Direct Projection Algorithm (Hwa et al., 2005). This enables one to develop parsed corpus for target languages having fewer linguistic tools with the help of a resourcerich source language. The DPA algorithm works on the assumption of Direct Correspondence which simply means that the relation between two words of the source language sentence can be projected directly between the corresponding words of the parallel target language sentence. However, we ﬁnd that this assumption does not hold good all the time. This leads to wrong parsed structure of the target language sentence. As a solution we propose an algorithm called pseudo DPA (pDPA) that can work even if Direct Correspondence assumption is not guaranteed. The proposed algorithm works in a recursive manner by considering the embedded phrase structures from outermost level to the innermost. The present work discusses the pDPA algorithm, and illustrates it with respect to English-Hindi language pair. Link Grammar based parsing has been considered as the underlying parsing scheme for this work. 
Statistical language models should improve as the size of the n-grams increases from 3 to 5 or higher. However, the number of parameters and calculations, and the storage requirement increase very rapidly if we attempt to store all possible combinations of n-grams. To avoid these problems, the reduced n-grams’ approach previously developed by O’Boyle (1993) can be applied. A reduced n-gram language model can store an entire corpus’s phrase-history length within feasible storage limits. Another theoretical advantage of reduced n-grams is that they are closer to being semantically complete than traditional models, which include all n-grams. In our experiments, the reduced n-gram Zipf curves are first presented, and compared with previously obtained conventional n-grams for both English and Chinese. The reduced n-gram model is then applied to large English and Chinese corpora. For English, we can reduce the model sizes, compared to 7-gram traditional model sizes, with factors of 14.6 for a 40-million-word corpus and 11.0 for a 500-million-word corpus while obtaining 5.8% and 4.2% improvements in perplexities. For Chinese, we gain a 16.9% perplexity reductions and we reduce the model size by a factor larger than 11.2. This paper is a step towards the modeling of English and Chinese using semantically complete phrases in an n-gram model.  
Deterministic parsing guided by treebankinduced classiﬁers has emerged as a simple and efﬁcient alternative to more complex models for data-driven parsing. We present a systematic comparison of memory-based learning (MBL) and support vector machines (SVM) for inducing classiﬁers for deterministic dependency parsing, using data from Chinese, English and Swedish, together with a variety of different feature models. The comparison shows that SVM gives higher accuracy for richly articulated feature models across all languages, albeit with considerably longer training times. The results also conﬁrm that classiﬁer-based deterministic parsing can achieve parsing accuracy very close to the best results reported for more complex parsing models. 
Japanese dependency structure is usually represented by relationships between phrasal units called bunsetsus. One of the biggest problems with dependency structure analysis in spontaneous speech is that clause boundaries are ambiguous. This paper describes a method for detecting the boundaries of quotations and inserted clauses and that for improving the dependency accuracy by applying the detected boundaries to dependency structure analysis. The quotations and inserted clauses are determined by using an SVM-based text chunking method that considers information on morphemes, pauses, ﬁllers, etc. The information on automatically analyzed dependency structure is also used to detect the beginning of the clauses. Our evaluation experiment using Corpus of Spontaneous Japanese (CSJ) showed that the automatically estimated boundaries of quotations and inserted clauses helped to improve the accuracy of dependency structure analysis. 
Automatically acquired lexicons with subcategorization information have already proved accurate and useful enough for some purposes but their accuracy still shows room for improvement. By means of diathesis alternation, this paper proposes a new filtering method, which improved the performance of Korhonen’s acquisition system remarkably, with the precision increased to 91.18% and recall unchanged, making the acquired lexicon much more practical for further manual proofreading and other NLP uses. 
We present a computationally tractable account of the interactions between sentence markers and focus marking in Somali. Somali, as a Cushitic language, has a basic pattern wherein a small ‘core’ clause is preceded, and in some cases followed by, a set of ‘topics’, which provide sceneseting information against which the core is interpreted. Some topics appear to carry a ‘focus marker’, indicating that they are particularly salient. We will outline a computationally tractable grammar for Somali in which focus marking emerges naturally from a consideration of the use of a range of sentence markers. 
We propose a collaborative framework for collecting Thai unknown words found on Web pages over the Internet. Our main goal is to design and construct a Webbased system which allows a group of interested users to participate in constructing a Thai unknown-word open dictionary. The proposed framework provides supporting algorithms and tools for automatically identifying and extracting unknown words from Web pages of given URLs. The system yields the result of unknownword candidates which are presented to the users for veriﬁcation. The approved unknown words could be combined with the set of existing words in the lexicon to improve the performance of many NLP tasks such as word segmentation, information retrieval and machine translation. Our framework includes word segmentation and morphological analysis modules for handling the non-segmenting characteristic of Thai written language. To take advantage of large available text resource on the Web, our unknown-word boundary identiﬁcation approach is based on the statistical string pattern-matching algorithm. Keywords: Unknown words, open dictionary, word segmentation, morphological analysis, word-boundary detection. 
Recognizing idioms in a sentence is important to sentence understanding. This paper discusses the lexical knowledge of idioms for idiom recognition. The challenges are that idioms can be ambiguous between literal and idiomatic meanings, and that they can be “transformed” when expressed in a sentence. However, there has been little research on Japanese idiom recognition with its ambiguity and transformations taken into account. We propose a set of lexical knowledge for idiom recognition. We evaluated the knowledge by measuring the performance of an idiom recognizer that exploits the knowledge. As a result, more than 90% of the idioms in a corpus are recognized with 90% accuracy. 
 optimum parameters for scoring dependency arcs  Various kinds of scored dependency graphs are proposed as packed shared data structures in combination with optimum dependency tree search algorithms. This paper classiﬁes the scored dependency graphs and discusses the speciﬁc features of the “Dependency Forest” (DF) which is the packed shared data structure adopted in the “Preference Dependency Grammar” (PDG), and proposes the “Graph Branch Algorithm” for computing the optimum dependency tree from a DF. This paper also reports the experiment showing the computational amount and behavior of the graph branch algorithm.  obtained by the discriminative learning method. There are various kinds of dependency analy- sis methods based on the scored DGs. This paper classiﬁes these methods based on the types of the DGs and the basic well-formed constraints and explains the features of the DF adopted in PDG(Hirakawa, 2006). This paper proposes the graph branch algorithm which searches the optimum dependency tree from a DF based on the branch and bound (B&B) method(Ibaraki, 1978) and reports the experiment showing the computational amount and behavior of the graph branch algorithm. As shown below, the combination of the DF and the graph branch algorithm enables the treatment of non-projective dependency analysis and optimum solution search satisfying the single  
We revisit the idea of history-based parsing, and present a history-based parsing framework that strives to be simple, general, and ﬂexible. We also provide a decoder for this probability model that is linear-space, optimal, and anytime. A parser based on this framework, when evaluated on Section 23 of the Penn Treebank, compares favorably with other stateof-the-art approaches, in terms of both accuracy and speed. 
Successful participation in dialogue as well as understanding written text requires, among others, interpretation of speciﬁcations implicitly conveyed through parallel structures. While those whose reconstruction requires insertion of a missing element, such as gapping and ellipsis, have been addressed to a certain extent by computational approaches, there is virtually no work addressing parallel structures headed by vice versa-like operators, whose reconstruction requires transformation. In this paper, we address the meaning reconstruction of such constructs by an informed reasoning process. The applied techniques include building deep semantic representations, application of categories of patterns underlying a formal reconstruction, and using pragmaticallymotivated and empirically justiﬁed preferences. We present an evaluation of our algorithm conducted on a uniform collection of texts containing the phrases in question. 
This paper describes an on-going project concerning with an ontological lexical resource based on the abundant conceptual information grounded on Chinese characters. The ultimate goal of this project is set to construct a cognitively sound and computationally effective character-grounded machine-understandable resource. Philosophically, Chinese ideogram has its ontological status, but its applicability to the NLP task has not been expressed explicitly in terms of language resource. We thus propose the ﬁrst attempt to locate Chinese characters within the context of ontology. Having the primary success in applying it to some NLP tasks, we believe that the construction of this knowledge resource will shed new light on theoretical setting as well as the construction of Chinese lexical semantic resources. 
This paper presents a speech understanding component for enabling robust situated human-robot communication. The aim is to gain semantic interpretations of utterances that serve as a basis for multi-modal dialog management also in cases where the recognized word-stream is not grammatically correct. For the understanding process, we designed semantic processable units, which are adapted to the domain of situated communication. Our framework supports the speciﬁc characteristics of spontaneous speech used in combination with gestures in a real world scenario. It also provides information about the dialog acts. Finally, we present a processing mechanism using these concept structures to generate the most likely semantic interpretation of the utterances and to evaluate the interpretation with respect to semantic coherence. 
This paper proposes an efﬁcient method of sentence retrieval based on syntactic structure. Collins proposed Tree Kernel to calculate structural similarity. However, structual retrieval based on Tree Kernel is not practicable because the size of the index table by Tree Kernel becomes impractical. We propose more efﬁcient algorithms approximating Tree Kernel: Tree Overlapping and Subpath Set. These algorithms are more efﬁcient than Tree Kernel because indexing is possible with practical computation resources. The results of the experiments comparing these three algorithms showed that structural retrieval with Tree Overlapping and Subpath Set were faster than that with Tree Kernel by 100 times and 1,000 times respectively. 
This paper describes the largest scale annotation project involving the Enron email corpus to date. Over 12,500 emails were classified, by humans, into the categories “Business” and “Personal”, and then subcategorised by type within these categories. The paper quantifies how well humans perform on this task (evaluated by inter-annotator agreement). It presents the problems experienced with the separation of these language types. As a final section, the paper presents preliminary results using a machine to perform this classification task. 
In this paper, we exploit non-local features as an estimate of long-distance dependencies to improve performance on the statistical spoken language understanding (SLU) problem. The statistical natural language parsers trained on text perform unreliably to encode non-local information on spoken language. An alternative method we propose is to use trigger pairs that are automatically extracted by a feature induction algorithm. We describe a light version of the inducer in which a simple modiﬁcation is efﬁcient and successful. We evaluate our method on an SLU task and show an error reduction of up to 27% over the base local model. 
Name tagging is a critical early stage in many natural language processing pipelines. In this paper we analyze the types of errors produced by a tagger, distinguishing name classification and various types of name identification errors. We present a joint inference model to improve Chinese name tagging by incorporating feedback from subsequent stages in an information extraction pipeline: name structure parsing, cross-document coreference, semantic relation extraction and event extraction. We show through examples and performance measurement how different stages can correct different types of errors. The resulting accuracy approaches that of individual human annotators. 
We propose an unsupervised segmentation method based on an assumption about language data: that the increasing point of entropy of successive characters is the location of a word boundary. A large-scale experiment was conducted by using 200 MB of unsegmented training data and 1 MB of test data, and precision of 90% was attained with recall being around 80%. Moreover, we found that the precision was stable at around 90% independently of the learning data size. 
We present a FrameNet-based semantic role labeling system for Swedish text. As training data for the system, we used an annotated corpus that we produced by transferring FrameNet annotation from the English side to the Swedish side in a parallel corpus. In addition, we describe two frame element bracketing algorithms that are suitable when no robust constituent parsers are available. We evaluated the system on a part of the FrameNet example corpus that we translated manually, and obtained an accuracy score of 0.75 on the classiﬁcation of presegmented frame elements, and precision and recall scores of 0.67 and 0.47 for the complete task. 
This paper explores techniques for reducing the effectiveness of standard authorship attribution techniques so that an author A can preserve anonymity for a particular document D. We discuss feature selection and adjustment and show how this information can be fed back to the author to create a new document D’ for which the calculated attribution moves away from A. Since it can be labor intensive to adjust the document in this fashion, we attempt to quantify the amount of effort required to produce the anonymized document and introduce two levels of anonymization: shallow and deep. In our test set, we show that shallow anonymization can be achieved by making 14 changes per 1000 words to reduce the likelihood of identifying A as the author by an average of more than 83%. For deep anonymization, we adapt the unmasking work of Koppel and Schler to provide feedback that allows the author to choose the level of anonymization. 
This paper proposes a novel method of building polarity-tagged corpus from HTML documents. The characteristics of this method is that it is fully automatic and can be applied to arbitrary HTML documents. The idea behind our method is to utilize certain layout structures and linguistic pattern. By using them, we can automatically extract such sentences that express opinion. In our experiment, the method could construct a corpus consisting of 126,610 sentences. 
Several NLP tasks are characterized by asymmetric data where one class label NONE, signifying the absence of any structure (named entity, coreference, relation, etc.) dominates all other classes. Classiﬁers built on such data typically have a higher precision and a lower recall and tend to overproduce the NONE class. We present a novel scheme for voting among a committee of classiﬁers that can signiﬁcantly boost the recall in such situations. We demonstrate results showing up to a 16% relative improvement in ACE value for the 2004 ACE relation extraction task for English, Arabic and Chinese. 
State-of-the-art computer-assisted translation engines are based on a statistical prediction engine, which interactively provides completions to what a human translator types. The integration of human speech into a computer-assisted system is also a challenging area and is the aim of this paper. So far, only a few methods for integrating statistical machine translation (MT) models with automatic speech recognition (ASR) models have been studied. They were mainly based on N best rescoring approach. N -best rescoring is not an appropriate search method for building a real-time prediction engine. In this paper, we study the incorporation of MT models and ASR models using ﬁnite-state automata. We also propose some transducers based on MT models for rescoring the ASR word graphs. 
A resource grammar is a standard library for the GF grammar formalism. It raises the abstraction level of writing domainspeciﬁc grammars by taking care of the general grammatical rules of a language. GF resource grammars have been built in parallel for eleven languages and share a common interface, which simpliﬁes multilingual applications. We reﬂect on our experience with the Russian resource grammar trying to answer the questions: how well Russian ﬁts into the common interface and where the line between languageindependent and language-speciﬁc should be drawn. 
In this paper, we present a system that automatically extracts the pros and cons from online reviews. Although many approaches have been developed for extracting opinions from text, our focus here is on extracting the reasons of the opinions, which may themselves be in the form of either fact or opinion. Leveraging online review sites with author-generated pros and cons, we propose a system for aligning the pros and cons to their sentences in review texts. A maximum entropy model is then trained on the resulting labeled set to subsequently extract pros and cons from online review sites that do not explicitly provide them. Our experimental results show that our resulting system identifies pros and cons with 66% precision and 76% recall. 
We propose a novel method for automatically interpreting compound nouns based on a predeﬁned set of semantic relations. First we map verb tokens in sentential contexts to a ﬁxed set of seed verbs using WordNet::Similarity and Moby’s Thesaurus. We then match the sentences with semantic relations based on the semantics of the seed verbs and grammatical roles of the head noun and modiﬁer. Based on the semantics of the matched sentences, we then build a classiﬁer using TiMBL. The performance of our ﬁnal system at interpreting NCs is 52.6%. 
We study a number of natural language decipherment problems using unsupervised learning. These include letter substitution ciphers, character code conversion, phonetic decipherment, and word-based ciphers with relevance to machine translation. Straightforward unsupervised learning techniques most often fail on the ﬁrst try, so we describe techniques for understanding errors and signiﬁcantly increasing performance. 
Syntactic parsing requires a ﬁne balance between expressivity and complexity, so that naturally occurring structures can be accurately parsed without compromising efﬁciency. In dependency-based parsing, several constraints have been proposed that restrict the class of permissible structures, such as projectivity, planarity, multi-planarity, well-nestedness, gap degree, and edge degree. While projectivity is generally taken to be too restrictive for natural language syntax, it is not clear which of the other proposals strikes the best balance between expressivity and complexity. In this paper, we review and compare the different constraints theoretically, and provide an experimental evaluation using data from two treebanks, investigating how large a proportion of the structures found in the treebanks are permitted under different constraints. The results indicate that a combination of the well-nestedness constraint and a parametric constraint on discontinuity gives a very good ﬁt with the linguistic data. 
In this paper, we compare the performance of a state-of-the-art statistical parser (Bikel, 2004) in parsing written and spoken language and in generating subcategorization cues from written and spoken language. Although Bikel’s parser achieves a higher accuracy for parsing written language, it achieves a higher accuracy when extracting subcategorization cues from spoken language. Our experiments also show that current technology for extracting subcategorization frames initially designed for written texts works equally well for spoken language. Additionally, we explore the utility of punctuation in helping parsing and extraction of subcategorization cues. Our experiments show that punctuation is of little help in parsing spoken language and extracting subcategorization cues from spoken language. This indicates that there is no need to add punctuation in transcribing spoken corpora simply in order to help parsers. 
This paper explores the role of information retrieval in answering “relationship” questions, a new class complex information needs formally introduced in TREC 2005. Since information retrieval is often an integral component of many question answering strategies, it is important to understand the impact of different termbased techniques. Within a framework of sentence retrieval, we examine three factors that contribute to question answering performance: the use of different retrieval engines, relevance (both at the document and sentence level), and redundancy. Results point out the limitations of purely term-based methods to this challenging task. Nevertheless, IR-based techniques provide a strong baseline on top of which more sophisticated language processing techniques can be deployed. 
We investigate the connection between part of speech (POS) distribution and content in language. We deﬁne POS blocks to be groups of parts of speech. We hypothesise that there exists a directly proportional relation between the frequency of POS blocks and their content salience. We also hypothesise that the class membership of the parts of speech within such blocks reﬂects the content load of the blocks, on the basis that open class parts of speech are more content-bearing than closed class parts of speech. We test these hypotheses in the context of Information Retrieval, by syntactically representing queries, and removing from them content-poor blocks, in line with the aforementioned hypotheses. For our ﬁrst hypothesis, we induce POS distribution information from a corpus, and approximate the probability of occurrence of POS blocks as per two statistical estimators separately. For our second hypothesis, we use simple heuristics to estimate the content load within POS blocks. We use the Text REtrieval Conference (TREC) queries of 1999 and 2000 to retrieve documents from the WT2G and WT10G test collections, with ﬁve different retrieval strategies. Experimental outcomes conﬁrm that our hypotheses hold in the context of Information Retrieval. 
A number of metrics for automatic evaluation of machine translation have been proposed in recent years, with some metrics focusing on measuring the adequacy of MT output, and other metrics focusing on ﬂuency. Adequacy-oriented metrics such as BLEU measure n-gram overlap of MT outputs and their references, but do not represent sentence-level information. In contrast, ﬂuency-oriented metrics such as ROUGE-W compute longest common subsequences, but ignore words not aligned by the LCS. We propose a metric based on stochastic iterative string alignment (SIA), which aims to combine the strengths of both approaches. We compare SIA with existing metrics, and ﬁnd that it outperforms them in overall evaluation, and works specially well in ﬂuency evaluation. 
We discuss Image Sense Discrimination (ISD), and apply a method based on spectral clustering, using multimodal features from the image and text of the embedding web page. We evaluate our method on a new data set of annotated web images, retrieved with ambiguous query terms. Experiments investigate different levels of sense granularity, as well as the impact of text and image features, and global versus local text features. 
In this paper we propose a small set of lexical conceptual relations which allow to encode adjectives in computational relational lexica in a principled and integrated way. Our main motivation comes from the fact that adjectives and certain classes of verbs, related in a way or another with adjectives, do not have a satisfactory representation in this kind of lexica. This is due to a great extent to the heterogeneity of their semantic and syntactic properties. We sustain that such properties are mostly derived from the relations holding between adjectives and other POS. Accordingly, our proposal is mainly concerned with the specification of appropriate cross-POS relations to encode adjectives in lexica of the type considered here. 
Dialogue systems are one of the most challenging applications of Natural Language Processing. In recent years, some statistical dialogue models have been proposed to cope with the dialogue problem. The evaluation of these models is usually performed by using them as annotation models. Many of the works on annotation use information such as the complete sequence of dialogue turns or the correct segmentation of the dialogue. This information is not usually available for dialogue systems. In this work, we propose a statistical model that uses only the information that is usually available and performs the segmentation and annotation at the same time. The results of this model reveal the great inﬂuence that the availability of a correct segmentation has in obtaining an accurate annotation of the dialogues. 
Information Extraction (IE) is a fundamental technology for NLP. Previous methods for IE were relying on co-occurrence relations, soft patterns and properties of the target (for example, syntactic role), which result in problems of handling paraphrasing and alignment of instances. Our system ARE (Anchor and Relation) is based on the dependency relation model and tackles these problems by unifying entities according to their dependency relations, which we found to provide more invariant relations between entities in many cases. In order to exploit the complexity and characteristics of relation paths, we further classify the relation paths into the categories of ‘easy’, ‘average’ and ‘hard’, and utilize different extraction strategies based on the characteristics of those categories. Our extraction method leads to improvement in performance by 3% and 6% for MUC4 and MUC6 respectively as compared to the state-of-art IE systems. 
This paper addresses the problem of acquiring lexical semantic relationships, applied to the lexical entailment relation. Our main contribution is a novel conceptual integration between the two distinct acquisition paradigms for lexical relations – the patternbased and the distributional similarity approaches. The integrated method exploits mutual complementary information of the two approaches to obtain candidate relations and informative characterizing features. Then, a small size training set is used to construct a more accurate supervised classifier, showing significant increase in both recall and precision over the original approaches. 
We developed a new method of transforming Japanese case particles when transforming Japanese passive sentences into active sentences. It separates training data into each input particle and uses machine learning for each particle. We also used numerous rich features for learning. Our method obtained a high rate of accuracy (94.30%). In contrast, a method that did not separate training data for any input particles obtained a lower rate of accuracy (92.00%). In addition, a method that did not have many rich features for learning used in a previous study (Murata and Isahara, 2003) obtained a much lower accuracy rate (89.77%). We conﬁrmed that these improvements were signiﬁcant through a statistical test. We also conducted experiments utilizing traditional methods using verb dictionaries and manually prepared heuristic rules and conﬁrmed that our method obtained much higher accuracy rates than traditional methods. 
Countability of English nouns is important in various natural language processing tasks. It especially plays an important role in machine translation since it determines the range of possible determiners. This paper proposes a method for reinforcing countability prediction by introducing a novel concept called one countability per discourse. It claims that when a noun appears more than once in a discourse, they will all share the same countability in the discourse. The basic idea of the proposed method is that mispredictions can be correctly overridden using efﬁciently the one countability per discourse property. Experiments show that the proposed method successfully reinforces countability prediction and outperforms other methods used for comparison. 
To solve a problemof how to evaluate computer-produced summaries, a number of automaticand manual methods have been proposed. Manual methods evaluate summaries correctly, because humans evaluate them, but are costly. On the other hand, automatic methods, which use evaluation tools or programs, are low cost, although these methods cannot evaluate summaries as accurately as manual methods. In this paper, we investigate an automatic evaluation method that can reduce the errors of traditional automatic methods by using several evaluation results obtained manually. We conducted some experiments using the data of the Text Summarization Challenge 2 (TSC-2). A comparison with conventional automatic methods shows that our method outperforms other methods usually used. 
This paper examines two problems in document-level sentiment analysis: (1) determining whether a given document is a review or not, and (2) classifying the polarity of a review as positive or negative. We first demonstrate that review identification can be performed with high accuracy using only unigrams as features. We then examine the role of four types of simple linguistic knowledge sources in a polarity classification system. 
We present a learning framework for structured support vector models in which boosting and bagging methods are used to construct ensemble models. We also propose a selection method which is based on a switching model among a set of outputs of individual classiﬁers when dealing with natural language parsing problems. The switching model uses subtrees mined from the corpus and a boosting-based algorithm to select the most appropriate output. The application of the proposed framework on the domain of semantic parsing shows advantages in comparison with the original large margin methods. 
We report initial results on the relatively novel task of automatic classiﬁcation of author personality. Using a corpus of personal weblogs, or ‘blogs’, we investigate the accuracy that can be achieved when classifying authors on four important personality traits. We explore both binary and multiple classiﬁcation, using differing sets of n-gram features. Results are promising for all four traits examined. 
This paper discusses sampling strategies for building a dependency-analyzed corpus and analyzes them with different kinds of corpora. We used the Kyoto Text Corpus, a dependency-analyzed corpus of newspaper articles, and prepared the IPAL corpus, a dependency-analyzed corpus of example sentences in dictionaries, as a new and different kind of corpus. The experimental results revealed that the length of the test set controlled the accuracy and that the longest-ﬁrst strategy was good for an expanding corpus, but this was not the case when constructing a corpus from scratch. 
We present a term recognition approach to extract acronyms and their deﬁnitions from a large text collection. Parenthetical expressions appearing in a text collection are identiﬁed as potential acronyms. Assuming terms appearing frequently in the proximity of an acronym to be the expanded forms (deﬁnitions) of the acronyms, we apply a term recognition method to enumerate such candidates and to measure the likelihood scores of the expanded forms. Based on the list of the expanded forms and their likelihood scores, the proposed algorithm determines the ﬁnal acronym-deﬁnition pairs. The proposed method combined with a letter matching algorithm achieved 78% precision and 85% recall on an evaluation corpus with 4,212 acronym-deﬁnition pairs. 
We introduce the possibility of combining lexical association measures and present empirical results of several methods employed in automatic collocation extraction. First, we present a comprehensive summary overview of association measures and their performance on manually annotated data evaluated by precision-recall graphs and mean average precision. Second, we describe several classiﬁcation methods for combining association measures, followed by their evaluation and comparison with individual measures. Finally, we propose a feature selection algorithm signiﬁcantly reducing the number of combined measures with only a small performance degradation. 
We investigate the use of machine learning in combination with feature engineering techniques to explore human multimodal clariﬁcation strategies and the use of those strategies for dialogue systems. We learn from data collected in a Wizardof-Oz study where different wizards could decide whether to ask a clariﬁcation request in a multimodal manner or else use speech alone. We show that there is a uniform strategy across wizards which is based on multiple features in the context. These are generic runtime features which can be implemented in dialogue systems. Our prediction models achieve a weighted f-score of 85.3% (which is a 25.5% improvement over a one-rule baseline). To assess the effects of models, feature discretisation, and selection, we also conduct a regression analysis. We then interpret and discuss the use of the learnt strategy for dialogue systems. Throughout the investigation we discuss the issues arising from using small initial Wizard-of-Oz data sets, and we show that feature engineering is an essential step when learning from such limited data.  duced cognitive load (Oviatt et al., 2004). In this paper we investigate the use of machine learning (ML) to explore human multimodal clariﬁcation strategies and the use of those strategies to decide, based on the current dialogue context, when a dialogue system’s clariﬁcation request (CR) should be generated in a multimodal manner. In previous work (Rieser and Moore, 2005) we showed that for spoken CRs in humanhuman communication people follow a contextdependent clariﬁcation strategy which systematically varies across domains (and even across Germanic languages). In this paper we investigate whether there exists a context-dependent “intuitive” human strategy for multimodal CRs as well. To test this hypothesis we gathered data in a Wizard-of-Oz (WOZ) study, where different wizards could decide when to show a screen output. From this data we build prediction models, using supervised learning techniques together with feature engineering methods, that may explain the underlying process which generated the data. If we can build a model which predicts the data quite reliably, we can show that there is a uniform strategy that the majority of our wizards followed in certain contexts.  
Most information extraction systems either use hand written extraction patterns or use a machine learning algorithm that is trained on a manually annotated corpus. Both of these approaches require massive human effort and hence prevent information extraction from becoming more widely applicable. In this paper we present URES (Unsupervised Relation Extraction System), which extracts relations from the Web in a totally unsupervised way. It takes as input the descriptions of the target relations, which include the names of the predicates, the types of their attributes, and several seed instances of the relations. Then the system downloads from the Web a large collection of pages that are likely to contain instances of the target relations. From those pages, utilizing the known seed instances, the system learns the relation patterns, which are then used for extraction. We present several experiments in which we learn patterns and extract instances of a set of several common IE relations, comparing several pattern learning and filtering setups. We demonstrate that using simple noun phrase tagger is sufficient as a base for accurate patterns. However, having a named entity recognizer, which is able to recognize the types of the relation attributes significantly, enhances the extraction performance. We also compare our approach with KnowItAll’s fixed generic patterns.  
We report on the development of a new automatic feedback model to improve information retrieval in digital libraries. Our hypothesis is that some particular sentences, selected based on argumentative criteria, can be more useful than others to perform well-known feedback information retrieval tasks. The argumentative model we explore is based on four disjunct classes, which has been very regularly observed in scientiﬁc reports: PURPOSE, METHODS, RESULTS, CONCLUSION. To test this hypothesis, we use the Rocchio algorithm as baseline. While Rocchio selects the features to be added to the original query based on statistical evidence, we propose to base our feature selection also on argumentative criteria. Thus, we restrict the expansion on features appearing only in sentences classiﬁed into one of our argumentative categories. Our results, obtained on the OHSUMED collection, show a signiﬁcant improvement when expansion is based on PURPOSE (mean average precision = +23%) and CONCLUSION (mean average precision = +41%) contents rather than on other argumentative contents. These results suggest that argumentation is an important linguistic dimension that could beneﬁt information retrieval. 
This paper proposes a method for incrementally translating English spoken language into Japanese. To realize simultaneous translation between languages with different word order, such as English and Japanese, our method utilizes the feature that the word order of a target language is ﬂexible. To resolve the problem of generating a grammatically incorrect sentence, our method uses dependency structures and Japanese dependency constraints to determine the word order of a translation. Moreover, by considering the fact that the inversion of predicate expressions occurs more frequently in Japanese spoken language, our method takes advantage of a predicate inversion to resolve the problem that Japanese has the predicate at the end of a sentence. Furthermore, our method includes the function of canceling an inversion by restating a predicate when the translation is incomprehensible due to the inversion. We implement a prototype translation system and conduct an experiment with all 578 sentences in the ATIS corpus. The results indicate improvements in comparison to two other methods. 
Recently proposed deterministic classiﬁerbased parsers (Nivre and Scholz, 2004; Sagae and Lavie, 2005; Yamada and Matsumoto, 2003) offer attractive alternatives to generative statistical parsers. Deterministic parsers are fast, efﬁcient, and simple to implement, but generally less accurate than optimal (or nearly optimal) statistical parsers. We present a statistical shift-reduce parser that bridges the gap between deterministic and probabilistic parsers. The parsing model is essentially the same as one previously used for deterministic parsing, but the parser performs a best-ﬁrst search instead of a greedy search. Using the standard sections of the WSJ corpus of the Penn Treebank for training and testing, our parser has 88.1% precision and 87.8% recall (using automatically assigned part-of-speech tags). Perhaps more interestingly, the parsing model is signiﬁcantly different from the generative models used by other wellknown accurate parsers, allowing for a simple combination that produces precision and recall of 90.9% and 90.7%, respectively. 
In this paper, we propose an implementable characterization of genre suitable for automatic genre identification of web pages. This characterization is implemented as an inferential model based on a modified version of Bayes’ theorem. Such a model can deal with genre hybridism and individualization, two important forces behind genre evolution. Results show that this approach is effective and is worth further research. 
The present paper proposes a method by which to translate outputs of a robust HPSG parser into semantic representations of Typed Dynamic Logic (TDL), a dynamic plural semantics defined in typed lambda calculus. With its higher-order representations of contexts, TDL analyzes and describes the inherently inter-sentential nature of quantification and anaphora in a strictly lexicalized and compositional manner. The present study shows that the proposed translation method successfully combines robustness and descriptive adequacy of contemporary semantics. The present implementation achieves high coverage, approximately 90%, for the real text of the Penn Treebank corpus. 
We are presenting a new, hybrid alignment architecture for aligning bilingual, linguistically annotated parallel corpora. It is able to align simultaneously at paragraph, sentence, phrase and word level, using statistical and heuristic cues, along with linguistics-based rules. The system currently aligns English and German texts, and the linguistic annotation used covers POS-tags, lemmas and syntactic constitutents. However, as the system is highly modular, we can easily adapt it to new language pairs and other types of annotation. The hybrid nature of the system allows experiments with a variety of alignment cues to ﬁnd solutions to word alignment problems like the correct alignment of rare words and multiwords, or how to align despite syntactic differences between two languages. First performance tests are promising, and we are setting up a gold standard for a thorough evaluation of the system. 
Statistical machine translation systems are based on one or more translation models and a language model of the target language. While many different translation models and phrase extraction algorithms have been proposed, a standard word n-gram back-off language model is used in most systems. In this work, we propose to use a new statistical language model that is based on a continuous representation of the words in the vocabulary. A neural network is used to perform the projection and the probability estimation. We consider the translation of European Parliament Speeches. This task is part of an international evaluation organized by the TC-STAR project in 2006. The proposed method achieves consistent improvements in the BLEU score on the development and test data. We also present algorithms to improve the estimation of the language model probabilities when splitting long sentences into shorter chunks.  
At present, adapting an Information Extraction system to new topics is an expensive and slow process, requiring some knowledge engineering for each new topic. We propose a new paradigm of Information Extraction which operates 'on demand' in response to a user's query. On-demand Information Extraction (ODIE) aims to completely eliminate the customization effort. Given a user’s query, the system will automatically create patterns to extract salient relations in the text of the topic, and build tables from the extracted information using paraphrase discovery technology. It relies on recent advances in pattern discovery, paraphrase discovery, and extended named entity tagging. We report on experimental results in which the system created useful tables for many topics, demonstrating the feasibility of this approach. 
In this paper we present a tool that uses comparable corpora to ﬁnd appropriate translation equivalents for expressions that are considered by translators as difﬁcult. For a phrase in the source language the tool identiﬁes a range of possible expressions used in similar contexts in target language corpora and presents them to the translator as a list of suggestions. In the paper we discuss the method and present results of human evaluation of the performance of the tool, which highlight its usefulness when dictionary solutions are lacking. 
Multiple sequence alignment techniques have recently gained popularity in the Natural Language community, especially for tasks such as machine translation, text generation, and paraphrase identiﬁcation. Prior work falls into two categories, depending on the type of input used: (a) parallel corpora (e.g., multiple translations of the same text) or (b) comparable texts (non-parallel but on the same topic). So far, only techniques based on parallel texts have successfully used syntactic information to guide alignments. In this paper, we describe an algorithm for incorporating syntactic features in the alignment process for non-parallel texts with the goal of generating novel paraphrases of existing texts. Our method uses dynamic programming with alignment decision based on the local syntactic similarity between two sentences. Our results show that syntactic alignment outrivals syntax-free methods by 20% in both grammaticality and ﬁdelity when computed over the novel sentences generated by alignment-induced ﬁnite state automata. 
This paper presents an unsupervised topic identiﬁcation method integrating linguistic and visual information based on Hidden Markov Models (HMMs). We employ HMMs for topic identiﬁcation, wherein a state corresponds to a topic and various features including linguistic, visual and audio information are observed. Our experiments on two kinds of cooking TV programs show the effectiveness of our proposed method. 
There are two decoding algorithms essential to the area of natural language processing. One is the Viterbi algorithm for linear-chain models, such as HMMs or CRFs. The other is the CKY algorithm for probabilistic context free grammars. However, tasks such as noun phrase chunking and relation extraction seem to fall between the two, neither of them being the best ﬁt. Ideally we would like to model entities and relations, with two layers of labels. We present a tractable algorithm for exact inference over two layers of labels and chunks with time complexity O(n2), and provide empirical results comparing our model with linear-chain models. 
This paper describes a system which generates animations for cooking actions in recipes, to help people understand recipes written in Japanese. The major goal of this research is to increase the scalability of the system, i.e., to develop a system which can handle various kinds of cooking actions. We designed and compiled the lexicon of cooking actions required for the animation generation system. The lexicon includes the action plan used for animation generation, and the information about ingredients upon which the cooking action is taken. Preliminary evaluation shows that our lexicon contains most of the cooking actions that appear in Japanese recipes. We also discuss how to handle linguistic expressions in recipes, which are not included in the lexicon, in order to generate animations for them. 
In this paper we report our work on building a POS tagger for a morphologically rich language- Hindi. The theme of the research is to vindicate the stand that- if morphology is strong and harnessable, then lack of training corpora is not debilitating. We establish a methodology of POS tagging which the resource disadvantaged (lacking annotated corpora) languages can make use of. The methodology makes use of locally annotated modestly-sized corpora (15,562 words), exhaustive morpohological analysis backed by high-coverage lexicon and a decision tree based learning algorithm (CN2). The evaluation of the system was done with 4-fold cross validation of the corpora in the news domain (www.bbc.co.uk/hindi). The current accuracy of POS tagging is 93.45% and can be further improved. 
When training the parameters for a natural language system, one would prefer to minimize 1-best loss (error) on an evaluation set. Since the error surface for many natural language problems is piecewise constant and riddled with local minima, many systems instead optimize log-likelihood, which is conveniently differentiable and convex. We propose training instead to minimize the expected loss, or risk. We deﬁne this expectation using a probability distribution over hypotheses that we gradually sharpen (anneal) to focus on the 1-best hypothesis. Besides the linear loss functions used in previous work, we also describe techniques for optimizing nonlinear functions such as precision or the BLEU metric. We present experiments training log-linear combinations of models for dependency parsing and for machine translation. In machine translation, annealed minimum risk training achieves significant improvements in BLEU over standard minimum error training. We also show improvements in labeled dependency parsing. 
We exploit the resources in the Arabic Treebank (ATB) and Arabic Gigaword (AG) to determine the best features for the novel task of automatically creating lexical semantic verb classes for Modern Standard Arabic (MSA). The verbs are classiﬁed into groups that share semantic elements of meaning as they exhibit similar syntactic behavior. The results of the clustering experiments are compared with a gold standard set of classes, which is approximated by using the noisy English translations provided in the ATB to create Levin-like classes for MSA. The quality of the clusters is found to be sensitive to the inclusion of syntactic frames, LSA vectors, morphological pattern, and subject animacy. The best set of parameters yields an Fβ=1 score of 0.456, compared to a random baseline of an Fβ=1 score of 0.205. 
We describe a generic framework for integrating various stochastic models of discourse coherence in a manner that takes advantage of their individual strengths. An integral part of this framework are algorithms for searching and training these stochastic coherence models. We evaluate the performance of our models and algorithms and show empirically that utilitytrained log-linear coherence models outperform each of the individual coherence models considered. 
The integration of sophisticated inference-based techniques into natural language processing applications first requires a reliable method of encoding the predicate-argument structure of the propositional content of text. Recent statistical approaches to automated predicateargument annotation have utilized parse tree paths as predictive features, which encode the path between a verb predicate and a node in the parse tree that governs its argument. In this paper, we explore a number of alternatives for how these parse tree paths are encoded, focusing on the difference between automatically generated constituency parses and dependency parses. After describing five alternatives for encoding parse tree paths, we investigate how well each can be aligned with the argument substrings in annotated text corpora, their relative precision and recall performance, and their comparative learning curves. Results indicate that constituency parsers produce parse tree paths that can more easily be aligned to argument substrings, perform better in precision and recall, and have more favorable learning curves than those produced by a dependency parser. 
This paper proposes a knowledge representation model and a logic proving setting with axioms on demand successfully used for recognizing textual entailments. It also details a lexical inference system which boosts the performance of the deep semantic oriented approach on the RTE data. The linear combination of two slightly different logical systems with the third lexical inference system achieves 73.75% accuracy on the RTE 2006 data. 
As an area of great linguistic and cultural diversity, Asian language resources have received much less attention than their western counterparts. Creating a common standard for Asian language resources that is compatible with an international standard has at least three strong advantages: to increase the competitive edge of Asian countries, to bring Asian countries to closer to their western counterparts, and to bring more cohesion among Asian countries. To achieve this goal, we have launched a two year project to create a common standard for Asian language resources. The project is comprised of four research items, (1) building a description framework of lexical entries, (2) building sample lexicons, (3) building an upperlayer ontology and (4) evaluating the proposed framework through an application. This paper outlines the project in terms of its aim and approach. 
Obtaining high-quality machine translations is still a long way off. A postediting phase is required to improve the output of a machine translation system. An alternative is the so called computerassisted translation. In this framework, a human translator interacts with the system in order to obtain high-quality translations. A statistical phrase-based approach to computer-assisted translation is described in this article. A new decoder algorithm for interactive search is also presented, that combines monotone and nonmonotone search. The system has been assessed in the TransType-2 project for the translation of several printer manuals, from (to) English to (from) Spanish, German and French. 
This paper presents a word support model (WSM). The WSM can effectively perform homophone selection and syllable-word segmentation to improve Chinese input systems. The experimental results show that: (1) the WSM is able to achieve tonal (syllables input with four tones) and toneless (syllables input without four tones) syllable-to-word (STW) accuracies of 99% and 92%, respectively, among the converted words; and (2) while applying the WSM as an adaptation processing, together with the Microsoft Input Method Editor 2003 (MSIME) and an optimized bigram model, the average tonal and toneless STW improvements are 37% and 35%, respectively. 
Sentence compression is a task of creating a short grammatical sentence by removing extraneous words or phrases from an original sentence while preserving its meaning. Existing methods learn statistics on trimming context-free grammar (CFG) rules. However, these methods sometimes eliminate the original meaning by incorrectly removing important parts of sentences, because trimming probabilities only depend on parents’ and daughters’ non-terminals in applied CFG rules. We apply a maximum entropy model to the above method. Our method can easily include various features, for example, other parts of a parse tree or words the sentences contain. We evaluated the method using manually compressed sentences and human judgments. We found that our method produced more grammatical and informative compressed sentences than other methods. 
This paper examines what kind of similarity between words can be represented by what kind of word vectors in the vector space model. Through two experiments, three methods for constructing word vectors, i.e., LSA-based, cooccurrence-based and dictionary-based methods, were compared in terms of the ability to represent two kinds of similarity, i.e., taxonomic similarity and associative similarity. The result of the comparison was that the dictionary-based word vectors better reﬂect taxonomic similarity, while the LSAbased and the cooccurrence-based word vectors better reﬂect associative similarity. 
There have been many proposals to extract semantically related words using measures of distributional similarity, but these typically are not able to distinguish between synonyms and other types of semantically related words such as antonyms, (co)hyponyms and hypernyms. We present a method based on automatic word alignment of parallel corpora consisting of documents translated into multiple languages and compare our method with a monolingual syntax-based method. The approach that uses aligned multilingual data to extract synonyms shows much higher precision and recall scores for the task of synonym extraction than the monolingual syntax-based approach. 
This paper proposes an approach to improve word alignment for languages with scarce resources using bilingual corpora of other language pairs. To perform word alignment between languages L1 and L2, we introduce a third language L3. Although only small amounts of bilingual data are available for the desired language pair L1-L2, large-scale bilingual corpora in L1-L3 and L2-L3 are available. Based on these two additional corpora and with L3 as the pivot language, we build a word alignment model for L1 and L2. This approach can build a word alignment model for two languages even if no bilingual corpus is available in this language pair. In addition, we build another word alignment model for L1 and L2 using the small L1-L2 bilingual corpus. Then we interpolate the above two models to further improve word alignment between L1 and L2. Experimental results indicate a relative error rate reduction of 21.30% as compared with the method only using the small bilingual corpus in L1 and L2. 
Spoken Language Understanding (SLU) addresses the problem of extracting semantic meaning conveyed in an utterance. The traditional knowledge-based approach to this problem is very expensive -- it requires joint expertise in natural language processing and speech recognition, and best practices in language engineering for every new domain. On the other hand, a statistical learning approach needs a large amount of annotated data for model training, which is seldom available in practical applications outside of large research labs. A generative HMM/CFG composite model, which integrates easy-toobtain domain knowledge into a data-driven statistical learning framework, has previously been introduced to reduce data requirement. The major contribution of this paper is the investigation of integrating prior knowledge and statistical learning in a conditional model framework. We also study and compare conditional random fields (CRFs) with perceptron learning for SLU. Experimental results show that the conditional models achieve more than 20% relative reduction in slot error rate over the HMM/CFG model, which had already achieved an SLU accuracy at the same level as the best results reported on the ATIS data. 
This paper describes an architecture to convert Sinhala Unicode text into phonemic specification of pronunciation. The study was mainly focused on disambiguating schwa-/\/ and /a/ vowel epenthesis for consonants, which is one of the significant problems found in Sinhala. This problem has been addressed by formulating a set of rules. The proposed set of rules was tested using 30,000 distinct words obtained from a corpus and compared with the same words manually transcribed to phonemes by an expert. The Grapheme-to-Phoneme (G2P) conversion model achieves 98 % accuracy. 
This paper describes an ongoing effort to parse the Hebrew Bible. The parser consults the bracketing information extracted from the cantillation marks of the Masoetic text. We first constructed a cantillation treebank which encodes the prosodic structures of the text. It was found that many of the prosodic boundaries in the cantillation trees correspond, directly or indirectly, to the phrase boundaries of the syntactic trees we are trying to build. All the useful boundary information was then extracted to help the parser make syntactic decisions, either serving as hard constraints in rule application or used probabilistically in tree ranking. This has greatly improved the accuracy and efficiency of the parser and reduced the amount of manual work in building a Hebrew treebank. Introduction The text of the Hebrew Bible (HB) has been carefully studied throughout the centuries, with detailed lexical, phonological and morphological analysis available for every verse of HB. However, very few attempts have been made at a verse-by-verse syntactic analysis. The only known effort in this direction is the Hebrew parser built by George Yaeger (Yaeger 1998, 2002), but the analysis is still incomplete in the sense that not all syntactic units are recognized and the accuracy of the trees are yet to be checked. Since a detailed syntactic analysis of HB is of interest to both linguistic and biblical studies, we launched a project to build a treebank of the Hebrew Bible. In this project, the trees are automatically generated by a parser and then  manually checked in a tree editor. Once a tree has been edited or approved, its phrase boundaries are recorded in a database. When the same verse is parsed again, the existing brackets will force the parser to produce trees whose brackets are exactly the same as those of the manually approved trees. Compared with traditional approaches to treebanking where the correct structure is preserved in a set of tree files, our approach has much more agility. In the event of design/format changes, we can automatically regenerate the trees according to the new specifications without manually touching the trees. The bracketing information will persist through the updates and the basic structure of the trees will remain correct regardless of the changes in the details of trees. We call this a “dynamic treebank” where, instead of maintaining a set of trees, we maintain a parser/grammar, a dictionary, a set of sentences, and a database of bracketing information. The trees can be generated at any time. Since our parser/grammar can consult known phrase boundaries to build trees, its performance can be greatly improved if large amounts of bracketing information are available. Human inspection and correction can provide those boundaries, but the amount of manual work can be reduced significantly if there is an existing source of bracketing information for us to use. Fortunately, a great deal of such information can be obtained from the cantillation marks of the Hebrew text. 
We present an elegant and extensible model that is capable of providing semantic interpretations for an unusually wide range of textual tables in documents. Unlike the few existing table analysis models, which largely rely on relatively ad hoc heuristics, our linguistically-oriented approach is systematic and grammar based, which allows our model (1) to be concise and yet (2) recognize a wider range of data models than others, and (3) disambiguate to a signiﬁcantly ﬁner extent the underlying semantic interpretation of the table in terms of data models drawn from relation database theory. To accomplish this, the model introduces Viterbi parsing under two-dimensional stochastic CFGs. The cleaner grammatical approach facilitates not only greater coverage, but also grammar extension and maintenance, as well as a more direct and declarative link to semantic interpretation, for which we also introduce a new, cleaner data model. In disambiguation experiments on recognizing relevant data models of unseen web tables from different domains, a blind evaluation of the model showed 60% precision and 80% recall. 
This paper proposes a semi-supervised boosting approach to improve statistical word alignment with limited labeled data and large amounts of unlabeled data. The proposed approach modifies the supervised boosting algorithm to a semisupervised learning algorithm by incorporating the unlabeled data. In this algorithm, we build a word aligner by using both the labeled data and the unlabeled data. Then we build a pseudo reference set for the unlabeled data, and calculate the error rate of each word aligner using only the labeled data. Based on this semisupervised boosting algorithm, we investigate two boosting methods for word alignment. In addition, we improve the word alignment results by combining the results of the two semi-supervised boosting methods. Experimental results on word alignment indicate that semisupervised boosting achieves relative error reductions of 28.29% and 19.52% as compared with supervised boosting and unsupervised boosting, respectively. 
This paper designs a novel lexical hub to disambiguate word sense, using both syntagmatic and paradigmatic relations of words. It only employs the semantic network of WordNet to calculate word similarity, and the Edinburgh Association Thesaurus (EAT) to transform contextual space for computing syntagmatic and other domain relations with the target word. Without any back-off policy the result on the English lexical sample of SENSEVAL-21 shows that lexical cohesion based on edge-counting techniques is a good way of unsupervisedly disambiguating senses. 
This investigation proposes an approach to modeling the discourse of spoken dialogue using semantic dependency graphs. By characterizing the discourse as a sequence of speech acts, discourse modeling becomes the identification of the speech act sequence. A statistical approach is adopted to model the relations between words in the user’s utterance using the semantic dependency graphs. Dependency relation between the headword and other words in a sentence is detected using the semantic dependency grammar. In order to evaluate the proposed method, a dialogue system for medical service is developed. Experimental results show that the rates for speech act detection and taskcompletion are 95.6% and 85.24%, respectively, and the average number of turns of each dialogue is 8.3. Compared with the Bayes’ classifier and the PartialPattern Tree based approaches, we obtain 14.9% and 12.47% improvements in accuracy for speech act identification, respectively. 
Negative life events play an important role in triggering depressive episodes. Developing psychiatric services that can automatically identify such events is beneficial for mental health care and prevention. Before these services can be provided, some meaningful semantic patterns, such as <lost, parents>, have to be extracted. In this work, we present a text mining framework capable of inducing variable-length semantic patterns from unannotated psychiatry web resources. This framework integrates a cognitive motivated model, Hyperspace Analog to Language (HAL), to represent words as well as combinations of words. Then, a cascaded induction process (CIP) bootstraps with a small set of seed patterns and incorporates relevance feedback to iteratively induce more relevant patterns. The experimental results show that by combining the HAL model and relevance feedback, the CIP can induce semantic patterns from the unannotated web corpora so as to reduce the reliance on annotated corpora. 
This paper compares different bilexical tree-based models for bilingual alignment. EM training for the new model beneﬁts from the dynamic programming “hook trick”. The model produces improved dependency structure for both languages. 
We proposed a subword-based tagging for Chinese word segmentation to improve the existing character-based tagging. The subword-based tagging was implemented using the maximum entropy (MaxEnt) and the conditional random ﬁelds (CRF) methods. We found that the proposed subword-based tagging outperformed the character-based tagging in all comparative experiments. In addition, we proposed a conﬁdence measure approach to combine the results of a dictionary-based and a subword-tagging-based segmentation. This approach can produce an ideal tradeoff between the in-vocaulary rate and out-of-vocabulary rate. Our techniques were evaluated using the test data from Sighan Bakeoff 2005. We achieved higher F-scores than the best results in three of the four corpora: PKU(0.951), CITYU(0.950) and MSR(0.971). 
We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation. Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-speciﬁc bilingual translation model. Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels). These models enable wordalignment process to leverage topical contents of document-pairs. Efﬁcient variational approximation algorithms are designed for inference and parameter estimation. With the inferred latent topics, BiTAM models facilitate coherent pairing of bilingual linguistic entities that share common topical aspects. Our preliminary experiments show that the proposed models improve word alignment accuracy, and lead to better translation quality. 
Automatic phrasing is essential to Mandarin textto-speech synthesis. We select word format as target linguistic feature and propose an HMMbased approach to this issue. Then we define four states of prosodic positions for each word when employing a discrete hidden Markov model. The approach achieves high accuracy of roughly 82%, which is very close to that from manual labeling. Our experimental results also demonstrate that this approach has advantages over those part-ofspeech-based ones. 
Statistical machine translation is quite robust when it comes to the choice of input representation. It only requires consistency between training and testing. As a result, there is a wide range of possible preprocessing choices for data used in statistical machine translation. This is even more so for morphologically rich languages such as Arabic. In this paper, we study the effect of different word-level preprocessing schemes for Arabic on the quality of phrase-based statistical machine translation. We also present and evaluate different methods for combining preprocessing schemes resulting in improved translation quality. 
This paper presents an extensive evaluation of ﬁve different alignments and investigates their impact on the corresponding MT system output. We introduce new measures for intrinsic evaluations and examine the distribution of phrases and untranslated words during decoding to identify which characteristics of different alignments affect translation. We show that precision-oriented alignments yield better MT output (translating more words and using longer phrases) than recalloriented alignments. 
We present a method for unsupervised topic modelling which adapts methods used in document classiﬁcation (Blei et al., 2003; Grifﬁths and Steyvers, 2004) to unsegmented multi-party discourse transcripts. We show how Bayesian inference in this generative model can be used to simultaneously address the problems of topic segmentation and topic identiﬁcation: automatically segmenting multi-party meetings into topically coherent segments with performance which compares well with previous unsupervised segmentation-only methods (Galley et al., 2003) while simultaneously extracting topics which rate highly when assessed for coherence by human judges. We also show that this method appears robust in the face of off-topic dialogue and speech recognition errors. 
We consider the task of unsupervised lecture segmentation. We formalize segmentation as a graph-partitioning task that optimizes the normalized cut criterion. Our approach moves beyond localized comparisons and takes into account longrange cohesion dependencies. Our results demonstrate that global analysis improves the segmentation accuracy and is robust in the presence of speech recognition errors. 
We present an approach to pronoun resolution based on syntactic paths. Through a simple bootstrapping procedure, we learn the likelihood of coreference between a pronoun and a candidate noun based on the path in the parse tree between the two entities. This path information enables us to handle previously challenging resolution instances, and also robustly addresses traditional syntactic coreference constraints. Highly coreferent paths also allow mining of precise probabilistic gender/number information. We combine statistical knowledge with well known features in a Support Vector Machine pronoun resolution classiﬁer. Signiﬁcant gains in performance are observed on several datasets. 
Syntactic knowledge is important for pronoun resolution. Traditionally, the syntactic information for pronoun resolution is represented in terms of features that have to be selected and deﬁned heuristically. In the paper, we propose a kernel-based method that can automatically mine the syntactic information from the parse trees for pronoun resolution. Speciﬁcally, we utilize the parse trees directly as a structured feature and apply kernel functions to this feature, as well as other normal features, to learn the resolution classiﬁer. In this way, our approach avoids the efforts of decoding the parse trees into the set of ﬂat syntactic features. The experimental results show that our approach can bring signiﬁcant performance improvement and is reliably effective for the pronoun resolution task. 
It has previously been assumed in the psycholinguistic literature that ﬁnite-state models of language are crucially limited in their explanatory power by the locality of the probability distribution and the narrow scope of information used by the model. We show that a simple computational model (a bigram part-of-speech tagger based on the design used by Corley and Crocker (2000)) makes correct predictions on processing difﬁculty observed in a wide range of empirical sentence processing data. We use two modes of evaluation: one that relies on comparison with a control sentence, paralleling practice in human studies; another that measures probability drop in the disambiguating region of the sentence. Both are surprisingly good indicators of the processing difﬁculty of garden-path sentences. The sentences tested are drawn from published sources and systematically explore ﬁve different types of ambiguity: previous studies have been narrower in scope and smaller in scale. We do not deny the limitations of ﬁnite-state models, but argue that our results show that their usefulness has been underestimated. 
We propose in this paper a method for quantifying sentence grammaticality. The approach based on Property Grammars, a constraint-based syntactic formalism, makes it possible to evaluate a grammaticality index for any kind of sentence, including ill-formed ones. We compare on a sample of sentences the grammaticality indices obtained from PG formalism and the acceptability judgements measured by means of a psycholinguistic analysis. The results show that the derived grammaticality index is a fairly good tracer of acceptability scores. 
In this paper we present a novel approach for inducing word alignments from sentence aligned data. We use a Conditional Random Field (CRF), a discriminative model, which is estimated on a small supervised training set. The CRF is conditioned on both the source and target texts, and thus allows for the use of arbitrary and overlapping features over these data. Moreover, the CRF has efﬁcient training and decoding processes which both ﬁnd globally optimal solutions. We apply this alignment model to both French-English and Romanian-English language pairs. We show how a large number of highly predictive features can be easily incorporated into the CRF, and demonstrate that even with only a few hundred word-aligned training sentences, our model improves over the current state-ofthe-art with alignment error rates of 5.29 and 25.8 for the two tasks respectively. 
In this paper we investigate ChineseEnglish name transliteration using comparable corpora, corpora where texts in the two languages deal in some of the same topics — and therefore share references to named entities — but are not translations of each other. We present two distinct methods for transliteration, one approach using phonetic transliteration, and the second using the temporal distribution of candidate pairs. Each of these approaches works quite well, but by combining the approaches one can achieve even better results. We then propose a novel score propagation method that utilizes the co-occurrence of transliteration pairs within document pairs. This propagation method achieves further improvement over the best results from the previous step. 
We present a novel method for extracting parallel sub-sentential fragments from comparable, non-parallel bilingual corpora. By analyzing potentially similar sentence pairs using a signal processinginspired approach, we detect which segments of the source sentence are translated into segments in the target sentence, and which are not. This method enables us to extract useful machine translation training data even from very non-parallel corpora, which contain no parallel sentence pairs. We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system. 
Instances of a word drawn from different domains may have different sense priors (the proportions of the different senses of a word). This in turn affects the accuracy of word sense disambiguation (WSD) systems trained and applied on different domains. This paper presents a method to estimate the sense priors of words drawn from a new domain, and highlights the importance of using well calibrated probabilities when performing these estimations. By using well calibrated probabilities, we are able to estimate the sense priors effectively to achieve signiﬁcant improvements in WSD accuracy. 
Combination methods are an effective way of improving system performance. This paper examines the beneﬁts of system combination for unsupervised WSD. We investigate several voting- and arbiterbased combination strategies over a diverse pool of unsupervised WSD systems. Our combination methods rely on predominant senses which are derived automatically from raw text. Experiments using the SemCor and Senseval-3 data sets demonstrate that our ensembles yield signiﬁcantly better results when compared with state-of-the-art. 
Fine-grained sense distinctions are one of the major obstacles to successful Word Sense Disambiguation. In this paper, we present a method for reducing the granularity of the WordNet sense inventory based on the mapping to a manually crafted dictionary encoding sense hierarchies, namely the Oxford Dictionary of English. We assess the quality of the mapping and the induced clustering, and evaluate the performance of coarse WSD systems in the Senseval-3 English all-words task. 
In this paper, we present Espresso, a weakly-supervised, general-purpose, and accurate algorithm for harvesting semantic relations. The main contributions are: i) a method for exploiting generic patterns by filtering incorrect instances using the Web; and ii) a principled measure of pattern and instance reliability enabling the filtering algorithm. We present an empirical comparison of Espresso with various state of the art systems, on different size and genre corpora, on extracting various general and specific relations. Experimental results show that our exploitation of generic patterns substantially increases system recall with small effect on overall precision. 
This paper proposes a novel hierarchical learning strategy to deal with the data sparseness problem in relation extraction by modeling the commonality among related classes. For each class in the hierarchy either manually predefined or automatically clustered, a linear discriminative function is determined in a topdown way using a perceptron algorithm with the lower-level weight vector derived from the upper-level weight vector. As the upper-level class normally has much more positive training examples than the lower-level class, the corresponding linear discriminative function can be determined more reliably. The upperlevel discriminative function then can effectively guide the discriminative function learning in the lower-level, which otherwise might suffer from limited training data. Evaluation on the ACE RDC 2003 corpus shows that the hierarchical strategy much improves the performance by 5.6 and 5.1 in F-measure on least- and medium- frequent relations respectively. It also shows that our system outperforms the previous best-reported system by 2.7 in F-measure on the 24 subtypes using the same feature set. 
Shortage of manually labeled data is an obstacle to supervised relation extraction methods. In this paper we investigate a graph based semi-supervised learning algorithm, a label propagation (LP) algorithm, for relation extraction. It represents labeled and unlabeled examples and their distances as the nodes and the weights of edges of a graph, and tries to obtain a labeling function to satisfy two constraints: 1) it should be ﬁxed on the labeled nodes, 2) it should be smooth on the whole graph. Experiment results on the ACE corpus showed that this LP algorithm achieves better performance than SVM when only very few labeled examples are available, and it also performs better than bootstrapping for the relation extraction task. 
This paper proposes a generic mathematical formalism for the combination of various structures: strings, trees, dags, graphs and products of them. The polarization of the objects of the elementary structures controls the saturation of the final structure. This formalism is both elementary and powerful enough to strongly simulate many grammar formalisms, such as rewriting systems, dependency grammars, TAG, HPSG and LFG. 
This work provides the essential foundations for modular construction of (typed) uniﬁcation grammars for natural languages. Much of the information in such grammars is encoded in the signature, and hence the key is facilitating a modularized development of type signatures. We introduce a deﬁnition of signature modules and show how two modules combine. Our definitions are motivated by the actual needs of grammar developers obtained through a careful examination of large scale grammars. We show that our deﬁnitions meet these needs by conforming to a detailed set of desiderata. 
This paper investigates the use of sublexical units as a solution to handling the complex morphology with productive derivational processes, in the development of a lexical functional grammar for Turkish. Such sublexical units make it possible to expose the internal structure of words with multiple derivations to the grammar rules in a uniform manner. This in turn leads to more succinct and manageable rules. Further, the semantics of the derivations can also be systematically reﬂected in a compositional way by constructing PRED values on the ﬂy. We illustrate how we use sublexical units for handling simple productive derivational morphology and more interesting cases such as causativization, etc., which change verb valency. Our priority is to handle several linguistic phenomena in order to observe the effects of our approach on both the c-structure and the f-structure representation, and grammar writing, leaving the coverage and evaluation issues aside for the moment. 
A grammatical method of combining two kinds of speech repair cues is presented. One cue, prosodic disjuncture, is detected by a decision tree-based ensemble classiﬁer that uses acoustic cues to identify where normal prosody seems to be interrupted (Lickley, 1996). The other cue, syntactic parallelism, codiﬁes the expectation that repairs continue a syntactic category that was left unﬁnished in the reparandum (Levelt, 1983). The two cues are combined in a Treebank PCFG whose states are split using a few simple tree transformations. Parsing performance on the Switchboard and Fisher corpora suggests that these two cues help to locate speech repairs in a synergistic way. 
Spoken monologues feature greater sentence length and structural complexity than do spoken dialogues. To achieve high parsing performance for spoken monologues, it could prove effective to simplify the structure by dividing a sentence into suitable language units. This paper proposes a method for dependency parsing of Japanese monologues based on sentence segmentation. In this method, the dependency parsing is executed in two stages: at the clause level and the sentence level. First, the dependencies within a clause are identiﬁed by dividing a sentence into clauses and executing stochastic dependency parsing for each clause. Next, the dependencies over clause boundaries are identiﬁed stochastically, and the dependency structure of the entire sentence is thus completed. An experiment using a spoken monologue corpus shows this method to be effective for efﬁcient dependency parsing of Japanese monologue sentences. 
This paper describes a parser which generates parse trees with empty elements in which traces and ﬁllers are co-indexed. The parser is an unlexicalized PCFG parser which is guaranteed to return the most probable parse. The grammar is extracted from a version of the PENN treebank which was automatically annotated with features in the style of Klein and Manning (2003). The annotation includes GPSG-style slash features which link traces and ﬁllers, and other features which improve the general parsing accuracy. In an evaluation on the PENN treebank (Marcus et al., 1993), the parser outperformed other unlexicalized PCFG parsers in terms of labeled bracketing fscore. Its results for the empty category prediction task and the trace-ﬁller coindexation task exceed all previously reported results with 84.1% and 77.4% fscore, respectively. 
We explore the use of restricted dialogue contexts in reinforcement learning (RL) of effective dialogue strategies for information seeking spoken dialogue systems (e.g. COMMUNICATOR (Walker et al., 2001)). The contexts we use are richer than previous research in this area, e.g. (Levin and Pieraccini, 1997; Schefﬂer and Young, 2001; Singh et al., 2002; Pietquin, 2004), which use only slot-based information, but are much less complex than the full dialogue “Information States” explored in (Henderson et al., 2005), for which tractabe learning is an issue. We explore how incrementally adding richer features allows learning of more effective dialogue strategies. We use 2 user simulations learned from COMMUNICATOR data (Walker et al., 2001; Georgila et al., 2005b) to explore the effects of different features on learned dialogue strategies. Our results show that adding the dialogue moves of the last system and user turns increases the average reward of the automatically learned strategies by 65.9% over the original (hand-coded) COMMUNICATOR systems, and by 7.8% over a baseline RL policy that uses only slot-status features. We show that the learned strategies exhibit an emergent “focus switching” strategy and effective use of the ‘give help’ action. 
Speech recognition problems are a reality in current spoken dialogue systems. In order to better understand these phenomena, we study dependencies between speech recognition problems and several higher level dialogue factors that define our notion of student state: frustration/anger, certainty and correctness. We apply Chi Square (χ2) analysis to a corpus of speech-based computer tutoring dialogues to discover these dependencies both within and across turns. Significant dependencies are combined to produce interesting insights regarding speech recognition problems and to propose new strategies for handling these problems. We also find that tutoring, as a new domain for speech applications, exhibits interesting tradeoffs and new factors to consider for spoken dialogue design. 
Data-driven techniques have been used for many computational linguistics tasks. Models derived from data are generally more robust than hand-crafted systems since they better reﬂect the distribution of the phenomena being modeled. With the availability of large corpora of spoken dialog, dialog management is now reaping the beneﬁts of data-driven techniques. In this paper, we compare two approaches to modeling subtask structure in dialog: a chunk-based model of subdialog sequences, and a parse-based, or hierarchical, model. We evaluate these models using customer agent dialogs from a catalog service domain. 
We present a new semi-supervised training procedure for conditional random ﬁelds (CRFs) that can be used to train sequence segmentors and labelers from a combination of labeled and unlabeled training data. Our approach is based on extending the minimum entropy regularization framework to the structured prediction case, yielding a training objective that combines unlabeled conditional entropy with labeled conditional likelihood. Although the training objective is no longer concave, it can still be used to improve an initial model (e.g. obtained from supervised training) by iterative ascent. We apply our new training algorithm to the problem of identifying gene and protein mentions in biological texts, and show that incorporating unlabeled data improves the performance of the supervised CRF in this case. 
This paper proposes a framework for training Conditional Random Fields (CRFs) to optimize multivariate evaluation measures, including non-linear measures such as F-score. Our proposed framework is derived from an error minimization approach that provides a simple solution for directly optimizing any evaluation measure. Speciﬁcally focusing on sequential segmentation tasks, i.e. text chunking and named entity recognition, we introduce a loss function that closely reﬂects the target evaluation measure for these tasks, namely, segmentation F-score. Our experiments show that our method performs better than standard CRF training. 
Lasso is a regularization method for parameter estimation in linear models. It optimizes the model parameters with respect to a loss function subject to model complexities. This paper explores the use of lasso for statistical language modeling for text input. Owing to the very large number of parameters, directly optimizing the penalized lasso loss function is impossible. Therefore, we investigate two approximation methods, the boosted lasso (BLasso) and the forward stagewise linear regression (FSLR). Both methods, when used with the exponential loss function, bear strong resemblance to the boosting algorithm which has been used as a discriminative training method for language modeling. Evaluations on the task of Japanese text input show that BLasso is able to produce the best approximation to the lasso solution, and leads to a significant improvement, in terms of character error rate, over boosting and the traditional maximum likelihood estimation. 
We have developed an automated Japanese essay scoring system called Jess. The system needs expert writings rather than expert raters to build the evaluation model. By detecting statistical outliers of predetermined aimed essay features compared with many professional writings for each prompt, our system can evaluate essays. The following three features are examined: (1) rhetoric — syntactic variety, or the use of various structures in the arrangement of phases, clauses, and sentences, (2) organization — characteristics associated with the orderly presentation of ideas, such as rhetorical features and linguistic cues, and (3) content — vocabulary related to the topic, such as relevant information and precise or specialized vocabulary. The ﬁnal evaluation score is calculated by deducting from a perfect score assigned by a learning process using editorials and columns from the Mainichi Daily News newspaper. A diagnosis for the essay is also given. 
This paper proposes a method for detecting errors in article usage and singular plural usage based on the mass count distinction. First, it learns decision lists from training data generated automatically to distinguish mass and count nouns. Then, in order to improve its performance, it is augmented by feedback that is obtained from the writing of learners. Finally, it detects errors by applying rules to the mass count distinction. Experiments show that it achieves a recall of 0.71 and a precision of 0.72 and outperforms other methods used for comparison when augmented by feedback. 
This paper presents a pilot study of the use of phrasal Statistical Machine Translation (SMT) techniques to identify and correct writing errors made by learners of English as a Second Language (ESL). Using examples of mass noun errors found in the Chinese Learner Error Corpus (CLEC) to guide creation of an engineered training set, we show that application of the SMT paradigm can capture errors not well addressed by widely-used proofing tools designed for native speakers. Our system was able to correct 61.81% of mistakes in a set of naturallyoccurring examples of mass noun errors found on the World Wide Web, suggesting that efforts to collect alignable corpora of pre- and post-editing ESL writing samples offer can enable the development of SMT-based writing assistance tools capable of repairing many of the complex syntactic and lexical problems found in the writing of ESL learners. 
Transforming syntactic representations in order to improve parsing accuracy has been exploited successfully in statistical parsing systems using constituency-based representations. In this paper, we show that similar transformations can give substantial improvements also in data-driven dependency parsing. Experiments on the Prague Dependency Treebank show that systematic transformations of coordinate structures and verb groups result in a 10% error reduction for a deterministic data-driven dependency parser. Combining these transformations with previously proposed techniques for recovering nonprojective dependencies leads to state-ofthe-art accuracy for the given data set. 
Spoken language generation for dialogue systems requires a dictionary of mappings between semantic representations of concepts the system wants to express and realizations of those concepts. Dictionary creation is a costly process; it is currently done by hand for each dialogue domain. We propose a novel unsupervised method for learning such mappings from user reviews in the target domain, and test it on restaurant reviews. We test the hypothesis that user reviews that provide individual ratings for distinguished attributes of the domain entity make it possible to map review sentences to their semantic representation with high precision. Experimental analyses show that the mappings learned cover most of the domain ontology, and provide good linguistic variation. A subjective user evaluation shows that the consistency between the semantic representations and the learned realizations is high and that the naturalness of the realizations is higher than a hand-crafted baseline. 
This paper presents a method for building genetic language taxonomies based on a new approach to comparing lexical forms. Instead of comparing forms cross-linguistically, a matrix of languageinternal similarities between forms is calculated. These matrices are then compared to give distances between languages. We argue that this coheres better with current thinking in linguistics and psycholinguistics. An implementation of this approach, called PHILOLOGICON, is described, along with its application to Dyen et al.’s (1992) ninety-ﬁve wordlists from Indo-European languages. 
A good dictionary contains not only many entries and a lot of information concerning each one of them, but also adequate means to reveal the stored information. Information access depends crucially on the quality of the index. We will present here some ideas of how a dictionary could be enhanced to support a speaker/writer to find the word s/he is looking for. To this end we suggest to add to an existing electronic resource an index based on the notion of association. We will also present preliminary work of how a subset of such associations, for example, topical associations, can be acquired by filtering a network of lexical co-occurrences extracted from a corpus. 
We investigate the utility of supertag information for guiding an existing dependency parser of German. Using weighted constraints to integrate the additionally available information, the decision process of the parser is inﬂuenced by changing its preferences, without excluding alternative structural interpretations from being considered. The paper reports on a series of experiments using varying models of supertags that signiﬁcantly increase the parsing accuracy. In addition, an upper bound on the accuracy that can be achieved with perfect supertags is estimated. 
We present a novel approach for discovering word categories, sets of words sharing a signiﬁcant aspect of their meaning. We utilize meta-patterns of highfrequency words and content words in order to discover pattern candidates. Symmetric patterns are then identiﬁed using graph-based measures, and word categories are created based on graph clique sets. Our method is the ﬁrst pattern-based method that requires no corpus annotation or manually provided seed patterns or words. We evaluate our algorithm on very large corpora in two languages, using both human judgments and WordNetbased evaluation. Our fully unsupervised results are superior to previous work that used a POS tagged corpus, and computation time for huge corpora are orders of magnitude faster than previously reported. 
We present BAYESUM (for “Bayesian summarization”), a model for sentence extraction in query-focused summarization. BAYESUM leverages the common case in which multiple documents are relevant to a single query. Using these documents as reinforcement for query terms, BAYESUM is not afﬂicted by the paucity of information in short queries. We show that approximate inference in BAYESUM is possible on large data sets and results in a stateof-the-art summarization system. Furthermore, we show how BAYESUM can be understood as a justiﬁed query expansion technique in the language modeling for IR framework. 
We present an unsupervised learning algorithm that mines large text corpora for patterns that express implicit semantic relations. For a given input word pair X :Y with some unspecified semantic relations, the corresponding output list of patterns P1, , Pm is ranked according to how well each pattern Pi expresses the relations between X and Y . For example, given X = ostrich and Y = bird , the two highest ranking output patterns are “X is the largest Y” and “Y such as the X”. The output patterns are intended to be useful for finding further pairs with the same relations, to support the construction of lexicons, ontologies, and semantic networks. The patterns are sorted by pertinence, where the pertinence of a pattern Pi for a word pair X :Y is the expected relational similarity between the given pair and typical pairs for Pi . The algorithm is empirically evaluated on two tasks, solving multiple-choice SAT word analogy questions and classifying semantic relations in noun-modifier pairs. On both tasks, the algorithm achieves stateof-the-art results, performing significantly better than several alternative pattern ranking algorithms, based on tf-idf. 
In this paper we investigate the beneﬁt of stochastic predictor components for the parsing quality which can be obtained with a rule-based dependency grammar. By including a chunker, a supertagger, a PP attacher, and a fast probabilistic parser we were able to improve upon the baseline by 3.2%, bringing the overall labelled accuracy to 91.1% on the German NEGRA corpus. We attribute the successful integration to the ability of the underlying grammar model to combine uncertain evidence in a soft manner, thus avoiding the problem of error propagation. 
We introduce an error mining technique for automatically detecting errors in resources that are used in parsing systems. We applied this technique on parsing results produced on several million words by two distinct parsing systems, which share the syntactic lexicon and the pre-parsing processing chain. We were thus able to identify missing and erroneous information in these resources. 
Statistical parsers trained and tested on the Penn Wall Street Journal (WSJ) treebank have shown vast improvements over the last 10 years. Much of this improvement, however, is based upon an ever-increasing number of features to be trained on (typically) the WSJ treebank data. This has led to concern that such parsers may be too ﬁnely tuned to this corpus at the expense of portability to other genres. Such worries have merit. The standard “Charniak parser” checks in at a labeled precisionrecall f -measure of 89.7% on the Penn WSJ test set, but only 82.9% on the test set from the Brown treebank corpus. This paper should allay these fears. In particular, we show that the reranking parser described in Charniak and Johnson (2005) improves performance of the parser on Brown to 85.2%. Furthermore, use of the self-training techniques described in (McClosky et al., 2006) raise this to 87.8% (an error reduction of 28%) again without any use of labeled Brown data. This is remarkable since training the parser and reranker on labeled Brown data achieves only 88.4%. 
Lexical classes, when tailored to the application and domain in question, can provide an effective means to deal with a number of natural language processing (NLP) tasks. While manual construction of such classes is difﬁcult, recent research shows that it is possible to automatically induce verb classes from cross-domain corpora with promising accuracy. We report a novel experiment where similar technology is applied to the important, challenging domain of biomedicine. We show that the resulting classiﬁcation, acquired from a corpus of biomedical journal articles, is highly accurate and strongly domainspeciﬁc. It can be used to aid BIO-NLP directly or as useful material for investigating the syntax and semantics of verbs in biomedical texts. 
Various methods have been proposed for automatic synonym acquisition, as synonyms are one of the most fundamental lexical knowledge. Whereas many methods are based on contextual clues of words, little attention has been paid to what kind of categories of contextual information are useful for the purpose. This study has experimentally investigated the impact of contextual information selection, by extracting three kinds of word relationships from corpora: dependency, sentence co-occurrence, and proximity. The evaluation result shows that while dependency and proximity perform relatively well by themselves, combination of two or more kinds of contextual information gives more stable performance. We’ve further investigated useful selection of dependency relations and modiﬁcation categories, and it is found that modiﬁcation has the greatest contribution, even greater than the widely adopted subjectobject combination. 
Accurately representing synonymy using distributional similarity requires large volumes of data to reliably represent infrequent words. However, the na¨ıve nearestneighbour approach to comparing context vectors extracted from large corpora scales poorly (O(n2) in the vocabulary size). In this paper, we compare several existing approaches to approximating the nearestneighbour search for distributional similarity. We investigate the trade-off between efﬁciency and accuracy, and ﬁnd that SASH (Houle and Sakuma, 2005) provides the best balance. 
Sentence compression is the task of producing a summary at the sentence level. This paper focuses on three aspects of this task which have not received detailed treatment in the literature: training requirements, scalability, and automatic evaluation. We provide a novel comparison between a supervised constituentbased and an weakly supervised wordbased compression algorithm and examine how these models port to different domains (written vs. spoken text). To achieve this, a human-authored compression corpus has been created and our study highlights potential problems with the automatically gathered compression corpora currently used. Finally, we assess whether automatic evaluation measures can be used to determine compression quality. 
Ordering information is a difﬁcult but important task for applications generating natural-language text. We present a bottom-up approach to arranging sentences extracted for multi-document summarization. To capture the association and order of two textual segments (eg, sentences), we deﬁne four criteria, chronology, topical-closeness, precedence, and succession. These criteria are integrated into a criterion by a supervised learning approach. We repeatedly concatenate two textual segments into one segment based on the criterion until we obtain the overall segment with all sentences arranged. Our experimental results show a signiﬁcant improvement over existing sentence ordering strategies. 
We have constructed a corpus of news articles in which events are annotated for estimated bounds on their duration. Here we describe a method for measuring inter-annotator agreement for these event duration distributions. We then show that machine learning techniques applied to this data yield coarse-grained event duration information, considerably outperforming a baseline and approaching human performance. 
In this paper we deﬁne a novel similarity measure between examples of textual entailments and we use it as a kernel function in Support Vector Machines (SVMs). This allows us to automatically learn the rewrite rules that describe a non trivial set of entailment cases. The experiments with the data sets of the RTE 2005 challenge show an improvement of 4.4% over the state-of-the-art methods. 
We present an efﬁcient algorithm for the redundancy elimination problem: Given an underspeciﬁed semantic representation (USR) of a scope ambiguity, compute an USR with fewer mutually equivalent readings. The algorithm operates on underspeciﬁed chart representations which are derived from dominance graphs; it can be applied to the USRs computed by large-scale grammars. We evaluate the algorithm on a corpus, and show that it reduces the degree of ambiguity signiﬁcantly while taking negligible runtime. 
The psycholinguistic literature provides evidence for syntactic priming, i.e., the tendency to repeat structures. This paper describes a method for incorporating priming into an incremental probabilistic parser. Three models are compared, which involve priming of rules between sentences, within sentences, and within coordinate structures. These models simulate the reading time advantage for parallel structures found in human data, and also yield a small increase in overall parsing accuracy. 
We present a novel classiﬁer-based deterministic parser for Chinese constituency parsing. Our parser computes parse trees from bottom up in one pass, and uses classiﬁers to make shift-reduce decisions. Trained and evaluated on the standard training and test sets, our best model (using stacked classiﬁers) runs in linear time and has labeled precision and recall above 88% using gold-standard part-of-speech tags, surpassing the best published results. Our SVM parser is 2-13 times faster than state-of-the-art parsers, while producing more accurate results. Our Maxent and DTree parsers run at speeds 40-270 times faster than state-of-the-art parsers, but with 5-6% losses in accuracy. 
We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank. Starting with a simple Xbar grammar, we learn a new grammar whose nonterminals are subsymbols of the original nonterminals. In contrast with previous work, we are able to split various terminals to different degrees, as appropriate to the actual complexity in the data. Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation. On the other hand, our grammars are much more compact and substantially more accurate than previous work on automatic annotation. Despite its simplicity, our best grammar achieves an F1 of 90.2% on the Penn Treebank, higher than fully lexicalized systems. 
Partial cognates are pairs of words in two languages that have the same meaning in some, but not all contexts. Detecting the actual meaning of a partial cognate in context can be useful for Machine Translation tools and for Computer-Assisted Language Learning tools. In this paper we propose a supervised and a semisupervised method to disambiguate partial cognates between two languages: French and English. The methods use only automatically-labeled data; therefore they can be applied for other pairs of languages as well. We also show that our methods perform well when using corpora from different domains. 
This paper investigates conceptually and empirically the novel sense matching task, which requires to recognize whether the senses of two synonymous words match in context. We suggest direct approaches to the problem, which avoid the intermediate step of explicit word sense disambiguation, and demonstrate their appealing advantages and stimulating potential for future research. 
This paper presents a new approach based on Equivalent Pseudowords (EPs) to tackle Word Sense Disambiguation (WSD) in Chinese language. EPs are particular artificial ambiguous words, which can be used to realize unsupervised WSD. A Bayesian classifier is implemented to test the efficacy of the EP solution on Senseval-3 Chinese test set. The performance is better than state-of-the-art results with an average F-measure of 0.80. The experiment verifies the value of EP for unsupervised WSD. 
As natural language understanding research advances towards deeper knowledge modeling, the tasks become more and more complex: we are interested in more nuanced word characteristics, more linguistic properties, deeper semantic and syntactic features. One such example, explored in this article, is the mention detection and recognition task in the Automatic Content Extraction project, with the goal of identifying named, nominal or pronominal references to real-world entities—mentions— and labeling them with three types of information: entity type, entity subtype and mention type. In this article, we investigate three methods of assigning these related tags and compare them on several data sets. A system based on the methods presented in this article participated and ranked very competitively in the ACE’04 evaluation. 
Hidden Markov models (HMMs) are powerful statistical models that have found successful applications in Information Extraction (IE). In current approaches to applying HMMs to IE, an HMM is used to model text at the document level. This modelling might cause undesired redundancy in extraction in the sense that more than one ﬁller is identiﬁed and extracted. We propose to use HMMs to model text at the segment level, in which the extraction process consists of two steps: a segment retrieval step followed by an extraction step. In order to retrieve extractionrelevant segments from documents, we introduce a method to use HMMs to model and retrieve segments. Our experimental results show that the resulting segment HMM IE system not only achieves near zero extraction redundancy, but also has better overall extraction performance than traditional document HMM IE systems. 
This paper presents a new web mining scheme for parallel data acquisition. Based on the Document Object Model (DOM), a web page is represented as a DOM tree. Then a DOM tree alignment model is proposed to identify the translationally equivalent texts and hyperlinks between two parallel DOM trees. By tracing the identified parallel hyperlinks, parallel web documents are recursively mined. Compared with previous mining schemes, the benchmarks show that this new mining scheme improves the mining coverage, reduces mining bandwidth, and enhances the quality of mined parallel sentences. 
This paper describes the development of QuestionBank, a corpus of 4000 parseannotated questions for (i) use in training parsers employed in QA, and (ii) evaluation of question parsing. We present a series of experiments to investigate the effectiveness of QuestionBank as both an exclusive and supplementary training resource for a state-of-the-art parser in parsing both question and non-question test sets. We introduce a new method for recovering empty nodes and their antecedents (capturing long distance dependencies) from parser output in CFG trees using LFG f-structure reentrancies. Our main ﬁndings are (i) using QuestionBank training data improves parser performance to 89.75% labelled bracketing f-score, an increase of almost 11% over the baseline; (ii) back-testing experiments on nonquestion data (Penn-II WSJ Section 23) shows that the retrained parser does not suffer a performance drop on non-question material; (iii) ablation experiments show that the size of training material provided by QuestionBank is sufﬁcient to achieve optimal results; (iv) our method for recovering empty nodes captures long distance dependencies in questions from the ATIS corpus with high precision (96.82%) and low recall (39.38%). In summary, QuestionBank provides a useful new resource in parser-based QA research. 
We present an algorithm which creates a German CCGbank by translating the syntax graphs in the German Tiger corpus into CCG derivation trees. The resulting corpus contains 46,628 derivations, covering 95% of all complete sentences in Tiger. Lexicons extracted from this corpus contain correct lexical entries for 94% of all known tokens in unseen text. 
For many years, statistical machine translation relied on generative models to provide bilingual word alignments. In 2005, several independent efforts showed that discriminative models could be used to enhance or replace the standard generative approach. Building on this work, we demonstrate substantial improvement in word-alignment accuracy, partly though improved training methods, but predominantly through selection of more and better features. Our best model produces the lowest alignment error rate yet reported on Canadian Hansards bilingual data. 
We propose a novel reordering model for phrase-based statistical machine translation (SMT) that uses a maximum entropy (MaxEnt) model to predicate reorderings of neighbor blocks (phrase pairs). The model provides content-dependent, hierarchical phrasal reordering with generalization based on features automatically learned from a real-world bitext. We present an algorithm to extract all reordering events of neighbor blocks from bilingual data. In our experiments on Chineseto-English translation, this MaxEnt-based reordering model obtains signiﬁcant improvements in BLEU score on the NIST MT-05 and IWSLT-04 tasks. 
In this paper, we argue that n-gram language models are not sufﬁcient to address word reordering required for Machine Translation. We propose a new distortion model that can be used with existing phrase-based SMT decoders to address those n-gram language model limitations. We present empirical results in Arabic to English Machine Translation that show statistically signiﬁcant improvements when our proposed model is used. We also propose a novel metric to measure word order similarity (or difference) between any pair of languages based on word alignments. 
This paper presents a study on if and how automatically extracted keywords can be used to improve text categorization. In summary we show that a higher performance — as measured by micro-averaged F-measure on a standard text categorization collection — is achieved when the full-text representation is combined with the automatically extracted keywords. The combination is obtained by giving higher weights to words in the full-texts that are also extracted as keywords. We also present results for experiments in which the keywords are the only input to the categorizer, either represented as unigrams or intact. Of these two experiments, the unigrams have the best performance, although neither performs as well as headlines only. 
Words and character-bigrams are both used as features in Chinese text processing tasks, but no systematic comparison or analysis of their values as features for Chinese text categorization has been reported heretofore. We carry out here a full performance comparison between them by experiments on various document collections (including a manually word-segmented corpus as a golden standard), and a semi-quantitative analysis to elucidate the characteristics of their behavior; and try to provide some preliminary clue for feature term choice (in most cases, character-bigrams are better than words) and dimensionality setting in text categorization systems. 
Cross-language Text Categorization is the task of assigning semantic classes to documents written in a target language (e.g. English) while the system is trained using labeled documents in a source language (e.g. Italian). In this work we present many solutions according to the availability of bilingual resources, and we show that it is possible to deal with the problem even when no such resources are accessible. The core technique relies on the automatic acquisition of Multilingual Domain Models from comparable corpora. Experiments show the effectiveness of our approach, providing a low cost solution for the Cross Language Text Categorization task. In particular, when bilingual dictionaries are available the performance of the categorization gets close to that of monolingual text categorization. 
Recent developments in statistical modeling of various linguistic phenomena have shown that additional features give consistent performance improvements. Quite often, improvements are limited by the number of features a system is able to explore. This paper describes a novel progressive training algorithm that selects features from virtually unlimited feature spaces for conditional maximum entropy (CME) modeling. Experimental results in edit region identification demonstrate the benefits of the progressive feature selection (PFS) algorithm: the PFS algorithm maintains the same accuracy performance as previous CME feature selection algorithms (e.g., Zhou et al., 2003) when the same feature spaces are used. When additional features and their combinations are used, the PFS gives 17.66% relative improvement over the previously reported best result in edit region identification on Switchboard corpus (Kahn et al., 2005), which leads to a 20% relative error reduction in parsing the Switchboard corpus when gold edits are used as the upper bound. 
We ﬁrst show how a structural locality bias can improve the accuracy of state-of-the-art dependency grammar induction models trained by EM from unannotated examples (Klein and Manning, 2004). Next, by annealing the free parameter that controls this bias, we achieve further improvements. We then describe an alternative kind of structural bias, toward “broken” hypotheses consisting of partial structures over segmented sentences, and show a similar pattern of improvement. We relate this approach to contrastive estimation (Smith and Eisner, 2005a), apply the latter to grammar induction in six languages, and show that our new approach improves accuracy by 1–17% (absolute) over CE (and 8–30% over EM), achieving to our knowledge the best results on this task to date. Our method, structural annealing, is a general technique with broad applicability to hidden-structure discovery problems. 
Short vowels and other diacritics are not part of written Arabic scripts. Exceptions are made for important political and religious texts and in scripts for beginning students of Arabic. Script without diacritics have considerable ambiguity because many words with diﬀerent diacritic patterns appear identical in a diacritic-less setting. We propose in this paper a maximum entropy approach for restoring diacritics in a document. The approach can easily integrate and make eﬀective use of diverse types of information; the model we propose integrates a wide array of lexical, segmentbased and part-of-speech tag features. The combination of these feature types leads to a state-of-the-art diacritization model. Using a publicly available corpus (LDC’s Arabic Treebank Part 3), we achieve a diacritic error rate of 5.1%, a segment error rate 8.5%, and a word error rate of 17.3%. In case-ending-less setting, we obtain a diacritic error rate of 2.2%, a segment error rate 4.0%, and a word error rate of 7.2%. 
This paper explores the relationship between the translation quality and the retrieval effectiveness in Machine Translation (MT) based Cross-Language Information Retrieval (CLIR). To obtain MT systems of different translation quality, we degrade a rule-based MT system by decreasing the size of the rule base and the size of the dictionary. We use the degraded MT systems to translate queries and submit the translated queries of varying quality to the IR system. Retrieval effectiveness is found to correlate highly with the translation quality of the queries. We further analyze the factors that affect the retrieval effectiveness. Title queries are found to be preferred in MT-based CLIR. In addition, dictionary-based degradation is shown to have stronger impact than rule-based degradation in MT-based CLIR. 
The trend in information retrieval systems is from document to sub-document retrieval, such as sentences in a summarization system and words or phrases in question-answering system. Despite this trend, systems continue to model language at a document level using the inverse document frequency (IDF). In this paper, we compare and contrast IDF with inverse sentence frequency (ISF) and inverse term frequency (ITF). A direct comparison reveals that all language models are highly correlated; however, the average ISF and ITF values are 5.5 and 10.4 higher than IDF. All language models appeared to follow a power law distribution with a slope coefficient of 1.6 for documents and 1.7 for sentences and terms. We conclude with an analysis of IDF stability with respect to random, journal, and section partitions of the 100,830 full-text scientific articles in our experimental corpus. 
We present a novel translation model based on tree-to-string alignment template (TAT) which describes the alignment between a source parse tree and a target string. A TAT is capable of generating both terminals and non-terminals and performing reordering at both low and high levels. The model is linguistically syntaxbased because TATs are extracted automatically from word-aligned, source side parsed parallel texts. To translate a source sentence, we ﬁrst employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string. Our experiments show that the TAT-based model signiﬁcantly outperforms Pharaoh, a state-of-the-art decoder for phrase-based models. 
This paper proposes a named entity recognition (NER) method for speech recognition results that uses conﬁdence on automatic speech recognition (ASR) as a feature. The ASR conﬁdence feature indicates whether each word has been correctly recognized. The NER model is trained using ASR results with named entity (NE) labels as well as the corresponding transcriptions with NE labels. In experiments using support vector machines (SVMs) and speech data from Japanese newspaper articles, the proposed method outperformed a simple application of textbased NER to ASR results in NER Fmeasure by improving precision. These results show that the proposed method is effective in NER for noisy inputs. 
We approach the zero-anaphora resolution problem by decomposing it into intra-sentential and inter-sentential zeroanaphora resolution. For the former problem, syntactic patterns of the appearance of zero-pronouns and their antecedents are useful clues. Taking Japanese as a target language, we empirically demonstrate that incorporating rich syntactic pattern features in a state-of-the-art learning-based anaphora resolution model dramatically improves the accuracy of intra-sentential zero-anaphora, which consequently improves the overall performance of zeroanaphora resolution. 
An automatic word spacing is one of the important tasks in Korean language processing and information retrieval. Since there are a number of confusing cases in word spacing of Korean, there are some mistakes in many texts including news articles. This paper presents a high-accurate method for automatic word spacing based on self-organizing Ò-gram model. This method is basically a variant of Ò-gram model, but achieves high accuracy by automatically adapting context size. In order to ﬁnd the optimal context size, the proposed method automatically increases the context size when the contextual distribution after increasing it dose not agree with that of the current context. It also decreases the context size when the distribution of reduced context is similar to that of the current context. This approach achieves high accuracy by considering higher dimensional data in case of necessity, and the increased computational cost are compensated by the reduced context size. The experimental results show that the self-organizing struc- ture of Ò-gram model enhances the basic model. 
 Due to the historical and cultural reasons, English phases, especially the proper nouns and new words, frequently appear in Web pages written primarily in Asian languages such as Chinese and Korean. Although these English terms and their equivalences in the Asian languages refer to the same concept, they are erroneously treated as independent index units in traditional Information Retrieval (IR). This paper describes the degree to which the problem arises in IR and suggests a novel technique to solve it. Our method firstly extracts an English phrase from Asian language Web pages, and then unifies the extracted phrase and its equivalence(s) in the language as one index unit. Experimental results show that the high precision of our conceptual unification approach greatly improves the IR performance. 
Word alignment using recency-vector based approach has recently become popular. One major advantage of these techniques is that unlike other approaches they perform well even if the size of the parallel corpora is small. This makes these algorithms worth-studying for languages where resources are scarce. In this work we studied the performance of two very popular recency-vector based approaches, proposed in (Fung and McKeown, 1994) and (Somers, 1998), respectively, for word alignment in English-Hindi parallel corpus. But performance of the above algorithms was not found to be satisfactory. However, subsequent addition of some new constraints improved the performance of the recency-vector based alignment technique signiﬁcantly for the said corpus. The present paper discusses the new version of the algorithm and its performance in detail. 
This paper proposes methods for extracting loanwords from Cyrillic Mongolian corpora and producing a Japanese–Mongolian bilingual dictionary. We extract loanwords from Mongolian corpora using our own handcrafted rules. To complement the rule-based extraction, we also extract words in Mongolian corpora that are phonetically similar to Japanese Katakana words as loanwords. In addition, we correspond the extracted loanwords to Japanese words and produce a bilingual dictionary. We propose a stemming method for Mongolian to extract loanwords correctly. We verify the effectiveness of our methods experimentally. 
Morphological disambiguation is the process of assigning one set of morphological features to each individual word in a text. When the word is ambiguous (there are several possible analyses for the word), a disambiguation procedure based on the word context must be applied. This paper deals with morphological disambiguation of the Hebrew language, which combines morphemes into a word in both agglutinative and fusional ways. We present an unsupervised stochastic model – the only resource we use is a morphological analyzer – which deals with the data sparseness problem caused by the aﬃxational morphology of the Hebrew language. We present a text encoding method for languages with aﬃxational morphology in which the knowledge of word formation rules (which are quite restricted in Hebrew) helps in the disambiguation. We adapt HMM algorithms for learning and searching this text representation, in such a way that segmentation and tagging can be learned in parallel in one step. Results on a large scale evaluation indicate that this learning improves disambiguation for complex tag sets. Our method is applicable to other languages with aﬃx morphology. 
Developing better methods for segmenting continuous text into words is important for improving the processing of Asian languages, and may shed light on how humans learn to segment speech. We propose two new Bayesian word segmentation methods that assume unigram and bigram models of word dependencies respectively. The bigram model greatly outperforms the unigram model (and previous probabilistic models), demonstrating the importance of such dependencies for word segmentation. We also show that previous probabilistic models rely crucially on suboptimal search procedures. 
We present MAGEAD, a morphological analyzer and generator for the Arabic language family. Our work is novel in that it explicitly addresses the need for processing the morphology of the dialects. MAGEAD performs an on-line analysis to or generation from a root+pattern+features representation, it has separate phonological and orthographic representations, and it allows for combining morphemes from different dialects. We present a detailed evaluation of MAGEAD. 
We present a method for Noun Phrase chunking in Hebrew. We show that the traditional definition of base-NPs as nonrecursive noun phrases does not apply in Hebrew, and propose an alternative definition of Simple NPs. We review syntactic properties of Hebrew related to noun phrases, which indicate that the task of Hebrew SimpleNP chunking is harder than base-NP chunking in English. As a confirmation, we apply methods known to work well for English to Hebrew data. These methods give low results (F from 76 to 86) in Hebrew. We then discuss our method, which applies SVM induction over lexical and morphological features. Morphological features improve the average precision by ~0.5%, recall by ~1%, and F-measure by ~0.75, resulting in a system with average performance of 93% precision, 93.4% recall and 93.2 Fmeasure.* 
With performance above 97% accuracy for newspaper text, part of speech (POS) tagging might be considered a solved problem. Previous studies have shown that allowing the parser to resolve POS tag ambiguity does not improve performance. However, for grammar formalisms which use more ﬁne-grained grammatical categories, for example TAG and CCG, tagging accuracy is much lower. In fact, for these formalisms, premature ambiguity resolution makes parsing infeasible. We describe a multi-tagging approach which maintains a suitable level of lexical category ambiguity for accurate and efﬁcient CCG parsing. We extend this multitagging approach to the POS level to overcome errors introduced by automatically assigned POS tags. Although POS tagging accuracy seems high, maintaining some POS tag ambiguity in the language processing pipeline results in more accurate CCG supertagging. 
In this paper, we present a method for guessing POS tags of unknown words using local and global information. Although many existing methods use only local information (i.e. limited window size or intra-sentential features), global information (extra-sentential features) provides valuable clues for predicting POS tags of unknown words. We propose a probabilistic model for POS guessing of unknown words using global information as well as local information, and estimate its parameters using Gibbs sampling. We also attempt to apply the model to semisupervised learning, and conduct experiments on multiple corpora. 
In this paper, we present a novel global reordering model that can be incorporated into standard phrase-based statistical machine translation. Unlike previous local reordering models that emphasize the reordering of adjacent phrase pairs (Tillmann and Zhang, 2005), our model explicitly models the reordering of long distances by directly estimating the parameters from the phrase alignments of bilingual training sentences. In principle, the global phrase reordering model is conditioned on the source and target phrases that are currently being translated, and the previously translated source and target phrases. To cope with sparseness, we use N-best phrase alignments and bilingual phrase clustering, and investigate a variety of combinations of conditioning factors. Through experiments, we show, that the global reordering model signiﬁcantly improves the translation accuracy of a standard Japanese-English translation task. 
This paper presents a novel training algorithm for a linearly-scored block sequence translation model. The key component is a new procedure to directly optimize the global scoring function used by a SMT decoder. No translation, language, or distortion model probabilities are used as in earlier work on SMT. Therefore our method, which employs less domain speciﬁc knowledge, is both simpler and more extensible than previous approaches. Moreover, the training procedure treats the decoder as a black-box, and thus can be used to optimize any decoding scheme. The training algorithm is evaluated on a standard Arabic-English translation task. 
The noisy channel model approach is successfully applied to various natural language processing tasks. Currently the main research focus of this approach is adaptation methods, how to capture characteristics of words and expressions in a target domain given example sentences in that domain. As a solution we describe a method enlarging the vocabulary of a language model to an almost inﬁnite size and capturing their context information. Especially the new method is suitable for languages in which words are not delimited by whitespace. We applied our method to a phoneme-to-text transcription task in Japanese and reduced about 10% of the errors in the results of an existing method. 
Call centers handle customer queries from various domains such as computer sales and support, mobile phones, car rental, etc. Each such domain generally has a domain model which is essential to handle customer complaints. These models contain common problem categories, typical customer issues and their solutions, greeting styles. Currently these models are manually created over time. Towards this, we propose an unsupervised technique to generate domain models automatically from call transcriptions. We use a state of the art Automatic Speech Recognition system to transcribe the calls between agents and customers, which still results in high word error rates (40%) and show that even from these noisy transcriptions of calls we can automatically build a domain model. The domain model is comprised of primarily a topic taxonomy where every node is characterized by topic(s), typical Questions-Answers (Q&As), typical actions and call statistics. We show how such a domain model can be used for topic identiﬁcation of unseen calls. We also propose applications for aiding agents while handling calls and for agent monitoring based on the domain model. 
The paper presents a new model for context- dependent interpretation of linguistic expressions about spatial proximity between objects in a nat- ural scene. The paper discusses novel psycholin- guistic experimental data that tests and veriﬁes the model. The model has been implemented, and en- ables a conversational robot to identify objects in a scene through topological spatial relations (e.g. “X near Y”). The model can help motivate the choice between topological and projective prepositions. 
This paper investigates a machine learning approach for temporally ordering and anchoring events in natural language texts. To address data sparseness, we used temporal reasoning as an oversampling method to dramatically expand the amount of training data, resulting in predictive accuracy on link labeling as high as 93% using a Maximum Entropy classifier on human annotated data. This method compared favorably against a series of increasingly sophisticated baselines involving expansion of rules derived from human intuitions. 
We present a perceptron-style discriminative approach to machine translation in which large feature sets can be exploited. Unlike discriminative reranking approaches, our system can take advantage of learned features in all stages of decoding. We ﬁrst discuss several challenges to error-driven discriminative approaches. In particular, we explore different ways of updating parameters given a training example. We ﬁnd that making frequent but smaller updates is preferable to making fewer but larger updates. Then, we discuss an array of features and show both how they quantitatively increase BLEU score and how they qualitatively interact on speciﬁc examples. One particular feature we investigate is a novel way to introduce learning into the initial phrase extraction process, which has previously been entirely heuristic. 
We introduce a semi-supervised approach to training for statistical machine translation that alternates the traditional Expectation Maximization step that is applied on a large training corpus with a discriminative step aimed at increasing word-alignment quality on a small, manually word-aligned sub-corpus. We show that our algorithm leads not only to improved alignments but also to machine translation outputs of higher quality. 
 We present a hierarchical phrase-based statistical machine translation in which a target sentence is eﬃciently generated in left-to-right order. The model is a class of synchronous-CFG with a Greibach Normal Form-like structure for the projected production rule: The paired target-side of a production rule takes a phrase preﬁxed form. The decoder for the targetnormalized form is based on an Earlystyle top down parser on the source side. The target-normalized form coupled with our top down parser implies a left-toright generation of translations which enables us a straightforward integration with ngram language models. Our model was experimented on a Japanese-to-English newswire translation task, and showed statistically signiﬁcant performance improvements against a phrase-based translation system.  
In the past years, a number of lexical association measures have been studied to help extract new scientiﬁc terminology or general-language collocations. The implicit assumption of this research was that newly designed term measures involving more sophisticated statistical criteria would outperform simple counts of cooccurrence frequencies. We here explicitly test this assumption. By way of four qualitative criteria, we show that purely statistics-based measures reveal virtually no difference compared with frequency of occurrence counts, while linguistically more informed metrics do reveal such a marked difference. 
Many algorithms have been developed to harvest lexical semantic resources, however few have linked the mined knowledge into formal knowledge repositories. In this paper, we propose two algorithms for automatically ontologizing (attaching) semantic relations into WordNet. We present an empirical evaluation on the task of attaching partof and causation relations, showing an improvement on F-score over a baseline model. 
We propose a novel algorithm for inducing semantic taxonomies. Previous algorithms for taxonomy induction have typically focused on independent classiﬁers for discovering new single relationships based on hand-constructed or automatically discovered textual patterns. By contrast, our algorithm ﬂexibly incorporates evidence from multiple classiﬁers over heterogenous relationships to optimize the entire structure of the taxonomy, using knowledge of a word’s coordinate terms to help in determining its hypernyms, and vice versa. We apply our algorithm on the problem of sense-disambiguated noun hyponym acquisition, where we combine the predictions of hypernym and coordinate term classiﬁers with the knowledge in a preexisting semantic taxonomy (WordNet 2.1). We add 10, 000 novel synsets to WordNet 2.1 at 84% precision, a relative error reduction of 70% over a non-joint algorithm using the same component classiﬁers. Finally, we show that a taxonomy built using our algorithm shows a 23% relative F-score improvement over WordNet 2.1 on an independent testset of hypernym pairs. 
In a new approach to large-scale extraction of facts from unstructured text, distributional similarities become an integral part of both the iterative acquisition of high-coverage contextual extraction patterns, and the validation and ranking of candidate facts. The evaluation measures the quality and coverage of facts extracted from one hundred million Web documents, starting from ten seed facts and using no additional knowledge, lexicons or complex tools. 
Named Entity recognition (NER) is an important part of many natural language processing tasks. Current approaches often employ machine learning techniques and require supervised data. However, many languages lack such resources. This paper presents an (almost) unsupervised learning algorithm for automatic discovery of Named Entities (NEs) in a resource free language, given a bilingual corpora in which it is weakly temporally aligned with a resource rich language. NEs have similar time distributions across such corpora, and often some of the tokens in a multi-word NE are transliterated. We develop an algorithm that exploits both observations iteratively. The algorithm makes use of a new, frequency based, metric for time distributions and a resource free discriminative approach to transliteration. Seeded with a small number of transliteration pairs, our algorithm discovers multi-word NEs, and takes advantage of a dictionary (if one exists) to account for translated or partially translated NEs. We evaluate the algorithm on an English-Russian corpus, and show high level of NEs discovery in Russian. 
This paper proposes a novel composite kernel for relation extraction. The composite kernel consists of two individual kernels: an entity kernel that allows for entity-related features and a convolution parse tree kernel that models syntactic information of relation examples. The motivation of our method is to fully utilize the nice properties of kernel methods to explore diverse knowledge for relation extraction. Our study illustrates that the composite kernel can effectively capture both flat and structured features without the need for extensive feature engineering, and can also easily scale to include more features. Evaluation on the ACE corpus shows that our method outperforms the previous best-reported methods and significantly outperforms previous two dependency tree kernels for relation extraction. 
In this paper, we present a method that improves Japanese dependency parsing by using large-scale statistical information. It takes into account two kinds of information not considered in previous statistical (machine learning based) parsing methods: information about dependency relations among the case elements of a verb, and information about co-occurrence relations between a verb and its case element. This information can be collected from the results of automatic dependency parsing of large-scale corpora. The results of an experiment in which our method was used to rerank the results obtained using an existing machine learning based parsing method showed that our method can improve the accuracy of the results obtained using the existing method. 
This paper presents a hybrid approach to question answering in the clinical domain that combines techniques from summarization and information retrieval. We tackle a frequently-occurring class of questions that takes the form “What is the best drug treatment for X?” Starting from an initial set of MEDLINE citations, our system ﬁrst identiﬁes the drugs under study. Abstracts are then clustered using semantic classes from the UMLS ontology. Finally, a short extractive summary is generated for each abstract to populate the clusters. Two evaluations—a manual one focused on short answers and an automatic one focused on the supporting abstracts—demonstrate that our system compares favorably to PubMed, the search system most widely used by physicians today. 
In this paper we investigate a novel method to detect asymmetric entailment relations between verbs. Our starting point is the idea that some point-wise verb selectional preferences carry relevant semantic information. Experiments using WordNet as a gold standard show promising results. Where applicable, our method, used in combination with other approaches, signiﬁcantly increases the performance of entailment detection. A combined approach including our model improves the AROC of 5% absolute points with respect to standard models. 
In this paper we present how the automatic extraction of events from text can be used to both classify narrative texts according to plot quality and produce advice in an interactive learning environment intended to help students with story writing. We focus on the story rewriting task, in which an exemplar story is read to the students and the students rewrite the story in their own words. The system automatically extracts events from the raw text, formalized as a sequence of temporally ordered predicate-arguments. These events are given to a machine-learner that produces a coarse-grained rating of the story. The results of the machine-learner and the extracted events are then used to generate ﬁne-grained advice for the students. 
We investigate generalizations of the allsubtrees "DOP" approach to unsupervised parsing. Unsupervised DOP models assign all possible binary trees to a set of sentences and next use (a large random subset of) all subtrees from these binary trees to compute the most probable parse trees. We will test both a relative frequency estimator for unsupervised DOP and a maximum likelihood estimator which is known to be statistically consistent. We report state-ofthe-art results on English (WSJ), German (NEGRA) and Chinese (CTB) data. To the best of our knowledge this is the first paper which tests a maximum likelihood estimator for DOP on the Wall Street Journal, leading to the surprising result that an unsupervised parsing model beats a widely used supervised model (a treebank PCFG). 
The present work advances the accuracy and training speed of discriminative parsing. Our discriminative parsing method has no generative component, yet surpasses a generative baseline on constituent parsing, and does so with minimal linguistic cleverness. Our model can incorporate arbitrary features of the input and parse state, and performs feature selection incrementally over an exponential feature space during training. We demonstrate the ﬂexibility of our approach by testing it with several parsing strategies and various feature sets. Our implementation is freely available at: http://nlp.cs.nyu.edu/parser/. 
We investigate prototype-driven learning for primarily unsupervised grammar induction. Prior knowledge is speciﬁed declaratively, by providing a few canonical examples of each target phrase type. This sparse prototype information is then propagated across a corpus using distributional similarity features, which augment an otherwise standard PCFG model. We show that distributional features are effective at distinguishing bracket labels, but not determining bracket locations. To improve the quality of the induced trees, we combine our PCFG induction with the CCM model of Klein and Manning (2002), which has complementary stengths: it identiﬁes brackets but does not label them. Using only a handful of prototypes, we show substantial improvements over naive PCFG induction for English and Chinese grammar induction. 
In this paper, we explore correlation of dependency relation paths to rank candidate answers in answer extraction. Using the correlation measure, we compare dependency relations of a candidate answer and mapped question phrases in sentence with the corresponding relations in question. Different from previous studies, we propose an approximate phrase mapping algorithm and incorporate the mapping score into the correlation measure. The correlations are further incorporated into a Maximum Entropy-based ranking model which estimates path weights from training. Experimental results show that our method signiﬁcantly outperforms state-ofthe-art syntactic relation-based methods by up to 20% in MRR. 
This paper describes an algorithm for propagating verb arguments along lexical chains consisting of WordNet relations. The algorithm creates verb argument structures using VerbNet syntactic patterns. In order to increase the coverage, a larger set of verb senses were automatically associated with the existing patterns from VerbNet. The algorithm is used in an in-house Question Answering system for re-ranking the set of candidate answers. Tests on factoid questions from TREC 2004 indicate that the algorithm improved the system performance by 2.4%. 
Work on the semantics of questions has argued that the relation between a question and its answer(s) can be cast in terms of logical entailment. In this paper, we demonstrate how computational systems designed to recognize textual entailment can be used to enhance the accuracy of current open-domain automatic question answering (Q/A) systems. In our experiments, we show that when textual entailment information is used to either ﬁlter or rank answers returned by a Q/A system, accuracy can be increased by as much as 20% overall. 
We investigate the unsupervised detection of semi-ﬁxed cue phrases such as “This paper proposes a novel approach. . . 1” from unseen text, on the basis of only a handful of seed cue phrases with the desired semantics. The problem, in contrast to bootstrapping approaches for Question Answering and Information Extraction, is that it is hard to ﬁnd a constraining context for occurrences of semi-ﬁxed cue phrases. Our method uses components of the cue phrase itself, rather than external context, to bootstrap. It successfully excludes phrases which are different from the target semantics, but which look superﬁcially similar. The method achieves 88% accuracy, outperforming standard bootstrapping approaches. 
This article describes a robust semantic parser that uses a broad knowledge base created by interconnecting three major resources: FrameNet, VerbNet and PropBank. The FrameNet corpus contains the examples annotated with semantic roles whereas the VerbNet lexicon provides the knowledge about the syntactic behavior of the verbs. We connect VerbNet and FrameNet by mapping the FrameNet frames to the VerbNet Intersective Levin classes. The PropBank corpus, which is tightly connected to the VerbNet lexicon, is used to increase the verb coverage and also to test the effectiveness of our approach. The results indicate that our model is an interesting step towards the design of more robust semantic parsers. 
This paper presents the particular use of “Jibiki” (Papillon’s web server development platform) for the LexALP1 project. LexALP’s goal is to harmonise the terminology on spatial planning and sustainable development used within the Alpine Convention2, so that the member states are able to cooperate and communicate efﬁciently in the four ofﬁcial languages (French, German, Italian and Slovene). To this purpose, LexALP uses the Jibiki platform to build a term bank for the contrastive analysis of the specialised terminology used in six different national legal systems and four different languages. In this paper we present how a generic platform like Jibiki can cope with a new kind of dictionary. 
Thesauri and ontologies provide important value in facilitating access to digital archives by representing underlying principles of organization. Translation of such resources into multiple languages is an important component for providing multilingual access. However, the specificity of vocabulary terms in most ontologies precludes fully-automated machine translation using general-domain lexical resources. In this paper, we present an efficient process for leveraging human translations when constructing domain-specific lexical resources. We evaluate the effectiveness of this process by producing a probabilistic phrase dictionary and translating a thesaurus of 56,000 concepts used to catalogue a large archive of oral histories. Our experiments demonstrate a cost-effective technique for accurate machine translation of large ontologies. 
This paper focuses on the use of advanced techniques of text analysis as support for collocation extraction. A hybrid system is presented that combines statistical methods and multilingual parsing for detecting accurate collocational information from English, French, Spanish and Italian corpora. The advantage of relying on full parsing over using a traditional window method (which ignores the syntactic information) is ﬁrst theoretically motivated, then empirically validated by a comparative evaluation experiment. 
Statistical MT has made great progress in the last few years, but current translation models are weak on re-ordering and target language ﬂuency. Syntactic approaches seek to remedy these problems. In this paper, we take the framework for acquiring multi-level syntactic translation rules of (Galley et al., 2004) from aligned tree-string pairs, and present two main extensions of their approach: ﬁrst, instead of merely computing a single derivation that minimally explains a sentence pair, we construct a large number of derivations that include contextually richer rules, and account for multiple interpretations of unaligned words. Second, we propose probability estimates and a training procedure for weighting these rules. We contrast different approaches on real examples, show that our estimates based on multiple derivations favor phrasal re-orderings that are linguistically better motivated, and establish that our larger rules provide a 3.63 BLEU point increase over minimal rules. 
Certain distinctions made in the lexicon of one language may be redundant when translating into another language. We quantify redundancy among source types by the similarity of their distributions over target types. We propose a languageindependent framework for minimising lexical redundancy that can be optimised directly from parallel text. Optimisation of the source lexicon for a given target language is viewed as model selection over a set of cluster-based translation models. Redundant distinctions between types may exhibit monolingual regularities, for example, inﬂexion patterns. We deﬁne a prior over model structure using a Markov random ﬁeld and learn features over sets of monolingual types that are predictive of bilingual redundancy. The prior makes model selection more robust without the need for language-speciﬁc assumptions regarding redundancy. Using these models in a phrase-based SMT system, we show signiﬁcant improvements in translation quality for certain language pairs. 
This paper describes a study of the patterns of translational equivalence exhibited by a variety of bitexts. The study found that the complexity of these patterns in every bitext was higher than suggested in the literature. These ﬁndings shed new light on why “syntactic” constraints have not helped to improve statistical translation models, including ﬁnitestate phrase-based models, tree-to-string models, and tree-to-tree models. The paper also presents evidence that inversion transduction grammars cannot generate some translational equivalence relations, even in relatively simple real bitexts in syntactically similar languages with rigid word order. Instructions for replicating our experiments are at http://nlp.cs.nyu.edu/GenPar/ACL06 
We propose a new hierarchical Bayesian n-gram model of natural languages. Our model makes use of a generalization of the commonly used Dirichlet distributions called Pitman-Yor processes which produce power-law distributions more closely resembling those in natural languages. We show that an approximation to the hierarchical Pitman-Yor language model recovers the exact formulation of interpolated Kneser-Ney, one of the best smoothing methods for n-gram language models. Experiments verify that our model gives cross entropy results superior to interpolated Kneser-Ney and comparable to modiﬁed Kneser-Ney. 
Chatting is a popular communication media on the Internet via ICQ, chat rooms, etc. Chat language is different from natural language due to its anomalous and dynamic natures, which renders conventional NLP tools inapplicable. The dynamic problem is enormously troublesome because it makes static chat language corpus outdated quickly in representing contemporary chat language. To address the dynamic problem, we propose the phonetic mapping models to present mappings between chat terms and standard words via phonetic transcription, i.e. Chinese Pinyin in our case. Different from character mappings, the phonetic mappings can be constructed from available standard Chinese corpus. To perform the task of dynamic chat language term normalization, we extend the source channel model by incorporating the phonetic mapping models. Experimental results show that this method is effective and stable in normalizing dynamic chat language terms. 
This paper presents a discriminative pruning method of n-gram language model for Chinese word segmentation. To reduce the size of the language model that is used in a Chinese word segmentation system, importance of each bigram is computed in terms of discriminative pruning criterion that is related to the performance loss caused by pruning the bigram. Then we propose a step-by-step growing algorithm to build the language model of desired size. Experimental results show that the discriminative pruning method leads to a much smaller model compared with the model pruned using the state-of-the-art method. At the same Chinese word segmentation F-measure, the number of bigrams in the model can be reduced by up to 90%. Correlation between language model perplexity and word segmentation performance is also discussed. 
A web search with double checking model is proposed to explore the web as a live corpus. Five association measures including variants of Dice, Overlap Ratio, Jaccard, and Cosine, as well as CoOccurrence Double Check (CODC), are presented. In the experiments on Rubenstein-Goodenough’s benchmark data set, the CODC measure achieves correlation coefficient 0.8492, which competes with the performance (0.8914) of the model using WordNet. The experiments on link detection of named entities using the strategies of direct association, association matrix and scalar association matrix verify that the double-check frequencies are reliable. Further study on named entity clustering shows that the five measures are quite useful. In particular, CODC measure is very stable on wordword and name-name experiments. The application of CODC measure to expand community chains for personal name disambiguation achieves 9.65% and 14.22% increase compared to the system without community expansion. All the experiments illustrate that the novel model of web search with double checking is feasible for mining associations from the web. 
This paper introduces a novel framework for the accurate retrieval of relational concepts from huge texts. Prior to retrieval, all sentences are annotated with predicate argument structures and ontological identiﬁers by applying a deep parser and a term recognizer. During the run time, user requests are converted into queries of region algebra on these annotations. Structural matching with pre-computed semantic annotations establishes the accurate and efﬁcient retrieval of relational concepts. This framework was applied to a text retrieval system for MEDLINE. Experiments on the retrieval of biomedical correlations revealed that the cost is sufﬁciently small for real-time applications and that the retrieval precision is signiﬁcantly improved. 
A query speller is crucial to search engine in improving web search relevance. This paper describes novel methods for use of distributional similarity estimated from query logs in learning improved query spelling correction models. The key to our methods is the property of distributional similarity between two terms: it is high between a frequently occurring misspelling and its correction, and low between two irrelevant terms only with similar spellings. We present two models that are able to take advantage of this property. Experimental results demonstrate that the distributional similarity based models can significantly outperform their baseline systems in the web query spelling correction task. 
This paper presents an approach to incrementally generating locative expressions. It addresses the issue of combinatorial explosion inherent in the construction of relational context models by: (a) contextually deﬁning the set of objects in the context that may function as a landmark, and (b) sequencing the order in which spatial relations are considered using a cognitively motivated hierarchy of relations, and visual and discourse salience. 
Japanese case markers, which indicate the grammatical relation of the complement NP to the predicate, often pose challenges to the generation of Japanese text, be it done by a foreign language learner, or by a machine translation (MT) system. In this paper, we describe the task of predicting Japanese case markers and propose machine learning methods for solving it in two settings: (i) monolingual, when given information only from the Japanese sentence; and (ii) bilingual, when also given information from a corresponding English source sentence in an MT context. We formulate the task after the well-studied task of English semantic role labelling, and explore features from a syntactic dependency structure of the sentence. For the monolingual task, we evaluated our models on the Kyoto Corpus and achieved over 84% accuracy in assigning correct case markers for each phrase. For the bilingual task, we achieved an accuracy of 92% per phrase using a bilingual dataset from a technical domain. We show that in both settings, features that exploit dependency information, whether derived from gold-standard annotations or automatically assigned, contribute significantly to the prediction of case markers.1 
In this paper we investigate how to automatically determine if two document collections are written from different perspectives. By perspectives we mean a point of view, for example, from the perspective of Democrats or Republicans. We propose a test of different perspectives based on distribution divergence between the statistical models of two collections. Experimental results show that the test can successfully distinguish document collections of different perspectives from other types of collections. 
Subjectivity and meaning are both important properties of language. This paper explores their interaction, and brings empirical evidence in support of the hypotheses that (1) subjectivity is a property that can be associated with word senses, and (2) word sense disambiguation can directly beneﬁt from subjectivity annotations. 
This paper demonstrates a conceptually simple but effective method of increasing the accuracy of QA systems on factoid-style questions. We define the notion of an inverted question, and show that by requiring that the answers to the original and inverted questions be mutually consistent, incorrect answers get demoted in confidence and correct ones promoted. Additionally, we show that lack of validation can be used to assert no-answer (nil) conditions. We demonstrate increases of performance on TREC and other question-sets, and discuss the kinds of future activities that can be particularly beneficial to approaches such as ours. 
Uniﬁcation grammars are widely accepted as an expressive means for describing the structure of natural languages. In general, the recognition problem is undecidable for uniﬁcation grammars. Even with restricted variants of the formalism, offline parsable grammars, the problem is computationally hard. We present two natural constraints on uniﬁcation grammars which limit their expressivity. We ﬁrst show that non-reentrant uniﬁcation grammars generate exactly the class of contextfree languages. We then relax the constraint and show that one-reentrant uniﬁcation grammars generate exactly the class of tree-adjoining languages. We thus relate the commonly used and linguistically motivated formalism of uniﬁcation grammars to more restricted, computationally tractable classes of languages. 
We propose WIDL-expressions as a ﬂexible formalism that facilitates the integration of a generic sentence realization system within end-to-end language processing applications. WIDL-expressions represent compactly probability distributions over ﬁnite sets of candidate realizations, and have optimal algorithms for realization via interpolation with language model probability distributions. We show the effectiveness of a WIDL-based NLG system in two sentence realization tasks: automatic translation and headline generation. 
This paper presents a method for adapting a language generator to the strengths and weaknesses of a synthetic voice, thereby improving the naturalness of synthetic speech in a spoken language dialogue system. The method trains a discriminative reranker to select paraphrases that are predicted to sound natural when synthesized. The ranker is trained on realizer and synthesizer features in supervised fashion, using human judgements of synthetic voice quality on a sample of the paraphrases representative of the generator’s capability. Results from a cross-validation study indicate that discriminative paraphrase reranking can achieve substantial improvements in naturalness on average, ameliorating the problem of highly variable synthesis quality typically encountered with today’s unit selection synthesizers. 
This paper shows that a simple two-stage approach to handle non-local dependencies in Named Entity Recognition (NER) can outperform existing approaches that handle non-local dependencies, while being much more computationally efﬁcient. NER systems typically use sequence models for tractable inference, but this makes them unable to capture the long distance structure present in text. We use a Conditional Random Field (CRF) based NER system using local features to make predictions and then train another CRF which uses both local information and features extracted from the output of the ﬁrst CRF. Using features capturing non-local dependencies from the same document, our approach yields a 12.6% relative error reduction on the F1 score, over state-of-theart NER systems using local-information alone, when compared to the 9.3% relative error reduction offered by the best systems that exploit non-local information. Our approach also makes it easy to incorporate non-local information from other documents in the test corpus, and this gives us a 13.3% error reduction over NER systems using local-information alone. Additionally, our running time for inference is just the inference time of two sequential CRFs, which is much less than that of other more complicated approaches that directly model the dependencies and do approximate inference. 
This paper presents an adaptive learning framework for Phonetic Similarity Modeling (PSM) that supports the automatic construction of transliteration lexicons. The learning algorithm starts with minimum prior knowledge about machine transliteration, and acquires knowledge iteratively from the Web. We study the active learning and the unsupervised learning strategies that minimize human supervision in terms of data labeling. The learning process refines the PSM and constructs a transliteration lexicon at the same time. We evaluate the proposed PSM and its learning algorithm through a series of systematic experiments, which show that the proposed framework is reliably effective on two independent databases. 
Machine Transliteration is to transcribe a word written in a script with approximate phonetic equivalence in another language. It is useful for machine translation, cross-lingual information retrieval, multilingual text and speech processing. Punjabi Machine Transliteration (PMT) is a special case of machine transliteration and is a process of converting a word from Shahmukhi (based on Arabic script) to Gurmukhi (derivation of Landa, Shardha and Takri, old scripts of Indian subcontinent), two scripts of Punjabi, irrespective of the type of word. The Punjabi Machine Transliteration System uses transliteration rules (character mappings and dependency rules) for transliteration of Shahmukhi words into Gurmukhi. The PMT system can transliterate every word written in Shahmukhi. 
This paper presents an approach for Multilingual Document Clustering in comparable corpora. The algorithm is of heuristic nature and it uses as unique evidence for clustering the identiﬁcation of cognate named entities between both sides of the comparable corpora. One of the main advantages of this approach is that it does not depend on bilingual or multilingual resources. However, it depends on the possibility of identifying cognate named entities between the languages used in the corpus. An additional advantage of the approach is that it does not need any information about the right number of clusters; the algorithm calculates it. We have tested this approach with a comparable corpus of news written in English and Spanish. In addition, we have compared the results with a system which translates selected document features. The obtained results are encouraging. 
This study aims at identifying when an event written in text occurs. In particular, we classify a sentence for an event into four time-slots; morning, daytime, evening, and night. To realize our goal, we focus on expressions associated with time-slot (time-associated words). However, listing up all the time-associated words is impractical, because there are numerous time-associated expressions. We therefore use a semi-supervised learning method, the Naïve Bayes classifier backed up with the Expectation Maximization algorithm, in order to iteratively extract time-associated words while improving the classifier. We also propose to use Support Vector Machines to filter out noisy instances that indicates no specific time period. As a result of experiments, the proposed method achieved 0.864 of accuracy and outperformed other methods. 
Given a parallel corpus, semantic projection attempts to transfer semantic role annotations from one language to another, typically by exploiting word alignments. In this paper, we present an improved method for obtaining constituent alignments between parallel sentences to guide the role projection task. Our extensions are twofold: (a) we model constituent alignment as minimum weight edge covers in a bipartite graph, which allows us to ﬁnd a globally optimal solution efﬁciently; (b) we propose tree pruning as a promising strategy for reducing alignment noise. Experimental results on an English-German parallel corpus demonstrate improvements over state-of-the-art models. 
In this paper, we discuss how to utilize the co-occurrence of answers in building an automatic question answering system that answers a series of questions on a specific topic in a batch mode. Experiments show that the answers to the many of the questions in the series usually have a high degree of co-occurrence in relevant document passages. This feature sometimes can’t be easily utilized in an automatic QA system which processes questions independently. However it can be utilized in a QA system that processes questions in a batch mode. We have used our pervious TREC QA system as baseline and augmented it with new answer clustering and co-occurrence maximization components to build the batch QA system. The experiment results show that the QA system running under the batch mode get significant performance improvement over our baseline TREC QA system. 
The authors present a tokenizer and ﬁnite-state morphological analyzer [Beesley and Karttunen 2003] for Malagasy, based primarily on the discussion of Malagasy morphology in Keenan and Polinsky [1998] and Randriamasimanana [1986]. Words in Malagasy are built from roots by means of a variety of morphological operations such as compounding, afﬁxation and reduplication. The authors analyze productive patterns of nominal and verbal morphology, and describe genitive compounding and sufﬁxation for nouns and various derivational processes involving compounding and afﬁxation for verbs. This work offers a computational analysis of Malagasy morphology, and forms the basis of a computational grammar and lexicon of Malagasy within the framework of the PARGRAM project. Keywords: Malagasy, Austronesian, Morphological Analyzer, Fnite-State Morphology, PARGRAM 1. Malagasy in the PARGRAM Project Malagasy is an Austronesian language spoken by about six million people on the island of Madagascar [Grimes 1999]. Along with Welsh, it is a focus of the Verb-Initial Grammars subproject (http://users.ox.ac.uk/˜cpgl0015/pargram/) within the PARGRAM initiative, a collaborative project to develop computational lexicons and grammars within the shared linguistic framework of Lexical Functional Grammar [Butt et al. 2002]. The objective of PARGRAM is to develop parallel grammars for a range of different languages2 using a shared linguistic framework and shared grammar writing techniques and technology. However, each project within the PARGRAM umbrella is driven by a different set of goals. For example, the English, German and Japanese grammars have been under development for a number of years; these grammars aim for very broad and robust coverage 
This paper presents an analysis of Internally Headed Relative Clause (IHRC) construction in Japanese within the framework of Combinatory Categorial Grammar [Steedman 2000]. Shimoyama [1999] argues that when an IHRC appears within the scope of a universal quantifier, the interpretation of the IHRC exemplifies E-type anaphora and that the LF representation of the IHRC should have a variable bound by the quantifier in the matrix clause. To accommodate this argument Shimoyama posits a free variable of a functional type to which the bound variable is applied, and whose denotation is determined by the context-dependent assignment function. However, since there is in principle no limit to the number of quantifiers in the matrix clause (and accordingly that of bound variables in the IHRC), the semantic type of the free variable would be highly ambiguous if the IHRC occurs within the scope of multiple quantifiers. The current analysis assumes that the interpretation of IHRCs exhibits an instance of generalized Skolem term [Steedman 2005], a term whose denotation varies with the value of bound variables introduced by scope-taking operators, but which is interpreted as a constant in the absence of such operators. This paper provides a straightforward account for the semantics of the construction without invoking the complexities of the type ambiguity of free variables. Keywords: Combinatory Categorial Grammar, Generalized Skolem Term, Internally Headed Relative Clause, Japanese, Quantification 1. Introduction This paper presents an analysis of Internally Headed Relative Clause (IHRC) construction in Japanese paying particular attention to the effect of quantification on its interpretation. (1)  ∗ Graduate School of International Cultural Studies, Tohoku University, 41 Kawauchi, Aoba-ku, Sendai 980-8576, Japan. Tel: +81-22-795-7550, Fax: +81-22-795-7850 E-Mail: otake@linguist.jp; kei@insc.tohoku.ac.jp [Received April 30, 2006; Revised October 19, 2006; Accepted November 7, 2006]  334  Rui Otake and Kei Yoshimoto  illustrates the basic form of the construction:1  Taroo-ga [Hanako-ga ringo-o muita] no-o  tabeta.  Taro-NOM Hanako-NOM apple-ACC peeled NML-ACC ate  (1)  ‘Taro ate the apple that Hanako peeled.’  The bracketed clause Hanako-ga ringo-o muita ‘Hanako peeled an apple’ is followed by the nominalizer no and the accusative particle -o, thereby construed as the object of the matrix verb tabeta ‘ate’. Since the verb requires as its semantic restriction that the accusative argument be an edible thing, it anaphorically picks up the referent of ringo ‘apple’ from the embedded clause. This kind of construction is often contrasted with the Externally Headed Relative Clauses (EHRC), which is illustrated in (2).  Taroo-ga [Hanako-ga muita] ringo-o tabeta.  Taro-NOM Hanako-NOM peeled apple-ACC ate  (2)  ‘Taro ate the apple that Hanako peeled.’  As can be seen from the translation, (1) and (2) have almost the same meaning. But we hasten to add that IHRCs are not always paraphrasable to the EHRC version, for the former construction is subject to some pragmatic condition for its felicitous use, which we will not attempt to specify. A terminological note: we use the term the antecedent of an IHRC to mean the referent of the IHRC which functions as the argument of the matrix predicate. And we also use the term the head of the IHRC to refer to the linguistic element in the IHRC which describes the antecedent. For example, the antecedent of the IHRC in (1) is the apple that Hanako peeled, and the head is the noun ringo ‘apple’. It is important to notice that we define IHRC construction in terms of the nominal character of its anaphoric referent, despite the name suggesting that the presence of the head noun inside the relative clause is the defining feature of the construction. In fact, there are cases where the IHRC has no explicit nominal head. Such examples will be dealt with in section 2.2. This paper is organized as follows. In section 2, we review the observation made by Shimoyama [1999] and her E-type analysis of IHRC. We then address the problem that the E-type analysis would raise focusing on multiply quantified IHRCs. We also discuss the  
This paper proposes a new framework for a system which will help online volunteers to perform translations on their PCs while sharing resources and tools and communicating via websites. The current status of such online volunteer translators and their translation practices and tools are examined, along with related work also being discussed. General requirements are derived from these considerations. The approach taken in this study for dealing with heterogeneous linguistic resources relies on an XML structure maximizing efficiency and enabling all of the desired functionalities. The QRLex environment is under development and implements this new framework. Keywords: Computer-Aided Translation, Web Search for Translation, Memory Translation, Helping Volunteer Translators, Linguistique Ressources. 1. Introduction There have been many misconceptions concerning Machine Translation. In the early days, some researchers promised to "replace translators", while others like Bar-Hillel warned against the impossibility of FAHQMT (Fully Automatic High Quality Machine Translation) in general. The famous ALPAC report negatively evaluated the performance of Machine Translation (MT) systems at the end of 19661. It is also known as the “infamous” ALPAC ∗ Laboratoire CLIPS-GETA-IMAG, Université Joseph Fourier, 385, rue de la Bibliothèque, Grenoble, France E-mail : {youcef.bey; christian.boitet}@imag.fr + Graduate School of Education, The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo, 113-0033, Japan E-mail: kyo@p.u-tokyo.ac.jp 1 As a matter of fact, the ALPAC committee worked on obsolete and incorrect data, and did not even investigate a significant project in the same city (Washington D.C.), the GAT (Georgetown Automatic Translation) project. Further, several members of the committee were themselves heads of labs that received finds to work on MT. These members preferred to work on theoretical linguistics and AI (as it later became known) instead of the necessary engineering. [Received May 1, 2006; Revised October 20, 2006; Accepted November 7, 2006]  350  Youcef Bey et al.  report because it was biased, as explained in detail in the no less famous counter-report by Zbigniew Pankowicz, the polyglot USAF official who oversaw MT funding at RADC (Rome Air Development Center) from the early days until 1985. The truth, as recognized by Bar-Hillel in 1972 at a seminar on the usability of MT organized at Austin, Texas2, is that MT can be quite useful in practice even if the "translational quality" is medium or poor. Later, when MT became widely available (first at the European Community, then on the French Minitel, then on PCs, and finally on the Web), it became clear that commercial MT, if properly used as a tool to help translators by offering them a kind of "pretranslation", can be used efficiently as "MT for translators". According to [Allen 2001] and according to experiments presented on [MT POST-EDITING 2006], the productivity of translators is often multiplied by three, for a variety of tasks. Nevertheless, it was, and still is, true that MT has three conflicting goals that cannot be achieved together: full automaticity, high quality, and general coverage. However, two of these goals can indeed be achieved together. For example, the METEO system3 showed decisively that an MT system specialized to an "adequate" sublanguage, in that case the language of weather bulletins ( as opposed to weather situations or warnings), can produce better translations than the humans previously employed . Indeed, before 1980—1985, it took about 5—10 minutes to post-edit a bulletin translated by a junior translator, while it has taken only one minute from 1985 on, when METEO reached its top quality. Here, MT is really "MT for revisers" in that MT output can be post-edited without reference to the source text. By contrast, wide coverage fully automatic MT systems cannot be used in this way at all. Because of unsolved ambiguities, the number of possible valid translations with very different meanings (including nonsensical ones) is extremely high, and it is not feasible to show them all to the post-editor. Exactly the same systems that can be very useful to bilingual post-editors are useless to monolingual post-editors. Martin Kay wrote that: “...this happens when the attempt is made to mechanize the non-mechanical or something whose mechanistic substructure science has not yet been revealed...” [Kay 1997] Yes, but that is not really the point! The point is that the same problem arises with human 2 One of the few research groups really working on MT that continued to be funded after 1966 in the US, such as that of Pr Wang (University of California, Berkeley), and the newly founded Systran and Logos. 3 That is the name of the operational system, which was further developed, improved and deployed on PCs by John Chandioux and his team, starting from the TAUM-METEO prototype built by the TAUM group at Université de Montréal around 1975-76.  Data Management in QRLex, an Online Aid System for Volunteer Translators’ 351 translators: if they are asked to translate texts far out of their domain, they also produce incomprehensible results, impossible or very difficult for monolingual domain specialists to post-edit. Their errors are different, and their translations are more grammatical, but the time required to obtain polished translations based on their draft (or to decide that they are too poor for that and the text must be retranslated by a specialist) is on the same order. In any case, this perception of MT has promoted research on computer aided human translation, which exploits the potential of computers to support translator skills and intelligence [Hutchins 1998]. Many industries have made large investments in developing useful translation-aid tools. These efforts have resulted in commercial Computer-Aided Translation (CAT) systems such as TM-2 (IBM), Trados, Déjà Vu, Transit, and Similis, which usually contain three components:  bilingual editors (often embedded in text or document processors such as Word, WordPerfect, Ichitarou, Interleaf, etc.),  on-line terminology banks and dictionaries (the latter being modifiable by translators and immediately updated), and  translation memory systems (TM), which seek exact or fuzzy matches of the source segments to retrieve their translations as proposed translation [Bowker 2002]. There are two situations in professional translation. In the case of large and repetitive translation jobs such as successive versions of a product documentation, TM is quite useful, and MT is not used, even in the rare cases where its integration is foreseen, as in TM-2: the distance between users and developers is too great, so that MT dictionaries, especially for terminology, are not updated fast enough from the translators' dictionaries, whereas TM grows and becomes increasingly useful [Boitet 2005]. In the case of individual translators working on a variety of jobs, TM is not really useful, because the quantity of past translations of similar texts is too small. Accordingly, cheap commercial MT is used to obtain preliminary translation. (Even if no translations of complete segments are correct, many fragments are correct; hence MT functions as a kind of dictionary in context). Relatively few individual translators use commercial CAT tools anyway because of their high price. What is the situation for online volunteer translators? Here again, there are two cases. In the first case, to which the QRLex system is addressed, translators are online in that they access a website to get documents to translate, retrieve resources such as dictionaries and TMs, deposit finished translations, and communicate with other translators. However, they don't translate online. Rather, they work on their PCs (or PDAs), just like many professional translators. However, they cannot afford to use commercial PC-oriented CAT tools, and, until now, they have not benefited from shared resources as do their professional counterparts.  352  Youcef Bey et al.  In the second and more recently encountered case, volunteer translators do translate online, as on Translationwiki [TRANSLATIONWIKI 2006]. The documents to be translated are automatically segmented (paragraphs, sentences) and put up for translation. No CAT functions or resources are available. In all cases, the CAT tools and resources available, if any, do not provide content and functions that fully satisfy all translators. There is thus a real need to aid online volunteer translators and their communities by providing them with a free environment with a rich set of linguistic resources and tools, and improved workflow and data management. Recently, the number of volunteer translators has been growing sharply. Volunteers form or join communities, and they translate thousands of documents in different fields, thereby showing the true way to break the language barrier. These developments are mainly due to the Internet's crucial role in allowing translators to take part in such volunteer translation activities. According to our study, volunteer translator communities are mainly of two types:  Mission-oriented translator communities: strongly-coordinated groups of volunteers involved in translating clearly defined sets of documents (Linux-like communities). These communities translate what can be loosely called technical documentation, such as Linux documentation [TRADUC 2005], W3C specifications, and documentation as well as software (interface, messages, online help) of open source products. For example, in the W3C consortium, 301 volunteer translators are involved in translating thousands of specification documents into approximately 41 languages [W3C 2005]. Documentation in the Mozilla project exists in 70 languages, and is translated by hundreds of volunteer translators located in different countries [MOZILLA 2005].  Network communities of subject-oriented translators: individual translators who translate online documents such as news, analysis, and reports and make translation available on personal or group Web pages [TEANOTWEAR 2005] [PAXHUMANA 2006]. These translators are often involved in non-identified projects. They form translator groups with no a priori orientation, but they share similar opinions about events (anti-war humanitarian communities, translation of reports, news translation, humanitarian help, etc.). 
This paper argues that small corpora are useful in testing specific linguistic hypotheses, particularly those dealing with rhetoric, stylistics, and sociolinguistics. In particular, we hypothesize that creating a database of U.S. presidential speeches will allow for a diachronic exploration of language use at the highest political level, and enable a contrast to be drawn between legislative advances for minorities in the United States and the integration of those advances into the presidential lexicon. In order to test this hypothesis, we examine the corpora of State of the Union Addresses from 1945 to 2006. We demonstrate that while there was clearly a shift two decades ago to systematically portraying human beings as being made up of two genders, or being subsumed under a gender-neutral term, other aspects of gender, such as parenthood, are still stereotyped by American presidents. In short, analyzing lexical instances related to ‘people’ in the State of the Union address allows us not only to reflect on the values held by U.S. presidents, but also to systematically uncover how they use language to exercise power on the very people they are elected to serve. Keywords: Small Corpora, Politics, Language, Gender, Diachronic Analysis 1. Introduction In the latter half of the twentieth century, American presidents have had the enviable task of shaping the way Americans think about themselves by delivering a State of the Union address near the beginning of each calendar year. This speech is broadcast live across the nation on major television and radio channels. In the address, the president emphasizes his accomplishments to date and sets out a new agenda for the year. Topics touched upon may include both foreign and domestic policy, and run the gamut from justification for war to a ∗ Graduate Institute of Linguistics & Department of Foreign Languages and Literatures, National Taiwan University, 1 Roosevelt Road, Section 4, Taipei 106, Taiwan Phone: +886-2-2366-1381 ext. 307 Fax: +886-2-2363-5358 E-mail: kathleenahrens@yahoo.com [Received May 3, 2006; Revised November 3, 2006; Accepted November 7, 2006]  378  Kathleen Ahrens  fervent plea to pass an education bill. The complete text of the address appears the following day in major newspapers and in on-line news resources. Thus, these addresses constitute a narrow, but influential media genre, since subsequent discourse in the news media often reports on the proposals put forth by the president in his own terminology [Barrett 2004]. This terminology reflects the ideology of the ruling political party, and it is this ideology that is used to exercise power “through the manufacture of consent” [Fairclough 2001]. Moreover, as Van Dijk [1993] notes, “More control over more properties of text and context, involving more people, is thus generally (though not always) associated with more influence, and hence with hegemony” (p. 257). Thus, a linguistic analysis of presidential speeches has the potential to shed light on how the president views (and wants the country to view) economic, political, and social issues of the day. Recent advances in corpus linguistics have facilitated the collection and analysis of presidential speeches. Kowal et al. [1997], for example, created a specially marked-up corpus of Inaugural Addresses by hand in order to look at the interaction between literacy and orality in presidential speeches, while Charteris-Black [2004, 2005], who looked only at the text, was able to examine the use of metaphor as well as rhetorical devices used by U.S. presidents based on the corpora of U.S. presidential speeches found on-line. Lim [2002] also used the corpora of U.S. inaugural addresses, as well as the annual messages (State of the Union addresses), in order to identify rhetorical change in presidential speeches. He argued that presidential speeches have become more anti-intellectual, as well as more abstract, democratic, assertive, and conversational, according to changes that can be seen in the categories of words that occur in speeches over time. He also found that words related to cognitive processes and states have decreased since Hoover, while interjections have increased, indicating a decline in the difficulty of presidential rhetoric. In addition, words having to do with ‘kinship’, which Lim suggests reflects democratic rhetoric, have increased substantially since Franklin Roosevelt, as have words relating to ‘children’ and ‘youth’ since Carter. While Lim’s paper suggests that frequency of lexical use is a way of judging what is important to the president and to the public, to date there has been no systematic analysis of changes in lexical use within the scope of presidential speeches. Thus, it is the goal of this paper to demonstrate that by combining presidential speeches into a corpus, subtle changes in language use over time can be determined by examining the frequency of occurrence of key words as well as their associated collocations [Stubbs 1996]. In order to examine this issue, we will explore language use pertaining to ‘people’ in all of the State of the Union addresses (SUO corpus) from 1945 to 2006 by analyzing the tokens:  Using a Small Corpus to Test Linguistic Hypotheses:  379  Evaluating ‘People’ in the State of the Union Addresses  humankind, mankind, man, men, woman, women, mother, father and parent.1 We will demonstrate that while there has clearly been shift away from using man, men, and mankind to refer to all human beings, other aspects of gender, such as parenthood, are still stereotyped by American presidents. 2. Methodology for Corpus Creation The State of the Union (SOU) corpus was downloaded one speech at a time from the C-Span website (c-span.org). All State of the Union speeches from 1945-2006 (excluding Nixon’s five SOU speeches from 1970-1974) were directly downloaded to text files (Table 1). Nixon’s speeches were printed out from Adobe files, manually typed into a document file, and then saved to a text file. These five files were then checked by two additional readers for accuracy and any errors or omissions were corrected. Each file was then imported into Microsoft Word and the president’s words in the speech were highlighted and counted with the word-count feature. In most cases, this meant that the heading was omitted (i.e. “President Harry S. Truman’s Address Before a Joint Session of Congress”). In other cases, information about where and when the speech was given, or who introduced the President also had to be omitted from the word count. In the case of Bush Jr., indications of “(Applause.)” had to be deleted as well.2 In short, every effort was made to include the words used by the president himself in the word count.3 In addition, it is important to note that these speeches were given orally from a prepared text. The version that being examined here is the version that was provided for the written, historical record and the content may therefore vary slightly from the actual words that the president spoke.  
A pragmatic Chinese word segmentation approach is presented in this paper based on mixing language models. Chinese word segmentation is composed of several hard sub-tasks, which usually encounter different difficulties. The authors apply the corresponding language model to solve each special sub-task, so as to take advantage of each model. First, a class-based trigram is adopted in basic word segmentation, which applies the Absolute Discount Smoothing algorithm to overcome data sparseness. The Maximum Entropy Model (ME) is also used to identify Named Entities. Second, the authors propose the application of rough sets and average mutual information, etc. to extract special features. Finally, some features are extended through the combination of the word cluster and the thesaurus. The authors’ system participated in the Second International Chinese Word Segmentation Bakeoff, and achieved 96.7 and 97.2 in F-measure in the PKU and MSRA open tests, respectively. Keywords: Word Segmentation, N-Gram, Maximum Entropy Model, Rough Sets, Word Cluster, Machine Learning 1. Introduction The word is a logical semantic and syntactic unit in natural language. Unlike English, there is no delimiter to mark word boundaries in Chinese language, so, in most Chinese NLP tasks, word segmentation is the foundational task which transforms the Chinese character string into a word sequence. It is a prerequisite to POS tagging, parser or further applications, such as Information Extraction, and the Question Answer system. Word segmentation has attracted long-term attention in the research community for more 
Chen, Keh-Jiann see Bai, Ming-Hong, 11(3): 223-244 see Li, Shih-Min, 11(3): 245-280 Chen, Li-Mei see Chang, Ke-Jia, 11(3): 281-296 Chien, Jen-Tzung see Chueh, Chuang-Hua, 11(1): 37-56 Chueh, Chuang-Hua Hsin-Min Wang, and Jen-Tzung Chien. A Maximum Entropy Approach for Semantic Language Modeling; 11(1): 37-56 
Recognizing transliteration names is challenging due to their flexible formulation and lexical coverage. In our approach, we employ the Web as a giant corpus. The patterns extracted from the Web are used as a live dictionary to correct speech recognition errors. The plausible character strings recognized by an Automated Speech Recognition (ASR) system are regarded as query terms and submitted to Google. The top N snippets are entered into PAT trees. The terms of the highest scores are selected. Our experiments show that the ASR model with a recovery mechanism can achieve 21.54% performance improvement compared with the ASR only model on the character level. The recall rate is improved from 0.20 to 0.42, and the MRR from 0.07 to 0.31. For collecting transliteration names, we propose a named entity (NE) ontology generation engine, called the XNE-Tree engine, which produces relational named entities by a given seed. The engine incrementally extracts high co-occurring named entities with the seed. A total of 7,642 named entities in the ontology were initiated by 100 seeds. When the bi-character language model is combined with the NE ontology, the ASR recall rate and MRR are improved to 0.48 and 0.38, respectively. 1. Introduction Named entities [MUC 1998], which denote persons, locations, organizations, etc., are common foci of searchers. Thompson and Dozier [1997] showed that named entity recognition (NER) could improve the performance of information retrieval systems. Capturing named entities is challenging due to their flexible formulation and novelty. The issues behind speech recognition make named entity recognition more challenging on the ∗ Department of Computer Science and Information Engineering, National Taiwan University, Taipei, 106 Taiwan E-mail: {d91022 , hhchen}@csie.ntu.edu.tw + Department of Computer Science and Engineering, National Sun Yat-Sen University, 70, Lien-Hai Road, Kaohsiung, 804 Taiwan E-mail: cpchen@cse.nsysu.edu.tw [Received February 16, 2006; Revised June 23, 2006; Accepted July 10, 2006]  184  Ming-Shun Lin et al.  spoken level than on the written level. This paper focuses on a special type of named entities, called transliteration names. They describe foreign people, places, etc. Spoken transliteration name recognition is useful for many applications. For example, cross language image retrieval via spoken queries aims to employ the latter in one language to retrieve images with captions in another language [Lin et al. 2004]. In the past, Appelt and Martin [1999] adapted the TextPro system to process transcripts generated by a speech recognizer. Miller et al. [2000] analyzed the effects of out-of-vocabulary errors and the loss of punctuation on name finding in automatic speech recognition. Huang and Waibel [2002] proposed an adaptive method of named entity extraction for the meeting understanding. Chen [2003] dealt with spoken cross-language access to image collections. The coverage of a lexicon is one of the major issues in spoken transliteration name access. Recently, researchers are interested in using the Web, which provides a huge collection of up-to-date data, as a corpus. Keller and Lapata [2003] employed the Web to obtain frequencies for bigrams that are unseen in a given corpus. Named entities are important objects in web documents. Building named entity relationship chains from the web is an important task. Matsuo et al. [2004] found social networks of trust from related web pages. Google sets1 extracts named entity from web pages by inputting a few named entities. For some emerging applications like personal name disambiguation [Fleischman and Hovy 2004] [Mann and Yarowsky 2003], social chain finding [Bekkerman and McCallum 2005] [Culotta et al. 2004] [Raghavan et al. 2004], etc., glossary-based representations of named entities are not enough. For collecting transliteration names and building a bi-character language model, we propose a named entity (NE) ontology generation engine, called the XNE-Tree engine. This engine produces relational named entities by given a seed. The engine uses Google to incrementally extract high co-occurrence named entities from related web pages and those named entities have similar relational properties with the seed. In each iterative step, the seed will be replaced by its siblings or descendants, which form new seeds. In this way, the XNE-Tree engine will build a tree structure as follows with the original seed as a root. Seed  
This paper presents an empirical study of word error minimization approaches for Mandarin large vocabulary continuous speech recognition (LVCSR). First, the minimum phone error (MPE) criterion, which is one of the most popular discriminative training criteria, is extensively investigated for both acoustic model training and adaptation in a Mandarin LVCSR system. Second, the word error minimization (WEM) criterion, used to rescore N-best word strings, is appropriately modified for a Mandarin LVCSR system. Finally, a series of speech recognition experiments is conducted on the MATBN Mandarin Chinese broadcast news corpus. The experiment results demonstrate that the MPE training approach reduces the character error rate (CER) by 12% for a system initially trained with the maximum likelihood (ML) approach. Meanwhile, for unsupervised acoustic model adaptation, MPE-based linear regression (MPELR) adaptation outperforms conventional maximum likelihood linear regression (MLLR) in terms of CER reduction. When the WEM decoding approach is used for N-best rescoring, a slight performance gain over the conventional maximum a posteriori (MAP) decoding method is also observed. Keywords: Broadcast News, Continuous Speech Recognition, Discriminative Training, Minimum Phone Error, Word Error Minimization  ∗ Graduate Institute of Computer and Information Engineering, National Taiwan Normal University, Taipei, Taiwan E-mail: rogerkuo@iis.sinica.edu.tw + Institute of Information Science, Academia Sinica, Taipei, Taiwan [Received February 23, 2006; Revised August 24, 2006; Accepted September 5, 2006]  202  Jen-Wei Kuo et al.  1. Introduction Due to advances in computer technology and the growth of the Internet, large volumes of multimedia content, such as broadcast news, lectures, voice mails, and digital archives continue to grow and fill our computers, networks, and lives. It is obvious that speech is the richest source of information for the large volumes of multimedia content; thus, associated speech processing technologies will play an increasingly important role in multimedia organization and retrieval in the future. Among these technologies, automatic speech recognition (ASR) has long been the focus of research in the speech processing community. Automatic speech recognition is a pattern classification task that classifies sound segments into different linguistic categories based on the acoustic vector sequence extracted from the speech signal. Traditionally, in most pattern classification applications, the goal of classifier design is to reduce the probability of errors by using the minimum error rate (MER) criterion [Duda et al. 2000]. Under this paradigm, the problems of classifier optimization are resolved by minimizing the expected loss over the training data directly. The zero-one loss function, which simply assigns no loss to a correct classification and a unit loss to an error, is often employed for this purpose. For example, in ASR, a hypothesized word sequence containing one or more word errors, or a totally different sequence, as compared to the correct sequence, will incur the same amount of loss. However, the most common performance evaluation metrics adopted in ASR often consider individual word errors, instead of merely counting the string-level errors. The use of the zero-one loss function leads to a mismatch between classifier optimization and performance evaluation. In recent years, a common practice in ASR has been to replace the zero-one loss function with alternative loss functions that consider word- or phone-level errors. In practice, such improved loss functions can be used in both model parameter estimation (i.e., classifier optimization) and speech decoding. In this paper, we present an empirical study of word error minimization approaches for Mandarin large vocabulary continuous speech recognition (LVCSR). The minimum phone error (MPE) criterion is extensively investigated in both acoustic model training and adaptation; while the word error minimization (WEM) criterion is exploited to rescore N-best word strings. The remainder of the paper is organized as follows. In Section 2, the general background of the Bayes risk and overall risk criteria is given, and their use in ASR is explained. Section 3 presents the application of the MPE criterion for acoustic model training, and Section 4 describes its extension to unsupervised linear regression based acoustic model adaptation. The use of the WEM criterion for speech decoding is discussed in Section 5. The experiment setup is detailed in Section 6 and a series of speech recognition experiments is described in Section 7. Finally, we present the conclusions drawn from the research in Section 8.  An Empirical Study of Word Error Minimization Approaches for  203  Mandarin Large Vocabulary Continuous Speech Recognition  2. Bayes Risk and Overall Risk Given an acoustic vector sequence O , the goal of an ASR system is to make a decision αu (O) that identifies O as a certain word sequence u from a hypothesized space Wh of all possible word sequences in the language. Let L(u, c) be the loss incurred by the decision αu (O) , where the correct (i.e., reference) transcription is c . Actually, we have no prior knowledge of the correct transcription; in other words, any arbitrary word sequence s in Wh could be identical to c . Consequently, for each possible decision αu (O) , the expected loss (or risk) is calculated as [Duda et al. 2000]:  R(αu (O) | O) = ∑s∈Wh L(u, s)P(s | O) ,  (1)  where P(s | O) is the posterior probability of the word sequence s given that the acoustic vector sequence O is observed. Therefore, the Bayes decision αopt (O) is made by selecting the action with the minimum expected loss, i.e.,  αopt (O) = arg min R (αu (O) | O)  u∈Wh  .  (2)  =  arg min u∈Wh  ∑ s∈Wh  L(u,  s)P(s  |  O)  In supervised training, on the other hand, the correct transcription of each training utterance O is known, and the overall risk Rall of all possible training utterances is defined as:  Rall = ∫ R(αc (O) | O)P(O)dO ,  (3)  where the integral extends over the whole acoustic space. However, in practice, we can only obtain the approximate overall risk Rall by summing the risks over a finite number of training utterances, i.e.,  Rall  =  ∑ r  R(α  cr  (Or ) | Or )P(Or )  ,  (4)  =∑ r  ∑s∈Whr L(cr , s)P(s | Or )P(Or )  where Whr and cr, respectively, denote a set of likely hypothesized word sequences and the reference word sequence associated with the training utterance Or ; and the distribution P(s | Or ) is always assumed to be governed by some underlying parametric distributions. To ensure that ASR is as accurate as possible, we need to design a classifier and estimate the parameters in P(s | Or ) more carefully in order to minimize the overall risk Rall. By applying  204  Jen-Wei Kuo et al.  the Bayes rule and replacing the probability P(Or | s) with its parameterization, pλ (Or | s) , Eq. (4) can be expressed as:  Rall = ∑ r  ∑  s∈Whr L(cr , s)pλ ∑u∈Whr pλ (Or  (Or | s)P(s) | u)P(u)  P(Or  )  ,  (5)  where pλ (Or | s) and pλ (Or | u) are, respectively, the acoustic model likelihoods for s and u under the acoustic model parameter set λ ; and P(s) and P(u) are the respective language model probabilities for s and u . The parameters of both the acoustic model and the language model can be estimated by minimizing Rall. However, in this study, we only focus on the discriminative estimation of the acoustic model parameters, and adopt the conventional approach for language model training. Moreover, it is assumed that the prior probability P(Or ) is uniformly distributed. As a result, the overall risk becomes  Rall = ∑ r  ∑s∈Whr L(cr , s)pλ (Or | s)P(s) , ∑u∈Whr pλ (Or | u)P(u)  (6)  and the optimal parameter set, λopt , can be estimated by minimizing the overall risk of the training utterances  λopt = arg min ∑ λr  ∑s∈Whr L(cr , s)pλ (Or | s)P(s) . ∑u∈Whr pλ (Or | u)P(u)  (7)  To minimize the overall risk, as shown by Equations (4) to (7), the hypothesized word sequence with a lower loss should have a larger posterior probability, and vice versa. How to select an appropriate loss function L(⋅,⋅) used in the above equations remains an open research issue. In most pattern classification tasks, to minimize the probability of classification errors, the loss function is often chosen based on the minimum error rate (MER) criterion. This leads directly to the following symmetrical zero-one loss function [Duda et al. 2000]:  L(u,  s)  =  ⎧0 ⎨⎩1  ,u = s . ,u ≠ s  (8)  The loss function assigns no loss if u = s , and assigns a unit loss when a classification error occurs. In ASR, a hypothesized word sequence that is identical to the correct transcription does not introduce a loss; however, a hypothesized word sequence containing one or more  An Empirical Study of Word Error Minimization Approaches for  205  Mandarin Large Vocabulary Continuous Speech Recognition  不斷  豪雨 不斷  SIL 妙語  無端 不斷  不斷 陶藝  無端  太多 良心 台東  台東  兩人  失蹤  台中 兩人 失蹤  兩人  失蹤  太重 兩任  Figure 1. A word lattice can efficiently encode a large number of possible hypothesized word sequences.  word errors, or a totally different sequence, compared to the correct sequence, will incur the same unit loss. Thus, minimizing the overall risk is equivalent to minimizing the expected string error rate (SER) of the training utterances. Nevertheless, SER is not a sufficient metric for the evaluation of ASR performance because, with this metric, all incorrectly hypothesized word sequences are regarded as having the same cost of recognition risk. Instead, the loss function could be defined as the distance of the hypothesized word sequence to the correct transcription. For this purpose, the string edit or Levenshtein distance [Levenshtein 1966] associated with the word error rate (WER) can be adopted. It is believed that WER is more suitable than SER in reflecting differences in ASR results. Optimization using the Levenshtein-based loss function is often referred to as word error minimization (WEM). However, in complicated ASR tasks, such as LVCSR, it is impossible to perform optimization over the hypothesized space Whr of each training utterance Or without using a pruning technique because such hypothesized spaces usually contain an extremely large number of hypothesized word sequences. Recently, some practical strategies have been proposed to resolve this problem. For instance, a reduced hypothesized space in the form of an N-best list [Schwartz and Chow 1990] or a lattice [Ortmanns 1997] can be generated for each training utterance by only retaining recognized hypotheses with higher probabilities. The optimization process can then be applied efficiently to the reduced hypothesized space. Figure 1 illustrates an example of a word lattice.  206  Jen-Wei Kuo et al.  3. Minimum Phone Error (MPE) Training This section describes in detail the application of the minimum phone error (MPE) criterion to acoustic model training. As mentioned in the previous section, the hypothesized space Whr of a given training utterance Or can be reduced to a smaller space represented by a number of the most likely hypothesized word sequences associated with Or . The N-best list contains the N most likely sequences generated by applying the Viterbi algorithm, which has to retain at least N-best search hypotheses at both the HMM (Hidden Markov Model) acoustic model-level and word-level recombination points during the speech decoding process. For each hypothesized word sequence on the N-best list, it is relatively easy to compute the standard Levenshtein distance to the correct transcription directly. Based on this observation, Kaiser et al. proposed overall risk criterion estimation (ORCE) for acoustic model training [Kaiser et al. 2000, 2002; Na et al. 1995]. This approach takes the N-best list as the reduced hypothesized space to obtain training statistics, and applies the extended Baum-Welch algorithm [Gopalakrishnan et al. 1991; Normandin 1991] for parameter optimization. In experiments on the TIMIT database, the authors achieved a 21% word error rate reduction compared to the baseline system. However, an N-best list usually contains too much redundant information, i.e., two hypothesized word sequences may look very similar, which makes the training procedure inefficient. An alternative representation is the word lattice (or graph), illustrated in Figure 1, which only stores hypothesized word arcs at different segments of the time frames. Although it cannot be guaranteed that all word sequences generated from a word lattice will have higher probabilities than those not presented, it is believed that the approximation will not affect the performance significantly. Nevertheless, for the lattice structure, using the standard Levenshtein distance measure as the loss function is an issue, since it makes the implementation of computing the distance more complicated. Recently, two approaches have been proposed to deal with this problem. One focuses on how to design loss functions that approximate the Levenshtein distance measure, such as MPE training. The other concentrates on the design of algorithms to segment the word lattice so as to make the computation of the Levenshtein distance feasible, such as the minimum Bayes risk discriminative training (MBRDT) approach [Doumpiotis et al. 2003, 2004]. To efficiently reduce the complexity of the hypothesized space in MBRDT, a lattice segmentation algorithm is applied to divide the lattice into several non-overlapping components. It has been shown that MBRDT achieves a considerable performance improvement over the baseline system trained with the maximum likelihood (ML) criterion. The MPE training approach, which is one of the most attractive discriminative training techniques, tries to optimize an acoustic model’s parameters by minimizing the expected phone error rate. The objective function of MPE is given as [Povey 2004]:  An Empirical Study of Word Error Minimization Approaches for  207  Mandarin Large Vocabulary Continuous Speech Recognition  Reference transcription 比 0 Word arc “好在” on lattice  他  好  太多  好在  86  33  65  Reference phone alignment  b_i i  t_a  a  h_a au t_a ai d_u uo  (4) (10) (9)  (12)  (10) (8) (6) (8) (8) (12)  2 82  04  14 23  35  45 53 59 67 75  86  Phone boundary on word arc “好在”  h_a  au tz_a ai  33  43  55 58 65  e: proportion of length of phone “au” correct phone=au : -1+2*e correct phone!=au : -1+e  h_a au t_a 2/10 8/8 2/6 =0.2 =1.0 =0.33 -0.8 1.0 -0.67 max  Raw accuracy of phone “au” = max(-0.8, 1.0, -0.67)  1.0  Figure 2. Raw phone accuracy calculation.  FMPE (λ) = ∑ r  ∑s∈Wlrat pλ (Or | s)P(s) A(cr , s) , ∑u∈Wlrat pλ (Or | u)P(u)  (9)  where Wlrat is the lattice generated by the speech recognizer, used to represent a reduced hypothesized space of word sequences; and A(cr , s) is the raw accuracy of word sequence s , which is an approximation of the true accuracy computed globally using the standard Levenshtein distance. It is obvious that maximizing the objective function is equivalent to minimizing the expected phone error. The raw accuracy A(cr , s) is defined as:  A(cr , s) = ∑ A′(cr , q) ,  (10)  q∈s  where q is the phone involved in s , and A′(cr , q) is a local function used to calculate the raw phone accuracy of each phone q in s . The phone accuracy is calculated locally on each phone arc of the word lattice, instead of globally on each hypothesized word sequence. Given a word arc on the word lattice, the time boundaries of the phone arcs can be determined by aligning the corresponding speech segment with its constituent HMM acoustic models. Figure 2 shows the calculation of raw phone accuracy. Notice that we adopt INITIAL/FINAL units instead of phone units as the acoustic units in our Mandarin LVCSR system. Therefore, for  208  Jen-Wei Kuo et al.  Reference phone transcription (cr ) Hypothesized phone sequence (s)  b_i i  t_a  a  h_a au t_a ai d_u  uo  (4) (10) (9)  (12)  (10) (8) (6) (8) (8) (12)  04  14  23  35  45 53 59 67 75  86  ch_i i  t_a  a  h_a  au tz_a ai d_u  u  03  14 22  33  43  55 58 65  74  86  Raw phone accuracy -0.25 1.0 0.78 0.67 0.6  1.0 -0.5 0.5 0.75 0.0  Raw accuracy of the hypothesized phone sequence = 4.55  True accuracy of the hypothesized phone sequence = 7 Figure 3. Approximate accuracy versus exact accuracy.  simplicity, each INITIAL or FINAL unit is regarded as a phone in the elucidation. In Figure 2, the raw phone accuracy of phone “au” involved in the word arc “好在” is calculated in the following steps. First, the word arc “好在” is aligned with time boundaries of a phone sequence to obtain the start and end time boundaries of the phone “au”. Second, for each phone q′ in the correct transcription, we calculate the overlapped portion of “au” in time frames, and denote it as e(q′,"au ") . Finally, the raw phone accuracy of phone “au”, i.e., A′(cr ,"au ") , is calculated using the following formula:  A′(cr  ,  "  au  ")  =  max q′  ⎧−1+ 2e("au ", q′)  ⎨ ⎩  −1 +  e(" au  ", q′)  if q′ = "au " . otherwise  (11)  It is obvious that A′(cr ,"au ") ranges from 1 to -1+ 1/ Tr , where Tr is the length of observation Or in terms of the time frames. For example, if the phone arc “au” overlays at least one phone q′ in the correct transcription with the same identity in time, “au” is considered to be a correct phone, i.e., A′(cr ,"au ") = 1 . Figure 3 compares the accuracy of a hypothesized word sequence obtained via the approximate function discussed here and the exact calculation using the Levenshtein distance. According to Povey’s work [Povey 2004], the auxiliary function for optimizing the objective function of MPE in Eq. (9) is  HMPE (λ, λ ) = ∑r ∑ q∈Wlrat  ∂FMPE (λ) ∂ log pλ (Or |  q)  λ=λ  log  p(Or  |  q)  ,  (12)  An Empirical Study of Word Error Minimization Approaches for  209  Mandarin Large Vocabulary Continuous Speech Recognition  where λ is the current model parameter set, q is a specific phone arc in Wlrat , and pλ (Or | q) is the likelihood given the phone arc q . Note that HMPE (λ, λ ) is a weak-sense auxiliary function of FMPE (λ) around λ = λ with the following property:  ∂FMPE (λ)  = ∂HMPE (λ, λ ) .  (13)  ∂λ λ=λ  ∂λ  λ =λ  In other words, both the objective and auxiliary functions have the same derivative with respect to λ when they are evaluated at the current estimate λ . For simplicity, we only consider the MPE-based estimation of mean vectors and covariance matrices in HMMs. The state transition probabilities and mixture weights trained by the ML criterion remain unchanged. As a result, in this study, the final auxiliary function for MPE training is expressed as:  gMPE (λ, λ ) = ∑r  ∑  t =eq ∑  ∑  γ  r q  MPE  γ  r qm  (t  )  log  N  (or  (t),  µm  ,  Σm  )  ,  (14)  q∈Wlrat t =sq m  where sq and eq represent the start and end times of the phone arc q , respectively; m is  the mixture index of the acoustic models; µm and Σm are, respectively, the mean vector  and covariance matrix for mixture  m;  γ  r qm  (t  )  is the occupation probability for mixture  m  on  q;  or (t)  is the observation vector at time  t;  and  γ  r q  MPE  represents  ∂FMPE (λ)  in Eq. (12), which can be expressed as:  ∂ log pλ (Or | q) λ=λ  ∂FMPE (λ)  ∑ pλ (Or | v′)P(v′) A(v′, sr ) ∑ pλ (Or | u′)P(u′)  = v′∈Wlrat ,q∈v′  u′∈Wlrat ,q∈u′  ∂ log pλ (Or | q) λ=λ  ∑ pλ (Or | u′)P(u′) u′∈Wlrat ,q∈u′  ∑ pλ (Or | u)P(u)  u∈Wlrat  . (15)  ∑ pλ (Or | v)P(v) A(v, sr ) ∑ pλ (Or | u′)P(u′)  − v∈Wlrat  u′∈Wlrat ,q∈u′  ∑ pλ (Or | u)P(u)  ∑ pλ (Or | u)P(u)  u∈Wlrat  u∈Wlrat  In Eq. (15),  ∑ pλ (Or | u′)P(u′) u′∈Wlrat ,q∈u′  is the occupation probability of phone arc  q;  ∑ pλ (Or | u)P(u) u∈Wlrat  ∑ pλ (Or | v′)P(v′) A(v′, sr ) v′∈Wlrat ,q∈v′  is the weighted average accuracy of hypothesized word sequences  ∑ pλ (Or | u′)P(u′) u′∈Wlrat ,q∈u′  210  Jen-Wei Kuo et al.  in  Wlrat  that include  q ; and  ∑ pλ (Or | v)P(v) A(v, sr ) v∈Wlrat  is the weighted average accuracy of all  ∑ pλ (Or | u)P(u) u∈Wlrat  hypothesized word sequences in Wlrat . All three quantities can be calculated efficiently.  Since maximizing the weak sense auxiliary function with respect to λ does not  guarantee an increase in the objective function, the auxiliary function is augmented with an  extra smoothing function  g  smooth EB  (λ  ,  λ  )  to moderate the parameter update and prevent  extreme parameter values being estimated. The following is an example of a smoothing  function:  g  smooth EB  (λ,  λ  )  =  ∑  m  −  Dm 2  ⎡⎣log(| Σm  |) + (µm  −  µm )T  Σm−1(µm  −  µm  )  +  tr(Σm  Σ  m−1 )  ⎤ ⎦  ,  (16)  where  D m  is a per-mixture level controlling constant. Note that  g  smooth EB  (λ  ,  λ  )  is deemed a  log-Gaussian prior distribution with a differential value of zero with respect to λ when it is  evaluated at the current estimate λ . Therefore, the differentials of the augmented auxiliary  function with respect to µm and Σm are computed as shown, respectively, in the following  equations:  ( ) ∂(gMPE  (λ, λ ) + gEsmBmoth (λ, λ )) ∂µm  =  ∑r  ∑ q∈Wlrat  t =eq ∑ t =sq  γ  r q  MPEγ  r qm  (t  )Σm−1  or (t) − µm  − Dm ⎡⎣Σm−1(µm − µm )⎤⎦ ,  (17)  ( ) ( )( ) ∂(gMPE (λ,λ ) + gEsmBooth (λ,λ )) ∂Σ−m1  =  ∑r  ∑ q∈Wlrat  t =eq ∑ t =sq  γ  r q  MPE γ  r qm  (t)  ⎡ ⎢⎣  
Using lexical semantic knowledge to solve natural language processing problems has been getting popular in recent years. Because semantic processing relies heavily on lexical semantic knowledge, the construction of lexical semantic databases has become urgent. WordNet is the most famous English semantic knowledge database at present; many researches of word sense disambiguation adopt it as a standard. Because of the success of WordNet, there is a trend to construct WordNet in different languages. In this paper, we propose a methodology for constructing Chinese WordNet by extracting information from a bilingual terminology bank. We developed an algorithm of word-to-word alignment to extract the English-Chinese translation-equivalent word pairs first. Then, the algorithm disambiguates word senses and maps Chinese word senses to WordNet synsets to achieve the goal. In the word-to-word alignment experiment, this alignment algorithm achieves the f-score of 98.4%. In the word sense disambiguation experiment, the extracted senses cover 36.89% of WordNet synsets and the accuracy of the three proposed disambiguation rules achieve the accuracies of 80%, 83% and 87%, respectively. Keywords: Word Alignment, Word Sense Disambiguation, WordNet, EM Algorithm, Sense Tagging. 1. Introduction Using lexical semantic knowledge to solve natural language processing problems has been getting popular in recent years. Especially for word sense disambiguation, the semantic lexicon plays a very important role. However, all semantic approaches depend on knowledge of some well established semantic lexical databases which provide semantic information of ∗ Institute of Information Science, Academia Sinica E-mail: mhbai@sinica.edu.tw; kchen@iis.sinica.edu.tw + Department of Computer Science, National Tsing Hua University E-mail: jschang@cs.nthu.edu.tw [Received February 16, 2006; Revised November 28, 2006; Accepted November 30, 2006]  224  Ming-Hong Bai et al.  words, such as the different senses of a word, the synonymous or hyperonymy relation between words, etc. WordNet is a famous semantic lexical database which owns rich lexical information. [Miller 1990]. It not only covers a large set of vocabularies but also establishes a complete taxonomic structure for word senses. Synonymous word senses are grouped into synsets. These synsets are further associated by semantic relations, including hypernyms, hyponyms, holonyms, meronyms, etc. The WordNet has been applied to a wide range of applications, such as word sense disambiguation, information retrieval, computer-assisted language learning, etc. It has apparently become the de facto standard for English word senses now. Because of the success of WordNet, there is a universally shared interest in construction of WordNet-like and WordNet-embedded lexical databases in different languages. One of the most famous projects is EuroWordNet (EWN). Its goal is to construct a WordNet-like system containing several European languages. Since constructing a WordNet for a new language is a difficult and labor intensive task, using the resources of WordNet to speed up the construction has begun a new trend. Many researchers, such as [Atserias et al. 1997], [Daude et al. 1999] and [Chang et al. 2003], have tried to associate WordNet synsets to other languages automatically with appropriate translations from bilingual dictionaries. The limitation of using bilingual dictionaries as mapping tables for translation equivalences between two languages is the narrow scopes of the dictionaries, since dictionaries usually contain prototypical translations only. For example, the first sense of word "plant" in WordNet is "plant, works, industrial plant"; it was translated as "GongChang"(工廠) in a Chinese-English bilingual dictionary. However, in actual text, it may be also translated as "Chang"( 廠 ), "GongChang"(工場), "ChangFang"(廠房), "suo"(所, such as ‘power plant’/發電所), etc. Various translations, obviously, add complexity and difficulty to map word senses into WordNet synsets. Instead of using bilingual dictionaries, we adopt a bilingual terminology bank as the semantic lexical database. The latter includes various compound words, in which a word in a different compounding structure may have different translations, thus there are more translation candidates which can be chosen. A bilingual terminology bank has not only helped to avoid the problem of the limited scope of prototypical translations made by common bilingual dictionaries, but has also helped to disambiguate word senses by various translations and collocations [Diab et al. 2002], [Bhattacharya 2004]. Nevertheless, using bilingual terminology banks has to face two main challenges: Firstly, we have to deal with the problem of word-to-word alignment for multi-words terms. Secondly, we have to solve the problem of sense ambiguity of the English translation. The approaches for solving these two problems are the major focuses of the paper. The rest of paper is divided into four sections. Section 2 introduces the resources of this  Sense Extraction and Disambiguation for  225  Chinese Words from Bilingual Terminology Bank  paper. Section 3 describes the methodology. Experimental setup and results will be addressed in Section 4. A conclusion is provided in Section 5 along with directions for future research.  2. Resources  In this study, we use two dictionaries as the resources to extract semantic information:  a) The Bilingual Terminology Bank from NICT [NICT 2004]  b)A English-Chinese dictionary [Proctor 1988]  The Bilingual Terminology Bank from NICT contains 63 classes of terminologies, with a total of 1,046,058 Chinese terms with their English translations. Among them, 629,352 terms are compounds, which is about 60 percent of the total. The English-Chinese dictionary contains 208,163 words which are used as a supplement. We also adopt WordNet 2.0 as the medium for sense linking. Figure 1 shows some sample entries of the Bilingual Terminology Bank from NICT.  English succulent stem  Chinese 肉質莖  Class Botany  common base current gain  共基電流增益  Electrical Engineering  sliding brush  滑動電刷  Naval Architecture  point of increase  增值點  Mathematics  group carry  成組進位  Computer Science  swine fever  豬瘟  Animal Science  light measurements  光量測  Metrology  reductional grouping  染色體減數分群 Botany  oil film strength  油膜強度  Metrology  normalized quadrature spectrum  標準化四分譜  Meteorology  Figure 1. sample entries of the Bilingual Terminology Bank from NICT.  In English, a compound is usually composed of words and blanks; the latter being a natural boundary to separate words. On the contrary, in Chinese there are no blanks in compound words, so we need to segment words before applying word alignment algorithms. In this paper, we adopt the CKIP Chinese Word Segmentation System, which was developed by the CKIP group of Academia Sinica [CKIP 2006].  3. Methodology  The algorithm can be divided into the following two steps: 1. Find the word to word alignment for each entry in the terminology bank,  226  Ming-Hong Bai et al.  2. Assign a synset to the Chinese word sense by resolving the sense ambiguities of its aligned English word. The first step is to find all possible English translations for each Chinese word, which make it possible to link Chinese words to WordNet synsets. Since the English translation may be ambiguous, the purpose of second step is to employ a word sense disambiguation algorithm to select the appropriate synset for the Chinese word. For example, the term pair (water tank, 水 槽 ) will be aligned as (water/水 tank/槽 ) in the first step, so the Chinese word 槽 can be linked to WordNet synsets by its translation tank. But tank has five senses in WordNet as follows: tank_n_1: an enclosed armored military vehicle, tank_n_2: a large vessel for holding gases or liquids, tank_n_3: as much as a tank will hold, tank_n_4: a freight car that transports liquids or gases in bulk, tank_n_5: a cell for violent prisoners. The second step is applied to select the best sense translation. In the following subsections, we will describe the detail algorithm of word alignment in section 3.1 and word sense disambiguation in section 3.2. 3.1 Word Alignment For a Chinese term and its English translation, it is natural to think that the Chinese term is translated from the English term word for word. So, the purpose of word alignment is to connect the words which have a translation relationship between the Chinese term and its English portion. In past years, several statistical-based word alignment methods have been proposed. [Brown et al. 1993] proposed a method of word alignment which consists of five translation models, also known as the IBM translation models. Each model focuses on some features of a sentence pair to estimate the translation probability. [Vogel et al. 1996] proposed the Hidden-Markov alignment model which makes the alignment probabilities dependent on the alignment position of the previous word rather than on the absolute positions. [Och and Ney 2000] proposed some methods to adjust the IBM models to improve alignment performance. The word alignment task in this paper only focuses on the term pairs of a bilingual terminology bank. Since the length of a term is usually far less than a sentence, some features, such as word position, are no longer important in the task. In this paper, we employ the IBM-1 model, which only focuses on lexical generating probability, to align the words of a bilingual terminology bank.  Sense Extraction and Disambiguation for  227  Chinese Words from Bilingual Terminology Bank  3.1.1 Modeling Word Alignment For convenience, we follow the notion of [Brown et al. 1993], which defines word alignment as follows: Suppose we have a English term e = e1,e2,…,en where ei is an English word, and its corresponding Chinese term c = c1,c2,…,cm where cj is a Chinese word. An alignment from e to c can be represented by a series a=a1,a2,…,am where each aj is an integer between 0 and n, such that if cj is partial (or total) translation of ei , then aj = i and if it is not translation of any English word, then aj=0. For example, the alignments shown in Figure 2 are two possible alignments from English to Chinese for the term pair (practice teaching, 教學 實習), (a) can be represented by a=1,2 while (b) can be represented by a=2,1.  (a)  (b)  Figure 2. two possible alignments from English to Chinese for the term pair (practice teaching, 教學 實習).  In the word alignment stage, given a pair of terms c and e, we want to find the most likely alignment a=a1,a2,…,am , to maximize the alignment probability P(a|c,e) for the pair. The formula can be represented as follows:  aˆ = arg max P(a | c,e) ,  (1)  a  where aˆ is the best alignment of the possible alignments. Suppose we already have lexical translation probabilities for each of the lexical pairs, then, the alignment probability P(a|c,e) can be estimated by means of the lexical translation probabilities as follows:  P(a  | c,e)  =  P(a,c | e) P(c | e)  =  m ∏ P(c j j =1  | ea j  )/  P(c | e) .  The probability of c given e, P(c|e), is a constant for a given term pair (c,e), so formula 1 can be estimated as follows:  228  Ming-Hong Bai et al.  m  aˆ  =  arg max a  ∏ P(c j j =1  |  ea j  )  .  (2)  For example, the probability of the alignment shown in Figure 2 (a) can be estimated by: P(c1|e1)P(c2|e2) = P( 教學 | practice) P( 實習 | teaching) = 0.000480 x 1.14x10-13 =5.48x10-17.  While (b) can be estimated by: P(c1|e2)p(c2|e1) = P( 教學 | teaching)P( 實習 | practice ) = 0.6953 x 0.0940 = 0.0654.  In this example, the probability of alignment (b) is larger than (a) in Figure 2. So the alignment (b), (教學/teaching 實習/practice), is a better choice than (a), (教學/practice 實習 /teaching), for the term pair (practice teaching, 教學 實習). The remaining problem of this stage is how to estimate the translation probability p(c|e) for all possible English-Chinese lexical pairs.  3.1.2 Translation Probability Estimation The method of our translation probability estimation uses the IBM model 1 [Brown et al. 1993], which is based on the EM algorithm [Dempster et al. 1977], for maximizing the likelihood of generating the Chinese terms, which is the target language, given the English portion, which is the source language. Suppose we have an English term e and its Chinese translation c in the terminology bank T; e is a word in e, and c is a word in c. The probability of word c given word e, P(c|e), can be estimated by iteratively re-estimating the following EM formulae: Initialization:  P(c | e) = 1 ;  (3)  |C|  E-step:  m  Z  (c,  e;  c,e)  =  ∑ ∀a  P(a  |  c,  e)  ∑δ j =1  (c,  c  j  )δ  (e,  ea  j  )  ,  (4)  P(a | c,e) =  P(a,c | e)  =  ∏  m j =1  P(c  j  |  ea j  )  ;  (5)  ∑ P(a ',c | e) ∀a '  ∑  ∏  m j =1  P(c  j  ∀a '  |  ea ' j  )  Sense Extraction and Disambiguation for  229  Chinese Words from Bilingual Terminology Bank  M-step:  P(c | e) =  ∑|tT=|1 Z (c, e;c(t) , e(t) ) . ∑ ∑|tT=|1 Z (v, e;c(t) , e(t) )  (6)  ∀v∈C  In the EM training process, we initially assume that the translation probability for any Chinese word c given English word e, P(c|e), is uniformly distributed as in formula 3, where C denotes the set of all Chinese words in the terminology bank. In the E-step, we estimate the expected number of times that e connects to c in the term pair (c,e). As in formula 4, we sum up the expected counts of the connection from e to c over all possible alignments which contain the connection. Formula 5 is the detailed definition of the probability of an alignment a given (c,e). Usually, it is hard to evaluate the formulae in E-step. Fortunately, it has been proven [Brown et al. 1993] that the expectation formulae, 4 and 5, can be merged and simplified as follows:  m  Z  (c,  e; c ,e)  =  ∑ a  P(a  |  c,  e )  ∑δ j =1  (c,  c  j  )δ  (e,  ea  j  )  =  ∑  ∏  m j =1  P(c  j  a  |  ea j  )∑mj=1δ (c, c j )δ (e, eaj  )  ∑  ∏  m j =1  P(c  j  ∀a '  |  ea ' j  )  =  P(c  |  e)∏  m j =1,c  j  ≠c  ∑  n i=0,ei  ≠  e  P  (c  j  ∏  m j =1  ∑in=0  P(c j  |  ei )  | ei )  m  n  ∑ δ (c, c j ) ∑ δ (e, ei )  j =1  i=0  =  P(c | e) ∑in=0 P(c | ei )  m  n  ∑ δ (c, c j ) ∑ δ (e, ei ) .  j =1  i=0  (7)  After merging and simplifying, as formula 7, the E-step becomes very simple and effective for computing.  In the M-step, we re-estimate the translation probability, P(c|e). As shown in formula 6, we sum up the expected number of connections from e to c over the whole bank divide by the expected number of c.  The training process will count the expected number, E-step, and re-estimate the translation probability, M-step, iteratively until it has converged.  For instance, as the example shown in Figure 2, the English term e= practice teaching and Chinese term c=教學 實習 are given. Assume the total number of Chinese words in the terminology bank is 100,000. Initially, the probabilities of each translation are as follows:  230  Ming-Hong Bai et al.  P( 教學 | practice) = 1 = 0.00001, |C|  P( 教學 | teaching) = 1 = 0.00001, |C|  P( 實習 | practice) = 1 = 0.00001, |C|  P( 實習 | teaching) = 1 = 0.00001. |C|  In E-step, we count the expected number for all possible connections in the term pair:  Z( 教學 , practice; e, c ) =  P(教學 | practice)  = 0.5,  P(教學 | practice) + P(教學 | teaching)  Z( 教學 , teaching; e, c ) =  P(教學  |  P(教學 | teaching) practice) + P(教學 |  teaching )  =  0.5,  Z( 實習 , practice; e, c ) =  P(實習  |  P(實習 | practice) practice) + P(實習 |  teaching )  =  0.5,  Z( 實習 , teaching; e, c ) =  P(實習  |  P(實習 | practice) practice) + P(實習 |  teaching )  =  0.5.  In M-step, we first count the global expected number of each translation by summing up the expected number of each data entry over the whole term bank:  |T | ∑  Z  (教學,  practice; e(t )  ,  c(t )  )  =0.7,  t =1  |T | ∑  Z  (教學,  teaching ; e(t )  ,  c(t)  )  =  43.72,  t =1  |T | ∑  Z  (實習,  practice; e(t )  , c(t )  )  =  5.37,  t =1  |T | ∑  Z  (實習, teaching;e(t)  , c(t )  )  =  0.95.  t =1  After the global expected number of each translation has been counted, we can re-estimate the translation probabilities by means of the expected numbers:  P( 教學 | practice) =  ∑|tT=|1Z (教學, practice;e(t) , c(t) ) ∑v∈C ∑|tT=|1Z (v, practice;e(t) , c(t) )  =  0.7 110.67  =  0.00632,  |T | ∑  Z  (教學, teaching; e(t )  , c(t )  )  P( 教學 | teaching) = t=1  = 43.72 = 0.35871,  ∑v∈C ∑|tT=|1Z (v,teaching;e(t) ,c(t) ) 121.88  Sense Extraction and Disambiguation for  231  Chinese Words from Bilingual Terminology Bank  |T | ∑  Z  (實習,  practice; e(t )  , c(t )  )  P( 實習 | practice) = t=1  = 5.37 = 0.04852,  ∑v∈C ∑|tT=|1Z (v, practice;e(t) ,c(t) ) 110.67  |T | ∑  Z  (實習,  teaching;  e(t  )  ,  c(t )  )  P( 實習 | teaching) = t=1  = 0.95 = 0.00779.  ∑v∈C ∑|tT=|1Z (v,teaching;e(t) ,c(t) ) 121.88  The training process will count the expected number and re-estimate the translation iteratively until it has converged. There are some translation probabilities estimated in this experiment shown in Figures 3-6.  English Chinese P( c | e )  water water water water water water water water water water  水 水位 水分 用水 地下水 水壓 水量 水管 位 水面  0.599932 0.048781 0.011677 0.011427 0.010800 0.009310 0.007905 0.007640 0.007471 0.006704  Figure 3. translation probabilities for water.  English Chinese P( c | e )  tank  槽  0.292606  tank  櫃  0.176049  tank  艙  0.077515  tank  箱  0.034325  tank  水  0.025067  tank  液  0.018411  tank  水槽  0.016570  tank  池  0.016157 
This paper aims to further probe into the problems of ambiguities for automatic identification of determinative-measure compounds (DMs) in Chinese and to develop sets of rules to identify DMs and their parts of speech. It is known that Chinese DMs are identifiable by regular expressions. DM rule matching helps one solve word segmentation ambiguities, and parts of speech help one improve sense recognition and part-of-speech tagging. In this paper, a deep analysis based on corpus data was studied. With analyses of error identification and disambiguation of DM compounds, the authors classified three types of ambiguities, i.e. word segmentation, sense, and pos ambiguities. DM rules are necessary complements to dictionaries and helpful to resolve word segmentation ambiguities by applying resolution principles and segmentation models. Sense and pos ambiguities are also expected to be resolved by different approaches during postprocessing. Keywords: Ambiguities, Word Segmentation Ambiguities, Sense Ambiguities, Part-of Speech Ambiguities, Determinative-Measure Compounds 1. Introduction To a speaker of English, one of the most striking features of the Mandarin noun phrase is the classifier. A classifier is a word that must occur with a number and/or a demonstrative, or certain quantifiers before the noun [Li and Thompson 1981]. Furthermore, Li and Thompson [1981] assert that any measure word can be a classifier, so the combination of demonstrative and/or number or quantifier plus a classifier or a measure is defined as a classifier phrase or a measure phrase. For example, san ge in san ge ren ‘three people’ (三個人), zhe zhan in zhe zhan deng ‘this lamp’ (這盞燈), ji jian in ji jian yifu ‘a few / how many garments’ (幾件衣 服), liu li in liu li lu ‘six miles of road’ (六里路), na jin in na jin yangrou ‘that tael of lamb’ ∗ Institute of Information Science, Academia Sinica, Taipei E-mail: {shihmin, jess}@hp.iis.sinica.edu.tw; {glaxy, kchen}@iis.sinica.edu.tw + Graduate Institute of Linguistics, National Chengchi University, Taipei E-mail: 95555501@nccu.edu.tw [Received February 16, 2006; Revised November 27, 2006; Accepted November 28, 2006]  246  Shih-Min Li et al.  (那斤羊肉) and ji gang in ji gang cu ‘a few / how many vats of vinegar’ (幾缸醋) are classifier phrases / measure phrases, which are regarded as Determinative-Measure (DM) compounds in Chao [1968]. A determinative (D) and a measure normally make a compound with unlimited versatility and form a transient word of no lexical import [Chao 1968]. Although the demonstratives, numerals, and measures may be listed exhaustedly, their combination is inexhaustible. It is impossible to list thoroughly all combinations of DMs in dictionaries. Therefore, it requires a representational model to express DM compounds in Chinese NLP. In Chinese, word segmentation, sense, and pos (i.e. part-of-speech) ambiguities commonly occur in certain constructions of DMs or DM-like structures. For examples:  (1) 三個月餅舖的銷售量  a. sange  yuebingpu  de xiaoshouliang  three-M moon-cake store  DE sales volume  three moon-cake stores’ sales volume  b. sangeyue  bingpu de xiaoshouliang  three-M months cake store DE sales volume  the cake store’s three-month sales volume  (2) 取此名  a. qu  ciming  choose this-M  choose this one (person)  b. qu  ci ming  name this name  name it this name  (3) 二十五年的審核、排隊、等待  a. ershiwunian  de shenhe paidui dengdai  twenty five-M DE examine line up wait  examining, lining up and waiting in the year of twenty five  b. ershiwunian de shenhe paidui dengdai  twenty five-M DE examine line up wait  examining, lining up and waiting for twenty five years  A Probe into Ambiguities of Determinative-Measure Compounds  247  The DM compound sange in example (1) modifies moon-cake stores as well as months. The string sangeyuebingpu can be segmented into either sange yuebingpu or sangeyue bingpu, which has word segmentation ambiguity. In example (2), ming functions either as a measure or as a noun. Although example (2) has two meanings and is sense ambiguous, the roles assigned to ciming in (2a) and (2b) are both the goal. In example (3), ershiwunian is a time referent, and it can either be a time point specifying the event-time of the verb or denote the period of time delimitating the time length of the event. In Sinica Treebank, no matter whether ershiwunian behaves as a time point or a time length, ershiwunian is tagged as a unit. When ershiwunian behaves as a time point, its pos is Ndaad and semantic role is time1. Furthermore, when ershiwunian behaves as a time length, its pos is DM and semantic role is duration. However, according to CKIP’s word segmentation standard of Sinica Corpus, the temporal and locative DM structures are combined together when the meaning of the structure is not obtained from the composition of the components of the structure. Therefore, the temporal DM ershiwunian in (3a) is combined as a unit and tagged as Nd in Sinica Corpus while that in (3b) is segmented into two units, i.e. ershiwu and nian, and tagged as Neu and Nf individually. Therefore, example (3) has pos ambiguity. The different degrees of ambiguities are shown in examples (1) to (3). In this paper, we examine and analyze Mandarin Chinese DMs in Sinica Corpus and Sinica Treebank. In section 3, we introduce the regular expression approach to identify DMs and their poses. In section 4, we make a study of the structures and ambiguities of DMs, and then try to analyze and disambiguate these DMs. Section 5 is for implementation and evaluation. 2. Literature Review To deal with DMs, first one must give a proper definition to DMs. Thus, one can delimit the scope of the discussion. There are numerous discussions on determinatives as well as measures, especially on the types of measures.2 The classification of measures is beyond the scope of this paper. To avoid confusion between classifiers and measures, one must pay attention to the relation between them. Tai [1994] asserts that in the literature on general grammar as well as Chinese grammar, classifiers and measures words are often treated together under one single framework of analysis. Chao [1968] treats classifiers as one kind of measure. In his definition, a measure is a bound morpheme which forms a DM compound with 
To foreigners, how to manage tone is the greatest challenge in learning Chinese. What causes foreign students to be unable to distinguish different tones is the phonological system of their native language. The accent in standard Japanese (Tokyo dialect) is distributed in the pitch change within each syllable, and the first syllable must be the opposite of the second in accent. The discrepancy between the tonal production of Japanese students learning Chinese and that of Chinese native speakers was investigated in this study. It is found that the two Japanese students in this study made the most frequent mistakes in reading Chinese disyllabic words when the first syllable was tone 2 or tone 3, and the tonal errors were mostly found in disyllabic words with tone combinations of 2-1, 2-4, and 3-4. We also found that in Group B (2-1, 2-2, 2-3, 2-4), whatever the original tones were, the two subjects always mispronounced them as 2-3. This is primarily attributed to the fact that, in Japanese, only one pitch peak is allowed in disyllabic compounds. Keywords: Japanese Students Learning Chinese, Disyllabic Words, Tonal Errors 1. Introduction One of the most distinct features of Chinese is tone, in which each syllable has its own fixed tone, including both high-low distinctions and rising-falling variations. The acoustic characteristics of tones are mainly determined by pitch value. Tones are relatively defined. This so called “relativity” is the stability of pitch within the pitch range of an individual speaker.  ∗ Graduate Institute of Teaching Chinese as a Second Language, National Kaohsiung Normal University, Kaohsiung, Taiwan + Department of Foreign Languages and Literature, National Cheng Kung University, 1 University Rd., Tainan, Taiwan. Phone: (06)2757575 ext 52231 E-mail: leemay@mail.ncku.edu.tw The author for correspondence is Li-Mei Chen. [Received February 16, 2006; Revised August 11, 2006; Accepted September 5, 2006]  282  Ke-Jia Chang et al.  Some educators have suggested that Chinese learners could compensate for their tonal errors by practicing monosyllabic words. Yet, in actual classroom settings, it is found that the practice of tone combination is more important, especially that of disyllabic words. This is because both the learners and teachers often neglect the collaborative pattern of tones in spontaneous speech, such as the rules of tone sandhi and the patterns of tone combination. Language teaching should aim at a definite goal, and the teaching of tone combination ought to be focused on disyllabic words [Zhu 1997]. Zhu’s argument is grounded in the following two reasons. First, almost all combination patterns of monosyllabic words in spontaneous speech are included in disyllabic words. Therefore, disyllabic words could be regarded as the foundation. Second, modern Chinese is mostly made up of disyllabic words. Practicing disyllabic words could solve most problems in tone combinations. The changes of Chinese tone in connected speech pose a serious problem to Chinese learners. It is also found in classroom settings that Japanese students often stumble in communication because of their tonal errors. This paper studies the phenomenon of tonal errors in disyllabic words made by Japanese students learning Chinese, particularly in finding which tones these errors mostly occur in. It also investigates the negative transfer effect of the Japanese accent in learning Chinese tones by Japanese students, for the purpose of making certain contributions to Chinese pronunciation pedagogy. 2. Literature Review 2.1 The Phonetic Features of Chinese and Japanese In Chinese, each syllable has its fixed tone. The high and low, falling and rising pitches depend on the vibration rate of the vocal cords (Figure 1). The constitution of Chinese tone is not determined only by pitch level, but also by transition patterns. There is a level tone, a rising tone, a falling tone, and a falling-rising tone which are caused by change in pitch. In addition to pitch, the intensity and duration of sound are also relevant to the make-up of the tone. Intensity indicates the weight or strength of a sound. For instance, the neutral tone in Chinese is related to sound intensity. The easiest and the most effective way to transcribe and record tones is the system of tone-letter proposed by [Chao 1968]. It classifies tone pitch into five degrees, and divides a perpendicular line into four parts to signify the particular location of the tone pitch on the scale. The low, mid-low, middle, mid-high, and high pitches are indicated by the numbers 1 to 5 respectively. The accurate tone-letter of each tone is represented by the high and low pitch, the rising and falling pitch, or the fluctuation of pitch. In a Chinese disyllabic phrase, the tones of the first and the second words are compromised for the sake of being euphonious [Wu 1992]. It is natural to make the pitch in the second syllable lower than that in the first. Take a disyllabic word with two rising pitches for example, the  Tonal Errors of Japanese Students Learning Chinese: A Study of Disyllabic Words 283 second rising pitch turns into low-rising (Figure 2). In a disyllabic word with two falling pitches, both syllables are lower due to the mutual influence of these two falling pitches (Figure 3).  Figure 1. Frequency of calibration  Nian  Nian  Figure 2. Word of tone 2-2 (年年, year-year) pronounced by the Chinese native  speaker. (The blue line signifies tone.)  Jheng  Jh  Figure 3. Word of tone 4-4 (政治, politics) pronounced by the Chinese native  speaker.  In Japanese phonetic features, accent bears the closest relationship to Chinese tone. There are two types of accents in the languages of the world [Hiroshi 2003]. One is “stress accent”, which uses the intensity of sounds to differentiate various lexical items. The other is “pitch accent”, which uses the pitch of sounds to distinguish one word from another. Japanese  284  Ke-Jia Chang et al.  is classified as pitch accent language (termed as “accent” in the following sections). According to several researchers [Wang 1997; Jun 2001], the Japanese accent can be classified into two types--flat and non-flat. Wang also mentioned that some moras in Japanese must be pronounced with high pitch, and some with low pitch. “Mora” in Japanese means the duration of a sound. The accent in Japanese displays in the mora instead of in the syllable. For example, the word “shinbun” (news) has two syllables but four moras. The difference between flat and non-flat type lies in the existence of the accent. The accent means there is a transition from high to low pitch in a word. The flat type does not have the accent whereas the non-flat type does. Falling type can also be classified into three patterns—H-L, L-H-L, and L-H. In the Compact Japanese-Chinese Dictionary [Liu et al. 2002], pitch change is illustrated by ◎, ①, ②, ③, ④, ⑤ at the end of the word. ◎ means that the first mora is pronounced with low pitch and the remaining moras are produced with high pitch, which may spread to the subsequent auxiliary. This is the flat type, such as hashi (chopsticks), and tomodachi (friend). ① means the first mora is pronounced with high pitch whereas the subsequent with low pitch. This tone falls into the H-L pattern, such as neko (cat). ② means that the first mora is pronounced with low pitch, the second with high pitch, and the subsequent with low pitch, including the following auxiliary. The words composed of two moras in this tone belong to the L-H pattern, while those composed of three or more moras belong to the L-H-L pattern, such as kawa (river) and nomimono (beverage). ③ means the first mora is pronounced with low pitch, the second and the third with high pitch, and the subsequent as well as the following auxiliary with low pitch. Words composed of three moras of this tone fall into the L-H pattern, while those composed of more than four moras belongs to the L-H-L pattern, such as otoko (man) and mizuumi (lake). ④ means that the first mora is pronounced with low pitch, the second to fourth with high pitch, and the subsequent with low pitch. Words composed of four moras in this tone belong to the L-H pattern, while those composed of five or more moras follow the L-H-L pattern, such as otouto (junior brother) and watashibune (ferry boat). ⑤ means the first mora is pronounced with low pitch, the second to the fifth with high pitch, and the subsequent mora with low pitch. In this tone, words with five moras belong to the L-H pattern, while those composed of six or more moras use the L-H-L pattern, such as oshougats (New Year) and tansangasu (carbon dioxide). According to the patterns stated above, the accent in standard Japanese (Tokyo dialect) has the following characteristics. First, there can only be one part with high pitch in a word (with one mora or several consecutive moras). Second, the pitches of the first and the second moras must differ. If the first mora is pronounced with high pitch, the second one must be with low pitch. In the same way, if the first is with low pitch, the second must be with high pitch. Third, the pitch change in the first and third tones in Chinese does not occur in the Japanese accent.  Tonal Errors of Japanese Students Learning Chinese: A Study of Disyllabic Words 285  The Japanese accent and Chinese tone seem to be represented by pitch change. In Japanese, the accent is represented in the pitch change of each mora within a word. The basic component is a mora. However, in Chinese, tone is displayed in the pitch change within each syllable. The basic unit is a morpheme. There are four basic tones in Chinese (except for the neutral tone)—the high-level tone, mid-rising tone, low-falling tone, and high-falling tone. In terms of tone values, they are marked as 55, 35, 214, and 51 respectively (Table 1).  Table 1. The diacritics in the system of tone-letter designated by [Chao 1968]  tone types  Yinping (High-level)  Yangping  Shangsheng  Qusheng  (mid-rising) (falling-rising) (high-falling)  tone values  55  35  214  51  examples  mā  má  mǎ  mà  duration  mid-short  mid-long  longest  shortest  2.2 The Tonal Errors of Japanese Students Learning Chinese There are three common errors made by Japanese students in learning Chinese [Zhu 1994]—flat tone, mispronunciation of multi-syllabic words, and stress of the neutral tone. Many Japanese students of Chinese pronounce disyllabic words in Chinese with rising-falling tones, regardless of their original tones, such as changing “chun1fong1” (spring breeze) to “chun2fong1” (pure breeze), and changing “fang1bian4” (convenient) to “fang2biang4” (room convenient). The cause of this mispronunciation is related to the “euphonic change” in Japanese. Whatever the original pitch pattern is, when two words are combined into one lexical item, only the L-H-L pattern is allowed. For example, the original pitch of “waseda” belongs to the H-L pattern while that of “daigaku” (university) the L-H pattern. When these two words are combined, the pitch of “wasedadaigaku” (Waseda University) turns into the L-H-L pattern. This is because in Japanese, there cannot be two pitch changes in one word, which means that only one pitch peak is allowed in Japanese compounds. It is very difficult for Japanese students to distinguish tone 3 from tone 4, tone 2 from tone 3, and tone 2 from tone 4 in Chinese [He 1997]. They easily mistake tone 3 for tone 2.  286  Ke-Jia Chang et al.  3. Methodology 3.1 Subjects The subjects in this study were two Japanese students with a basic-intermediate level in Chinese. Both of them were from the Chinese Learning Center in National Sun Yat-sen University and had studied Chinese for three to six months. Subject X had better Chinese ability than subject Y. There was another subject Z, whose native language is Chinese, serving as the control group in this study. All subjects were required to read out the disyllabic words listed in the word chart in the same manner. 3.2 Procedure This study is divided into three parts. The first part is to make real-life interviews so as to collect natural data to supplement the word chart. The second is to ask the subjects to read out the disyllabic words in the word chart. In order to maintain the objectivity of this research, the word chart is divided into two lists, A and B, in which the contents are completely the same with only different arrangement of the order. The design of the word chart primarily follows that of [Zhu 1997]. 3.3 Design of Word Chart There are four tones in Chinese. If all four tones are arranged into disyllabic words, sixteen combination pairs are retrieved. Including the neutral tone, there are twenty possible combination pairs. In this study, these twenty pairs are divided into five groups--A, B, C, D, and E. The number 1, 2, 3, 4, 5 represent the high pitch, rising pitch, falling-rising pitch, falling pitch, and neutral tone, respectively, as illustrated below. A：1-1、1-2、1-3、1-4 B：2-1、2-2、2-3、2-4 C：3-1、3-2、3-3、3-4 D：4-1、4-2、4-3、4-4 E：1-5、2-5、3-5、4-5 To avoid expectation of a pattern from the subjects, each group in the word chart has been rearranged. The word chart has been supplemented with Chinese phonetic symbols (bpmf) and all the disyllabic words listed come from basic vocabulary. Before the recording, the subjects were familiarized with the demo word chart with no time limit, and were not  Tonal Errors of Japanese Students Learning Chinese: A Study of Disyllabic Words 287  informed of the correct pronunciation. During the formal recording, if the subjects made any mistakes, they were allowed to self-correct with assistance from others prohibited. The third part of this experiment was to ask Chinese native speakers to take auditory tests and pick up the tonal errors of subject X and subject Y. The tonal errors were analyzed with the phonetic analysis software PRAAT.  3.4 Methods of Analysis First, we identified the tonal errors made by subject X and Y in pronouncing these five groups of sounds (A, B, C, D, E), and judged which subject had the most errors. Then, we investigated the ratio of tonal errors of these two Japanese subjects in each group of sounds to draw a comparison to the tone production of Chinese native speakers. 4. Results and Discussion  4.1 Ratio of Tonal Errors From Table 2 we can see that the tonal errors of Subjects X and Y are mostly concentrated in the sounds in Groups B and C. In other words, they are mostly compounds consisting of a first syllable that is tone 2 or tone 3. Table 2. Ratio of tonal errors of subjects X and Y Second Syllable  
Automatic translation evaluation is popular in development of MT systems, but further research is necessary for better evaluation methods and selection of an appropriate evaluation suite. This paper is an attempt for an in-depth analysis of the performance of MT evaluation methods. Difficulty, discriminability and reliability characteristics are proposed and tested in experiments. Visualization of the evaluation scores, which is more intuitional, is proposed to see the translation quality and is shown as a natural way to assemble different evaluation methods. Keywords: Machine Translation, Performance, Analysis, Visualization, Clustering, Natural Language Processing 1. Introduction Machine translation (MT) evaluation activities have accompanied MT research and system development. The ALPAC report [ALPAC 1966], which has greatly influenced machine translation research activities, is the first historical MT evaluation activity. With new developments in natural language processing technology coming in the 1990s, the black-box evaluation has been instantiated by the methodology of DARPA [Doyon et al. 1998], which measures fluency, accuracy, and informativeness on a 5-point scale. The ISLE Project takes an approach that focuses on how an MT system serves the follow-on human processing rather than on what it is unlikely to do well [ISLE 2000]. Since manual evaluation is labor-intensive and time-consuming, many researchers are making efforts towards reliable automatic MT evaluation methods. A problem is that the methods cannot be characterized by precision and recall as in other natural language ∗ Southeast University, Nanjing, China 210096 Tel: +86-512-68880263 E-mail: jyao@suda.edu.cn + School of Computer Science and Technology, Soochow University, Suzhou, China, 2150006 ∗∗ South China University of Technology, Guangzhou 510641, China [Received March 21, 2006; Revised October 11, 2006; Accepted October 16, 2006]  298  Jianmin Yao et al.  processing activities such as POS tagging or phrase identification. A new quality system is necessary. This paper aims for performance analysis and better illustration of machine translation evaluation, which can help developers know about the improvement in the quality of their system, and help users easily distinguish between MT systems. Section 2 reviews related research in the MT field and its evaluation. Section 3 studies the metrics and experiments for comparison of MT evaluation methods. Section 4 proposes an algorithm for visualizing the MT system quality, and draws a dendrogram for the systems by clustering. A conclusion is given in the last section. 2. Related Work MT evaluation had not been a very powerful aid in machine translation research until automatic evaluation methods were broadly studied. Now, different heuristics are employed for automatic MT evaluation. This section gives a brief review of the main automatic MT evaluation methods and studies on the performance of these methods. 2.1 Automatic Evaluation Methods Some automatic methods focus on specific syntactic features for translation evaluation. [Jones and Galliers 1993] utilizes linguistic information such as the balance of parse trees, N-grams, semantic co-occurrence, and other information as indicators of translation quality. A balanced tree was a negative indicator of Englishness, probably because English is right-branching. Other factors are also utilized in translation evaluation for their indication of the language quality. [Brew and Thompson. 1994], whose criteria involve word frequency, POS tagging distribution and other text features, compares human rankings and automatic measures to decide the translation quality. These linguistic features are extracted as a reflection of the overall translation quality. Another type of evaluation method involves comparison of the translation result with human translations. [Keiji et al. 2001] evaluates the translation output by measuring the similarity between the translation output and translation answer candidates from a parallel corpus. [Yasuhiro et al. 2001] uses multiple edit distances to automatically rank machine translation output by translation examples. While the IBM BLEU method [Papineni et al. 2001] and the NIST MT evaluation [NIST 2002] compare MT output with expert reference translations in terms of the statistics of word N-grams. [Melamed et al. 2003] adopts the maximum matching size of the translation and the reference as the similarity measure for the score. [Niben and Och 2000] scores a sentence on the basis of scores of translations in a database with the smallest edit distance. [Yokoyama et al. 1999] proposes a two-way MT based evaluation method, which compares output Japanese sentences with the original  Performance Analysis and Visualization of Machine Translation Evaluation  299  Japanese sentence for word identification, the correctness of the modification, the syntactic dependency and the parataxis. Another path of MT evaluation is based on test suites. A weighted average of the scores for separate grammatical points is taken as the score of the system. The typological test covers vocabulary size, lexical capacity, phrase, syntactic correctness, etc. [Yu 1993] designs a test suite consisting of sentences with various test points. [Guessoum and Zantout 2001] proposes a semi-automatic evaluation method of the grammatical coverage machine translation systems via a database of unfolded grammatical structures. [Koh et al. 2001] describes their test suite constructed on the basis of fine-grained classification of linguistic phenomena. 2.2 Performance of an Automatic Evaluation Method The ISLE has made some efforts to develop a specification of performance for the MT evaluation methods [ISLE 2000]. A list of the desiderata demands that at least the measure: 1) must be easy to define, clear, and intuitive; 2) must correlate well with human judgments under all conditions, genres, domains, etc.; 3) must be ‘tight’, exhibiting as little variance as possible across evaluators, or for equivalent inputs; 4) must be cheap to prepare; 5) must be cheap to apply; 7) should be automated, if possible. These criteria give a broad coverage of the characteristics of the evaluation methods, but further work is needed to measure them in a consistent and objective way. [Popescu-Belis 1999] argues that the MT evaluation metrics should have its upper limit, lower limit, and should be monotonic in quality measure. The above measures are qualitative attributives of MT evaluation methods. If it can further be automated, it will help the researchers find a much easier and consistent way to compare different systems. Only recently, researchers began quantitative studies. Some recent works include [Forner and White 2001] on the correlation between intelligibility and fidelity and noun compound translation. [Papineni et al. 2001] and [Melamed et al. 2003] study the correlation between human scoring and automatic evaluation results. After DARPA took the BLEU method as the evaluation method for MT systems, the correlation between human and machine translation evaluation has become a standard criterion of MT quality scoring, though many researchers are arguing against its efficacy. On the whole, methodological study of automatic evaluation methods has just started and needs to be further deepened. This paper is an attempt to refine the correlation measures and justify their usage in machine translation evaluation. The following section aims for a proposal of some criteria of the performance of MT evaluation measures, which will give linguists a better understanding of the MT evaluation task and its results.  300  Jianmin Yao et al.  3. MT Evaluation Performance Analysis Up to now, the analysis of MT evaluation methods has remained a preliminary comparison of human and automatic scores. Further study is important to propose better evaluation measures and better understanding of the automatic evaluation results. This paper is an endeavor to provide more details of MT evaluation methods. A list of quantitative measures on basis of education measurement theory [Wang 2001] is proposed in section 3.1, and experimental study of the measures is made in section 3.2.  3.1 MT Evaluation Performance Metrics  3.1.1 Consistency and Reliability Reliability is the most important issue in MT evaluation. Correlation is often utilized for description of the consistency between different score results as by various MT evaluation methods or test suites, as follows:  rtt =  ∑ Xa Xb − (∑ Xa )(∑ Xb ) / n  ,  ∑  X  2 a  −  (∑  X  a  )2  /  n  ∑  X  2 b  −  (∑  Xb  )2  /  n  (1)  where X a and Xb refer to scores of the two MT evaluation results; n is the number of test questions in the test suite; rtt is the consistency between the two test results. If the scores are rank-based, reliability can be calculated by Spearman rank correlation as  rtt  =1−  6∑ D2 n(n2 −1)  ,  (2)  where D is the difference between ranks of the same test by different evaluators; n is the sample size. The correlation coefficient between the automatic results and the human results shows the reliability of the automatic evaluation method. On the other hand, if the correlation is between two automatic results, it shows consistency between the two methods, thus, also showing whether they can compensate for each other.  3.1.2 Discriminability The discriminability of an MT evaluation method reflects the ability to distinguish between minor differences in translation qualities. For a test with higher discriminability, a better system should be scored higher, and vice-versa. The MT evaluation result should be fine-grained so that even small changes in the translation quality could be correctly shown. The discriminability of a test can be calculated on the basis of the MT evaluation result, as  Performance Analysis and Visualization of Machine Translation Evaluation  301  follows:  D = ( X H − X L ) /(H − L) .  (3)  In the equation, X H / X L is the score for the best/worst system; H / L is highest/lowest possible score of the test.  3.1.3 Difficulty The difficulty refers to the degree of the difficulty of the test, which has a great influence on the test result. The difficulty of the test changes the distribution, discriminability, and dispersion of the scores. For example, if the test is so difficult that none of the systems outputs the right answer, one cannot distinguish between systems via the MT evaluation result. This is also the case if the test is too easy. The difficulty of the test questions can be calculated as  P = ( X − L) /(H − L) .  (4)  In the equation, X is the average score of the systems, while H/L is the highest/lowest possible score for the test. The difficulty of the test question is closely interrelated with the discriminability, efficacy, and other characteristics of the evaluation. According to education measurement theory, a difficulty of around 0.5 is helpful for discriminating the systems to be scored [Wang 2001]. In the section above, a proposal of performance metrics for MT evaluation measures and the proposal’s test suite has been given. These metrics help in analyzing the efficacy of the evaluation methods. The next section gives some experimental examples of the evaluation performance, which verifies the metrics mentioned above.  3.2 Experiments on MT Evaluation Performance  3.2.1 Test of Consistency, Discriminability and Difficulty Since the MT evaluation performance metrics proposed in section 3.1 are language-independent, they can be applied to evaluation results in any language. The open source of human evaluation results in [Darwin 2001] on eight English-to-Japanese MT systems is taken for analysis in this section. The authors of this paper do research on the open source evaluation results for two reasons: it is available to any researcher, and thus is easier to duplicate the experiment and analysis; also, the open source data is appropriate in data size and reliability and saves time for more manual work. In the experiment in [Darwin 2001], two evaluators score 8 systems on a 5-point scale showing intelligibility and accuracy. The experimental setup and details are listed in the appendix following this paper. Based on the measures proposed in the last section, this paper’s authors make an analysis of the  302  Jianmin Yao et al.  characteristics of the MT evaluation results.  The first experiment is to test the consistency between MT evaluation results from different measures (accuracy and intelligibility), different evaluators, and different test suites. According to equation (1) and (2), based on the data in Table A1 and A2 in the appendix, one gets the correlation coefficients in Table 1, which shows the correlation coefficients for the MT evaluation results.  In Table 1, rows 1 and 2 show a consistency between MT evaluation results by metrics of intelligibility and accuracy. Rows 3 to 5 show consistency between two human evaluators A and B. Rows 6 to 8 show consistency between MT evaluation results by the same evaluator A on different parts of the 300 hundred sentences.  Table1. The correlation coefficients for the MT evaluation results achieved from different evaluation measures of intelligibility and accuracy), different evaluators (named as A and B) and various test suites (3 parts of 300 sentences).  Item1  Item2  Intelligibility Accuracy  Other conditions Overall average scores  Correlation option  Correlation  Pearson 0.998  Intelligibility Accuracy  Overall average scores  Spearman 1.000  Evaluator A Evaluator B Intelligibility for all 300 sentences Pearson 0.991  Evaluator A Evaluator B Accuracy for all 300 sentences Pearson 0.998  Evaluator A Evaluator B Accuracy for all 300 sentences Spearman 0.994  Sent#1-100 Sent#101-200 Intelligibility evaluator A  Pearson 0.964  Sent#1-100 Sent#201-300 Intelligibility evaluator A  Pearson 0.968  Sent#101-200 Sent#201-300 Intelligibility evaluator A  Pearson 0.945  From the definition in section 3.1, one knows that correlation between different human evaluation results is an upper bound of automatic MT evaluation performance. Correlation with a human evaluation also reflects the reliability of the automatic evaluation result. As seen in Table 1, all correlation coefficients are higher than 0.9, which is a strong hint of consistency. First, the correlation coefficient between intelligibility and accuracy are 0.998 and 1.000, respectively. This reminds researchers that the two metrics have quite similar scores, and a researcher may just measure one and know the other by regression analysis. Second, the coefficient is also high for correlation between different evaluators and different parts of the test suite, which shows that scores from both evaluators and from different sentences agree with each other on the whole. This is also the case for automatic measures. From previous study, one knows that some automatic evaluation methods are highly correlated with human evaluation, for example, a correlation of around 0.99 for BLEU and NIST [NIST 2002]. GTM (General Text Matching) claims a 0.8 level which is better than BLEU on the same test suite  Performance Analysis and Visualization of Machine Translation Evaluation  303  [Melamed et al. 2003]. The difference between [Melamed et al. 2003] and [NIST 2003] gives researchers a strong signal that consistency is a key factor, but not the only one, in MT evaluation performance. Another key issue seen from Table 1 is that rows 6 to 8 have a lower correlation coefficient than the rows above. It reminds the researchers that different metrics, such as intelligibility and accuracy, different evaluator A and B, as in the experiments, have a higher correlation coefficient than the same evaluator on different test suites with the same MT evaluation measure of intelligibility. Thus, the difficulty and size of the test suite is another key factor in MT evaluation. The following is further analysis of the influence of test suites.  3.2.2 Influence of the Test Suite  For the different parts of the test suite, the researchers have the discriminability and difficulty of intelligibility calculated using equations (3) and (4), which can give one a hint of the reason for their influence on the MT evaluation results.  Table 2. Discriminability and difficulty of test suites with intelligibility by different evaluators. The 300 sentences in the test suite are divided into 3 parts and evaluated with intelligibility separately.  Sentences 1-100 1-100 101-200 101-200 201-300 201-300 All 300 All 300  Evaluator A B A B A B A B  Discriminability 0.23 0.31 0.23 0.31 0.24 0.34 0.23 0.32  Difficulty 0.50 0.44 0.56 0.62 0.43 0.53 0.50 0.53  From Table 2, one can see that different parts of a test set may have different difficulty and discriminability levels. Since all evaluation tasks need better discriminability capability, the evaluator needs to pick out proper test sentences for the evaluation task. Taking evaluator A as an example, the difficulty of different parts of the test suites are 0.50 for sentences 1-100, 0.56 for sentences 101-200, and 0.43 for sentences 201-300. The different difficulty levels led to different correlation coefficients between different parts of the test suites. For example, sentences 101-200 and 201-300 differ greatly in difficulty, and the difference in correlation coefficients is also lower in Table 1 (only 0.945). Another factor found in Table 2 is that the results of evaluators A and B have different discriminability, the former about 0.23, and the latter 0.32. That means their evaluation score has a different distribution style. In fact, this  304  Jianmin Yao et al.  phenomenon has a vital influence on the correlation coefficient of two evaluation results, which is highly related to the evaluation result. The above study of the evaluation performance is made on a public-available Japanese test suite. One does have to notice that the evaluation performance measures are language-independent, which ensures the applicability of the method to the Chinese language, or other language pairs. To study other performance measures, a test on a Chinese suite is made below. As described above, besides the difficulty and discriminability, another key factor for the test suite is the size. The larger the size of the test suite, the more stable and reliable the MT evaluation result becomes. Taking the popular automatic evaluation methods of BLEU and GTM as example, the influence of the size of the test suite, i.e. the number of sentences it contains, is tested using the 863 National High-tech Program MT evaluation corpus. This corpus is widely used for the evaluation of MT systems in mainland China. The corpus contains 1019 sentences. An experiment was carried out on the BLEU and GTM methods to test the influence of the size of a test suite for an English-to-Chinese translation system. The result is shown in Figure 1. When the test suite is small, i.e. there is small number of sentences in the test suite, the MT evaluation score fluctuates violently. While when the test suite contains more than 80 sentences, the fluctuation becomes less violent and goes flat after 400 sentences. Figure 1 shows that the two methods have similar tendencies, which shows that they have similar demands of the test suite size.  0.9 0.8 0.7 0.6 0.5 0.4 
Automatically acquired lexicons with subcategorization information have been shown to be accurate and useful for some purposes, but their accuracy still shows room for improvement and their usefulness in many applications remains to be investigated. This paper proposes a two-fold filtering method, which in experiments improved the performance of a Chinese acquisition system remarkably, with an increased precision rate of 76.94% and a recall rate of 83.83%, making the acquired lexicon much more practical for further manual proofreading and other NLP uses. And as far as we know, at the present time, these figures represent the best overall performance achieved in Chinese subcategorization acquisition and in similar researches focusing on other languages. Keywords: Filter, Chinese, SCF, Diathesis Alternation 1. Introduction Subcategorization is a process that classifies a syntactic category into its subsets. [Chomsky 1965] defined the function of strict subcategorization features as appointing a set of constraints that dominate the selection of verbs and other arguments in deep structure. Subcategorization of verbs, as well as categorization of all words in a language, is often implemented by means of functional distributions, which constitute different environments or distributional patterns accessible for a verb or word. Such a distribution or environment is called a subcategorization frame (SCF), and is usually combined with both syntactic and semantic information. Therefore, verb subcategorization involves much more information than verb classification, which usually only classifies verbs into groups. SCFs, on the other hand, 
In this paper, we present an integrated method to machine translation from Cantonese to English text. Our method combines example-based and rule-based methods that rely solely on example translations kept in a small Example Base (EB). One of the bottlenecks in example-based Machine Translation (MT) is a lack of knowledge or redundant knowledge in its bilingual knowledge base. In our method, a flexible comparison algorithm, based mainly on the content words in the source sentence, is applied to overcome this problem. It selects sample sentences from a small Example Base. The Example Base only keeps Cantonese sentences with different phrase structures. For the same phrase structure sentences, the EB only keeps the most simple sentence. Target English sentences are constructed with rules and bilingual dictionaries. In addition, we provide a segmentation algorithm for MT. A feature of segmentation algorithm is that it not only considers the source language itself but also its corresponding target language. Experimental results show that this segmentation algorithm can effectively decrease the complexity of the translation process. Keywords: Example-Based Machine Translation (EBMT), Rule-Based Machine Translation (RBMT), Example Base (EB). 1. Introduction Although Machine Translation has been an important research topic for many years, the development of a useful Machine Translation system has been very slow. Researchers have found that developing a practical MT system is a very challenging task. Nevertheless, in our age of increasing internationalization, machine translation has a clear and intermediate 
In this paper, we compare four typical spoken language identification (LID) systems. We introduce a novel acoustic segment modeling approach for the LID system frontend. It is assumed that the overall sound characteristics of all spoken languages can be covered by a universal collection of acoustic segment models (ASMs) without imposing strict phonetic definitions. The ASM models are used to decode spoken utterances into strings of segment units in parallel phone recognition (PPR) and universal phone recognition (UPR) frontends. We also propose a novel approach to LID system backend design, where the statistics of ASMs and their co-occurrences are used to form ASM-derived feature vectors, in a vector space modeling (VSM) approach, as opposed to the traditional language modeling (LM) approach, in order to discriminate between individual spoken languages. Four LID systems are built to evaluate the effects of two different frontends and two different backends. We evaluate the four systems based on the 1996, 2003 and 2005 NIST Language Recognition Evaluation (LRE) tasks. The results show that the proposed ASM-based VSM framework reduces the LID error rate quite significantly when compared with the widely-used parallel PRLM method. Among the four configurations, the PPR-VSM system demonstrates the best performance across all of the tasks. Keywords: Automatic Language Identification, Acoustic Segment Models, Universal Phone Recognizer, Parallel Phone Recognizers, Vector Space Modeling 1. Introduction Automatic language identification (LID) is the process of determining the language identity corresponding to a spoken query. It is an important technology in many applications, such as spoken language translation, multilingual speech recognition [Ma et al. 2002], and spoken ∗ Institute for Infocomm Research, 21 Heng Mui Keng Terrace, Singapore, 119613 Phone: (65) 68747866 Fax: (65) 6775 5014 E-mail: {mabin, hli}@i2r.a-star.edu.sg [Received February 3 2006; Revised April 26, 2006; Accepted May 3, 2006]  160  Bin Ma and Haizhou Li  document retrieval [Dai et al. 2003]. In the past few decades, many statistical approaches to LID have been developed [Kirchhoff et al. 2002] [Li and Ma 2005] [Matrouf et al. 1998] [Nagarajan and Murthy 2004] [Parandekar and Kirchhoff 2003] [Singer et al. 2003] [Torres-Carrasquillo et al. 2002] [Yan and Barnard 1995] [Zissman 1996] by exploiting recent advances in the acoustic modeling [Singer et al. 2003] [Torres-Carrasquillo et al. 2002] of phone units and the language modeling of n-grams of these phones [Li and Ma 2005] [Parandekar and Kirchhoff 2003]. Acoustic phone models are used in language-dependent continuous phone recognition to convert speech utterances into sequences of phone symbols in a tokenization process. Then the scores from acoustic models and the scores from language models are combined to obtain a language-specific score for making a final LID decision [Zissman 1996]. Syllable-like units have also been studied [Nagarajan and Murthy 2004]. To further improve the LID performance, other information, such as articulatory and acoustic features [Kirchhoff et al. 2002] [Sugiyama 1991], lexical knowledge [Adda-Decker et al. 2003] [Ma et al. 2002] and prosody [Hazen and Zue 1994], have also been integrated into LID systems. Zissman [1996] experimentally showed that phonetic language models can sometimes be more powerful than MFCC-based Gaussian mixture models (GMMs) [Torres-Carrasquillo et al. 2002]. Therefore the fusion of high-level features and good utilization of their statistics are two important research topics for LID. To make use of high-level features, the LID problem can be taken as consisting of two sub-problems, the tokenization problem and the classification problem. When the tokenization problem is addressed, a fundamental question that arises is whether phone definition is really needed to identify spoken languages. When human beings are constantly exposed to a language without being given any linguistic knowledge, they learn to determine the language’s identity by perceiving some of the speech cues in the language. It is also noteworthy that in human perceptual experiments, listeners with multilingual background often perform better than monolingual listeners in identifying unfamiliar languages [Muthusamy et al. 1994]. These results motivate us to look for useful speech cues for LID along the same line of a recently proposed automatic speech attribute transcription (ASAT) paradigm for automatic speech recognition [Lee 2004]. When we address the classification problem, we find that the strategies such as feature representation for spoken documents and classifier design principles have direct impacts on LID performance. In this paper, we adopt the acoustic segment modeling approach to address the tokenization problem. It is assumed that the sound characteristics of all spoken languages can be covered by a set of acoustic units without strict phonetic definitions, which are called acoustic segment models (ASMs) [Lee et al. 1998]. They can be used to decode spoken utterances into strings of such units. We also propose a vector space modeling approach (VSM)  A Comparative Study of Four Language Identification Systems  161  to classifier design where the statistics of the units and their co-occurrences corresponding to spoken utterances are used to construct feature vectors. Hidden Markov modeling (HMM) [Rabiner 1989] is the dominant approach to acoustic modeling. A collection of ASMs is established from the bottom up in an unsupervised manner using HMM, and has been used to construct an acoustic lexicon for isolated word recognition with high accuracy [Lee et al. 1998]. In LID research, a large body of prior work in LID has been devoted to the PR-LM framework (the phone-recognition frontend followed by the language model backend) [Zissman 1996] and its variations, where phonetic units are used as acoustic units. This is also referred to as the phonotactic approach. The phonotactic approach has been shown to achieve superior performance in NIST LRE tasks especially when it is fused with acoustic scores [Singer et al. 2003]. In this paper, we investigate four LID system configurations cast in a formalism of frontend feature extraction and backend classifier, namely parallel phone recognizer (PPR) and universal phone recognizer (UPR) frontends, and n-gram language model (LM) and vector space model (VSM) backends. We show that the ASM-based PPR-VSM system configuration achieves the best performance across 1996, 2003 and 2005 NIST Language Recognition Evaluation tasks. This paper is organized as follows. In Section 2, we introduce the acoustic segment modeling approach. In Section 3, we discuss LID systems by studying their frontends and backends. In Section 4, we present the experimental results on four front-backend combinations. We draw conclusions in Section 5. 2. Acoustic Segment Modeling A tokenizer is needed to convert spoken utterances into sequences of fundamental acoustic units specified in an acoustic inventory. We believe that units that are not linked to a particular phonetic definition can be more universal, and therefore conceptually easier to adopt. Such acoustic units are thus highly desirable for universal language characterization, especially for rarely observed languages, languages without orthographies, or languages without well-documented phonetic dictionary. A number of variants have been developed along these lines, which have been referred to as language-independent acoustic phone models. Hazen and Zue [1994] reported using 87 phones from the multilingual OGI-TS corpus. Berkling and Barnard [1994a] explored the possibility of finding and using only those phones that best discriminate between language pairs. Berkling and Barnard [1994b] and Corredor-Ardoy et al. [1997] used phone clustering algorithms to find common sets of phones for languages. However, these systems could only operate when a phonetically transcribed database was available. On a separate front, a general effort to circumvent the need for phonetic transcription can be traced back to [Lee et al. 1998] on automatic speech recognition, where ASM was constructed in an unsupervised manner.  162  Bin Ma and Haizhou Li  Some recent studies have applied this concept to LID [Sai Jayram et al. 2003]. Motivated by the above efforts, we propose here an ASM method for establishing a universal representation of acoustic units for multiple languages. 2.1 Augmented Phoneme Inventory (API) Attempts have been made to derive a universal collection of phones to cover all sounds described in an international phonetic inventory, e.g. International Phonetic Alphabet (IPA) or Worldbet [Hieronymus 1994]. In practice, this is a challenging endeavor because we need a large collection of labeled speech samples for all languages. Note that these sounds overlap considerably across languages. One possible approximation approach is to use a set of phonemes from several languages to form a superset, called an augmented phoneme inventory (API) here. This idea has been explored in previous works [Berkling and Barnard 1994a] [Berkling and Barnard 1994b] [Corredor-Ardoy et al. 1997] [Hazen and Zue 1994]. A good inventory needs to phonetically cover as many targeted languages as possible. This method can be effective when phonemes from all targeted languages form a closed set, as studied by Hazen and Zue [1994]. Human perceptual experiments have also shown a similar effect, where listeners’ LID performance improved as their exposure to each language increased [Muthusamy et al. 1994]. This API-based tokenization approach was recently explored [Ma et al. 2005] by using a set of all 124 phones and 4 noise units from English, Korean, and Mandarin, and by extrapolating them to nine other languages in the NIST LRE tasks. This set of 128 units is referred to as API-I in Table 1, which is a proprietary phone set defined for the IIR-LID1 database. Many preliminary LID experiments were conducted using the IIR-LID database and the API-I phone set. For example, we have explored an API-based approach to universal language characterization [Ma et al. 2005] and a text categorization approach to LID [Gao et al. 2005], which formed the basis for the vector based feature extraction approach discussed in the next section. To expand the acoustic and phonetic coverage, we further used another larger set of APIs with 258 phones, from the six languages in the OGI-TS2 multi-language telephone speech database. These six languages all appear in the NIST LRE tasks. This set will be referred to as API-II. A detailed breakdown of how the two phone sets were formed with phone counts for each language is given in Table 1.  
This paper presents different methods of handling pronunciation variations in Cantonese large-vocabulary continuous speech recognition. In an LVCSR system, three knowledge sources are involved: a pronunciation lexicon, acoustic models and language models. In addition, a decoding algorithm is used to search for the most likely word sequence. Pronunciation variation can be handled by explicitly modifying the knowledge sources or improving the decoding method. Two types of pronunciation variations are defined, namely, phone changes and sound changes. Phone change means that one phoneme is realized as another phoneme. A sound change happens when the acoustic realization is ambiguous between two phonemes. Phone changes are handled by constructing a pronunciation variation dictionary to include alternative pronunciations at the lexical level or dynamically expanding the search space to include those pronunciation variants. Sound changes are handled by adjusting the acoustic models through sharing or adaptation of the Gaussian mixture components. Experimental results show that the use of a pronunciation variation dictionary and the method of dynamic search space expansion can improve speech recognition performance substantially. The methods of acoustic model refinement were found to be relatively less effective in our experiments. Keywords: Automatic Speech Recognition, Pronunciation Variation, Cantonese 1. Introduction Given a speech input, automatic speech recognition (ASR) is a process of generating possible hypotheses for the underlying word sequence. This can be done by establishing a mapping between the acoustic features and the yet to be determined linguistic representations. Given ∗ Department of Electronic Engineering, The Chinese University of Hong Kong, Shatin, N. T., Hong Kong Tel: 852-26098267 Fax: 852-26035558 E-mail: tanlee@ee.cuhk.edu.hk The author for correspondence is Tan Lee. + Micrpsoft Research Asia, 5th Floor, Sigma Center, 49 Zhichun Road, Haidian, Beijing 100080, China [Received June 16, 2005; Revised January 20, 2006; Accepted January 23, 2006]  18  Tan Lee et al.  the high variability of human speech, such mapping is in general not one-to-one. Different linguistic symbols can give rise to similar speech sounds, while the same linguistic symbol may also be realized in different pronunciations. The variability is due to co-articulation, regional accents, speaking rate, speaking style, etc. Pronunciation modeling is aimed at providing an effective mechanism by which ASR systems can be adapted to pronunciation variability. Pronunciation variations can be divided into two types: phone change and sound change [Kam 2003] [Liu and Fung 2003]. In [Saraçlar and Khudanpur 2000] [Liu 2002], they are also referred to as complete change and partial change, respectively. A phone change happens when a baseform (canonical) phoneme is realized as another phoneme, which is referred to as its surface-form. The baseform pronunciation is considered to be the “standard” pronunciation that the speaker is supposed to use. Surface-form pronunciations are the actual pronunciations that different speakers may use. A sound change can be described as variation in phonetic properties, such as nasalization, centralization, voicing, etc. Acoustically, the variant sound is considered to be neither the baseform nor any surface-form phoneme. In other words, we cannot find an appropriate unit in the language’s phoneme inventory to represent the sound. In terms of the scope of such variations, pronunciation variations can be divided into word-internal and cross-word variations [Strik and Cucchiarini 1999]. There have been many studies on modeling pronunciation variations for improving ASR performance. They are focused mainly on two problems: 1) prediction of the pronunciation variants, and 2) effective use of pronunciation variation information in the recognition process [Strik and Cucchiarini 1999]. Knowledge-based approaches use findings from linguistic studies, existing pronunciation dictionaries, and phonological rules to predict the pronunciation variations that could be encountered in ASR [Aubert and Dugast 1995] [Kessens et al. 1999]. Data-driven approaches attempt to discover the pronunciation variants and the underlying rules from acoustic signals. This is done by performing automatic phone recognition and aligning the recognized phone sequences with reference transcriptions to find out the surface forms [Saraçlar et al. 2000] [Wester 2003]. Some studies used hand-labelled corpora [Riley et al. 1999]. The key components of a large-vocabulary continuous speech recognition system are the acoustic models, the pronunciation lexicon and the language models [Huang et al. 2001]. The acoustic models are a set of hidden Markov models (HMM) that characterize the statistical variations of input speech. Each HMM represents a specific sub-word unit, e.g. a phoneme. The pronunciation lexicon and the language models are used to define and constrain the ways sub-word units can be concatenated to form words and sentences. They are used to define a search space from which the most likely word string(s) can be determined with a computationally efficient decoding algorithm. Within such a framework, pronunciation  Modeling Cantonese Pronunciation Variations for  19  Large-Vocabulary Continuous Speech Recognition  variations can be handled by modifying one or more of the knowledge sources or improving the decoding algorithm. Phone changes can be handled by replacing the baseform transcription with surface-form transcriptions, i.e. the actual pronunciations observed. In an LVCSR system, this can be done by either augmenting the baseform lexicon with the additional pronunciation variants [Kessens et al. 1999] [Liu et al. 2000] [Byrne et al. 2001], or expanding the search space during the decoding process to include those variants [Kam and Lee 2002]. In order to deal with sound changes, pronunciation modeling must be applied at a lower level, for example, on the individual states of a hidden Markov model (HMM) [Saraçlar et al. 2000]. In general, acoustic models are trained solely with baseform transcriptions. It is assumed that all training utterances follow exactly the canonical pronunciations. This convenient, but apparently unrealistic, assumption renders the acoustic models inadequate in representing the variations of speech sounds. To alleviate this problem, various methods of acoustic model refinement were proposed [Saraçlar et al. 2000] [Venkataramani and Byrne 2001] [Liu 2002]. In this paper, the pronunciation variations in continuous Cantonese speech are studied. The linguistic and acoustic properties of spoken Cantonese are considered in the analysis of pronunciation variations and, subsequently, the design of pronunciation modeling techniques for LVCSR. Like in most conventional approaches, phone changes are anticipated by using an augmented pronunciation lexicon. The lexicon includes the most frequently occurring alternative pronunciations that are derived from training data. We also describe a novel method of dynamically expanding the search space during decoding to include pronunciation variants that are predicted with context-dependent pronunciation models. For sound changes, we propose to measure the similarities between confused baseform and surface-form models at the Gaussian mixture component level and, accordingly, refine the models through sharing and adaptation of the relevant mixture components. In the next section, the properties of spoken Cantonese are described and the fundamentals of Cantonese LVCSR are explained. In Section 3, different methods of modeling pronunciation variations at the lexical level are presented in detail and experimental results are given. The techniques for handling sound changes through acoustic model refinement are described in Section 4. Conclusions are given in Section 5.  2. Cantonese LVCSR 2.1 About Cantonese Cantonese is one of the major Chinese dialects. It is the mother tongue of over 60 million people in Southern China and Hong Kong [Grimes et al. 2000]. The basic unit of written Cantonese is a Chinese character [Chao 1965]. Chinese characters are ideographic, meaning that they contain no information about pronunciation. There are more than ten thousand  20  Tan Lee et al.  distinctive characters. In Cantonese, each of them is pronounced as a single syllable that carries a specific tone. A sentence is spoken as a string of monosyllabic sounds. A character may have multiple pronunciations, and a syllable typically corresponds to a number of different characters.  A Cantonese syllable is formed by concatenating two types of phonological units: the Initial and the Final, as shown in Figure 1 [Hashimoto 1972]. There are 20 Initials (including the null Initial) and 53 Finals in Cantonese, in contrast to 23 Initials and 37 Finals in Mandarin. Table 1 and Table 2 list the Initials and Finals of Cantonese. They are labeled using Jyut Ping, a phonemic transcription scheme proposed by the Linguistic Society of Hong Kong [LSHK 1997]. In terms of the manner of articulation, the 20 Initials can be categorized into seven classes: null, plosive, affricate, fricative, glide, liquid, and nasal. The 53 Finals can be divided into five categories: vowel (long), diphthong, vowel with nasal coda, vowel with stop coda, and syllabic nasal. Except for [m] and [ng], each Final contains at least one vowel element. The stop codas, i.e., -p, -t and -k, are unreleased. In Cantonese, there are more than 600 legitimate Initial-Final combinations, which are referred to as base syllables.  Initial [Onset]  BASE SYLLABLE  Final  Nucleus  [Coda]  Figure 1. The composition of a Cantonese syllable. [] means optional.  Table 1.The Cantonese Initials  Jyut Ping symbols Manner of Articulation  [b]  Plosive, unaspirated  [d]  Plosive, unaspirated  [g]  Plosive, unaspirated  [p]  Plosive, aspirated  [t]  Plosive, aspirated  [k]  Plosive, aspirated  [gw]  Plosive, unaspirated, lip-rounded  [kw]  Plosive, aspirated, lip-rounded  [z]  Affricate, unaspirated  [c]  Affricate, aspirated  [s]  Fricative  [f]  Fricative  [h]  Fricative  [j]  Glide  [w]  Glide  [l]  Liquid  [m]  Nasal  [n]  Nasal  [ng]  Nasal  Place of Articulation Labial Alveolar Velar Labial Alveolar Velar Velar, labial Velar, labial Alveolar Alveolar Alveolar Dental-labial Vocal Alveolar Labial Lateral Labial Alveolar Velar  Modeling Cantonese Pronunciation Variations for  21  Large-Vocabulary Continuous Speech Recognition  Table 2.The 53 Cantonese Finals  CODA  Nil -i -u -p -t -k -m -n -ng  -aa- [aa] [aai] [aau] [aap] [aat] [aak] [aam] [aan] [aang]  N  -a-  [ai] [au] [ap] [at] [ak] [am] [an] [ang]  U -e- [e] [ei]  [ek]  [eng]  C -i- [i]  [iu] [ip] [it] [ik] [im] [in] [ing]  L -o- [o] [oi] [ou]  [ot] [ok]  [on] [ong]  E -u- [u] [ui] U -yu- [yu] S -oe- [oe] [eoi]  [ut] [uk] [yut] [eot] [oek]  [un] [ung] [yun] [eon] [oeng]  [m]  [ng]  From phonological points of view, Cantonese has nine tones that are featured by differently stylized pitch patterns. They are divided into two categories: entering tones and non-entering tones. The entering tones occur exclusively with syllables ending in a stop coda (-p, -t, or –k). They are contrastively shorter in duration than the non-entering tones. In terms of pitch level, each entering tone coincides roughly with a non-entering counterpart. In many transcription schemes, only six distinctive tone categories are defined. They are labeled as Tone 1 to Tone 6 in the Jyu Ping system. If tonal difference is considered, the total number of distinctive tonal syllables is about 1,800.  Table 3 gives an example of a Chinese word and its spoken form in Cantonese. The word 我們 (meaning “we”) is pronounced as two syllables. The first syllable is formed from the Initial [ng] and the Final [o], with Tone 5. The second syllable is formed from the Initial [m] and the Final [un], with Tone 4.  Table 3.An example Chinese word and its Cantonese pronunciations  Word Chinese characters Base syllables Initial & Final Tone  我 我們 們  ngo  [ng] [o]  5  mun  [m] [un]  4  2.2 Linguistic Studies on Pronunciation Variations in Cantonese Over the past twenty years, there have been sociolinguistic studies on how phonetic variations in Cantonese are related with social characteristics of speakers such as sex, age, and educational background. They have revealed some systematic patterns underlying the phonetic variations [Bauer and Benedict 1997] [Bourgerie 1990] [Ho 1994]. Table 4 gives a summary of the major observations in these studies.  22  Tan Lee et al.  Table 4. Major phonetic variations in Cantonese observed by sociolinguistic studies  [n] ~ [l] Initial consonants [ng]~ null [gw] → [g]  Inter-change between nasal and lateral Initials Inter-change between velar nasal and null Initial. Change from labialized velar to delabialized velar before back-round vowel [o]  Syllabic nasal  [ng] Change from velar nasal to bilabial nasal → [m]  -ng → -n Final consonants -k ~ -t -k ~ -p  Change from velar nasal coda to dental nasal coda Inter-change between velar stop coda and dental or glottal stop coda  It was found that [n]→[l], [ng]→null, and [gw]→[g] correlate with the sex and age of a speaker [Bourgerie 1990]. Older people make these substitutions much less frequently than younger generations. Female speakers tend to substitute [n] with [l], and delete [ng] more frequently than males. A correlation with the formality of the speech situation was also observed [Bourgerie 1990]. In casual speech, [l], null Initial, and [g] occur more frequently. According to [Bauer and Benedict 1997], the variations are also related to the development of neighboring dialects in the Pearl River Delta.  When the preceding syllable ends with a nasal coda, there is a tendency to substitute the Initial [l] of the succeeding syllable with [n] [Ho 1994]. Labial dissimilation is probably the cause of the change [gw]→[g], when the right context is -o, for example “gwok” 國 (country), pronounced as “gok” 角 (corner). The sequence of the two lip-rounded segments -w- and -o- become redundant or unnecessary with the second one driving out the first. The change [ng]→[m] is due to the fact that when [ng] occurs in the presence of a bilabial coda, its place of articulation changes to bilabial. For example, “sap ng” 十五 (fifteen) becomes “sap m” through the perseverance of the bilabial closure of the coda -p into the articulation of the following syllabic nasal. This is referred to as perseveratory assimilation [Bauer and Benedict 1997].  Other pronunciation variations are due to the dialectal accents of non-native speakers, who may have difficulties mastering some of the Cantonese pronunciations. They sometimes use the pronunciation of their mother tongue to pronounce a Cantonese word, for example, “ngo” 我 (me) is pronounced as “wo” by a Mandarin speaker.  2.3 Cantonese LVCSR: the Baseline System Figure 2 gives the functional block diagram of a typical LVCSR system. At the front-end processing module, the input speech is analyzed and converted into a sequence of acoustic feature vectors, denoted by O . The goal of speech recognition is to determine the most probable word sequence W , given the observation O . With the Bayes’ formula, the decision  Modeling Cantonese Pronunciation Variations for  23  Large-Vocabulary Continuous Speech Recognition  can be made as  W * = arg max P(W | O) = arg max P(O | W )P(W ) .  (1)  W  W  Usually the acoustic models are built at the sub-word level. Let B be the sub-word sequence that represents W . Eq. (1) can be written as  W * = arg max P(O | B)P(B | W )P(W ) ,  (2)  W  where P(O | B) and P(W ) are referred to as the (sub-word level) acoustic models and the language models, respectively. P(B | W ) is given by a pronunciation lexicon. In the case of Chinese speech recognition, the sub-word units can be either syllables, Initials and Finals, or phone-like units. The recognition output is typically represented as a sequence of Chinese characters. The details of our baseline system for Cantonese LVCSR are given below.  Knowledge Sources  Language Model P(W)  Pronunciation Lexicon P (B |W )  Acoustic Model P(O|B)  Input Speech  Front-End Processing  Acoustic Feature vectors  Decoder  Figure 2. A typical LVCSR system  Recognized word sequence  Front-end processing Acoustic feature vectors are computed every 10 msec. Each feature vector is composed of 39 elements, which includes 12 Mel-frequency cepstral coefficients, log energy, and their first-order and second-order derivatives. The analysis window size is 25 msec.  24  Tan Lee et al.  Acoustic models The acoustic models are right-context-dependent cross-word Initials and Finals models [Wong 2000]. The number of HMM states for Initial and Final units are 3 and 5, respectively. Each state is represented by a mixture of 16 Gaussian components. The decision tree based state clustering approach is used to allow the sharing of parameters among models. Pronunciation lexicons and language models The lexicon contains about 6,500 entries, among which 60% are multi-character words and the others are single-character words [Wong 2000]. These words were selected from a newspaper text corpus of 98 million Chinese characters. The out-of-vocabulary percentage is about 1% [Wong 2000]. For each word entry, the canonical pronunciation(s) is specified in the form of Initials and Finals [CUPDICT 2003]. The language models are word bi-grams that were trained with the same text corpus described above. Decoder The search space is formed from lexical trees that are derived from the pronunciation lexicon. One-pass Viterbi search is used to determine the most probable word sequence [Choi 2001]. The acoustic models were trained using CUSENT, which is a read speech corpus of continuous Cantonese sentences collected at the Chinese University of Hong Kong [Lee et al. 2002]. There are over 20,000 gender-balanced training utterances. The test data in CUSENT consists of 1,200 utterances from 6 male and 6 female speakers. The performance of the LVCSR system is measured in terms of word error rate (WER) for the 1,200 test utterances. The baseline WER is 25.34%. 3. Handling Phone Change with Pronunciation Models The pronunciation lexicon used in the baseline system provides only the baseform pronunciation for each of the word entries. In real speech, the baseform pronunciations are realized differently, depending on the speakers, speaking styles, etc. Phone change means that the pronunciation variation can be considered as one or more Initial or Final (IF) unit in the baseform pronunciation being substituted by another IF unit. Note that the substituting surface-form unit is also one of the legitimate IF units, as listed in Tables 1 and 2. A pronunciation model (PM) is a descriptive and predictive model by which the surface-form pronunciation(s) can be derived from the baseform one. There have been three different types of models proposed by previous studies. They are: 1) phonological rules for generating pronunciation variations [Wester 2003] [Kessens et al. 2003], 2) a pronunciation variation dictionary (PVD) that explicitly lists alternative pronunciations [Aubert and Dugast 1995] [Kessens et al. 1999] [Liu et al. 2000], and 3) statistical decision trees that predict pronunciation variations according to phonetic context [Riley et al. 1999] [Fosler-Lussier  Modeling Cantonese Pronunciation Variations for  25  Large-Vocabulary Continuous Speech Recognition  1999] [Saraçlar et al. 2000]. In this study, two different approaches to handling phone changes in Cantonse ASR are formulated and evaluated. The first approach uses a probabilistic PVD to augment the baseform lexicon. This is a straightforward and commonly used method that has been proven effective for various tasks and languages [Strik and Cucchiarini 1999]. In the second approach, pronunciation variation information is introduced during the decoding process. Decision tree based PMs are used to dynamically expand the search space. In [Saraçlar et al. 2000], a similar idea was presented. Decision tree based PMs were applied to a word lattice to construct a recognition network that includes surface-form realizations.  3.1 Use of a Pronunciation Variation Dictionary (PVD) In this study, the information about Cantonese pronunciation variations is obtained through the data-driven approach. This is done by aligning the baseform transcriptions with the recognized surface-form IF sequences for all training utterances. For each training utterance, the surface-form IF sequence is obtained through phoneme recognition with the acoustic models as described in Section 2.3. To reflect the syllable structure of Cantonese, the recognition output is constrained to be a sequence of Initial-Final pairs. With this approach, only substitutions at the IF level are considered pronunciation variations. Partial change of an IF unit and the deletion of an entire Initial or Final are not reflected in the surface-form IF sequences. The surface-form phoneme sequence is then aligned with the baseform transcription. This gives a phoneme accuracy of 90.33%. The recognition errors are due, at least partially, to phoneme-level pronunciation variation. For a particular baseform phoneme b and a surface-form phoneme s, the probability of b being pronounced as s is computed based on the number of times that b is recognized as s. This probability is referred to as the variation probability (VP). As a result, each pair of IF units is described with a probability of being confused. This is also referred to as a confusion matrix [Liu et al. 2000]. It is assumed that systematic phone change can be detected by a relatively high VP, while a low VP is more likely due to recognition errors. A VP threshold is used to prune those less frequent surface-form pronunciations. As a result, for each baseform IF unit, we can find a certain number of surface-form units, each with a pre-computed VP. A straightforward way of handling pronunciation variation is to augment the basic pronunciation lexicon with alternative pronunciations [Strik and Cucchiarini 1999]. Such an augmented lexicon is named a pronunciation variation dictionary (PVD). In the PVD, each word can have multiple pronunciations, each being assigned a word-level variation probability (VP). The PVD can be obtained from the IF confusion matrix. The word-level VP is given by multiplying the phone-level VPs of all the individual phonemes in the surface-form pronunciation. With the use of the PVD, the goal of speech recognition is essentially to search  26  Tan Lee et al.  for the most probable word sequence by considering all possible surface-form realizations.  This can be conceptually illustrated by modifying Eq. (2) as  W * = arg max P(O | SW ,k )P(SW ,k | W )P(W ) ,  (3)  W ,k  where SW ,k denotes one of the surface-forms realizations of W . P(SW ,k W ) are obtained from the word-level VPs.  3.2 Prediction of Pronunciation Variation during Decoding The PVD includes both context-independent and context-dependent phone changes. Since each word is treated individually, the phonetic context being considered is limited to within the word. To deal with cross-word context-dependent phone changes, we propose applying pronunciation models at the decoding level. Our baseline system uses a one-pass search algorithm [Choi 2001]. The search space is structured as lexical trees. Each node on a tree corresponds to a baseform IF unit. The search is token based. Each token represents a path that reaches a particular lexical node. The propagation of tokens follows the lexical trees, which cover only the legitimate phoneme sequences as specified by the pronunciation lexicon. The search algorithm can be modified in a way that the number of alive tokens is increased to account for pronunciation variations. When a path extends from a particular IF node, its destination node can be either the legitimate node (baseform pronunciation) or any of the predicted surface-form nodes. In other words, the search space is dynamically expanded during the search process. In this approach, a context-dependent pronunciation model is needed to predict the surface-form phoneme given the baseform phoneme and its context. It is implemented using the decision tree clustering technique, following the approaches described in [Riley et al. 1999] [Fosler-Lussier 1999]. Each baseform phoneme is described using a decision tree. Given a baseform phoneme, as well as its left context (the right context is not available in a forward Viterbi search), the respective decision-tree pronunciation model (DTPM) gives all possible surface-form realizations and their corresponding VPs [Kam and Lee 2002]. Like the confusion matrix, the DTPM is trained with the phoneme recognition outputs for the CUSENT training utterances. The training involves an optimization process by which the surface-form phonemes are clustered based on phonetic context. At a particular node of the tree, a set of “yes/no” questions about the phonetic context are evaluated. Each question leads to a different partition of the training data. The question that minimizes the overall conditional entropy of the surface-form realizations is selected for that node. The node-splitting process stops when there are too few training data [Kam 2003].  Modeling Cantonese Pronunciation Variations for  27  Large-Vocabulary Continuous Speech Recognition  3.3 Experimental Results and Discussion  Table 5 gives the recognition results with the use of PVDs that are constructed with different values of the VP threshold. The baseline system uses the basic pronunciation lexicon that contains 6,451 words. The size of the PVD increases as the VP threshold decreases. It is obvious that the introduction of pronunciation variants improves recognition performance. The best performance is attained with a VP threshold of 0.05. In this case, the PVD contains 8,568 pronunciations for the 6,451 words, i.e. 1.33 pronunciation variants per word. With a very small value for the VP threshold, e.g. 0.02, the recognition performance is not good because there are too many pronunciation variants being included and some of them do not really represent pronunciation variation.  Table 5. Recognition results of using a PVD with different VP thresholds  VP threshold  Baseline  0.02  0.05  0.10  0.15  0.20  Word error rate (%) No. of word entries in the PVD  25.34 6,451  23.91 20,840  23.49 8,568  23.70 7,356  23.64 7,210  23.58 7,171  Table 6 shows the recognition results attained by using the DTPM for dynamic search space expansion. It appears that this approach is as effective as the PVD. Unlike the results for the PVD, the performance with a VP threshold of 0.2 is better than that with a threshold of 0.05. This means that the predictions made by the DTPM should be pruned more stringently than the IF confusion matrix. Because of its context-dependent nature, the DTPM has relatively less training data, and the variation probabilities cannot be reliably estimated. It is preferable not to include those unreliably predicted pronunciation variants.  Table 6. Recognition results by dynamic search space expansion  Baseline  VP threshold  0.05  0.2  Word error rate (%) 25.34  23.53  23.27  By analyzing the recognition results in detail, it is observed that many errors are corrected by allowing the following pronunciation variations:  Initials: [gw]→[g], [n]→[l], [ng]→null Finals: [ang]→[an], [ng]→[m] (syllabic nasal)  These observations match well with the findings in sociolinguistic studies on Cantonese phonology (Section 2.2).  28  Tan Lee et al.  4. Handling Sound Change by Acoustic Model Refinement Unlike phone changes, a sound change cannot be described as a simple substitution of one phoneme for another. It is regarded as a partial change from the baseform phoneme to a surface-form phoneme [Liu and Fung 2003]. Our approaches presented below attempt to refine the acoustic models to handle the acoustic variation caused by sound changes. The acoustic models are continuous-density HMMs. The output probability density function (pdf) at each HMM state is a mixture of Gaussian distributions. The use of multiple mixture components is intended to describe complex acoustic variabilities. The acoustic models trained only according to the baseform pronunciations are referred to as baseform models. Each baseform phoneme may have different surface-form realizations. The acoustic models representing these surface-form phonemes are referred to as surface-form models. A baseform model doesnot reflect the acoustic properties of the relevant surface-form phonemes. One way of dealing with this deficiency is through the sharing of Gaussian mixture components among the baseform and surface-form models. In [Saraçlar et al. 2000], a state-level pronunciation model (SLPM) was proposed. It allows the HMM states of a baseform model to share the output densities of its surface-form phonemes. A state-to-state alignment was obtained from decision-tree PMs, and the most frequently confused state pairs were involved in parameter sharing. In [Liu and Fung 2004], the method of phonetic mixtures tying was applied to deal with sound changes. A set of so-called extended phone units were derived from acoustic training data to describe the most prominent phonetic confusion. These units were then modeled by mixture tying with the baseform models. In this study, we investigate both the sharing and adaptation of the acoustic model parameters at the mixture level [Kam et al. 2003].  4.1 Sharing of Mixture Components First of all, the states of the baseform and surface-form models are aligned. It is assumed that both models have the same number of states. Then, state j of the baseform model is aligned with state j of the surface-form model. Consider a baseform phoneme B . The output pdf at state j is given as  M  b j (ot ) = ∑ w jm N (ot ; µ jm , ∑ jm ) ,  (4)  m=1  where M is the number of Gaussian mixture components, and w jm is the weight for the mth mixture component. The baseform output pdf can be modified to include the contributions from the surface-form states  Modeling Cantonese Pronunciation Variations for  29  Large-Vocabulary Continuous Speech Recognition  N  bj '(ot ) = VP(B, B) ⋅ b j (ot ) +  ∑ n=1  VP(Sn ,  B) ⋅  qSn ,  j  (ot  )  ,  (5)  Sn ≠B  where Sn denotes the nth surface-form of B , N is the total number of surface-forms, VP(Sn , B) is the variation probability of Sn with respect to baseform B , and qsn , j (ot ) denotes the output pdf of state j of the nth surface-form model. The number of mixture components in the resultant baseform model depends on N . More surface-form pronunciations bring in more mixture components to the modified baseform state. As the number of mixture components is changed, re-estimation of mixture weights is required.  4.2 Adaptation of Mixture Components  Although sharing mixture components yields an acoustically richer model, it also greatly increases the model size for which more memory space and higher computation complexities are required. Moreover, if the baseform and surface-form mixture components are very similar, including them all in the modified baseform is unnecessarily superfluous.  We propose to refine the baseform acoustic models through parameters adaptation. The total number of model parameters remains unchanged. Like in the approach of mixture sharing, the states of the baseform and surface-form models are aligned. The surface-forms are generated from the IF confusion matrix. Consider the aligned states of the baseform phoneme B and one of its surface-forms S . Let mB (i) and mS ( j) denote the ith mixture component in the baseform state and the jth mixture component in the surface-form state, respectively, where i, j = 1, 2,", M . The distances between all pairs ( mB (i) , mS ( j) ) are computed. Then each surface-form component is paired up with the nearest baseform component. That is, for each mS ( j) , we find  iˆ = arg min d (mB (i), mS ( j)) .  (6)  mB (i)  The “distance” between two Gaussian distributions is calculated using the Kullback-Leibler divergence (KLD) [Myrvoll and Soong 2003]. Given two multivariate Gaussian distributions f and g , the symmetric KLD has the following closed form  d( f ,g) =  
The conventional n-gram language model exploits only the immediate context of historical words without exploring long-distance semantic information. In this paper, we present a new information source extracted from latent semantic analysis (LSA) and adopt the maximum entropy (ME) principle to integrate it into an n-gram language model. With the ME approach, each information source serves as a set of constraints, which should be satisfied to estimate a hybrid statistical language model with maximum randomness. For comparative study, we also carry out knowledge integration via linear interpolation (LI). In the experiments on the TDT2 Chinese corpus, we find that the ME language model that combines the features of trigram and semantic information achieves a 17.9% perplexity reduction compared to the conventional trigram language model, and it outperforms the LI language model. Furthermore, in evaluation on a Mandarin speech recognition task, the ME and LI language models reduce the character error rate by 16.9% and 8.5%, respectively, over the bigram language model. Keywords: Language Modeling, Latent Semantic Analysis, Maximum Entropy, Speech Recognition  1. Introduction  Language modeling plays an important role in automatic speech recognition (ASR). Given a speech signal O , the most likely word sequence Wˆ is obtained by maximizing a posteriori  probability p(W O) , or, equivalently, the product of acoustic likelihood p(OW ) and prior  probability of word sequence p(W ) :  Wˆ = arg max p(W O) = arg max p(O W ) p(W ) .  (1)  W  W  ∗ Department of Computer Science and Information Engineering, National Cheng Kung University, Tainan, Taiwan, R. O. C E-mail: chchueh@chien.csie.ncku.edu.tw + Institute of Information Science, Academia Sinica, Taipei, Taiwan, R. O. C [Received June 16, 2005; Revised January 2, 2006; Accepted January 3, 2006]  38  Chuang-Hua Chueh et al.  This prior probability corresponds to the language model that is useful in characterizing regularities in natural language. Also, this language model has been widely employed in optical character recognition, machine translation, document classification, information retrieval [Ponte and Croft 1998], and many other applications. In the literature, there were several approaches have been taken to extract different linguistic regularities in natural language. The structural language model [Chelba and Jelinek 2000] extracted the relevant syntactic regularities based on predefined grammar rules. Also, the large-span language model [Bellegarda 2000] was feasible for exploring the document-level semantic regularities. Nevertheless, the conventional n-gram model was effective at capturing local lexical regularities. In this paper, we focus on developing a novel latent semantic n-gram language model for continuous Mandarin speech recognition.  When considering an n-gram model, the probability of a word sequence W is written as a product of probabilities of individual words conditioned on their preceding n-1 words  p(W ) = p(w1, w2 ,  T , wT ) ≅ ∏  p(wi  T wi−n+1,..., wi−1) = ∏ p(wi  wii−−1n+1) ,  (2)  i =1  i=1  where wii−−1n+1 represents historical words for word wi , and the n-gram parameter p(wi wii−−1n+1) is usually obtained via the maximum likelihood estimation:  p(wi  wii−−1n+1) =  c( wii− n +1 ) c( wii−−1n +1 )  .  (3)  Here, c(wii−n+1) is the number of occurrences of word sequence wii−n+1 in the training data.  Since the n-gram language model is limited by the span of window size n, it is difficult to characterize long-distance semantic information in n-gram probabilities. To deal with the issue of insufficient long-distance word dependencies, several methods have been developed by incorporating semantic or syntactic regularities in order to achieve long-distance language modeling.  One simple combination approach is performed using the linear interpolation of different information sources. With this approach, each information source is characterized by a separate model. Various information sources are combined using weighted averaging, which minimizes overall perplexity without considering the strengths and weaknesses of the sources in particular contexts. In other words, the weights were optimized globally instead of locally. The hybrid model obtained in this way cannot guarantee the optimal use of different information sources [Rosenfeld 1996]. Another important approach is based on Jaynes’ maximum entropy (ME) principle [Jaynes 1957]. This approach includes a procedure for setting up probability distributions on the basis of partial knowledge. Different from linear interpolation, this approach determines probability models with the largest randomness and  A Maximum Entropy Approach for Semantic Language Modeling  39  simultaneously captures all information provided by various knowledge sources. The ME framework was first applied to language modeling in [Della Pietra et al. 1992]. In the following, we survey several language model algorithms where the idea of information combination is adopted. In [Kuhn and de Mori 1992], the cache language model was proposed to merge domain information by boosting the probabilities of words in the previously-observed history. In [Zhou and Lua 1999], n-gram models were integrated with the mutual information (MI) of trigger words. The MI-Trigram model achieved a significant reduction in perplexity. In [Rosenfeld 1996], the information source provided by trigger pairs was incorporated into an n-gram model under the ME framework. Long-distance information was successfully applied in language modeling. This new model achieved a 27% reduction in perplexity and a 10% reduction in the word error rate. Although trigger pairs are feasible for characterizing long-distance word associations, this approach only considers the frequently co-occurring word pairs in the training data. Some important semantic information with low frequency of occurrence is lost. To compensate for this weakness, the information of entire historical contexts should be discovered. Since the words used in different topics are inherently different in probability distribution, topic-dependent language models have been developed accordingly. In [Clarkson and Robinson 1997], the topic language model was built based on a mixture model framework, where topic labels were assigned. Wu and Khudanpur [2002] proposed an ME model by integrating n-gram, syntactic and topic information. Topic information was extracted from unsupervised clustering in the original document space. A word error rate reduction of 3.3% was obtained using the combined language model. In [Florian and Yarowsky 1999], a delicate tree framework was developed to represent the topic structure in text articles. Different levels of information were integrated by performing linear interpolation hierarchically. In this paper, we propose a new semantic information source using latent semantic analysis (LSA) [Deerwester et al. 1990; Berry et al. 1995], which is used for reducing the disambiguity caused by polysemy and synonymy [Deerwester et al. 1990]. Also, the relations of semantic topics and target words are incorporated with n-gram models under the ME framework. We illustrate the performance of the new ME model by investigating perplexity in language modeling and the character-error rate in continuous Mandarin speech recognition. The paper is organized as follows. In the next section, we introduce an overview of the ME principle and its relations to other methods. In Section 3, the integration of semantic information and n-gram model via linear interpolation and maximum entropy is presented. Section 4 describes the experimental results. The evaluation of perplexity and character-error rate versus different factors is conducted. The final conclusions drawn from this study are discussed in Section 5.  40  Chuang-Hua Chueh et al.  2. Maximum Entropy Principle  2.1 ME Language Modeling The underlying idea of the ME principle [Jaynes 1957] is to subtly model what we know, and assume nothing about what we do not know. Accordingly, we choose a model that satisfies all the information we have and that makes the model distribution as uniform as possible. Using the ME model, we can combine different knowledge sources for language modeling [Berger et al. 1996]. Each knowledge source provides a set of constraints, which must be satisfied to find a unique ME solution. These constraints are typically expressed as marginal distributions. Given features f1, , fN , which specify the properties extracted from observed data, the expectation of fi with respect to empirical distribution p(h, w) of history h and word w is calculated by  p( fi ) = ∑ p(h, w) fi (h, w) ,  (4)  h,w  where fi (⋅) is a binary-valued feature function. Also, using conditional probabilities in language modeling, we yield the expectation with respect to the target conditional distribution p(w h) by  p( fi ) = ∑ p(h) p(w h) fi (h, w) .  (5)  h,w  Because the target distribution is required to contain all the information provided by these features, we specify these constraints  p( fi ) = p( fi ), for i = 1, ,N .  (6)  Under these constraints, we maximize the conditional entropy or uniformity of distribution p(w h) . Lagrange optimization is adopted to solve this constrained optimization problem. For each feature fi , we introduce a Lagrange multiplier λi . The Lagrangian function Λ( p, λ) is extended by  N  Λ( p, λ) = H ( p) + ∑ λi ⎡⎣ p( fi ) − p( fi )⎤⎦ ,  (7)  i=1  with conditional entropy defined by  H ( p) = − ∑ p(h) p(w h) log p(w h) .  (8)  h,w  A Maximum Entropy Approach for Semantic Language Modeling  41  Finally, the target distribution p(w h) is estimated as a log-linear model distribution  p(w  h)  =  
This paper addresses the problem of audio change detection and speaker tracking in broadcast TV streams. A two-pass audio change detection algorithm, which includes detection of the potential change boundaries and refinement, is proposed. Speaker tracking is performed based on the results of speaker change detection. In speaker tracking, Wiener filtering, endpoint detection of pitch, and segmental cepstral feature normalization are applied to obtain a more reliable result. The algorithm has low complexity. Our experiments show that the algorithm achieves very satisfactory results. Keywords: Speaker Tracking, Audio Segmentation, Entropy, GMM 1. Introduction Broadcast TV programs are rich multimedia information resources. They contain large amounts of AV (audio & video) contents including speech, music, images, motion, text, and so on. Finding ways to extract and manage these various kinds of AV content information is becoming extremely important and necessary for application-oriented multimedia content mining and management. The analysis and classification of audio data are important tasks in many applications, such as speaker tracking, speech recognition, and content-based indexing. Among of them, target speaker tracking in TV streams is an important research topic for TV scene analysis. In contrast with general speaker recognition, speaker detection in audio streams usually requires segments of relatively homogenous speech and speaker tracking in this task should also determine the target speakers’ locations, in other word, the starting and ending times. In such applications, effective methods for segmenting continuous audio streams * The High-Tech. Innovation Center, Institute of Automation, Chinese Academy of Sciences, Beijing, China. E-mail: {jmbai, hcjiang, slzhang, swzhang, xubo}@hitic.ia.ac.cn [Received June 15, 2005; Revised January 23, 2006; Accepted January 27, 2006]  58  Junmei Bai et al.  into homogeneous segments are required. The problem of acoustic segmentation and classification has become crucial for the application of automatic speech recognition to audio stream processing. The automatic segmentation of long audio streams and the clustering of audio segments according to different acoustic characteristics have received much attention recently [Lu and Zhang 2002; Chen and Gopalakrishnan 1998; Delacourt and Wellekens 2000; Wilcox et al. 1994; Pietquin et al. 2002]. To detect target speakers in an audio stream, it is best to segment the audio stream into homogeneous regions according to changes in speaker identity, environmental conditions and channel conditions. In fact, there are no explicit cues of changes among these audio signals, and the same speaker may appear multiple times in audio streams. Thus, it is not easy to segment an audio stream correctly. Various segmentation algorithms proposed in the literature [Lu and Zhang 2002; Chen and Gopalakrishnan 1998; Delacourt and Wellekens 2000; Ajmera et al. 2003; Cettolo and Federico 2000] can be categorized as follows [Chen et al. 1998]: 1) Decoder-guided segmentation algorithms: The input audio stream is first decoded by an automation speech recognition (ASR) systems, and then the desired segments are produced by cutting the input at the silence locations generated by the decoder. Other information from the decoder, such as gender information, can also be utilized in segmentation. 2) Model-based segmentation algorithms: Different models, e.g., Gaussian mixture models, are build for a fixed set of acoustic classes, such as telephone speech, pure music, etc, from a training corpus. In these schemes, a sliding window approach and multivariate Gaussian models are generally used. Decisions about the maximum likelihood boundary are made. 3) Metric-based segmentation algotithms: The audio stream is segmented at places where maxima of the distances between neighboring windows appear, and distance measures, such as the KL distance and the generalized likelihood ratio (GLR) distance [Fisher et al. 2003], are utilized. These methods are not very successful at detecting acoustic changes that occur in data [Chen et al.1998]. Decoder-guided segmentation only places boundaries at silence locations, which in general have no direct connection with acoustic changes in the data. Model-based segmentation usually can not be generalized to unseen acoustic conditions. Meanwhile, both model-based and metric-based segmentation rely on a threshold which sometimes lacks stability and robustness. In addition, model-based segmentation does not generalize to unseen acoustic conditions. As for target speaker detection, which is similar to general speaker verification, the traditional methods focus on likelihood ratio detection and template matching. Among these approaches, Gaussian Mixture Models (GMMs) have been the most successful so far  Robust Target Speaker Tracking in Broadcast TV Streams  59  [Reynolds et al. 2000]. Reynolds also extended of these methods by adapting the speaker model from a universal background model (UBM). The speaker detector we adopted in our experiments is based on adapted GMMs. In the target speaker detecting system, we also used the segmental cepstral mean and variance normalization (SCMVN) to normalize the cepstral coefficients to get robust segmental parameter statistics that are suitable for various kinds of environmental conditions. 2. Overview The task of automatic speaker tracking involves finding target speakers in test audio streams. Given an audio stream, all the segments containing a target speaker’s voice must be located with the starting and ending times. The general approach to speaker tracking consists of three steps: audio segmentation, audio classification, and speaker verification. A complete block diagram of the proposed speaker tracking system is shown in Figure 1. The diagram shows how the components of the system fit together.  M 1  M 2  M 3 Figure 1. Block diagram of the speaker tracking system components M1—Segmentation Module, M2 —Classification Module, M3—Speaker verification The three steps are defined as three modules in Figure 1, denoted as M1, M2, and M3. First, audio streams are segmented in M1 by means of two-pass audio segmentation. Then, in M2, these audio segments are classified into different classes, such as speech, music, noise and so on. Last, the speech segments are tested in M3 to verify if target speakers appear in the audio streams. Sometimes, M2 is not necessary when the speaker verification module can distinguish target speakers with other audio signals with acceptable precision. The individual blocks will be described in detail in following sections.  60  Junmei Bai et al.  3. Two-Pass Audio Segmentation The goal of automatic segmentation of audio signals is to detect changes in speaker identity, environmental conditions, and channel conditions. The problem is to find acoustic change detection points in an audio stream. A two-pass segmentation process for audio streams is presented in this paper. First, audio segmentation based on entropy is used to detect potential audio change points. Then, speaker change boundary refinement based on Bayesian decisions is applied. 3.1 First-Pass Segmentation Based on Entropy In the first pass, we use entropy measures to determine the turn candidates. Entropy is a measure of the uncertainty or disorder in a given distribution [Papoulis 1991]. There are many methods for calculating entropy. Ajmera calculates entropy based on posterior probabilities and sets it as one of the features for discriminating speech and music [Ajmera et al. 2003]. It is a model-based classification scheme that makes decisions based on the scores of audio signals to two models: a speech model and a music model. Generally, the speech model is estimated from lots of speech spoken by different speakers, and it acts as a universal model. Thus, it is not suitable for distinguishing different speakers, particularly unknown speakers. The entropy method used in this work is also an extension of the model-based segmentation scheme. Generally, model-based methods apply a maximum likelihood of the Gaussian process with a penalty weight to detect turns in audio streams. By appropriately defining this penalty, one can generate decisions based on the Akaike Information Criterion (AIC), the Bayesian Information Criterion (BIC), the Consistent AIC (CAIC), the Minimum Description Length (MDL) principle, and the Minimum Message Length (MML) principle. It has been found that BIC, MDL, and CAIC give the best results and that with proper tuning, all three produce comparable results [Cettolo et al. 2000]. In this paper, entropy is calculated based on statistical parameters of audio features. The decision rule is not based on scores but on the shape of the entropy contour. In order to clearly show the performance of our method, it is compared with BIC in this paper. The of entropy-based audio segmentation scheme is described in detail in the following: Entropy of a Gaussian Random Variable [You et al. 2004]: Assume a random variable X of dimension K. The entropy of the random variable (RV) is computed by first estimating its probability distribution function (pdf). We can compute the pdf either from the RV’s histogram or from a parameterized distribution. The latter is used to reduce the amount of computation. Assume that the pdf follows a K-dimensional Gaussian density:  Robust Target Speaker Tracking in Broadcast TV Streams  61  P(  X  )  =|  2πΣ  |−  
The segment model (SM) is a family of methods that use the segmental distribution rather than frame-based density (e.g. HMM) to represent the underlying characteristics of the observation sequence. It has been proved to be more precise than HMM. However, their high level of complexity prevents these models from being used in practical systems. In this paper, we propose a framework that can reduce the computational complexity of the Constrained Mean Trajectory Segment Model (CMTSM), one type of SM, by fixing the number of regions in a segment so as to share the intermediate computation results. Our work is twofold. First, we compare the complexity of SM with that of HMM and point out the source of the complexity in SM. Secondly, a fast CMTSM framework is proposed, and two examples are used to illustrate this framework. The fast CMTSM achieves a 95.0% string accurate rate in the speaker-independent test on our mandarin digit string data corpus, which is much higher than the performance obtained with HMM-based system. At the mean time, we successfully keep the computation complexity of SM at the same level as that of HMM. Keywords: Speech Recognition, Segment Model, Mandarin Digit String Recognition 1. Introduction The Hidden Markov Model (HMM) [Rabiner et al. 1993] has been used successfully for 
In this paper, a new robust wavelet-based voice activity detection (VAD) algorithm derived from the discrete wavelet transform (DWT) and Teager energy operation (TEO) processing is presented. We decompose the speech signal into four subbands by using the DWT. By means of the multi-resolution analysis property of the DWT, the voiced, unvoiced, and transient components of speech can be distinctly discriminated. In order to develop a robust feature parameter called the speech activity envelope (SAE), the TEO is then applied to the DWT coefficients of each subband. The periodicity of speech signal is further exploited by using the subband signal auto-correlation function (SSACF) for. Experimental results show that the proposed SAE feature parameter can extract the speech activity under poor SNR conditions and that it is also insensitive to variable-level of noise. Keywords: Voice Activity Detection, Auto-Correlation, Wavelet, Teager Energy 1. Introduction Voice activity detection (VAD) refers to the ability to distinguish speech from noise and is an integral part of a variety of speech communication systems, such as speech coding, speech recognition, hand-free telephony, and echo cancellation. In the GSM-based communication system, a VAD scheme is used to lengthen the battery power through discontinuous transmission when speech-pause is detected [Freeman et al. 1989]. Moreover, a VAD algorithm can be used under a variable bit rate of the speech coding system in order to control the average bit rate and the overall quality of speech coding [Kondoz et al. 1994]. Perviously, ∗ Department of Electrical and Control Engineering, National Chiao-Tung University, HsinChu, Taiwan E-mail: bwu@cssp.cn.nctu.edu.tw + Information & Communications Research Laboratories, Industrial Technology Research Institute, HsinChu, Taiwan E-mail: kunching@itri.org.tw [Received May 13, 2005; Revised Oct. 8, 2005; Accepted Oct. 17, 2005]  88  Bing-Fei Wu and Kun-Ching Wang  Sohn et al. [Sohn et al. 1998] presented a VAD algorithm that adopts a novel noise spectrum adaptation by applying soft decision techniques. The decision rule is drawn from the generalized likelihood ratio test by assuming that the noise statistics are known a priori. Cho et al. [Cho et al. 2001] presented an improved version of the algorithm designed by Sohn. Specifically, Cho presented a smoothed likelihood ratio test to reduce the detection errors. Furthermore, Beritelli et al. [Beritelli et al. 1998] developed a fuzzy VAD using a pattern matching block consisting of a set of six fuzzy rules. Additionally, Nemer et al. [Nemer et al. 2001] designed a robust algorithm based on higher order statistics (HOS) in the residual domain of the linear prediction coding coefficients (LPC). Meanwhile, the International Telecommunication Union-Telecommunications Sector (ITU-T) designed G. 729B VAD [Benyassine et al. 1997], which consists of a set of metrics, including line spectral frequencies (LSF), low band energy, zero-crossing rate (ZCR), and full-band energy. However, the common feature parameters mentioned above are based on averages over windows of fixed length or are derived through analysis based on a uniform time-frequency resolution. For example, it is well known that speech signals contain many transient components and exhibit the non-stationary property. The classical Fourier Transform (FT) works well for wide sense stationary signals but fails in the case of non-stationary signals since it applies only uniform-resolution analysis. Conversely, if the multi-resolution analysis (MRA) property of DWT [Strang et al. 1996] is used, the classification of speech into voiced, unvoiced or transient components can be accomplished. The periodic property is an inherent characteristic of speech signals and is commonly used to characterize speech. In this paper, the periodic properties of subband signals are exploited to accurately extract speech activity. In fact, voiced or vowel speech sounds have a stronger periodic property than unvoiced sounds and noise signals, and this property is concentrated in low frequency bands. Thus, we let the low frequency bands have high resolution in order to enhance the periodic property by decomposing only the low band in each level. Three-level wavelet decomposition is further divided into four non-uniform subbands. Consequently, the well-known “Auto-Correlaction Function (ACF)” is defined in the subband domain to evaluate the periodic intensity of each subband, and is denoted as the “Subband Signal Auto-Correlaction Function (SSACF)”. Generally speaking, the existing methods for suppressing noise are almost all based on the frequency domain. However, these methods indeed waste too much computing power in on-line work. Considering computing complexity, the Teager energy operator (TEO), which is a powerful nonlinear operator and has been successfully used in various speech processing applications [Kaiser et al. 1990],[Bovik et al. 1993],[Jabloun et al. 1999] is applied to eliminate noise components from the wavelet coefficients in each subband priori to SSACF measurement. Consequently, to evaluate the periodic intensity of each subband signal, a Mean-Delta method [Ouzounov et al. 2004] is  Voice Activity Detection Based on Auto-Correlation Function Using  89  Wavelet Transform and Teager Energy Operator  applied in the envelope of each SSACF. First, the Delta SSACF, similar to the delta-cepstrum evaluation, is used to measure the local variation of each SSACF. Next, since the DSSACF is averaged over its length, the value of the Mean DSSACF (MDSSACF) can almost describe the amount of periodicity in each subband. Eventually, by only summing the values of the four MDSSACFs, we can apply a robust feature parameter, called the speech activity envelope (SAE) parameter. Experimental results show that the envelope of the SAE feature parameter can accurately indicate the boundary of speech activity under poor SNR conditions and that it is also insensitive to variable-level noise. In addition, the proposed wavelet-based VAD can be performed on-line. This paper is organized as follows. Section 2 describes the proposed algorithm based on DWT and TEO. In addition, the proposed robust feature parameter is also discussed. Section 3 evaluates the performance of the proposed algorithm and compares it with that of other wavelet-based VAD algorithms and ITU-T G.729B VAD. Finally, Section 4 presents conclusions. 2. The Proposed Algorithm Based on DWT and TEO In this section, each part for the proposed VAD algorithm is discussed in turn.  2.1 Discrete Wavelet Transform The wavelet transform (WT) is based on time-frequency signal analysis. This wavelet analysis adopts a windowing technique with variable-sized regions. It allows the use of long time intervals when we want more precise low-frequency information, and shorter regions where we want high-frequency information. It is well known that speech signals contain many transient components and exhibit the non-stationary property. When we make use of the MRA property of the WT, better time-resolution is needed in the high frequency range to detect the rapid changing transient component of a signal, while better frequency resolution is needed in the low frequency range to track slowly time-varying formants more precisely. Through MRA analysis, the classification of speech into voiced, unvoiced or transient components can be accomplished. An efficient way to implement this DWT using filter banks was developed in 1988 by Mallat [Mallat 1989]. In Mallat’s algorithm, the j -level approximations Aj and details D j of the input signal are determined by using quadrature mirror filters (QMF). Figure 1 shows that the decomposed subband signals A and D are the approximation and detail parts of the input speech signal obtained by using the high-pass filter and low-pass filter, implemented with the Daubechies family wavelet, where the symbol ↓2 denotes an operator of downsampling by 2. In fact, a voiced or vowel speech sound has more significant periodicity than an unvoiced sound on noise signal. Thus, the periodicity of a subband signal can be exploited to accurately  90  Bing-Fei Wu and Kun-Ching Wang  extract speech activity. In addition, the periodicity is almostly concentrated in low frequency bands, so we let the low frequency bands have high resolution in order to enhance the periodic property by decomposing only low bands in each level. Figure 2 employed the used structure of three-level wavelet decomposition. By using DWT, we can divide the speech signal into four non-uniform subbands. The wavelet decomposition structure can be used to obtain the most significant periodicity in the subband domain.  Figure 1. Discrete wavelet transform (DWT) using filter banks  Figure 2. Structure of three-level wavelet decomposition 2.2 Teager Energy Operator It has been observed that the TEO can enhance the discriminability between speech and noise and further suppress noise components from noisy speech signals [Jabloun et al. 1999]. Compared with the traditional noise suppression approach based on the frequency domain, the TEO based noise suppression can be more easily implemented through the time domain.  Voice Activity Detection Based on Auto-Correlation Function Using  91  Wavelet Transform and Teager Energy Operator  In continuous-time, the TEO is defined as  ψ c[s(t)] = [s(t)]2 − s(t)s(t) , where s(t) is a continuous-time signal and s = ds dt . In discrete-time, the TEO can be approximated by  ψ d [s(n)] = s(n)2 − s(n +1)s(n −1) ,  (1)  where s(n) is a discrete-time signal. Let us consider a speech signal s(n) degraded by uncorrelated additive noise u(n) , the resulting signal is shown below:  y(n) = s(n) + u(n) .  (2)  The Teager energy of the noisy speech signal ψ d [ y(n)] is given by  ψ [ y(n)] =ψ d [s(n)] +ψ d [u(n)] + 2ψd [s(n),u(n)] ,  (3)  where ψ d [s(n)] and ψ d [u(n)] are the Teager energy of the discrete speech signal and the additive noise, respectively. The subscript d means the “discrete.” ψd [s(n),u(n)] is the cross-ψ d energy of s(n) and v(n) , such that  ψd [s(n),u(n)] = s(n)u(n) − 0.5s(n −1) ⋅u(n +1) − 0.5s(n +1) ⋅ u(n −1) ,  (4)  where the symbol ⋅ denotes the inner product. Since s(n) and u(n) are zero mean and independent, the expected value of the cross-ψ d energy is zero. Thus, Eq.(5) can be derived from Eq.(3) as shown below:  E {ψ [ y(n)]} = E {ψ [s(n)]} + E {ψ [u(n)]} .  (5)  d  d  d  Experimental results show that the Teager energy of the speech is much higher than that  of the noise. Thus, compared with E {ψ d [ y(n)]} , E {ψ d [u(n)]} is negligible as shown by  E {ψ d [ y(n)]} ≈ E {ψ d [s(n)]} .  (6)  2.3 Subband Signal Auto-Correlation Function (SSACF)  The definition of the “Auto-Correlation Function (ACF)” used to measure the self-periodic intensity of subband signal sequences is shown below:  p−k  R(k) = ∑ s(n)s(n + k), k = 0,1,......p ,  (7)  n=0  where p is the length of ACF and k denotes the shift of the sample.  92  Bing-Fei Wu and Kun-Ching Wang  In this subsection, the ACF will be defined in the subband domain and called the “Subband Signal Auto-Correlation Function (SSACF).” It can be derived from the wavelet coefficients on each subband following TEO processing. Figure 3 displays that the waveform of the normalized SSACFs ( R(0) = 1 ) of each subband, respectively. It is observed that the SSACF of voiced speech has more obvious peaks than that of unvoiced speech and white noise does. In addition, for unvoiced speech, the ACF has more intense periodicity than white noise does, especially in the A3 subband.  Figure 3. Examples of normalized SSACF for voiced speech, unvoiced speech and white noise  2.4 Mean of the absolute values of the DSSACF (MDSSACF) To evaluate the periodic intensity of subband signals, a Mean-Delta method is applied here to each SSACF. First, a measure similar to delta cepstrum evaluation is used to estimate the periodic intensity of the SSACF, namely, the “Delta Subband Signal Auto-Correlation Function (DSSACF),” as shown below:  M  ∑ mR(k + m)  RM  (k)  =  m=−M M  ∑  m2  ,  (8)  m=−M  Voice Activity Detection Based on Auto-Correlation Function Using  93  Wavelet Transform and Teager Energy Operator  where RM is the DSSACF over an M -sample neighborhood. For a particular frame, it is computed by using only the frame’s SSACF (intra-frame processing), while the delta cepstrum is computed by using cepstrum coefficients from neighboring frames (inter-frame processing). It is observed that the DSSACF value is almost similar to the local variation over the SSACF.  Second, the delta of the SSACF is averaged over an M -sample neighborhood RM , where the mean of the absolute values of the DSSACF (MDSSACF) is given by  RM  =  
In this paper we study the problem of learning context-free grammar from a corpus. We investigate a technique that is based on the notion of minimum description length of the corpus. A cost as a function of grammar is deﬁned as the sum of the number of bits required for the representation of a grammar and the number of bits required for the derivation of the corpus using that grammar. On the Academia Sinica Balanced Corpus with part-of-speech tags, the overall cost, or description length, reduces by as much as 14% compared to the initial cost. In addition to the presentation of the experimental results, we also include a novel analysis on the costs of two special context-free grammars, where one derives only the set of strings in the corpus and the other derives the set of arbitrary strings from the alphabet. Index Terms: context-free grammar, Chinese language processing, description length, Academia Sinica Balanced Corpus. 
There are many methods to improve performances of statistical parsers. Among them, resolving structural ambiguities is a major task. In our approach, the parser produces a set of n-best trees based on a feature-extended PCFG grammar and then selects the best tree structure based on association strengths of dependency word-pairs. However, there is no sufficiently large Treebank producing reliable statistical distributions of all word-pairs. This paper aims to provide a self-learning method to resolve the problems. The word association strengths were automatically extracted and learned by parsing a giga-word corpus. Although the automatically learned word associations were not perfect, the built structure evaluation model improved the bracketed f-score from 83.09% to 86.59%. We believe that the above iterative learning processes can improve parsing performances automatically by learning word-dependence knowledge continuously from web. 1. Introduction How to solve structural ambiguity is an important task in building a high-performance statistical parser, particularly for Chinese. Since Chinese is analytic language, words play different grammatical functions without inflections. A great deal of ambiguous structures will be produced by parsers if no structure evaluator is applied. There are three main steps in our approach aim to disambiguate the structures. The first step is to have parser produce n-best structures. Secondly, we extract word-to-word association from large corpora and build semantic information. The last one is to build a structural evaluator to find the best tree structure from n-best. Formerly, there were some approaches proposed to resolve structure ambiguities. For instances, z to add on lexical dependencies. Collins (1999) solves structural ambiguity by extracting lexical dependencies from Penn WSJ Treebank and applying dependencies to the statistic model. Lexical dependency (or Word-to-word association, WA) is one type of semantic information. It is a current trend to add on semantic related information in traditional parsers. Some incorporated word-to-word association in their parsing models, such as the Dependency Parsing in Chen et al. (2004). They take advantage of statistic information of word dependency in the parsing process to produce dependency structures. However, word association methods suffer low coverage for lacking very large tree-annotated training corpora, while checking dependency relation between word pairs.  z to add on word semantic knowledge. CiLin and HowNet information are used in the statistic model in the experiment of Xiong et al. (2005). Their results prove to solve common parsing mistakes efficiently. z to use re-annotation method in grammar rule. Johnson (1998) thinks that re-annotating each node with the category of its parent category in Treebank is able to improve parsing performance. Klein et al. (2003) proposes internal/external/tag-splitting annotation strategies to obtain better results. z to build evaluator. Some people re-rank the structure values and find out the best parse (Collins, 2000; Charniak et al., 2005). At first hand, their parser produces a set of candidate parses for each sentence. Later, the reranker finds out the best tree through relevance features. The performance is better that without the reranker. This paper is going to show a self-learning method to produce imperfect (due to errors produced by automatic parsing) but unlimited amount of word association data to evaluate the n-best trees produced by a feature-extended PCFG grammar. The parser with this WA evaluation is considerably superior to those without evaluation. The organization of the paper is as follows: Section 2 describes how to generate n-best trees in a simple way. In Section 3, we account for building word-to-word association and a primitive semantic class as well. As to the design of evaluating model, our probability model, coordination of rule probability and word association probability are presented in section 4. In Section 5 we discuss and explain the experimental data and results. Ambiguities of PoS are to be considered in a practical system. Section 6 deals with further experiment on automatic tagging with PoS. Finally, we offer concluding remarks in section 7. 2. Feature extension of PCFG grammars for producing the n-best trees It is clear that Treebanks (Chen et al., 2003) provide not only instances of phrasal structures and word dependencies but also their statistical distributions. Recently, probabilistic preferences for grammar rules and feature dependencies were incorporated to resolve structure-ambiguities and had great improvements on parsing performances. However, the automatic extracted grammars and feature-dependence pairs suffer the problem of low coverage. We proposed different approaches to solve these two different types of low coverage problems. For the low coverage of extracted grammar, a linguistically-motivated grammar generalization method is proposed in Hsieh et al. (2005). And the low coverage of word association pairs is resolved by a self-learning method of automatic parsing and extracting word dependency pairs from very large corpora. The linguistically-motivated generalized grammars are derived from probabilistic context-free grammars (PCFG) by right-association binarization and feature embedding (Hsieh et al., 2005). The binarized grammars have better coverage than the original grammars directly extracted from treebank. Features are embedded in the lexical and phrasal categories to improve the precision of generalized grammar. The important features adopted in our grammar are described in the following: 2  Head (Head feature): Example: Linguistic motivations:  The PoS of phrasal head will propagate all intermediate nodes within the constituent. S(NP(Head:Nh: 他 )|S’-Head:VF(Head:VF: 叫 |S’-Head:VF(NP(Head:Nb: 李 四 )| VP(Head:VC:撿| NP (Head:Na:球))))) To constrain the sub-categorization frame.  Left (Leftmost feature): Example: Linguistic motivation:  The PoS of the leftmost constitute will propagate one–level to its intermediate mother-node only. S(NP(Head:Nh: 他 )|S’-Head:VF(Head:VF: 叫 |S’-NP(NP(Head:Nb: 李 四 )| VP(Head:VC:撿| NP(Head:Na:球))))) To constrain linear order of constituents.  Head 0/1 (Existence of phrasal head): Example: Linguistic motivation:  If phrasal head exists in intermediate node, the nodes will be marked with feature 1; otherwise 0. S(NP(Head:Nh:他)|S’-1(Head:VF:叫|S’-0(NP(Head:Nb:李四)|VP(Head:VC: 撿| NP(Head:Na:球))))) To enforce unique phrasal head in each phrase.  There are two functions in applying the embedded features: one is to increase the precision of the grammar and the other is to produce more candidate parse structures. With features embedded in phrasal categories, PCFG parsers are forced to produce varieties of different possible structures1. In order to achieve a better n-best oracle performance (i.e. the ceiling performance achieved by picking the best structure from n bests), we designed some different feature-embedded grammars and try to find a grammar with the better n-best oracle performance. For instance, “S(NP(Head:Nh:他)|Head:VF:叫| NP(Head:Nb:李四)| VP(Head:VC:撿| NP(Head:Na:球)))”. The explanations of feature sets are as follow. Rule type-1: Intermediate node: add on “Left and Head 1/0” features. Non-intermediate node: if there is only one member in the NP, add on “Head” feature. Example: S(NP-Head:Nh(Head:Nh:他)|S’-Head:VF-1(Head:VF:叫|S’-NP-0(NP-Head:Nb(Head:Nb:李 四)|VP(Head:VC:撿| NP-Head:Na(Head:Na:球))))) Rule type-2: Intermediate node: add on “Left and Head 1/0” features. Non-intermediate node: add on “Head and Left” features, if there is only one member in the NP, add on “Head” feature. Example: S-NP-Head:VF(NP-Head:Nh(Head:Nh:他)|S’-Head:VF-1(Head:VF:叫|S’-NP-0(NP-Head:Nb(Head:Nb:李 四)|VP-Head:VC(Head:VC:撿| NP-Head:Na(Head:Na:球))))) Rule type-3: Intermediate: add on “Left, and Head 1/0” features. Top-Level node: add on “Head and Left” features. (see example of S ) -NP-Head:VF Non-intermediate node: if there is only one member in the NP, add on “Head” feature. Example: S-NP-Head:VF(NP-Head:Nh(Head:Nh:他)|S’-Head:VF-1(Head:VF:叫|S’-NP-0(NP-Head:Nb(Head:Nb:李 四)|VP(Head:VC:撿| NP-Head:Na(Head:Na:球)))))  
 (Bayesian predictive classification)  (MAP)  (MLLR)  (MCELR)  (prior density)  (hyperparameter)  (MCE)  1. recognition)  (pattern  (signal)  (feature) (speech enhancement)  (model parameter) (compensation)  (discriminability)  2. 2.1 [4][8]  adaptation)  (model-based adaptation)  2.1.1  (maximum a posteriori, MAP)[4] MAP  (feature -based  MAP X f (X | Λ)  Λ MAP g(Λ | X ) g(Λ) X  Λ MAP = arg max g(Λ | X ) = arg max f ( X | Λ)g (Λ)  (1)  Λ  Λ  g(Λ)  g(Λ)  (non-informative prior) (1)  N N (ρ,τ 2)  µˆMAP  =  Nτ 2 σ 2 + Nτ 2  x +  σ2  σ2 + Nτ 2  ρ  x  (2) N (x,σ 2 )  2.1.2  linear regression, MLLR)[3][15] class)  (maximum likelihood (regression  d ×1  Λs  Ws  µˆ s  µˆs = A s µ s + bs = Wsξs  µs (3)  ξs  = [1  µ  T s  ]T  d × (d +1)  (d +1) ×1  Ws = [ As bs ]  { ( ) ( )} bs (x)  =  
Traditional search engine presents search result based on keyword matching of users’ query. It is the simplest method to gather documents associated with specific keywords. However, it’s possible for users to acquire undesired results due to inadequate acquisition.. One of these problems is most common users of IR systems type short queries. (Shen et al., 2005) From a single query, however, the retrieval system can only have very limited clue about the user’s information need. Mostly, it’s hard for users to realize what actual searching requirement is when proceeding searching activities, so there are certain of measure to solve the problem about vague queries issued from users in the past years.  1.2 Research Objective In this article, we try to provide users more selective query keywords which are related to original query not only for the suggestion but also to help users realizing the real requirement in searching behaviors when users submit too brief query to find out more wanted documents. In addition, we also decide to deliver the decision making authority to users of which documents seem to be more preferred by them for achieving user center approach. Otherwise, in order to provide each user with more personal searching environment and contents, we endeavor to propose several approaches to adapt search results according to each user’s information need. 2. Related Work 2.1 Review of Refining Short Query To solve the problem of low retrieval performance caused by inappropriate query terms, automatic query expansion techniques have been studied for the past 30 years. In a recent study (Jansen et al., 2000,March 1), the number of query terms used by most end users was no more than 2 when searching with a Web search engine, which is even less than that of searching online databases. The same study also pointed out that only 5% of queries were accompanied by any relevance feedback feature. 2.2 Categories of Query Expansion Query expansion techniques fall into two categories according to the way of implementation. One is to add new terms to an original query before searching, and the other is to formulate a new query on the basis of some retrieved documents of the previous search (Qiu, 1995). While the former is usually called a global or corpus-specific query expansion, the latter is called a local or query-specific query expansion. Global query expansion rely on thesauri that is a manually-built resource, as though WordNet-based (Mandala et al., 1999) provides the relation types include coordination, synonyms, hyponyms and etc for expanding the feature of original query terms. Local query expansion, which corresponds to feedback retrieval, can acquire relevance information by either user feedback (Robertson & Sparck Jones, 1976; Rocchio, 1971) or system feedback. Query expansion using user feedback based on relevance judgment made by users, brought a significant improvement in retrieval performance (Harman, 1992 June; G. Salton & Buckley, 1990). Another two theories about local query expansion is Local co-occurrence method (HE et al., 2002; ZHANG et al., 2002) and Latent Semantic Indexing (LSi_based) (Deerwester et al., 1990) 2.3 Personal Data Construction There are several way to gather the user’s information for constructing unique data each end user belongs to. One of these approach is to have users describe their general interests. For example, Google Personal asks users to build a profile of themselves by selecting categories of interests. Google’s PageRank algorithm can be described as personal web search techniques augmenting traditional text matching with a global notion of “importance” based on the linkage structure of the web. This global notion of importance can be specialized to create personalized views of importance. User profile data provide information about the users of a Web site. A user profile contains  demographic information (such as name, age, country, marital status, education, interests, etc.) for each user of a Web site, as well as information about the users’ interests and preferences. Such information is acquired through registration forms or questionnaires, or can be inferred by analyzing Web usage logs (Eirinaki & Vazirgiannis, 2003, February). Personal profiles can also be combined with the method mentioned above in the context of the Web search to create a personalized version of PageRank for setting the query-independent priors on Web pages. (Teevan et al., 2005). (Liu et al., 2002) used a similar technique for mapping user’s queries to categories based on the user’s search history. 3. Interactive IR system Generally in interactive situation, system collects user’s intention through designed interactive interface. In principle, every action of the user can potentially provide new evidence to help the system to better infer the user’s information need. Thus in order to respond optimally, the system should use all the evidence collected so far about the user. After collection of the user information, how to effectively select and analyze these data is critical to this kind of system. To retrieve more user demanded results, we carry out Linear Least Squares Fit (LLSF) algorithm to generate personal profile by matrix combination in which the personal searching result will be formed in document-term (DT) matrix, and Singular Value Decomposition (SVD) is used to reduce the dimensions of the original DT matrix. Moreover combining of document-cluster matrix and decomposed DT is to produce the final user profile M matrix. This process is also called Latent Semantic Indexing, which could extract the context-based terms out for expanding personalized query terms. Simultaneously, as far as possible to promote the retrieval accuracy, relevance feedback of probabilistic model is suitable to be involved in. And in the traditional Retrieval method likes TF*IDF weighting schema, existing problem of mis-weighting could be caused the poor retrieval result. To overcome this defect, we adopt smoothing function of TF*IDF which could be diminished the inadequate weighting result. Finally we try to optimize the result representation, the ranking algorithm is also seen to be critical. For improving the Term Frequency (TF) ranking model, ranking function considering density distribution is brought into our framework. 3.1 Retrieval Method After word recognition, each document is represented as a bag of words, but it does not mean that every word is a meaningful unit. For subsequently retrieval purpose, we need to set every recognized term a appropriate weight. Furthermore, when users try to issue single or shorter query for searching, we use traditional keyword matching method to catch documents indicated by the user as relevant and conduct query expansion from these first time extraction documents in which we expect to get a list of longer query words, then we carry out traditional vector space model (VSM) to extract more query-relevant documents for generating more user demanded queries. Because of the classical term weight model, TF*IDF scheme, usually has mis-weighting problem.  For example, a single document that contains the word “ERP” which only appears one time should not be deemed as relevant to a query containing “ERP” as a longer article that contains 20 occurrences of the word “ERP”. On the other hand, we ought not to assume that the longer document is 20 times more relevant. For this reason we prefer a smoothed version of TF and IDF(Croft & Harper, 1979) as listed below: A common term frequency (TF) expression is then modified:  TF = f (K + 1) f + KL  (1)  where L = the normalized length of document D. If the document is of average length, then L = 1.0. K = a constant, usually set between 1.0 and 2.0. f = specific term occurs in single document. The TF component is designed to increase in value quite modestly as f arises. For instance, if f ,K and L are 1, then TF = 1.0. If f were 9, then TF = 1.8. We can properly avoid the mis-weighting problem of conventional TF through this kind of effort. Smoothing inverse document frequency (IDF) prevents division by zero in the case where a term does not occur in the document collection at all.  IDFt  =  log( N − nt + 0.5) nt + 0.5  (2)  where N = the size of the collection, nt = the number of documents containing a given term, t.  3.2 Ranking Result As noted above, a Boolean search generally returns sets of documents that are unordered, or ordered by certain criteria unrelated to relevance, such as time or date. Most Web search engines are based on a different technology that ranks search results based upon the frequency distribution, term frequency, of query terms in the document collection. To cite an instance, if a document contains many occurrences of a query term “ERP”, this suggests that the document might be highly relevant to a query like “There are many software providers have ERP solutions, and the follow name lists which is one of the ERP providers?” For this reason, we consider several criteria to consider document ranking score, then we expect document which is more relevant to user’s demand will be rank in higher place through sorting specific ranking score. The viewpoint of our criteria separated into four factors between single keyword and individual document. There respectively are similarity, density, term frequency and title appearance. 3.2.1 Similarity We retrieve documents in VSM-model by comparing similarity information sim(k,d) among a keyword k and a document d, then defining a positive threshold value for judging which one passing this value is seen to be relevant. So the single item gets a higher similarity value that we have confidence which one is more relevant to issued query keyword. 3.2.2 Maximum Entropy Density Function By contrast, conventional ranking technology gives score to documents merely considered term frequency and regardless of the density distribution of specific keyword in subject document. But if  terms stated to be highly concentrated, it maybe mean that some topic is intensely described somewhere. So we carry out Maximum Entropy Function used to examine the density distribution of query keyword k, instead of just term frequency in considering document score. The original equation as formula (3) below, the value of E(K) becomes higher when p(k) in a average value that means probability distribution of k is more steady; E(K) has a lower value when p(k) is extremely in high and low value.  E(K ) = −∑ p(k) log p(k)  (3)  k∈K  So the entropy equation is revised to formula (4) (K. F. Jea & P. Y. Hsu, 2000), for ensuring the  state between E(K) and p(k) is positive in synchronous up and down.  E(K ) = −∑ p(k) log[1− p(k)]  (4)  k∈K  In physics, the meaning of density is that the degree of object distribution in the unit space.  Accordingly considering the keyword density distribution in unit length of document will be more  closed to reality and achieve the normalization.  After normalization adjustment, entropy equation is represented as follows formula (5):  ∑ n Es (K ) = − i =1  pi (k) log[1− Si  pi (k)]  (5)  where Pi(x) = the occurrence probability of term k in sentence i, Si = the length of sentence i, n =  number of sentences in a document. By this treatment, we can differentiate when document with same  term frequency of query keyword, and then rank them by density distribution consideration.  3.2.3 Term Frequency  Although term frequency (tf) is basis to rank the documents, high occurrence of keywords in a  document indicates that the weight of this document is remarkable significance. Therefore, we also  adopt concept of term frequency to ensure our ranking model. But basic tf weighting method emerges the problem of mis-weighting, likes mentioned before, so we transfer raw tf (ki , d j ) into normalization  according to maximum frequency of any term Maxtf j in a document d j .  3.2.4 Title Appearance When author composes particular topic, title often brings out overall theme or subject within article content. People surf on a search engine or even read news article, using title to decide whether to enter a website or further read an article they are interested is always an obviously evidence that these titles engage their concern. In the other words, if the numbers of query keywords k in a document’s title t have a higher frequency f (ki , t j ) means that this article is considered to be more relevant by the user. We formulate an equation of this concept as WT * f (ki ,t j ) , where WT is a constant we can adjust to determine the weighted stress of this factor. 3.2.5 Rscore Ranking To sum up these ranking factors, we merge these variables into single equation as formula (6):  Rscore  =  F (ki ,t j  )*[sim(ki , d j  )  +  Es (K )  +  tf (ki , d j )] Maxtf j  (6)  F  (ki ,  t  j  )  =  1,  WT  f (ki ,t j * f (ki,t  )= j ),  0  f  (ki  ,t  j  )  ≥  1,...n  where f (ki ,t j ) ∈ N , WT ∈ N , WT = weighting stress for occurrence of keyword in a title, f (ki ,t j ) = occurrence of keyword k in a title t, sim(ki , d j ) = similarity value between keywords k and a document d, Es (K) = sum of each keyword’s entropy value in a document d, tf (ki , d j ) = the frequency of keyword k in a document d, Maxtf j = the maximum frequency of any keyword k in a document d.  3.3 Query Expansion 3.3.1 Probabilistic Models of Query Expansion In a probabilistic framework, selecting terms and computing relevance weights are treated as two different problems. This model is used to compute more accurate weight estimates. Consider the term incidence contingency table in Table 1.  Table 1. Term Incidence Contingency Table (Jackson & Moulinier, 2002)  Containing the term Not containing the term Total  Relevant r R-r R  Non-relevant n-r (N-n)-(R-r) N-R  Total n N-n N  where N = the number of documents in the collection, R = the number of relevant documents for  this query, n = the number of documents having term t, r = the number of relevant documents  containing the term t. The term weight from the equation which we mentioned above, would then be  modified to take account of the relevance information as follows:  w' t,d  =  f (K +1) log (r + 0.5)(N − n − R + r + 0.5)  f + KL  (R − r + 0.5)(n − r + 0.5)  (7)  We utilize this re-expressed formula to re-weight the term within the vector space model when the user explicitly checks the retrieved document seen to be relevant or non-relevant. Subsequently, here address how terms are selected for expanding activity. In table 2 , we can obviously observe the post weighted scores are risen when choosing a list of documents relevant to the topic of “ERP”, ”PeopleSoft”, ”SAP” and ”Oracle” and non-relevant to “J.D.Edwards”, “鼎新”, “軟體部” and “Siebel”.  Table 2. Comparison of Re-weight activity  ERP PeopleSoft  SAP  Oracle J.D.Edwards 鼎新  軟體部  Siebel  Initial Weight 2.718 3.365  Re-weight  3.714 5.329  2.792 4.752  2.681 3.253  5.953 5.986  5.302 4.192  4.117 3.390  4.013 3.886  This model discussed by Robertson(1990) considers the distribution of scores for relevant and  non-relevant documents. The model leads to an “offer weight”, the larger the offer weight, the better  the candidate, which is used to rank candidate terms.  OWt  =  rt  log  (rt + 0.5)(N − nt − R + rt + 0.5) (R − rt + 0.5)(nt − rt + 0.5)  (8)  This two model proposed by Robertson tightly integrates query expansion using relevance  feedback and probabilistic retrieval.  3.3.2 LLSF Models of Query Expansion  At the beginning of the second stage expansion, we prefer to take user profile that not only has benefit  to provide extra information about personal search intention, but also greatly reduce the falsehood of  retrieved result. Furthermore, we adopt algorithm with respect to noise reducing, the Linear Least  Squares Fit (LLSF) method proposed by (Liu et al., 2002), to construct matrix as personal user profile.  In order to have one of the matrixes, we first need to introduce our cluster method with respect to  Single-Pass Clustering and 2-way K-Nearest-Neighbors (KNN) of Topic Detection and Tracking  (TDT).  3.3.2.1 TDT Clustering 1. Detecting New Cluster  A large number of clustering methods were studied in IR research. This section we adopt the TDT  proposed by CMU (Tang et al., 1999). One of the two algorithms is Single-Pass Clustering (SPC) for  clustering task and the other is 2-way K-Nearest-Neighbors (KNN) for automatic classification. Figure  1. demonstrates SPC flow chart.  Figure 1. Flow chart of Single-Pass Clustering Single-pass clustering follow the process as listed below and apply cosine as similarity calculation function: (1) Above all, taking out the first item in document collection as the first cluster. (2) Then take out the second item, calculating the similarity between item and clusters have been created. (3) If there is no similarity passes the threshold, instantaneously letting the incoming item be a new single cluster. (4) If the similarity passes the threshold we just set before, therefore categorize incoming item into appropriate candidate cluster. (5) If step 4 is selected, rescoring the centroid vector space of this cluster. (6) Iterating step 2 to 6, until dealing with entire incoming items. 2. Automatic Classifying 2-way KNN in TDT is used to classify the incoming item into proper classification by computing the relevance score. Which refers to compare objective cluster and else cluster that both take numbers of k Nearest-Neighbors. Objective clusters with respect to documents in this clusters which are prepared for  comparison; else cluster means documents in the clusters which different from objective clusters in the  candidate clusters. Formula (9) explains the calculation of relevance score.  r relevance _ score(x, kp, kn, D) =  
Linguistics is a science because linguists test hypotheses against empirical data, but this testing is done in a much more informal way than in almost any other science. Theoretical syntacticians, for example, violate protocols standard in the rest of the cognitive sciences by acting simultaneously as experimenter and subject, and by showing little concern with the issues of experimental design and quantitative analysis deemed essential in most sciences. Linguists recognize that their informally-collected data are often inconclusive; controversial native-speaker judgments are commonplace problems in both research and teaching. From my conversations with syntacticians, I get the sense that they would appreciate a tool for collecting judgments more reliably, yet this tool should be one that permits them to maintain their traditional focus on theory rather than method. This is where MiniJudge comes in. MiniJudge (www.ccunix.ccu.edu.tw/~lngproc/MiniJudge.htm) is software to help theoretical syntacticians design, run, and analyze linguistic judgment experiments quickly and painlessly. Because MiniJudge experiments involve testing the minimum number of speakers and sentences in the shortest amount of time, all while sacrificing the least amount of statistical power, I call them "minimalist" experiments. In this paper I first argue why a tool like MiniJudge is necessary. I then walk through a sample MiniJudge experiment on Chinese. Finally, I reveal MiniJudge's inner workings, which involve some underused or novel statistical techniques. Currently the only implementation of MiniJudge is MiniJudgeJS, which is written in JavaScript, HTML, and the statistical language R (www.r-project.org). 2. Balancing speed and reliability in syntactic judgment collection Though some readers may wonder why we should bother with judgments when we can simply analyze corpora, judgments and corpus data are actually complementary performance windows into linguistic competence, with their own strengths and weaknesses (see e.g. Penke & Rosenbach, 2004). The  question is how we can extract the maximum value out of judgments in the easiest possible way. 2.1. Experimental syntax Phillips and Lasnik (2003:61) are entirely right to emphasize that the "[g]athering of native-speaker judgments is a trivially simple kind of experiment, one that makes it possible to obtain large numbers of highly robust empirical results in a short period of time, from a vast array of languages." Even Labov (1996:102), who generally favors corpus data, admits that "[f]or the great majority of sentences cited by linguists," native-speaker intuitions "are reliable." Yet as Phillips and Lasnik (2003:61) also point out, "it is a truism in linguistics, widely acknowledged and taken into account, that acceptability ratings can vary for many reasons independent of grammaticality." Unfortunately, in actual practice linguists don't take the distinction between "acceptability" and "grammaticality" as seriously as they know they should, and their "trivially simple methods" become merely simple-minded (Schütze, 1996). Since the problem of detecting competence in performance is precisely the problem faced by experimental cognitive scientists every day (e.g., testing vision theories with optical illusions), a reasonable response to the syntactician's empirical challenges would be to adopt the protocols standard in the rest of the experimental cognitive sciences: multiple stimuli and subjects (naive ones rather than the bias-prone experimenters themselves), systematic controls, factorial designs, continuous response measures, filler items, counterbalancing, and statistical analysis. When judgments are collected with these more careful protocols, they often reveal hitherto unsuspected complexity. Recent examples of the growing experimental syntax literature include Sorace & Keller (2005) and Featherston (2005); Cowart (1997) is a user-friendly handbook. 2.2. Minimalist experimental syntax Full-fledged experimental syntax is complex, forcing the researcher to spend a lot of time on work that is not theoretically very interesting. The complexity of an experiment should actually be proportional to the subtlety of the effects it is trying to detect. Very clear judgments are detectable with traditional "trivially simple" methods; very subtle judgments may require full-fledged experimental methods. But in the vast area in between, a compromise seems appropriate, where methods are powerful enough to yield statistically valid results, yet are simple enough to apply quickly: a minimalist experimental syntax (see Table 1).  Table 1. Defining characteristics of minimalist experimental syntax  Binary yes/no judgments Experimental sentences only (no fillers) Very few sentence sets (about 10) Very few speakers (about 10-20)  No counterbalancing of sentence lists Maximum of two binary factors Random sentence order Order treated as a factor in the statistics  While conducting a minimalist experiment is much simpler than conducting a full-fledged judgment experiment (an explicit guide is given in Myers 2006), some steps may still be overly complex and/or intimidating to the novice experimenter, in particular the design of the experimental sentences and the statistical analysis. The purpose of the MiniJudge software is to automate these steps.  3. Using MiniJudge To show how MiniJudge is used, I describe a recent application of it to a morphosyntactic issue in Chinese; for another example, see MJInfo.htm#resultshelp, reachable through the MiniJudge homepage. MiniJudge has also been used to run syntax experiments on English and Taiwan Sign Language, as well as to run pilots for larger studies and to help teach basic concepts in experimental design. 3.1. Goal of the experiment He (2004) presents an interesting observation regarding the interaction of compound-internal phrase structure and affixation of the plural marker men. Part of his paradigm is shown in Table 2, where V = verb and O = object (based on his (2) & (4), pp. 2-3).  Table 2. The VOmen paradigm of He (2004)  [+VO] [-VO]  [+men] *zhizao yaoyan zhe men make rumor person PLURAL yaoyan zhizao zhe men rumor make person PLURAL  [-men] zhizao yaoyan zhe make rumor person yaoyan zhizao zhe rumor make person  He's analysis is not relevant here; the question is simply whether or not his observation about the judgment pattern in Table 2 is empirically correct. As a non-native speaker of Chinese, I have no intuitions myself. When I have informally asked colleagues and students to double-check the judgments, I have received a mixed response, with some ruling out men or VO entirely, but this misses the point, since He's claim concerns the ungrammaticality of the VOmen form relative to all the others. Some speakers shown He's starred and non-starred examples are willing to agree with his judgments, but it's likely that the star pattern has biased them. It may also be that He's generalization works for the few examples he cites, but fails in general. My goal, then, was to use MiniJudge to generate more examples to test systematically on native speakers. 3.2. The MiniJudgeJS interface MiniJudgeJS is simply a JavaScript-enabled HTML form. Input and output are handled entirely by text areas; generated text includes code to run statistical analyses in R. Like the rest of the MiniJudge family, MiniJudgeJS divides the experimental process into the steps listed in Table 3.  Table 3. The steps used by MiniJudge  I. Design experiment  II. Run experiment  Choose experimental factors Choose number of speakers  Choose set of prototype sentences Write instructions for speakers  Choose number of sentence sets Print or email survey forms  Segment prototype set (optional) Save schematic survey file  Replace segments (optional)  Save master list of test sentences  III. Analyze experiment Download and install R Enter raw results Generate data file Save data file Generate R code Paste R command code into R  3.3. Designing the experiment A MiniJudge experiment begins by choosing the experimental factors. In the case of the VOmen claim, the paradigm in Table 2 is derived via two binary factors: [±VO] (VO vs. OV) and [±men] (with or without men suffixation). As noted above, He's observation doesn't relate to each factor separately, but rather to an interaction: the combination of the factor values [+VO] and [+men] is claimed to result in lower acceptability, relative to overall judgments for [+VO] and for [+men]. The next step is to enter the prototype set of sentences (a pair if one factor, a quartet if two factors). Similar to the example sets shown in syntax papers and presentations, the prototype set serves multiple purposes. Most fundamentally, it helps to make the logic of factorial experimental design intuitive for novice experimenters. Syntacticians are not always aware of the importance of contrasting sentences that differ only in theoretically relevant factors, or of the central role played by interactions in many syntactic claims (for further discussion of the relevance of factors and interactions in syntax experiments, see MJInfo.htm#factorial and MJInfo.htm#interact). Another purpose of the prototype set is that it can be used to help generate further sentence sets that maintain the same factorial contrasts but vary in irrelevant lexical properties. In the case of the present experiment, the claim made in He (2004) says nothing about the particular verb, object, or head that is used. Thus the judgment pattern claimed for Table 2 above should also hold for the sets shown in Table 4 below, regardless of any additional influences from pragmatics, frequency, suffixlikeness (zhe vs. the others), or freeness (ren vs. the others); the stars here represent what He might predict (lexical content for the new sets was chosen with the help of Ko Yu-guang and Zhang Ning).  Table 4. Extending the VOmen paradigm of He (2004)  [+VO] [-VO] [+VO] [-VO]  [+men] *chuanbo bingdu yuan men spread virus person PLURAL bingdu chuanbo yuan men virus spread person PLURAL *sheji shipin ren men design ornaments person PLURAL shipin sheji ren men ornaments design person PLURAL  [-men] chuanbo bingdu yuan spread virus person bingdu chuanbo yuan virus spread person sheji shipin ren design ornaments person shipin sheji ren ornaments design person  MiniJudge partly automates the process of creating new sentence sets by dividing up the prototype sentences into the largest repeating segments and replacing them with user-chosen substitutes. The prototype segments for Table 2 are shown in the first row of Table 5. The user only has to find parallel substitutes for four segments, rather than having to construct whole new sentences while keeping track of the factorial design (Table 5 also shows the segments needed to generate the new sets in Table 4). The segmentation and set generation processes are designed to work equally well in English-like and Chinese-like orthographies. Of course, since MiniJudge knows no human language, it sometimes makes strange errors, so users are allowed to correct its output, or even to generate new sets manually.  Table 5. Prototype segments and new segments for the VOmen experiment  Set 1 (prototype) segments: zhizao yaoyan zhe  men  Set 2 segments:  chuanbo bingdu yuan  men  Set 3 segments:  sheji  shipin  ren  men  After the user has corrected and approved the master list of sentences, it can be saved to a file for use in reports (as I am doing here). In the present experiment, the master list contained 48 sentences (12 sets of 4 sentences each). This is an unusually large number of sentences for a MiniJudge experiment; significant results have been found with experiments with as few as 10 sentences. 3.4. Running the experiment In order to run a MiniJudge experiment, the user must make three decisions. The first concerns the maximum number of speakers to test. It is possible to get significant results with as few as 7 speakers, but in the present experiment, I generated 30 surveys. As it turned out, only 18 surveys were returned. The second decision concerns whether surveys will be distributed by printed form or by email. In MiniJudgeJS, printing surveys involves saving the them from a text area and printing them with a word processor. MiniJudgeJS cannot send email automatically, so emailed surveys must be individually copied and pasted. In the present experiment, I emailed thirty students, former students, or faculty of my linguistics department who did not know the purpose of the experiment. The final decision concerns the instructions, which the user may edit from a default. MiniJudgeJS requires that judgments be entered as 1 (yes) vs. 0 (no); in the current version, if surveys are to be collected electronically, these judgments must be typed before each sentence ID number. Chinese instructions for the VOmen experiment were written with the help of Ko Yu-guang. Surveys themselves are randomized individually to prevent order confounds, as is standard in psycholinguistics. The randomization algorithm, taken from Cowart (1997:101), results in every sentence having an equal chance to appear at any point in the experiment (by randomization of blocks), while simultaneously distributing sentence types evenly and randomly. Each survey starts with the instructions, followed by a speaker ID number (e.g., "##02"), and finally the survey itself, with each sentence numbered in the order seen by the speaker. Because the speakers' surveys intentionally hide the factorial design, the experimenter must save this information separately in a schematic survey file. This file is meant to be read only by MiniJudgeJS; as an example, the first line of the schematic survey file for the present experiment is explained in Table 6.  Table 6. The structure of the schematic survey information file for the VOmen experiment  File line: 01 Explanation: speaker ID number  20  05  sentence ID set ID  number  number  01 order in survey  -VO  -men  value of first value of  factor  second factor  After completed surveys have been returned, the experimenter pastes them into a text area in any order (as long as each survey still contains its ID number), and pastes the schematic survey information back into another text window. MiniJudgeJS extracts judgments from the surveys and creates a data file  in which each row represents a single observation, with IDs for speakers, sentences, and sets, presentation order of sentences, factor values (1 for [+] and -1 for [-]), and judgments. As an example, the first three lines of the data file for the VOmen experiment are shown in Table 7.  Speaker 1 1  Table 7. First three lines of data file for the VOmen experiment  Sentence Set  20  5  45  12  Order VO  
1. Introduction 1.1. Background The amount of information available in English on the Internet has grown exponentially for the past few years. Although a myriad of data are at our disposal, non-native speakers often find it difficult to wade through all of it since they may not be familiar with the terms or idioms being used in the texts. To ease the situation, a number of online machine translation (MT) systems such as SYSTRAN and Google Translate provide translation of source text on demand. Moreover, online dictionaries have mushroomed to provide access at any time and everywhere for second language learners. 1.2. Motivation MT systems and bilingual dictionary are designed to provide the services for non-English speakers or to ease learning difficulties for second language learners. Both require a lexicon which can be derived from aligning words in a parallel corpus. Furthermore, second language learners can benefit by learning from example sentences with translations. By looking at bilingual examples, we acquire knowledge of the usage and meaning of word in context. With word alignment result of a sentence pair, it is much easier to grab the essential  concepts of unfamiliar foreign words in a sentence pair. For instance, consider the English sentence “These factors will continue to play a positive role after its return” with its segmented Chinese translation “࠰ಥ  Ϋᓥ  ‫  ܝ‬வԬ  ૢ΁  ਗ਼ึ  ᘱᚃ  ೯౨  ጐ฽  Ъ͜” shown in Figure 1, where the solid dark lines are word alignment results of them and e, f stand for two sentences in two languages E, F respectively. If we don’t know the usage of “play” in the sense of “perform,” in this example sentence pair with the help of word alignment, we would quickly understand such meaning and learn useful expressions like “play … role” meaning “೯ ౨ … Ъ͜” in Chinese. e: These factors will continue to play a positive role after its return  f : ࠰ಥ  Ϋᓥ  ‫  ܝ‬வԬ  ૢ΁  ਗ਼ึ  ᘱᚃ  ೯౨  ጐ฽  Ъ͜  Figure 1. An example sentence pair.  Table 1. The word alignment of the example sentence pair.  i  j  e i  f j  
tmjiang@iis.sinica.edu.tw, hsu@iis.sinica.edu.tw Abstract Many grammar checkers in rule-based approach do not handle errors that come from various usages, for example, the usages of prepositions. To study the behavior of prepositions, we introduce the language model into a grammarchecking task. A language model is trained from a large training corpus, which contains many short phrases. It can be used for detecting and correcting certain types of grammar errors, where local information is sufficient to make decision. We conduct several experiments on finding the correct English prepositions. The experiment results show that the accuracy of open test is 71% and the accuracy of closed test is 89%. The accuracy is 70% on TOEFL-level tests. Keywords: Language Model, Grammar Checker, English preposition usage  1. Introduction Computer-Aided Language Learning is a fascinating area; however, the computer still lacks many abilities of a human teacher, for example, the ability of grammar checking. Technically, it is hard to build a grammar checker that can deal with all types of errors. There are errors caused beyond the knowledge of syntax. For example, to overcome the misusing of prepositions, a system requires more semantic knowledge. There are three major approaches to implement a grammar checker. The first strategy is the syntax-based checking [Jensen et al., 1993]. In this approach, a sentence is parsed into a tree structure. A sentence is correct if it can be parsed completely. Another choice is the statisticsbased checking [Attwell, 1987]. In this approach, the system built a list of POS tag sequences based on a POS-annotated corpus. A sentence with known POS tag sequence is considered as a correct one. The last one is the rule-based checking [Naber 2003], where a set of rules is built manually and used to match against a text. Park et al. proposed an online English grammar checker for students who take English as the second language. This system focuses on a limited category of frequently occurring grammatical mistakes in essays written by students. The grammar knowledge is represented in Prolog language. [Park 1997] We find that most grammar checkers do not deal with the errors of preposition usage. We suppose that it should be hard to write rules for all of the prepositions. To evaluate this difficulty, we introduce the language model into the grammar-checking task. Since a language model is usually trained from a large training corpus, it may contain many short phrases with prepositions. The Language Model (LM) is one of the popular natural language processing technology for various applications, like information retrieval, handwriting recognition, speech recognition, and  machine translation. [Jurafsky and Martin, 2000] [Manning and Schutze, 1999] An LM uses short history to predict the next word. Word prediction is an essential subtask of speech recognition, handwriting character recognition, augmentative communication for the disabled, and spelling error detection. An LM can estimate the probability of a sentence. Therefore, it can be a way to distinguish good usages from bad ones of English prepositions. Figure 1 shows a general architecture of an English grammar checker. An ideal system should consist of both rule-based and language model approaches. Linguistic knowledge of the rulebased system is acquired from domain experts. Statistical knowledge of the language model is gathered from training corpus by programs. In this paper, we design several experiments to assess the ability of the LM on the preposition usage problem.  Sentence Checker  Knowledge Base of English  Domain Knowledge  User Interface  Input Text  Analysis  Rule Base (English grammar) Knowledge acquisition  Learner  Domain Expert  Language Model (N-gram model) Training Corpus  Figure1. The Architecture of a general English Grammar Checker  2. Statistical language model We briefly restate the notation of N-gram language model. In this model, a sentence is viewed as a sequence of n words. The probability of a sentence in a language, say English, is defined as the probability of the sequence. P(w1n ) ≡ P(w1, w2,..., wn) That can be further decomposed by the chain rule of conditional probability under the Markov assumption.  P(w1n  )  =  P(w1)P(w2  |  w1)P(w3  |  w12  )...P(wn  |  wn−1 1  )  ∏ = P(w1)  n k=2  P(wk  |  wk −1 1  )  Since it is not possible to collect all the history, a prefix of size N, as an approximation, is used to replace each component in the product.  P(wn  |  w1n −1 )  ≈  P(wn  |  w ) n−1 n− N +1  Usually, the N is 1, 2, or 3, are named as unigrams: P(wn), bi-grams: P(wn|wn-1), and tri-grams: P(wn| wn-1w n-2) model, respectively. Next step is to estimate the n-gram approximation from corpus. The basic way is called  Maximum Likelihood Estimation (MLE), which calculates the relative frequency and is used as  the estimation of probability. For bi-gram:  And, for n-gram  P(wn  |  wn −1 )  =  C(wn−1wn ) C ( wn −1 )  P(wn  |  wn−1 n−N  +1  )  =  C(wnn−−1N +1wn ) C(wnn−−1N +1)  where C represents the count of each specified n-grams w in the corpus. MLE works well for  high-frequency n-gram; however, no matter how large the corpus is, there are always some low-  frequency n-grams. The frequency might be very low even zero. Some zeroes are really zeroes,  which means that they represent meaningless word combinations. However, some zeroes are not  really zeroes. They represent low frequency events that simply did not occur in the corpus and  might exist in real world. When using n-gram model, we cannot assign a probability to a  sequence where one of the component n-gram has a value of zero. An alternative solution is to  smooth the probability estimations so that no component in the sequences is given a probability  of zero.  2.1 Smoothing methods  To cope with the problem of unseen data, several smoothing methods are developed [Goodman,  2002]; they can be classified as discounting methods and model combination methods.  Discounting methods adjust the probability estimators, so that zero relative frequency in the  training data does not imply zero relative counts. Model combination methods combine available  models (unigram, bi-gram, tri-gram, etc.) by interpolation and back-off. To our knowledge,  Good-Turing discounting, absolute discounting and Chen-Goodman modified Kneser-Ney  discounting are three of best smoothing methods; therefore, we use them in our experiments.  [Chen and Goodman, 1998]  2.1.1 Good-Turing Discounting (GT) Good-Turing discounting adjusts the count of n-gram from r to r*, which is base on the  assumption that their distribution is binomial [Good, 1953].  r * = (r + 1) N r+1 Nr  r<M  where Nr is types of n-gram occurring r times, and M is a threshold usually smaller than 5. Note  that for r=0,  r* = N1 N0  where N0 is the number of n-grams that never occurred. The discounted probabilities are thus:  PGT  (w1...wn )  =  r* N  The Good-Turing formula only applies to the situation when r < 5, and need to renormalize to  ensure that everything sums to one.  2.1.2 Absolute Discounting (AD) In the absolute discounting model, all non-zero frequencies are discounted by a small constant  discount rate b. And all the unseen events gain the frequency uniformly. [Ney et al., 1994]  ∑ N 0  ⋅ P0  =  
We introduce a new interactive corpus exploration tool called InfoMagnets. InfoMagnets aims at making exploratory corpus analysis accessible to researchers who are not experts in text mining. As evidence of its usefulness and usability, it has been used successfully in a research context to uncover relationships between language and behavioral patterns in two distinct domains: tutorial dialogue (Kumar et al., submitted) and on-line communities (Arguello et al., 2006). As an educational tool, it has been used as part of a unit on protocol analysis in an Educational Research Methods course.  and the corpus-based behavioral research community. The purpose of our demonstration is to make the language technologies community more aware of opportunities for applications of language technologies to support corpus oriented behavioral research.  
This demonstration provides a historical perspective of a number of research and commercial systems in Spoken Language Technology over the past 20+ years. A series of chronologically ordered video clips from many sources will be presented to illustrate the many steps and the tremendous progress that has been achieved over the years. The clips themselves are drawn from diverse academic and commercial research labs, product presentations, and user applications. All show systems being demonstrated or in actual use. Over 20 different laboratory systems, products, and companies are represented in this collection of video materials. Each of the clips has previously been shown publicly. The present selection primarily focuses on speech and natural language systems for speech recognition and synthesis. Additional contributions to this collection are welcome. 1. Project Description Preparations of these materials are being done in conjunction with the History of Speech and Language Technology Project being conducted by Saras Institute, in affiliation with the Dibner Institute for the History of Science and Technology, at MIT (Cambridge, MA). The overall mission for this project is to collect, preserve, and make readily available information about significant research discoveries, technical achievements, and business 
We have implemented SmartNotes, a system that automatically acquires labeled meeting data as users take notes during meetings and browse the notes afterwards. Such data can enable meeting understanding components such as topic and action item detectors to automatically improve their performance over a sequence of meetings. The SmartNotes system consists of a laptop based note taking application, and a web based note retrieval system. We shall demonstrate the functionalities of this system, and will also demonstrate the labeled data obtained during typical meetings and browsing sessions. 
The MTTK alignment toolkit for statistical machine translation can be used for word, phrase, and sentence alignment of parallel documents. It is designed mainly for building statistical machine translation systems, but can be exploited in other multi-lingual applications. It provides computationally efﬁcient alignment and estimation procedures that can be used for the unsupervised alignment of parallel text collections in a language independent fashion. MTTK Version 1.0 is available under the Open Source Educational Community License. 
The semantic web (SW) vision is one in which rich, ontology-based semantic markup will become widely available. The availability of semantic markup on the web opens the way to novel, sophisticated forms of question answering. AquaLog is a portable question-answering system which takes queries expressed in natural language (NL) and an ontology as input, and returns answers drawn from one or more knowledge bases (KB). AquaLog presents an elegant solution in which different strategies are combined together in a novel way. AquaLog novel ontology-based relation similarity service makes sense of user queries. 
A general-purpose text annotation tool called Knowtator is introduced. Knowtator facilitates the manual creation of annotated corpora that can be used for evaluating or training a variety of natural language processing systems. Building on the strengths of the widely used Protégé knowledge representation system, Knowtator has been developed as a Protégé plug-in that leverages Protégé’s knowledge representation capabilities to specify annotation schemas. Knowtator’s unique advantage over other annotation tools is the ease with which complex annotation schemas (e.g. schemas which have constrained relationships between annotation types) can be defined and incorporated into use. Knowtator is available under the Mozilla Public License 1.1 at http://bionlp.sourceforge.net/Knowtator. 
SenseClusters is a freely available system that clusters similar contexts. It can be applied to a wide range of problems, although here we focus on word sense and name discrimination. It supports several different measures for automatically determining the number of clusters in which a collection of contexts should be grouped. These can be used to discover the number of senses in which a word is used in a large corpus of text, or the number of entities that share the same name. There are three measures based on clustering criterion functions, and another on the Gap Statistic. 
 Ndaona is a Matlab toolkit to create interactive three-dimensional models of data often found in NLP research, such as exploring the results of classiﬁcation and dimensionality reduction algorithms. Such models are useful for teaching, presentations and exploratory research (such as showing where a classiﬁcation algorithm makes mistakes). Ndaona includes embedding and graphics parameter estimation algorithms, and generates ﬁles in the format of Partiview (Levy, 2001), an existing free open-source fast multidimensional data displayer that has traditionally been used in the planetarium community. Partiview1 supports a number of enhancements to regular scatterplots that allow it to display more than three dimensions’ worth of information. 
We will demonstrate SconeEdit, a new tool for exploring and editing knowledge bases (KBs) that leverages interaction with domain texts. The tool provides an annotated view of user-selected text, allowing a user to see which concepts from the text are in the KB and to edit the KB directly from this Text View. Alongside the Text View, SconeEdit provides a navigable KB View of the knowledge base, centered on concepts that appear in the text. This unified tool gives the user a text-driven way to explore a KB and add new knowledge. 
This paper describes an automated system for assigning quality scores to recorded call center conversations. The system combines speech recognition, pattern matching, and maximum entropy classiﬁcation to rank calls according to their measured quality. Calls at both ends of the spectrum are ﬂagged as “interesting” and made available for further human monitoring. In this process, the ASR transcript is used to answer a set of standard quality control questions such as “did the agent use courteous words and phrases,” and to generate a question-based score. This is interpolated with the probability of a call being “bad,” as determined by maximum entropy operating on a set of ASR-derived features such as “maximum silence length” and the occurrence of selected n-gram word sequences. The system is trained on a set of calls with associated manual evaluation forms. We present precision and recall results from IBM’s North American Help Desk indicating that for a given amount of listening effort, this system triples the number of bad calls that are identiﬁed, over the current policy of randomly sampling calls. The application that will be demonstrated is a research prototype that was built in conjunction with IBM’s North American call centers. 1. INTRODUCTION Every day, tens of millions of help-desk calls are recorded at call centers around the world. As part of a typical call center operation a random sample of these calls is normally re-played to human monitors who score the calls with respect to a variety of quality related questions, e.g. • Was the account successfully identiﬁed by the agent? • Did the agent request error codes/messages to help determine the problem? • Was the problem resolved? • Did the agent maintain appropriate tone, pitch, volume and pace? This process suffers from a number of important problems: ﬁrst, the monitoring at least doubles the cost of each call (ﬁrst an operator is paid to take it, then a monitor to evaluate it). This causes the second problem, which is that therefore only a very small sample of calls, e.g. a fraction of a percent, is typically evaluated. The third problem arises from the fact that most calls are ordinary and uninteresting; with random sampling, the human monitors spend most of their time listening to uninteresting calls. This work describes an automated quality-monitoring system that addresses these problems. Automatic speech recognition is used to transcribe 100% of the calls coming in to a call center, and default quality scores are assigned based on features such as  key-words, key-phrases, the number and type of hesitations, and the average silence durations. The default score is used to rank the calls from worst-to-best, and this sorted list is made available to the human evaluators, who can thus spend their time listening only to calls for which there is some a-priori reason to expect that there is something interesting. The automatic quality-monitoring problem is interesting in part because of the variability in how hard it is to answer the questions. Some questions, for example, “Did the agent use courteous words and phrases?” are relatively straightforward to answer by looking for key words and phrases. Others, however, require essentially human-level knowledge to answer; for example one company’s monitors are asked to answer the question “Did the agent take ownership of the problem?” Our work focuses on calls from IBM’s North American call centers, where there is a set of 31 questions that are used to evaluate call-quality. Because of the high degree of variability found in these calls, we have investigated two approaches: 1. Use a partial score based only on the subset of questions that can be reliably answered. 2. Use a maximum entropy classiﬁer to map directly from ASR-generated features to the probability that a call is bad (deﬁned as belonging to the bottom 20% of calls). We have found that both approaches are workable, and we present ﬁnal results based on an interpolation between the two scores. These results indicate that for a ﬁxed amount of listening effort, the number of bad calls that are identiﬁed approximately triples with our call-ranking approach. Surprisingly, while there has been signiﬁcant previous scholarly research in automated call-routing and classiﬁcation in the call center , e.g. [1, 2, 3, 4, 5], there has been much less in automated quality monitoring per se. 2. ASR FOR CALL CENTER TRANSCRIPTION 2.1. Data The speech recognition systems were trained on approximately 300 hours of 6kHz, mono audio data collected at one of the IBM call centers located in Raleigh, NC. The audio was manually transcribed and speaker turns were explicitly marked in the word transcriptions but not the corresponding times. In order to detect speaker changes in the training data, we did a forced-alignment of the data and chopped it at speaker boundaries. The test set consists of 50 calls with 113 speakers totaling about 3 hours of speech. 2.2. Speaker Independent System The raw acoustic features used for segmentation and recognition are perceptual linear prediction (PLP) features. The features are  292 Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume, pages 292–295, New York City, June 2006. c 2006 Association for Computational Linguistics  Segmentation/clustering Manual Manual Manual Automatic Automatic  Adaptation Off-line Incremental No Adaptation Off-line Incremental  WER 30.2% 31.3% 35.9% 33.0% 35.1%  Table 1. ASR results depending on segmentation/clustering and adaptation type.  Accuracy Top 20% Bottom 20%  Random 20%  20%  QA  41%  30%  Table 2. Accuracy for the Question Answering system.  mean-normalized 40-dimensional LDA+MLLT features. The SI acoustic model consists of 50K Gaussians trained with MPE and uses a quinphone cross-word acoustic context. The techniques are the same as those described in [6]. 2.3. Incremental Speaker Adaptation In the context of speaker-adaptive training, we use two forms of feature-space normalization: vocal tract length normalization (VTLN) and feature-space MLLR (fMLLR, also known as constrained MLLR) to produce canonical acoustic models in which some of the non-linguistic sources of speech variability have been reduced. To this canonical feature space, we then apply a discriminatively trained transform called fMPE [7]. The speaker adapted recognition model is trained in this resulting feature space using MPE. We distinguish between two forms of adaptation: off-line and incremental adaptation. For the former, the transformations are computed per conversation-side using the full output of a speaker independent system. For the latter, the transformations are updated incrementally using the decoded output of the speaker adapted system up to the current time. The speaker adaptive transforms are then applied to the future sentences. The advantage of incremental adaptation is that it only requires a single decoding pass (as opposed to two passes for off-line adaptation) resulting in a decoding process which is twice as fast. In Table 1, we compare the performance of the two approaches. Most of the gain of full ofﬂine adaptation is retained in the incremental version. 2.3.1. Segmentation and Speaker Clustering We use an HMM-based segmentation procedure for segmenting the audio into speech and non-speech prior to decoding. The reason is that we want to eliminate the non-speech segments in order to reduce the computational load during recognition. The speech segments are clustered together in order to identify segments coming from the same speaker which is crucial for speaker adaptation. The clustering is done via k-means, each segment being modeled by a single diagonal covariance Gaussian. The metric is given by the symmetric K-L divergence between two Gaussians. The impact of the automatic segmentation and clustering on the error rate is indicated in Table 1.  Accuracy Top 20% Bottom 20%  Random 20%  20%  ME  49%  36%  Table 3. Accuracy for the Maximum Entropy system.  Accuracy Top 20% Bottom 20%  Random 20%  20%  ME + QA 53%  44%  Table 4. Accuracy for the combined system.  3. CALL RANKING 3.1. Question Answering This section presents automated techniques for evaluating call quality. These techniques were developed using a training/development set of 676 calls with associated manually generated quality evaluations. The test set consists of 195 calls. The quality of the service provided by the help-desk representatives is commonly assessed by having human monitors listen to a random sample of the calls and then ﬁll in evaluation forms. The form for IBM’s North American Help Desk contains 31 questions. A subset of the questions can be answered easily using automatic methods, among those the ones that check that the agent followed the guidelines e.g. • Did the agent follow the appropriate closing script? • Did the agent identify herself to the customer? But some of the questions require human-level knowledge of the world to answer, e.g. • Did the agent ask pertinent questions to gain clarity of the problem? • Were all available resources used to solve the problem? We were able to answer 21 out of the 31 questions using pattern matching techniques. For example, if the question is “Did the agent follow the appropriate closing script?”, we search for “THANK YOU FOR CALLING”, “ANYTHING ELSE” and “SERVICE REQUEST”. Any of these is a good partial match for the full script, “Thank you for calling, is there anything else I can help you with before closing this service request?” Based on the answer to each of the 21 questions, we compute a score for each call and use it to rank them. We label a call in the test set as being bad/good if it has been placed in the bottom/top 20% by human evaluators. We report the accuracy of our scoring system on the test set by computing the number of bad calls that occur in the bottom 20% of our sorted list and the number of good calls found in the top 20% of our list. The accuracy numbers can be found in Table 2. 3.2. Maximum Entropy Ranking Another alternative for scoring calls is to ﬁnd arbitrary features in the speech recognition output that correlate with the outcome of a call being in the bottom 20% or not. The goal is to estimate the probability of a call being bad based on features extracted from the automatic transcription. To achieve this we build a maximum  293  Fig. 1. Display of selected calls.  Fig. 2. Interface to listen to audio and update the evaluation form.  entropy based system which is trained on a set of calls with associated transcriptions and manual evaluations. The following equation is used to determine the score of a call C using a set of N predeﬁned features:  P (class/C)  =  
Structural information in language is important for obtaining a better understanding of a human communication (e.g., sentence segmentation, speaker turns, and topic segmentation). Human communication involves a variety of multimodal behaviors that signal both propositional content and structure, e.g., gesture, gaze, and body posture. These non-verbal signals have tight temporal and semantic links to spoken content. In my thesis, I am working on incorporating non-verbal cues into a multimodal model to better predict the structural events to further improve the understanding of human communication. Some research results are summarized in this document and my future research plan is described. 
This paper summarizes a largely automated method that uses online post-editing feedback to automatically improve translation rules. As a starting point, bilingual speakers’ local fixes are collected through an online Translation Correction Tool. Next, the Rule Refinement Module attacks the problem at its core and uses the local fixes to detect incorrect rules that need to be refined. Once the grammar and lexicon have been refined, the Machine Translation system not only produces the correct translation as fixed by the bilingual speaker, but is also able to generalize and correctly translates similar sentences. Thus, this work constitutes a novel approach to improving translation quality. Enhanced by the reaching power of the Internet, our approach becomes even more relevant to address the problem of how to automatically improve the quality of Machine Translation output. 
My PhD research has been on the algorithmic and formal aspects of computational linguistics, esp. in the areas of parsing and machine translation. I am interested in developing efﬁcient algorithms for formalisms with rich expressive power, so that we can have a better modeling of human languages without sacriﬁcing efﬁciency. In doing so, I hope to help integrating more linguistic and structural knowledge with modern statistical techniques, and in particular, for syntax-based machine translation (MT) systems. Among other projects, I have been working on kbest parsing, synchronous binarization, and syntaxdirected translation. 
In this paper we investigate the problem of identifying the perspective from which a document was written. By perspective we mean a point of view, for example, from the perspective of Democrats or Republicans. Can computers learn to identify the perspective of a document? Furthermore, can computers identify which sentences in a document strongly convey a particular perspective? We develop statistical models to capture how perspectives are expressed at the document and sentence levels, and evaluate the proposed models on a collection of articles on the Israeli-Palestinian conﬂict. The results show that the statistical models can successfully learn how perspectives are reﬂected in word usage and identify the perspective of a document with very high accuracy. 
The goal of my proposed dissertation work is to help answer two fundamental questions: (1) How is emotion communicated in speech? and (2) Does emotion modeling improve spoken dialogue applications? In this paper I describe feature extraction and emotion classiﬁcation experiments I have conducted and plan to conduct on three different domains: EPSaT, HMIHY, and ITSpoke. In addition, I plan to implement emotion modeling capabilities into ITSpoke and evaluate the effectiveness of doing so. 
We present our work on combining largescale statistical approaches with local linguistic analysis and graph-based machine learning techniques to compute a combined measure of semantic similarity between terms and documents for application in information extraction, question answering, and summarisation.  2 Dimensionality Reduction for Document and Term Representation A vector space representation of documents is very convenient because it puts documents in a Euclidean space where similarity measures such as inner product and cosine similarity or distance are immediately available. However, these measures will not be effective if they do not have a natural interpretation for the original text data.  
In this paper, we describe our hybrid approach to two key NLP technologies: biomedical named entity recognition (Bio-NER) and (Bio-SRL). In Bio-NER, our system successfully integrates linguistic features into the CRF framework. In addition, we employ web lexicons and template-based post-processing to further boost its performance. Through these broad linguistic features and the nature of CRF, our system outperforms state-ofthe-art machine-learning-based systems, especially in the recognition of protein names (F=78.5%). In Bio-SRL, first, we construct a proposition bank on top of the popular biomedical GENIA treebank following the PropBank annotation scheme. We only annotate the predicate-argument structures (PAS’s) of thirty frequently used biomedical verbs (predicates) and their corresponding arguments. Second, we use our proposition bank to train a biomedical SRL system, which uses a maximum entropy (ME) machinelearning model. Thirdly, we automatically generate argument-type templates, which can be used to improve classification of biomedical argument roles. Our experimental results show that a newswire English SRL system that achieves an F-score of 86.29% in the newswire English domain can maintain an F-score of 64.64%  when ported to the biomedical domain. By using our annotated biomedical corpus, we can increase that F-score by 22.9%. Adding automatically generated template features further increases overall F-score by 0.47% and adjunct (AM) F-score by 1.57%, respectively. 
This study investigates the support of multiple information seeking strategies (ISSs) within a single system, and the relation between varieties of ISSs and system design. It proposes to construct and evaluate an interactive information retrieval system which can adaptively support multiple ISSs, and allow change from one ISS to another within an ISS space. It is conducted in a series of steps: iterative designing -evaluating of several systems supporting different ISSs; specifying an interaction structure for multiple ISSs; and, implementing and evaluating a dynamically adaptive system supporting multiple ISSs. The study aims to make a contribution to interactive information retrieval drawing attention to user interface design, and to HCI, in integration of multiple support techniques within a single system framework. Keywords Information-seeking strategy, interaction structure, user interface design, evaluation, information retrieval 
We present a new type of neural probabilistic language model that learns a mapping from both words and explicit word features into a continuous space that is then used for word prediction. Additionally, we investigate several ways of deriving continuous word representations for unknown words from those of known words. The resulting model signiﬁcantly reduces perplexity on sparse-data tasks when compared to standard backoff models, standard neural language models, and factored language models. 
This paper describes a small, structured English corpus that is designed for translation into Less Commonly Taught Languages (LCTLs), and a set of re-usable tools for creation of similar corpora. 1 The corpus systematically explores meanings that are known to affect morphology or syntax in the world’s languages. Each sentence is associated with a feature structure showing the elements of meaning that are represented in the sentence. The corpus is highly structured so that it can support machine learning with only a small amount of data. As part of the REFLEX program, the corpus will be translated into multiple LCTLs, resulting in parallel corpora can be used for training of MT and other language technologies. Only the untranslated English corpus is described in this paper. 
We introduce a novel topic segmentation approach that combines evidence of topic shifts from lexical cohesion with linguistic evidence such as syntactically distinct features of segment initial contributions. Our evaluation demonstrates that this hybrid approach outperforms state-of-the-art algorithms even when applied to loosely structured, spontaneous dialogue. 
In this paper, we (1) propose a new dataset for testing the degree of relatedness between pairs of words; (2) propose a new WordNet-based measure of relatedness, and evaluate it on the new dataset. 
In this paper we present the results for building a grapheme-based speech recognition system for Thai. We experiment with different settings for the initial context independent system, different number of acoustic models and different contexts for the speech unit. In addition, we investigate the potential of an enhanced tree clustering method as a way of sharing parameters across models. We compare our system with two phoneme-based systems; one that uses a hand-crafted dictionary and another that uses an automatically generated dictionary. Experiment results show that the grapheme-based system with enhanced tree clustering outperforms the phoneme-based system using an automatically generated dictionary, and has comparable results to the phoneme-based system with the handcrafted dictionary. 
The performance of automatic speech summarisation has been improved in previous experiments by using linguistic model adaptation. We extend such adaptation to the use of class models, whose robustness further improves summarisation performance on a wider variety of objective evaluation metrics such as ROUGE-2 and ROUGE-SU4 used in the text summarisation literature. Summaries made from automatic speech recogniser transcriptions beneﬁt from relative improvements ranging from 6.0% to 22.2% on all investigated metrics. 
To overcome the problem of not having enough manually labeled relation instances for supervised relation extraction methods, in this paper we propose a label propagation (LP) based semi-supervised learning algorithm for relation extraction task to learn from both labeled and unlabeled data. Evaluation on the ACE corpus showed when only a few labeled examples are available, our LP based relation extraction can achieve better performance than SVM and another bootstrapping method. 
State-of-the-art Question Answering (QA) systems are very sensitive to variations in the phrasing of an information need. Finding the preferred language for such a need is a valuable task. We investigate that claim by adopting a simple MTbased paraphrasing technique and evaluating QA system performance on paraphrased questions. We found a potential increase of 35% in MRR with respect to the original question. 
Coreference resolution, like many problems in natural language processing, has most often been explored using datasets of written text. While spontaneous spoken language poses well-known challenges, it also offers additional modalities that may help disambiguate some of the inherent disﬂuency. We explore features of hand gesture that are correlated with coreference. Combining these features with a traditional textual model yields a statistically signiﬁcant improvement in overall performance. 
Prior work has shown that generalization of data in an Example Based Machine Translation (EBMT) system, reduces the amount of pre-translated text required to achieve a certain level of accuracy (Brown, 2000). Several word clustering algorithms have been suggested to perform these generalizations, such as kMeans clustering or Group Average Clustering. The hypothesis is that better contextual clustering can lead to better translation accuracy with limited training data. In this paper, we use a form of spectral clustering to cluster words, and this is shown to result in as much as 29.08% improvement over the baseline EBMT system. 
Georgian is a less commonly studied language with complex, non-concatenative verbal morphology. We present a computational model for generation and recognition of Georgian verb conjugations, relying on the analysis of Georgian verb structure as a word-level template. The model combines a set of ﬁnite-state transducers with a default inheritance mechanism.1 
In this paper, we study the effect of different word-level preprocessing decisions for Arabic on SMT quality. Our results show that given large amounts of training data, splitting off only proclitics performs best. However, for small amounts of training data, it is best to apply English-like tokenization using part-of-speech tags, and sophisticated morphological analysis and disambiguation. Moreover, choosing the appropriate preprocessing produces a signiﬁcant increase in BLEU score if there is a change in genre between training and test data. 
Several semi-supervised learning methods have been proposed to leverage unlabeled data, but imbalanced class distributions in the data set can hurt the performance of most algorithms. In this paper, we adapt the new approach of contrast classiﬁers for semi-supervised learning. This enables us to exploit large amounts of unlabeled data with a skewed distribution. In experiments on a speech act (agreement/disagreement) classiﬁcation problem, we achieve better results than other semi-supervised methods. We also obtain performance comparable to the best results reported so far on this task and outperform systems with equivalent feature sets.  trained on data with imbalanced class distributions. One reason is that most classiﬁers underlying these methods assume a balanced training set, and thus when one of the classes has a much larger number of examples than the other classes, the trained classiﬁer will be biased toward the majority class. The imbalance will propagate through subsequent iterations, resulting in a more skewed data set upon which a further biased classiﬁer will be trained. To exploit unlabeled data in learning an inherently skewed data distribution, we introduce a semi-supervised classiﬁcation method using contrast classiﬁers, ﬁrst proposed by Peng et al. (Peng et al., 2003). It approximates the posterior class probability given an observation using class-speciﬁc contrast classiﬁers that implicitly model the difference between the distribution of labeled data for that class and the unlabeled data.  
 Martha Palmer ICS and Linguistics U. of Colorado Boulder, CO martha.palmer @colorado.edu  Lance Ramshaw BBN Technologies 10 Moulton St. Cambridge, MA lance.ramshaw @bbn.com  Ralph Weischedel BBN Technologies 10 Moulton St. Cambridge, MA weischedel @bbn.com  Abstract* We describe the OntoNotes methodology and its result, a large multilingual richly-annotated corpus constructed at 90% interannotator agreement. An initial portion (300K words of English newswire and 250K words of Chinese newswire) will be made available to the community during 2007. 
Cross-language retrieval of spontaneous speech combines the challenges of working with noisy automated transcription and language translation. The CLEF 2005 CrossLanguage Speech Retrieval (CL-SR) task provides a standard test collection to investigate these challenges. We show that we can improve retrieval performance: by careful selection of the term weighting scheme; by decomposing automated transcripts into phonetic substrings to help ameliorate transcription errors; and by combining automatic transcriptions with manually-assigned metadata. We further show that topic translation with online machine translation resources yields effective CL-SR. 
This paper builds on recent research investigating sentence ordering in text production by evaluating the Centering-based metrics of coherence employed by Karamanis et al. (2004) using the data of Barzilay and Lapata (2005). This is the ﬁrst time that Centering is evaluated empirically as a sentence ordering constraint in several domains, verifying the results reported in Karamanis et al. 
This paper presents a new active learning paradigm which considers not only the uncertainty of the classifier but also the diversity of the corpus. The two measures for uncertainty and diversity were combined using the MMR (Maximal Marginal Relevance) method to give the sampling scores in our active learning strategy. We incorporated MMR-based active machinelearning idea into the biomedical namedentity recognition system. Our experimental results indicated that our strategies for active-learning based sample selection could significantly reduce the human effort. 
This paper evaluates the beneﬁt of deleting ﬁllers (e.g. you know, like) early in parsing conversational speech. Readability studies have shown that disﬂuencies (ﬁllers and speech repairs) may be deleted from transcripts without compromising meaning (Jones et al., 2003), and deleting repairs prior to parsing has been shown to improve its accuracy (Charniak and Johnson, 2001). We explore whether this strategy of early deletion is also beneﬁcial with regard to ﬁllers. Reported experiments measure the effect of early deletion under in-domain and out-of-domain parser training conditions using a state-of-the-art parser (Charniak, 2000). While early deletion is found to yield only modest beneﬁt for in-domain parsing, signiﬁcant improvement is achieved for out-of-domain adaptation. This suggests a potentially broader role for disﬂuency modeling in adapting text-based tools for processing conversational speech. 
The goal of the on-going project described in this paper is evaluation of the utility of Latent Semantic Analysis (LSA) for unsupervised word sense discrimination. The hypothesis is that LSA can be used to compute context vectors for ambiguous words that can be clustered together – with each cluster corresponding to a different sense of the word. In this paper we report first experimental result on tightness, separation and purity of sense-based clusters as a function of vector space dimensionality and using different distance metrics. 
Identifying a speaker’s role (anchor, reporter, or guest speaker) is important for ﬁnding the structural information in broadcast news speech. We present an HMM-based approach and a maximum entropy model for speaker role labeling using Mandarin broadcast news speech. The algorithms achieve classiﬁcation accuracy of about 80% (compared to the baseline of around 50%) using the human transcriptions and manually labeled speaker turns. We found that the maximum entropy model performs slightly better than the HMM, and that the combination of them outperforms any model alone. The impact of the contextual role information is also examined in this study. 
The identiﬁcation of personality by automatic analysis of conversation has many applications in natural language processing, from leader identiﬁcation in meetings to partner matching on dating websites. We automatically train models of the main ﬁve personality dimensions, on a corpus of conversation extracts and personality ratings. Results show that the models perform better than the baseline, and their analysis conﬁrms previous ﬁndings linking language and personality, while revealing many new linguistic and prosodic markers. 
We present a method for summarizing speech documents without using any type of transcript/text in a Hidden Markov Model framework. The hidden variables or states in the model represent whether a sentence is to be included in a summary or not, and the acoustic/prosodic features are the observation vectors. The model predicts the optimal sequence of segments that best summarize the document. We evaluate our method by comparing the predicted summary with one generated by a human summarizer. Our results indicate that we can generate ’good’ summaries even when using only acoustic/prosodic information, which points toward the possibility of text-independent summarization for spoken documents. 
We describe a method based on “tweaking” an existing learned sequential classiﬁer to change the recall-precision tradeoff, guided by a user-provided performance criterion. This method is evaluated on the task of recognizing personal names in email and newswire text, and proves to be both simple and effective. 
In this paper, we use tree kernels to exploit deep syntactic parsing information for natural language applications. We study the properties of different kernels and we provide algorithms for their computation in linear average time. The experiments with SVMs on the task of predicate argument classiﬁcation provide empirical data that validates our methods. 
We integrate PropBank semantic role labels to an existing statistical parsing model producing richer output. We show conclusive results on joint learning and inference of syntactic and semantic representations. 
Natural language generation (NLG) refers to the process of producing text in a spoken language, starting from an internal knowledge representation structure. Augmentative and Alternative Communication (AAC) deals with the development of devices and tools to enable basic conversation for language-impaired people. We present an applied prototype of an AAC-NLG system generating written output in English and Hebrew from a sequence of Bliss symbols. The system does not “translate” the symbols sequence, but instead, it dynamically changes the communication board as the choice of symbols proceeds according to the syntactic and semantic content of selected symbols, generating utterances in natural language through a process of semantic authoring. 
This paper presents a multi-modal featurebased system for extracting salient keywords from transcripts of instructional videos. Specifically, we propose to extract domain-speciﬁc keywords for videos by integrating various cues from linguistic and statistical knowledge, as well as derived sound classes and characteristic visual content types. The acquisition of such salient keywords will facilitate video indexing and browsing, and signiﬁcantly improve the quality of current video search engines. Experiments on four government instructional videos show that 82% of the salient keywords appear in the top 50% of the highly ranked keywords. In addition, the audiovisual cues improve precision and recall by 1.1% and 1.5% respectively. 
This paper proposes the usage of variant corpora, i.e., parallel text corpora that are equal in meaning but use different ways to express content, in order to improve corpus-based machine translation. The usage of multiple training corpora of the same content with different sources results in variant models that focus on speciﬁc linguistic phenomena covered by the respective corpus. The proposed method applies each variant model separately resulting in multiple translation hypotheses which are selectively combined according to statistical models. The proposed method outperforms the conventional approach of merging all variants by reducing translation ambiguities and exploiting the strengths of each variant model. 
We describe work in progress on using quantitative methods to classify writing systems according to Sproat’s (2000) classiﬁcation grid using unannotated data. We speciﬁcally propose two quantitative tests for determining the type of phonography in a writing system, and its degree of logography, respectively. 
Syntactic priming effects, modelled as increase in repetition probability shortly after a use of a syntactic rule, have the potential to improve language processing components. We model priming of syntactic rules in annotated corpora of spoken dialogue, extending previous work that was conﬁned to selected constructions. We ﬁnd that speakers are more receptive to priming from their interlocutor in task-oriented dialogue than in sponaneous conversation. Low-frequency rules are more likely to show priming. 
In this paper, we present results from a Broadcast News story segmentation system developed for the SRI NIGHTINGALE system operating on English, Arabic and Mandarin news shows to provide input to subsequent question-answering processes. Using a rule-induction algorithm with automatically extracted acoustic and lexical features, we report success rates that are competitive with state-ofthe-art systems on each input language. We further demonstrate that features useful for English and Mandarin are not discriminative for Arabic. 
We present a novel parser combination scheme that works by reparsing input sentences once they have already been parsed by several different parsers. We apply this idea to dependency and constituent parsing, generating results that surpass state-of-theart accuracy levels for individual parsers. 
 2 Discourse Relation Definitions  This paper describes a system which identifies discourse relations between two successive sentences in Japanese. On top of the lexical information previously proposed, we used phrasal pattern information. Adding phrasal information improves the system's accuracy 12%, from 53% to 65%.  
This paper shows that in the context of statistical weblog classiﬁcation for splog ﬁltering based on n-grams of tokens in the URL, further segmenting the URLs beyond the standard punctuation is helpful. Many splog URLs contain phrases in which the words are glued together in order to avoid splog ﬁltering techniques based on punctuation segmentation and unigrams. A technique which segments long tokens into the words forming the phrase is proposed and evaluated. The resulting tokens are used as features for a weblog classiﬁer whose accuracy is similar to that of humans (78% vs. 76%) and reaches 93.3% of precision in identifying splogs with recall of 50.9%. 
Word subject domains have been widely used to improve the performance of word sense disambiguation algorithms. However, comparatively little effort has been devoted so far to the disambiguation of word subject domains. The few existing approaches have focused on the development of algorithms specific to word domain disambiguation. In this paper we explore an alternative approach where word domain disambiguation is achieved via word sense disambiguation. Our study shows that this approach yields very strong results, suggesting that word domain disambiguation can be addressed in terms of word sense disambiguation with no need for special purpose algorithms. 
In this paper we present a scheme to select relevant subsets of sentences from a large generic corpus such as text acquired from the web. A relative entropy (R.E) based criterion is used to incrementally select sentences whose distribution matches the domain of interest. Experimental results show that by using the proposed subset selection scheme we can get significant performance improvement in both Word Error Rate (WER) and Perplexity (PPL) over the models built from the entire web-corpus by using just 10% of the data. In addition incremental data selection enables us to achieve signiﬁcant reduction in the vocabulary size as well as number of n-grams in the adapted language model. To demonstrate the gains from our method we provide a comparative analysis with a number of methods proposed in recent language modeling literature for cleaning up text. 
There are several approaches that model information extraction as a token classiﬁcation task, using various tagging strategies to combine multiple tokens. We describe the tagging strategies that can be found in the literature and evaluate their relative performances. We also introduce a new strategy, called Begin/After tagging or BIA, and show that it is competitive to the best other strategies. 
We exploit the resources in the Arabic Treebank (ATB) for the novel task of automatically creating lexical semantic verb classes for Modern Standard Arabic (MSA). Verbs are clustered into groups that share semantic elements of meaning as they exhibit similar syntactic behavior. The results of the clustering experiments are compared with a gold standard set of classes, which is approximated by using the noisy English translations provided in the ATB to create Levin-like classes for MSA. The quality of the clusters is found to be sensitive to the inclusion of information about lexical heads of the constituents in the syntactic frames, as well as parameters of the clustering algorithm . The best set of parameters yields an Fβ=1 score of 0.501, compared to a random baseline with an Fβ=1 score of 0.37. 
In the current work, we focus on systems that provide incremental directions and monitor the progress of mobile users following those directions. Such directions are based on dynamic quantities like the visibility of reference points and their distance from the user. An intelligent navigation assistant might take advantage of the user’s mobility within the setting to achieve communicative goals, for example, by repositioning him to a point from which a description of the target is easier to produce. Calculating spatial variables over a corpus of human-human data developed for this study, we trained a classiﬁer to detect contexts in which a target object can be felicitously described. Our algorithm matched the human subjects with 86% precision. 
This paper proposes an automatic method for disambiguating an acronym with multiple definitions, considering the context surrounding the acronym. First, the method obtains the Web pages that include both the acronym and its definitions. Second, the method feeds them to the machine learner. Cross-validation tests results indicate that the current accuracy of obtaining the appropriate definition for an acronym is around 92% for two ambiguous definitions and around 86% for five ambiguous definitions. 
This paper proposes an automatic method of reading proper names with multiple pronunciations. First, the method obtains Web pages that include both the proper name and its pronunciation. Second, the method feeds them to the learner for classification. The current accuracy is around 90% for open data.  manually tagged according to their pronunciation. Instead, we propose a method that automatically builds a pronunciation-tagged corpus using the Web as a source of training data for word pronunciation disambiguation. This paper is arranged as follows. Section 2 proposes solutions, and Sections 3 and 4 report experimental results. We offer our discussion in Section 5 and conclusions in Section 6. 2 The Proposed Methods  
A study was conducted to explore the potential of Natural Language Processing (NLP)based knowledge discovery approaches for the task of representing and exploiting the vital information contained in field service (trouble) tickets for a large utility provider. Analysis of a subset of tickets, guided by sublanguage theory, identified linguistic patterns, which were translated into rule-based algorithms for automatic identification of tickets’ discourse structure. The subsequent data mining experiments showed promising results, suggesting that sublanguage is an effective framework for the task of discovering the historical and predictive value of trouble ticket data. 
We report on a novel approach to generating strategies for spoken dialogue systems. We present a series of experiments that illustrate how an evolutionary reinforcement learning algorithm can produce strategies that are both optimal and easily inspectable by human developers. Our experimental strategies achieve a mean performance of 98.9% with respect to a predeﬁned evaluation metric. Our approach also produces a dramatic reduction in strategy size when compared with conventional reinforcement learning techniques (87% in one experiment). We conclude that this algorithm can be used to evolve optimal inspectable dialogue strategies. 
This paper describes the Lycos Retriever system, a deployed system for automatically generating coherent topical summaries of popular web query topics. 
This paper describes an affinity graph based approach to multi-document summarization. We incorporate a diffusion process to acquire semantic relationships between sentences, and then compute information richness of sentences by a graph rank algorithm on differentiated intra-document links and inter-document links between sentences. A greedy algorithm is employed to impose diversity penalty on sentences and the sentences with both high information richness and high information novelty are chosen into the summary. Experimental results on task 2 of DUC 2002 and task 2 of DUC 2004 demonstrate that the proposed approach outperforms existing state-of-theart systems. 
Automatic reading comprehension (RC) systems can analyze a given passage and generate/extract answers in response to questions about the passage. The RC passages are often constrained in their lengths and the target answer sentence usually occurs very few times. In order to generate/extract a speciﬁc precise answer, this paper proposes the integration of two types of “deep” linguistic features, namely word dependencies and grammatical relations, in a maximum entropy (ME) framework to handle the RC task. The proposed approach achieves 44.7% and 73.2% HumSent accuracy on the Remedia and ChungHwa corpora respectively. This result is competitive with other results reported thus far. 
Images (i.e., figures or tables) are important experimental results that are typically reported in bioscience full-text articles. Biologists need to access the images to validate research facts and to formulate or to test novel research hypotheses. We designed, evaluated, and implemented a novel user-interface, BioEx, that allows biologists to access images that appear in a full-text article directly from the abstract of the article. 
We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a conﬁdence measure approach. We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation. In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates. By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005.  
 This paper is concerned with the  summarization  of  spontaneous  conversations. Compared with broadcast  news, which has received intensive study,  spontaneous conversations have been less  addressed in the literature. Previous  work has focused on textual features  extracted from transcripts. This paper  explores and compares the effectiveness  of both textual features and speech-  related features. The experiments show  that these features incrementally improve  summarization performance. We also find  that speech disfluencies, which have been  removed as noise in previous work, help  identify important utterances, while the  structural feature is less effective than it  is in broadcast news.  
Statistical machine translation (SMT) is based on the ability to effectively learn word and phrase relationships from parallel corpora, a process which is considerably more difﬁcult when the extent of morphological expression differs signiﬁcantly across the source and target languages. We present techniques that select appropriate word segmentations in the morphologically rich source language based on contextual relationships in the target language. Our results take advantage of existing word level morphological analysis components to improve translation quality above state-of-the-art on a limited-data Arabic to English speech translation task. 
We present a probabilistic bilingual capitalization model for capitalizing machine translation outputs using conditional random ﬁelds. Experiments carried out on three language pairs and a variety of experiment conditions show that our model signiﬁcantly outperforms a strong monolingual capitalization model baseline, especially when working with small datasets and/or European language pairs. 
We begin by exploring theoretical and practical issues with phrasal SMT, several of which are addressed by syntax-based SMT. Next, to address problems not handled by syntax, we propose the concept of a Minimal Translation Unit (MTU) and develop MTU sequence models. Finally we incorporate these models into a syntax-based SMT system and demonstrate that it improves on the state of the art translation quality within a theoretically more desirable framework. 1. Introduction The last several years have seen phrasal statistical machine translation (SMT) systems outperform word-based approaches by a wide margin (Koehn 2003). Unfortunately the use of phrases in SMT is beset by a number of difficult theoretical and practical problems, which we attempt to characterize below. Recent research into syntaxbased SMT (Quirk and Menezes 2005; Chiang 2005) has produced promising results in addressing some of the problems; research motivated by other statistical models has helped to address others (Banchs et al. 2005). We refine and unify two threads of research in an attempt to address all of these problems simultaneously. Such an approach proves both theoretically more desirable and empirically superior. In brief, Phrasal SMT systems employ phrase pairs automatically extracted from parallel corpora. To translate, a source sentence is first partitioned into a sequence of phrases I = s1…sI. Each source phrase si is then translated into a target phrase ti. Finally the target phrases are permuted, and the translation is read off in order.  Beam search is used to approximate the optimal translation. We refer the reader to Keohn et al. (2003) for a detailed description. Unless otherwise noted, the following discussion is generally applicable to Alignment Template systems (Och and Ney 2004) as well. 1.1. Advantages of phrasal SMT Non-compositionality Phrases capture the translations of idiomatic and other non-compositional fixed phrases as a unit, side-stepping the need to awkwardly reconstruct them word by word. While many words can be translated into a single target word, common everyday phrases such as the English password translating as the French mot de passe cannot be easily subdivided. Allowing such translations to be first class entities simplifies translation implementation and improves translation quality. Local re-ordering Phrases provide memorized re-ordering decisions. As previously noted, translation can be conceptually divided into two steps: first, finding a set of phrase pairs that simultaneously covers the source side and provides a bag of translated target phrases; and second, picking an order for those target phrases. Since phrase pairs consist of memorized substrings of the training data, they are very likely to produce correct local reorderings. Contextual information Many phrasal translations may be easily subdivided into word-for-word translation, for instance the English phrase the cabbage may be translated word-for-word as le chou. However we note that la is also a perfectly reasonable wordfor-word translation of the, yet la chou is not a grammatical French string. Even when a phrase appears compositional, the incorporation of contextual information often improves translation  9 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 9–16, New York, June 2006. c 2006 Association for Computational Linguistics  quality. Phrases are a straightforward means of capturing local context. 1.2. Theoretical problems with phrasal SMT Exact substring match; no discontiguity Large fixed phrase pairs are effective when an exact match can be found, but are useless otherwise. The alignment template approach (where phrases are modeled in terms of word classes instead of specific words) provides a solution at the expense of truly fixed phrases. Neither phrasal SMT nor alignment templates allow discontiguous translation pairs. Global re-ordering Phrases do capture local reordering, but provide no global re-ordering strategy, and the number of possible orderings to be considered is not lessened significantly. Given a sentence of n words, if the average target phrase length is 4 words (which is unusually high), then the reordering space is reduced from n! to only (n/4)!: still impractical for exact search in most sentences. Systems must therefore impose some limits on phrasal reordering, often hard limits based on distance as in Koehn et al. (2003) or some linguistically motivated constraint, such as ITG (Zens and Ney, 2004). Since these phrases are not bound by or even related to syntactic constituents, linguistic generalizations (such as SVO becoming SOV, or prepositions becoming postpositions) are not easily incorporated into the movement models. Probability estimation To estimate the translation probability of a phrase pair, several approaches are used, often concurrently as features in a log-linear model. Conditional probabilities can be estimated by maximum likelihood estimation. Yet the phrases most likely to contribute important translational and ordering information—the longest ones—are the ones most subject to sparse data issues. Alternately, conditional phrasal models can be constructed from word translation probabilities; this approach is often called lexical weighting (Vogel et al. 2003). This avoids sparse data issues, but tends to prefer literal translations where the word-for-word probabilities are high Furthermore most approaches model phrases as bags of words, and fail to distinguish between local re-ordering possibilities.  Partitioning limitation A phrasal approach partitions the sentence into strings of words, making several questionable assumptions along the way. First, the probability of the partitioning is never considered. Long phrases tend to be rare and therefore have sharp probability distributions. This adds an inherent bias toward long phrases with questionable MLE probabilities (e.g. 1/1 or 2/2). 1 Second, the translation probability of each phrase pair is modeled independently. Such an approach fails to model any phenomena that reach across boundaries; only the target language model and perhaps whole-sentence bag of words models cross phrase boundaries. This is especially important when translating into languages with agreement phenomena. Often a single phrase does not cover all agreeing modifiers of a headword; the uncovered modifiers are biased toward the most common variant rather than the one agreeing with its head. Ideally a system would consider overlapping phrases rather than a single partitioning, but this poses a problem for generative models: when words are generated multiple times by different phrases, they are effectively penalized. 1.3. Practical problem with phrases: size In addition to the theoretical problems with phrases, there are also practical issues. While phrasal systems achieve diminishing returns due 
Parallel corpora are crucial for training SMT systems. However, for many language pairs they are available only in very limited quantities. For these language pairs a huge portion of phrases encountered at run-time will be unknown. We show how techniques from paraphrasing can be used to deal with these otherwise unknown source language phrases. Our results show that augmenting a stateof-the-art SMT system with paraphrases leads to signiﬁcantly improved coverage and translation quality. For a training corpus with 10,000 sentence pairs we increase the coverage of unique test set unigrams from 48% to 90%, with more than half of the newly covered items accurately translated, as opposed to none in current approaches.  served in the training data and therefore their translations will not be learned. Here we address the problem of unknown phrases. Speciﬁcally we show that upon encountering an unknown source phrase, we can substitute a paraphrase for it and then proceed using the translation of that paraphrase. We derive these paraphrases from resources that are external to the parallel corpus that the translation model is trained from, and we are able to exploit (potentially more abundant) parallel corpora from other language pairs to do so. In this paper we: • Deﬁne a method for incorporating paraphrases of unseen source phrases into the statistical machine translation process. • Show that by translating paraphrases we achieve a marked improvement in coverage and translation quality, especially in the case of unknown words which to date have been left untranslated.  
This paper presents a new approach to distortion (phrase reordering) in phrasebased machine translation (MT). Distortion is modeled as a sequence of choices during translation. The approach yields trainable, probabilistic distortion models that are global: they assign a probability to each possible phrase reordering. These “segment choice” models (SCMs) can be trained on “segment-aligned” sentence pairs; they can be applied during decoding or rescoring. The approach yields a metric called “distortion perplexity” (“disperp”) for comparing SCMs offline on test data, analogous to perplexity for language models. A decision-tree-based SCM is tested on Chinese-to-English translation, and outperforms a baseline distortion penalty approach at the 99% confidence level. 
Recognizing textual entailment is a challenging problem and a fundamental component of many applications in natural language processing. We present a novel framework for recognizing textual entailment that focuses on the use of syntactic heuristics to recognize false entailment. We give a thorough analysis of our system, which demonstrates state-of-the-art performance on a widely-used test set. 
This paper advocates a new architecture for textual inference in which ﬁnding a good alignment is separated from evaluating entailment. Current approaches to semantic inference in question answering and textual entailment have approximated the entailment problem as that of computing the best alignment of the hypothesis to the text, using a locally decomposable matching score. We argue that there are signiﬁcant weaknesses in this approach, including ﬂawed assumptions of monotonicity and locality. Instead we propose a pipelined approach where alignment is followed by a classiﬁcation step, in which we extract features representing high-level characteristics of the entailment problem, and pass the resulting feature vector to a statistical classiﬁer trained on development data. We report results on data from the 2005 Pascal RTE Challenge which surpass previously reported results for alignment-based systems. 
The study addresses the problem of automatic acquisition of entailment relations between verbs. While this task has much in common with paraphrases acquisition which aims to discover semantic equivalence between verbs, the main challenge of entailment acquisition is to capture asymmetric, or directional, relations. Motivated by the intuition that it often underlies the local structure of coherent text, we develop a method that discovers verb entailment using evidence about discourse relations between clauses available in a parsed corpus. In comparison with earlier work, the proposed method covers a much wider range of verb entailment types and learns the mapping between verbs with highly varied argument structures. 
This paper shows that inference rules with temporal constraints can be acquired by using verb-verb co-occurrences in Japanese coordinated sentences and verb-noun cooccurrences. For example, our unsupervised acquisition method could obtain the inference rule “If someone enforces a law, usually someone enacts the law at the same time as or before the enforcing of the law” since the verbs “enact” and “enforce” frequently co-occurred in coordinated sentences and the verbs also frequently cooccurred with the noun “law”. We also show that the accuracy of the acquisition is improved by using the occurrence frequency of a single verb, which we assume indicates how generic the meaning of the verb is. 
Deidentiﬁcation of clinical records is a crucial step before these records can be distributed to non-hospital researchers. Most approaches to deidentiﬁcation rely heavily on dictionaries and heuristic rules; these approaches fail to remove most personal health information (PHI) that cannot be found in dictionaries. They also can fail to remove PHI that is ambiguous between PHI and non-PHI. Named entity recognition (NER) technologies can be used for deidentiﬁcation. Some of these technologies exploit both local and global context of a word to identify its entity type. When documents are grammatically written, global context can improve NER. In this paper, we show that we can deidentify medical discharge summaries using support vector machines that rely on a statistical representation of local context. We compare our approach with three different systems. Comparison with a rulebased approach shows that a statistical representation of local context contributes more to deidentiﬁcation than dictionaries and hand-tailored heuristics. Comparison with two well-known systems, SNoW and IdentiFinder, shows that when the language of documents is fragmented, local context contributes more to deidentiﬁcation than global context.  
Named Entity Recognition (NER) is a fundamental task in text mining and natural language understanding. Current approaches to NER (mostly based on supervised learning) perform well on domains similar to the training domain, but they tend to adapt poorly to slightly different domains. We present several strategies for exploiting the domain structure in the training data to learn a more robust named entity recognizer that can perform well on a new domain. First, we propose a simple yet effective way to automatically rank features based on their generalizabilities across domains. We then train a classiﬁer with strong emphasis on the most generalizable features. This emphasis is imposed by putting a rank-based prior on a logistic regression model. We further propose a domain-aware cross validation strategy to help choose an appropriate parameter for the rank-based prior. We evaluated the proposed method with a task of recognizing named entities (genes) in biology text involving three species. The experiment results show that the new domainaware approach outperforms a state-ofthe-art baseline method in adapting to new domains, especially when there is a great difference between the new domain and the training domain.  
Named Entity recognition (NER) is an important part of many natural language processing tasks. Most current approaches employ machine learning techniques and require supervised data. However, many languages lack such resources. This paper presents an algorithm to automatically discover Named Entities (NEs) in a resource free language, given a bilingual corpora in which it is weakly temporally aligned with a resource rich language. We observe that NEs have similar time distributions across such corpora, and that they are often transliterated, and develop an algorithm that exploits both iteratively. The algorithm makes use of a new, frequency based, metric for time distributions and a resource free discriminative approach to transliteration. We evaluate the algorithm on an English-Russian corpus, and show high level of NEs discovery in Russian. 
Discriminative probabilistic models are very popular in NLP because of the latitude they afford in designing features. But training involves complex trade-offs among weights, which can be dangerous: a few highlyindicative features can swamp the contribution of many individually weaker features, causing their weights to be undertrained. Such a model is less robust, for the highly-indicative features may be noisy or missing in the test data. To ameliorate this weight undertraining, we introduce several new feature bagging methods, in which separate models are trained on subsets of the original features, and combined using a mixture model or a product of experts. These methods include the logarithmic opinion pools used by Smith et al. (2005). We evaluate feature bagging on linear-chain conditional random ﬁelds for two natural-language tasks. On both tasks, the feature-bagged CRF performs better than simply training a single CRF on all the features. 
This paper presents a new approach to combining outputs of existing word alignment systems. Each alignment link is represented with a set of feature functions extracted from linguistic features and input alignments. These features are used as the basis of alignment decisions made by a maximum entropy approach. The learning method has been evaluated on three language pairs, yielding signiﬁcant improvements over input alignments and three heuristic combination methods. The impact of word alignment on MT quality is investigated, using a phrase-based MT system. 
We present an unsupervised approach to symmetric word alignment in which two simple asymmetric models are trained jointly to maximize a combination of data likelihood and agreement between the models. Compared to the standard practice of intersecting predictions of independently-trained models, joint training provides a 32% reduction in AER. Moreover, a simple and efﬁcient pair of HMM aligners provides a 29% reduction in AER over symmetrized IBM model 4 predictions. 
 Recently, discriminative word alignment methods  have achieved state-of-the-art accuracies by extend-  ing the range of information sources that can be  easily incorporated into aligners. The chief advan-  tage of a discriminative framework is the ability  to score alignments based on arbitrary features of  the matching word tokens, including orthographic  form, predictions of other models, lexical context  and so on. However, the proposed bipartite match-  ing model of Taskar et al. (2005), despite being  tractable and effective, has two important limita-  tions. First, it is limited by the restriction that  words have fertility of at most one. More impor-  tantly, ﬁrst order correlations between consecutive  words cannot be directly captured by the model. In  this work, we address these limitations by enrich-  ing the model form. We give estimation and infer-  ence algorithms for these enhancements. Our best  model achieves a relative AER reduction of 25%  over the basic matching formulation, outperform-  ing intersected IBM Model 4 without using any  overly compute-intensive features. By including  predictions of other models as features, we achieve  AER  of  38 .  on  the  standard  Hansards  dataset.  
 We address the problem of unknown word sense detection: the identiﬁcation of corpus occurrences that are not covered by a given sense inventory. We model this as an instance of outlier detection, using a simple nearest neighbor-based approach to measuring the resemblance of a new item to a training set. In combination with a method that alleviates data sparseness by sharing training data across lemmas, the approach achieves a precision of 0.77 and recall of 0.82.  Figure 1: Wrong assignment due to missing sense: from the Hound of the Baskervilles, Ch. 14  
Recent years have seen increasing research on extracting and using temporal information in natural language applications. However most of the works found in the literature have focused on identifying and understanding temporal expressions in newswire texts. In this paper we report our work on anchoring temporal expressions in a novel genre, emails. The highly under-speciﬁed nature of these expressions ﬁts well with our constraintbased representation of time, Time Calculus for Natural Language (TCNL). We have developed and evaluated a Temporal Expression Anchoror (TEA), and the result shows that it performs signiﬁcantly better than the baseline, and compares favorably with some of the closely related work. 
We propose a solution to the annotation bottleneck for statistical parsing, by exploiting the lexicalized nature of Combinatory Categorial Grammar (CCG). The parsing model uses predicate-argument dependencies for training, which are derived from sequences of CCG lexical categories rather than full derivations. A simple method is used for extracting dependencies from lexical category sequences, resulting in high precision, yet incomplete and noisy data. The dependency parsing model of Clark and Curran (2004b) is extended to exploit this partial training data. Remarkably, the accuracy of the parser trained on data derived from category sequences alone is only 1.3% worse in terms of F-score than the parser trained on complete dependency structures. 
We present a simple, but surprisingly effective, method of self-training a twophase parser-reranker system using readily available unlabeled data. We show that this type of bootstrapping is possible for parsing when the bootstrapped parses are processed by a discriminative reranker. Our improved model achieves an f -score of 92.1%, an absolute 1.1% improvement (12% error reduction) over the previous best result for Wall Street Journal parsing. Finally, we provide some analysis to better understand the phenomenon. 
We develop dependency parsers for Arabic, English, Chinese, and Czech using Bayes Point Machines, a training algorithm which is as easy to implement as the perceptron yet competitive with large margin methods. We achieve results comparable to state-of-the-art in English and Czech, and report the ﬁrst directed dependency parsing accuracies for Arabic and Chinese. Given the multilingual nature of our experiments, we discuss some issues regarding the comparison of dependency parsers for different languages. 
We present a PCFG parsing algorithm that uses a multilevel coarse-to-ﬁne (mlctf) scheme to improve the eﬃciency of search for the best parse. Our approach requires the user to specify a sequence of nested partitions or equivalence classes of the PCFG nonterminals. We deﬁne a sequence of PCFGs corresponding to each partition, where the nonterminals of each PCFG are clusters of nonterminals of the original source PCFG. We use the results of parsing at a coarser level (i.e., grammar deﬁned in terms of a coarser partition) to prune the next ﬁner level. We present experiments showing that with our algorithm the work load (as measured by the total number of constituents processed) is decreased by a factor of ten with no decrease in parsing accuracy compared to standard CKY parsing with the original PCFG. We suggest that the search space over mlctf algorithms is almost totally unexplored so that future work should be able to improve signiﬁcantly on these results. 
We present an integrated probabilistic model for Japanese syntactic and case structure analysis. Syntactic and case structure are simultaneously analyzed based on wide-coverage case frames that are constructed from a huge raw corpus in an unsupervised manner. This model selects the syntactic and case structure that has the highest generative probability. We evaluate both syntactic structure and case structure. In particular, the experimental results for syntactic analysis on web sentences show that the proposed model signiﬁcantly outperforms known syntactic analyzers. 
We present a two stage parser that recovers Penn Treebank style syntactic analyses of new sentences including skeletal syntactic structure, and, for the ﬁrst time, both function tags and empty categories. The accuracy of the ﬁrst-stage parser on the standard Parseval metric matches that of the (Collins, 2003) parser on which it is based, despite the data fragmentation caused by the greatly enriched space of possible node labels. This ﬁrst stage simultaneously achieves near state-of-theart performance on recovering function tags with minimal modiﬁcations to the underlying parser, modifying less than ten lines of code. The second stage achieves state-of-the-art performance on the recovery of empty categories by combining a linguistically-informed architecture and a rich feature set with the power of modern machine learning methods. 
In this paper we present an extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources. These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels. We show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns. 
In this paper, we introduce a methodology for analyzing judgment opinions. We define a judgment opinion as consisting of a valence, a holder, and a topic. We decompose the task of opinion analysis into four parts: 1) recognizing the opinion; 2) identifying the valence; 3) identifying the holder; and 4) identifying the topic. In this paper, we address the first three parts and evaluate our methodology using both intrinsic and extrinsic measures. 
In this paper we present a novel featureenriched approach that learns to detect the conversation focus of threaded discussions by combining NLP analysis and IR techniques. Using the graph-based algorithm HITS, we integrate different features such as lexical similarity, poster trustworthiness, and speech act analysis of human conversations with featureoriented link generation functions. It is the first quantitative study to analyze human conversation focus in the context of online discussions that takes into account heterogeneous sources of evidence. Experimental results using a threaded discussion corpus from an undergraduate class show that it achieves significant performance improvements compared with the baseline system. 
This paper investigates the feasibility of automated scoring of spoken English proficiency of non-native speakers. Unlike existing automated assessments of spoken English, our data consists of spontaneous spoken responses to complex test items. We perform both a quantitative and a qualitative analysis of these features using two different machine learning approaches. (1) We use support vector machines to produce a score and evaluate it with respect to a mode baseline and to human rater agreement. We find that scoring based on support vector machines yields accuracies approaching inter-rater agreement in some cases. (2) We use classification and regression trees to understand the role of different features and feature classes in the characterization of speaking proficiency by human scorers. Our analysis shows that across all the test items most or all the feature classes are used in the nodes of the trees suggesting that the scores are, appropriately, a combination of multiple components of speaking proficiency. Future research will concentrate on extending the set of features and introducing new feature classes to arrive at a scoring model that comprises additional relevant aspects of speaking proficiency. 
The speed with which pronunciation dictionaries can be bootstrapped depends on the efficiency of learning algorithms and on the ordering of words presented to the user. This paper presents an active-learning word selection strategy that is mindful of human limitations. Learning rates approach that of an oracle system that knows the final LTS rule set. 
We identify problems with the Penn Treebank that render it imperfect for syntaxbased machine translation and propose methods of relabeling the syntax trees to improve translation quality. We develop a system incorporating a handful of relabeling strategies that yields a statistically signiﬁcant improvement of 2.3 BLEU points over a baseline syntax-based system. 
We present an approach to statistical machine translation that combines ideas from phrase-based SMT and traditional grammar-based MT. Our system incorporates the concept of multi-word translation units into transfer of dependency structure snippets, and models and trains statistical components according to stateof-the-art SMT systems. Compliant with classical transfer-based MT, target dependency structure snippets are input to a grammar-based generator. An experimental evaluation shows that the incorporation of a grammar-based generator into an SMT framework provides improved grammaticality while achieving state-of-the-art quality on in-coverage examples, suggesting a possible hybrid framework. 
Systems based on synchronous grammars and tree transducers promise to improve the quality of statistical machine translation output, but are often very computationally intensive. The complexity is exponential in the size of individual grammar rules due to arbitrary re-orderings between the two languages, and rules extracted from parallel corpora can be quite large. We devise a linear-time algorithm for factoring syntactic re-orderings by binarizing synchronous rules when possible and show that the resulting rule set significantly improves the speed and accuracy of a state-of-the-art syntax-based machine translation system. 
We investigate using the PARADISE framework to develop predictive models of system performance in our spoken dialogue tutoring system. We represent performance with two metrics: user satisfaction and student learning. We train and test predictive models of these metrics in our tutoring system corpora. We predict user satisfaction with 2 parameter types: 1) system-generic, and 2) tutoringspeciﬁc. To predict student learning, we also use a third type: 3) user affect. Alhough generic parameters are useful predictors of user satisfaction in other PARADISE applications, overall our parameters produce less useful user satisfaction models in our system. However, generic and tutoring-speciﬁc parameters do produce useful models of student learning in our system. User affect parameters can increase the usefulness of these models. 
Recent work in designing spoken dialogue systems has focused on using Reinforcement Learning to automatically learn the best action for a system to take at any point in the dialogue to maximize dialogue success. While policy development is very important, choosing the best features to model the user state is equally important since it impacts the actions a system should make. In this paper, we compare the relative utility of adding three features to a model of user state in the domain of a spoken dialogue tutoring system. In addition, we also look at the effects of these features on what type of a question a tutoring system should ask at any state and compare it with our previous work on using feedback as the system action. 
Dialog act (DA) tags are useful for many applications in natural language processing and automatic speech recognition. In this work, we introduce hidden backoff models (HBMs) where a large generalized backoff model is trained, using an embedded expectation-maximization (EM) procedure, on data that is partially observed. We use HBMs as word models conditioned on both DAs and (hidden) DAsegments. Experimental results on the ICSI meeting recorder dialog act corpus show that our procedure can strictly increase likelihood on training data and can effectively reduce errors on test data. In the best case, test error can be reduced by 6.1% relative to our baseline, an improvement on previously reported models that also use prosody. We also compare with our own prosody-based model, and show that our HBM is competitive even without the use of prosody. We have not yet succeeded, however, in combining the beneﬁts of both prosody and the HBM. 
This paper proposes to use a convolution kernel over parse trees to model syntactic structure information for relation extraction. Our study reveals that the syntactic structure features embedded in a parse tree are very effective for relation extraction and these features can be well captured by the convolution tree kernel. Evaluation on the ACE 2003 corpus shows that the convolution kernel over parse trees can achieve comparable performance with the previous best-reported feature-based methods on the 24 ACE relation subtypes. It also shows that our method significantly outperforms the previous two dependency tree kernels on the 5 ACE relation major types. 
In order for relation extraction systems to obtain human-level performance, they must be able to incorporate relational patterns inherent in the data (for example, that one’s sister is likely one’s mother’s daughter, or that children are likely to attend the same college as their parents). Hand-coding such knowledge can be time-consuming and inadequate. Additionally, there may exist many interesting, unknown relational patterns that both improve extraction performance and provide insight into text. We describe a probabilistic extraction model that provides mutual beneﬁts to both “top-down” relational pattern discovery and “bottom-up” relation extraction. 
We are trying to extend the boundary of Information Extraction (IE) systems. Existing IE systems require a lot of time and human effort to tune for a new scenario. Preemptive Information Extraction is an attempt to automatically create all feasible IE systems in advance without human intervention. We propose a technique called Unrestricted Relation Discovery that discovers all possible relations from texts and presents them as tables. We present a preliminary system that obtains reasonably good results. 
We present a method for induction of concise and accurate probabilistic contextfree grammars for efﬁcient use in early stages of a multi-stage parsing technique. The method is based on the use of statistical tests to determine if a non-terminal combination is unobserved due to sparse data or hard syntactic constraints. Experimental results show that, using this method, high accuracies can be achieved with a non-terminal set that is orders of magnitude smaller than in typically induced probabilistic context-free grammars, leading to substantial speed-ups in parsing. The approach is further used in combination with an existing reranker to provide competitive WSJ parsing results. 
We investigate prototype-driven learning for primarily unsupervised sequence modeling. Prior knowledge is speciﬁed declaratively, by providing a few canonical examples of each target annotation label. This sparse prototype information is then propagated across a corpus using distributional similarity features in a log-linear generative model. On part-of-speech induction in English and Chinese, as well as an information extraction task, prototype features provide substantial error rate reductions over competitive baselines and outperform previous work. For example, we can achieve an English part-of-speech tagging accuracy of 80.5% using only three examples of each tag and no dictionary constraints. We also compare to semi-supervised learning and discuss the system’s error trends. 
In this paper, we present a rule based model for morphological disambiguation of Turkish. The rules are generated by a novel decision list learning algorithm using supervised training. Morphological ambiguity (e.g. lives = live+s or life+s) is a challenging problem for agglutinative languages like Turkish where close to half of the words in running text are morphologically ambiguous. Furthermore, it is possible for a word to take an unlimited number of sufﬁxes, therefore the number of possible morphological tags is unlimited. We attempted to cope with these problems by training a separate model for each of the 126 morphological features recognized by the morphological analyzer. The resulting decision lists independently vote on each of the potential parses of a word and the ﬁnal parse is selected based on our conﬁdence on these votes. The accuracy of our model (96%) is slightly above the best previously reported results which use statistical models. For comparison, when we train a single decision list on full tags instead of using separate models on each feature we get 91% accuracy. 
We investigate the problem of training probabilistic context-free grammars on the basis of a distribution deﬁned over an inﬁnite set of trees, by minimizing the cross-entropy. This problem can be seen as a generalization of the well-known maximum likelihood estimator on (ﬁnite) tree banks. We prove an unexpected theoretical property of grammars that are trained in this way, namely, we show that the derivational entropy of the grammar takes the same value as the crossentropy between the input distribution and the grammar itself. We show that the result also holds for the widely applied maximum likelihood estimator on tree banks. 
We consider several empirical estimators for probabilistic context-free grammars, and show that the estimated grammars have the so-called consistency property, under the most general conditions. Our estimators include the widely applied expectation maximization method, used to estimate probabilistic context-free grammars on the basis of unannotated corpora. This solves a problem left open in the literature, since for this method the consistency property has been shown only under restrictive assumptions on the rules of the source grammar. 
Ranked lists of output trees from syntactic statistical NLP applications frequently contain multiple repeated entries. This redundancy leads to misrepresentation of tree weight and reduced information for debugging and tuning purposes. It is chieﬂy due to nondeterminism in the weighted automata that produce the results. We introduce an algorithm that determinizes such automata while preserving proper weights, returning the sum of the weight of all multiply derived trees. We also demonstrate our algorithm’s effectiveness on two large-scale tasks. 
The role of aggregation in natural language generation is to combine two or more linguistic structures into a single sentence. The task is crucial for generating concise and readable texts. We present an efﬁcient algorithm for automatically learning aggregation rules from a text and its related database. The algorithm treats aggregation as a set partitioning problem and uses a global inference procedure to ﬁnd an optimal solution. Our experiments show that this approach yields substantial improvements over a clustering-based model which relies exclusively on local information. 
We have explored the usefulness of incorporating speech and discourse features in an automatic speech summarization system applied to meeting recordings from the ICSI Meetings corpus. By analyzing speaker activity, turn-taking and discourse cues, we hypothesize that such a system can outperform solely text-based methods inherited from the ﬁeld of text summarization. The summarization methods are described, two evaluation methods are applied and compared, and the results clearly show that utilizing such features is advantageous and efﬁcient. Even simple methods relying on discourse cues and speaker activity can outperform text summarization approaches. 1. Introduction The task of summarizing spontaneous spoken dialogue from meetings presents many challenges: information is sparse; speech is disﬂuent and fragmented; automatic speech recognition is imperfect. However, there are numerous speech-speciﬁc characteristics to be explored and taken advantage of. Previous research on summarizing speech has concentrated on utilizing prosodic features [1, 2]. We have examined the usefulness of additional speech-speciﬁc characteristics such as discourse cues, speaker activity, and listener feedback. This speech features approach is contrasted with a second summarization approach using only textual features—a centroid method [3] using a latent semantic representation of utterances. These indi-  vidual approaches are compared to a combined approach as well as random baseline summaries. This paper also introduces a new evaluation scheme for automatic summaries of meeting recordings, using a weighted precision score based on multiple human annotations of each meeting transcript. This evaluation scheme is described in detail below and is motivated by previous ﬁndings [4] suggesting that n-gram based metrics like ROUGE [5] do not correlate well in this domain. 2. Previous Work In the ﬁeld of speech summarization in general, research investigating speech-speciﬁc characteristics has focused largely on prosodic features such as F0 mean and standard deviation, pause information, syllable duration and energy. Koumpis and Renals [1] investigated prosodic features for summarizing voicemail messages in order to send voicemail summaries to mobile devices. Hori et al. [6] have developed an integrated speech summarization approach, based on ﬁnite state transducers, in which the recognition and summarization components are composed into a single ﬁnite state transducer, reporting results on a lecture summarization task. In the Broadcast News domain, Maskey and Hirschberg [7] found that the best summarization results utilized prosodic, lexical, and structural features, while Ohtake et al. [8] explored using only prosodic features for summarization. Maskey and Hirschberg similarly found that prosodic features alone resulted in good quality summaries of  367 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 367–374, New York, June 2006. c 2006 Association for Computational Linguistics  
The TREC Deﬁnition and Relationship questions are evaluated on the basis of information nuggets that may be contained in system responses. Human evaluators provide informal descriptions of each nugget, and judgements (assignments of nuggets to responses) for each response submitted by participants. While human evaluation is the most accurate way to compare systems, approximate automatic evaluation becomes critical during system development. We present Nuggeteer, a new automatic evaluation tool for nugget-based tasks. Like the ﬁrst such tool, Pourpre, Nuggeteer uses words in common between candidate answer and answer key to approximate human judgements. Unlike Pourpre, but like human assessors, Nuggeteer creates a judgement for each candidatenugget pair, and can use existing judgements instead of guessing. This creates a more readily interpretable aggregate score, and allows developers to track individual nuggets through the variants of their system. Nuggeteer is quantitatively comparable in performance to Pourpre, and provides qualitatively better feedback to developers.  
The present methodology for evaluating complex questions at TREC analyzes answers in terms of facts called “nuggets”. The ofﬁcial F-score metric represents the harmonic mean between recall and precision at the nugget level. There is an implicit assumption that some facts are more important than others, which is implemented in a binary split between “vital” and “okay” nuggets. This distinction holds important implications for the TREC scoring model—essentially, systems only receive credit for retrieving vital nuggets—and is a source of evaluation instability. The upshot is that for many questions in the TREC testsets, the median score across all submitted runs is zero. In this work, we introduce a scoring model based on judgments from multiple assessors that captures a more reﬁned notion of nugget importance. We demonstrate on TREC 2003, 2004, and 2005 data that our “nugget pyramids” address many shortcomings of the present methodology, while introducing only minimal additional overhead on the evaluation ﬂow. 
We present an approach to building a test collection of research papers. The approach is based on the Cranﬁeld 2 tests but uses as its vehicle a current conference; research questions and relevance judgements of all cited papers are elicited from conference authors. The resultant test collection is different from TREC’s in that it comprises scientiﬁc articles rather than newspaper text and, thus, allows for IR experiments that include citation information. The test collection currently consists of 170 queries with relevance judgements; the document collection is the ACL Anthology. We describe properties of our queries and relevance judgements, and demonstrate the use of the test collection in an experimental setup. One potentially problematic property of our collection is that queries have a low number of relevant documents; we discuss ways of alleviating this. 
Test collections are essential to evaluate Information Retrieval (IR) systems. The relevance assessment set has been recognized as the key bottleneck in test collection building, especially on very large sized document collections. This paper addresses the problem of eﬃciently selecting documents to be included in the assessment set. We will show how machine learning techniques can ﬁt this task. This leads to smaller pools than traditional round robin pooling, thus reduces signiﬁcantly the manual assessment workload. Experimental results on TREC collections1 consistently demonstrate the effectiveness of our approach according to diﬀerent evaluation criteria. 
Language model information retrieval depends on accurate estimation of document models. In this paper, we propose a document expansion technique to deal with the problem of insufﬁcient sampling of documents. We construct a probabilistic neighborhood for each document, and expand the document with its neighborhood information. The expanded document provides a more accurate estimation of the document model, thus improves retrieval accuracy. Moreover, since document expansion and pseudo feedback exploit different corpus structures, they can be combined to further improve performance. The experiment results on several different data sets demonstrate the effectiveness of the proposed document expansion method. 
Large-scale web-search engines are generally designed for linear text. The linear text representation is suboptimal for audio search, where accuracy can be signiﬁcantly improved if the search includes alternate recognition candidates, commonly represented as word lattices. This paper proposes a method for indexing word lattices that is suitable for large-scale web-search engines, requiring only limited code changes. The proposed method, called Time-based Merging for Indexing (TMI), ﬁrst converts the word lattice to a posterior-probability representation and then merges word hypotheses with similar time boundaries to reduce the index size. Four alternative approximations are presented, which differ in index size and the strictness of the phrase-matching constraints. Results are presented for three types of typical web audio content, podcasts, video clips, and online lectures, for phrase spotting and relevance ranking. Using TMI indexes that are only ﬁve times larger than corresponding lineartext indexes, phrase spotting was improved over searching top-1 transcripts by 25-35%, and relevance ranking by 14%, at only a small loss compared to unindexed lattice search. 
We describe ﬁnite-state constraint relaxation, a method for applying global constraints, expressed as automata, to sequence model decoding. We present algorithms for both hard constraints and binary soft constraints. On the CoNLL-2004 semantic role labeling task, we report a speedup of at least 16x over a previous method that used integer linear programming. 
Recent work on semantic role labeling (SRL) has focused almost exclusively on the analysis of the predicate-argument structure of verbs, largely due to the lack of human-annotated resources for other types of predicates that can serve as training and test data for the semantic role labeling systems. However, it is wellknown that verbs are not the only type of predicates that can take arguments. Most notably, nouns that are nominalized forms of verbs and relational nouns generally are also considered to have their own predicate-argument structure. In this paper we report results of SRL experiments on nominalized predicates in Chinese, using a newly completed corpus, the Chinese Nombank. We also discuss the impact of using publicly available manually annotated verb data to improve the SRL accuracy of nouns, exploiting a widely-held assumption that verbs and their nominalizations share the same predicate-argument structure. Finally, we discuss the results of applying reranking techniques to improve SRL accuracy for nominalized predicates, which showed insigniﬁcant improvement. 
We present a novel statistical approach to semantic parsing, WASP, for constructing a complete, formal meaning representation of a sentence. A semantic parser is learned given a set of sentences annotated with their correct meaning representations. The main innovation of WASP is its use of state-of-the-art statistical machine translation techniques. A word alignment model is used for lexical acquisition, and the parsing model itself can be seen as a syntax-based translation model. We show that WASP performs favorably in terms of both accuracy and coverage compared to existing learning methods requiring similar amount of supervision, and shows better robustness to variations in task complexity and word order. 
ParaEval is an automated evaluation method for comparing reference and peer summaries. It facilitates a tieredcomparison strategy where recall-oriented global optimal and local greedy searches for paraphrase matching are enabled in the top tiers. We utilize a domainindependent paraphrase table extracted from a large bilingual parallel corpus using methods from Machine Translation (MT). We show that the quality of ParaEval’s evaluations, measured by correlating with human judgments, closely resembles that of ROUGE’s. 
This paper studies the impact of paraphrases on the accuracy of automatic evaluation. Given a reference sentence and a machine-generated sentence, we seek to ﬁnd a paraphrase of the reference sentence that is closer in wording to the machine output than the original reference. We apply our paraphrasing method in the context of machine translation evaluation. Our experiments show that the use of a paraphrased synthetic reference reﬁnes the accuracy of automatic evaluation. We also found a strong connection between the quality of automatic paraphrases as judged by humans and their contribution to automatic evaluation. 
This paper presents a solution to the problem of matching personal names in English to the same names represented in Arabic script. Standard string comparison measures perform poorly on this task due to varying transliteration conventions in both languages and the fact that Arabic script does not usually represent short vowels. Significant improvement is achieved by augmenting the classic Levenshtein edit-distance algorithm with character equivalency classes. 
 We propose a new document vector represen-  tation speciﬁcally designed for the document  clustering task. Instead of the traditional term-  based vectors, a document is represented as an  -dimensional vector, where is the number of  documents in the cluster. The value at each di-  mension of the vector is closely related to the  generation probability based on the language  model of the corresponding document. In-  spired by the recent graph-based NLP methods,  we reinforce the generation probabilities by it-  erating random walks on the underlying graph  representation. Experiments with k-means and  hierarchical clustering algorithms show icant improvements over the alternative  ¡s¢ig£n¤i¥f¢-  vector representation.  
It is practically impossible to build a word-based lexicon for speech recognition in agglutinative languages that would cover all the relevant words. The problem is that words are generally built by concatenating several preﬁxes and sufﬁxes to the word roots. Together with compounding and inﬂections this leads to millions of different, but still frequent word forms. Due to inﬂections, ambiguity and other phenomena, it is also not trivial to automatically split the words into meaningful parts. Rule-based morphological analyzers can perform this splitting, but due to the handcrafted rules, they also suffer from an out-of-vocabulary problem. In this paper we apply a recently proposed fully automatic and rather language and vocabulary independent way to build subword lexica for three different agglutinative languages. We demonstrate the language portability as well by building a successful large vocabulary speech recognizer for each language and show superior recognition performance compared to the corresponding word-based reference systems. 
A corpus is described consisting of non-scripted monologues and dialogues, recorded by 22 speakers, comprising a total of about 70.000 words, corresponding to well over 10 hours of speech. The monologues were recorded as one-way communication with blind partner where the speaker performed three different tasks: (S)he described a network consisting of various geometrical shapes in various colours. (S)he guided the listener through four different routes in a virtual city map.(S)he instructed the listener how to build a house from its individual parts. The dialogues are replicas of the HCRC map tasks (http://www.hcrc.ed.ac.uk/maptask/). Annotation is performed in Praat. The sound files are segmented into prosodic phrases, words, and syllables, always to the nearest zero-crossing in the waveform. It is supplied, in seven separate interval tiers, with an orthographical transcription, detailed part-of-speech tags, simplified part-of-speech tags, a phonological transcription, a broad phonetic transcription, the pitch relation between each stressed and post-tonic syllable, the phrasal intonation, and an empty tier for comments.
In the Masoretic text of the Hebrew Bible (HB), the cantillation marks function like a punctuation system that shows the division and subdivision of each verse, forming a tree structure which is similar to the prosodic tree in modern linguistics. However, in the Masoretic text, the structure is hidden in a complicated set of diacritic symbols and the rich information is accessible only to a few trained scholars. In order to make the structural information available to the general public and to automatic processing by the computer, we built a tree bank where the hierarchical structure of each HB verse is explicitly represented in XML format. We coded the punctuation system in a context-tree grammar which was then used by a CYK parser to automatically generate trees for the whole HB. The results show that (1) the CFG correctly encoded the annotation rules and (2) the annotation done by the Masoretes is highly consistent.
Techno-langue is the French National Program on HLT supported by the French Ministries in charge of Research, Industry and Culture. It addresses four action lines: creating basic language and software resources, organizing evaluation campaigns, participating in the standardization process and creating a Web Portal for disseminating information and surveys to a large audience. This paper presents the main results of the program and an ongoing initiative for launching a transnational program at the European level on a similar basis.
We present an overview of Regulus, an Open Source platform that supports corpus-based derivation of efficient domain-specific speech recognisers from general linguistically motivated unification grammars. We list available Open Source resources, which include compilers, resource grammars for various languages, documentation and a development environment. The greater part of the paper presents a series of experiments carried out using a medium-vocabulary medical speech translation application and a corpus of 801 recorded domain utterances, designed to investigate the impact on speech understanding performance of vocabulary size, grammatical coverage, presence or absence of various linguistic features, degree of generality of thegrammar and use or otherwise of probabilistic weighting in the CFGlanguage model. In terms of task accuracy, the most significant factors were the use of probabilistic weighting, the degree of generality of the grammar and the inclusion of features which model sortal restrictions.
This paper describes the implementation and evaluation of a generic component to extract temporal information from texts in Swedish. It proceeds in two steps. The first step extracts time expressions and events, and generates a feature vector for each element it identifies. Using the vectors, the second step determines the temporal relations, possibly none, between the extracted events and orders them in time. We used a machine learning approach to find the relations between events. To run the learning algorithm, we collected a corpus of road accident reports from newspapers websites that we manually annotated. It enabled us to train decision trees and to evaluate the performance of the algorithm.
On the example of the recent edition of the Frequency Dictionary of Czech wedescribe and explain some new general principles that should be followed forgetting better results for practical uses of frequency dictionaries. It ismainly adopting average reduced frequency instead of absolute frequency forordering items. The formula for calculation of the average reduced frequencyis presented in the contribution together with a brief explanation, including examples clarifying the difference between the measures. Then, the Frequency Dictionary of Czech and its parts are described.
This paper reports work carried out to develop a speller for Spanish at Microsoft Corporation, discusses the technique for isolated-word error correction used by the speller, provides general descriptions of the error data collection and error typology, and surveys a variety of linguistic considerations relevant when dealing with a world language spread over several countries and exposed to different language influences. We show that even though it has been claimed that the state of the art for practical applications based on isolated word error correction does not offer always a sensible set of ranked candidates for the misspelling, the introduction of a finer-grained categorization of errors and the use of their relative frequency has had a positive impact in the speller application developed for Spanish (the corresponding evaluation data is presented).
This paper presents the results of the usability evaluations that were conducted within TransType2, an international R{\&}D project the goal of which was to develop a novel approach to interactive machine translation. We briefly sketch the TransType system and then describe the methodology that we elaborated for the five rounds of user trials that were held on the premises of two translation agencies over the last eighteen months of the project. We provide the productivity results posted by the six translators who tested the system and we also discuss some of the non-quantitative factors which influenced the users reaction to TransType.
This paper describes an emotional speech database recorded for standard Basque. The database has been designed with the twofold purpose of being used for corpus based synthesis, and also of allowing the study of prosodic models for the emotions. The database is thus large, to get good corpus based synthesis quality and contains the same texts recorded in the six basic emotions plus the neutral style. The recordings were carried out by two professional dubbing actors, a man and a woman. The paper explains the whole creation process, beginning with the design stage, following with the corpus creation and the recording phases, and finishing with some learned lessons and hints.
Corpora annotated with structural and linguistic characteristics play a major role in nearly every area of language processing. During recent years a number of corpora and large data sets became known and available to research even in specialized fields such as medicine, but still however, targeted predominantly for the English language. This paper provides a description of the collection, encoding and linguistic processing of an ever growing Swedish medical corpus, the MEDLEX Corpus. MEDLEX consists of a variety of text-documents related to various medical text genres. The MEDLEX Corpus has been structurally annotated using the Corpus Encoding Standard for XML (XCES), lemmatized and automatically annotated with part-of-speech and semantic information (extended named entities and the Medical Subject Headings, MeSH, terminology). The results from the processing stages (part-of-speech, entities and terminology) have been merged into a single representation format and syntactically analysed using a cascaded finite state parser. Finally, the parsers results are converted into a tree structure that follows the TIGER-XML coding scheme, resulting a suitable for further exploration and fairly large Treebank of Swedish medical texts.
Many systems have been developed for creating syntactically annotated corpora. However, they mainly focus on interface usability and hardly pay attention toknowledge sharing among annotators in the task. In order to incorporate the functionality of knowledge sharing, we emphasized the importance of normalizingthe annotation process. As a first step toward knowledge sharing, this paper proposes a method of system initiative annotation in which the system suggests annotators the order of ambiguities to solve. To be more concrete, the system forces annotators to solve ambiguity of constituent structure in a top-down and depth-first manner, and then to solve ambiguity of grammatical category in a bottom-up and breadth-first manner. We implemented the system on top of eBonsai, our annotation tool, and conducted experiments to compare eBonsai and the proposed system in terms of annotation accuracy and efficiency. We found that at least for novice annotators, the proposed system is more efficient while keeping annotation accuracy comparable with eBonsai.
Automatic speech recognition (ASR) technology has achieved a level of maturity, where it is already practical to be used by novice users. However, most non-native speakers are still not comfortable with services including ASR systems, because of the accuracy on non-native speakers. This paper describes our approach in constructing a non-native corpus particularly in French for testing and adapting non-native speaker for automatic speech recognition. Finally, we also propose in this paper a method for detecting pronunciation variants and possible pronunciation mistakes by non-native speakers.
This paper describes the Pavia Typological Database (PTD), a follow-up to the MED-TYP database (Sans{\`o} 2004). The PTD is an ever-growing repository of primary linguistic data (words, clauses, sentences) documenting a number of morphosyntactic phenomena in the languages of Europe (and including in some cases languages from the Mediterranean area). Its prospective users are typologists wanting to access primary, typologically uninterpreted (but glossed) data, but also anyone interested in linguistic variation on a continental scale. The paper discusses the background and motivation for the creation of the PTD, its present coverage, the techniques used to annotate the primary data, and the general architecture of the database.
In this paper building statistical language models for Persian language using a corpus and incorporating them in Persian continuous speech recognition (CSR) system are described. We used Persian Text Corpus for building the language models. First we preprocessed the texts of corpus by correcting the different orthography of words. Also, the number of POS tags was decreased by clustering POS tags manually. Then we extracted word based monogram and POS-based bigram and trigram language models from the corpus. We also present the procedure of incorporating language models in a Persian CSR system. By using the language models 27.4{\%} reduction in word error rate was achieved in the best case.
This paper compares the effectiveness of two different Thai search engines by using a blind evaluation. The probabilistic-based dictionary-less search engine is evaluated against the traditional word-based indexing method. The web documents from 12 Thai newspaper web sites consisting of 83,453 documents are used as the test collection. The relevance judgment is conducted on the first five returned results from each system. The evaluation process is completely blind. That is, the retrieved documents from both systems are shown to the judges without any information about thesearch techniques. Statistical testing shows that the dictionary-less approach is better than the word-based indexingapproach in terms of the number of found documents and the number of relevance documents.
Large scale annotated corpora are very important not only inlinguistic research but also in practical natural language processingtasks since a number of practical tools such as Part-of-speech (POS) taggers and syntactic parsers are now corpus-based or machine learning-based systems which require some amount of accurately annotated corpora. This article presents an annotated corpus management tool that provides various functions that include flexible search, statistic calculation, and error correction for linguistically annotated corpora. The target of annotation covers POS tags, base phrase chunks and syntactic dependency structures. This tool aims at helping development of consistent construction of lexicon and annotated corpora to be used by researchers both in linguists and language processing communities.
In thesauri, conceptual structures or semantic networks, relationships are too often vague. For instance, in terminology, the relationships between concepts are often reduced to the distinction established by standard (ISO 704, 1987) and (ISO 1087, 1990) between hierarchical relationships (genus-species relationships and part/whole relationships) and non-hierarchical relationships (time, space, causal relationships, etc.). The semantics of relationships are vague because the principal users of these relationships are industrial actors (translators of technical handbooks, terminologists, data-processing specialists, etc.). Nevertheless, the consistency of the models built must always be guaranteed... One possible approach to this problem consists in organizing the relationships in a typology based on logical properties. For instance, we typically use only the general relation Is-a. It is too vague. We assume that general relation Is-a is characterized by asymmetry. This asymmetry is specified in: (1) the belonging of one individualizable entity to a distributive class, (2) Inclusion among distributive classes and (3) relation part of (or composition).
This article describes an exclusively resource-based method of morphological annotation of written Korean text. Korean is an agglutinative language. Our annotator is designed to process text before the operation of a syntactic parser. In its present state, it annotates one-stem words only. The output is a graph of morphemes annotated with accurate linguistic information. The granularity of the tagset is 3 to 5 times higher than usual tagsets. A comparison with a reference annotated corpus showed that it achieves 89{\%} recall without any corpus training. The language resources used by the system are lexicons of stems, transducers of suffixes and transducers of generation of allomorphs. All can be easily updated, which allows users to control the evolution of the performances of the system. It has been claimed that morphological annotation of Korean text could only be performed by a morphological analysis module accessing a lexicon of morphemes. We show that it can also be performed directly with a lexicon of words and without applying morphological rules at annotation time, which speeds up annotation to 1,210 words. The lexicon of words is obtained from the maintainable language resources through a fully automated compilation process.
In this paper, we present the results of a preliminary investigation that aims at constructing a repository of preposition syntactic and semantic behaviors. A preliminary frame-based format for representing their prototypical behavior is then proposed together with related inferential patterns that describe functional or paradigmatic relations between preposition senses.
This paper presents a project that aims at building lexical resources for terminology. By lexical resources, we mean dictionaries that provide detailed lexico-semantic information on terms, i.e. lexical units the sense of which can be related to a special subject field. In terminology, there is a lack of such resources. The specific dictionaries we are currently developing describe basic French and Korean terms that belong to the fields of computer science and the Internet (e.g. computer, configure, user-friendly, Web, browse, spam). This paper presents the structure of the French and Korean articles: each component is examined and illustrated with examples. We then describe the corpus-based methodology and the different computer applications used for developing the articles. Our methodology comprises five steps: design of the corpora, selection of terms; sense distinction; definition of actantial structures and listing of semantic relations. Details on the current state of each database are also given.
We present a series of experiments investigating face-to-face interaction between an Embodied Conversational Agent (ECA) and a human interlocutor. The ECA is embodied by a video realistic talking head with independent head and eye movements. For a beneficial application in face-to-face interaction, the ECA should be able to derive meaning from communicational gestures of a human interlocutor, and likewise to reproduce such gestures. Conveying its capability to interpret human behaviour, the system encourages the interlocutor to show appropriate natural activity. Therefore it is important that the ECA knows how to display what would correspond to mental states in humans. This allows to interpret the machine processes of the system in terms of human expressiveness and to assign them a corresponding meaning. Thus the system may maintain an interaction based on human patterns. During a first experiment we investigated the ability of our talking head to direct user attention with facial deictic cues (Raidt, Bailly et al. 2005). Users interact with the ECA during a simple card game offering different levels of help and guidance through facial deictic cues. We analyzed the users performance and their perception of the quality of assistance given by the ECA. The experiment showed that users profit from its presence and its facial deictic cues. In the continuative series of experiments presented here, we investigated the effect of an enhancement of the multimodality of the deictic gestures by adding a spoken instruction.
This paper presents my manual skeleton parsing on a sample text of approximately 100,000 word tokens (or about 2,500 sentences) taken from the PFR Chinese Corpus with a clearly defined parsing scheme of 17 constituent labels. The manually-parsed sample skeleton treebank is one of the very few extant Chinese treebanks. While Chinese part-of-speech tagging and word segmentation have been the subject of concerted research for many years, the syntactic annotation of Chinese corpora is a comparatively new field. The difficulties that I encountered in the production of this treebank demonstrate some of the peculiarities of Chinese syntax. A noteworthy syntactic property is that some serial verb constructions tend to be used as if they were compound verbs. The two transitive verbs in series, unlike common transitive verbs, do not take an object separately within the construction; rather, the serial construction as a whole is able to take the same direct object and the perfective aspect marker le. The skeleton-parsed sample treebank is evaluated against Eyes {\&} Leech (1993)s criteria and proves to be accurate, uniform and linguistically valid.
Computer-aided summarisation is a technology developed at the University of Wolverhampton as a complement to automatic summarisation, to produce high quality summaries with less effort. To achieve this, a user-friendly environment which incorporates several well-known summarisation methods has been developed. This paper presents the main features of the computer-aided summarisation environment and explains the changes introduced to it as a result of user feedback.
This paper describes the Chinese NomBank Project, the goal of which is to annotate the predicate-argument structure of nominalized predicates in Chinese. The Chinese Nombank extends the general framework of the English and Chinese Proposition Banks to the annotation of nominalized predicates and adds a layer of semantic annotation to the Chinese Treebank. We first outline the scope of the work by discussing the markability of the nominalized predicates and their arguments. We then attempt to provide a categorization of the distribution of the arguments of nominalized predicates. We also discuss the relevance of the event/result distinction to the annotation of nominalized predicates and the phenomenon of incorporation. Finally we discuss some cross-linguistic differences between English and Chinese.
In this paper we evaluate the Basic Travel Expression Corpus (BTEC), developed by ATR (Advanced Telecommunication Research Laboratory), Japan. BTEC was specifically developed as a wide-coverage, consistent corpus containing basic Japanese travel expressions with English counterparts, for the purpose of providing basic data for the development of high quality speech translation systems. To evaluate the corpus, we introduce a quantitative method for evaluating the sufficiency of qualitatively well-defined corpora, on the basis of LNRE methods that can estimate the potential growth patterns of various sparse data by fitting various skewed distributions such as the Zipfian group of distributions, lognormal distribution, and inverse Gauss-Poisson distribution to them. The analyses show the coverage of lexical items of BTEC vis-a-vis the possible targets implicitly defined by the corpus itself, and thus provides basic insights into strategies for enhancing BTEC in future.
In this paper, we analyze nurses' dialogue and conversation data sets after manual transcriptions and show their features. Recently, medical risk management has been recognized as very important for both hospitals and their patients. To carry out medical risk management, it is important to model nursing activities as well as to collect many accident and incident examples. Therefore, we are now researching strategies of modeling nursing activities in order to understand them (E-nightingale Project). To model nursing activities, it is necessary to collect data of nurses' activities in actual situations and to accurately understand these activities and situations. We developed a method to determine any type of nursing activity from voice data. However we found that our method could not determine several activities because it misunderstood special nursing terms. To improve the accuracy of this method, we focus on analyzing nurses' dialogue and conversation data and on collecting special nursing terms. We have already collected 800 hours of nurses' dialogue and conversation data sets in hospitals to find the tendencies and features of how nurses use special terms such as abbreviations and jargon as well as new terms. Consequently, in this paper we categorize nursing terms according to their usage and effectiveness. In addition, based on the results, we show a rough strategy for building nursing dictionaries.
In this paper we provide an overview of the first evaluation contest for named entity recognition in Portuguese, HAREM, which features several original traits and provided the first state of the art for the field in Portuguese, as well as a public-domain evaluation architecture.
This paper describes an acceptance test procedure for evaluating a spoken language translation system between Catalan and Spanish. The procedure consists of two independent tests. The first test was an utterance-oriented evaluation for determining how the use of speech benefits communication. This test allowed for comparing relative performance of the different system components, explicitly: source text to target text, source text to target speech, source speech to target text, and source speech to target speech. The second test was a task-oriented experiment for evaluating if users could achieve some predefined goals for a given task with the state of the technology. Eight subjects familiar with the technology and four subjects not familiar with the technology participated in the tests. From the results we can conclude that state of technology is getting closer to provide effective speech-to-speech translation systems but there is still lot of work to be done in this area. No significant differences in performance between users that are familiar with the technology and users that are not familiar with the technology were evidenced. This constitutes, as far as we know, the first evaluation of a Spoken Translation System that considers performance at both, the utterance level and the task level.
Sumerian is a long-extinct language documented throughout the ancient MiddleEast, arguably the first language for which we have written evidence, and is a language isolate (i.e. no related languages have so far been identified). The Electronic Text Corpus of Sumerian Literature (ETCSL), based at theUniversity of Oxford, aims to make accessible on the web over 350 literary workscomposed during the late third and early second millennia BCE. The transliterations and translations can be searched, browsed and read online using the tools of the website. In this paper we describe the creation of linguistic analysis and corpus search tools for Sumerian, as part of the development of the ETCSL. This is designed to enable Sumerian scholars, students and interested laymen to analyse the texts online and electronically, and to further knowledge about the language.
This paper describes the Norwegian speech database FonDat1 designedfor development and assessment of Norwegian unit selection speechsynthesis. The quality of unit selection speech synthesis systems depends highly on the database used. The database should contain sufficient phonemicand prosodic coverage. High quality unit selection synthesis alsorequires that the database is annotated with accurate information about identity and position of the units.Traditionally this involves much manual work, either by hand labelingthe entire database or by correcting automatic annotations. We are working on methods for a complete automation of the annotationprocess. To validate these methods a realistic unit selectionsynthesis database is needed.In addition to serve as a testbed for annotation tools and synthesisexperiments, the process of producing the database using automaticmethods is in itself an important result.FonDat1 contains studio recordings of approximately 2000 sentencesread by two professional speakers, one male and one female. 10{\%} ofthe database is manually annotated.
Computational lexicons are among the most important resources for natural language processing (NLP). Their importance is even greater in languages with rich morphology, where the lexicon is expected to provide morphological analyzers with enough information to enable themto correctly process intricately inflected forms. We describe the Haifa Lexicon of Contemporary Hebrew, the broadest-coverage publicly available lexicon of Modern Hebrew, currently consisting of over 20,000 entries.While other lexical resources of Modern Hebrew have been developed in the past, this is the first publicly available large-scale lexicon of the language. In addition to supporting morphological processors (analyzers and generators), which was our primary objective, thelexicon is used as a research tool in Hebrew lexicography and lexical semantics. It is open for browsing on the web and several search tools and interfaces were developed which facilitate on-line access to its information. The lexicon is currently used for a variety of NLP applications.
This paper presents in brief several ongoing projects whose aim is to develop the first LRs for Macedonian, in particular the raw corpus compiled by Prof. George Mitrevski at the Auburn University, the preparation for the compilation of a reference corpus for the Macedonian written language at the MASA (Macedonian Academy of Sciences and Arts), the first small annotated corpus of the Macedonian translation of the Orwells 1984, the electronic dictionary of simple words created by Aleksandar Petrovski for the Macedonian module in the frame of the corpus processing system Intex/Nooj and the Morphological dictionary developed by the LTRC (Language Technology Research Center). Further we discuss the importance of the development of the basic LRs for Macedonian as a means of preservation and a prerequisite for the creation of the first commercial language products for this Slavic language.
Machine translation systems, whether rule-based, example-based, or statistical, all rely on dictionaries that are in essence mappings between individual words of the source and the target language. Criteria for the disambiguation of ambiguous words and for differences in word order between the two languages are not accounted for in the lexicon. Instead, these important issues are dealt with in the translation engines. Because the engines tend to be compact and (even with data-oriented approaches) do not fully reflect the complexity of the problem, this approach generally does not account for the more fine grained facets of word behavior. This leads to wrong generalizations and, as a consequence, translation quality tends to be poor. In this paper we suggest to approach this problem by using a new type of lexicon that is not based on individual words but on pairs of words. For each pair of consecutive words in the source language the lexicon lists the possible translations in the target language together with information on order and distance of the target words. The process of machine translation is then seen as a combinatorial problem: For all word pairs in a source sentence all possible translations are retrieved from the lexicon and then those translations are discarded that lead to contradictions when constructing the target sentence. This process implicitly leads to word sense disambiguation and to language specific reordering of words.
This paper describes the CESART project which deals with the evaluation of terminological resources acquisition tools. The objective of the project is to propose and validate an evaluation protocol allowing one to objectively evaluate and compare different systems for terminology application such as terminological resource creation and semantic relation extraction. The project also aims to create quality-controlled resources such as domain-specific corpora, automatic scoring tool, etc.
This paper describes the methodology and tools that are the basis of our platform AAILE.4 AAILE has been built for supplying those working in the construction of lexicons for syntactic parsing with more efficient ways of visualizing and analyzing data extracted from corpus. The platform offers support using techniques such as similarity measures, clustering and pattern classification.
In this work, a spontaneous speech corpus of broadcasted television material in European Portuguese (EP) is presented. We decided to name it ProGmatica as it is meant to combine prosody information under a pragmatic framework. Our purpose is to analyse, describe and predict the prosodic patterns that are involved in speech acts and discourse events. It is also our goal to relate both prosody and pragmatics to emotion, style and attitude. In future developments, we intend, by this way, to provide EP TTS systems with pragmatic and emotional dimensions. From the whole recorded material we selected, extracted and saved prototypical speech acts with the help of speech analysis tools. We have a multi-speaker corpus, where linguistic, paralinguistic and extra linguistic information are labelled and related to each other. The paper is organized as follows. In section one, a brief state-of-the-art for the available EP corpora containing prosodic information is presented. In section two, we explain the pragmatic criteria used to structure this database. Then, we describe how the speech signal was labelled and which information layers were considered. In section three, we propose a prosodic prediction model to be applied to each speech act in future. In section four, some of the main problems we went through are discussed and future work is presented.
We present the IQMT Framework for Machine Translation Evaluation Inside QARLA. IQMT offers a common workbench in which existing evaluation metrics can be utilized and combined. It provides i) a measure to evaluate the quality of any set of similarity metrics (KING), ii) a measure to evaluate the quality of a translation using a set of similarity metrics (QUEEN), and iii) a measure to evaluate the reliability of a test set (JACK). The first release of the IQMT package is freely available for public use. Current version includes a set of 26 metrics from 7 different well-known metric families, and allows the user to supply its own metrics. For future releases, we are working on the design of new metrics that are able to capture linguistic aspects of translation beyond lexical ones.
The aim of this work is the presentation and preliminary evaluation of an XML annotation scheme for marking bridging anaphors of the form definite article + N in Italian. The scheme is based on a corpus-study. The data we collected from the evaluation experiment seem to support the reliability of the scheme, although some problems still remain open.
In TC-STAR a variety of Language Resources (LR) is being produced. In this contribution we address the resources that have been created for Automatic Speech Recrognition and Spoken Language Translation. As yet, these are 14 LR in total: two training SLR for ASR (English and Spanish), three development LR and three evaluation LR for ASR (English, Spanish, Mandarin), and three development LR and three evaluation LR for SLT (English-Spanish, Spanish-English, Mandarin-English). In this paper we describe the properties, validation, and availability of these resources.
Chat language refers to the special human language widely used in the community of digital network chat. As chat language holds anomalous characteristics in forming words, phrases, and non-alphabetical characters, conventional natural language processing tools are ineffective to handle chat language text. Previous research shows that knowledge based methods perform less effectively in proc-essing unseen chat terms. This motivates us to construct a chat language corpus so that corpus-based techniques of chat language text processing can be developed and evaluated. However, creating the corpus merely by hand is difficult. One, this work is manpower consuming. Second, annotation inconsistency is serious. To minimize manpower and annotation inconsistency, a two-stage incre-mental annotation approach is proposed in this paper in constructing a chat language corpus. Experiments conducted in this paper show that the performance of corpus annotation can be improved greatly with this approach.
While much effort is expended in the curation of language resources, such investment is largely irrelevant if users cannot locate resourcesof interest. The Open Language Archives Community (OLAC) was established to define standards for the description of language resources and providecore infrastructure for a virtual digital library, thus addressing the resource discovery issue. In this paper we consider naturalistic user search behaviour in the Open Language Archives Community. Specifically, we have collected the query logs from the OLAC Search Engine over a 2 year period, collecting in excess of 1.2 million queries, in over 450K user search sessions. Subsequently we have mined these to discover user search patterns of various types, all pertaining to the discovery of language resources.A number of interesting observations can be made based on this analysis, in this paper we report on a range of properties and behaviours based on empirical evidence.
This paper proposes a discrimination method for hierarchical relationsbetween word pairs. The method is a statistical one using an encyclopedic corpus' extracted and organized from Web pages.In the proposed method, we use the statistical naturethat hyponyms' descriptionstend to include hypernyms whereas hypernyms' descriptions do notinclude all of the hyponyms.Experimental results show that the method detected 61.7{\%} of therelations in an actual thesaurus.
Natural language processing researchers currently have access to a wealth of information about words and word senses. This presents problems as well as resources, as it is often difficult to search through and coordinate lexical information across various data sources. We have approached this problem by creating a shared environment for various lexical resources. This browser, BULB (Brandeis Unified Lexical Browser) and its accompanying front-end provides the NLP researcher with a coordinated display from many of the available lexical resources, focusing, in particular, on a newly developed lexical database, the Brandeis Semantic Ontology (BSO). BULB is a module-based browser focusing on the interaction and display of modules from existing NLP tools. We discuss the BSO, PropBank, FrameNet, WordNet, and CQP, as well as other modules which will extend the system. We then outline future extensions to this work and present a release schedule for BULB.
We describe SProUTomat, a tool for daily building, testing and evaluating a complex general-purpose multilingual natural language text processor including its linguistic resources (lingware). Software and lingware are developed, maintained and extended in a distributed manner by multiple authors and projects, i.e., the source code stored in a version control system is modified frequently. The modular design of different, dedicated lingware modules like tokenizers, morphology, gazetteers, type hierarchy, rule formalism on the one hand increases flexibility and re-usability, but on the other hand may lead to fragility with respect to changes. Therefore, frequent testing as known from software engineering is necessary also for lingware to warrant a high level of quality and overall stability of the system. We describe the build, testing and evaluation methods for LT software and lingware we have developed on the basis of the open source, platform-independent Apache Ant tool and the configurable evaluation tool JTaCo.
The RNC now it is a 120 million-word collection of Russian text, thus, it is the most representative and authoritative corpus of the Russian language. It is available in the Internet at www.ruscorpora.ru. The RNC contains texts of all genres and types, which covers Russian from 19 up to 21 centuries. The practice of national corpora constructing has revealed that it's indispensable to include in the RNC the sub-corpora of spoken language. Therefore, the constructors of the RNC have an intention to include in it about 10 million words of Spoken Russian. Oral speech in the Corpus is represented in the standard Russian orthography. Although this decision made impossible any phonetic exploration of the Spoken Russian Corpus, but studying Spoken Russian from any other linguistic point of view is completely available. In addition to traditional annotations (metatextual and morphological), in Spoken Sub-corpus there is sociological annotation. Unlike the standard oral speech, which is spontaneous and isn't intended to be reproduced, Multimedia Spoken Russian (MSR) is otherwise in great deal premeditated and evidently meant to be reproduced. MSR is also to be included in the RNC: first of all we plan to make the very interesting and provocative part of the RNC from the textual ingredient of about 300 Russian films.
In this paper we describe SOBA, a sub-component of the SmartWeb multi-modal dialog system. SOBA is a component for ontologybased information extraction from soccer web pages for automatic population of a knowledge base that can be used for domainspecific question answering. SOBA realizes a tight connection between the ontology, knowledge base and the information extraction component. The originality of SOBA is in the fact that it extracts information from heterogeneous sources such as tabular structures, text and image captions in a semantically integrated way. In particular, it stores extracted information in a knowledge base, and in turn uses the knowledge base to interpret and link newly extracted information with respect to already existing entities.
This paper is dealing with a new web interface to display linguistic data on the web. This new web interface is a general proposal for the web. Its present name is Alexandria. Alexandria is an amazing tool that can be downloaded free of charge, under certain conditions. Although the initial idea was hatched six or seven years ago, its technical realization has only been feasible for the past two years. If you want to read the HTML page, for instance http://www.memodata.com, double click on any word at random and you'll see a window open with a definition of the word followed by a list of synonyms and expressions using the word. If not, your browser is not in French. Then, you have to use the menu to modify the target language and choice the French between 22 languages.
Our research is concerned with the development of robotic systems which can support people in household environments, such as taking care of elderly people. A central goal of our research consists in creating robot systems which are able to learn and communicate about a given environment without the need of a specially trained user. For the communication with such users it is necessary that the robot is able to communicate multimodally, which especially includes the ability to communicate in natural language. Our research is concerned with the development of robotic systems which can support people in household environments, such as taking care of elderly people. A central goal of our research consists in creating robot systems which are able to learn and communicate about a given environment without the need of a specially trained user. For the communication with such users it is necessary that the robot is able to communicate multimodally, which especially includes the ability to communicate in natural language. We believe that the ability to communicate naturally in multimodal communication must be supported by the ability to access contextual information, with topical knowledge being an important aspect of this knowledge. Therefore, we currently develop a topic tracking system for situated human-robot communication on our robot systems. This paper describes the BITT (Bielefeld Topic Tracking) corpus which we built in order to develop and evaluate our system. The corpus consists of human-robot communication sequences about a home-like environment, delivering access to the information sources a multimodal topic tracking system requires.
The Euroling stemmer is developed for a commercial web site and intranet search engine called SiteSeeker. SiteSeeker is basically used in the Swedish domain but to some extent also for the English domain. CST's lemmatiser comes from the Center for Language Technology, University of Copenhagen and was originally developed as a research prototype to create lemmatisation rules from training data. In this paper we compare the performance of the stemmer that uses handcrafted rules for Swedish, Danish and Norwegian as well one stemmer for Greek with CST's lemmatiser that uses training data to extract lemmatisation rules for Swedish, Danish, Norwegian and Greek. The performances of the two approaches are about the same with around 10 percent errors. The handcrafted rule based stemmer techniques are easy to get started with if the programmer has the proper linguistic knowledge. The machine trained sets of lemmatisation rules are very easy to produce without having linguistic knowledge given that one has correct training data.
We describe experiments in parsing the German TIGER Treebank. In parsing the complete treebank, 86.44{\%} of the sentences receive full parses; 13.56{\%} receive fragment parses. We discuss the methods used to enhance coverage and parsing quality and we present an evaluation on a gold standard, to our knowledge the first one for a deep grammar of German. Considering the selection performed by our current version of a stochastic disambiguation component, we achieve an f-score of 84.2{\%}, the upper and lower bounds being 87.4{\%} and 82.3{\%} respectively.
We present an alignment strategy that specifically deals with the correct alignment of rare German nominal compounds to their English multiword translations. It recognizes compounds and multiwords based on their character lengths and on their most frequent POS-patterns, and aligns them based on their length ratios. Our approach is designed on the basis of a data analysis on roughly 500 German hapax legomena, and as it does not use any frequency or co-occurrence information, it is well-suited to align rare compounds, but also achieves good results for more frequent expressions. Experiment results show that the strategy is able to correctly identify correct translations for 70{\%} of the compound hapaxes in our data set. Additionally, we checked on 700 randomly chosen entries in the dictionary that was automatically generated by our alignment tool. Results of this experiment also indicate that our strategy works for non-hapaxes as well, including finding multiple correct translations for the same head compound.
This paper describes the automatic extraction of French subcategorization frames from corpora. The subcategorization frames have been acquired via VISL, a dependency-based parser (Bick 2003), whose verb lexicon is currently incomplete with respect to subcategorization frames. Therefore, we have implemented binomial hypothesis testing as a post-parsing filtering step. On a test set of 104 frequent verbs we achieve lower bounds on type precision at 86.8{\%} and on token recall at 54.3{\%}. These results show that, contra (Korhonen et al. 2000), binomial hypothesis testing can be robust for determining subcategorization frames given corpus data. Additionally, we estimate that our extracted subcategorization frames account for 85.4{\%} of all frames in French corpora. We conclude that using a language resource, such as the VISL parser, with a currently unevaluated (and potentially high) error rate can yield robust results in conjunction with probabilistic filtering of the resource output.
This paper describes the design of speech act tags for spoken dialogue corpora and its evaluation. Compared with the tags used for conventional corpus annotation, the proposed speech intention tag is specialized enough to determine system operations. However, detailed information description increases tag types. This causes an ambiguous tag selection. Therefore, we have designed an organization of tags, with focusing attention on layered tagging and context-dependent tagging. Over 35,000 utterance units in the CIAIR corpus have been tagged by hand. To evaluate the reliability of the intention tag, a tagging experiment was conducted. The reliability of tagging is evaluated by comparing the tagging among some annotators using kappa value. As a result, we confirmed that reliable data could be built. This corpus with speech intention tag could be widely used from basic research to applications of spoken dialogue. In particular, this would play an important role from the viewpoint of practical use of spoken dialogue corpora.
The effectiveness of CCTV surveillance networks is in part determined by their ability to perceive possible threats. Our traditional means for determining a level of threat has been to manually observe a situation through the network and take action as appropriate. The increasing scale of such surveillance networks has however made such an approach untenable, leading us look for a means by which processes may be automated. Here we investigate the language used by security experts in an attempt to look for patterns in the way in which they describe events as observed through a CCTV camera. It is suggested that natural language based descriptions of events may provide the basis for an index which may prove an important component for future automated surveillance systems.
This paper presents the use of the {\^a}Jibiki{\^a} generic dictionary online development platform in the case of the GDEF Estonian-French bilingual dictionary building project. This platform has been developed mainly by Mathieu Mangeot and Gilles S{\~A}{\^A}{\copyright}rasset based on their research work in the domain. The platform is generic and thus can be used in (almost) any kind of dictionary development project from simple monolingual lexicons to complex multilingual pivot dictionaries as well as terminological resources. The platform is available online, thus it allows entry writers to work and collaborate from any part of the world. It consists in two main modules and data management tools. There is one module for elaborating complex queries on the data and one module for editing entries online. The editing modules generate automatically an interface from the XML structure of the entry.
Recently, monologue data such as lecture and commentary by professionals have been considered as valuable intellectual resources, and have been gathering attention. On the other hand, in order to use these monologue data effectively and efficiently, it is necessary for the monologue data not only just to be accumulated but also to be structured. This paper describes the construction of a Japanese spoken monologue corpus in which dependency structure is given to each utterance. Spontaneous monologue includes a lot of very long sentences composed of two or more clauses. In these sentences, there may exist the subject or the adverb common to multi-clauses, and it may be considered that the subject or adverb depend on multi-predicates. In order to give the dependency information in a real fashion, our research allows that a bunsetsu depends on multiple bunsetsus.
We present the efforts involved in designing SI-PRON, a comprehensive machine-readable pronunciation lexicon for Slovenian. It has been built from two sources and contains all the lemmas from the Dictionary of Standard Slovenian (SSKJ), the most frequent inflected word forms found in contemporary Slovenian texts, and a first pass of inflected word forms derived from SSKJ lemmas. The lexicon file contains the orthography, corresponding pronunciations, lemmas and morphosyntactic descriptors of lexical entries in a format based on requirements defined by the W3C Voice Browser Activity. The current version of the SI-PRON pronunciation lexicon contains over 1.4 million lexical entries. The word list determination procedure, the generation and validation of phonetic transcriptions, and the lexicon format are described in the paper. Along with Onomastica, SI-PRON presents a valuable language resource for linguistic studies and research of speech technologies for Slovenian. The lexicon is already being used by the AlpSynth Slovenian text-to-speech synthesis system and for generating audio samples of the SSKJ word list.
EngValLex is the name of an FGD-compliant valency lexicon of English verbs, built from the PropBank-Lexicon and following the structure of Vallex, the FGD-based lexicon of Czech verbs. EngValLex is interlinked with the PropBank-Lexicon, thus preserving the original links between the PropBank-Lexicon and the PropBank-Corpus. Therefore it is also supposed to be part of corpus annotation. This paper describes the automatic conversion of the PropBank-Lexicon into Pre-EngValLex, as well as the progress of its subsequent manual refinement (EngValLex). At the start, the Propbank-arguments were automatically re-labeled with functors (semantic labels of FGD) and the PropBank-rolesets were split into the respective example sentences, which became FGD-valency frames of Pre-EngValLex. Human annotators check and correct the labels and make the preliminary valency frames FGD-compliant. The most essential theoretical difference between the original and EngValLex is the syntactic alternations used by the PropBank-Lexicon, not yet employed within the Czech framework. The alternation-based approach substantially affects the conception of the frame, making in very different from the one applied within the FGD-framework. Preserving the valuable alternation information required special linguistic rules for keeping, altering and re-merging the automatically generated preliminary valency frames.
Statistical Language Models (LM) are highly dependent on their training resources. This makes it not only difficult to interpret evaluation results, it also has a deteriorating effect on the use of an LM-based application. This question has already been studied by others. Considering a specific domain (text prediction in a communication aid for handicapped people) we want to address the problem from a different point of view: the influence of the language register. Considering corpora from five different registers, we want to discuss three methods to adapt a language model to its actual language resource ultimately reducing the effect of training dependency: (a) A simple cache model augmenting the probability of the n last inserted words; (b) a user dictionary, keeping every unseen word; and (c) a combined LM interpolating a base model with a dynamically updated user model. Our evaluation is based on the results obtained from a text prediction system working on a trigram LM.
While both spoken and written language processing stand to benefit from parsing, the standard Parseval metrics (Black et al., 1991) and their canonical implementation (Sekine and Collins, 1997) are only useful for text. The Parseval metrics are undefined when the words input to the parser do not match the words in the gold standard parse tree exactly, and word errors are unavoidable with automatic speech recognition (ASR) systems. To fill this gap, we have developed a publicly available tool for scoring parses that implements a variety of metrics which can handle mismatches in words and segmentations, including: alignment-based bracket evaluation, alignment-based dependency evaluation, and a dependency evaluation that does not require alignment. We describe the different metrics, how to use the tool, and the outcome of an extensive set of experiments on the sensitivity.
This paper proposes a named entity (NE) ontology generation engine, called XNE-Tree engine, which produces relational named entities by given a seed. The engine incrementally extracts high co-occurring named entities with the seed by using a common search engine. In each iterative step, the seed will be replaced by its siblings or descendants, which form new seeds. In this way, XNE-Tree engine will build a tree structure with the original seed as a root incrementally. Two seeds, Chinese transliteration names of Nicole Kidman (a famous actress) and Ernest Hemingway (a famous writer), are experimented to evaluate the performance of the XNE-Tree.!`@!`@For test the applicability of the ontology, we employ it to a phoneme-character conversion system, which convert input phoneme syllable sequences to text strings. Total 100 Chinese transliteration names, including 50 person names and 50 location names are used as test data. We derive an ontology composed of 7,642 named entities. The results of phoneme-character conversion show that both the recall rate and the MRR are improved from 0.79 and 0.50 to 0.84 to 0.55, respectively.
This paper reports findings from the elaboration of a typology of spelling errors for Spanish. It also discusses previous generalizations about spelling error patterns found in other studies and offers new insights on them. The typology is based on the analysis of around 76K misspellings found in real-life texts produced by humans. The main goal of the elaboration of the typology was to help in the im-plementation of a spell checker that detects context-independent misspellings in general unrestricted texts with the most common con-fusion pairs (i.e. error/correction pairs) to improve the set of ranked correction candidates for misspellings. We found that spelling er-rors are language dependent and are closely related to the orthographic rules of each language. The statistical data we provide on spell-ing error patterns in Spanish and their comparison with other data in other related works are the novel contribution of this paper. In this line, this paper shows that some of the general statements found in the literature about spelling error patterns apply mainly to English and cannot be extrapolated to other languages.
Automatic markup and editing of anaphora and coreference is performed within one system. The processing is trained using memory based learning, and representations derive from various lexical resources. The current model reaches an expected combined precision and recall of F=62. The further improvement of the coreference detection is work in progress. Editing of coreference is separated into a module working on an xml-file. The editing mechanism can thus be reused in other projects. The editor is designed to store a copy on the server of all files that are edited over the internet using our demonstrator. This might help us to expand our database of texts annotated for anaphora and coreference. Further research includes creating high coverage lexical resources, and modules for other languages. The current system is trained on Norwegian bokm{\mbox{$^\circ$}}al, but we hope to extend this to other languages with available tools (e.g. POS-taggers).
The manual quantitative analysis of CIAIR simultaneous interpretation corpus and the collection of interpreting patterns This paper provides an investigation of simultaneous interpreting patterns using a bilingual spoken monologue corpus. 4,578 pairs of English-Japanese aligned utterances in CIAIR simultaneous interpretation database were used. This investigation is the largest scale as the observation of simultaneous interpreting speech. The simultaneous interpreters are required to generate the target speech simultaneously with the source speech. Therefore, they have various kinds of strategies to raise simultaneity. In this investigation, the simultaneous interpreting patterns with high frequency and high flexibility were extracted from the corpus. As a result, we collected 203 cases out of aligned utterances in which simultaneous interpretersf strategies for raising simultaneity were observed. These 203 cases could be categorized into 12 types of interpreting pattern. It was clarified that 4.5 percent of the English-Japanese monologue data were fitted in those interpreting patterns. These interpreting patterns can be expected to be used as interpreting rules of simultaneous machine interpretation.
The purpose of Oriental COCOSDA is to exchange ideas, to share information and to discuss regional matters on creation, utilization, dissemination of spoken language corpora of oriental languages and also on the assessment methods of speech recognition/synthesis systems as well as to promote speech research on oriental languages. A series of International Workshop on East Asian Language Resources and Evaluation (EALREW) or Oriental COCOSDA Workshop has been held annually since the preparatory meeting held in 1997. After that, we have had a series of workshops every year in Japan, Taiwan, China, Korea, Thailand, Singapore, India and Indonesia. The Oriental COCOSDA is managed by a convener, three advisory members, and 21 representatives from ten regions in Oriental countries. We need much more Pan-Asia collaboration with research organizations and consortia, though there are some domestic activities in Oriental countries. We note that speech research has become popular gradually in Oriental countries including Malaysia, Vietnam, Xinjang Uygur Autonomous Region of China, etc. We plan to hold future Oriental COCOSDA meetings in these places in order to promote speech research there.
We describe an approach to automatically detect and annotate definitions for technical terms in German text corpora. This approach focuses on verbs that typically appear in definitions (= definitor verbs). We specify search patterns based on the valency frames of these definitor verbs and use them (1) to detect and delimit text segments containing definitions and (2) to annotate their main functional components: the definiendum (the term that is defined) and the definiens (meaning postulates for this term). On the basis of these annotations we aim at automatically extracting WordNet-style semantic relations that hold between the head nouns of the definiendum and the head nouns of the definiens. In this paper, we will describe our annotation scheme for definitions and report on two studies: (1) a pilot study that evaluates our definition extraction approach using a German corpus with manually annotated definitions as a gold standard. (2) A feasibility study that evaluates the possibility to extract hypernym, hyponym and holonym relations from these annotated definitions.
The NEMLAR project: Network for Euro-Mediterranean LAnguage Resource and human language technology development and support (www.nemlar.org) was a project supported by the EC with partners from Europe and Arabic countries, whose objective is to build a network of specialized partners to promote and support the development of Arabic Language Resources (LRs) in the Mediterranean region. The project focused on identifying the state of the art of LRs in the region, assessing priority requirements through consultations with language industry and communication players, and establishing a protocol for developing and identifying a Basic Language Resource Kit (BLARK) for Arabic, and to assess first priority requirements. The BLARK is defined as the minimal set of language resources that is necessary to do any pre-competitive research and education, in addition to the development of crucial components for any future NLP industry. Following the identification of high priority resources the NEMLAR partners agreed to focus on, and produce three main resources, which are 1) Annotated Arabic written corpus of about 500 K words, 2) Arabic speech corpus for TTS applications of 2x5 hours, and 3) Arabic broadcast news speech corpus of 40 hours Modern Standard Arabic. For each of the resources underlying linguistic models and assumptions of the corpus, technical specifications, methodologies for the collection and building of the resources, validation and verification mechanisms were put and applied for the three LRs.
This paper presents a framework for Thai morphological analysis based on the theoretical background of conditional random fields. We formulate morphological analysis of an unsegmented language as the sequential supervised learning problem. Given a sequence of characters, all possibilities of word/tag segmentation are generated, and then the optimal path is selected with some criterion. We examine two different techniques, including the Viterbi score and the confidence estimation. Preliminary results are given to show the feasibility of our proposed framework.
This paper presents a corpus search system utilizing lexical dependency structure. The user's query consists of lexical dependency structure. The user's query consists of a sequence of keywords. For a given query, the system automatically generates the dependency structure patterns which consist of keywords in the query, and returns the sentences whose dependency structures match the generated patterns. The dependency structure patterns are generated by using two operations: combining and interpolation, which utilize dependency structures in the searched corpus. The operations enable the system to generate only the dependency structure patterns that occur in the corpus. The system achieves simple and intuitive corpus search and it is enough linguistically sophisticated to utilize structural information.
Manual annotation of various media streams, time series data and also text sequences is still a very time consuming work that has to be carried out in many areas of linguistics and beyond. Based on many theoretical discussions and practical experiences professional tools have been deployed such as ELAN that support the researcher in his/her work. Most of these annotation tools operate on local computers. However, since more and more language resources are stored in web-accessible archives, researchers want to take profit from the new possibilities. ANNEX was developed to fill this gap, since it allows web-based analysis of complex annotated media streams, i.e., the users dont have to download resources and dont have to download and install programs. By simply using a normal web-browser they can start their linguistic work. Yet, due to the architecture of the Internet, ANNEX does not offer the options to create annotations, but this feature will come. However, users have to be aware of the fact that media streaming does not offer that high accuracy as on local computers.
The paper presents the SVEZ-IJS corpus, a large parallel annotated English-Slovene corpus containing translated legal texts of the European Union, the ACQUIS Communautaire. The corpus contains approx. 2 x 5 million words and was compiled from the translation memory obtained from the Translation Unit of the Slovene Government Office for European Affairs. The corpus is encoded in XML, accordingto the Text Encoding Initiative Guidelines TEI P4, where each translation memory unit contains useful metadata and the two aligned segments (sentences). Both the Slovene and English text islinguistically annotated at the word-level, by context disambiguatedlemmas and morphosyntactic descriptions, which follow the MULTEXTguidelines. The complete corpus is freely available for research, either via an on-line concordancer, or for downloading from the corpushome page at http://nl.ijs.si/svez/.
Language Archiving, Resource Management LAMUS is a web-based service that allows researchers to deposit their language resources into a language resources archive. It was developed at the MPI for Psycholinguistics for stricter control of the archive coherence and consistency and allowing wider use of the archiving facilities without increasing the workload for archive and corpus managers. LAMUS is based on the use of IMDI metadata standard for language resources and offers metadata search and browsing over the archive.
The DAM-LR project aims at virtually integrating various European language resource archives that allow users to navigate and operate in a single unified domain of language resources. This type of integration introduces Grid technology to the humanities disciplines and forms a federation of archives. It is the basis for establishing a research infrastructure for language resources which will finally enable eHumanities. Currently, the complete architecture is designed based on a few well-known components and some components are already tested. Based on the technological insights gathered and due to discussions within the international DELAMAN network the ethical and organizational basis for such a federation is defined.
Central Ontologies are increasingly important to manage interoperability between different types of language resources. This was the reason for ISO to set up a new committee ISO TC37/SC4 taking care of language resource management issues. Central to the work of this committee is the definition of a framework for a central registry of data categories that are important in the domain of language resources. This paper describes an application programming interface that was designed to request services from this data category registry. The DCR is operational and the described API has already been tested from a lexicon application.
A number of serious reasons will convince an increasing amount of researchers to store their relevant material in centers which we will call ``language resource archives''. They combine the duty of taking care of long-term preservation as well as the task to give access to their material to different user groups. Access here is meant in the sense that an active interaction with the data will be made possible to support the integration of new data, new versions or commentaries of all sorts. Modern Language Resource Archives will have to adhere to a number of basic principles to fulfill all requirements and they will have to be involved in federations to create joint language resource domains making it even simpler for the researchers to access the data. This paper makes an attempt to formulate the essential pillars language resource archives have to adhere to.
Metadata descriptions of language resources become an increasing necessity since the shear amount of language resources is increasing rapidly and especially since we are now creating infrastuctures to access these resources via the web through integrated domains of language resource archives. Yet, the metadata frameworks offered for the domain of language resources (IMDI and OLAC), although mature, are not as widely accepted as necessary. The lack of confidence in the stability and persistence of the concepts and formats introduced by these metadata sets seems to be one argument for people to not invest the time needed for metadata creation. The introduction of these concepts into an ISO standardization process may convince contributors to make use of the terminology. The availability of the ISO Data Category Registry that includes a metadata profile will also offer the opportunity for researchers to construct their own metadata set tailored to the needs of the project at hand, but nevertheless supporting interoperability.
LEXUS provides a flexible framework for the maintaining lexical structure and content. It is the first implementation of the Lexical Markup Framework model currently being developed at ISO TC37/SC4. Amongst its capabilities are the possibility to create lexicon structures, manipulate content and use of typed relations. Integration of well established Data Category Registries is supported to further promote interoperability by allowing access to well established linguistic concepts. Advanced linguistic functionality is offered to assist users in cross lexica operations such as search and comparison and merging of lexica. To enable use within various user groups the look and feel of each lexicon may be customized. In the near future more functionality will be added including integration with other tools accessing lexical content.
A WordNet is a lexical database in which nouns, verbs, adjectives and adverbs are organized in a conceptual hierarchy, linking semantically and lexically related concepts. Such semantic lexicons have become oneof the most valuable resources for a wide range of NLP research and applications, such as semantic tagging, automatic word-sense disambiguation, information retrieval and document summarisation. Following the WordNet design for the English languagedeveloped at Princeton, WordNets for a number of other languages havebeen developed in the past decade, taking the idea into the domain ofmultilingual processing. This paper reports on the prototype SloveneWordNet which currently contains about 5,000 top-level concepts. Theresource has been automatically translated from the Serbian WordNet, with the help of a bilingual dictionary, synset literals ranked according to the frequency of corpus occurrence, and results manually corrected. The paper presents the results obtained, discusses some problems encountered along the way and points out some possibilitiesof automated acquisition and refinement of synsets in the future.
To meet a variety of needs in information modeling, software development and integration as well as knowledge management and reuse, various groups within industry, academia, and government have been developing and deploying sharable and reusable models known as ontologies. Ontologies play an important role in knowledge representation. In this paper, we address the problem of capturing knowledge needed for indexing and retrieving art resources. We describe a case study in which we attempt to construct an ontology for a subset of art. The aim of the present ontology is to build an extensible repository of knowledge and information about artists, their works and materials used in artistic creations. Influenced by the recent interest in colours and colouring materials, mainly shared by French researchers and linguists, an ontology prototype has been developed using Prot{\'e}g{\'e}. It allows to organize and catalog information about artists, art works, colouring materials and related colours.
We describe the implementation of a FrameNet-based semantic role labeling system for Swedish text. To train the system, we used a semantically annotated corpus that was produced by projection across parallel corpora. As part of the system, we developed two frame element bracketing algorithms that are suitable when no robust constituent parsers are available. Apart from being the first such system for Swedish, this is, as far as we are aware, the first semantic role labeling system for a language for which no role-semantic annotated corpora are available. The estimated accuracy of classification of pre-segmented frame elements is 0.75, and the precision and recall measures for the complete task are 0.67 and 0.47, respectively.
Utilization of computer tools in linguistic research has gained importance with the maturation of media frameworks for the handling of digital audio and video. The increased use of these tools in gesture, sign language and multimodal interaction studies has led to stronger requirements on the flexibility, the efficiency and in particular the time accuracy of annotation tools. This paper describes the efforts made to make ELAN a tool that meets these requirements, with special attention to the developments in the area of time accuracy. In subsequent sections an overview will be given of other enhancements in the latest versions of ELAN that makes it a useful tool in multimodality research.
At the MPI for Psycholinguistics a large archive with language resources has been created with contributions from many different individual researchers and research projects. All of these resources, in particular annotated media streams and multimedia lexica, are accessible via the web and can be utilized with the help of web-based utilization frameworks. Therefore, the archive lends itself to motivate users to operate across the boundaries of single corpora and to support cross-language work. This, however, can only be done when the problems of interoperability, in particular at the level of linguistic encoding, can be solved in an efficient way. Two Max-Planck-Institutes are cooperating to build a framework that allows users to easily create their own practical ontologies and if wanted to relate their concepts to central ontologies.
We introduce MaltParser, a data-driven parser generator for dependency parsing. Given a treebank in dependency format, MaltParser can be used to induce a parser for the language of the treebank. MaltParser supports several parsing algorithms and learning algorithms, and allows user-defined feature models, consisting of arbitrary combinations of lexical features, part-of-speech features and dependency features. MaltParser is freely available for research and educational purposes and has been evaluated empirically on Swedish, English, Czech, Danish and Bulgarian.
This paper presents the SINOD database, which is the first Slovenian non-native speech database. It will be used to improve the performance of large vocabulary continuous speech recogniser for non-native speakers. The main quality impact is expected for acoustic models and recognisers vocabulary. The SINOD database is designed as supplement to the Slovenian BNSI Broadcast News database. The same BN recommendations were used for both databases. Two interviews with non-native Slovenian speakers were incorporated in the set. Both non-native speakers were female, whereas the journalist was Slovenian native male speaker. The transcription approach applied in the production phase is presented. Different statistics and analyses of database are given in the paper.
This paper presents an overview of the work in progress at the W3C to produce a conversion of WordNet to the RDF/OWL representation language in use in the Semantic Web community. Such a standard representation is useful to provide application developers a high-quality resource and to promote interoperability. Important requirements in this conversion process are that it should be complete and should stay close to WordNet's conceptual model. The paper explains the steps taken to produce the conversion and details design decisions such as the composition of the class hierarchy and properties, the addition of suitable OWL semantics and the chosen format of the URIs. Additional topics include a strategy to incorporate OWL and RDFS semantics in one schema such that both RDF(S) infrastructure and OWL infrastructure can interpret the information correctly, problems encountered in understanding the Prolog source files and the description of the two versions that are provided (Basic and Full) to accommodate different usages of WordNet.
We describe a case study in the reuse and transfer of tools in language resource development, from a corpus of spoken Dutch to a corpus of written Dutch. Once tools for a particular language have been developed, it is logical, but not trivial to reuse them for other types or registers of the language than the tools were originally designed for. This paper reviews the decisions and adaptations necessary to make this particular transfer from spoken to written language, focusing on a part-of-speech tagger and a lemmatizer. While the lemmatizer can be transferred fairly straightforwardly, the tagger needs to be adaptated considerably. We show how it can be adapted without starting from scratch. We describe how the part-of-speech tagset was adapted and how the tagger was retrained to deal with written-text phenomena it had not been trained on earlier.
NIST has coordinated machine translation (MT) evaluations for several years using an automatic and repeatable evaluation measure. Under the Global Autonomous Language Exploitation (GALE) program, NIST is tasked with implementing an edit-distance-based evaluation of MT. Here edit distance is defined to be the number of modifications a human editor is required to make to a system translation such that the resulting edited translation contains the complete meaning in easily understandable English, as a single high-quality human reference translation. In preparation for this change in evaluation paradigm, NIST conducted two proof-of-concept exercises specifically designed to probe the data space, to answer questions related to editor agreement, and to establish protocols for the formal GALE evaluations. We report here our experimental design, the data used, and our findings for these exercises.
This paper describes the design, collection and current status of the Hans Christian Andersen (HCA) conversation corpus. The corpus consists of five separate corpora and represents transcription and annotation of some 57 hours of English spoken and deictic gesture user-system interaction recorded mainly with children 2002-2005. The corpora were collected as part of the development and evaluation process of two consecutive research prototypes. The set-up used to collect each corpus is described as well as our use of each corpus in system development. We describe the annotation of each corpus and briefly present various uses we have made of the corpora so far. The HCA corpus was made publicly available at http://www.niceproject.com/data/ in March 2006.
This paper discusses evaluation methodologies for a particular kind of meaning models known as word-space models, which use distributional information to assemble geometric representations of meaning similarities. Word-space models have received considerable attention in recent years, and have begun to see employment outside the walls of computational linguistics laboratories. However, the evaluation methodologies of such models remain infantile, and lack efforts at standardization. Very few studies have critically assessed the methodologies used to evaluate word spaces. This paper attempts to fill some of this void. It is the central goal of this paper to answer the question how can we determine whether a given word space is a good word space?
In this paper, we propose a formal framework that takes into account the influence of the intended context of use of an NLP system on the procedure and the metrics used to evaluate the system. We introduce in particular the notion of a context-dependent quality model and explain how it can be adapted to a given context of use. More specifically, we define vector-space representations of contexts of use and of quality models, which are connected by a generic contextual quality model (GCQM). For each domain, experts in evaluation are needed to build a GCQM based on analytic knowledge and on previous evaluations, using the mechanism proposed here. The main inspiration source for this work is the FEMTI framework for the evaluation of machine translation, which implements partly the present model, and which is described briefly along with insights from other domains.
We present a non-deterministic finite-state transducer that acts as a tokenizer and normalizer for free text that is input to a broad-coverage LFG of German. We compare the basic tokenizer used in an earlier version of the grammar and the more sophisticated tokenizer that we now use. The revised tokenizer increases the coverage of the grammar in terms of full parses from 68.3{\%} to 73.4{\%} on sentences 8,001 through 10,000 of the TiGer Corpus.
The performance of three different taggers (Treetagger, Freeling and GRAMPAL) is evaluated on three different languages, i.e. English, Italian and Spanish. The materials are transcripts from the European Parliament Interpreting Corpus (EPIC), a corpus of original (source) and simultaneously interpreted (target) speeches. Owing to the oral nature of our materials and to the specific characteristics of spoken language produced in simultaneous interpreting, the chosen taggers have to deal with non-standard word order, disfluencies and other features not to be found in written language. Parts of the tagged sub-corpora were automatically extracted in order to assess the success rate achieved in tagging and lemmatisation. Errors and problems are discussed for each tagger, and conclusions are drawn regarding future developments.
We present RefRef, a tool for viewing and exploring coreference space, which is publicly available for research purposes. Unlike similar tools currently available whose main goal is to assist the annotation process of coreference links, RefRef is dedicated for viewing and exploring coreference-annotated data, whether manually tagged or automatically resolved. RefRef is also highly customizable, as the tool is being made available with the source code. In this paper we describe the main functionalities of RefRef as well as some possibilities for customization to meet the specific needs of the users of such coreference-annotated text.
In-car automatic speech recognition (ASR) is usually evaluated behaviour for different levels of noise. Yet this is interesting for car manufacturers in order to predict system performances for different speeds and different car models and thus allow to design speech based applications in a better way. It therefore makes sense to split the single WER into SNR dependent WERs, where SNR stands for the signal to noise ratio, which is an appropriate measure for the noise level. In this paper a SNR measure based on the concept of the Articulation Index is developed, which allows the direct comparison with human recognition performance.
As part of a project to construct an interactive program which will encourage children to play with language by building jokes, we have developed a large lexical database, closely based on WordNet. As well as the standard WordNet information about part of speech, synonymy, hyponymy, etc, we have added phonetic representations and symbolic links allowing attachment of pictures. All information is represented in a relational database, allowing powerful searches using SQL via a Java API. The lexicon has a facility to label subsets of the lexicon with symbolic names, and we are working to incorporate some educationally relevant word lists as sublexicons. This should also allow us to improve the familiarity ratings which the lexicon assigns to words.
In this article, we present an experiment that aims to evaluate the feasibility of a superficial morphological analysis, to analyse unknown constructed neologisms. For any morphosyntactic analyser, lexical incompleteness is a real problem. This lack of information is partly due to lexical creativity, and more especially to the productivity of some morphological processes. We present here a set of word formation rules based on constructional morphology principles that can be used to improve the performance of an Italian morphosyntactic analyser. These rules use only simple computing techniques in order to ensure efficiency because any improvements in coverage must not slow down the entire system. In the second part of this paper, we describe a method for constraining the rules, and an evaluation of these constraints in terms of performance. Great improvements are achieved in reducing the number of incorrect analyses of unknown neologisms (noise), although this is at the cost of some increase in silence (correct analyses which are no longer produced). This classic trade-off between noise and silence, however, can hardly be avoided and we believe that this experiment successfully demonstrates the feasibility of superficial analysis in improving performance and points the way to other avenues of research.
In the paper we address two practical problems concerning the use of corpora in translation studies. The first stems from the limited resources available for targeted languages and genres within languages, whereas translation researchers and students need: sufficiently large modern corpora, either reflecting general language or specific to a problem domain. The second problem concerns the lackof a uniform interface for accessing the resources, even when the yexist. We deal with the first problem by developing a framework for semi-automatic acquisition of large corpora from the Internet for the languages relevant for our research and training needs. We outline the methodology used and discuss the composition of Internet-derived corpora. We deal with the second problem by developing a uniform interface to our corpora. In addition to standard options for choosingcorpora and sorting concordance lines, the interface can compute the list of collocations and filter the results according touser-specified patterns in order to detect language-specific syntacticstructures.
This paper presents resources and functionalities for the recognition and selection of affective evaluative terms. An affective hierarchy as an extension of the WordNet-Affect lexical database was developed in the first place. The second phase was the development of a semantic similarity function, acquired automatically in an unsupervised way from a large corpus of texts, which allows us to put into relation concepts and emotional categories. The integration of the two components is a key element for several applications.
This paper describes the Relative Ordering Tool for Evaluation (ROTE) which is designed to support the process of building a parameterised quality model for evaluation. It is a very simple tool which enables users to specify the relative importance of quality characteristics (and associated metrics) to reflect the users' particular requirements. The tool allows users to order any number of quality characteristics by comparing them in a pair-wise fashion. The tool was developed in the context of a collaborative project developing a text mining system. A full scale evaluation of the text mining system was designed and executed for three different users and the ROTE tool was successfully applied by those users during that process. The tool will be made available for general use by the evaluation community.
In this paper we present a tool for finding appropriate translation equivalents for words from the general lexicon using comparable corpora. For a phrase in the source language the tool suggests arange of possible expressions used in similar contexts in target language corpora. In the paper we discuss the method and present results of human evaluation of the performance of the tool.
The traditional dialect vocabulary of the Netherlands and Flanders is recorded and researched in several Dutch and Belgian research institutes and universities. Most of these distributed dictionary creation and research projects collaborate in the Permanent Overlegorgaan Regionale Woordenboeken (ReWo). In the project digital databases and digital tools for WBD and WLD (D-square) the dialect data published by two of these dictionary projects (Woordenboek van de Brabantse Dialecten and Woordenboek van de Limburgse Dialecten) is being digitised. One of the additional goals of the D-square project is the development of an infrastructure for electronic access to all dialect dictionaries collaborating in the ReWo. In this paper we will firstly reconsider the nature of the core data types - form, sense and location - present in the different dialect dictionaries and the ways these data types are further classified. Next we will focus on the problems encountered when trying to unify this dictionary data and their classifications and suggest solutions. Finally we will look at several implementation issues regarding a specific encoding for the dictionaries.
This paper describes the methodology used to develop a part-of-speech tagger for Irish, which is used to annotate a corpus of 30 million words of text with part-of-speech tags and lemmas. The tagger is evaluated using a manually disambiguated test corpus and it currently achieves 95{\%} accuracy on unrestricted text. To our knowledge, this is the first part-of-speech tagger for Irish.
Search engines on the web and most existing question-answering systems provide the user with a set of hyperlinks and/or web page extracts containing answer(s) to a question. These answers are often incoherent to a certain degree (equivalent, contradictory, etc.). It is then quite difficult for the user to know which answer is the correct one. In this paper, we present an approach which aims at providing synthetic numerical answers in a question-answering system. These answers are generated in natural language and, in a cooperative perspective, the aim is to explain to the user the variation of numerical values when several values, apparently incoherent, are extracted from the web as possible answers to a question. We present in particular how lexical resources are essential to answer extraction from the web, to the characterization of the variation mode associated with the type of information and to answer generation in natural language.
In this paper we introduce a public resource named BACO (Base de Co-Ocorr{\^e}ncias), a very large textual database built from the WPT03 collection, a publicly available crawl of the whole Portuguese web in 2003. BACO uses a generic relational database engine to store 1.5 million web documents in raw text (more than 6GB of plain text), corresponding to 35 million sentences, consisting of more than 1000 million words. BACO comprises four lexicon tables, including a standard single token lexicon, and three n-gram tables (2-grams, 3-grams and 4-grams) with several hundred million entries, and a table containing 780 million co-occurrence pairs. We describe the design choices and explain the preparation tasks involved in loading the data in the relational database. We present several statistics regarding storage requirements and we demonstrate how this resource is currently used.
Semantic Web and NLP We describe an implemented offline procedure that maps OWL/RDF-encoded ontologies with large, dynamically maintained instance data to named entity recognition (NER) and information extraction (IE) engine resources, preserving hierarchical concept information and links back to the ontology concepts and instances. The main motivations are (i) improving NER/IE precision and recall in closed domains, (ii) exploiting linguistic knowledge (context, inflection, anaphora) for identifying ontology instances in texts more robustly, (iii) giving full access to ontology instances and concepts in natural language processing results, e.g. for subsequent ontology queries, navigation or inference, (iv) avoiding duplication of work in development and maintenance of similar resources in independent places, namely lingware and ontologies. We show an application in hybrid deep-shallow natural language processing that is e.g. used for question analysis in closed domains. Further applications could be automatic hyperlinking or other innovative semantic-web related applications.
Since 1987, the National Institute of Standards and Technology has been providing evaluation infrastructure for the Automatic Speech Recognition (ASR), and more recently referred to as the Speech-To-Text (STT), research community. From the first efforts in the Resource Management domain to the present research, the NIST SCoring ToolKit (SCTK) has formed the tool set for system developers to make continued progress in many domains; Wall Street Journal, Conversational Telephone Speech (CTS), Broadcast News (BN), and Meetings (MTG) to name a few. For these domains, the community agreed to declared sections of simultaneous speech as not scoreable. While this had minor impact on most of these domains, the highly interactive nature of Meeting speech rendered a very large fraction of the test material not scoreable. This paper documents a multi-dimensional extension of the Dynamic Programming solution to Levenshtein Edit Distance calculations capable of evaluating STT systems during periods of overlapping, simultaneous speech.
This paper describes version 1.3 of the FreeLing suite of NLP tools. FreeLing was first released in February 2004 providing morphological analysis and PoS tagging for Catalan, Spanish, and English. From then on, the package has been improved and enlarged to cover more languages (i.e. Italian and Galician) and offer more services: Named entity recognition and classification, chunking, dependency parsing, and WordNet based semantic annotation. FreeLing is not conceived as end-user oriented tool, but as library on top of which powerful NLP applications can be developed. Nevertheless, sample interface programs are provided, which can be straightforwardly used as fast, flexible, and efficient corpus processing tools. A remarkable feature of FreeLing is that it is distributed under a free-software LGPL license, thus enabling any developer to adapt the package to his needs in order to get the most suitable behaviour for the application being developed.
Many recent NLP applications, including machine translation and information retrieval, could benefit from semantic analysis of language data on the sentence level. This paper presents a method for automatic disambiguation of verb valency frames on Czech data. For each verb occurrence, we extracted features describing its local context. We experimented with diverse types of features, including morphological, syntax-based, idiomatic, animacy and WordNet-based features. The main contribution of the paper lies in determining which ones are most useful for the disambiguation task. The considered features were classified using decision trees, rule-based learning and a Na{\"\i
We introduce the problem of automatic textual anonymisation and present a new publicly-available, pseudonymised benchmark corpus of personal email text for the task, dubbed ITAC (Informal Text Anonymisation Corpus). We discuss the method by which the corpus was constructed, and consider some important issues related to the evaluation of textual anonymisation systems. We also present some initial baseline results on the new corpus using a state of the art HMM-based tagger. We introduce the problem of automatic textual anonymisation and present a new publicly-available, pseudonymised benchmark corpus of personal email text for the task, dubbed ITAC (Informal Text Anonymisation Corpus). We discuss the method by which the corpus was constructed, and consider some important issues related to the evaluation of textual anonymisation systems. We also present some initial baseline results on the new corpus using a state of the art HMM-based tagger.
Documentation and retrieval processes at the Netherlands Institute for Sound and Vision are organized around a common thesaurus. To help improve the quality of these processes the thesaurus was transformed into a RDF/OWL ontology and extended on basis of implicit information and external resources. A thesaurus browser web application was designed, implemented and tested on future users.
The main goal of this paper is to present a first approach to an automatic detection of conceptual relations between two terms in specialised written text. Previous experiments on the basis of the manual analysis lead the authors to implement an automatic query strategy combining the term candidates proposed by an extractor together with a list of verbal syntactic patterns used for the relations refinement. Next step on the research will be the integration of the results into the term extractor in order to attain more restrictive pieces of information directly reused for the ontology building task.
The growing interest in open-domain question answering is limited by the lack of evaluation and training resources. To overcome this resource bottleneck for German, we propose a novel methodology to acquire new question-answer pairs for system evaluation that relies on volunteer collaboration over the Internet. Utilizing Wikipedia, a popular free online encyclopedia available in several languages, we show that the data acquisition problem can be cast as a Web experiment. We present a Web-based annotation tool and carry out a distributed data collection experiment. The data gathered from the mostly anonymous contributors is compared to a similar dataset produced in-house by domain experts on the one hand, and the German questions from the from the CLEF QA 2004 effort on the other hand. Our analysis of the datasets suggests that using our novel method a medium-scale evaluation resource can be built at very small cost in a short period of time. The technique and software developed here is readily applicable to other languages where free online encyclopedias are available, and our resulting corpus is likewise freely available.
Interactive question answering systems should allow users to lead a coherent information seeking dialogue. Compared with systems that only locally evaluate a question, interactive systems facilitate the information seeking process and provide a more natural feel. We show that by extending a QA system to handle several types of anaphora and ellipsis, the naturalness of the interaction can be considerably improved. We describe an implementation in our prototype QA system for German and give a walk-through example of the enhanced interaction capabilities.
We discuss preprocessing and tokenisation standards within DELPH-IN, a large scale open-source collaboration providing multiple independent multilingual shallow and deep processors. We discuss (i) a component-specific XML interface format which has been used for some time to interface preprocessor results to the PET parser, and (ii) our implementation of a more generic XML interface format influenced heavily by the (ISO working draft) Morphosyntactic Annotation Framework (MAF). Our generic format encapsulates the information which may be passed from the preprocessing stage to a parser: it uses standoff-annotation, a lattice for the representation of structural ambiguity, intra-annotation dependencies and allows for highly structured annotation content. This work builds on the existing Heart of Gold middleware system, and previous work on Robust Minimal Recursion Semantics (RMRS) as part of an inter-component interface. We give examples of usage with a number of the DELPH-IN processing components and deep grammars.
We describe a project aimed at creating a deeply annotated corpus of Russian texts. The annotation consists of comprehensive morphological marking, syntactic tagging in the form of a complete dependency tree, and semantic tagging within a restricted semantic dictionary. Syntactic tagging is using about 80 dependency relations. The syntactically annotated corpus counts more than 28,000 sentences and makes an autonomous part of the Russian National Corpus (www.ruscorpora.ru). Semantic tagging is based on an inventory of semantic features (descriptors) and a dictionary comprising about 3,000 entries, with a set of tags assigned to each lexeme and its argument slots. The set of descriptors assigned to words has been designed in such a way as to construct a linguistically relevant classification for the whole Russian vocabulary. This classification serves for discovering laws according to which the elements of various lexical and semantic classes interact in the texts. The inventory of semantic descriptors consists of two parts, object descriptors (about 90 items in total) and predicate descriptors (about a hundred). A set of semantic roles is thoroughly elaborated and contains about 50 roles.
In this paper we present a novel method for automatic text summarization through text extraction, using computational semantics. The new idea is to view all the extracted text as a whole and compute a score for the total impact of the summary, instead of ranking for instance individual sentences. A greedy search strategy is used to search through the space of possible summaries and select the summary with the highest score of those found. The aim has been to construct a summarizer that can be quickly assembled, with the use of only a very few basic language tools, for languages that lack large amounts of structured or annotated data or advanced tools for linguistic processing. The proposed method is largely language independent, though we only evaluate it on English in this paper, using ROUGE-scores on texts from among others the DUC 2004 task 2. On this task our method performs better than several of the systems evaluated there, but worse than the best systems.
This paper presents a corpus of non-native speech that contains pronunciation variants of European city names from fivecountries spoken by speakers of four native languages. It was originally designed as a research tool for the study ofpronunciation errors by non-native speakers in the pronunciation of foreign city names. The corpus has now been released. Followinga brief sketch of the research context in which this data collection was established, the first part of this paper describes the contents and technical specifications of the corpus (design, speakers, language material, recording conditions).Compared to corpora of native speech, non-native speech compilations raise a number of additional difficulties that requirespecific attention and methodology. Therefore, the second part of the paper aims to point out some of these general issuesfrom the perspective of the experience gained in our research. Strategies to deal with these difficulties will be exploredalong with their specific benefits and shortfalls, concluding that non-native speech corpora require a number of specificdesign guidelines which are often difficult to put into practice.
This paper addresses the task of recognizing acronym-definition pairs in Swedish (medical) texts as well as the compilation of a freely available sample of such manually annotated pairs. A material suitable not only for supervised learning experiments, but also as a testbed for the evaluation of the quality of future acronym-definition recognition systems. There are a number of approaches to the identification described in the literature, particularly within the biomedical domain, but none of those addresses the variation and complexity exhibited in a language other than English. This is realized by the fact that we can have a mixture of two languages in the same document and/or sentence, i.e. Swedish and English; that Swedish is a compound language that significantly deteriorates the performance of previous approaches (without adaptations) and, most importantly, the fact that there is a large variation of possible acronym-definition permutations realized in the analysed corpora, a variation that is usually ignored in previous studies.
Opinion retrieval aims to tell if a document is positive, neutral or negative on a given topic. Opinion extraction further identifies the supportive and the non-supportive evidence of a document. To evaluate the performance of technologies for opinionated tasks, a suitable corpus is necessary. This paper defines the annotations for opinionated materials. Heterogeneous experimental materials are annotated, and the agreements among annotators are analyzed. How human can monitor opinions of the whole is also examined. The corpus can be employed to opinion extraction, opinion summarization, opinion tracking and opinionated question answering.
We introduce Talbanken05, a Swedish treebank based on a syntactically annotated corpus from the 1970s, Talbanken76, converted to modern formats. The treebank is available in three different formats, besides the original one: two versions of phrase structure annotation and one dependency-based annotation, all of which are encoded in XML. In this paper, we describe the conversion process and exemplify the available formats. The treebank is freely available for research and educational purposes.
This paper investigates the problem of automatically annotating resources with NP coreference information using a parallel corpus, English-Romanian, in order to transfer, through word alignment, coreference chains from the English part to the Romanian part of the corpus. The results show that we can detect Romanian referential expressions and coreference chains with over 80{\%} F-measure, thus using our method as a preprocessing step followed by manual correction as part of an annotation effort for creating a large Romanian corpus with coreference information is worthwhile.
In this paper we perform a preliminary evaluation on how Semantic Web technologies such as RDF and OWL can be used to perform textual encoding. Among the potential advantages, we notice how RDF, given its conceptual graph structure, appears naturally suited to deal with overlapping hierarchies of annotations, something notoriously problematic using classic XML based markup. To conclude, we show how complex querying can be performed using slight modifications of already existing Semantic Web query tools.
This paper offers a comparison of two resources for Polish adult learners of English. The first has been designed for Polish-English Literacy Tutor (PELT), a multimodal system for foreign language learning, as training input to speech recognition system for highly accented, strongly variable second language speech. The second corpus is a task-specific resource designed in the PELT framework to investigate the vowel space of English produced by Poles. Presented are linguistically and technologically challenging aspects of the two ventures and their complementary character.
We describe an experiment on collecting large language and topic specific corpora automatically by using a focused Web crawler. Our crawler combines efficient crawling techniques with a common text classification tool. Given a sample corpus of medical documents, we automatically extract query phrases and then acquire seed URLs with a standard search engine. Starting from these seed URLs, the crawler builds a new large collection consisting only of documents that satisfy both the language and the topic model. The manual analysis of acquired English and German medicine corpora reveals the high accuracy of the crawler. However, there are significant differences between both languages.
We explore the feasibility of using only unsupervised means to identify non-words, i.e. typos, in a frequency list derived from a large corpus of Dutch and to distinguish between these non-words and real-words in the language. We call the system we built and evaluate in this paper ciccl, which stands for Corpus-Induced Corpus Clean-up. The algorithm on which ciccl is primarily based is the anagram-key hashing algorithm introduced by (Reynaert, 2004). The core correction mechanism is a simple and effective method which translates the actual characters which make up a word into a large natural number in such a way that all the anagrams, i.e. all the words composed of precisely the same subset of characters, are allocated the same natural number. In effect, this constitutes a novel approximate string matching algorithm for indexed text search. This is because by simple addition, subtraction or a combination of both, all variants within reach of the range of numerical values defined in the alphabet are retrieved by iterating over the alphabet. ciccl's input consists primarily of corpus derived frequency lists, from which it derives valuable morphological information by performing frequency counts over the substrings of the words, which are then used to perform decompounding, as well as for distinguishing between most likely correctly spelled words and typos.
This article describes an interface for searching and browsing multimodal recordings of group meetings. We provide first an overall perspective of meeting processing and retrieval applications, and distinguish between the media/modalities that are recorded and the ones that are used for browsing. We then proceed to describe the data and the annotations that are stored in a meeting database. Two scenarios of use for the transcript-based query and browsing interface (TQB) are then outlined: search and browse vs. overview and browse. The main functionalities of TQB, namely the database backend and the multimedia rendering solutions are described. An outline of evaluation perspectives is finally provided, with a description of the user interaction features that will be monitored.
In this paper, we present our work on generating an annotated corpus for extracting information about the typical durations of events from texts. We include the annotation guidelines, the event classes we categorized, the way we use normal distributions to model vague and implicit temporal information, and how we evaluate inter-annotator agreement. The experimental results show that our guidelines are effective in improving the inter-annotator agreement.
We consider the problem of identifying automatic translations from manual translations of the same sentence. Using two different similarity metrics (BLEU and Levenshtein edit distance), we found out that automatic translations are closer to each other than they are to manual translations. We also use phylogenetic trees to provide a visual representation of the distances between pairs of individual sentences in a set of translations. The differences in lexical distance are statistically significant, both for Chinese to English and for Arabic to English translations.
At ATR, we are collecting and analysing meetings data using a table-top sensor device consisting of a small 360-degree camera surrounded by an array of high-quality directional microphones. This equipment provides a stream of information about the audio and visual events of the meeting which is then processed to form a representation of the verbal and non-verbal interpersonal activity, or discourse flow, during the meeting. This paper describes the resulting corpus of speech and video data which is being collected for the abovere search. It currently includes data from 12 monthly sessions, comprising 71 video and 33 audio modules. Collection is continuingmonthly and is scheduled to include another ten sessions.
As Chinese is an ideographic character-based language, the words in the texts are not delimited by spaces. Indexing of Chinese documents is impossible without a proper segmentation algorithm. Many Chinese segmentation algorithms have been proposed in the past. Traditional segmentation algorithms cannot operate without a large dictionary or a large corpus of training data. Nowadays, the Web has become the largest corpus that is ideal for Chinese segmentation. Although the search engines do not segment texts into proper words, they maintain huge databases of documents and frequencies of character sequences in the documents. Their databases are important potential resources for segmentation. In this paper, we propose a segmentation algorithm by mining web data with the help from search engines. It is the first unified segmentation algorithm for Chinese language from different geographical areas. Experiments have been conducted on the datasets of a recent Chinese segmentation competition. The results show that our algorithm outperforms the traditional algorithms in terms of precision and recall. Moreover, our algorithm can effectively deal with the problem of segmentation ambiguity, new word (unknown word) detection, and stop words.
This paper presents the results (1st phase) of the on-going research in the Computational Linguistics Laboratory at Aut{\'o}noma University of Madrid (LLI-UAM) aiming at the development of a multi-lingual parallel corpus (Arabic-Spanish-English) aligned on the sentence level and tagged on the POS level. A multilingual parallel corpus which brings together Arabic, Spanish and English is a new resource for the NLP community that completes the present panorama of parallel corpora. In the first part of this study, we introduce the novelty of our approach and the challenges encountered to create such a corpus. This introductory part highlights the main features of the corpus and the criteria applied during the selection process. The second part focuses on two main stages: basic processing (tokenization and segmentation) and alignment. Methodology of alignment is explained in detail and results obtained in the three different linguistic pairs are compared. POS tagging and tools used in this stage are discussed in the third part. The final output is available in two versions: the non-aligned version and the aligned one. The latter adopts the TMX (Translation Memory Exchange) standard format. At the end, the section dedicated to the future work points out the key stages concerned with extending the corpus and the studies that can benefit, directly or indirectly, from such a resource.
This report describes the Ohio State University Quake 2004 corpus of English spontaneous task-oriented two-person situated dialog. The corpus was collected using a first-person display of an interior space (rooms, corridors, stairs) in which the partners collaborate on a treasure hunt task. The corpus contains exciting new features such as deictic and exophoric reference, language that is calibrated against the spatial arrangement of objects in the world, and partial-observability of the task world imposed by the perceptual limitations inherent in the physical arrangement of the world. The corpus differs from prior dialog collections which intentionally restricted the interacting subjects from sharing any perceptual context, and which allowed one subject (the direction-giver or system) to have total knowledge of the state of the task world. The corpus consists of audio/video recordings of each person's experience in the virtual world and orthographic transcriptions. The virtual world can also be used by other researchers who want to conduct additional studies using this stimulus.
This paper presents a new corpus-based method for calculating the semantic similarity of two target words. Our method, called Second Order Co-occurrencePMI (SOC-PMI), uses Pointwise Mutual Information to sort lists of important neighbor words of the two target words. Then we consider the words which are common in both lists and aggregate their PMI values (from the opposite list) to calculate the relative semantic similarity. Our method was empirically evaluated using Miller and Charlers (1991) 30 noun pair subset, Ruben-stein and Goodenoughs (1965) 65 noun pairs, 80 synonym test questions from the Test of English as a Foreign Language (TOEFL), and 50 synonym test questions from a collection of English as a Second Language (ESL) tests. Evaluation results show that our method outperforms several competing corpus-based methods.
This paper proposes a tool kit to produce a domain ontology for text mining, based on case frames automatically constructed from a raw corpus of a specific domain. Since case frames are strongly related to implicit facts hidden in large domain-specific corpora, we can say that case frames are a promising device for text mining. The aim of the tool kit is to enable automatic analysis of event reports, from which implicit factors of the events are to be extracted. The tool kit enables us to produce a domain ontology by iterating associative retrieval of case frames and manual refinement. In this study, the tool kit is applied to the Japan Airlines pilot report collection, and a domain ontology of contributing factors in the civil aviation domain is experimentally produced. A lot of interesting examples are found in the ontology. In addition, a brief examination of the production process shows the efficiency of the tool kit.
Multimodal user interfaces offer more intuitive interaction for end-users, however, usually only through predefined input schemes. This paper describes a user experiment for multimodal interaction pattern identification, using head gesture and speech inputs for a 3D graph manipulation. We show that a direct mapping between head gestures and the 3D object predominates, however even for such a simple task inputs vary greatly between users, and do not exhibit any clustering pattern. Also, in spite of the high degree of expressiveness of linguistic modalities, speech commands in particular tend to use a limited vocabulary. We observed a common set of verb and adverb compounds in a majority of users. In conclusion, we recommend that multimodal user interfaces be individually customisable or adaptive to users interaction preferences.
Recently, the Prague Dependency Treebank 2.0 (PDT 2.0) has emerged as the largest text corpora annotated on the level of tectogrammatical representation (linguistic meaning) described in Sgall et al. (2004) and containing about 0.8 milion words (see Hajic (2004)). We hope that this level of annotation is so close to the meaning of the utterances contained in the corpora that it should enable us to automatically transform texts contained in the corpora to the form of knowledge base, usable for information extraction, question answering, summarization, etc. We can use Multilayered Extended Semantic Networks (MultiNet) described in Helbig (2006) as the target formalism. In this paper we discuss the suitability of such approach and some of the main issues that will arise in the process. In section 1, we introduce formalisms underlying PDT 2.0 and MultiNet, in section 2. We describe the role MultiNet can play in the system of Functional Generative Description (FGD), section 3 discusses issues of automatic conversion to MultiNet and section 4 gives some conclusions.
The importance of computer learner corpora for research in both second language acquisition and foreign language teaching is rapidly increasing. Computer learner corpora can provide us with data to describe the learners interlanguage system at different points of its development and they can be used to create pedagogical tools. In this paper, we first present a new computer learner corpus in French. We then describe an analyzer called Direkt Profil, that we have developed using this corpus. The system carries out a sentence analysis based on developmental sequences, i.e. local morphosyntactic phenomena linked to a development in the acquisition of French as a foreign language. We present a brief introduction to developmental sequences and some examples in French. In the final section, we introduce and evaluate a method to optimize the definition and detection of learner profiles using machine-learning techniques.
It is acknowledged that a good phonemic transcription of proper names is imperative for the success of many modern speech-based services such as directory assistance, car navigation, etc. It is also known that state-of-the-art general-purpose grapheme-to-phoneme (g2p) converters perform rather poorly on many name categories. This paper proposes to use a g2p-p2p tandem comprising a state-of-the-art general-purpose g2p converter that produces an initial transcription and a name category specific phoneme-to-phoneme (p2p) converter that aims at correcting the mistakes made by the g2p converter. The main body of the paper describes a novel methodology for the automatic construction of the p2p converter. The methodology is implemented in a software toolbox that will be made publicly available in a form that will permit the user to design a p2p converter for an arbitrary name category. To give a proof of concept, the toolbox was used for the development of three p2p converters for first names, surnames and geographical names respectively. The obtained systems are small (few rules) and effective: significant improvements (up to 50{\%} relative) of the grapheme-to-phoneme conversion are obtained. These encouraging results call for a further development and improvement of the approach.
The purpose of this work is to propose a new methodology to ameliorate the Markov Cluster (MCL) Algorithm that is well known as an efficient way of graph clustering (Van Dongen, 2000). The MCL when applied to a graph of word associations has the effect of producing concept areas in which words are grouped into the similar topics or similar meanings as paradigms. However, since a word is determined to belong to only one cluster that represents a concept, Markov clusters cannot show the polysemy or semantic indetermination among the properties of natural language. Our Recurrent MCL (RMCL) allows us to create a virtual adjacency relationship among the Markov hard clusters and produce a downsized and intrinsically informative semantic network of word association data. We applied one of the RMCL algorithms (Stepping-stone type) to a Japanese associative concept dictionary and obtained a satisfactory level of performance in refining the semantic network generated from MCL.
Large speech corpora (LSC) constitute an indispensable resource for conducting research in speech processing and for developing real-life speech applications. In 2004 the Spoken Dutch Corpus (CGN) became available, a corpus of standard Dutch as spoken by adult natives in the Netherlands and Flanders. Owing to budget constraints, CGN does not include speech of children, non-natives, elderly people and recordings of speech produced in human-machine interactions. Since such recordings would be extremely useful for conducting research and for developing HLT applications for these specific groups of speakers of Dutch, a new project, JASMIN-CGN, was started which aims at extending CGN in different ways: by collecting a corpus of contemporary Dutch as spoken by children of different age groups, non-natives with different mother tongues and elderly people in the Netherlands and Flanders and, in addition, by collecting speech material in a communication setting that was not envisaged in CGN: human-machine interaction. We expect that the knowledge gathered from these data can be generalized to developing appropriate systems also for other speaker groups (i.e. adult natives). One third of the data will be collected in Flanders and two thirds in the Netherlands.
We present a framework that combines a web-based text acquisition tool, a term extractor and a two-level workflow management system tailored for facilitating dictionary updates. Our aim is to show that, thanks to such a methodology, it is possible to monitor data sources and rapidly review and code new dictionary entries. Once approved, these new entries can feed in real-time client dictionary-based applications that need to be continuously kept up to date.
As Speech Recognition Systems improve, they become suitable for facingnew problems. Multilingual speech recognition is one such problems.In the present work, the case of the Comunitat Valenciana multilingual environment is studied.The official languages in the Comunitat Valenciana (Spanish and Valencian) share most of their acoustic units, and their vocabularies and syntax are quite similar.They have influenced each other for many years.A small corpus on an Information System task was developed for experimentationpurposes.This choice will make it possible to develop a working prototype in the future,and it is simple enough to build semi-automatic language models.The design of the acoustic corpus is discussed, showing that all combinations of accents have been studied (native, non-native speakers, male, female, etc.).
In this paper we describe a machine translation prototype in which we use only minimal resources for both the source and the target language. A shallow source language analysis, combined with a translation dictionary and a mapping system of source language phenomena into the target language and a target language corpus for generation are all the resources needed in the described system. Several approaches are presented.
In 2004 a consortium of ministries and organizations in the Netherlands and Flanders launched the comprehensive Dutch-Flemish HLT programme STEVIN (a Dutch acronym for Essential Speech and Language Technology Resources). To guarantee its Dutch-Flemish character, this large-scale programme is carried out under the auspices of the intergovernmental Dutch Language Union (NTU). The aim of STEVIN is to contribute to the further progress of HLT for the Dutch language, by raising awareness of HLT results, stimulating the demand of HLT products, promoting strategic research in HLT, and developing HLT resources that are essential and are known to be missing. Furthermore, a structure was set up for the management, maintenance and distribution of HLT resources. The STEVIN programme, which will run from 2004 to 2009, resulted from HLT activities in the Dutch language area, which were reported on at previous LREC conferences (2000, 2002, 2004). In this paper we will explain how different activities are combined in one comprehensive programme. We will show how cooperation can successfully be realized between different parties (language and speech technology, Flanders and the Netherlands, academia, industry and policy institutions) so as to achieve one common goal: progress in HLT.
This paper describes a trilingual sign language dictionary (Japanese Sign Language and American Sign Language, and Korean Sign Language) which helps those who learn each sign language directly from their mother sign language. Our discussion covers two main points. The first describes the necessity of a trilingual dictionary. Since there is no universal sign language or real international sign language deaf people should learn at least four languages: they want to talk to people whose mother tongue is different from their owns, the mother sign language, the mother spoken language as the first intermediate language, the target spoken language as the second intermediate language, and the sign language in which they want to communicate. Those two spoken languages become language barriers for deaf people and our trilingual dictionary will remove the barrier. The second describes the use of computer. As the use of computers becomes widespread, it is increasingly convenient to study through computer software or Internet facilities. Our WWW dictionary system provides deaf people with an easy means of access using their mother-sign language, which means they don't have to overcome the barrier of learning a foreign spoken language. It also provides a way for people who are going to learn three sign languages to look up new vocabulary. We are further planning to examine how our dictionary system could be used to educate and assist deaf people.
The aim of the paper is twofold. Firstly, an approach is presented how to select the correct antecedent for an anaphoric element according to the kind of text segments in which both of them occur. Basically, information on logical text structure (e.g. chapters, sections, paragraphs) is used in order to select the antecedent life span of a linguistic expression, i.e. some linguistic expressions are more likely to be chosen as an antecedent throughout the whole text than others. In addition, an appropriate search scope for an anaphora expressed by an expression can be defined according to the document structuring elements that include the linguistic expression. Corpus investigations give rise to the supposition that logical text structure influences the search scope of candidates for antecedents. Second, a solution is presented how to integrate the resources used for anaphora resolution. In this approach, multi-layered XML annotation is used in order to make a set of resources accessible for the anaphora resolution system.
The development of communication technologies has contributed to the appearance of new forms in the written language that scientists have to study according to their peculiarities (typing or viewing constraints, synchronicity, etc). In the particular case of SMS (Short Message Service), studies are complicated by a lack of data, mainly due to technical constraints and privacy considerations. In this paper, we present a corpus of 30,000 French SMS collected through a project in Belgium named Faites don de vos SMS {\`a} la science (Give your SMS to Science). This corpus is unique in its quality, its size and the fact that the SMS have been manually translated into standard French. We will first describe the collection process and discuss the writers' profiles. Then we will explain in detail how the translation was carried out.
In modern information retrieval systems, effective indexing can be achieved by removal of stop words. Till now many stop word lists have been developed for English language. However, no standard stop word list has been constructed for Chinese language yet. With the fast development of information retrieval in Chinese language, exploring the evaluation of Chinese stop word lists becomes critical. In this paper, to save the time and release the burden of manual comparison, we propose a novel stop word list evaluation method with a mutual information-based Chinese segmentation methodology. Experiments have been conducted on training texts taken from a recent international Chinese segmentation competition. Results show that effective stop word lists can improve the accuracy of Chinese segmentation significantly.
The present paper describes psycholinguistic experiments aimed at exploring the way people behave while accessing electronic dictionaries. In our work we focused on the access by meaning that, in comparison with the access by form, is currently less studied and very seldom implemented in modern dictionary interfaces. Thus, the goal of our experiments was to explore dictionary users requirements and to study what services an intelligent dictionary interface should be able to supply to help solving access by meaning problems. We tested several access-supporting enhancements of electronic dictionaries based on various language resources (corpora, wordnets, word association norms and explanatory dictionaries). Experiments were carried out with native speakers of three European languages  English, Czech and Russian. Results for monolingual and bilingual cases are presented.
In this paper we outline the German speech data collection for the SmartWeb project, which is fundedby the German Ministry of Science and Education. We focus on the SmartWeb Handheld Corpus (SHC), which has been collected by the Bavarian Archive for Speech Signals (BAS) at the Phonetic Institute (IPSK) of Munich University. Signals of SHC are being recorded in real-life environments(indoor and outdoor) with real background noise as well as real transmission line errors.We developed a new elicitation method and recording technique, calledsituational prompting, which facilitates collecting realistic dialogue speech data in a cost efficient way.We can show that almost realistic speech queries to a dialogue system issued over a mobile PDA or smart phonecan be collected very efficiently using an automatic speech server.We describe the technical and linguistic features of the resulting speech corpus, which will bepublicly available at BAS or ELDA.
The main objective of this paper is to introduce an alternation-based model of valency lexicon of Czech verbs VALLEX. Alternations describe regular changes in valency structure of verbs -- they are seen as transformations taking one lexical unit and return a modified lexical unit as a result. We characterize and exemplify syntactically-based and semantically-based' alternations and their effects on verb argument structure. The alternation-based model allows to distinguish a minimal form of lexicon, which provides compact characterization of valency structure of Czech verbs, and an expanded form of lexicon useful for some applications.
The paper represents the Turdis database of spontaneous conversations in tourist domain in Slovenian language. Database was built for use in developing speech-to-speech translation components, however it can be used also for developing dialog systems or used for linguistic researches. The idea was to record a database of telephone conversations in tourism where the naturalness of conversations is affected as little as possible while we still obtain a permission for recording from all the speakers. When recording in studio environment there can be many problems. It is especially difficult to imitate a tourist agent if a speaker does not have such experiences and therefore lacks the background knowledge that a tourist agent has. Therefore the Turdis database was recorded with professional tourist agents. The agreement with local tourist companies enabled that we recorded a tourist agent while he was at his working place in his working time answering the telephone. Callers were contacted individually and asked to use the Turdis system and make a call to selected tourist company. Technically the recording was done using PC ISDN card. Database was orthographically transcribed with Transcriber tool. At the present it includes cca 43 000 words.
Three advanced German speech corpora have been collected during theGerman SmartWeb project. One of them, the SmartWeb MotorbikeCorpus (SMC) is described in this paper.As with all SmartWeb speech corpora SMC is designed for a dialogue system dealing with open domains.The corpus is recorded under the special circumstances of a motorbike ride and contains utterances of the driver related to information retrieval from various sources and different topics. Audio tracks show characteristic noise from the engine and surrounding traffic as well as drop outs caused by the transmission over Bluetooth and the UMTS mobile network. We discuss the problems of the technical setup and the fully automatic evocation of natural-spoken queries by means of dialogue-like sequences.
Knowledge has been crucial for the countrys development and business intelligence, where valuable knowledge is distributed over several websites with heterogeneous formats. Moreover, finding the needed information is a complex task since there has been lack of semantic relation and organization. Even if it has been found, an overload may occur because there is no content digestion. This paper focuses on ontology-driven knowledge extraction with natural language processing techniques and a framework of usercentric design for accessing the required information based on their demands. These demands can be expressed in the form of Knowwhat, Know-why, Know-where, Know-when, Know-how, and Know-who for a question answering system.
Temporal relations between events and times are often difficult to discover, time-consuming and expensive. In this paper a corpus study is performed to derive a strong relation between discourse structure, as revealed by Veins theory, and the temporal links between entities, as addressed in the TimeML annotation standard. The data interpretation helps us gain insight on how Veins theory can improve the manual and even (semi-) automatic detection of temporal relations.
In the present contribution we claim that corpus annotation serves, among other things, as an invaluable test for linguistic theories standing behind the annotation schemes, and as such represents an irreplaceable resource of linguistic information for the build-up of grammars. To support this claim we present four linguistic phenomena for the study and relevant description of which in grammar a deep layer of corpus annotation as introduced in the Prague Dependency Treebank has brought important observations, namely the information structure of the sentence, condition of projectivity and word order, types of dependency relations and textual coreference.
We describe an experiment with Czech-English word alignment. Half a thousand sentences were manually annotated by two annotators in parallel and the most frequent reasons for disagreement are described. We evaluate the accuracy of GIZA++ alignment toolkit on the data and identify that lemmatization of the Czech part can reduce alignment error to a half. Furthermore we document that about 38{\%} of tokens difficult for GIZA++ were difficult for humans already.
The aim of this paper is to present the MORBO/COMP project, which has reached its final stage in development and will soon be published on-line. MORBO/COMP is large database of compound types in over 20 languages. The data for these languages have been collected and analysed by a group of morphologists from various European countries.
General purpose ontologies and domain ontologies make up the infrastructure of the Semantic Web, which allow for accurate data representations with relations, and data inferences. In our approach to multimodal dialogue systems providing question answering functionality (SMARTWEB), the ontological infrastructure is essential. We aim at an integrated approach in which all knowledge-aware system modules are based on interoperating ontologiesin a common data model. The discourse ontology is meant to provide the necessary dialogue- and HCI concepts. We present the ontological syntactic structure of multimodal question answering results as partof this discourse ontology which extends the W3C EMMA annotation framework and uses MPEG-7 annotations. In addition, we describe anextension to ontological result structures where automatic and context-based sorting mechanisms can be naturally incorporated.
This paper discusses the parameterized Equivalence Class Method for Dutch, an approach developed to incorporate standard lexical representations for Dutch idioms into representations required by any specific NLP system with as minimal manual work as possible. The purpose of the paper is to give an overview of parameters applicable to Dutch, which are determined by examining a large set of data and two Dutch NLP systems. The effects of the introduced parameters are evaluated and the results presented.
This paper describes the development process of a contextualized corpus for research on Human-Robot Communication. The data have been collected in two Wizard-of-Oz user studies performedwith 22 and 5 users respectively in a scenario that is called the HomeTour. In this scenario the users show the environment (a single room, or a whole floor) to the robot using a combination of speech and gestures. The corpus has been transcribed and annotated with respect to gestures and conversational acts, thus forming a core annotation. We have also annotated or linked other types of data, e.g., laser range finder readings, positioning analysis, questionnaire data and task descriptions that form the annotated context of the scenario. By providing a rich set of different annotated data, thecorpus is thus an important resource both for research on natural language speech interfaces for robots and for research on human-robot communication in general.
Tagging as the most crucial annotation of language resources can still be challenging when the corpus size is big and when the corpus data is not homogeneous. The Chinese Gigaword Corpus is confounded by both challenges. The corpus containsroughly 1.12 billion Chinese characters from two heterogeneous sources: respective news in Taiwan and in Mainland China. In other words, in addition to its size, the data also contains two variants of Chinese that are known to exhibit substantial linguistic differences. We utilize Chinese Sketch Engine as the corpus query tool, by which grammar behaviours of the two heterogeneous resources could be captured and displayed in a unified web interface. In this paper, we report our answer to the two challenges to effectively tag this large-scale corpus. The evaluation result shows our mechanism of tagging maintains high annotation quality.
Valence dictionaries are dictionaries in which logical predicates (most of the times verbs) are inventoried alongside with the semantic and syntactic information regarding the role of the arguments with which they combine, as well as the syntactic restrictions these arguments have to obey. In this article we present the incipient stage of the project Syntactic and semantic database in XML format: an HPSG representation of verb valences in Romanian. Its aim is the development of a valence dictionary in XML format for a set of 3000 Romanian verbs. Valences are specified for each sense of each verb, alongside with an illustrative example, possible argument alternations and a set of multiword expressions in which the respective verb occurs with the respective sense. The grammatical formalism we make use of is Head-driven Phrase Structure Grammar, which offers one of the most comprehensive frames of encoding various types of linguistic information for lexical items. XML is the most appropriate mark-up language for describing information structured in HPSG framework. The project can be further on extended so that to cover all Romanian verbs (around 7000) and also other predicates (nouns, adjectives, prepositions).
Many learning tasks require substantial skills training. Ideally, the student might benefit the most from having a human expert  a teacher or trainer  at hand throughout, but human expertise remains a scarce resource. The second-best solution could be to do skills training with a computer-based self-training system. This vision of the computer as tutor currently motivates increasing efforts world-wide, in all manner of fields, including that of computer-assisted language learning, or CALL. But, as pointed out by Hincks [2003], along with the growth of the CALL area comes a growing need for empirical evidence that CALL systems have a beneficial effect. This point is reiterated by Chapelle [2002] who defines the goal for Computer Assisted Second Language Research as the gathering of evidence for the effect of CALL and instructional design. This paper presents results of a field test of our pronunciation training system which enables immigrants and others to self-train their pronunciation skills of single Danish words.
Entities are pivotal in describing events and objects, and also very important in Document Summarization. In general only explicit entities which can be extracted by a Named Entity Recognizer are used in real applications. However, implicit entities hidden behind the phrases or words, e.g. entity referred by the phrase cross border, are proved to be helpful in Document Summarization. In our experiment, we extract the implicit entities from the web resources.
In Japanese, syntactic structure of a sentence is generally represented by the relationship between phrasal units, or bunsetsus inJapanese, based on a dependency grammar. In the same way, thesyntactic structure of a sentence in a large, spontaneous, Japanese-speech corpus, the Corpus of Spontaneous Japanese (CSJ), isrepresented by dependency relationships between bunsetsus. This paper describes the criteria and definitions of dependency relationships between bunsetsus in the CSJ. The dependency structure of the CSJ is investigated, and the difference in the dependency structures ofwritten text and spontaneous speech is discussed in terms of thedependency accuracies obtained by using a corpus-based model. It is shown that the accuracy of automatic dependency-structure analysis canbe improved if characteristic phenomena of spontaneous speech such as self-corrections, basic utterance units in spontaneous speech, and bunsetsus that have no modifiee are detected and used for dependency-structure analysis.
The ZT corpus (Basque Corpus of Science and Technology) is a tagged collection of specialized texts in Basque, which wants to be a main resource in research and development about written technical Basque: terminology, syntax and style. It will be the first written corpus in Basque which will be distributed by ELDA (at the end of 2006) and it wants to be a methodological and functional reference for new projects in the future (i.e. a national corpus for Basque). We also present the technology and the tools to build this Corpus. These tools, Corpusgile and Eulia, provide a flexible and extensible infrastructure for creating, visualizing and managing corpora and for consulting, visualizing and modifying annotations generated by linguistic tools.
Spoken dialogue systems are common interfaces to backend data in information retrieval domains. As more data is made available on the Web and IE technology matures, dialogue systems, whether they be speech- or text-based, will be more in demand to provide user-friendly access to this data. However, dialogue systems must become both easier to configure, as well as more informative than the traditional form-based systems that are currently available. We present techniques in this paper to address the issue of automating both content selection for use in summary responses and in system initiative queries.
The proliferation of multilingual documentation in our Information Society has become a common phenomenon. This documentation is usually categorised by hand, entailing a time-consuming and arduous burden. This is particularly true in the case of keyword assignment, in which a list of keywords (descriptors) from a controlled vocabulary (thesaurus) is assigned to a document. A possible solution to alleviate this problem comes from the hand of the so-called Machine-Aided Indexing (MAI) systems. These systems work in cooperation with professional indexer by providing a initial list of descriptors from which those most appropiated will be selected. This way of proceeding increases the productivity and eases the task of indexers. In this paper, we propose a statistical text classification framework for bilingual documentation, from which we derive two novel bilingual classifiers based on the naive combination of monolingual classifiers. We report preliminary results on the multilingual corpus Acquis Communautaire (AC) that demonstrates the suitability of the proposed classifiers as the backend of a fully-working MAI system.
The paper presents a tool for keyword extraction from multilingual resources developed within the AXMEDIS project. In this tool lexical collocations (Sinclair, 1991) internal to documents are used to enhance the performance obtained through standard statistical procedure. A first set of mono-term keywords is extracted through the TF.IDF algorithm (Salton, 1989). The internal analysis of the document generates a second set of multi-term keywords based on the first set, rather than on multi-term frequency comparison with a general resource (Witten et al. 1999). Collocations in which a mono-term keyword occurs as the head are considered as multi-term keywords, and are assumed to increase the identification of the content. The evaluation compares the results of the TF.IDF procedure and the ones obtained with the enhanced procedure in terms of precision. Each set of keywords received a value from the point of view of a possible user, regarding: (a) overall efficiency of the whole set of keywords for the identification of the content; (b) adequacy of each extracted keyword. Results show that multi-term keywords increase the content identification with a 100{\%} relative factor and that the adequacy is enhanced in 33{\%} of cases.
Automatic question answering (QA) is a complex task, which lies in the cross-road of Natural Language Processing, Information Retrieval and Human Computer Interaction. A typical QA system has four modules  question processing, document retrieval, answer extraction and answer presentation. In each of these modules, a multitude of tools can be used. Therefore, the performance evaluation of each of these components is of great importance in order to check their impact in the global performance, and to conclude whether these components are necessary, need to be improved or substituted.This paper describes some experiments performed in order to evaluate several components of the question answering system Esfinge.We describe the experimental set up and present the results of error analysis based on runtime logs of Esfinge. We present the results of component analysis, which provides good insights about the importance of the individual components and pre-processing modules at various levels, namely stemming, named-entity recognition, PoS Filtering and filtering of undesired answers. We also present the results of substituting the document source in which Esfinge tries to find possible answers and compare the results obtained using web sources such as Google, Yahoo and BACO, a large database of web documents in Portuguese.
In this paper, we describe the set-up process and an initial evaluation of a unit-selection speech synthesizer. The synthesizer is specific in that it is intended to speak with a prominent voice. As a consequence, only very limited resources were available for setting up the unit database. These resources have been extracted from an audio book, segmented with the help of an HMM-based wrapper, and then used with the non-uniform unit-selection approach implemented in the Bonn Open Synthesis System (BOSS). In order to adapt the database to the BOSS implementation, the label files were amended by phrase boundaries, converted to XML, amended by prosodic and spectral information, and then further converted to a MySQL relational database structure. The BOSS system selects units on the basis of this information, adding individual unit costs to the concatenation costs given by MFCC and F0 distances. The paper discusses the problems which occurred during the database set-up, the invested effort, as well as the quality level which can be reached by this approach.
Cross-language information retrieval consists in providing a query in one language and searching documents in one or different languages. These documents are ordered by the probability of being relevant to the user's request. The highest ranked document is considered to be the most likely relevant document. The LIC2M cross-language information retrieval system is a weighted Boolean search engine based on a deep linguistic analysis of the query and the documents. This system is composed of a linguistic analyzer, a statistic analyzer, a reformulator, a comparator and a search engine. The linguistic analysis processes both documents to be indexed and queries to extract concepts representing their content. This analysis includes a morphological analysis, a part-of-speech tagging and a syntactic analysis. In this paper, we present the deep linguistic analysis used in the LIC2M cross-lingual search engine and we will particularly focus on the impact of the syntactic analysis on the retrieval effectiveness.
In this paper we describe the annotation of COMPARA, currently the largest post-edited parallel corpora which include Portuguese. We describe the motivation, the results so far, and the way the corpus is being annotated. We also provide the first grounded results about syntactical ambiguity in Portuguese. Finally, we discuss some interesting problems in this connection.
The new EU member countries face the problems of terminology resource fragmentation and lack of coordination in terminology development in general. The EuroTermBank project aims at contributing to improve the terminology infrastructure of the new EU countries and the project will result in a centralized online terminology bank - interlinked to other terminology banks and resources - for languages of the new EU member countries. The main focus of this paper is on a description of how to identify best practice within terminology work seen from a broad perspective. Surveys of real life terminology work have been conducted and these surveys have resulted in identification of scenario specific best practice descriptions of terminology work. Furthermore, this paper will present an outline of the specific criteria that have been used for selection of existing term resources to be included in the EuroTermBank database.
This paper presents the TagShare project and the linguistic resources and tools for the shallow processing of Portuguese developed in its scope. These resources include a 1 million token corpus that has been accurately hand annotated with a variety of linguistic information, as well as several state of the art shallow processing tools capable of automatically producing that type of annotation. At present, the linguistic annotations in the corpus are sentence and paragraph boundaries, token boundaries, morphosyntactic POS categories, values of inflection features, lemmas and namedentities. Hence, the set of tools comprise a sentence chunker, a tokenizer, a POS tagger, nominal and verbal analyzers and lemmatizers, a verbal conjugator, a nominal inflector, and a namedentity recognizer, some of which underline several online services.
In this paper we will present Corp{\'o}grafo, a mature web-based environment for working with corpora, for terminology extraction, and for ontology development. We will explain Corp{\'o}grafos workflow and describe the most important information extraction methods used, namely its term extraction, and definition / semantic relations identification procedures. We will describe current Corp{\'o}grafo users and present a brief overview of the XML format currently used to export terminology databases. Finally, we present future improvements for this tool.
In this paper we argue for the need to construct a data base of Romanian syllables. We explain the reasons for our choice of the DOOM corpus which we have used. We describe the way syllabification was performed and explain how we have constructed the data base. The main quantitative aspects which we have extracted from our research are presented. We also computed the entropy of the syllables and the entropy of the syllables w.r.t. the consonant-vowel structure. The results are compared with results of similar researches realized for different languages.
In this paper we present a model to transfer a grammatical formalism in another. The model is applicable only on restrictive conditions. However, it is fairly useful for many purposes: parsing evaluation, researching methods for truly combining different parsing outputs to reach better parsing performances, and building larger syntactically annotated corpora for data-driven approaches. The model has been tested over a case study: the translation of the Turin Tree Bank Grammar to the Shallow Grammar of the CHAOS Italian parser.
The paper presents a corpus-based study aimed at an analysis of ontological and terminological commitments in the discourse of specialist communities. The analyzed corpus contains the lectures delivered by the Nobel Prize winners in Physics and Economics. The analysis focuses on (a) the collocational use of automatically identified domain-specific terms and (b) a description of meta-discourse in the lectures. Candidate terms are extracted based on the z-score of frequency and weirdness. Compounds comprising these candidate terms are then identified using the ontology representation system Prot{\'e}g{\'e}. This method is then replicated to complete analysis by including an investigation of metadiscourse markers signalling how writers project themselves into their work.
In this paper we claim that an integration of FrameNet and WordNet will improve interoperability, user-friendliness and usability of both lexical resources. If the former provides a sophisticated representational structure compared to a narrow lexical coverage, the latter - on the other side - supplies a dense network of word senses and semantic relations although not supporting advanced accessibility (i.e., via frames). According to the integration perspective we present in the paper, we introduce LexiPass methodology, which combines Burckardts tool WordNet Detour of FrameNet with basic statistical analysis, enabling frame-guided search and extraction of domain synsets from WordNet.
The present research focuses on annotation issues in the context of the acoustic detection of fear-type emotions for surveillance applications. The emotional speech material used for this study comes from the previously collected SAFE Database (Situation Analysis in a Fictional and Emotional Database) which consists of audio-visual sequences extracted from movie fictions. A generic annotation scheme was developed to annotate the various emotional manifestations contained in the corpus. The annotation was carried out by two labellers and the two annotations strategies are confronted. It emerges that the borderline between emotion and neutral vary according to the labeller. An acoustic validation by a third labeller allows at analysing the two strategies. Two human strategies are then observed: a first one, context-oriented which mixes audio and contextual (video) information in emotion categorization; and a second one, based mainly on audio information. The k-means clustering confirms the role of audio cues in human annotation strategies. It particularly helps in evaluating those strategies from the point of view of a detection system based on audio cues.
The IBM Models (Brown et al., 1993) enjoy great popularity in the machine translation community because they offer high quality word alignments and a free implementation is available with the GIZA++ Toolkit (Och and Ney, 2003). Several methods have been developed to overcome the asymmetry of the alignment generated by the IBM Models. A remaining disadvantage, however, is the high model complexity. This paper describes a word alignment training procedure for statistical machine translation that uses a simple and clear statistical model, different from the IBM models. The main idea of the algorithm is to generate a symmetric and monotonic alignment between the target sentence and a permutation graph representing different reorderings of the words in the source sentence. The quality of the generated alignment is shown to be comparable to the standard GIZA++ training in an SMT setup.
In this paper, we assess an aspect of the quality of the broad phonetic transcriptions in the Spoken Dutch Corpus (CGN). The corpus contains speech from native speakers of Dutch originating from The Netherlands and the Dutch speaking part of Belgium. The phonetic transcriptions were made by transcribers from both regions. In previous research, we have identified regional differences in the transcribers' behaviour. In this paper, we explore the precise sources of the regional bias in the CGN transcriptions and we evaluate its impact on the phonetic transcriptions. More specifically, (1) the regional bias in the canonical transcriptions that served as the basis for the verification task of the transcribers is critically analysed, and (2) we verify in an experiment the regional bias introduced by the transcribers themselves. The possible effects of this inherent regional bias in the CGN transcriptions on subsequent linguistic analyses are briefly discussed.
This paper describes the test collections produced for the Patent Retrieval Task in the Fifth NTCIR Workshop. We performed the invalidity search task, in which each participant group searches a patent collection for the patents that can invalidate the demand in an existing claim. For this purpose, we performed both document and passage retrieval tasks. We also performed the automatic patent classification task using the F-term classification system. The test collections will be available to the public for research purposes.
This paper describes results of the first successful effort in applying a stochastic strategy  or, namely, a second order Markov model paradigm implemented by the TnT trigram tagger  to morphosyntactic tagging of Croatian texts. Beside the tagger, for purposes of both training and testing, we had at our disposal only a 100 Kw Croatia Weekly newspaper subcorpus, manually tagged using approximately 1000 different MULTEXT-East v3 morphosyntactic tags. The test basically consisted of randomly assigning a variable size portion of the corpus for the taggers training procedure and also another fixed-size portion, sized at 10{\%} of the corpus, for the tagging procedure itself; this method allowed us not only to provide preliminary results regarding tagger accuracy on Croatian texts, but also to inspect the behavior of the stochastic tagging paradigm in general. The results were then taken from the test case providing 90{\%} of the corpus for training purposes and varied from around 86{\%} in the worst case scenario up to a peak of around 95{\%} correctly assigned full MSD tags. Results on PoS only expectedly reached the human error level, with TnT correctly tagging above 98{\%} of test sets on average. Most MSD errors occurred on types with the highest number of candidate tags per word form  nouns, pronouns and adjectives  while errors on PoS, although following the same pattern, were almost insignificant. Detailed insight on tagging, F-measure for all PoS categories is provided in the course of the paper along with other facts of interest.
In this paper we present a proposal for extending the standard Wizard of Oz experimental methodology to language-enabled multimodal systems. We first discuss how Wizard of Oz experiments involving multimodal systems differ from those involving voice-only systems. We then go on to discuss the Extended Wizard of Oz methodology and the Wizard of Oz testing environment and protocol that we have developed. We then describe an example of applying this methodology to Archivus, a multimodal system for multimedia meeting retrieval and browsing. We focus in particular on the tools that the wizards would need to successfully and efficiently perform their tasks in a multimodal context. We conclude with some general comments about which questions need to be addressed when developing and using the Wizard of Oz methodology for testing multimodal systems.
The construction of a 500-million-word reference corpus of written Dutch has been identified as one of the priorities in the Dutch/Flemish STEVIN programme. For part of this corpus, manually corrected syntactic annotations will be provided. The paper presents the background of the syntactic annotation efforts, the Alpino parser which is used as an important tool for constructing the syntactic annotations, as well as a number of other annotation tools and guidelines. For the full STEVIN corpus, automatically derived syntactic annotations will be provided in a later phase of the programme. A number of arguments is provided suggesting that such a resource can be very useful for applications in information extraction, ontology building, lexical acquisition, machine translation and corpus linguistics.
Although significant advances have been made recently in the Question Answering technology, more steps have to be undertaken in order to obtain better results. Moreover, the best systems at the CLEF and TREC evaluation exercises are very complex systems based on custom-built, expensive ontologies whose aim is to provide the systems with encyclopedic knowledge. In this paper we investigated the use of Wikipedia, the open domain encyclopedia, for the Question Answering task. Previous works considered Wikipedia as a resource where to look for the answers to the questions. We focused on some different aspects of the problem, such as the validation of the answers as returned by our Question Answering System and on the use of Wikipedia categories in order to determine a set of patterns that should fit with the expected answer. Validation consists in, given a possible answer, saying wether it is the right one or not. The possibility to exploit the categories ofWikipedia was not considered until now. We performed our experiments using the Spanish version of Wikipedia, with the set of questions of the last CLEF Spanish monolingual exercise. Results show that Wikipedia is a potentially useful resource for the Question Answering task.
We describe a gold standard for semantic verb classes which is based on human associations to verbs. The associations were collected in a web experiment and then applied as verb features in a hierarchical cluster analysis. We claim that the resulting classes represent a theory-independent gold standard classification which covers a variety of semantic verb relations, and whose features can be used to guide the feature selection in automatic processes. To evaluate our claims, the association-based classification is validated against two standard approaches to semantic verb classes, GermaNet and FrameNet.
The orthography of Gikuyu includes a number of accented characters to represent the entire vowel system. These characters are however not readily available on standard computer keyboards and are usually represented as the nearest available character. This can render reading and understanding written texts more difficult. This paper describes a system that is able to automatically place these accents in Gikuyu text on the basis of local graphemic context. This approach avoids the need for an extensive digital lexicon, typically not available for resource-scarce languages. Using an extended trigram based-approach, the experiments show that this method can achieve a very high accuracy even with a limited amount of digitally available textual data. The experiments on Gikuyu are contrasted with experiments on French, German and Dutch.
One of the greatest challenges in the design of speech recognition based interfaces is about the navigation through the different service hierarchies and structures. On the one hand, the interactions based on human machine dialogues force a high level of hierarchical structuring of services, and on the other hand, it is necessary to wait for the last phases of the user interface development to obtain a global vision of the dialogue problems by means of user trials. To tackle these problems, Telef{\'o}nica M{\'o}viles Espa{\~n}a has carried out several projects with the final aim to define a corporate methodology based on rapid prototyping of the user interfaces, so that designers could integrate the process of design of voice interfaces with emulations of the navigation through the flow charts. This was also the starting point for a specific software product (MEDUSA) which addresses the needs of rapid prototyping of these user interfaces from the earliest stages of the design and analysis phases.
This paper describes the SALSA corpus, a large German corpus manually annotated with manual role-semantic annotation, based on the syntactically annotated TIGER newspaper corpus. The first release, comprising about 20,000 annotated predicate instances (about half the TIGER corpus), is scheduled for mid-2006. In this paper we discuss the annotation framework (frame semantics) and its cross-lingual applicability, problems arising from exhaustive annotation, strategies for quality control, and possible applications.
We present a new, unique and freely available parallel corpus containing European Union (EU) documents of mostly legal nature. It is available in all 20 official EU languages, with additional documents being available in the languages of the EU candidate countries. The corpus consists of almost 8,000 documents per language, with an average size of nearly 9 million words per language. Pair-wise paragraph alignment information produced by two different aligners (Vanilla and HunAlign) is available for all 190+ language pair combinations. Most texts have been manually classified according to the EUROVOC subject domains so that the collection can also be used to train and test multi-label classification algorithms and keyword-assignment software. The corpus is encoded in XML, according to the Text Encoding Initiative Guidelines. Due to the large number of parallel texts in many languages, the JRC-Acquis is particularly suitable to carry out all types of cross-language research, as well as to test and benchmark text analysis software across different languages (for instance for alignment, sentence splitting and term extraction).
In this paper, we describe the SALTO tool. It was originally developed for the annotation of semantic roles in the frame semantics paradigm, but can be used for graphical annotation of treebanks with general relational information in a simple drag-and-drop fashion. The tool additionally supports corpus management and quality control.
In this paper, we introduce the annotated KNACK-2002 corpus of Dutch written text. The corpus features five different annotation layers, ranging from the annotation of morphological boundaries at the word level, over the annotation of part-of-speech tags and phrase chunks at the syntactic level to the annotation of named entities at the semantic level and coreferential relations at the discourse level. We believe the corpus is unique in the Dutch language area because of its richness of annotation layers, providing researchers with a useful gold standard data set for different NLP tasks in the domains of morphology, (morpho)syntax, semantics and discourse.
Recent work has aimed at discovering ontological relations from text corpora. Most approaches are based on the assumption that verbs typically indicate semantic relations between concepts. However, the problem of finding the appropriate generalization level for the verb's arguments with respect to a given taxonomy has not received much attention in the ontology learning community. In this paper, we address the issue of determining the appropriate level of abstraction for binary relations extracted from a corpus with respect to a given concept hierarchy. For this purpose, we reuse techniques from the subcategorization and selectional restrictions acquisition communities. The contribution of our work lies in the systematic analysis of three different measures. We conduct our experiments on the Genia corpus and the Genia ontology and evaluate the different measures by comparing the results of our approach with a gold standard provided by one of the authors, a biologist.
This paper describes a novel method for reducing the transcription effort in the construction of task-adapted acoustic models for a practical automatic speech recognition (ASR) system. We have to prepare actual data samples collected in the practical system and transcribe them for training the task-adapted acoustic models. However, transcribing utterances is a time-consuming and laborious process. In the proposed method, we firstly adapt initial models to acoustic environment of the system using a small number of collected data samples with transcriptions. And then, we automatically select informative training data samples to be transcribed from a large-sized speech corpus based on acoustic likelihoods of the models. We perform several experimental evaluations in the framework of Takemarukun, a practical speech-oriented guidance system. Experimental results show that 1) utterance sets with low likelihoods cause better task-adapted models compared with those with high likelihoods although the set with the lowest likelihoods causes the performance degradation because of including outliers, and 2) MLLR adaptation is effective for training the task-adapted models when the amount of the transcribed data is small and EM training outperforms MLLR if we transcribe more than around 10,000 utterances.
We used four Part-of-Speech taggers, which are available for research purposes and were originally trained on text to tag a corpus of transcribed multiparty spoken dialogues. The assigned tags were then manually corrected. The correction was first used to evaluate the four taggers, then to retrain them. Despite limited resources in time, money and annotators we reached results comparable to those reported for the taggers on text. Based on our experience we present guidelines to produce reliably POS tagged corpora of new domains.
In our work, we present an analysis of the TimeBank corpus---the only available reference sample of TimeML-compliant annotation---from the point of view of its utility as a training resource for developing automated TimeML annotators. We are encouraged by experimental results indicative of the potential of TimeBank; at the same time, closer inspection of causes for some systematic errors shows off certain deficiencies in the corpus, primarily to do with small size and inconsistent annotation. Our analysis suggests that even a reference resource, developed outside of a rigorous process of training corpus design and creation, can be extremely valuable for training and development purposes. The analysis also highlights areas of correction and improvement for evolving the current reference corpus into a community infrastructure resource.
Case frames are important knowledge for a variety of NLP systems, especially when wide-coverage case frames are available. To acquire such large-scale case frames, it is necessary to automatically compile them from an enormous amount of corpus. In this paper, we consider the web as a corpus. We first build a huge text corpus from the web, and then construct case frames from the corpus. It is infeasible to do these processes by one CPU, and thus we employ a high-performance computing environment composed of 350 CPUs. The acquired corpus consists of 470M sentences, and the case frames compiled from them have 90,000 verb entries. The case frames contain most examples of usual use, and are ready to be applied to lots of NLP analyses and applications.
This paper presents the protocol of EASY the evaluation campaign for syntactic parsers of French in the EVALDA project of the TECHNOLANGUE program. We describe the participants, the corpus and its genre partitioning, the annotation scheme, which allows for the annotation of both constituents and relations, the evaluation methodology and, as an illustration, the results obtained by one participant on half of the corpus.
Japanese adverbs are classified as either declarative or normal; the former declare the communicative intention of the speaker, while the latter convey a manner of action, a quantity, or a degree by which the adverb modifies the verb or adjective that it accompanies. We have automatically classified adverbs as either declarative or not declarative using a machine-learning method such as the maximum entropy method. We defined adverbs having positive or negative connotations as the positive data. We classified adverbs in the EDR dictionary and IPADIC used by Chasen using this result and built an adverb dictionary that contains descriptions of the communicative intentions of the speaker.
Much everyday knowledge about physical aspects of objects does not exist as computer data, though such computer-based knowledge will be needed to communicate with next generation voice-commanded personal robots as well in other applications involving visual scene recognition. The largest attempt at manually creating common-sense knowledge, the CYC project, has not yet produced the information needed for these tasks. A new direction is needed, based on an automated approach to knowledge extraction. In this article we present our project to mine web text to find properties of objects that are not currently stored in computer readable form.
Given the increasing number of neologisms in biomedicine (names of genes, diseases, molecules, etc.), the rate of acronyms used in literature also increases. Existing acronym dictionaries cannot keep up with the rate of new creations. Thus, discovering and disambiguating acronyms and their expanded forms are essential aspects of text mining and terminology management. We present a method for clustering long forms identified by an acronym recognition method. Applying the acronym recognition method to MEDLINE abstracts, we obtained a list of short/long forms. The recognized short/long forms were classified by abiologist to construct an evaluation set for clustering sets of similar long forms. We observed five types of term variation in the evaluation set and defined four similarity measures to gathers the similar longforms (i.e., orthographic, morphological, syntactic, lexico semantic variants, nested abbreviations). The complete-link clustering with the four similarity measures achieved 87.5{\%} precision and 84.9{\%} recall on the evaluation set.
One of the main challenges in biomedical text mining is the identification of terminology, which is a key factor for accessing and integrating the information stored in literature. Manual creation of biomedical terminologies cannot keep pace with the data that becomes available. Still, many of them have been used in attempts to recognise terms in literature, but their suitability for text mining has been questioned as substantial re-engineering is needed to tailor the resources for automatic processing. Several approaches have been suggested to automatically integrate and map between resources, but the problems of extensive variability of lexical representations and ambiguity have been revealed. In this paper we present a methodology to automatically maintain a biomedical terminological database, which contains automatically extracted terms, their mutual relationships, features and possible annotations that can be useful in text processing. In addition to TermDB, a database used for terminology management and storage, we present the following modules that are used to populate the database: TerMine (recognition, extraction and normalisation of terms from literature), AcroTerMine (extraction and clustering of acronyms and their long forms), AnnoTerm (annotation and classification of terms), and ClusTerm (extraction of term associations and clustering of terms).
This paper describes a method for extracting translations of terms across languages, using parallel corpora. The extracted term correspondences are such that they are useful when performing query expansion for cross language information retrieval, or for bilingual lexicon extraction. The method makes use of the mutual information measure and allows for mapping between single word- to multi-word terms and vice versa. The method is scalable (accommodates addition or removal of data) and produces high quality results, while keeping the computational costs low enough for allowing on-the-fly translations in e.g., cross language information retrieval systems. The work was carried out in collaboration with Intrafind Software AG (Munich, Germany).
Data sparsity is a large problem in natural language processing that refers to the fact that language is a system of rare events, so varied and complex, that even using an extremely large corpus, we can never accurately model all possible strings of words. This paper examines the use of skip-grams (a technique where by n-grams are still stored to model language, but they allow for tokens to be skipped) to overcome the data sparsity problem. We analyze this by computing all possible skip-grams in a training corpus and measure how many adjacent (standard) n-grams these cover in test documents. We examine skip-gram modelling using one to four skips with various amount of training data and test against similar documents as well as documents generated from a machine translation system. In this paper we also determine the amount of extra training data required to achieve skip-gram coverage using standard adjacent tri-grams.
The paper describes the main principles of development and current state of Linguistic Ontology on Natural Sciences and Technology intended for information-retrieval tasks. In the development of the ontology we combined three different methodologies: development of information-retrieval thesauri, development of wordnets, formal ontology research. Combination of these methodologies allows us to develop large ontologies for broad domains.
Scenario Question Answering is a relatively new direction in Question Answering (QA) research that presents a number of challenges for evaluation. In this paper, we propose a comprehensive evaluation strategy for Scenario QA, including amethodology for building reusable test collections for Scenario QA and metrics for evaluating system performance over such test collections. Using this methodology, we have built a test collection, which we have made available for public download as a service to the research community. It is our hope that widespread availability of quality evaluation materials fuels research in new approaches to the Scenario QA task.
A stochastic parsing component has been applied on a French spoken language dialogue corpus, recorded in the framework of the MEDIA evaluation campaign. Realized as an ergodic HMM using Viterbide coding, the parser outputs the most likely semantic representation given a transcribed utterance as input. The semantic sequences used for training and testing have been derived from the semantic representations of the MEDIA corpus. The HMM parameters have been estimated given the word sequences along with their semantic representation. The performance score of the stochastic parser has been automatically determined using the mediaval tool applied to a held out reference corpus. Evaluation results will be presented in the paper.
We present an approach to discriminant-based MRS banking, i.e. the construction of an annotated corpus where each input item is paired with a logical-form semantics. Semantic annotations are produced by parsing with a broad-coverage precision grammar, followed by manual disambiguation. The selection of the preferred analysis for each item (and hence its semantic form) builds on a notion of semantic discriminants, essentially localized dependencies extracted from a full-fledged, underspecified semantic representation.
A highly accurate Named Entity (NE) corpus for Hungarian that is publicly available for research purposes is introduced in the paper, along with its main properties. The results of experiments that apply various Machine Learning models and classifier combination schemes are also presented to serve as a benchmark for further research based on the corpus. The data is a segment of the Szeged Corpus (Csendes et al., 2004), consisting of short business news articles collected from MTI (Hungarian News Agency, www.mti.hu). The annotation procedure was carried out paying special attention to annotation accuracy. The corpus went through a parallel annotation phase done by two annotators, resulting in a tagging with inter-annotator agreement rate of 99.89{\%}. Controversial taggings were collected and discussed by the two annotators and a linguist with several years of experience in corpus annotation. These examples were tagged following the decision they made together, and finally all entities that had suspicious or dubious annotations were collected and checked for consistency. We consider the result of this correcting process virtually be free of errors. Our best performing Named Entity Recognizer (NER) model attained an accuracy of 92.86{\%} F measure on the corpus.
In this paper we present on-going investigations on how complex syntactic annotation, combined with linguistic semantics, can possibly help in supporting the semi-automatic building of (shallow) ontologies from text by proposing an automated extraction of (possibly underspecified) semantic relations from linguistically annotated text.
We describe tools for the extraction of collocations not only in the form of word combinations, but also of data about the morphosyntactic properties of collocation candidates. Such data are needed for a detailed lexical description of collocations, and to support both their recognition in text and the generation of collocationally acceptable text. We describe the tool architecture, report on a case study based on noun+verb collocations, and we give a first rough evaluation of the data quality produced.
This paper describes the development of CiceroArabic, the first wide coverage named entity recognition (NER) system for Modern Standard Arabic. Capable of classifying 18 different named entity classes with over 85{\%} F, CiceroArabic utilizes a new 800,000-word annotated Arabic newswire corpus in order to achieve high performance without the need for hand-crafted rules or morphological information. In addition to describing results from our system, we show that accurate named entity annotation for a large number of semantic classes is feasible, even for very large corpora, and we discuss new techniques designed to boost agreement and consistency among annotators over a long-term annotation effort.
Growing privacy and security concerns mean there is an increasing need for data to be anonymized before being publically released. We present a module for anonymizing references implemented as part of the SQUAD tools for specifying and testing non-proprietary means of storing and marking-up data using universal (XML) standards and technologies. The tool is implemented on top of the GUITAR anaphoric resolver.
We present two collections of lexical items with idiosyncratic distribution. The collections document the behavior of German and English bound words (BW, such as English headway), i.e., words which can only occur in one expression (make headway). BWs are a problem for both general and idiomatic dictionaries since it is unclear whether they have an independent lexical status and to what extent the expressions in which they occur are typical idiomatic expressions. We propose a system which allows us to document the information about BWs from dictionaries and linguistic literature, together with corpus data and example queries for major text corpora. We present our data structure and point to other phraseologically oriented collections. We will also show differences between the German and the English collection.
We describe an architecture for the parallel construction of a tagger lexicon and an annotated reference corpus for the part-of-speech tagging of Nothern Sotho, a Bantu language of South Africa, for which no tagged resources have been available so far. Our tools make use of grammatical properties (morphological and syntactic) of the language. We use symbolic pretagging, followed by stochastic tagging, an architecture which proves useful not only for the bootstrapping of tagging resources, but also for the tagging of any new text. We discuss the tagset design, the tool architecture and the current state of our ongoing effort.
This paper describes the restructuring process of a large corpus of historical documents and the system architecture that is used for accessing it. The initial challenge of this process was to get the most out of existing material, normalizing the legacy markup and harvesting the inherent information using widely available standards. This resulted in a conceptual and technical restructuring of the formerly existing corpus. The development of the standardized markup and techniques allowed the inclusion of important new materials, such as original 16th and 17th century prints and manuscripts; and enlarged the potential user groups. On the technological side, we were grounded on the premise that open standards are the best way of making sure that the resources will be accessible even after years in an archive. This is a welcomed result in view of the additional consequence of the remodeled corpus concept: it serves as a repository for important historical documents, some of which had been preserved for 500 years in paper format. This very rich material can from now on be handled freely for linguistic research goals.
In this paper we present a novel resource for studying the semantics of verb relations. The resource is created by mixing sense relational knowledge enclosed in WordNet, frame knowledge enclosed in VerbNet and corpus knowledge enclosed in PropBank. As a result, a set of about 1000 frame pairs is made available. A frame pair represents a pair of verbs in a peculiar semantic relation accompanied with specific information, such as: the syntactic-semantic frames of the two verbs, the mapping among their thematic roles and a set of textual examples extracted from the PennTreeBank. We specifically focus on four relations: Troponymy, Causation, Entailment and Antonymy. The different steps required for the mapping are described in detail and statistics on resource mutual coverage are reported. We also propose a practical use of the resource for the task of Textual Entailment acquisition and for Question Answering. A first attempt for automate the mapping among verb arguments is also presented: early experiments show that simple techniques can achieve good results, up to 85{\%} F-Measure.
A series of different automatic query expansion techniques has been suggested in Information Retrieval. To estimate how suitable a document term is as an expansion term, the most popular of them use a measure of the frequency of the co-occurrence of this term with one or several query terms. The benefit of the use of the linguistic relations that hold between query terms is often questioned. If a linguistic phenomenon is taken into account, it is the phrase structure or lexical compound. We propose a technique that is based on the restricted lexical cooccurrence (collocation) of query terms. We use the knowledge on collocations formed by query terms for two tasks: (i) document relevance clustering done in the first stage of local query expansion and (ii) choice of suitable expansion terms from the relevant document cluster. In this paper, we describe the first task, providing evidence from first preliminary experiments on Spanish material that local relevance clustering benefits largely from knowledge on collocations.
Opinion mining (OM) is a recent subdiscipline at the crossroads of information retrieval and computational linguistics which is concerned not with the topic a document is about, but with the opinion it expresses. OM has a rich set of applications, ranging from tracking users opinions about products or about political candidates as expressed in online forums, to customer relationship management. In order to aid the extraction of opinions from text, recent research has tried to automatically determine the PNpolarity of subjective terms, i.e. identify whether a term that is a marker of opinionated content has a positive or a negative connotation. Research on determining whether a term is indeed a marker of opinionated content (a subjective term) or not (an objective term) has been instead much scarcer. In this work we describe SENTIWORDNET, a lexical resource in which each WORDNET synset sis associated to three numerical scores Obj(s), Pos(s) and Neg(s), describing how objective, positive, and negative the terms contained in the synset are. The method used to develop SENTIWORDNET is based on the quantitative analysis of the glosses associated to synsets, and on the use of the resulting vectorial term representations for semi-supervised synset classi.cation. The three scores are derived by combining the results produced by a committee of eight ternary classi.ers, all characterized by similar accuracy levels but different classification behaviour. SENTIWORDNET is freely available for research purposes, and is endowed with a Web-based graphical user interface.
This paper presents an approach to dialogue understanding based on a deep parsing and rule-based semantic analysis. Its performance in the semantic evaluation performed in the framework of the EVALDA/MEDIA campaign is encouraging. The MEDIA project aims to evaluate natural language understanding systems for French on a hotel reservation task (Devillers et al., 2004). For the evaluation, five participating teams had to produce an annotated version of the input utterances in compliance with a commonly agreed format (the MEDIA formalism). An approach based on symbolic processing was not straightforward given the conditions of the evaluation but we achieved a score close to that of statistical systems, without needing an annotated corpus. Despite the architecture has been designed for this campaign, exclusively dedicated to spoken dialogue understanding, we believe that our approach based on a LTAG parser and two ontologies can be used in real dialogue systems, providing quite robust speech understanding and facilities for interfacing with a dialogue manager and the application itself.
This paper introduces ongoing and current work within Internationalization (i18n) Activity, in the World Wide Web Consortium (W3C). The focus is on aspects of the W3C i18n Activity which are of benefit for the creation and manipulation of multilingual language resources. In particular, the paper deals with ongoing work concerning encoding, visualization and processing of characters; current work on language and locale identification; and current work on internationalization of markup. The main usage scenario is the design of multilingual corpora. This includes issues of corpus creation and manipulation.
This paper looks at a class of systems which pose severe problems in evaluation design for current conventional approaches to evaluation. After describing the two conventional evaluation paradigms: the functionality paradigm as typified by evaluation campaigns and the ISO inspired user-centred paradigm typified by the work of the EAGLES and ISLE projects, it goes on to outline the problems posed by the evaluation of systems which are designed to work in critical interaction with a human expert user and to work over vast amounts of data. These systems pose problems for both paradigms although for different reasons. The primary aim of this paper is to provoke discussion and the search for solutions. We have no proven solutions at present. However, we describe a programme of exploratory research on which we have already embarked, which involves ground clearing work which we expect to result in a deep understanding of the systems and users, a pre-requisite for developing a general framework for evaluation in this field.
This paper presents research on Greeklish, that is, a transliteration of Greek using the Latin alphabet, which is used frequently in Greek e-mail communication. Greeklish is not standardized and there are a number of competing conventions co-existing in communication, based on personal preferences regarding similarities between Greek and Latin letters in shape, sound, or keyboard position. Our research has led to the development of All Greek to me! the first automatic transliteration system that can cope with any type of Greeklish. In this paper we first present previous research on Greeklish, describing other approaches that have attempted to deal with the same problems. We then provide a brief description of our approach, illustrating the functional flowchart of our system and the main ideas that underlie it. We present measures of system performance, based on about a years worth of usage as a public web service, and preliminary research, based on the same corpus, on the use of Greeklish and the trends in preferred Latin-Greek letter mapping. We evaluate the consistency of different transliteration patterns among users as well as the within-user consistency based on coherent principles. Finally we outline planned future research to further understand the use of Greeklish and improve All Greek to me! to function reliably embedded in integrated communication platforms bridging e-mail to mobile telephony and ubiquitous connectivity.
Feature extraction is still a disputed issue for the recognition of emotions from speech. Differences in features for male and female speakers are a well-known problem and it is established that gender-dependent emotion recognizers perform better than gender-independent ones. We propose a way to improve the discriminative quality of gender-dependent features: The emotion recognition system is preceded by an automatic gender detection that decides upon which of two gender-dependent emotion classifiers is used to classify an utterance. This framework was tested on two different databases, one with emotional speech produced by actors and one with spontaneous emotional speech from a Wizard-of-Oz setting. Gender detection achieved an accuracy of about 90 {\%} and the combined gender and emotion recognition system improved the overall recognition rate of a gender-independent emotion recognition system by 2-4 {\%}.
In this paper we report on constructing a finite state automaton comprising automatically extracted terminology and significant collocation patterns from a training corpus of specialist news (Reuters Financial News). The automaton can be used to unambiguously identify sentiment-bearing words that might be able to make or break people, companies, perhaps even governments. The paper presents the emerging face of corpus linguistics where a corpus is used to bootstrap both the terminology and the significant meaning bearing patterns from the corpus. Much of the current content analysis software systems require a human coder to eyeball terms and sentiment words. Such an approach might yield very good quality results on small text collections but when confronted with a 40-50 million word corpus such an approach does not scale, and a large-scale computer-based approach is required. We report on the use of Grid computing technologies and techniques to cope with this analysis.
The WISPR project (``Welsh and Irish Speech Processing Resources'') has been building text-to-speech synthesis systems for Welsh and for Irish, as well as building links between the developers and potential users of the software. The Welsh half of the project has encountered various challenges, in the areas of the tokenisation of input text, the formatting of letter-to-sound rules, and the implementation of the ``greedy algorithm'' for text selection. The solutions to these challenges have resulted in various tools which may be of use to other developers using Festival for TTS for other languages. These resources are made freely available.
We describe the integration of some multilingual language resources in ontological descriptions, with the purpose of providing ontologies, which are normally using concept labels in just one (natural) language, with multilingual facility in their design and use in the context of Semantic Web applications, supporting both the semantic annotation of textual documents with multilingual ontology labels and ontology extraction from multilingual text sources.
In this paper we discuss five different corpora annotated forprotein names. We present several within- and cross-dataset proteintagging experiments showing that different annotation schemes severelyaffect the portability of statistical protein taggers. By means of adetailed error analysis we identify crucial annotation issues thatfuture annotation projects should take into careful consideration.
Many natural language processing tasks make use of a lexicon  typically the words collected from some annotated training data along with their associated properties. We demonstrate here the utility of corpora-independent lexicons derived from machine readable dictionaries. Lexical information is encoded in the form of features in a Conditional Random Field tagger providing improved performance in cases where: i) limited training data is made available ii) the data is case-less and iii) the test data genre or domain is different than that of the training data. We show substantial error reductions, especially on unknown words, for the tasks of part-of-speech tagging and shallow parsing, achieving up to 20{\%} error reduction on Penn TreeBank part-of-speech tagging and up to a 15.7{\%} error reduction for shallow parsing using the CoNLL 2000 data. Our results here point towards a simple, but effective methodology for increasing the adaptability of text processing systems by training models with annotated data in one genre augmented with general lexical information or lexical information pertinent to the target genre (or domain).
In this work, the creation of a large-scale Arabic to French statistical machine translation system is presented. We introduce all necessary steps from corpus aquisition, preprocessing the data to training and optimizing the system and eventual evaluation. Since no corpora existed previously, we collected large amounts of data from the web. Arabic word segmentation was crucial to reduce the overall number of unknown words. We describe the phrase-based SMT system used for training and generation of the translation hypotheses. Results on the second CESTA evaluation campaign are reported. The setting was inthe medical domain. The prototype reaches a favorable BLEU score of40.8{\%}.
Algorithms for automatic term extraction in a specific domain should consider at least two issues, namely Unithood and Termhood (Kageura, 1996). Unithood refers to the degree of a string to occur as a word or a phrase. Termhood (Chen Yirong, 2005) refers to the degree of a word or a phrase to occur as a domain specific concept. Unlike unithood, study on termhood is not yet widely reported. In classified corpora, the class information provides the cue to the nature of data and can be used in termhood calculation. Three algorithms are provided and evaluated to investigate termhood based on classified corpora. The three algorithms are based on lexicon set computing, term frequency and document frequency, and the strength of the relation between a term and its document class respectively. Our objective is to investigate the effects of these different termhood measurement features. After evaluation, we can find which features are more effective and also, how we can improve these different features to achieve the best performance. Preliminary results show that the first measure can effectively filter out independent terms or terms of general use.
In this paper we will focus on corpora as a resource for researching language processing for terminological purposes. Based on the TEI guide, we present the templates used to tag our TxtCeram corpus and its features when working with WordSmith, a text analysis tool. We present an experiment for studying the frequency of hyperonyms in the introduction section of texts, while testing WordSmiths suitability to work with our tagged corpus.
IMORPH{\=E} is a significantly extended version of MORPHE, a morphology description compiler. MORPHEs morphology description language is based on two constructs: 1) a morphological form hierarchy, whose nodes relate and differentiate surface forms in terms of the common and distinguishing inflectional features of lexical items; and 2) transformational rules, attached to leaf nodes of the hierarchy, which generate the surface form of an item from the base form stored in the lexicon. While MORPHEs approach to morphology description is intuitively appealing and was successfully used for generating the morphology of several European languages, its application to Modern Standard Arabic yielded morphological descriptions that were highly complex and redundant. Previous modifications and enhancements attempted to capture more elegantly and concisely different aspects of the complex morphology of Arabic, finding theoretical grounding in Lexeme-Based Morphology. Those extensions are being incorporated in a more flexible and less ad hoc fashion in IMORPHE, which retains the unique features of our previous work but embeds them in an inheritance-based framework in order to achieve even more concise and modular morphology descriptions and greater runtime efficiency, and lays the groundwork for IMORPHE to become an analyzer as well as a generator.
In this paper we discuss the way of evaluating topic segmentation, from mathematical measures on variously constructed reference corpus to contextual evaluation depending on different topic segmentation usages. We present an overview of the different ways of building reference corpora and of mathematically evaluating segmentation methods, and then we focus on three tasks which may involve a topic segmentation: text extraction, information retrieval and document presentation. We have developed two graphical interfaces, one for an intrinsic comparison, and the other one dedicated to an evaluation in an information retrieval context. These tools will be very soon distributed under GPL licences on the Technolangue project web page.
A major barrier to the development of accurate and realistic models of human emotions is the absence of multi-cultural / multilingual databases of real-life behaviours and of a federative and reliable annotation protocol. QUB and LIMSI teams are working towards the definition of an integrated coding scheme combining their complementary approaches. This multilevel integrated scheme combines the dimensions that appear to be useful for the study of real-life emotions: verbal labels, abstract dimensions and contextual (appraisal based) annotations. This paper describes this integrated coding scheme, a protocol that was set-up for annotating French and English video clips of emotional interviews and the results (e.g. inter-coder agreement measures and subjective evaluation of the scheme).
Translation In this work we investigate new possibilities for improving the quality of statistical machine translation (SMT) by applying word reorderings of the source language sentences based on Part-of-Speech tags. Results are presented on the European Parliament corpus containing about 700k sentences and 15M running words. In order to investigate sparse training data scenarios, we also report results obtained on about 1{\textbackslash}{\%} of the original corpus. The source languages are Spanish and English and target languages are Spanish, English and German. We propose two types of reorderings depending on the language pair and the translation direction: local reorderings of nouns and adjectives for translation from and into Spanish and long-range reorderings of verbs for translation into German. For our best translation system, we achieve up to 2{\textbackslash}{\%} relative reduction of WER and up to 7{\textbackslash}{\%} relative increase of BLEU score. Improvements can be seen both on the reordered sentences as well as on the rest of the test corpus. Local reorderings are especially important for the translation systems trained on the small corpus whereas long-range reorderings are more effective for the larger corpus.
Evaluation of automatic translation output is a difficult task. Several performance measures like Word Error Rate, Position Independent Word Error Rate and the BLEU and NIST scores are widely use and provide a useful tool for comparing different systems and to evaluate improvements within a system. However the interpretation of all of these measures is not at all clear, and the identification of the most prominent source of errors in a given system using these measures alone is not possible. Therefore some analysis of the generated translations is needed in order to identify the main problems and to focus the research efforts. This area is however mostly unexplored and few works have dealt with it until now. In this paper we will present a framework for classification of the errors of a machine translation system and we will carry out an error analysis of the system used by the RWTH in the first TC-STAR evaluation.
The primary aim of the project SENSEM (Sentence Semantics, BFF2003-06456) is the construction of a Lexical Data Base illustrating the syntactic and semantic behavior of each of the senses of the 250 most frequent verbs of Spanish. With this objective in mind, we are currently building an annotated corpus consisting of sentences extracted from the electronic version of the newspaper El Peri{\'o}dico de Catalunya, totalling approximately 1 million words, with 100 examples of each verb. By the time of the conference, we will be about to complete the annotation of 25,000 sentences, which means roughly a corpus of 800,000 words. Approximately 400,000 of them will have been revised. We expect to make the corpus publicly available by the end of 2006.
We present here an open-source software platform for the integration of speech translation components. This tool is useful to integrate into a common framework different automatic speech recognition, spoken language translation and text-to-speech synthesis solutions, as demonstrated in the evaluation of the European LC-STAR project, and during the development of the national ALIADO project. Gaia operates with great flexibility, and it has been used to obtain the text and speech corpora needed when performing speech translation. The platform follows a modular distributed approach, with a specifically designed extensible network protocol handling the communication with the different modules. A well defined and publicly available API facilitates the integration of existing solutions into the architecture. Completely functional audio and text interfaces together with remote monitoring tools are provided.
This article presents a set of morphological tools for six small endangered minority languages belonging to the Uralic language family, Udmurt, Komi, Eastern Mari, Northern Mansi, Tundra Nenets and Nganasan. Following an introduction to the languages, the two sets of tools used in the project (MorphoLogic's Humor tools and the Xerox Finite State Tool) are described and compared. The article is concluded by a comparison of the six computational morphologies.
The newly founded European Centre of Excellence for Speech Synthesis (ECESS) is an initiative to promote the development of the European research area (ERA) in the field of Language Technology. ECESS focuses on the great challenge of high-quality speech synthesis which is of crucial importance for future spoken-language technologies. The main goals of ECESS are to achieve the critical mass needed to promote progress in TTS technology substantially, to integrate basic research know-how related to speech synthesis and to attract public and private funding. To this end, a common system architecture based on exchangeable modules supplied by the ECESS members is to be established. The XML-based interface that connects these modules is the topic of this paper.
We present a formalism capable of searching and optionally replacing forests of subtrees within labelled trees. In particular, the formalism is developed to process linguistic treebanks. When used as a substitution tool, the interpreter processes rewrite rules consisting of left and right side. The left side specifies a forest of subtrees to be searched for within a tree by imposing a set of constraints encoded as a query formula. The right side contains the respective substitutions for these subtrees. In the search mode only the left side is present. The formalism is fully implemented. The performance of the implemented tool allows to process even large linguistic corpora in acceptable time. The main contribution of the presented work consists of the expressiveness of the query formula, in the elegant and intuitive way the rules are written (and their easy reversibility), and in the performance of the implemented tool.
We describe and compare different methods for creating a dictionary of words with their corresponding semantic orientation (SO). We tested how well different dictionaries helped determine the SO of entire texts. To extract SO for each individual word, we used a common method based on pointwise mutual information. Mutual information between a set of seed words and the target words was calculated using two different methods: a NEAR search on the search engine Altavista (since discontinued); an AND search on Google. These two dictionaries were tested against a manually annotated dictionary of positive and negative words. The results show that all three methods are quite close, and none of them performs particularly well. We discuss possible further avenues for research, and also point out some potential problems in calculating pointwise mutual information using Google.
This poster is a preliminary report of our experiments for detecting semantically shifted terms between different domains for the purposes of new concept extraction. A given term in one domain may represent a different concept in another domain. In our approach, we quantify the degree of similarity of words between different domains by measuring the degree of overlap in their domain-specific semantic spaces. The domain-specific semantic spaces are defined by extracting families of syntactically similar words, i.e. words that occur in the same syntactic context. Our method does not rely on any external resources other than a syntactic parser. Yet it has the potential to extract semantically shifted terms between two different domains automatically while paying close attention to contextual information. The organization of the poster is as follows: Section 1 provides our motivation. Section 2 provides an overview of our NLP technology and explains how we extract syntactically similar words. Section 3 describes the design of our experiments and our method. Section 4 provides our observations and preliminary results. Section 5 presents some work to be done in the future and concluding remarks.
In this paper, process of the Czech Lombard Speech Database (CLSD'05) acquisition is presented. Feature analyses have proven a strong appearance of Lombard effect in the database. In the small vocabulary recognition task, significant performance degradation was observed for the Lombard speech recorded in the database. Aim of this paper is to describe the hardware platform, scenarios and recording tool used for the acquisition of CLSD'05. During the database recording and processing, several difficulties were encountered. The most important question was how to adjust the level of speech feedback for the speaker. A method for minimization of the speech attenuation introduced to the speaker by headphones is proposed in this paper. Finally, contents and corpus of the database are presented to outline it's suitability for analysis and modeling of Lombard effect. The whole CLSD'05 database with a detailed documentation is now released for public use.
This paper is concerned with the fundamentals of multidimensional dialogue act annotation, i.e. with what it means to annotate dialogues with information about the communicative acts that are performed with the utterances, taking various 'dimensions' into account. Two ideas seem to be prevalent in the literature concerning the notion of dimension: (1) dimensions correspond to different types of information; and (2) a dimension is formed by a set of mutually exclusive tags. In DAMSL, for instance, the terms dimension and layer are used sometimes in the sense of (1) and sometimes in that of (2). We argue that being mutually exclusive is not a good criterion for a set of dialogue act types to constitute a dimension, even though the description of an object in a multidimensional space should never assign more than one value per dimension. We define a dimension of dialogue act annotation as an aspect of participating in a dialogue that can be addressed independently by means of dialogue acts. We show that DAMSL dimensions such as Info-request, Statement, and Answer do not qualify as proper dimensions, and that the communicative functions in these categories do not fall in any specific dimension, but should be considered as general-purpose in the sense that they can be used in any dimension. We argue that using the notion of dimension that we propose, a multidimensional taxonomy of dialogue acts emerges that optimally supports multidimensional dialogue act annotation.
We present here the choices which were made within the framework of three oral corpora projects: Socio-linguistics studies on Orleans (ESLO), Phonology of the Contemporary French (PFC), the Archivage corpus of the LACITO lab. This comparative presentation of three corpora of audio linguistic resources comes from a analysis about the options the project have to operate to describe them for discovery purposes and to compare the contents. The aim is to illustrate the interest to think the interoperability and the methodology of codings and the metadata. Through this step, we want to simplify the technical creation of audio corpora and thus the constitution of linguistic resources, usable by enlarged academic and industrial communities.
This paper presents an on-going project aiming at enhancing the OPAC (Online Public Access Catalog) search system of the Library of the Free University of Bozen-Bolzano with multilingual access. The Multilingual search system (MUSIL), we have developed, integrates advanced linguistic technologies in a user friendly interface and bridges the gap between the world of free text search and the world of conceptual librarian search. In this paper we present the architecture of the system, its interface and preliminary evaluations of the precision of the search results.
This paper describes a featurized functional dependency corpus automatically derived from the Penn Treebank. Each word in the corpus is associated with over three dozen features describing the functional syntactic structure of a sentence as well as some shallow morphology. The corpus was created for use in probabilistic surface generation, but could also be useful as a resource for the study of English and the development of other NLP applications.
One of the most critical components in the process of building automatic speech recognition (ASR) capabilities for a new language is the lexicon, or pronouncing dictionary. For practical reasons, it is desirable to manually create only the minimal lexicon using available native-speaker phonetic expertise and, then, use the resulting seed lexicon for machine learning based induction of a high-quality letter-to-sound (L2S) model for generation of pronunciations for the remaining words of the language. This paper examines the viability of this scenario, specifically investigating three possible strategies for selection of lexemes (words) for manual transcription  choosing the most frequent lexemes of the language, choosing lexemes randomly, and selection of lexemes via an information theoretic diversity measure. The relative effectiveness of these three strategies is evaluated as a function of the number of lexemes to be transcribed to create a bootstrapping lexicon. Generally, the newly developed orthographic diversity based selection strategy outperforms the others for this scenario where a limited number of lexemes can be transcribed. The experiments also provide generally useful insight into expected L2S accuracy sacrifice as a function of decreasing training set size.
As part of evaluating a summary automati-cally, it is usual to determine how much of the contents of one or more human-produced ideal summaries it contains. Past automated methods such as ROUGE compare using fixed word ngrams, which are not ideal for a variety of reasons. In this paper we describe a framework in which summary evaluation measures can be instantiated and compared, and we implement a specific evaluation method using very small units of content, called Basic Elements that address some of the shortcomings of ngrams. This method is tested on DUC 2003, 2004, and 2005 systems and produces very good correlations with human judgments.
Although WordNets have been developed for a number of languages, no attempts to construct a Japanese WordNet have been known to exist. Taking this into account, we launched a project to automatically translate the Princeton WordNet into Japanese by a method of unsupervised word-sense disambiguation using bilingual comparable corpora. The method we propose aligns English word associations with those in Japanese and iteratively calculates a correlation matrix of Japanese translations of an English word versus its associated words. It then determines the Japanese translation for the English word in a synset by calculating scores for translation candidates according to the correlation matrix and the associated words appearing in the gloss appended to the synset. This method is not robust because a gloss only contains a few associated words. To overcome this difficulty, we extended the method so that it retrieves texts by using the gloss as a query and uses the retrieved texts as well as the gloss to calculate scores for translation candidates. A preliminary experiment using Wall Street Journal and Nihon Keizai Shimbun corpora demonstrated that the proposed method is promising for constructing a Japanese WordNet.
This paper describes a system for extracting typed dependency parses of English sentences from phrase structure parses. In order to capture inherent relations occurring in corpus texts that can be critical in real-world applications, many NP relations are included in the set of grammatical relations used. We provide a comparison of our system with Minipar and the Link parser. The typed dependency extraction facility described here is integrated in the Stanford Parser, available for download.
The importance of frequency for phonological phenomena has long been noticed in the literature. However, frequency information available for phonological units in Portuguese is scarce, non-replicable, corpus dependent, and hard to obtain due to the non-existence of a free tool for public use. This paper describes FreP, a new electronic tool that provides frequency counts of phonological units at the word-level and below from Portuguese written text: namely, major classes of segments, syllables and syllable types, phonological clitics, clitic types and size, prosodic words and their shape, word stress location, and syllable type by position within the word and/or status relative to word stress. Useful applications of FreP in general linguistics, phonology, language acquisition and development, speech evaluation and therapy are also described. Forthcoming extensions of the tool include the ability to extract frequency information for different varieties of Portuguese, Brazilian Portuguese in particular, and the ability to provide a SAMPA output from the written text, together with the frequency of segmental features, like manner, place of articulation and laryngeal features. Updated information on FreP can be found at http://www.fl.ul.pt/LaboratorioFonetica/FreP.
The last decade has seen a large increase in the number of available corpus query systems. Some of these are optimized for a particular kind of linguistic annotation (e.g., time-aligned, treebank, word-oriented, etc.). In this paper, we report on our own corpus query system, called Emdros. Emdros is very generic, and can be applied to almost any kind of linguistic annotation using almost any linguistic theory. We describe Emdros and its query language, showing some of the benfits that linguists can derive from using Emdros for their corpora. We then describe the underlying database model of Emdros, and show how two corpora can be imported into the system. One of the two is a parallel corpus of Hungarian and English (the Hunglish corpus), while the other is a treebank of German (the TIGER Corpus). In order to evaluate the performance of Emdros, we then run some performance tests. It is shown that Emdros has extremely good performance on small corpora (less than 1 million words), and that it scales well to corpora of many millions of words.
Recent work in several computational linguistics (CL) applications (especially question answering) has shown the value of semantics (in fact, many people argue that the current performance ceiling experienced by so many CL applications derives from their inability to perform any kind of semantic processing). But the absence of a large semantic information repository that provides representations for sentences prevents the training of statistical CL engines and thus hampers the development of such semantics-enabled applications. This talk refers to recent work in several projects that seek to annotate large volumes of text with shallower or deeper representations of some semantic phenomena. It describes one of the essential problemscreating, managing, and annotating (at large scale) the meanings of words, and outlines the Omega ontology, being built at ISI, that acts as term repository. The talk illustrates how one can proceed from words via senses to concepts, and how the annotation process can help verify good concept decisions and expose bad ones. Much of this work is performed in the context of the OntoNotes project, joint with BBN, the Universities of Colorado and Pennsylvania, and ISI, that is working to build a corpus of about 1M words (English, Chinese, and Arabic), annotated for shallow semantics, over the next few years.
The goal of this paper is (1) to illustrate a specific procedure for merging different monolingual lexicons, focussing on techniques for detecting and mapping equivalent lexical entries, and (2) to sketch a production model that enables one to obtain lexical resources via unification of existing data. We describe the creation of a Unified Lexicon (UL) from a common sample of the Italian PAROLE-SIMPLE-CLIPS phonological lexicon and of the Italian LCSTAR pronunciation lexicon. We expand previous experiments carried out at ILC-CNR: based on a detailed mechanism for mapping grammatical classifications of candidate UL entries, a consensual set of Unified Morphosyntactic Specifications (UMS) shared by lexica for the written and spoken areas is proposed. The impact of the UL on cross-validation issues is analysed: by looking into conflicts, mismatches and diverging classifications can be detected in both resources. The work presented is in line with the activities promoted by ELRA towards the development of methods for packaging new language resources by combining independently created resources, and was carried out as part of the ELRA Production Committee activities. ELRA aims to exploit the UL experience to carry out such merging activities for resources available on the ELRA catalogue in order to fulfill the users' needs.
In this contribution we present a new methodology to compile large language resources for domain-specific taxonomy learning. We describe the necessary stages to deal with the rich morphology of an agglutinative language, i.e. Korean, and point out a second order machine learning algorithm to unveil term similarity from a given raw text corpus. The language resource compilation described is part of a fully automatic top-down approach to construct taxonomies, without involving the human efforts which are usually required.
The paper describes a general method (as well as its implementation and evaluation) for deriving mapping systems for different tagsets available in existing training corpora (gold standards) for a specific language. For each pair of corpora (tagged with different tagsets), one such mapping system is derived. This mapping system is then used to improve the tagging of each of the two corpora with the tagset of the other (this process will be called cross-tagging). By reapplying the algorithm to the newly obtained corpora, the accuracy of the underlying training corpora can also be improved. Furthermore, comparing the results with the gold standards makes it possible to assess the distributional adequacy of various tagsets used in processing the language in case. Unlike other methods, such as those reported in (Brants, 1995) or (Tufis {\&} Dragomirescu, 2004), which assume a subsumption relation between the considered tagsets, and as such they aim at minimizing the tagsets by eliminating the feature-value redundancy, this method is applicable for completely unrelated tagsets. Although the experiments were focused on morpho-syntactic (POS) tagging, the method is applicable to other types of tagging as well.
The paper briefly describes the RoCo project and, in details, one of its first outcomes, the RoCo-News corpus. RoCo-News is a middle-sized journalistic corpus of Romanian, abundant in proper names, numerals and named entities. The initially raw text was previously segmented with MtSeg segmenter, then POS annotated with TNT tagger. RoCo-News was further lemmatized and validated. Because of limited human resources, time constraints and the dimension of the corpus, hand validation of each individual token was out of question. The validation stage required a coherent methodology for automatically identifying as many POS annotation and lemmatization errors as possible. The hand validation process was focused on these automatically spotted possible errors. This methodology relied on three main techniques for automatic detection of potential errors: 1. when lemmatizing the corpus, we extracted all the triples that were not found in the word-form lexicon; 2. we checked the correctness of POS annotation for closed class lexical categories, technique described by (Dickinson {\&} Meurers, 2003); 3. we exploited the hypothesis (Tufiº, 1999) according to which an accurately tagged text, re-tagged with the language model learnt from it (biased evaluation) should have more than 98{\%} tokens identically tagged.
In this paper, we present and evaluate a new method to convert Constraint Grammar (CG) parses of running text into Constituent Treebanks. The conversion is two-step - first a grammar-based method is used to bridge the gap between raw CG annotation and full dependency structure, then phrase structure bracketing and non-terminal nodes are introduced by clustering sister dependents, effectively building one syntactic treebank on top of another. The method is compared with another approach (Bick 2003-2), where constituent structures are arrived at by employing a function-tag based Phrase Structure Grammar (PSG). Results are evaluated on a small reference corpus for both raw and revised CG input, with bracketing F-Scores of 87.5{\%} for raw text and 97.1{\%} for revised CG input, and a raw text edge label accuracy of 95.9{\%} for forms and 86{\%} for functions, or 99.7{\%} and 99.4{\%}, respectively, for revised CG. By applying the tools to the CG-only part of the Danish Arboretum treebank we were able to increase the size of the treebank by 86{\%}, from 197.400 to 367.500 words.
The aligning and merging of ontologies with overlapping information are actual one of the most active domain of investigation in the Semantic Web community. Multilingual lexical ontologies thesauri are fundamental knowledge sources for most NLP projects addressing multilinguality. The alignment of multilingual lexical knowledge sources has various applications ranging from knowledge acquisition to semantic validation of interlingual equivalence of presumably the same meaning express in different languages. In this paper, we present a general method for aligning ontologies, which was used to align a conceptual thesaurus, lexicalized in 20 languages with a partial version of it lexicalized in Romanian. The objective of our work was to align the existing terms in the Romanian Eurovoc to the terms in the English Eurovoc and to automatically update the Romanian Eurovoc. The general formulation of the ontology alignment problem was set up along the lines established by Heterogeneity group of the KnowledgeWeb consortium, but the actual case study was motivated by the needs of a specific NLP project.
Phrase alignment is the task that requires the constituent phrases of two halves of a bitext to be aligned. In order to align phrases, one must discover them first and this article presents a method of aligning phrases that are discovered automatically. Here, the notion of a 'phrase' will be understood as being given by a subtree of a dependency-like structure of a sentence called linkage. To discover phrases, we will make use of two distinct, language independent methods: the IBM-1 model (Brown et al., 1993) adapted to detect linkages and Constrained Lexical Attraction Models (Ion {\&} Barbu Mititelu, 2006). The methods will be combined and the resulted model will be used to annotate the bitext. The accuracy of phrase alignment will be evaluated by obtaining word alignments from link alignments and then by checking the F-measure of the latter word aligner.
Sentence alignment is a task that requires not only accuracy, as possible errors can affect further processing, but also requires small computation resources and to be language pair independent. Although many implementations do not use translation equivalents because they are dependent on the language pair, this feature is a requirement for the accuracy increase. The paper presents a hybrid sentence aligner that has two alignment iterations. The first iteration is based mostly on sentences length, and the second is based on a translation equivalents table estimated from the results of the first iteration. The aligner uses a Support Vector Machine classifier to discriminate between positive and negative examples of sentence pairs.
In this paper we discuss a rule-based approach to chunking implemented using the LT-XML2 and LT-TTT2 tools. We describe the tools and the pipeline and grammars that have been developed for the task of chunking. We show that our rule-based approach is easy to adapt to different chunking styles and that the mark-up of further linguistic information such as nominal and verbal heads can be added to the rules at little extra cost. We evaluate our chunker against the CoNLL 2000 data and discuss discrepancies between our output and the CoNLL mark-up as well as discrepancies within the CoNLL data itself. We contrast our results with the higher scores obtained using machine learning and argue that the portability and flexibility of our approach still make it a more practical solution.
The task of identifying the language in which a given document (ranging from a sentence to thousands of pages) is written has been relatively well studied over several decades. Automated approachesto written language identification are used widely throughout research and industrial contexts, over both oral and written source materials. Despite this widespread acceptance, a review of previous research in written language identification reveals a number of questions which remain openand ripe for further investigation.
This paper describes automatic terminology intelligibility estimation for readership-oriented technical writing. We assume that the term frequency weighted by the types of documents can be an indicator of the term intelligibility for a certain readership. From this standpoint, we analyzed the relationship between the following: average intelligibility levels of 46 technical terms that were rated by about 120 laymen; numbers of documents that an Internet search
The aim of the Nordic SYMBERED project - funded by NUH (the Nordic Development Centre for Rehabilitation Technology) - is to develop a user friendly editing tool that makes use of concept coding to produce web pages with flexible graphical symbol support targeted towards people with Augmentative and Alternative Communication (AAC) needs. Documents produced with the editing tool will be in XML/XHTML format, well suited for publishing on the Internet. These documents will then contain natural language text, such as Swedish or English. Some, or all, of the words in the text will be marked with a concept code defining its meaning. The coded words/concepts may then easily be represented by alternative kinds of graphical symbols and by additional text representations in alternative languages. Thus, within one web document created by the author with the SYMBERED tool, one symbol language can easily be swapped for another. This means that a Bliss and a PCS symbol user can each have his/her preferred kind of symbol support. The SYMBERED editing tool will initially support a limited vocabulary in four to five Nordic languages plus English, and three to four symbol systems, with built-in extensibility to cover more languages and symbol systems.
In this paper, we report on methods to detect and repair lexical errors for deep grammars. The lack of coverage has for long been the major problem for deep processing. The existence of various errors in the hand-crafted large grammars prevents their usage in real applications. The manual detection and repair of errors requires asignificant amount of human effort. An experiment with the British National Corpus shows about 70{\%} of the sentences contain unknownword(s) for the English Resource Grammar. With the help of error mining methods, many lexical errors are discovered, which cause a large part of the parsing failures. Moreover, with a lexical type predictor based on a maximum entropy model, new lexical entries are automatically generated. The contribution of various features for the model is evaluated. With the disambiguated full parsing results, the precision of the predictor is enhanced significantly.
There has been a lot of psychological researches on emotion and nonverbal communication. Yet, these studies were based mostly on acted basic emotions. This paper explores how manual annotation and image processing can cooperate towards the representation of spontaneous emotional behaviour in low resolution videos from TV. We describe a corpus of TV interviews and the manual annotations that have been defined. We explain the image processing algorithms that have been designed for the automatic estimation of movement quantity. Finally, we explore how image processing can be used for the validation of manual annotations.
In this paper we describe WS4LR, the workstation for lexical resources, a software tool developed within the Human Language Technology Group at the Faculty of Mathematics, University of Belgrade. The tool is aimed at manipulating heterogeneous lexical resources, and the need for such a tool came from the large volume of resources the Group has developed in the course of many years and within different projects. The tool handles morphological dictionaries, wordnets, aligned texts and transducers equally and has already proved very useful for various tasks. Although it has so far been used mainly for Serbian, WS4LR is not language dependent and can be successfully used for resources in other languages provided that they follow the described formats and methodologies. The tool operates on the .NET platform and runs on a personal computer under Windows 2000/XP/2003 operating system with at least 256MB of internal memory.
Lexical classifications have proved useful in supporting various natural language processing (NLP) tasks. The largest verb classification for English is Levin's (1993) work which defined groupings of verbs based on syntactic properties. VerbNet - the largest computational verb lexicon currently available for English - provides detailed syntactic-semantic descriptions of Levin classes. While the classes included are extensive enough for some NLP use, they are not comprehensive. Korhonen and Briscoe (2004) have proposed a significant extension of Levin's classification which incorporates 57 novel classes for verbs not covered (comprehensively) by Levin. This paper describes the integration of these classes into VerbNet. The result is the most extensive Levin-style classification for English verbs which can be highly useful for practical applications.
In this paper we describe the structure and development of the Brandeis Semantic Ontology (BSO), a large generative lexicon ontology and lexical database. The BSO has been designed to allow for more widespread access to Generative Lexicon-based lexical resources and help researchers in a variety of computational tasks. The specification of the type system used in the BSO largely follows that proposed by the SIMPLE specification (Busa et al., 2001), which was adopted by the EU-sponsored SIMPLE project (Lenci et al., 2000).
When dialogue models are evaluated today, this is normally done by using some evaluation method to collect data, often involving users interacting with the system model, and then subsequently analysing the collected data. We present a tool called DialogDesigner that enables automatic evaluation performed directly on the dialogue model and that does not require any data collection first. DialogDesigner is a tool in support of rapid design and evaluation of dialogue models. The first version was developed in 2005 and enabled developers to create an electronic dialogue model, get various graphical views of the model, run a Wizard-of-Oz (WOZ) simulation session, and extract different presentations in HTML. The second version includes extensions in terms of support for automatic dialogue model evaluation. Various aspects of dialogue model well-formedness can be automatically checked. Some of the automatic analyses simply perform checks based on the state and transition structure of the dialogue model while the core part are based on act-topic annotation of prompts and transitions in the dialogue model and specification of act-topic patterns. This paper focuses on the version 2 extensions.
This paper describes one phase of a large-scale machine translation (MT) quality assurance project. We explore a novel approach to discriminating MT-unsuitable source sentences by predicting the expected quality of the output. The resources required include a set of source/MT sentence pairs, human judgments on the output, a source parser, and an MT system. We extract a number of syntactic, semantic, and lexical features from the source sentences only and train a classifier that we call the Syntactic, Semantic, and Lexical Model (SSLM) (cf. Gamon et al., 2005; Liu {\&} Gildea, 2005; Rajman {\&} Hartley, 2001). Despite the simplicity of the approach, SSLM scores correlate with human judgments and can help determine whether sentences are suitable or unsuitable for translation by our MT system. SSLM also provides information about which source features impact MT quality, connecting this work with the field of controlled language (CL) (cf. Reuther, 2003; Nyberg {\&} Mitamura, 1996). With a focus on the input side of MT, SSLM differs greatly from evaluation approaches such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002) and METEOR (Banerjee {\&} Lavie, 2005) in that these other systems compare MT output with reference sentences for evaluation and do not provide feedback regarding potentially problematic source material. Our method bridges the research areas of CL and MT evaluation by addressing the importance of providing MT-suitable English input to enhance output quality.
The recognition of named entities is now a well-developed area, with a range of symbolic and machine learning techniques that deliver high accuracy extraction and categorisation of a variety of entity types. However, there are still some named entity phenomena that present problems for existing techniques; in particular, relatively little work has explored the disambiguation of conjunctions appearing in candidate named entity strings. We demonstrate that there are in fact four distinct uses of conjunctions in the context of named entities; we present some experiments using machine-learned classifiers to disambiguate the different uses of the conjunction, with 85{\%} of test examples being correctly classified.
The TST Centre manages a broad collection of Dutch digital language resources. It is an initiative of the Dutch Language Union (Nederlandse Taalunie), and is meant to reinforce research in the area of language and speech technology. It does this by stimulating the reuse of these language resources. The TST Centre keeps these resources up to date, facilitates their availability, and offers services such as providing information, documentation, online access, offering catalogues, custom-made data, etc. Also, the TST Centre strives for a uniformised, if not standardised, treatment of language resources of the same nature. A well-thought, structured administration system is needed to manage the various language resources, their updates, derived products, IPR, user administration, etc. We will discuss the organisation, tasks and services of the TST Centre, and the language resources it maintains. Also, we will look into practical data management solutions, IPR issues, and our activities in standardisation and linking language resources.
The MULINCO project (MUltiLINgual Corpus of the University of Copenhagen) started early 2005. The purpose of this cross-disciplinary project is to create a corpus platform for education and research in monolingual and translation studies. The project covers two main types of corpus texts: literary and non-literary. The platform is being developed using available tools as far as possible, and integrating them in a very open architecture. In this paper we describe the current status and future developments of both the text and tool side of the corpus platform, and we show some examples of student exercises taking advantage of tagged and aligned texts.
In this paper we present LeXFlow, a web application framework where lexicons already expressed in standardised format semi-automatically interact by reciprocally enriching themselves. LeXFlow is intended for, on the one hand, paving the way to the development of dynamic multi-source lexicons; and on the other, for fostering the adoption of standards. Borrowing from techniques used in the domain of document workflows, we model the activity of lexicon management as a particular case of workflow instance, where lexical entries move across agents and become dynamically updated. To this end, we have designed a lexical flow (LF) corresponding to the scenario where an entry of a lexicon A becomes enriched via basically two steps. First, by virtue of being mapped onto a corresponding entry belonging to a lexicon B, the entry(LA) inherits the semantic relations available in lexicon B. Second, by resorting to an automatic application that acquires information about semantic relations from corpora, the relations acquired are integrated into the entry and proposed to the human encoder. As a result of the lexical flow, in addition, for each starting lexical entry(LA) mapped onto a corresponding entry(LB) the flow produces a new entry representing the merging of the original two.
In this paper, we investigate whether it is possible to bootstrap a named entity tagger for textual databases by exploiting the database structure to automatically generate domain and database-specific gazetteer lists. We compare three tagging strategies: (i) using the extracted gazetteers in a look-up tagger, (ii) using the gazetteers to automatically extract training data to train a database-specific tagger, and (iii) using a generic named entity tagger. Our results suggest that automatically built gazetteers in combination with a look-up tagger lead to a relatively good performance and that generic taggers do not perform particularly well on this type of data.
Speech synthesis or text-to-speech (TTS) systems are currently available for a number of the world's major languages, but for thousands of other, unsupported, languages no such technology is available. While awaiting the development of such technology, we propose using an existing TTS system for a major language (the base language, BL) to ``fake'' TTS for an unsupported language (the target language, TL). This paper describes the factors which determine the choice of a suitable BL for a given TL, and describe an experiment with a fake Somali TTS system evaluated in the real-life situation of a doctorpatient dialogue. 28 Somali participants were asked to judge the comprehensibility of 25 short Somali sentences recorded with a German TTS system. Results suggest that ``faking it'' provides reasonable stop-gap TTS for unsupported languages.
In an age when demand for innovative and motivating language teaching methodologies is at a very high level, TREAT - the Trilingual REAding Tutor - combines the most advanced natural language processing (NLP) techniques with the latest second and third language acquisition (SLA/TLA) research in an intuitive and user-friendly environment that has been proven to help adult learners (native speakers of L1) acquire reading skills in an unknown L3 which is related to (cognate with) an L2 they know to some extent. This corpus-based methodology relies on existing linguistic resources, as well as materials that are easy to assemble, and can be adapted to support other pairs of L2-L3 related languages, as well. A small evaluation study conducted at the Leeds University Centre for Translation Studies indicates that, when using TREAT, learners feel more motivated to study an unknown L3, acquire significant linguistic knowledge of both the L3 and L2 rapidly, and increase their performance when translating from L3 into L1.
This paper introduces a number theoretical and practical issues related to the Syllabus. Syllabusis a multi-lingua ontology based tool, designed to improve the applications of the European Directives in the various European countries.
KUNSTI is the Norwegian national language technology programme, running 2001-2006 inclusive. The goal of the programme is to boost Norwegian language technology research. In this paper we describe the background, the objectives, the methodology applied in the management of the programme, the projects selected, and our first conclusions. We also describe national programmes form Sweden, France and Germany and compare objectives and methods.
The paper presents an evaluation of maxent POS disambiguation systems that incorporate an open source morphological analyzer to constrain the probabilistic models. The experiments show that the best proposed architecture, which is the first application of the maximum entropy framework in a Hungarian NLP task, outperforms comparable state of the art tagging methods and is able to handle out of vocabulary items robustly, allowing for efficient analysis of large (web-based) corpora.
This paper describes a range of experiments using empirical methods to adapt theWordNet noun ontology for specific use in the biomedical domain. Our basic technique is to extract relationships between terms using the Ohsumed corpus, a large collection of abstracts from PubMed, and to compare the relationships extracted with those that would be expected for medical terms, given the structure of the WordNet ontology. The linguistic methods involve the use of a variety of lexicosyntactic patterns that enable us to extract pairs of coordinate noun terms, and also related groups of adjectives and nouns, using Markov clustering. This enables us in many cases to analyse ambiguous words and select the correct meaning for the biomedical domain. While results are often encouraging, the paper also highlights evident problems and drawbacks with the method, and outlines suggestions for future work.
In this paper we present an application of AGTK to a corpus of spoken Italian annotated at many different linguistic levels. The work consists of two parts: a) the presentation of AG-SpIt, a toolkit devoted to corpus data management that we developed according to AGTK proposals; b) the presentation of corpus structure together with some examples and results of cross-level linguistic analyses obtained querying the database (SpIt-MDb). As this work is still an ongoing investigation, results must be considered preliminary, as a demo illustrating the potentiality of the tool and the advantages it introduces to validate linguistic theories and annotation systems. Currently, SpIt-MDb is a linguistic resource under development; it represents one of the first attempts to create an Italian corpus labelled at various linguistic levels (from acoustic/sub-phonetic, to textual/pragmatic ones) which can be queried in the interrelations among levels.
In this paper we discuss the explicit representation of character features pertaining to written language resources, which we argue are critically necessary in the long term of archiving language data. Much focus on the creation of language resources and their associated preservation is at the level of the corpus itself; however it is generally accepted that long term interpretation of these language resources requires more than a best practice data format. In particular, where language resources are created in linguistic fieldwork, and especially for minority languages, the need for preservation not only of the resource itself, but of additional metadata which allows for the resource to be accurately interpreted in the future is becoming a topic of research in itself. In this paper we extend earlier work on semantically based character decomposition to include representation of character properties in a variety of models, and a mechanism for exploiting these properties through queries.
Parsing, one of the more successful areas of Natural Language Processing has mostly been concerned with syntactic structure. Though uncovering the syntactic structure of sentences is very important, in many applications a meaningrepresentation for the input must be derived as well. We report on PrincPar, a parser that builds full meaning representations. It integrates LCFLEX, a robust parser, with alexicon and ontology derived from two lexical resources, VerbNet and CoreLex that represent the semantics of verbs and nouns respectively. We show that these two different lexical resources that focus on verbs and nouns can be successfully integrated. We report parsing results on a corpus of instructional text and assess the coverage of those lexical resources. Our evaluation metric is the number of verb frames that are assigned a correct semantics: 72.2{\%} verb frames are assigned a perfect semantics, and another 10.9{\%} are assigned a partially correctsemantics. Our ultimate goal is to develop a (semi)automatic method to derive domain knowledge from instructional text, in the form of linguistically motivated action schemes.
We developed a method for automatically distinguishing the machine-translatable and non-machine-translatable parts of a given sentence for a particular machine translation (MT) system. They can be distinguished by calculating the similarity between a source-language sentence and its back translation for each part of the sentence. The parts with low similarities are highly likely to be non-machine-translatable parts. We showed that the parts of a sentence that are automatically distinguished as non-machine-translatable provide useful information for paraphrasing or revising the sentence in the source language to improve the quality of the translation by the MT system. We also developed a method of providing knowledge useful to effectively paraphrasing or revising the detected non-machine-translatable parts. Two types of knowledge were extracted from the EDR dictionary: one for transforming a lexical entry into an expression used in the definition and the other for conducting the reverse paraphrasing, which transforms an expression found in a definition into the lexical entry. We found that the information provided by the methods helped improve the machine translatability of the originally input sentences.
Results are presented of an ongoing project of the Dutch TST-centre for language and speech technology aiming at linking of various lexical databases. The project involves four Dutch monolingual lexicons: WlNT05, e-Lex, RBN and RBBN. These databases differ in organisational structure and content. To enable linkage between these lexicons, we developed a common feature value set and a common organisational structure. Both are based upon existing standards for the creation and reusability of lexicons: the Lexical Markup Framework and the EAGLES standard. Examples of the content and structure of each of the lexical databases are presented in their original form. Also, the structure and content is shown when mapped onto the common framework and feature value set. Thus, the commonalities and the complementarity of the lexical databases are more readily apparent. Besides, this elaboration of the databases opens up the opportunity for mutual enrichment.
We describe a number of experiments carried out to address the problem of creating summaries from multiple sources in multiple languages. A centroid-based sentence extraction system has been developed which decides the content of the summary using texts in different languages and uses sentences from English sources alone to create the final output. We describe the evaluation of the system in the recent Multilingual Summarization Evaluation MSE 2005 using the pyramids and ROUGE methods.
Lexical networks such as WordNet are known to have a lack of topical relations although these relations are very useful for tasks such as text summarization or information extraction. In this article, we present a method for automatically building from a large corpus a lexical network whose relations are preferably topical ones. As it does not rely on resources such as dictionaries, this method is based on self-bootstrapping: a network of lexical cooccurrences is first built from a corpus and then, is filtered by using the words of the corpus that are selected by the initial network. We report an evaluation about topic segmentation showing that the results got with the filtered network are the same as the results got with the initial network although the first one is significantly smaller than the second one.
In this paper, we describe the role and the use of WORDNET as an external lexical resource in a methodology for matching hierarchical classification schemas. The main difference between our methodology and others which were presented is that we pay a lot of effort in eliciting the meaning of the structures we match, and we do this by using extensively lexical knowledge about the words occurring in labels. The result of this elicitation process is encoded in a formal language, called WDL (WORDNET Description Logic), which is our proposal for injecting lexical semantics into more standard knowledge representation languages.
For the present work, we deal with the significant problem of high imbalance in data in binary or multi-class classification problems. We study two different linguistic applications. The former determines whether a syntactic construction (environment) co-occurs with a verb in a natural text corpus consists a subcategorization frame of the verb or not. The latter is called Name Entity Recognition (NER) and it concerns determining whether a noun belongs to a specific Name Entity class. Regarding the subcategorization domain, each environment is encoded as a vector of heterogeneous attributes, where a very high imbalance between positive and negative examples is observed (an imbalance ratio of approximately 1:80). In the NER application, the imbalance between a name entity class and the negative class is even greater (1:120). In order to confront the plethora of negative instances, we suggest a search tactic during training phase that employs Tomek links for reducing unnecessary negative examples from the training set. Regarding the classification mechanism, we argue that Bayesian networks are well suited and we propose a novel network structure which efficiently handles heterogeneous attributes without discretization and is more classification-oriented. Comparing the experimental results with those of other known machine learning algorithms, our methodology performs significantly better in detecting examples of the rare class.
In the framework of the DIHANA project, we present the acquisitionprocess of a spontaneous speech dialogue corpus in Spanish. Theselected application consists of information retrieval by telephone for nationwide trains. A total of 900 dialogues from 225 users were acquired using the Wizard of Oz technique. In this work, we present the design and planning of the dialogue scenes and the wizard strategy used for the acquisition of the corpus. Then, we also present the acquisition tools and a description of the acquisition process.
We describe the treatment of verbs with prepositional complements inHaGenLex, a semantically based computer lexicon for German.Prepositional verbs such as bestehen auf (insist on) subcategorize for a prepositional phrase where the preposition usually has no independent meaning of its own. The lexical semantic information inHaGenLex is specified by means of MultiNet, a full-fledged knowledge representation formalism, which proves to be particularly useful for representing the semantics of verbs with prepositional complements.We indicate how the semantic representation in HaGenLex can be used to define semantic classes of prepositional verbs and briefly discuss the relation of these classes to Levin's verb classes. Moreover, wepresent first results on the automatic identification of prepositionalverbs by corpus-based methods.
This paper describes the ARCADE II project, concerned with the evaluation of parallel text alignment systems. The ARCADE II project aims at exploring the techniques of multilingual text alignment through a fine evaluation of the existing techniques and the development of new alignment methods. The evaluation campaign consists of two tracks devoted to the evaluation of alignment at sentence and word level respectively. It differs from ARCADE I in the multilingual aspect and the investigation of lexical alignment.
The paper reports on the results of the exploitation of two Italian lexicons (ItalWordNet and SIMPLE-CLIPS) in an Open-Domain Question Answering application for Italian. The intent is to analyse the behavior of the lexicons in application in order to understand what are their limits and points of strength. The final aim of the paper is contributing to the debate about usefulness of computational lexicons in NLP, by providing evidence from the point of view of a particular application.
This paper describes the development of an Arabic document image collection containing 34,651 documents from 1,378 different books and 25 topics with their relevance judgments. The books from which the collection is obtained are a part of a larger collection 75,000 books being scanned for archival and retrieval at the bibliotheca Alexandrina (BA). The documents in the collection vary widely in topics, fonts, and degradation levels. Initial baseline experiments were performed to examine the effectiveness of different index terms, with and without blind relevance feedback, on Arabic OCR degraded text.
In this paper we describe the gathering of a corpus of synchronised speech and text interaction over the network. The data collection scenarios characterise audio meetings with a significant textual component. Unlike existing meeting corpora, the corpus described in this paper emphasises temporal relationships between speech and text media streams. This is achieved through detailed logging and timestamping of text editing operations, actions on shared user interface widgets and gesturing, as well as generation of speech activity profiles. A set of tools has been developed specifically for these purposes which can be used as a data collection platform for the development of meeting browsers. The data gathered to date consists of nearly 30 hours of recorded audio and time stamped editing operations and gestures.
In this paper we present the continuation of our research on the ability of native Greek adults to identify their mother tongue from synthesized stimuli which contain only prosodic - melodic and rhythmic - information. In the first section we present the ideas that underlie our theory, together with a brief review of our preliminary results. In the second section the detailed description of our experimental approach is given, as well as the results and their statistical analysis. In the final two sections we provide the conclusions derived from our experiments and the future work we are planning to carry out.
With syntactically annotated corpora becoming increasingly available for a variety of languages and grammatical frameworks, tree query tools have proven invaluable to linguists and computer scientists for both data exploration and corpus-based research. We provide a combined engine for tree query (Tregex) and manipulation (Tsurgeon) that can operate on arbitrary tree data structures with no need for preprocessing. Tregex remedies several expressive and implementational limitations of existing query tools, while Tsurgeon is to our knowledge the most expressive tree manipulation utility available.
Evaluating Question Answering (QA) Systems is a very complex task: state-of-the-art systems involve processing whose influences and contributions on the final result are not clear and need to be studied. We present some key points on different aspects of the QA Systems (QAS) evaluation: mainly, as performed during large-scale campaigns, but also with clues on the evaluation of QAS typical software components; the last part of this paper, is devoted to a brief presentation of the French QA campaign EQueR and presents two issues: inter-annotator agreement during campaign and the reuse of reference patterns.
In this paper we present work in progress for the creation of the Italian Content Annotation Bank (I-CAB), a corpus of Italian news annotated with semantic information at different levels. The first level is represented by temporal expressions, the second level is represented by different types of entities (i.e. person, organizations, locations and geo-political entities), and the third level is represented by relations between entities (e.g. the affiliation relation connecting a person to an organization). So far I-CAB has been manually annotated with temporal expressions, person entities and organization entities. As we intend I-CAB to become a benchmark for various automatic Information Extraction tasks, we followed a policy of reusing already available markup languages. In particular, we adopted the annotation schemes developed for the ACE Entity Detection and Time Expressions Recognition and Normalization tasks. As the ACE guidelines have originally been developed for English, part of the effort consisted in adapting them to the specific morpho-syntactic features of Italian. Finally, we have extended them to include a wider range of entities, such as conjunctions.
The EU project NEMLAR (Network for Euro-Mediterranean LAnguage Resources) on Arabic language resources carried out two surveys on the availability of Arabic LRs in the region, and on industrial requirements. The project also worked out a BLARK (Basic Language Resource Kit) for Arabic. In this paper we describe the further development of the BLARK concept made during the work on a BLARK for Arabic, as well as the results for Arabic.
The aim of this article is to provide a statistical representation of significant terms used in the field of Natural Language Processing from the 1960s till nowadays, in order to draft a survey on the most significant research trends in that period. By retrieving these keywords it should be possible to highlight the ebb and flow of some thematic topics. The NLP terminological sample derives from a database created for this purpose using the DBT software (Textual Data Base, ILC patent).
For research and development of an approach for automatically answering why-questions (why-QA) a data collection was created. The data set was obtained by way of elicitation and comprises a total of 395 why-questions. For each question, the data set includes the source document and one or two user-formulated answers. In addition, for a subset of the questions, user-formulated paraphrases are available. All question-answer pairs have been annotated with information on topic and semantic answer type. The resulting data set is of importance not only for our research, but we expect it to contribute to and stimulate other research in the field of why-QA.
The paper discusses shallow semantic annotation of Bulgarian treebank. Our goal is to construct the next layer of linguistic interpretation over the morphological and syntactic layers that have already been encoded in the treebank. The annotation is called shallow because it encodes only the senses for the non-functional words and the relations between the semantic indices connected to them. We do not encode quantifiers and scope information. An ontology is employed as a stock of the concepts and relations that form the word senses. Our lexicon is based on the Generative Lexicon (GL) model (Pustejovsky 1995) as it was implemented in the SIMPLE project (Lenci et. al. 2000). GL defines the way in which the words are connected to the concepts and the relations in the ontology. Also it provides mechanisms for literal sense changes like type-coercion, metonymy, and similar. Some of these phenomena are presented in the annotation.
This paper describes the planning and creation of the Mixer and Transcript Reading corpora, their properties and yields, and reports on the lessons learned during their development.
Research on emotional real-life data has to tackle the problem of their annotation. The annotation of emotional corpora raises the issue of how different coders perceive the same multimodal emotional behaviour. The long-term goal of this paper is to produce a guideline for the selection of annotators. The LIMSI team is working towards the definition of a coding scheme integrating emotion, context and multimodal annotations. We present the current defined coding scheme for emotion annotation, and the use of soft vectors for representing a mixture of emotions. This paper describes a perceptive test of emotion annotations and the results obtained with 40 different coders on a subset of complex real-life emotional segments selected from the EmoTV Corpus collected at LIMSI. The results of this first study validate previous annotations of emotion mixtures and highlight the difference of annotation between male and female coders.
Hantology, a character-based Chinese language resource is created to provide an infrastructure for language processing and research on the writing system. Unlike alphabetic or syllabic writing systems, the ideographic writing system of Chinese poses both a challenge and an opportunity. The challenge is that a totally different resources structure must be created to represent and process speakers conventionalization of the language. The rare opportunity is that the structure itself is enriched with conceptual classification and can be utilized for ontology building. We describe the contents and possible applications of Hantology in this paper. The applications of Hantology include: (1) an account for the diachronic development of Chinese lexica (2) character-based language processing, (3) a study of conceptual structure differences in Chinese and English, and (4) comparisons of different ideographic writing systems.
This paper reports on the multilingual Language Resources (MLRs), i.e. parallel corpora and terminological lexicons for less widely digitally available languages, that have been developed in the INTERA project and the methodology adopted for their production. Special emphasis is given to the reality factors that have influenced the MLRs development approach and their final constitution. Building on the experience gained in the project, a production model has been elaborated, suggesting ways and techniques that can be exploited in order to improve LRs production taking into account realistic issues.
Aiming to compile a thesaurus of adjectives, we discuss how to extract abstract nouns categorizing adjectives, clarify the semantic and syntactic functions of these abstract nouns, and manually evaluate the capability to extract the instance-category relations. We focused on some Japanese syntactic structures and utilized possibility of omission of abstract noun to decide whether or not a semantic relation between an adjective and an abstract noun is an instance-category relation. For 63{\%} of the adjectives (57 groups/90 groups) in our experiments, our extracted categories were found to be most suitable. For 22 {\%} of the adjectives (20/90), the categories in the EDR lexicon were found to be most suitable. For 14{\%} of the adjectives (13/90), neither our extracted categories nor those in EDR were found to be suitable, or examinees own categories were considered to be more suitable. From our experimental results, we found that the correspondence between a group of adjectives and their category name was more suitable in our method than in the EDR lexicon.
This paper presents Shalmaneser, a software package for shallow semantic parsing, the automatic assignment of semantic classes and roles to free text. Shalmaneser is a toolchain of independent modules communicating through a common XML format. System output can be inspected graphically. Shalmaneser can be used either as a black box to obtain semantic parses for new datasets (classifiers for English and German frame-semantic analysis are included), or as a research platform that can be extended to new parsers, languages, or classification paradigms.
In recent years, following the rapid development in the Semantic Web and Knowledge Management research, ontologies have become more in demand in Natural Language Processing. An increasing number of systems use ontologies either internally, for modelling the domain of the application, or as data structures that hold the output resulting from the work of the system, in the form of knowledge bases. While there are many ontology editing tools aimed at expert users, there are very few which are accessible to users wishing to create simple structures without delving into the intricacies of knowledge representation languages. The approach described in this paper allows users to create and edit ontologies simply by using a restricted version of the English language. The controlled language described within is based on an open vocabulary and a restricted set of grammatical constructs. Sentences written in this language unambiguously map into a number of knowledge representation formats including OWL and RDF-S to allow round-trip ontology management.
This paper describes a pilot project which developed a methodology for NP and event coreference annotation consisting of detailed annotation schemes and guidelines. In order to develop this, a small sample annotated corpus in the domain of terrorism/security was built. The methodology developed can be used as a basis for large-scale annotation to produce much-needed resources. In contrast to related projects, ours focused almost exclusively on the development of annotation guidelines and schemes, to ensure that future annotations based on this methodology capture the phenomena both reliably and in detail. The project also involved extensive discussions in order to redraft the guidelines, as well as major extensions to PALinkA, our existing annotation tool, to accommodate event as well as NP coreference annotation.
This paper presents the COMBINA-PT project, a study of corpus-extracted Portuguese Multiword (MW) expressions. The objective of this on-going project is to compile a large lexical database of multiword (MW) units of the Portuguese language, automatically extracted from a balanced 50 million word corpus, and manually validated with the help of lexical association measures. MW expressions considered in the database include named entities and lexical associations with different degrees of cohesion, ranging from frozen groups, which undergo little or no variation, to lexical collocations composed of words that tend to occur together and that constitute syntactic dependencies, although with a low degree of fixedness. This new resource has a two-fold objective: (i) to be an important research tool which supports the development of MW expressions typologies and their lexicographic treatment; (ii) to be of major help in developing and evaluating language processing tools able of dealing with MW expressions.
In Arabic speech communities, there is a diglossic gap between written/formal Modern Standard Arabic (MSA) and spoken/casual colloquial dialectal Arabic (DA): the common spoken language has no standard representation in written form, while the language observed in texts has limited occurrence in speech. Hence the task of developing language resources to describe and model DA speech involves extra work to establish conventions for orthography and grammatical analysis. We describe work being done at the LDC to develop lexicons for DA, comprising pronunciation, morphology and part-of-speech labeling for word forms in recorded speech. Components of the approach are: (a) a two-layer transcription, providing a consonant-skeleton form and a pronunciation form; (b) manual annotation of morphology, part-of-speech and English gloss, followed by development of automatic word parsers modeled on the Buckwalter Morphological Analyzer for MSA; (c) customized user interfaces and supporting tools for all stages of annotation; and (d) a relational database for storing, emending and publishing the transcription corpus as well as the lexicon.
We present an Open Source framework called MOOD developed in order tofacilitate the development of a Statistical Machine Translation Decoder.MOOD has been modularized using an object-oriented approach which makes itespecially suitable for the fast development of state-of-the-art decoders. Asa proof of concept, a clone of the pharaoh decoder has been implemented andevaluated. This clone named ramses is part of the current distribution of MOOD.
In this paper, we describe the methodological procedures and issues that emerged from the development of a pilot Levantine Arabic Treebank (LATB) at the Linguistic Data Consortium (LDC) and its use at the Johns Hopkins University (JHU) Center for Language and Speech Processing workshop on Parsing Arabic Dialects (PAD). This pilot, consisting of morphological and syntactic annotation of approximately 26,000 words of Levantine Arabic conversational telephone speech, was developed under severe time constraints; hence the LDC team drew on their experience in treebanking Modern Standard Arabic (MSA) text. The resulting Levantine dialect treebanked corpus was used by the PAD team to develop and evaluate parsers for Levantine dialect texts. The parsers were trained on MSA resources and adapted using dialect-MSA lexical resources (some developed especially for this task) and existing linguistic knowledge about syntactic differences between MSA and dialect. The use of the LATB for development and evaluation of syntactic parsers allowed the PAD team to provide feedbasck to the LDC treebank developers. In this paper, we describe the creation of resources for this corpus, as well as transformations on the corpus to eliminate speech effects and lessen the gap between our pre-existing MSA resources and the new dialectal corpus
We present a SwedishTurkish Parallel Corpus aimed to be used in linguistic research, teaching, and applications in natural language processing, primarily machine translation. The corpus being under development is built by using a Basic LAnguage Resource Kit (BLARK) for the two languages which is then used in the automatic alignment phase to improve alignment accuracy. The corpus is balanced with respect to source and target language and is automatically processed using the Uplug toolkit.
We describe the Cubreporter information access system which allows access to news archives through the use of natural language technology. The system includes advanced text search, question answering, summarization, and entity profiling capabilities. It has been designed taking into account the characteristics of the background gathering task.
Inter-annotator consistency is a concern for any corpus building effort relying on human annotation. Adjudication is as effective way to locate and correct discrepancies of various kinds. It can also be both difficult and time-consuming. This paper introduces Linguistic Data Consortium (LDC)s model for decision point-based annotation and adjudication, and describes the annotation tools developed to enable this approach for the Automatic Content Extraction (ACE) Program. Using a customized user interface incorporating decision points, we improved adjudication efficiency over 2004 annotation rates, despite increased annotation task complexity. We examine the factors that lead to more efficient, less demanding adjudication. We further discuss how a decision point model might be applied to annotation tools designed for a wide range of annotation tasks. Finally, we consider issues of annotation tool customization versus development time in the context of a decision point model.
Usually a high portion of the different word forms in a corpusreceive no reading by the lexical and/or morphological analysis.These unknown words constitute a huge problem for NLP analysis tasks likePOS-tagging or syntactic parsing. We present a parameterizable (in principle language-independent) corpus-basedapproach for the interpretation of unknown words that only needs a tokenizedcorpus and can be used in both offline and online applications. In combination with a few linguistic (language-dependent) rules unknown verbs, adjectives, nouns, multiword units etc. are identified.Depending on the recognized word class(es), more detailed morphosyntactic and semantic information is additionally identified in opposite to the majority ofother unknown word guessing methods,which only uses a very narrow decision window to assign an unknown wordits correct reading respective Part-of-Speech tag in a given text. We tested our approach by experiments with German data and received very promising results.
In this paper we present a real (as opposed to Wizard-of-Oz) Human-Computer QA-oriented spoken dialog corpus collected with our Ritel platform. This corpus has been orthographically transcribed and annotated in terms of Specific Entities and Topics. Twelve main topics have been chosen. They are refined into 22 sub-topics. The Specific Entities are from five categories and cover Named Entities, linguistic entities, topic-defining entities, general entities and extended entities. The corpus contains 582 dialogs for 6 hours of user speech.
We take a novel approach to rapid, low-cost development of morpho-syntactically annotated resources without using parallel corpora or bilingual lexicons. The overall research question is how to exploit language resources and properties to facilitate and automate the creation of morphologically annotated corpora for new languages. This portability issue is especially relevant to minority languages, for which such resources are likely to remain unavailable in the foreseeable future. We compare the performance of our system on languages that belong to different language families (Romance vs. Slavic), as well as different language pairs within the same language family (Portuguese via Spanish vs. Catalan via Spanish). We show that across language families, the most difficult category is the category of nominals (the noun homonymy is challenging for morphological analysis and the order variation of adjectives within a sentence makes it challenging to create a realiable model), whereas different language families present different challenges with respect to their morpho-syntactic descriptions: for the Slavic languages, case is the most challenging category; for the Romance languages, gender is more challenging than case. In addition, we present an alternative evaluation metric for our system, where we measure how much human labor will be needed to convert the result of our tagging to a high precision annotated resource.
We describe our work on Greek Named Entity Recognition using comparatively three different machine learning techniques: (i) Support Vector Machines (SVM), (ii) Maximum Entropy and (iii) Onetime, a shortcut method based on previous work of one of the authors. The majority of our systems features use linguistic knowledge provided by: morphology, punctuation, position of the lexical units within a sentence and within a text, electronic dictionaries, and the outputs of external tools (a tokenizer, a sentence splitter, and a Hellenic version of Brills Part of Speech Tagger). After testing we observed that the application of a few simple Post Testing Classification Correction (PTCC) rules created after the observation of output errors, improved the results of the SVM and the Maximum Entropy systems output. We achieved very good results with the three methods. Our best configurations (Support Vector Machines with a second degree polynomial kernel and Maximum Entropy) achieved both after the application of PTCC rules an overall F-measure of 91.06.
We introduce a large computational subcategorizationlexicon which includes subcategorization frame (SCF) and frequencyinformation for 6,397 English verbs. This extensive lexicon was acquiredautomatically from five corpora and the Web using the current version of the comprehensive subcategorization acquisition system of Briscoe and Carroll (1997). The lexicon is provided freely for research use, along with a script which can be used to filter and build sub-lexicons suited for different natural languageprocessing (NLP) purposes. Documentation is also provided whichexplains each sub-lexicon option and evaluates its accuracy.
This paper describes the architecture of the American National Corpus and the design decisions we have made in order to make the corpus easy to use with a variety of existing tools with varying functionality, and to allow for layering multiple annotations over the data. The overall goal of the ANC project is to provide an open linguistic infrastructure for American English, consisting of as many self-generated or contributed annotations of the data as possible together with derived. The availability of a wide variety of annotations for the same data and in a common format should significantly simplify the processing required to extract annotations from different sources and enable use of the ANC and its annotations with off-the-shelf software.
A Linguistic Annotation Framework (LAF) is being developed within the International Standards Organization Technical Committee 37 Sub-committee on Language Resource Management (ISO TC37 SC4). LAF is intended to provide a standardized means to represent linguistic data and its annotations that is defined broadly enough to accommodate all types of linguistic annotations, and at the same time provide means to represent precise and potentially complex linguistic information. The general principles informing the design of LAF have been previously reported (Ide and Romary, 2003; Ide and Romary, 2004a). This paper describes some of the more technical aspects of the LAF design that have been addressed in the process of finalizing the specifications for the standard.
There has been an increasing interest in utilizing a wide variety of knowledge sources in order to perform automatic tagging of speech events, such as sentence boundaries and dialogue acts. In addition to the word spoken, the prosodic content of the speech has been proved quite valuable in a variety of spoken language processing tasks such as sentence segmentation and tagging, disfluency detection, dialog act segmentation and tagging, and speaker recognition. In this paper, we report on an open source prosodic feature extraction tool based on Praat, with a description of the prosodic features and the implementation details, as well as a discussion of its extension capability. We also evaluate our tool on a sentence boundary detection task and report the system performance on the NIST RT04 CTS data.
We propose a method that uses information from WordNet glosses to assign semantic tags to individual word meanings, rather than to entire words. The produced lists of annotated words will be used in sentiment annotation of texts and phrases and in other NLP tasks. The method was implemented in the Semantic Tag Extraction Program (STEP) and evaluated on the category of sentiment (positive, negative or neutral) using two human-annotated lists. The lists were first compared to each other and then used to assess the accuracy of the proposed system. We argue that significant disagreement on sentiment tags between the two human-annotated lists reflects a naturally occurring ambiguity of words located on the periphery of the category of sentiment. The category of sentiment, thus, is believed to be structured as a fuzzy set. Finally, we evaluate the generalizability of STEP to other semantic categories on the example of the category of words denoting increase/decrease in magnitude, intensity or quality of some state or process. The implications of this study for both semantic tagging system development and for performance evaluation practices are discussed.
This paper illustrates relevant details of an on-going semantic-role annotation work based on a framework called MULTILAYERED/DIMENSIONAL SEMANTIC FRAME ANALYSIS (MSFA for short) (Kuroda and Isahara, 2005b), which is inspired by, if not derived from, Frame Semantics/Berkeley FrameNet approach to semantic annotation (Lowe et al., 1997; Johnson and Fillmore, 2000).
In this paper, we describe the second release of a suite of language analysers, developed over the last five years, called wraetlic, which includes tools for several partial parsing tasks, both for English and Spanish. It has been successfully used in fields such as Information Extraction, thesaurus acquisition, Text Summarisation and Computer Assisted Assessment.
This paper discusses an augmentation of a corpus ofresearch abstracts in biomedical domain (the GENIA corpus) with two kinds of annotations: tree annotation and event annotation. The tree annotation identifies the linguistic structure that encodes the relations among entities. The event annotation reveals the semantic structure of the biological interaction events encoded in the text. With these annotations we aim to provide a link between the clue and the target of biological event information extraction.
In our paper we present the design and interface of ASK, a language learner corpus of Norwegian as a second language which contains essays collected from language tests on two different proficiency levels as well as personal data from the test takers. In addition, the corpus also contains texts and relevant personal data from native Norwegians as control data. The texts as well as the personal data are marked up in XML according to the TEI Guidelines. In order to be able to classify errors in the texts, we have introduced new attributes to the TEI corr and sic tags. For each error tag, a correct form is also in the text annotation. Finally, we employ an automatic tagger developed for standard Norwegian, the Oslo-Bergen Tagger, together with a facility for manual tag correction. As corpus query system, we are using the Corpus Workbench developed at the University of Stuttgart together with a web search interface developed at Aksis, University of Bergen. The system allows for searching for combinations of words, error types, grammatical annotation and personal data.
We report on our experience with manual alignment of Czech and English parallel corpus text. We applied existing guidelines for English and French and augmented them to cover systematically occurring cases in our corpus. We describe the main extensions covered in our guidelines and provide examples. We evaluated both intra- and inter-annotator agreement and obtained very good results of Kappa well above 0.9 and agreement of 95{\%} and 93{\%}, respectively.
This paper presents a study on synchronization of linguistic annotation and numerical data on a video corpus of French Sign Language. We detail the methodology and sketches out the potential observations that can be provided by such a kind of mixed annotation. The corpus is composed of three views: close-up, frontal and top. Some image processing has been performed on each video in order to provide global information on the movement of the signers. That consists of the size and position of a bounding box surrounding the signer. Linguists have studied this corpus and have provided annotations on iconic structures, such as ``personal transfers'' (role shifts). We used an annotation software, ANVIL, to synchronize linguistic annotation and numerical data. This new approach of annotation seems promising for automatic detection of linguistic phenomena, such as classification of the signs according to their size in the signing space, and detection of some iconic structures. Our first results must be consolidated and extended on the whole corpus. The next step will consist of designing automatic processes in order to assist SL annotation.
Optimizing the production, maintenance and extension of lexical resources is one the crucial aspects impacting Natural Language Processing (NLP). A second aspect involves optimizing the process leading to their integration in applications. With this respect, we believe that the production of a consensual specification on lexicons can be a useful aid for the various NLP actors. Within ISO, the purpose of LMF is to define a standard for lexicons. LMF is a model that provides a common standardized framework for the construction of NLP lexicons. The goals of LMF are to provide a common model for the creation and use of lexical resources, to manage the exchange of data between and among these resources, and to enable the merging of large number of individual electronic resources to form extensive global electronic resources. In this paper, we describe the work in progress within the sub-group ISO-TC37/SC4/WG4. Various experts from a lot of countries have been consulted in order to take into account best practices in a lot of languages for (we hope) all kinds of NLP lexicons.
We are presenting a method to recognise geographical references in free text. Our tool must work on various languages with a minimum of language-dependent resources, except a gazetteer. The main difficulty is to disambiguate these place names by distinguishing places from persons and by selecting the most likely place out of a list of homographic place names world-wide. The system uses a number of language-independent clues and heuristics to disambiguate place name homographs. The final aim is to index texts with the countries and cities they mention and to automatically visualise this information on geographical maps using various tools.
Compounds constitute a specific issue in search, in particular in languages where they are written in one word, as is the case for Danish and the other Scandinavian languages. For such languages, expansion of the query compound into separate lemmas is a way of finding the often frequent alternative synonymous phrases in which the content of a compound can also be expressed. However, it is crucial to note that the number of irrelevant hits is generally very high when using this expansion strategy. The aim of this paper is to examine how we can obtain better search results on split compounds, partly by looking at the internal structure of the original compound, partly by analyzing the context in which the split compound occurs. We perform an NP analysis and introduce a new, linguistically based threshold for retrieved hits. The results obtained by using this strategy demonstrate that compound splitting combined with a shallow linguistic analysis focusing on the recognition of NPs can improve search by bringing down the number of irrelevant hits.
The growing of multilingual information processing technology has created the need of linguistic resources, especially lexical database. Many attempts were put to alter the traditional dictionary to computational dictionary, or widely named as computational lexicon. TCLs Computational Lexicon (TCLLEX) is a recent development of a large-scale Thai Lexicon, which aims to serve as a fundamental linguistic resource for natural language processing research. We design either terminology or ontology for structuring the lexicon based on the idea of computability and reusability.
The LOIS (Lexical Ontologies for legal Information Sharing) project The legal knowledge base resulting from the LOIS (Lexical Ontologies for legal Information Sharing) (Lexical Ontologies for legal Information Sharing) project consists of legal WordNets in six languages (Italian, Dutch, Portuguese, German, Czech, English). Its architecture is based on the EuroWordNet (EWN) framework (Vossen et al, 1997). Using the EWN framework assures compatibility of the LOIS WordNets with EWN, allowing them to function as an extension of EWN for the legal domain. For each legal system, the document-derived legal concepts are integrated into a taxonomy, which links into existing formal ontologies. These give the legal wordnets a first formal backbone, which can, in future, be further extended. The database consists of 33,000 synsets, and is aimed to be used in information retrieval, where it provides mono- and multi-lingual access to European legal databases for legal experts as well as for laymen. The LOIS knowledge base also provides a flexible, modular architecture that allows integration of multiple classification schemes, and enables the comparison of legal systems by exploring translation, equivalence and structure across the different legal wordnets.
This paper describes an alternative approach to morphological language modeling, which incorporates constraints on the morphological production of new words.This is done by applying the constraints as a preprocessing step in which only one morphological production rule can be applied to an extended lexicon of knownmorphemes, lemmas and word forms. This approach is used to extend the CELEX Dutch morphological database, so that a higher coverage can be reached on a largecorpus of Dutch newspaper articles. We present experimental results on the coverage of this extended database and use the extension to further evaluate our morphologicalsystem, as well as the impact of the constraints on the coverage of out-of-vocabulary words.
In the field of Computational Linguistics, many lexical resources have been developed which aim at encoding complex lexical semantic information according to different linguistic models (WordNet, Frame Semantics, Generative Lexicon, etc.). However, these resources are often not easily accessible nor available in their entirety. Yet, from the point of view of the continuous growth of technology (Semantic Web), their visibility, availability and integration are becoming of utmost importance. ItalWordNet and PAROLE/SIMPLE/CLIPS are two resources which, tackling lexical semantics from different perspectives and being at least partially complementary, can profit from linking each other. In this paper we address the issue of the linking of these resources focusing on the most problematic part of the lexicon: the second order entities. In particular, after a brief description of the two resources, their different approaches to the verb semantics are described; an accurate comparison of a set of verbal entries belonging to Speech Act semantic class is carried out aiming at evaluate the possibilities and the advantages of a semiautomatic link.
This article outlines the evaluation protocol and provides the main results of the French Evaluation Campaign for Machine Translation Systems, CESTA. Following the initial objectives and evaluation plans, the evaluation metrics are briefly described: along with fluency and adequacy assessed by human judges, a number of recently proposed automated metrics are used. Two evaluation campaigns were organized, the first one in the general domain, and the second one in the medical domain. Up to six systems translating from English into French, and two systems translating from Arabic into French, took part in the campaign. The numerical results illustrate the differences between classes of systems, and provide interesting indications about the reliability of the automated metrics for French as a target language, both by comparison to human judges and using correlations between metrics. The corpora that were produced, as well as the information about the reliability of metrics, constitute reusable resources for MT evaluation.
Market surveys have pointed out translators demand for integrated specialist dictionaries in translation memory tools which they could use in addition to their own compiled dictionaries or stored translated parts of text. For this purpose the German specialist dictionary publisher, Langenscheidt Fachverlag in Munich has developed a method and tools together with experts from the University Rennes 2 in France and well known Translation Memory Providers. The conversion-tools of dictionary entries (lemma-oriented) in terminological entries (concept-oriented) are based on lexicographical and terminological ISO standards: ISO 1951 for dictionaries and ISO 16642 for terminology. The method relies on the analysis of polysemic structures into a set of data categories that can be recombined into monosemic entries compatible with most of the terminology management engines on the market. The whole process is based on the TermBridge semantic repository (http://www.genetrix.org ) for terminology and machine readable dictionaries and on a XML model LexTerm which is a subset of Geneter (ISO 16642 Annex C). It illustrates the interest for linguistic applications to define data elements in semantic repositories so that they are reusable in various contexts. This operation is fully integrated in the editorial XML workflow and applies to a series of specialist dictionaries which are now available.
This paper deals with the design of a synthesis database for a high quality corpus-based Speech Synthesis system in Spanish. The database has been designed for speech synthesis, speech conversion and expressive speech. The design follows the specifications of TC-STAR project and has been applied to collect equivalent English and Mandarin synthesis databases. The sentences of the corpus have been selected mainly from transcribed speech and novels. The selection criterion is a phonetic and prosodic coverage. The corpus was completed with sentences specifically designed to cover frequent phrases and words. Two baseline speakers and four bilingual speakers were recorded. Recordings consist of 10 hours of speech for each baseline speaker and one hour of speech for each voice conversion bilingual speaker. The database is labelled and segmented. Pitch marks and phonetic segmentation was done automatically and up to 50{\%} manually supervised. The database will be available at ELRA.
In our paper, we present a method for incorporating available linguistic information into a statistical language model that is used in ASR system for transcribing spontaneous speech. We employ the class-based language model paradigm and use the morphological tags as the basis for world-to-class mapping. Since the number of different tags is at least by one order of magnitude lower than the number of words even in the tasks with moderately-sized vocabularies, the tag-based model can be rather robustly estimated using even the relatively small text corpora. Unfortunately, this robustness goes hand in hand with restricted predictive ability of the class-based model. Hence we apply the two-pass recognition strategy, where the first pass is performed with the standard word-based n-gram and the resulting lattices are rescored in the second pass using the aforementioned class-based model. Using this decoding scenario, we have managed to moderately improve the word error rate in the performed ASR experiments.
This paper presents a semantic classification of reflexive verbs in Bulgarian, augmenting the morphosyntactic classes of verbs in the large Bulgarian Lexical Data Base - a language resource utilized in a number of Language Engineering (LE) applications. Thesemantic descriptors conform to the Unified Eventity Representation (UER), developed by Andrea Schalley. The UER is a graphical formalism, introducing the object-oriented system design to linguistic semantics. Reflexive/non-reflexive verb pairs are analyzed where the non-reflexive member of the opposition, a two-place predicate, is considered the initial linguistic entity from which the reflexive correlate is derived. The reflexive verbs are distributed into initial syntactic-semantic classes which serve as the basis for defining the relevant semantic descriptors in the form of EVENTITY FRAME diagrams. The factors that influence the categorization of the reflexives are the lexical paradigmaticapproach to the data, the choice of only one reading for each verb, top level generalization of the semantic descriptors. The language models described in this paper provide the possibility for building linguistic components utilizable in knowledge-driven systems.
We describe a method for the automatic extraction of a Stochastic Lexicalized Tree Insertion Grammar from a linguistically rich HPSG Treebank. The extraction method is strongly guided by HPSG-based head and argument decomposition rules. The tree anchors correspond to lexical labels encoding fine-grained information. The approach has been tested with a German corpus achieving a labeled recall of 77.33{\%} and labeled precision of 78.27{\%}, which is competitive to recent results reported for German parsing using the Negra Treebank.
Pragmatics is the study of how people exchange meanings through the use of language. In this paper we describe our experience with regard to texts belonging to a large contemporary corpus of written language, in order to verify the uses, changes and flexibility of the meaning of Proper Names (PN). As a matter of fact, while building the lexical semantic database ItalWordNet (IWN), a considerable set of PN (up to now, about 4,000) has been inserted and studied. We give prominence to the polysemy of PN and their shifting or moving from one class to another as an example of the extensibility of language and the possibility of change considering meaning as a dynamic process. Many examples of the sense shifting phenomenon can be evidenced by textual corpora. By comparing the percentages regarding the texts belonging to two different periods of time, an increasing use of the PN with sense extension has been verified. This evidence could confirm the tendency to consider the derived or extended senses as more salient and prevailing on the base senses, confirming a gradual fixation of meaning during the time. The object of our study (in progress) is to observe the uses of sense extensions also examining in detail freshly coined examples and taking into account their relationship with meta representational capacity and human creativity and the ways in which linguistic dynamics can activate the meaning potential of the words.
Lexical information for South African Bantu languages is not readily available in the form of machine-readable lexicons. At present the availability of lexical information is restricted to a variety of paper dictionaries. These dictionaries display considerable diversity in the organisation and representation of data. In order to proceed towards the development of reusable and suitably standardised machine-readable lexicons for these languages, a data model for lexical entries becomes a prerequisite. In this study the general purpose model as developed by Bell {\&} Bird (2000) is used as a point of departure. Firstly, the extent to which the Bell {\&} Bird (2000) data model may be applied to and modified for the above-mentioned languages is investigated. Initial investigations indicate that modification of this data model is necessary to make provision for the specific requirements of lexical entries in these languages. Secondly, a data model in the form of an XML DTD for the languages in question, based on our findings regarding (Bell {\&} Bird, 2000) and (Weber, 2002) is presented. Included in this model are additional particular requirements for complete and appropriate representation of linguistic information as identified in the study of available paper dictionnaries.
This paper presents an experience in laboratory dealing with the constitution of a corpus of multimodal spontaneous expressions of emotions. The originality of this corpus resides in its characteristics (interactions between a virtual actor and humans learning a theater text), in its content (multimodal spontaneous expressions of emotions) and in its two sources of characterization (by the participant and by one of his/her close relation). The corpus collection is part of a study on the fusion of multimodal information (verbal, facial, gestural, postural, and physiological) to improve the detection and characterization of expressions of emotions in human-machine interaction (HMI).
The Language Grid, recently proposed by one of the authors, is a language infrastructure available on the Internet. It aims to resolve the problems of accessibility and usability inherent in the currently available language services. The infrastructure will accommodate an operational environment in which a user and/or a software agent can develop a language service that is tailored to specific requirements derived from the various situations of intercultural communication. In order to effectively operate the infrastructure, each atomic language service has to be discovered by the planner of a composite service and incorporated into the composite service scenario. Meta-description of an atomic service is crucial to accomplish the planning process. This paper focuses on dictionary access services and proposes an abstract dictionary model that is vital for the accurate meta-description of such a service. In principle, the proposed model is based on the organization compatible with Princeton WordNet. Computational lexicons, including the EDR dictionary, as well as a range of human monolingual/bilingual dictionaries are uniformly organized into a WordNet-like lexical concept system. A modeling example with a few dictionary instances demonstrates the fundamental validity of the model.
The users demand has determined the need to manage the growing new technical maritime terminology which includes very different domains such as the juridical or commercial ones. A terminological database was built by exploiting the computational tools of ItalWordNet (IWN) and its lexical-semantic model (EuroWordNet).This paper concerns the development of database structure and data coding, relevance of the concepts of term and domain, information potential of the terms, complexity of this domain and detailed ontology structuring recently undertaken and still in progress. Our domain structure is described defining a core set of terms representing the two main sub-domains specified in technical-nautical and maritime transport terminology. These terms are sufficiently general to be the root nodes of the core ontology we are developing. They are mostly domain-dependent, but the link with the Top Ontology of IWN remains, endorsing either general and foundation information, or detailed description directly connected with the specific domain. Through the semantic relations linking the synsets, every term inherits the top ontology definitions and becomes itself an integral part of the structure. While codifying a term in the maritime database, the reference is at the same time allowed to the Base Concepts of the terminological ontology embedding the term in the semantic network, showing that upper and core ontologies make it possible for the framework to integrate different views on the same domain in a meaningful way.
This paper reports on an endeavour of creating basic linguistic resources for geo-referencing of Polish free-text documents. We have defined a fine-grained named entity hierarchy, produced an exhaustive gazetteer, and developed named-entity grammars for Polish. Additionally, an annotated corpus for the cadastral domain was prepared for evaluation purposes. Our baseline approach to geo-referencing is based on application of aforementioned resources and a lightweight co-referencing technique which utilizes string-similarity metric of Jaro-Winkler. We carried out a detailed evaluation of detecting locations, organizations and persons, which revealed that best results are obtained via application of a combined grammar for all types. The application of lightweight co-referencing for organizations and persons improves recall but deteriorates precision, and no gain is observed for locations. The paper is accompanied by a demo, a geo-referencing application capable of: (a) finding documents and text fragments based on named entities and (b) populating the spatial ontology from texts.
As a practical information guidance system, we have been developing a speech-oriented system named ``Takemaru-kun''. The system has been operated on a public space since Nov. 2002. The system answers to user's question about the hall facilities, sightseeing, transportation, weather information around the city, etc. All triggered inputs to the system have been recorded since the operation started. And all system inputs during 22 months are manually transcribed and labelled for speakers gender and age category. In this paper, we conduct a long-term prosody analysis of user speech to find a clue to obtain users attitude from a users speech. In this preliminary analysis, it is observed that F0 decreases regardless of age and gender category when the stability of the dialogue system is not established.
Current Semantic Web implementation efforts pose a number of challenges. One of the big ones among them is development and evolution of specific resources --- the ontologies --- as a base for representation of the meaning of the web. This paper deals with the automatic acquisition of semantic relations from the text of scientific publications (journal articles, conference papers, project descriptions, etc.). We also describe the process of building of corresponding ontological resources and their application for semi--automatic generation of scientific portals. Extracted relations and ontologies are crucial for the structuring of the information at the portal pages, automatic classification of the presented documents as well as for personalisation at the presentation level. Besides a general description of the portal generating system, we give also a detailed overview of extraction of semantic relations in the form of a domain--specific ontology. The overview consists of presentation of an architecture of the ontology extraction system, description of methods used for mining of semantic relations and analysis of selected results and examples.
We aim to automatically induce a PoS tagset for Italian by analysing the distributional behaviour of Italian words. To this end, we propose an algorithm that (a) extracts information from loosely labelled dependency structures that encode only basic and broadly accepted syntactic relations, namely Head/Dependent and the distinction of dependents into Argument vs. Adjunct, and (b) derives a possible set of word classes. The paper reports on some preliminary experiments carried out using the induced tagset in conjunction with state-of-the-art PoS taggers. The method proposed to design a proper tagset exploits little, if any, language-specific knowledge: hence it is in principle applicable to any language.
One of the crucial issues in semantic parsing is how to reduce costs of collecting a sufficiently large amount of labeled data. This paper presents a new approach to cost-saving annotation of example sentences with predicate-argument structure information, taking Japanese as a target language. In this scheme, a large collection of unlabeled examples are first clustered and selectively sampled, and for each sampled cluster, only one representative example is given a label by a human annotator. The advantages of this approach are empirically supported by the results of our preliminary experiments, where we use an existing similarity function and naive sampling strategy.
The DiaCORIS project aims at the construction of a diachronic corpus comprising written Italian texts produced between 1861 and 1945, extending the structure and the research possibilities of the synchronic 100-million word corpus CORIS/CODIS. A preliminary in depth study has been performed in order to design a representative and well balanced sample of the Italian language over a time period that contains all the main events of contemporary Italian history from the National Unification to the end of the Second World War. The paper describes in detail such design processes as the definition of the main subcorpora and their proportions, the type of documents inserted in each part of the corpus, the document annotation schema and the technological infrastructure designed to manage the corpus access as well as the web interface to corpus data.
This paper presents a methodology for adding a layer of semantic annotation to a syntactically annotated corpus of Basque (EPEC), in terms of semantic roles. The proposal we make here is the combination of three resources: the model used in the PropBank project (Palmer et al., 2005), an in-house database with syntactic/semantic subcategorization frames for Basque verbs (Aldezabal, 2004) and the Basque dependency treebank (Aduriz et al., 2003). In order to validate the methodology and to confirm whether the PropBank model is suitable for Basque and our treebank design, we have built lexical entries and labelled all argument and adjuncts occurring in our treebank for 3 Basque verbs. The result of this study has been very positive, and has produced a methodology adapted to the characteristics of the language and the Basque dependency treebank. Another goal of this study was to study whether semi-automatic tagging was possible. The idea is to present the human taggers a pre-tagged version of the corpus. We have seen that many arguments could be automatically tagged with high precision, given only the verbal entries for the verbs and a handful of examples.
The EDR electronic dictionary is a machine-tractable dictionary developed for advanced computer-based processing of natural lan-guage. This dictionary comprises eleven sub-dictionaries, including a concept dictionary, word dictionaries, bilingual dictionaries, co-occurrence dictionaries, and a technical terminology dictionary. In this study, we focus on the concept dictionary and aim to revise the arrangement of concepts for improving the EDR electronic dictionary. We believe that unsuitable concepts in a class differ from other concepts in the same class from an abstract perspective. From this notion, we first try to automatically extract those concepts unsuited to the class. We then try semi-automatically to amend the concept explications used to explain the meanings to human users and rearrange them in suitable classes. In the experiment, we try to revise those concepts that are the lower-concepts of the concept human in the concept hierarchy and that are directly arranged under concepts with concept explications such as person as defined by  and person viewed from . We analyze the result and evaluate our approach.
This paper describes the methodology adopted to jointly develop the Basque WordNet and a hand annotated corpora (the Basque Semcor). This joint development allows for better motivated sense distinctions, and a tighter coupling between both resources. The methodology involves edition, tagging and refereeing tasks. We are currently half way through the nominal part of the 300.000 word corpus (roughly equivalent to a 500.000 word corpus for English). We present a detailed description of the task, including the main criteria for difficult cases in the edition of the senses and the tagging of the corpus, with special mention to multiword entries. Finally we give a detailed picture of the current figures, as well as an analysis of the agreement rates.
This article describes the real-time speech recognition system for closed-captioning of TV ice-hockey commentaries. Automatic transcription of TV commentary accompanying an ice-hockey match is usually a hard task due to the spontaneous speech of a commentator put often into a very loud background noise created by the public, music, siren, drums, whistle, etc. Data for building this system was collected from 41 matches that were played during World Championships in years 2000, 2001, and 2002 and were transmitted by the Czech TV channels. The real-time closed-captioning system is based on the class-based language model designed after careful analysis of training data and OOV words in new (till now unseen) commentaries with the goal to decrease an OOV (Out-Of-Vocabulary) rate and increase recognition accuracy.
Facing the huge amount of textual and terminological data in the life sciences, we present a theoretical basis for the linguistic analysis of chemical terms. Starting with organic compound names, we conduct a morpho-semantic deconstruction into morphemes and yield a semantic representation of the terms' functional and structural properties. These semantic representations imply both the molecular structure of the named molecules and their class membership. A crucial feature of this analysis, which distinguishes it from all similar existing systems, is its ability to deal with terms that do not fully specify a structure as well as terms for generic classes of chemical compounds. Such `underspecified' terms occur very frequently in scientific literature. Our approach will serve for the support of manual database curation and as a basis for text processing applications.
In the framework of the Word Sense Disambiguation (WSD) and lexical transfer in Machine Translation (MT), the representation of word meanings is one critical issue. The conceptual vector model aims at representing thematic activations for chunks of text, lexical entries, up to whole documents. Roughly speaking, vectors are supposed to encode ideas associated to words or expressions. In this paper, we first expose the conceptual vectors model and the notions of semantic distance and contextualization between terms. Then, we present in details the text analysis process coupled with conceptual vectors, which is used in text classification, thematic analysis and vector learning. The question we focus on is whether a thesaurus is really needed and desirable for bootstrapping the learning. We conducted two experiments with and without a thesaurus and are exposing here some comparative results. Our contribution is that dimension distribution is done more regularly by an emergent procedure. In other words, the resources are more efficiently exploited with an emergent procedure than with a thesaurus terms (concepts) as listed in a thesaurus somehow relate to their importance in the language but nor to their frequency in usage neither to their power of discrimination or representativeness.
In this paper we discuss the problem of sense disambiguation using lexical resources like ontologies or thesauri with a focus on the application of sense detection and merging methods in information retrieval systems. For an information retrieval task it is important to detect the meaning of a query word for retrieving the related relevant documents. In order to recognize the meaning of a search word, lexical resources, like WordNet, can be used for word sense disambiguation. But, analyzing the WordNet structure, we see that this ontology is fraught with different problems. The too fine grained distinction between word senses, for example, is unfavorable for a usage in information retrieval. We describe related problems and present four implemented online methods to merge SynSets based on relations like hypernyms and hyponyms, and further context information like glosses and domain. Afterwards we show a first evaluation of our approach, compare the different merging methods and discuss briefly future work.
This paper examines the use of parallel and comparable corpora for automatic acquisition of semantics-extraction patterns. It presents a new method of the pattern extraction which takes advantage of parallel texts to ``port'' text mining solutions from a source language to a target language. It is shown thatthe technique can help in situations when the extraction procedure is to beapplied in a language (languages) with a limited set of available resources,e.g. domain-specific thesauri. The primary motivation of our work lies in a particular multilingual e-learning system. For testing purposes, other applications of the given approach were implemented. They include pattern extraction from general texts (tested on wordnet relations), acquisition of domain-specific patterns from large parallel corpus of legal EU documents, and mining of subjectivity expressions for multilingual opinion extraction system.
This paper describes an interdisciplinary approach which brings together the fields of corpus linguistics and translation studies. It presents ongoing work on the creation of a corpus resource in which translation shifts are explicitly annotated. Translation shifts denote departures from formal correspondence between source and target text, i.e. deviations that have occurred during the translation process. A resource in which such shifts are annotated in a systematic way will make it possible to study those phenomena that need to be addressed if machine translation output is to resemble human translation. The resource described in this paper contains English source texts (parliamentary proceedings) and their German translations. The shift annotation is based on predicate-argument structures and proceeds in two steps: first, predicates and their arguments are annotated monolingually in a straightforward manner. Then, the corresponding English and German predicates and arguments are aligned with each other. Whenever a shift - mainly grammatical or semantic - has occurred, the alignment is tagged accordingly.
In the Ph@ttSessionz project, geographically distributed high-bandwidth recordings of adolescent speakers are performed in public schools all over Germany. To achieve a consistent technical signal quality, a standard configuration of recording equipment is sent to the participating schools. The recordings are made using the SpeechRecorder software for prompted speech recordings via the WWW. During a recording session, prompts are downloaded from a server, and the speech data is uploaded to the server in a background process. This paper focuses on the technical aspects of the distributed Ph@ttSessionz speech recordings and their annotation.
Implementation of legal bilingualism in Hong Kong after 1997 has necessitated the production of voluminous and extensive court proceedings and judgments in both Chinese and English. For the former, Cantonese, a dialect of Chinese, is the home language of more than 90{\%} of the population in Hong Kong and so used in the courts. To record speech in Cantonese verbatim, a Chinese Computer-Aided Transcription system has been developed. The transcription system converts stenographic codes into Chinese text, i.e. from phonetic to orthographic representation of the language. The main challenge lies in the resolution of the sever ambiguity resulting from homocode problems in the conversion process. Cantonese Chinese is typified by problematic homonymy, which presents serious challenges. The N-gram statistical model is employed to estimate the most probable character string of the input transcription codes. Domain-specific corpora have been compiled to support the statistical computation. To improve accuracy, scalable techniques such as domain-specific transcription and special encoding are used. Put together, these techniques deliver 96{\%} transcription accuracy.
The paper presents advances in the use of semantic features and interlingua relations for word sense disambiguation (WSD) as part of unification-based deep processing grammars. Formally we present an extension of Minimal Recursion Semantics, introducing sortal specifications as well as linterlingua semantic relations as a means of semantic decomposition.
We present the results of two trials testing procedures for the annotation of emotion and mental state of the AMI corpus. The first procedure is an adaptation of the FeelTrace method, focusing on a continuous labelling of emotion dimensions. The second method is centered around more discrete labeling of segments using categorical labels. The results reported are promising for this hard task.
The aim of the Media-Evalda project is to evaluate the understanding capabilities of dialog systems. This paper presents the Media protocol for speech understanding evaluation and describes the results of the June 2005 literal evaluation campaign. Five systems, both symbolic or corpus-based, participated to the evaluation which is based on a common semantic representation. Different scorings have been performed on the system results. The understanding error rate, for the Full scoring is, depending on the systems, from 29{\%} to 41.3{\%}. A diagnosis analysis of these results is proposed.
We propose a bootstrapping approach to creating a phrase-level alignment over a sentence-aligned parallel corpus, reporting concrete treebank annotation work performed on a sample of sentence tuples from the Europarl corpus, currently for English, French, German, and Spanish. The manually annotated seed data will be used as the basis for automatically labelling the rest of the corpus. Some preliminary experiments addressing the bootstrapping aspects are presented.The representation format for syntactic correspondence across parallel text that we propose as the starting point for a process of successive refinement emphasizes correspondences of major constituents that realize semantic arguments or modifiers; language-particular details of morphosyntactic realization are intentionally left largely unlabelled. We believe this format is a good basis for training NLPtools for multilingual application contexts in which consistency across languages is more central than fine-grained details in specific languages (in particular, syntax-based statistical Machine Translation).
This work adresses the use of confidence measures for extracting well recognized words with very low error rate from automatically transcribed segments in a unsupervised way. We present and compare several confidence measures and propose a method to merge them into a new one. We study its capabilities on extracting correct recognized word-segments compared to the amount of rejected words. We apply this fusion measure to select audio segments composed of words with a high confidence score. These segments come from an automatic transcription of french broadcast news given by our speech recognition system based on the CMU Sphinx3.3 decoder. Injecting new data resulting from unsupervised treatments of raw audio recordings in the training corpus of acoustic models gives statistically significant improvement (95{\%} confident interval) in terms of word error rate. Experiments have been carried out on the corpus used during ESTER, the french evaluation campaign.
This paper presents a case study concerning the challenges and requirements posed by next generation language resources, realized as an overall model of open, distributed and collaborative language infrastructure. If a sort of new paradigm for language resource sharing is required, we think that the emerging and still evolving technology connected to Grid computing is a very interesting and suitable one for a concrete realization of this vision. Given the current limitations of Grid computing, it is very important to test the new environment on basic language analysis tools, in order to get the feeling of what are the potentialities and possible limitations connected to its use in NLP. For this reason, we have done some experiments on a module of the Linguistic Miner, i.e. the extraction of linguistic patterns from restricted domain corpora. The Grid environment has produced the expected results (reduction of the processing time, huge storage capacity, data redundancy) without any additional cost for the final user.
Topic: lexical ressources, international project Abstract: The LEXADV-project is a Scandinavian research project (2004-2006, financed by Nordplus Sprog) with the aim of extending three Scandinavian semantic lexicons building on the SIMPLE lexicon model (Lenci et al., 2000) with the word class of adverbs. In the lexicons of approx. 400 Danish, Norwegian and Swedish adverbs the different senses are described with a semantic type and a set of semantic features. A classification covering the many meanings that adverbs can have has been established and integrated in the original SIMPLE ontology. Similarly new features have been added to the model in order to describe the adverb senses. The working method of the project builds on the fact that the vocabularies of Danish, Norwegian and Swedish are closely related. An encoding tool has been developed with the special purpose of permitting easy transfer of semantic types and features between entries in the three languages. The Danish adverb senses have been described first, based on the definition in a modern, comprehensive Danish dictionary. Afterwards the lemmas have been translated and the semantic data have been copied into the Swedish as well as into the Norwegian equivalent entry. Finally these copies have been evaluated and when necessary adjusted by native speakers.
The paper reports on the development methodology of a system aimed at multi-domain multi-lingual recognition and classification of names in texts, the focus being on the linguistic resources used for training and testing purposes. The corpus presented here has been collected and annotated in the framework of different projects the critical issue being the development of a final resource that is homogenous, re-usable and adaptable to different domains and languages with a view to robust multi-domain and multi-lingual NERC.
Six sites participated in the Interlingual Annotation of Multilingual Text Corpora (IAMTC) project (Dorr et al., 2004; Farwell et al., 2004; Mitamura et al., 2004). Parsed versions of English translations of news articles in Arabic, French, Hindi, Japanese, Korean and Spanish were annotated by up to ten annotators. Their task was to match open-class lexical items (nouns, verbs, adjectives, adverbs) to one or more concepts taken from the Omega ontology (Philpot et al., 2003), and to identify theta roles for verb arguments. The annotated corpus is intended to be a resource for meaning-based approaches to machine translation. Here we discuss inter-annotator agreement for the corpus. The annotation task is characterized by annotators freedom to select multiple concepts or roles per lexical item. As a result, the annotation categories are sets, the number of which is bounded only by the number of distinct annotator-lexical item pairs. We use a reliability metric designed to handle partial agreement between sets. The best results pertain to the part of the ontology derived from WordNet. We examine change over the course of the project, differences among annotators, and differences across parts of speech. Our results suggest a strong learning effect early in the project.
Annotation projects dealing with complex semantic or pragmatic phenomena face the dilemma of creating annotation schemes that oversimplify the phenomena, or that capture distinctions conventional reliability metrics cannot measure adequately. The solution to the dilemma is to develop metrics that quantify the decisions that annotators are asked to make. This paper discusses MASI, distance metric for comparing sets, and illustrates its use in quantifying the reliability of a specific dataset. Annotations of Summary Content Units (SCUs) generate models referred to as pyramids which can be used to evaluate unseen human summaries or machine summaries. The paper presents reliability results for five pairs of pyramids created for document sets from the 2003 Document Understanding Conference (DUC). The annotators worked independently of each other. Differences between application of MASI to pyramid annotation and its previous application to co-reference annotation are discussed. In addition, it is argued that a paradigmatic reliability study should relate measures of inter-annotator agreement to independent assessments, such as significance tests of the annotated variables with respect to other phenomena. In effect, what counts as sufficiently reliable intera-annotator agreement depends on the use the annotated data will be put to.
Speech interfaces and dialogue processing abilities have promise for improving the utility of open-domain question answering (QA).We propose a novel method of resolving disambiguation problems arisen in those speech and dialogue enhanced QA tasks. The proposed method exploits passage retrieval, which is one of main components common in many QA systems. The basic idea of the method is that the similarity with some passage in the target documents can be used to select the appropriate question from the candidates. In this paper, we applied the method to solve two subtasks of QA, which are (1) N-best rescoring of LVCSR outputs, which selects a most appropriate candidate as a question sentence, in speech-driven QA (SDQA) task and (2) context processing, which compose a complete question sentence from a submitted incomplete one by using the elements appeared in the dialogue context, in information access dialogue (IAD) task. For both tasks, a dynamic passage retrieval is introduced to further improve the performance. The experimental results showed that the proposed method is quite effective in order to improve the performance of QA in both two tasks.
Temporal annotation is a complex task characterized by low markup speed and low inter-annotator agreements scores. Tango is a graphical annotation tool for temporal relations. It is developed for the TimeML annotation language and allows annotators to build a graph that resembles a timeline. Temporal relations are added by selecting events and drawing labeled arrows between them. Tango is integrated with a temporal closure component and includes features like SmartLink, user prompting and automatic linking of time expressions. Tango has been used to create two corpora with temporal annotation, TimeBank and the AQUAINT Opinion corpus.
This paper presents the work done to annotate a corpus of spoken Danish with information structure tags, and describes a preliminary study in which the corpus has been used to investigate the relation between focus and intra-clausal pauses. The study indicates that the pauses that do fall within the focus domain tend to precede property-expressing words by which the object in focus is distinguished from other similar ones.
A simple and flexible schema for storing and presenting monolingual language resources is proposed. In this format, data for 18 different languages is already available in various sizes. The data is provided free of charge for online use and download. The main target is to ease the application of algorithms for monolingual and interlingual studies.
This paper describes the unfolding of the EASy evaluation campaign for french parsers as well as the techniques employed for the participation of laboratory LPL to this campaign. Three symbolic parsers based on a same resource and a same formalism (Property Grammars) are described and evaluated. The first results of this evaluation are analyzed and lead to the conclusion that symbolic parsing in a constraint-based formalism is efficient and robust.
We present a large parallel corpus of texts published by the United Nations Organization, which we exploit for the creation ofphrase-based statistical machine translation (SMT) systems for new language pairs. We present a setup where phrase tables for these language pairs are used for translation between languages for which parallel corpora of sufficient size are so far not available. We give some preliminary results for this novel application of SMT and discuss further refinements.
This paper describes an effort to investigate the incrementally deepening development of an interlingua notation, validated by human annotation of texts in English plus six languages. We begin with deep syntactic annotation, and in this paper present a series of annotation manuals for six different languages at the deep-syntactic level of representation. Many syntactic differences between languages are removed in the proposed syntactic annotation, making them useful resources for multilingual NLP projects with semantic components.
This paper presents the audio corpus developed in the framework of the ESTER evaluation campaign of French broadcast news transcription systems. This corpus includes 100 hours of manually annotated recordings and 1,677 hours of non transcribed data. The manual annotations include the detailed verbatim orthographic transcription, the speaker turns and identities, information about acoustic conditions, and name entities. Additional resources generated by automatic speech processing systems, such as phonetic alignments and word graphs, are also described.
Third generation (3G) services boost mobile multimodal interaction offering users richer communication alternatives for accessing different applications and information services. These 3G services provide more interaction alternatives as well as active learning possibilities than previous technologies but, at the same time, these facts increase the complexity of user interfaces. Therefore, usability in multimodal interfaces has become a key factor in the service design process. In this paper we present the work done to evaluate the usability of automatic video services based on avatars with real potential users of a video-voice mail service. We describe the methodology, the tests carried out and the results and conclusions of the study. This study addresses UMTS/3G problems like the interface model, the voice-image synchronization and the user attention and memory. All the user tests have been carried out using a mobile device to take into account the constraints imposed by the screen size and the presentation and interaction limitations of a current mobile phone.
In this paper we present an experiment to automatically generate annotated training corpora for a supervised word sense disambiguation module operating in an English-Hungarian and a Hungarian-English machine translation system. Training examples for the WSD module of the MT system are produced by annotating ambiguous lexical items in the source language (words having several possible translations) with their proper target language translations. Since manually annotating training examples is very costly, we are experimenting with a method to produce examples automatically from parallel corpora. Our algorithm relies on monolingual and bilingual lexicons and dictionaries in addition to statistical methods in order to annotate examples extracted from a large English-Hungarian parallel corpus accurately aligned at sentence level. In the paper, we present an experiment with the English noun state, where we categorized the different occurrences in the Hunglish parallel corpus. For this noun, most of the examples were covered by multiword lexical items originating from our lexical sources.
This paper describes our contribution to let end users configure mixed-initiative spoken dialogue systems to suit their personalized goals. The main problem that we want to address is the reconfiguration of spoken language dialogue systems to deal with generic plug and play artifacts. Such reconfiguration can be seen as a portability problem and is a critical research issue. In order to solve this problem we describe a hybrid approach to design ubiquitous domain models that allows the dialogue system to perform recognition of available tasks on the fly. Our approach considers two kinds of domain knowledge: the global knowledge and the local knowledge. The global knowledge, that is modeled using a top-down approach, is associated at design time with the dialogue system itself. The local knowledge, that is modeled using a bottom-up approach, is defined with each one of the artifacts. When an artifact is activated or deactivated, a bilateral process, supported by a broker, updates the domain knowledge considering the artifact local knowledge. We assume that everyday artifacts are augmented with computational capabilities and semantic descriptions supported by their own knowledge model. A case study focusing a microwave oven is depicted.
The paper reports on a detailed quantitative analysis of distributional language data of both Italian and Czech, highlighting the relative contribution of a number of distributed grammatical factors to sentence-based identification of subjects and direct objects. The work is based on a Maximum Entropy model of stochastic resolution of grammatical conflicting constraints, and is demonstrably capable of putting explanatory theoretical accounts to the challenging test of an extensive, usage-based empirical verification.
In the paper we present the actual state of development of an international standard for syntactic annotation, called SynAF. This standard is being prepared by the Technical Committee ISO/TC 37 (Terminology and Other Language Resources), Subcommittee SC 4 (Language Resource Management), in collaboration with the European eContent Project LIRICS (Linguistic Infrastructure for Interoperable Resources and Systems).
This paper describes the EQueR-EVALDA Evaluation Campaign, the French evaluation campaign of Question-Answering (QA) systems. The EQueR Evaluation Campaign included two tasks of automatic answer retrieval: the first one was a QA task over a heterogeneous collection of texts - mainly newspaper articles, and the second one a specialised one in the Medical field over a corpus of medical texts. In total, seven groups participated in the General task and five groups participated in the Medical task. For the General task, the best system obtained 81.46{\%} of correct answers during the evalaution of the passages, while it obtained 67.24{\%} during the evaluation of the short answers. We describe herein the specifications, the corpora, the evaluation, the phase of judgment of results, the scoring phase and the results for the two different types of evaluation.
Linguistic Resources for the Study of the Portuguese African Varieties is an ongoing project that aims at the constitution, treatment, analysis and availability of a corpus of the African varieties of Portuguese, with 3 million words of written and spoken texts, constituted by five comparable subcorpora, corresponding to the varieties of Angola, Cape Verde, Guinea-Bissau, Mozambique and Sao Tome and Principe. This material will allow intra and intercorpora comparative studies, which will make visible variations that result from discursive and pragmatic differences of each corpus and aspects of linguistic unity or diversity that characterise the spoken Portuguese of this referred five African countries. The five corpora are comparable in size (600,000 words each), in chronology (the last 30 years) and in types and genres (24,000 spoken words and c. 580,000 written words, the last belonging to newspapers, literature and varia). The corpus is automatically annotated and after the extraction of alphabetical lists of lexical forms, these data will be automatically lemmatised. Five separated lists of vocabulary for each variety will be established. A tool for word extraction and preferential calculus according to predefined indexes in order to achieve lexicon comparison of the African Portuguese Varieties is being developed. Concordances extraction will be also performed.
In this paper, we propose a corpus-based approach to the construction of a Pan-Chinese lexical resource, starting out with the aim to enrich existing Chinese thesauri in the Pan-Chinese context. The resulting thesaurus is thus expected to contain not only the core senses and usages of Chinese lexical items but also usages specific to individual Chinese speech communities. We introduce the ideas behind the construction of the resource, outline the steps to be taken, and discuss some preliminary analyses. The work is backed up by a unique and large Chinese synchronous corpus containing textual data from various Chinese speech communities including Hong Kong, Beijing, Taipei and Singapore.
The Dutch Language Corpus Initiative (D-Coi) project aims to specify the design of a 500-million-word reference corpus of written Dutch, and to put the tools and procedures in place that are needed to actually construct such a corpus. One of the tasks in the project is to conduct a user requirements study that should provide the basis for the eventual design of the 500-million-word reference corpus. The present paper outlines the user requirements analysis and reports the results so far.
Question-answering (QA) systems aim at providing either a small passage or just the answer to a question in natural language. We have developed several QA systems that work on both English and French. This way, we are able to provide answers to questions given in both languages by searching documents in both languages also. In this article, we present our French monolingual system FRASQUES which participated in the EQueR evaluation campaign of QA systems for French in 2004. First, the QA architecture common to our systems is shown. Then, for every step of the QA process, we consider which steps are language-independent, and for those that are language-dependent, the tools or processes that need to be adapted to switch for one language to another. Finally, our results at EQueR are given and commented; an error analysis is conducted, and the kind of knowledge needed to answer a question is studied.
The paper gives an overview of the evaluation methods of memory-based translation systems: Translation Memories (TM) and Example Based Machine Translation (EBMT) systems. After a short comparison with the well-discussed methods of evaluation of Machine Translation (MT) Systems we give a brief overview of current methodology on memory-based applications. We propose a new aspect, which takes the content of memory into account: a measure to describe the correspondence between the memory and the current segment to translate. We also offer a brief survey of a linguistically enriched translation memory on which these new methods will be tested.
In this article we present T2O - a workbench to assist the process of translating heterogeneous resources into ontologies, to enrich and add multilingual information, to help programming with them, and to support ontology publishing. T2O is an ontology algebra.
This paper describes a simple approach of statistical language modelling for bilingual lexicon acquisition from Amharic-English parallel corpora. The goal is to induce a seed translation lexicon from sentence-aligned corpora. The seed translation lexicon contains matches of Amharic lexemes to weekly inflected English words. Purely statistical measures of term distribution are used as the basis for finding correlations between terms. An authentic scoring scheme is codified based on distributional properties of words. For low frequency terms a two step procedure of: first a rough alignment; and then an automatic filtering to sift the output and improve the precision is made. Given the disparity of the languages and the small size of corpora used the results demonstrate the viability of the approach.
ISA and ICA are two web interfaces for interactive alignment of parallel texts. ISA provides an interface for automatic and manual sentence alignment. It includes cognate filters and uses structural markup to improve automatic alignment and provides intuitive tools for editing them. Alignment results can be saved to disk or sent via e-mail. ICA provides an interface to the clue aligner from the Uplug toolbox. It allows one to set various parameters and visualizes alignment results in a two-dimensional matrix. Word alignments can be edited and saved to disk.
In this paper we present the setup of an extensive Wizard-of-Oz environment used for the data collection and the development of a dialogue system. The envisioned Perception and Interaction Assistant will act as an independent dialogue partner. Passively observing the dialogue between the two human users with respect to a limited domain, the system should take the initiative and get meaningfully involved in the communication process when required by the conversational situation. The data collection described here involves audio and video data. We aim at building a rich multi-media data corpus to be used as a basis for our research which includes, inter alia, speech and gaze direction recognition, dialogue modelling and proactivity of the system. We further aspire to obtain data with emotional content to perfom research on emotion recognition, psychopysiological and usability analysis.
The Parmenides project developed a text mining application applied in three different domains exemplified by case studies for the three user partners in the project. During the lifetime of the project (and in parallel with the development of the system itself) an evaluation framework was developed by the authors in conjunction with the users, and was eventually applied to the system. The object of the exercise was two-fold: firstly to develop and perform a complete user-centered evaluation of the system to assess how well it answered the users' requirements and, secondly, to develop a general framework which could be applied in the context of other users' requirements and (with some modification) to similar systems. In this paper we describe not only the framework but the process of building and parameterising the quality model for each case study and, perhaps most interestingly, the way in which the quality model and users' requirements and expectations evolved over time.
In this paper, a pilot study for the development of a corpus of Dutch Aphasic Speech (CoDAS) is presented. Given the lack of resources of this kind not only for Dutch but also for other languages, CoDAS will be able to set standards and will contribute to the future research in this area. Given the special character of the speech contained in CoDAS, we cannot simply carry over the design and annotation protocols of existing corpora, such as the Corpus Gesproken Nederlands or CHILDES. However, they have been assumed as starting point. We have investigated whether and how the procedures and protocols for the annotation (part-of-speech tagging) and transcription (orthographic and phonetic) used for the CGN should be adapted in order to annotate and transcribe aphasic speech properly. Besides, we have established the basic requirements with respect to text types, metadata, and annotation levels that CoDAS should fulfill.
All systems for automatic sign language translation and recognition, in particular statistical systems, rely on adequately sized corpora. For this purpose, we created the Phoenix corpus that is based on German television weather reports translated into German Sign Language. It comes with a rich annotation of the video data, a bilingual text-based sentence corpus and a monolingual German corpus. All systems for automatic sign language translation and recognition, in particular statistical systems, rely on adequately sized corpora. For this purpose, we created the Phoenix corpus that is based on German television weather reports translated into German Sign Language. It comes with a rich annotation of the video data, a bilingual text-based sentence corpus and a monolingual German corpus.
In this paper we present an original approach to natural language query interpretation which has been implemented withinthe FuLL (Fuzzy Logic and Language) Italian project of BC S.r.l. In particular, we discuss here the creation of linguisticand ontological resources, together with the exploitation of existing ones, for natural language-driven database access andretrieval. Both the database and the queries we experiment with are Italian, but the methodology we broach naturally extends to other languages.
Present-day machine translation technologies crucially depend on the size and quality of lexical resources. Much of recent research in the area has been concerned with methods to build bilingual dictionaries automatically. In this paper we propose a methodology for the automatic detection of cognates between two languages based solely on the orthography of words. From a set of known cognates, the method induces rules capturing regularities of orthographic mutations that a word undergoes when migrating from one language into the other. The rules are then applied as a preprocessing step before measuring the orthographic similarity between putative cognates. As a result, the method allows to achieve an improvement in the F-measure of 11,86{\%} in comparison with detecting cognates based only on the edit distance between them.
Tokenisers, lemmatisers and POS taggers are vital to the linguistic and digital furtherment of any language. In this paper, we present an open source toolkit for Malay incorporating a word and sentence tokeniser, a lemmatiser and a partial POS tagger, based on heavy reuse of pre-existing language resources. We outline the software architecture of each component, and present an evaluation of each over a 26K word sample of Malay text.
This paper discusses the inherent difficulties in evaluating systems for theme detection. Such systems are based essentially on unsupervised clustering aiming to discover the underlying structure in a corpus of texts. As the structures are precisely unknown beforehand, it is difficult to devise a satisfactory evaluation protocol. Several problems are posed by cluster evaluation: determining the optimal number of clusters, cluster content evaluation, topology of the discovered structure. Each of these problems has been studied separately but some of the proposed metrics portray significant flaws. Moreover, no benchmark has been commonly agreed upon. Finally, it is necessary to distinguish between task-oriented and activity-oriented evaluation as the two frameworks imply different evaluation protocols. Possible solutions to the activity-oriented evaluation can be sought from the data and text mining communities.
This paper describes a joint initiative of the Catalan and Spanish Government to produce Language Resources for the Catalan language. A similar methodology to the Basic Language Resource Kit (BLARK) concept was applied to determine the priorities on the production of the Language Resources. The paper shows the LR and tools currently available for the Catalan Language both for Language and Speech technologies. The production of large databases for Automatic Speech Recognition purposes already started. All the resources generated in the project follow EU standards, will be validated by an external centre and will be free and public available through ELRA.
The adverb ``then'' is among the most frequent Englishtemporal adverbs, being also capable of filling a variety of semantic roles. The identification of anaphoric usages of ``then``is important for temporal expression resolution, while thetemporal relationship usage is important for event ordering. Given that previous work has not tackled the identification and temporal resolution of anaphoric ``then'', this paper presents a machine learning approach for setting apart anaphoric usages and a rule-based normaliser that resolves it with respect to an antecedent. The performance of the two modules is evaluated. The present paper also describes the construction of an annotated corpus and the subsequent derivation of training data required by the machine learning module.
This paper describes morphdb.hu, a Hungarian lexical database and morphological grammar. Morphdb.hu is the outcome of a several-year collaborative effort and represents the resource with the widest coverage and broadest range of applicability presently available for Hungarian. The grammar resource is the formalization of well-founded theoretical decisions handling inflection and productive derivation. The lexical database was created by merging three independent lexical databases, and the resulting resource was further extended.
In this paper, we present the first sizable grammar built for Vietnamese using LTAG, developed over the past two years, named vnLTAG. This grammar aims at modelling written language and is general enough to be both application- and domain-independent. It can be used for the morpho-syntactic tagging and syntactic parsing of Vietnamese texts, as well as text generation. We then present a robust parsing scheme using vnLTAG and a parser for the grammar. We finish with an evaluation using a test suite.
This work focuses on semi-automatic extraction of verb-noun collocations from a corpus, performed to provide lexical evidence for the manual lexicographical processing of Support Verb Constructions (SVCs) in the Swedish-Czech Combinatorial Valency Lexicon of Predicate Nouns. Efficiency of pure manual extractionprocedure is significantly improved by utilization of automatic statistical methods based lexical association measures.
The main aim of this study is to describe the process of creating a speech database to be used in corpus based text-to-speech synthesis. To help achieve natural sounding speech synthesis, the database construction was aimed at rich phonetic and prosodic coverage based on variable length units (phoneme, diphone, triphone) from different phonetic and prosodic contexts. Following previous work on determining the optimal coverage (Szklanny and Oliver, 2005), text selection was based on the existing text corpus containing parliamentary statements. Corpus balancing was followed by recording of the material. Automatic segmentation was performed, followed by both an automatic and manual check of the data to determine speaker specific phenomena and correct the labelling. Additionally, prosodic annotation involving assignment of the intonation contours was performed in order to assess the accent realisation and determine the prosodic coverage of the database. The prototype speech synthesiser was built to determine the validity of the above steps and test the resulting voice quality.
This paper describes an ongoing Portuguese Language grammar checker project, called CoGrOO1-Corretor Gramatical para OpenOffice (Grammar Checker for OpenOffice), based on CETENFOLHA, a Brazilian Portuguese morphosyntactic annotated Corpus. Two of its features are highlighted: - hybrid architecture, mixing rules and statistics; - free software project. This project aims at checking grammatical errors such as nominal and verbal agreement, crase (the coalescence of preposition a (to) + definitive singular determiner a yielding {\`a}), nominal and verbal government and other common errors in Brazilian Portuguese Language. We also present some empirical results based on the implemented techniques.
The paper reports on the evaluation of a rule-based technique to model prototypical non-native pronunciation variants on the symbolic transcription level. This technique was developed to explore the possibility of an automatic generation of adapted pronunciation lexicons for different non-native speaker groups. The rule sets, which are currently available for nine language directions, are based on non-native speech data compiled specifically for this purpose. Since manual phonetic annotations are available for the speech data, the evaluation was performed on the transcription level by measuring the phonetic distance of the automatically generated pronunciations variants and actual pronunciations of non-native speakers. One of the central questions to be addressed by the evaluation is whether the rules have any predictive value: It has to be determined if and to what degree the rules are capable of generating realistic pronunciation variants for previously unseen speakers. Secondly, the rules should not only represent the pronunciations of individual speakers adequately; instead, they should be representative of speaker groups (cross-speaker representation). The paper outlines the evaluation methodology and presents results for selected language directions.
The present work illustrates the main results of an experiment on errors and repairs in spoken language transcription, with significant relevance for the evaluation of validity, reliability and correctness of transcriptions of speech belonging to several different typologies, set for the annotation of spoken corpora. In particular, we dealt with errors and repair strategies that appear on the first drafts of the transcription process that are not easily detectable with automatic post-editing procedures. 20 participants were asked to give an accurate transcription of 22 short utterances, repeated from one to four times, belonging two non-spontaneous (10) and spontaneous conversation (10). Error analysis suggests a general preference for meaning preservation even after the alteration of the original form, and for the preference for certain error patterns and repair strategies.
Recently, there have been efforts to construct written corpora by using the WWW. A promising approach to build Web corpora is to run automated queries to search engines and download pages found in this way. This makes it possible to build corpora rapidly and economically, but we cannot control what are contained in resulting corpora. Under these circumstances, it is important to verify the general nature of Web corpora. This study, in particular, investigated effects of two essential factors on three Japanese corpora that we built: seed terms used for queries; and time interval between different corpus construction sessions, which measures the stability of query results over time. We evaluated the corpora qualitatively, in terms of domains, genres and typical lexical items. Results show these two patterns: 1) both seed selection and time interval affect the distribution of text and lexicon; 2) the effect of seed selection is much stronger. The prominent effect of seed selection suggests that a good understanding of the cause-and-effect relation between seeds and retrieved documents is an important step to gain some control over the characteristics of Web corpora, in particular, for the construction of general corpora meant to represent a language as a whole.
In this paper we present a new approach to ontology learning. Its basis lies in a dynamic and iterative view of knowledge acquisition for ontologies. The Abraxas approach is founded on three resources, a set of texts, a set of learning patterns and a set of ontological triples, each of which must remain in equilibrium. As events occur which disturb this equilibrium various actions are triggered to re- establish a balance between the resources. Such events include acquisition of a further text from external resources such as the Web or the addition of ontological triples to the ontology. We develop the concept of a knowledge gap between the coverage of an ontology and the corpus of texts as a measure triggering actions. We present an overview of the algorithm and its functionalities.
The pronunciation lexicon is a fundamental element in an automatic speech transcription system. It associates each lexical entry (usually a grapheme), with one or more phonemic or phone-like forms, the pronunciation variants. Thorough knowledge of the target language is a priori necessary to establish the pronunciation baseforms and variants. The reliance on human expertise can pose difficulties in developing a system for a language where such knowledge may not be readily available. In this article a speech recognizer is used to help select pronunciation variants in Amharic, the official language of Ethiopia, focusing on alternate choices for vowels. This study is carried out using an audio corpus composed of 37 hours of speech from radio broadcasts which were orthographically transcribed by native speakers. Since the corpus is relatively small for estimating pronunciation variants, a first set of studies were carried out at a syllabic level. Word lexica were then constructed based on the observed syllable occurences. Automatic alignments were compared for lexica containing different vowel variants, with both context-independent and context-dependent acoustic models sets. The variant2+ measure proposed in (Adda-Decker and Lamel, 1999) is used to assess the potential need for pronunciation variants.
As web searches increase, there is a need to represent the search results in the most comprehensible way possible. In particular, we focus on search results from queries about people and places. The standard method for presentation of search results is an ordered list determined by the Web search engine. Although this is satisfactory in some cases, when searching for people and places, presenting the information indexed by time may be more desirable. We are developing a system called Cronopath, which generates a timeline of web search engine results by determining the time frame of each document in the collection and linking elements in the timeline to the relevant articles. In this paper, we propose evaluation guidelines for judging the quality of automatically generated timelines based on a set of common features.
We describe a corpus of multimodal dialogues with an MP3player collected in Wizard-of-Oz experiments and annotated with a richfeature set at several layers. We are using the Nite XML Toolkit (NXT) to represent and further process the data. We designed an NXTdata model, converted experiment log file data and manualtranscriptions into NXT, and are building tools for additionalannotation using NXT libraries. The annotated corpus will be used to (i) investigate various aspects of multimodal presentation andinteraction strategies both within and across annotation layers; (ii) design an initial policy for reinforcement learning of multimodalclarification requests.
Digital image collections in libraries and other curatorial institutions grow too rapidly to create new descriptive metadata for subject matter search or browsing. CLiMB (Computational Linguistics for Metadata Building) was a project designed to address this dilemma that involved computer scientists, linguists, librarians, and art librarians. The CLiMB project followed an iterative evaluation model: each next phase of the project emerged from the results of an evaluation. After assembling a suite of text processing tools to be used in extracting metada, we conducted a formative evaluation with thirteen participants, using a survey in which we varied the order and type of four conditions under which respondents would propose or select image search terms. Results of the formative evaluation led us to conclude that a CLiMB ToolKit would work best if its main function was to propose terms for users to review. After implementing a prototype ToolKit using a browser interface, we conducted an evaluation with ten experts. Users found the ToolKit very habitable, remained consistently satisfied throughout a lengthy evaluation, and selected a large number of terms per image.
Traditionally, context features used in word sense disambiguation are based on collocation statistics and use only minimal syntactic and semantic information. Corpus Pattern Analysis is a technique for producing knowledge-rich context features that capture sense distinctions. It involves (1) identifying sense-carrying context patterns and using the derived context features to discriminate between the unseen instances. Both stages require manual seeding. In this paper, we show how to automate inducing sense-discriminating context features from a sense-tagged corpus.
Entailment rules are rules where the left hand side (LHS) specifies some knowledge which entails the knowledge expressed n the RHS of the rule, with some degree of confidence. Simple entailment rules can be combined in complex entailment chains, which n turn are at the basis of entailment-based reasoning, which has been recently proposed as a pervasive and application independent approach to Natural Language Understanding. We present the first elease of a large-scale repository of entailment rules at the lexical level, which have been derived from a number of available resources, including WordNet and a word similarity database. Experiments on the PASCAL-RTE dataset show that this resource plays a crucial role in recognizing textual entailment.
A critical step in Question Answering design is the definition of the models for question focus identification and answer extraction. In case of factoid questions, we can use a question classifier (trained according to a target taxonomy) and a named entity recognizer. Unfortunately, this latter cannot be applied to generate answers related to non-factoid questions. In this paper, we tackle such problem by designing classifiers of non-factoid answers. As the feature design for this learning task is very complex, we take advantage of tree kernels to generate large feature set from the syntactic parse trees of passages relevant to the target question. Such kernels encode syntactic and lexical information in Support Vector Machines which can decide if a sentence focuses on a target taxonomy subject. The experiments with SVMs on the TREC 10 dataset show that our approach is an interesting future research.
The EVALDA/EvaSy project is dedicated to the evaluation of text-to-speech synthesis systems for the French language. It is subdivided into four components: evaluation of the grapheme-to-phoneme conversion module (Boula de Mare{\"u
Tokenisation is an important first pre-processing step required to adequately test finite-state morphological analysers. In agglutinative languages each morpheme is concatinatively added on to form a complete morphological structure. Disjunctive agglutinative languages like Northern Sotho write these morphemes, for certain morphological categories only, as separate words separated by spaces or line breaks. These breaks are, by their nature, different from breaks that separate words that are written conjunctively. A tokeniser is required to isolate categories, like a verb, from raw text before they can be correctly morphologically analysed. The authors have successfully produced a finite state tokeniser for Northern Sotho, where verb segments are written disjunctively but nominal segments conjunctively. The authors show that since reduplication in the Northern Sotho language does not affect the pre-processing tokeniser, the disjunctive standard verbal segment as a construct in Northern Sotho is deterministic, finite-state and a regular Type 0 language in the Chomsky hierarchy and that the copulative verbal segment, due to its semi-disjunctivism, is ambiguously non-deterministic.
In this paper, we describe a formal constraint mechanism, which we label Conceptual Constraint Variables (CCVs), introduced to restrict surface patterns during automated text analysis with the objective of increasing precision in the representation of informational contents. We briefly present, and exemplify, the various types of CCVs applicable to the English texts of our corpora, and show how these constraints allow us to resolve some of the problems inherent to surface pattern recognition, more specifically, those related to the resolution of conceptual or syntactic ambiguities introduced by the most frequent English prepositions.
In this paper, we look into the notion of cross-media decision mechanisms, focussing on ones that work within multimedia documents for a variety of applications, such as the generation of intelligent multimedia presentations and multimedia indexing. In order for these mechanisms to go beyond the identification of semantic equivalence relations between media, which is what integration does, appropriate corpora and annotations are needed. Drawing from our experience in the REVEAL THIS project, we indicate the characteristics that such corpora should have, and suggest a number of annotations that would allow for training/designing such mechanisms. We conclude with a view on the suitability of two related markup languages (MPEG-7 and EMMA) for accommodating the suggested annotations.
We illustrate the effectiveness of medium-sized carefully tagged bilingual core corpus, that is, semantic typology patterns in our term together with some examples to give concrete evidence of its usefulness. The most important characteristic of these semantic typology patterns is the bridging mechanism between two languages which is based on sequences syntactic codes and semantic codes. This characteristic gives both wide coverage and flexible applicability of core bilingual core corpus though its volume size is not so large. A further work is to be done for grasping some intuitive feeling of pertinent coarseness and fineness of patterns. Here coarseness feeling is concerning the generalization in phrase-level and clause-level semantic patterns and fineness is concerning word-level semantic patterns. Based on this feeling we will complete the core tagged bilingual corpora while enhancing the necessary support functions and utilities.
We present SlinkET, a parser for identifying contexts of event modality in text developed within the TARSQI (Temporal Awareness and Reasoning Systems for Question Interpretation) research framework. SlinkET is grounded on TimeML, a specification language for capturing temporal and event related information in discourse, which provides an adequate foundation to handle event modality. SlinkET builds on top of a robust event recognizer, and provides each relevant event with a value that specifies the degree of certainty about its factuality; e.g., whether it has happened or holds (factive or counter-factive), whether it is being reported or witnessed by somebody else (evidential), or if it is introduced as a possibility (modal). It is based on well-established technology in the field (namely, finite-state techniques), and informed with corpus-induced knowledge that relies on basic information, such as morphological features, POS, and chunking. SlinkET is under continuing development and it currently achieves a performance ratio of 70{\%} F1-measure.
This presentation reports on recent progress the Linguistic Data Consortium has made in addressing the needs of multiple research communities by collecting, annotating and distributing, simplifying access and developing standards and tools. Specifically, it describes new trends in publication, a sample of recent projects and significant improvements to LDC Online that improve access to LDC data especially for those with limited computing support.
In the framework of the EU funded project TC-STAR (Technology and Corpora for Speech to Speech Translation),research on TTS aims on providing a synthesized voice sounding like the source speaker speaking the target language. To progress in this direction, research is focused on naturalness, intelligibility, expressivity and voice conversion both, in the TC-STAR framework. For this purpose, specifications on large, high quality TTS databases have been developed and the data have been recorded for UK English, Spanish and Mandarin. The development of speech technology in TC-STAR is evaluation driven. Assessment of speech synthesis is needed to determine how well a system or technique performs in comparison to previous versions as well as other approaches (systems {\&} methods). Apart from testing the whole system, all components of the system will be evaluated separately. This approach grants better assesment of each component as well as identification of the best techniques in the different speech synthesisprocesses.This paper describes the specifications of Language Resources for speech synthesis and the specifications for evaluation of speech synthesis activities.
We present the national project Parlare italiano: osservatorio degli usi linguistici, funded by the Italian Ministry of Education, Scientific Research and University (PRIN 2004). Ten research groups participate to the project from various Italian universities. The project has four fundamental objectives: 1) to plan a national website that collects the most recent theoretical and applied results on spoken language; 2) to create an observatory of the linguistic usages of the Italian spoken language; 3) to delineate and implement standard and formalized methods and procedures for the study of spoken language; 4) to develop a training program for young researchers. The website will be accessible starting from November 2006.
The agreement in gender and number is a critical problem in statistical language modeling. One of the main problems in the speech recognition of French language is the presence of misrecognized words due to the bad agreement (in gender and number) between words. Statistical language models do not treat this phenomenon directly. This paper focuses on how to handle the issue of agreements. We introduce an original model called Features-Cache (FC) to estimate the gender and the number of the word to predict. It is a dynamic variable-length Features-Cache for which the size is determined in accordance to syntagm delimitors. This model does not need any syntactic parsing, it is used as any other statistical language model. Several models have been carried out and the best one achieves an improvement of more than 8 points in terms of perplexity.
We present the lexico-semantic foundations underlying a multilingual lexicon the entries of which are constituted by so-called subwords. These subwords reflect semantic atomicity constraints in the medical domain which diverge from canonical lexicological understanding in NLP. We focus here on criteria to identify and delimit reasonable subword units, to group them into functionally adequate synonymy classes and relate them by two types of lexical relations. The lexicon we implemented on the basis of these considerations forms the lexical backbone for MorphoSaurus, a cross-language document retrieval engine for the medical domain.
We investigate a corpus of geographical distributions of 17,126 Finnish dialect words. Our goal is to automatically find sets of words characteristic to geographical regions. Though our approach is related to the problem of dividing the investigation area into linguistically (and geographically) relatively coherent dialect regions, we do not aim at constructing more or less questionable dialect regions. Instead, we let the boundaries of the regions overlap to get insight to the degree of lexical change between adjacent areas. More concretely, we study the applicability of data clustering approaches to find sets of words with tight spatial distributions, and to cluster the extracted distributions according to their distribution areas. The extracted words belonging to the same cluster can then be utilized as a means to characterize the lexicon of the region. We also automatically pick up words with occurrences appearing in two or more areas that are geographically far from each other. These words may give valuable insight to, e.g., the study of cultural history and history of settlement.
State-of-the-art statistical approaches to the Coreference Resolution task rely on sophisticated modeling, but very few (10-20) simple features. In this paper we propose to extend the standard feature set substantially, incorporating more linguistic knowledge. To investigate the usability of linguistically motivated features, we evaluate our system for a variety of machine learners on the standard dataset (MUC-7) with the traditional learning set-up.
Tasks performed on machine translation (MT) output are associated with input text types such as genre and topic. Predictive Linguistic Assessments of Translation Output, or PLATO, MT Evaluation (MTE) explores a predictive relationship between linguistic metrics and the information processing tasks reliably performable on output. PLATO assigns a linguistic signature, which cuts across the task-based and automated metric paradigms. Here we report on PLATO assessments of clarity, coherence, morphology, syntax, lexical robustness, name-rendering, and terminology in a comparison of Arabic MT engines in which register differentiates the input. With a team of 10 assessors employing eight linguistic tests, we analyzed the results of five systems processing of 10 input texts from two distinct linguistic registers: a total we analyzed 800 data sets. The analysis pointed to specific areas, such as general lexical robustness, where system performance was comparable on both types of input. Divergent performance, however, was observed on clarity and name-rendering assessments. These results suggest that, while systems may be considered reliable regardless of input register for the lexicon-dependent triage task, register may have an affect on the suitability of MT systems output for relevance judgment and information extraction tasks, which rely on clearness and proper named-entity rendering. Further, we show that the evaluation metrics incorporated in PLATO differentiate between MT systems performance on a text type for which they are presumably optimized and one on which they are not.
In this paper we report an experiment of an automated metric used to analyse the grammaticality of machine translation output. The approach (Rajman, Hartley, 2001) is based on the distribution of the linguistic information within a translated text, which is supposed similar between a learning corpus and the translation. This method is quite inexpensive, since it does not need any reference translation. First we describe the experimental method and the different tests we used. Then we show the promising results we obtained on the CESTA data, and how they correlate well with human judgments.
WordNet is the reference sense inventory of most of the current Word Sense Disambiguation systems. Unfortunately, it encodes too fine-grained distinctions, making it difficult even for humans to solve the ambiguity of words in context. In this paper, we present a method for reducing the granularity of the WordNet sense inventory based on the mapping to a manually crafted dictionary encoding sense groups, namely the Oxford Dictionary of English. We assess the quality of the mapping and discuss the potential of the method.
The Basic Language Resource Kit (BLARK) proposed by Krauwer is designed for the creation of initial textual resources. There are a number of toolkits for the development of spoken language resources and systems, but tools for second level resources, that is, resources which are the result of processing primary level speech resources such as speech recordings. Typically, processing of this kind in phonetics is done manually, with the aid of spreadsheets multi-purpose statistics software. We propose a Basic Language and Speech Kit (BLAST) as an extension to BLARK and suggest a strategy for integrating the kit into the Natural Language Toolkit (NLTK). The prototype kit is evaluated in an application to examining temporal properties of spoken Brazilian Portuguese.
At least in the realm of fast parsing, the masscount distinction has led the life of a wallflower. We argue in this paper that this should not be so. In particular, we argue, both theoretical linguistics and computational linguistics can gain by a corpus-based investigation of this distinction: Computational linguists get more accurate parses; the knowledge extracted from these parses becomes more reliable; theoretical linguists are presented with new data in a field that has been intensely discussed and yet remains in a state that is not satisfactory from a practical point of view.
This paper will discuss issues relevant to corpus development and publication at the LDC and will illustrate those issues by examining the history of three LDC corpora. This paper will also briefly examine alternative corpus creation and distribution methods and their challenges. The intent of this paper is to increase the available linguistic resources by describing the regulatory and technical environment and thus improving the understanding and interaction between corpus providers and distributors.
A dedicated resource, consisting of annotated speech tools, and workflow design, was developed for the detailed investigation of discourse phenomena in Taiwan Mandarin. The discourse phenomena have functions which are associated with positions in utterances, and temporal properties, and include discourse markers (NAGE, NA, e.g. hesitation, utterance initiation), discourse particles (A, e.g. utterance finality, utterance continuity, focus, etc.), and fillers (UHN, hesitation). The distribution of particles in relation to their position in utterances and the temporal properties of particles are investigated. The results of the investigation diverge considerably from claims in existing grammars of Mandarin with respect to utterance position, and show in general greater length than for regular syllables. These properties suggest the possibility of developing an automatic discourse item tagger.
This paper describes an approach to Natural Language access to databases based on ontologies. Their role is to make the central part of the translation process independent both of the specific language and of the particular database schema. The input sentence is parsed and the parse tree is semantically annotated via references to the ontology describing the application. This first step is, of course, language dependent: the parsing process depends on the syntax of the language and the annotation depends on the meaning of words, expressed as links between words and concepts in the ontology. Then, the annotated tree is used to produce an ontological query, i.e. a query expressed in terms of paths on the ontology. This second step is entirely language- and DB-independent. Finally, the ontological query is translated into a standard SQL query, on the basis of a concept-to-DB mapping, specifying how each concept and relation is mapped onto the database.
The paper describes the ALVIS annotation format and discusses the problems that we encountered for the indexing of large collections of documents for topic specific search engines. This paper is exemplified on the biological domain and on MedLine abstracts, as developing a specialized search engine for biologist is one of the ALVIS case studies. The ALVIS principle for linguistic annotations is based on existing works and standard propositions. We made the choice of stand-off annotations rather than inserted mark-up, and annotations are encoded as XML elements which form the linguistic subsection of the document record.
We propose a new method for measuring the threshold of 50{\%} sentence intelligibility in noisy or multi-source speech communication situations (Speech Reception Threshold, SRT). Our SRT-test complements those available e.g. for English, German, Dutch, Swedish and Finnish by a French test method. The approach we take is based on semantically unpredictable sentences (SUS), which can principally be created for various languages. This way, the proposed method enables better cross-language comparisons of intelligibility tests. As a starting point for the French language, a set of 288 sentences (24 lists of 12 sentences each) was created. Each of the 24 lists is optimized for homogeneity in terms of phoneme-distribution as compared to average French, and for word occurrence frequency of the employed monosyllabic keywords as derived from French language databases. Based on the optimized text material, a speech target sentence database has been recorded with a trained speaker. A test calibration was carried out to yield uniform measurement results over the set of target sentences. First intelligibility measurements show good reliability of the method.
Linguistic Data Consortium has recently embarked on an effort to create integrated linguistic resources and related infrastructure for language exploitation technologies within the DARPA GALE (Global Autonomous Language Exploitation) Program. GALE targets an end-to-end system consisting of three major engines: Transcription, Translation and Distillation. Multilingual speech or text from a variety of genres is taken as input and English text is given as output, with information of interest presented in an integrated and consolidated fashion to the end user. GALE's goals require a quantum leap in the performance of human language technology, while also demanding solutions that are more intelligent, more robust, more adaptable, more efficient and more integrated. LDC has responded to this challenge with a comprehensive approach to linguistic resource development designed to support GALE's research and evaluation needs and to provide lasting resources for the larger Human Language Technology community.
This paper describes Champollion, a lexicon-based sentence aligner designed for robust alignment of potential noisy parallel text. Champollion increases the robustness of the alignment by assigning greater weights to less frequent translated words. Experiments on a manually aligned Chinese  English parallel corpus show that Champollion achieves high precision and recall on noisy data. Champollion can be easily ported to new language pairs. Its freely available to the public.
The Eclipse-Annotator is an extensible tool for the creation of multimodal language resources. It is based on the TASX-Annotator, which has been refactored in order to fit into the plugin based architecture of the new application.
In this paper we give an overview of the approach adopted to add a layer of semantic information to the Greek Dependency Treebank [GDT]. Our ultimate goal is to come up with a large corpus, reliably annotated with rich semantic structures. To this end, a corpus has been compiled encompassing various data sources and domains. This collection has been preprocessed, annotated and validated on the basis of dependency representation. Taking into account multi-layered annotation schemes designed to provide deeper representations of structure and meaning, we describe the methodology followed as regards the semantic layer, we report on the annotation process and the problems faced and we conclude with comments on future work and exploitation of the resulting resource.
The paper investigates the issue of portability of methods and results over treebanks in different languages and annotation formats. In particular, it addresses the problem of converting an Italian treebank, the Turin University Treebank (TUT), developed in dependency format, into the Penn Treebank format, in order to possibly exploit the tools and methods already developed and compare the adequacy of information encoding in the two formats. We describe the procedures for converting the two annotation formats and we present an experiment that evaluates some linguistic knowledge extracted from the two formats, namely sub-categorization frames.
The paper presents an on-line dialectal resource, ALT-Web, which gives access to the linguistic data of the Atlante Lessicale Toscano, a specially designed linguistic atlas in which lexical data have both a diatopic and diastratic characterisation. The paper focuses on: the dialectal data representation model; the access modalities to the ALT dialectal corpus; ontology-based search.
This paper describes LDC's efforts in collecting, creating and processing different types of linguistic data, including lexicons, parallel text, multiple translation corpora, and human assessment of translation quality, to support the research and development in Machine Translation. Through a combination of different procedures and core technologies, the LDC was able to create very large, high quality, and cost-efficient corpora, which have contributed significantly to recent advances in Machine Translation. Multiple translation corpora and human assessment together facilitate, validate and improve automatic evaluation metrics, which are vital to the development of MT systems. The Bilingual Internet Text Search (BITS) and Champollion sentence aligner enable the finding and processing of large quantities of parallel text. All specifications and tools used by LDC and described in the paper are or will be available to the general public.
We report on the success of a two-pass approach to annotating metadata, speech effects and syntactic structure in English conversational speech: separately annotating transcribed speech for structural metadata, or structural events, (fillers, speech repairs ( or edit dysfluencies) and SUs, or syntactic/semantic units) and for syntactic structure (treebanking constituent structure and shallow argument structure). The two annotations were then combined into a single representation. Certain alignment issues between the two types of annotation led to the discovery and correction of annotation errors in each, resulting in a more accurate and useful resource. The development of this corpus was motivated by the need to have both metadata and syntactic structure annotated in order to support synergistic work on speech parsing and structural event detection. Automatic detection of these speech phenomena would simultaneously improve parsing accuracy and provide a mechanism for cleaning up transcriptions for downstream text processing. Similarly, constraints imposed by text processing systems such as parsers can be used to help improve identification of disfluencies and sentence boundaries. This paper reports on our efforts to develop a linguistic resource providing both spoken metadata and syntactic structure information, and describes the resulting corpus of English conversational speech.
The paper presents a new language processing toolkit developed at Adam Mickiewicz University. Its functionality includes currently tokenization, sentence splitting, dictionary-based morphological analysis, heuristic morphological analysis of unknown words, spelling correction, pattern search, and generation of concordances. It is organized as a collection of command-line programs, each performing one operation. The components may be connected in various ways to provide various text processing services. Also new user-deoned components may be easily incorporated into the system. The toolkit is destined for processing raw (not annotated) text corpora. The system was originally intended for Polish, but its adaptation to other languages is possible.
Annotated parallel texts are an important resource for quantitative and qualitative linguistic research. Creating parallel corpora enables the generation of (bilingual) lexica, provides a basis for the extraction of data used for translation memories, makes is possible to describe the differences between text versions simply allows scientists to create texts in cooperation. We describe the design and implementation of an interactive editor allowing the user to annotate parallel texts: SAM, the Script Annotation Manager.
We will describe new cross-lingual strategies for the development multilingual information services on mobile devices. The novelty of our approach is the intelligent modeling of cross-lingual application domains and the combination of textual translation with speech generation. The final system helps users to speak foreign languages and communicate with the local people in relevant situations, such as restaurant, taxi and emergencies. The advantage of our information services is that they are robust enough for the use in real-world situations. They are developed for the Beijing Olympic Games 2008, where most foreigners will have to rely on translation assistance. Their deployment is foreseen as part of the planned ubiquitous mobile information system of the Olympic Games.
This paper describes the issues involved in extending a trans-lingual lexicon, the TextWise Conceptual Interlingua (CI), with Arabic terms. The Conceptual Interlingua is based on the Princeton English WordNet (Fellbaum, 1998). It is a central component in the cross-lingual information retrieval (CLIR) system CINDOR (Conceptual INterlingua for DOcument Retrieval). Arabic has a rich morphological system combining templatic and affixational paradigms for both inflectional and derivational morphology. This rich morphology poses a major challenge to the design and building of the Arabic CI and also its validation. This is because the available resources for Arabic, whether manually constructed bilingual lexicons or lexicons automatically derived from bilingual parallel corpora, exist at different levels of morphological representation. We describe here the issues and decisions made in the design and construction of the Arabic-English CI using different types of manual and automatic resources. We also present the results of an extensive validation of the Arabic CI and briefly discuss the evaluation of its use for CLIR on the TREC Arabic Benchmark collection.
In the paper we report realization of SyntLex project aiming at construction of a full lexicon grammar for Polish. The lexicon-grammar based paradigm in computer linguistics is derived from the predicate logic and attributes a central role to the predicative constructions. An important class of syntactic constructions in many languages (French, English, Polish and other Slavonic languages in particular) are those based on verbo-nominal collocations, with the verb playing a support role with respect to the noun considered as carrying the predicative information. In this paper we refer to the former research by one of the authors aiming at full description of verbo-nominal predicative constructions for Polish in the form of an electronic resource for LI applications. We describe procedures to complete and corpus-validate the resource obtained so far.
This paper describes the main features of the Italian Metaphor Database, buing built at the University of Perugia (Italy). The database is being developed as a resource to be used both as a knowledge base on conceptual metaphors in Italian and their lexical expressions, and to enrich general lexical resources. The reason to develop such a database is that most NLP systems have to deal with metaphorical expressions sooner or later but, as previous research has shown, existing lexical resources for Italian do not contain complete and consistent data on metaphors, empirically derived but theoretically motivated. Thus, by referring to the Cognitive Theory of metaphor, conceptual metaphors instantiated in Italian are being represented in the resource, together with data on the way they are expressed in the language (i.e., through lexical units or multiword expressions), examples of them found within a corpus, and data on metaphorical linguistic expressions encoded/missing within ItalWordNet.
In recent years there has been a growing interest in clarifying the process of Information Extraction (IE) from documents, particularly when coupled with Machine Learning. We believe that a fundamental step forward in clarifying the IE process would be to be able to perform comparative evaluations on the use of different representations. However, this is difficult because most of the time the way information is represented is too tightly coupled with the algorithm at an implementation level, making it impossible to vary representation while keeping the algorithm constant. A further motivation behind our work is to reduce the complexity of designing, developing and testing IE systems. The major contribution of this work is in defining a methodology and providing a software infrastructure for representing language resources independently of the algorithm, mainly for Information Extraction but with application in other fields - we are currently evaluating its use for ontology learning and document classification.
We describe the innovative use of describing an existing natural language pipeline using the Semantic Web, and focus on how the performance and results of the components may be described. Earlier work has shown how NLP Web Services can be automatically composed via Semantic Web Service composition, and once the results of NLP components can be stored directly, they can also be used to direct the composition, leading to advances in the sharing and evaluation of NLP resources.
This paper constitutes a preliminary report on the work carried out on semantic content annotation in the LIRICS project, in close collaboration with the activities of ISO TC 37/SC 4/TDG 31. This consists primarily of: (1) identifying commonalities in alternative approaches to the annotation and representation of various types of semantic information; and (2) developing methodological principles and concepts for identifying and characterising representational concepts for semantic content. The LIRICS project does not aim to develop a standard format for the annotation and representation of semantic content, but at providing well-defined descriptive concepts. In particular, the aim is to build an on-line registry of definitions of such concepts, called data categories, in accordance with ISO standard 12620. These semantic data categories are abstract concepts, whose use is not restricted to any particular format or representation language. We advocate the use of the metamodel as a tool to extract the most important of these abstract overarching concepts, with examples from dialogue act, temporal, reference and semantic role annotation.
In this paper, we present PYCOT, a pronoun resolution toolkit. This toolkit is written in the Python programming language and is intended to be an addition to the open-source NLTK collection of natural language processing tools. We discuss the design of the module as well as studies of its performance on pronoun resolution in English and in Korean.
This paper describes a dialogue data collection experiment and resulting corpus for dialogues between a senior mobile journalist and a junior cub reporter back at the office. The purpose of the dialogue is for the mobile journalist to collect background information in preparation for an interview or on-the-site coverage of a breaking story. The cub reporter has access to text archives that contain such background information. A unique aspect of these dialogues is that they capture information-seeking behavior for an open-ended task against a large unstructured data source. Initial analyses of the corpus show that the experimental design leads to real-time, mixedinitiative, highly interactive dialogues with many interesting properties.
This paper describes the utility of semantic resources such as the Web, WordNet and gazetteers in the answer selection process for a question-answering system. In contrast with previous work using individual semantic resources to support answer selection, our work combines multiple resources to boost the confidence scores assigned to correct answers and evaluates different combination strategies based on unweighted sums, weighted linear combinations, and logistic regression. We apply our approach to select answers from candidates produced by three different extraction techniques of varying quality, focusing on TREC questions whose answers represent locations or proper-names. Our experimental results demonstrate that the combination of semantic resources is more effective than individual resources for all three extraction techniques, improving answer selection accuracy by as much as 32.35{\%} for location questions and 72{\%} for proper-name questions. Of the combination strategies tested, logistic regression models produced the best results for both location and proper-name questions.
Speech technology applications, such as speech recognition, speech synthesis, and speech dialog systems, often require corpora based on highly customized specifications. Existing corpora available to the community, such as TIMIT and other corpora distributed by LDC and ELDA, do not always meet the requirements of such applications. In such cases, the developers need to create their own corpora. The creation of a highly customized speech corpus, however, could be a very expensive and time-consuming task, especially for small organizations. It requires multidisciplinary expertise in linguistics, management and engineering as it involves subtasks such as the corpus design, human subject recruitment, recording, quality assurance, and in some cases, segmentation, transcription and annotation. This paper describes LDC's recent involvement in the creation of a low-cost yet highly-customized speech corpus for a commercial organization under a novel data creation and licensing model, which benefits both the particular data requester and the general linguistic data user community.
We present NOMOS, an open-source software framework for annotation, processing, and analysis of multimodal corpora. NOMOS is designed for use by annotators, corpus developers, and corpus consumers, emphasizing configurability for a variety of specific annotation tasks. Its features include synchronized multi-channel audio and video playback, compatibility with several corpora, platform independence, and mixed display of capabilities and a well-defined method for layering datasets. Second, we describe how the system is used. For corpus development and annotation we present a typical use scenario involving the creation of a schema and specialization of the user interface. For processing and analysis we describe the GUI- and Java-based methods available, including a GUI for query construction and execution, and an automatically generated schema-conforming Java API for processing of annotations. Additionally, we present some specific annotation and research tasks for which NOMOS has been specialized and used, annotation and research tasks for which NOMOS has been specialized and used, including topic segmentation and decision-point annotation of meetings.
We present a new corpus of tutorial dialogs on mathematical theorem proving that was collected in a Wizard-of-Oz setup. Our study is a follow up on a previous experiment conducted in a similar simulated environment. A major difference between the current and the previous experimental setup was that in this study we varied the presentation of the study-material with which the subjects were provided. One sub-group of the subjects was presented with a highly formalized presentation consisting mainly of formulas, while the other with a presentation mainly in natural language. Our goal was to obtain more data on the kind of mixed-language that is characteristic of informal mathematical discourse. We hypothesized that the language style of the subjects' interaction with the simulated system will reflect the style of presentation of the study-material. In the paper we briefly present the experimental setup, the corpus, and a preliminary quantitative result of the corpus analysis.
Task-based machine translation (MT) evaluation asks, how well do people perform text-handling tasks given MT output? This method of evaluation yields an extrinsic assessment of an MT engine, in terms of users task performance on MT output. While this method is time-consuming, its key advantage is that MT users and stakeholders understand how to interpret the assessment results. Prior experiments showed that subjects can extract individual who-, when-, and where-type elements of information from MT output passages that were not especially fluent. This paper presents the results of a pilot study to assess a slightly more complex task: when given such wh-items already identified in an MT output passage, how well can subjects properly select from and place these items into wh-typed slots to complete a sentence-template about the passages event? The results of the pilot with nearly sixty subjects, while only preliminary, indicate that this task was extremely challenging: given six test templates to complete, half of the subjects had no completely correct templates and 42{\%} had exactly one completely correct template. The provisional interpretation of this pilot study is that event-based template completion defines a task ceiling, against which to evaluate future improvements on MT engines.
The Linguistic Data Consortium (LDC) has created various annotated linguistic data for a variety of common task evaluation programs and projects to create shared linguistic resources. The majority of these annotated linguistic data were created with highly customized annotation tools developed at LDC. The Annotation Graph Toolkit (AGTK) has been used as a primary infrastructure for annotation tool development at LDC in recent years. Thanks to the direct feedback from annotation task designers and annotators in-house, annotation tool development at LDC has entered a new, more mature and productive phase. This paper describes recent additions to LDC's annotation tools that are newly developed or significantly improved since our last report at the Fourth International Conference on Language Resource and Evaluation Conference in 2004. These tools are either directly based on AGTK or share a common philosophy with other AGTK tools.
Multilingual Question Answering systems are generally very complex, integrating several sub-modules to achieve their result. Global metrics (such as average precision and recall) are insufficient when evaluating the performance of individual sub-modules and their influence on each other. In this paper, we present a modular approach to error analysis and evaluation; we use manually-constructed, gold-standard input for each module to obtain an upper-bound for the (local) performance of that module. This approach enables us to identify existing problem areas quickly, and to target improvements accordingly.
This paper presents an evaluation of a spoken dialog system for automotive environments. Our overall goal was to measure the impact of user-system interaction on the users driving performance, and to determine whether adding context-awareness to the dialog system might reduce the degree of user distraction during driving. To address this issue, we incorporated context-awareness into a spoken dialog system, and implemented three system features using user context, network context and dialog context. A series of experiments were conducted under three different configurations: driving without a dialog system, driving while using a context-aware dialog system, and driving while using a context-unaware dialog system. We measured the differences between the three configurations by comparing the average car speed, the frequency of speed changes and the angle between the cars direction and the centerline on the road. These results indicate that context-awareness could reduce the degree of user distraction when using a dialog system during driving.
This paper describes a peer-to-peer architecture for representing and disseminating linguistic corpora, linguistic annotation, and resources such as lexical databases and gazetteers. The architecture is based upon a Universal Database technology in which all information is represented in globally identified, extensible bundles of attribute-value pairs. These objects are replicated at will between peers in the network, and the business rules that implement replication involve checking digital signatures and proper attribution of data, to avoid information being tampered with or abuse of copyright. Universal identifiers enable comprehensive standoff annotation and commentary. A carefully constructed publication mechanism is described that enables different users to subscribe to material provided by trusted publishers on recognized topics or themes. Access to content and related annotation is provided by distributed indexes, represented using the same underlying data objects as the rest of the database.
The paper presents a software system that embodies a lexico-syntactic approach to the task of Textual Entailment. Although the approach is based on a minimal set of resources it is highly confident. The architecture of the system is open and can be easily expanded with more and deeper processing modules. Results on a standard data set are presented.
A first step in answering complex questions, such as those in the Relationship' task of the Text REtrieval Conference's Question Answering track (TREC/QA), is finding passages likely to contain pieces of the answer---passage retrieval. We introduce semantic overlap scoring, a new passage retrieval algorithm that facilitates credit assignment for inexact matches between query and candidate answer. Our official submission ranked best among fully automatic systems, at 23{\%} F-measure, while the best system, with manual input, reached 28{\%}. We use our Nuggeteer tool to robustly evaluate each component of our Relationship system post hoc. Ablation studies show that semantic overlap scoring achieves significant performance improvements over a standard passage retrieval baseline.
Generating answers to complex questions in the form of multi-document summaries requires access to question decomposition methods. In this paper we present three methods for decomposing complex questions and we evaluate their impact on the responsiveness of the answers they enable.
Answering questions that ask about temporal information involves several forms of inference. In order to develop question answering capabilities that benefit from temporal inference, we believe that a large corpus of questions and answers that are discovered based on temporal information should be available. This paper describes our methodology for creating AnswerTime-Bank, a large corpus of questions and answers on which Question Answering systems can operate using complex temporal inference.
In this paper we present the first phase of the ongoing SpaceBank project that attempts to create a linguistic resource for annotating and reasoning with spatial information from text. SpaceBank is the spatial counterpart of TimeBank, an electronic resource for temporal semantics and reasoning. The paper focuses on building an ontology of lexicalized spatial concepts. The textual occurrences of the concepts in this ontology will be annotated using the SpaceML language, briefly described here. SpaceBank is designed to be integrated with TimeBank, for a spatio-temporal model of the textual information.
An ontology describes conceptual knowledge in a specific domain. A lexical base collects a repository of words and gives independent definition of concepts. In this paper, we propose to use FCA as a tool to help constructing an ontology through an existing lexical base. We mainly address two issues. The first issue is how to select attributes to visualize the relations between lexical terms. The second issue is how to revise lexical definitions through analysing the relations in the ontology. Thus the focus is on the effect of interaction between a lexical base and an ontology for the purpose of good ontology construction. Finally, experiments have been conducted to verify our ideas.
The goal of implementing a keyword extraction system is to increase as near as 100{\%} of precision and recall. These values are affected by the amount of extracted keywords. There are two groups of errors happened i.e. false-rejected and false-accepted keywords. To improve the performance of the system, false-rejected keywords should be recovered and the false-accepted keywords should be reduced. In this paper, we enhance the conventional keyword extraction systems by attaching the keyword recovery function. This function recovers the previously false-rejected keywords by comparing their semantic information with the contents of each relevant document. The function is automated in three processes i.e. Domain Identification, Knowledge Base Generation and Keyword Determination. Domain identification process identifies domain of interest by searching domains from domain knowledge base by using extracted keywords.The most general domains are selected and then used subsequently. To recover the false-rejected keywords, we match them with keywords in the identified domain within the domain knowledge base rely on their semantics by keyword determination process. To semantically recover keywords, definitions of false-reject keywords and domain knowledge base are previously represented in term of conceptual graph by knowledge base generator process. To evaluate the performance of the proposed function, EXTRACTOR, KEA and our keyword-database-mapping based keyword extractor are compared. The experiments were performed in two modes i.e. training and recovering. In training mode, we use four glossaries from the Internet and 60 articles from the summary sections of IEICE transaction. While in the recovering mode, 200 texts from three resources i.e. summary section of 15 chapters in a computer textbook and articles from IEICE and ACM transactions are used. The experimental results revealed that our proposed function improves the precision and recall rates of the conventional keyword extraction systems approximately 3-5{\%} of precision and 6-10{\%} of recall, respectively.
This paper presents an annotated Chinese collocation bank developed at the Hong Kong Polytechnic University. The definition of collocation with good linguistic consistency and good computational operability is first discussed and the properties of collocations are then presented. Secondly, based on the combination of different properties, collocations are classified into four types. Thirdly, the annotation guideline is presented. Fourthly, the implementation issues for collocation bank construction are addressed including the annotation with categorization, dependency and contextual information. Currently, the collocation bank is completed for 3,643 headwords in a 5-million-word corpus.
ItalWordNet (IWN) and PAROLE/SIMPLE/CLIPS (PSC), the two largest electronic, general-purpose lexical resources of Italian language present many compatible aspects although they are based on two different lexical models having their own underlying principles and peculiarities. Such compatibility prompted us to study the feasibility of semi-automatically linking and eventually merging the two lexicons. To this purpose, the mapping of the ontologies on which basis both lexicons are structured was performed and the sets of semantic relations enabling to relate lexical units were compared. An overview of this preliminary phase is provided in this paper. The linking methodology and related problematic issues are described. Beyond the advantage for the end user to dispose of a more exhaustive and in-depth lexical information combining the potentialities and most outstanding features offered by the two lexical models, resulting benefits and enhancements for the two resources are illustrated that definitely legitimize the soundness of this linking and merging initiative.
Most African countries follow an oral tradition system to transmit their cultural, scientific and historic heritage through generations. This ancestral knowledge accumulated during centuries is today threatened of disappearing. This paper presents the first steps in the building of an automatic speech to text transcription for African oral patrimony, particularly the Djibouti cultural heritage. This work is dedicated to process Somali language, which represents half of the targeted Djiboutian audio archives. The main problem is the lack of annotated audio and textual resources for this language. We describe the principal characteristics of audio (10 hours) and textual (3M words) training corpora collected. Using the large vocabulary speech recognizer engine, Speeral, developed at the Laboratoire Informatique dAvignon (LIA) (computer science laboratory of Avignon), we obtain about 20.9{\%} word error rate (WER). This is an encouraging result, considering the small size of our corpora. This first recognizer of Somali language will serve as a reference and will be used to transcribe some Djibouti cultural archives. We will also discuss future ways of research like sub-words indexing of audio archives, related to the specificities of the Somali language.
Recent improvements in speech recognition technology have resulted in products that can now demonstrate commercial value in a variety of applications. Many vendors are marketing products which combine ASR applications including continuous dictation, command-and-control interfaces, and transcription of recorded speech at an accuracy of 98{\%}. In this study, we measured the accuracy of certain commercially available desktop speech recognition engines in multiple languages. Using word error rate as a benchmark, this work compares recognition accuracy across eight languages and the products of three manufacturers. Results show that two systems performed almost the same while a third system recognized at lower accuracy, although none of the systems reached the claimed accuracy. Read speech was recognized better than spontaneous speech. The systems for US-English, Japanese and Spanish showed higher accuracy than the systems for UK-English, German, French and Chinese.
We present an annotation scheme for emotionally relevant behavior at the speaker contribution level in multiparty conversation. The scheme was applied to a large, publicly available meeting corpus by three annotators, and subsequently labeled with emotional valence. We report inter-labeler agreement statistics for the two schemes, and explore the correlation between speaker valence and behavior, as well as that between speaker valence and the previous speaker's behavior. Our analyses show that the co-occurrence of certain behaviors and valence classes significantly deviates from what is to be expected by chance; in isolated cases, behaviors are predictive of valence.
This paper introduces a recently initiated project that focuses on building a lexical resource for Modern Standard Arabic based on the widely used Princeton WordNet for English (Fellbaum, 1998). Our aim is to develop a linguistic resource with a deep formal semantic foundation in order to capture the richness of Arabic as described in Elkateb (2005). Arabic WordNet is being constructed following methods developed for EuroWordNet (Vossen, 1998). In addition to the standard wordnet representation of senses, word meanings are also being defined with a machine understandable semantics in first order logic. The basis for this semantics is the Suggested Upper Merged Ontology and its associated domain ontologies (Niles and Pease, 2001). We will greatly extend the ontology and its set of mappings to provide formal terms and definitions for each synset. Tools to be developed as part of this effort include a lexicographer's interface modeled on that used for EuroWordNet, with added facilities for Arabic script, following Black and Elkateb's earlier work (2004).
This paper reports a large-scale non-probabilistic parsing experiment with a deep LFG parser. We briefly introduce the parser we used, named SXLFG, and the resources that were used together with it. Then we report quantitative results about the parsing of a multi-million word journalistic corpus. We show that we can parse more than 6 million words in less than 12 hours, only 6.7{\%} of all sentences reaching the 1s timeout. This shows that deep large-coverage non-probabilistic parsers can be efficient enough to parse very large corpora in a reasonable amount of time.
KB-N is a web-accessible searchable Knowledge Bank comprising A) a parallel corpus of quality assured and calibrated English and Norwegian text drawn from economic-administrative knowledge domains, and B) a domain-focused database representing that knowledge universe in terms of defined concepts and their respective bilingual terminological entries. A central mechanism in connecting A and B is an algorithm for the automatic extraction of term candidates from aligned translation pairs on the basis of linguistic, lexical and statistical filtering (first ever for Norwegian). The system is designed and programmed by Paul Meurer at Aksis (UiB). An important pilot application of the term base is subdomain and collocations based word-sense disambiguation for LOGON, a system for Norwegian-to-English MT currently being developed.
It is an ongoing debate whether categorical systems created by some experts are an appropriate way to help users finding useful resources in the internet. However for the much more restricted domain of language documentation such a category system might still prove reasonable if not indispensable. This article gives an overview over the particular IMDI category set and presents a rough evaluation of its practical use at the Max-Planck-Institute Nijmegen.
Advances in location aware computing and the convergence of geographic and textual information systems will require a comprehensive, extensible, information rich framework called the Information Commons Gazetteer that can be freely disseminated to small devices in a modular fashion. This paper describes the infrastructure and datasets used to create such a resource. The Gazetteer makes use of MAYA Design's Universal Database Architecture; a peer-to-peer system based upon bundles of attribute-value pairs with universally unique identity, and sophisticated indexing and data fusion tools. The Gazetteer primarily constitutes publicly available geographic information from various agencies that is organized into a well-defined scalable hierarchy of worldwide administrative divisions and populated places. The data from various sources are imported into the commons incrementally and are fused with existing data in an iterative process allowing for rich information to evolve over time. Such a flexible and distributed public resource of the geographic places and place names allows for both researchers and practitioners to realize location aware computing in an efficient and useful way in the near future by eliminating redundant time consuming fusion of disparate sources.
In this paper, we introduce a new lexical resource for French which is freely available as the second version of the Lefff (Lexique des formes fl{\'e}chies du fran{\c{c}}ais - Lexicon of French inflected forms). It is a wide-coverage morphosyntactic and syntactic lexicon, whose architecture relies on properties inheritance, which makes it more compact and more easily maintainable and allows to describe lexical entries independantly from the formalisms it is used for. For these two reasons, we define it as a meta-lexicon. We describe its architecture, several automatic or semi-automatic approaches we use to acquire, correct and/or enrich such a lexicon, as well as the way it is used both with an LFG parser and with a TAG parser based on a meta-grammar, so as to build two large-coverage parsers for French. The web site of the Lefff is http://www.lefff.net/.
The study which is reported here aims at investigating the extent to which the conceptual and representational tools provided by a lexical model designed for the semantic representation of general language may suit the requirements of knowledge modelling in a domain-specific perspective. A general linguistic ontology and a set of semantic links, which allow classifying, describing and interconnecting word senses, play a central role in structuring and representing such knowledge. The health and medicine vocabulary has been taken as a case study for this investigation.
In this paper LexikoNet, a large lexical ontology of German nouns is presented. Unlike GermaNet and the Princeton WordNet, LexikoNet has distinguished type and role hypernyms right from the outset and organizes those lexemes in a parallel, independent hierarchy. In addition to roles and types, LexikoNet uses meronymic and holonymic relations as well as the instance relation. LexikoNet is based on a conceptual hierarchy of currently 1,470 classes to which approximately 90,000 word senses taken from a large German monolingual dictionary, the W{\textbackslash}``orterbuch der deutschen Gegenwartssprache (WDG), are attached. The conceptual classes provide a useful degree of abstraction for the lexicographic description of selectional restrictions, thus making LexikoNet a useful filtering tool for corpus based lexicographic analysis. LexikoNet is currently used in-house as a filter for lexicographic extraction tasks in the DWDS project. Furthermore, it is used as a classification tool of the words of the week provided for the newspaper Die ZEIT on www.zeit.de
This paper reports on the evaluation activities conducted in the first year of the TC-STAR project. The TC-STAR project, financed by the European Commission within the Sixth Framework Program, is envisaged as a long-term effort to advance research in the core technologies of Speech-to-Speech Translation (SST). SST technology is a combination of Automatic Speech Recognition (ASR), Spoken Language Translation (SLT) and Text To Speech (TTS).
This article describes the first CHIL evaluation campaign in which 12 technologies were evaluated. The major outcomes of the first evaluation campaign are the so-called Evaluation Packages. An evaluation package is the full documentation (definition and description of the evaluation methodologies, protocols and metrics) alongside the data sets and software scoring tools, which an organisation needs in order to perform the evaluation of one or more systems for a given technology. These evaluation packages will be made available to the community through ELDA General Catalogue.
The Cross-Language Evaluation Forum (CLEF) promotes research into the development of truly multilingual systems capable of retrieving relevant information from collections in many languages and in mixed media. The paper discusses some of the main results achieved in the first six years of activity.
This paper presents an overview of the Multilingual Question Answering evaluation campaigns which have been organized at CLEF (Cross Language Evaluation Forum) since 2003. Over the years, the competition has registered a steady increment in the number of participants and languages involved. In fact, from the original eight groups which participated in 2003 QA track, the number of competitors in 2005 rose to twenty-four. Also, the performances of the systems have steadily improved, and the average of the best performances in the 2005 saw an increase of 10{\%} with respect to the previous year.
This paper reports on prosodic evaluation in the framework of the EVALDA/EvaSy project for text-to-speech (TTS) evaluation for the French language. Prosody is evaluated using a prosodic transplantation paradigm. Intonation contours generated by the synthesis systems are transplanted on a common segmental content. Both diphone based synthesis and natural speech are used. Five TTS systems are tested along with natural voice. The test is a paired preference test (with 19 subjects), using 7 sentences. The results indicate that natural speech obtains consistently the first rank (with an average preference rate of 80{\%}), followed by a selection based system (72{\%}) and a diphone based system (58{\%}). However, rather large variations in judgements are observed among subjects and sentences, and in some cases synthetic speech is preferred to natural speech. These results show the remarkable improvement achieved by the best selection based synthesis systems in terms of prosody. In this way; a new paradigm for evaluation of the prosodic component of TTS systems has been successfully demonstrated.
The PLS is generally (and truly) characterized by two attributes: structural and functional. Let us ﬁrst turn to the structural point of view, namely, the School’s endeavor to view language as a system of systems rather than to study individual phenomena as ad hoc, non-systematic issues. The Circle shared de Saussure’s understanding of language as a system of (bilateral) signs, in which only oppositions rather than ﬁxed ∗ Logo on the Indiana University Linguistic Club tee-shirt, 1984. ∗∗ Matematicko-fyzika´ln´ı fakulta, Univerzita Karlova, Malostranske´ na´mest´ı 25, CZ-11800 Praha, Czech Republic. This paper is the text of the talk given on receipt of the ACL’s Lifetime Achievement Award in 2006. © 2006 Association for Computational Linguistics  Computational Linguistics  Volume 32, Number 4  entities play a role. As mentioned above, this attitude was most apparently reﬂected in the study of phonology as a system displaying distinctive features and employing the notion of binary oppositions. Jakobson (1929) presented the phonological repertory (both in synchrony and in diachrony) as a system of oppositions (mainly binary, privative), based on acoustic distinctive features and understood as the clue to the sound and meaning relationship. Along with phonemes and morphemes, the sentence was also recognized as one of the fundamental ﬁelds of systematic oppositions, that is, as an ingredient of la langue. Mathesius (1928, 1936) formulated a concept of functional syntax; a structural view of syntax, based on the dependency relation, was elaborated by Tesnie`re (1934), a French member of the Circle, who was a professor of the Ljubljana University; his monograph was published only posthumously (Tesnie`re 1959), but his papers were known in Prague, and his approach to syntax was applied to Czech by Sˇmilauer (1947), who combined dependency syntax with a constituent-based view of the relation between predicate and subject. Dependency-based approaches, which understand the verb as the center of the sentence structure and describe this structure on the basis of binary relations between heads and their modiﬁers, have been for a long time a matter of Continental syntactic theories rather than of the mainstream syntactic approaches on the other side of the Atlantic. However, the notion of head can be found also in Bloomﬁeld (1933) when referring to the names of the main constituents of the sentence, that is, NP (noun phrase, with N as its head) and VP (verb phrase, with V as its head). In the framework of the Chomskyan approach, originally based exclusively on the concept of immediate constituents, the notion of head becomes the basic notion of X-bar theory. Originally, four categories were singled out as possible heads of their respective maximal projections, namely, N, V, Adj, and P(rep); as remarked by James McCawley (personal communication, around 1990), such a theory may be interesting unless the speciﬁcation of the set of basic categories grows beyond some reasonable limit. McCawley’s critical remark reﬂected the gradual development of X-bar theory, which allowed practically any constituent (or, more generally speaking, any arbitrary symbol for a grammatical value) to act as the head, dependent on the needs of the analysis of this or that construction.1 The very name head-driven phrase structure grammar, an inﬂuential theory combining an immediate constituent approach with elements of a dependency-based approach, as proposed by Pollard and Sag (1994), explicitly points out that the theory takes account of the main element within a constituent. Although their approach is constituent based (working with a lexically based X-bar syntactic theory; [Pollard and Sag 1994, page 362]), the authors are aware that the notion of constituent structure is widespread but that it is not based on sufﬁciently convincing direct evidence. The authors refer to Hudson’s (1984) approach and claim that it belongs to exceptions that do not overestimate the constituent structuring of sentence elements. It is sometimes doubted if the direction of the dependency relation, namely, the determination of the governor and the dependent in each pair (syntagm) can be reliably stated. We believe that in the prototypical case, the main criterion for this distinction  
∗ School of Informatics, University of Edinburgh, 2 Buccleuch Place, Edinburgh EH8 9LW, UK. E-mail: mlap@inf.ed.ac.uk Submission received: 28 December 2005; accepted for publication: 6 May 2006. © 2006 Association for Computational Linguistics  Computational Linguistics  Volume 32, Number 4  surface generation correlate signiﬁcantly with human judgments on sentence quality and understandability. Given their simplicity, automatic evaluation methods cannot be considered as a direct replacement for human evaluations (see Callison-Burch, Osborne, and Koehn [2006] for discussion on some problematic aspects of BLEU). However, they can be usefully employed during system development, for example, for quickly assessing modeling ideas or for comparing across different system conﬁgurations (Papineni et al. 2002; Bangalore, Rambow, and Whittaker 2000). Automatic methods have concentrated on evaluation aspects concerning lexical choice (e.g., words or phrases shared between reference and system translations), content selection (e.g., document units shared between reference and system summaries), and grammaticality (e.g., how many insertions, substitutions, or deletions are required to transform a generated sentence to a reference string). Another promising, but, less studied, avenue for automatic evaluation is information ordering. The task concerns ﬁnding an acceptable ordering for a set of preselected information-bearing items (Lapata 2003; Barzilay and Lee 2004). It is an essential step in concept-to-text generation, multidocument summarization, and other text synthesis problems. Depending on the application and domain at hand, the items to be ordered may vary greatly from propositions (Karamanis 2003; Dimitromanolaki and Androutsopoulos 2003) to trees (Mellish et al. 1998) or sentences (Lapata 2003; Barzilay and Lee 2004). It is therefore not surprising that evaluation methods have concentrated primarily on the generated orders, thus abstracting away from the items themselves. More concretely, Lapata (2003) proposed the use of Kendall’s τ, a measure of rank correlation, as a means of estimating the distance between a system-generated and a human-generated gold-standard order. Rank correlation is an appealing way of evaluating information ordering: It is a well-understood and widely used measure of the strength of association between two variables; it is computed straightforwardly and can operate over distinct linguistic units (e.g., sentences, trees, or propositions). Indeed, several studies have adopted Kendall’s τ as a performance measure for evaluating the output of information-ordering components both in the context of concept-to-text generation (Karamanis and Mellish 2005; Karamanis 2003) and summarization (Lapata 2003; Barzilay and Lee 2004; Okazaki, Matsuo, and Ishizuka 2004). Despite its growing popularity, no study to date has investigated whether Kendall’s τ correlates with human judgments on the information-ordering task. This is in marked contrast with other automatic evaluation methods that have been shown to correlate with human assessments. In this article, we aim to rectify this and undertake two studies that examine whether there is indeed a relationship between τ and behavioral data. We ﬁrst brieﬂy introduce Kendall’s τ and explain how it can be employed for evaluating information ordering (Section 2). Next, we present a controlled experimental study that examines whether Kendall’s τ is correlated with human ratings (Section 3). A commonly raised criticism of the judgment elicitation methodology is that it is not ﬁne-grained enough to rule out possible confounds. In the information-ordering task, for example, we cannot be certain that subjects rate a document low because it is genuinely badly organized and, therefore, difﬁcult to comprehend or because they are unfamiliar with its content or simply disinterested or distracted. Similar confounds also arise in the evaluation of the output of MT systems, where it may be difﬁcult to tease apart whether subjects’ ratings reﬂect their assessment of the quality of the translated text or its subject matter and structure. To eliminate such confounds, we follow our judgment elicitation study with an on-line reading experiment and demonstrate that τ is also correlated with processing time (Section 4). Our second experiment provides 472  Lapata  Automatic Evaluation of Information Ordering  additional evidence for the validity of τ as a measure of text well-formedness. Discussion of our results concludes the article. 2. Kendall’s Measure In common with other automatic evaluation methods, we assume that we have access to a reference output that in most cases will be created by one or several humans. Our task is to compare a system-produced ordering of items against a reference order. For ease of exposition, let us assume that our information-ordering component is part of a generation application whose ultimate goal is to generate coherent and understandable text. It is not crucially important how the items to be ordered are represented. They can be facts in a database (Duboue and McKeown 2001), propositions (Karamanis 2003), discourse trees (Mellish et al. 1998), or sentences (Lapata 2003; Barzilay and Lee 2004). Now, we can think of the items as objects for which a ranking must be produced. Table 1 gives an example of a reference text containing 10 items (A–J) and the orders (i.e., rankings) produced by two hypothetical systems. We can then calculate how much the system orders differ from the reference order, the underlying assumption being that acceptable orders should be fairly similar to the reference. A number of metrics can be employed for this purpose, such as Spearman’s correlation coefﬁcient (rs) for ranked data, Cayley distance, or Kendall’s τ (see Lebanon and Lafferty [2002] for an overview). Here we describe Kendall’s τ (Kendall 1938) and explain why it is an appropriate choice for information-ordering tasks. Let Y = y1 . . . yn be a set of items to be ranked. Let π and σ denote two distinct orderings of Y, and S(π, σ) the minimum number of adjacent transpositions needed to bring π to σ. Kendall’s τ is deﬁned as:  τ  =  
In this article, we present a language-independent, unsupervised approach to sentence boundary detection. It is based on the assumption that a large number of ambiguities in the determination of sentence boundaries can be eliminated once abbreviations have been identiﬁed. Instead of relying on orthographic clues, the proposed system is able to detect abbreviations with high accuracy using three criteria that only require information about the candidate type itself and are independent of context: Abbreviations can be deﬁned as a very tight collocation consisting of a truncated word and a ﬁnal period, abbreviations are usually short, and abbreviations sometimes contain internal periods. We also show the potential of collocational evidence for two other important subtasks of sentence boundary disambiguation, namely, the detection of initials and ordinal numbers. The proposed system has been tested extensively on eleven different languages and on different text genres. It achieves good results without any further amendments or language-speciﬁc resources. We evaluate its performance against three different baselines and compare it to other systems for sentence boundary detection proposed in the literature. 1. Introduction The sentence is a fundamental and relatively well understood unit in theoretical and computational linguistics. Many linguistic phenomena—such as collocations, idioms, and variable binding, to name a few—are constrained by the abstract concept ‘sentence’ in that they are conﬁned by sentence boundaries. The successful determination of these boundaries is thus a prerequisite for proper sentence processing. Sentence boundary detection is not a trivial task, though. Graphemes often serve more than one purpose in writing systems. The period, which is employed as sentence boundary marker, is no exception. It is also used to mark abbreviations, initials, ordinal numbers, and ellipses. Moreover, a period can be used to mark an abbreviation and a sentence boundary at the same time. In such cases, the second period is haplologically omitted and only one period is used as end-of-sentence and abbreviation marker.1 Sentence boundary detection  ∗ Sprachwissenschaftliches Institut, Ruhr-Universita¨t Bochum, 44780 Bochum, Germany. E-mail: tibor@linguistics.rub.de. ∗∗ Sprachwissenschaftliches Institut, Ruhr-Universita¨t Bochum, 44780 Bochum, Germany. E-mail: strunk@linguistics.rub.de. 
∗ Department of Signal Theory and Communications, Campus Nord, Barcelona 08034, Spain. Submission received: 9 August 2005; revised submission received: 26 April 2006; accepted for publication: 5 July 2006 © 2006 Association for Computational Linguistics  Computational Linguistics  Volume 32, Number 4  replaced by phrase-based translation models (Zens, Och, and Ney 2002; Koehn, Och, and Marcu 2003) which are directly estimated from aligned bilingual corpora by considering relative frequencies, and second, the noisy channel approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney 2002). As an extension of the machine translation problem, technological advances in the ﬁelds of automatic speech recognition (ASR) and text to speech synthesis (TTS) made it possible to envision the challenge of spoken language translation (SLT) (Kay, Gawron, and Norvig 1992). According to this, SMT has also been approached from a ﬁnite-state point of view as the most natural way of integrating ASR and SMT (Riccardi, Pieraccini, and Bocchieri 1996; Vidal 1997; Knight and Al-Onaizan 1998; Bangalore and Riccardi 2000). In this SMT approach, translation models are implemented by means of ﬁnitestate transducers for which transition probabilities are learned from bilingual data. As opposed to phrase-based translation models, which consider probabilities between target and source units referred to as phrases, ﬁnite-state translation models rely on probabilities among sequences of bilingual units, which are deﬁned by the transitions of the transducer. The translation system described in this article implements a translation model that has been derived from the ﬁnite-state perspective—more speciﬁcally, from the work of Casacuberta (2001) and Casacuberta and Vidal (2004). However, whereas in this earlier work the translation model is implemented by using a ﬁnite-state transducer, in the system presented here the translation model is implemented by using n-grams. In this way, the proposed translation system can take full advantage of the smoothing and consistency provided by standard back-off n-gram models. The translation model presented here actually constitutes a language model of a sort of “bilanguage” composed of bilingual units, which will be referred to as tuples (de Gispert and Marin˜ o 2002). An alternative approach, which relies on bilingual-unit unigram probabilities, was developed by Tillmann and Xia (2003); in contrast, the approach presented here considers bilingualunit n-gram probabilities. In addition to the tuple n-gram translation model, the translation system presented here implements four speciﬁc feature functions that are log-linearly combined along with the translation model for performing the decoding (Marin˜ o et al. 2005). This article is intended to provide a detailed description of the n-gram-based translation system, as well as to demonstrate the system performance in a widedomain, large-vocabulary translation task. The article is structured as follows. First, Section 2 presents a complete description of the n-gram-based translation model. Then, Section 3 describes in detail the additional feature functions that, along with the translation model, compose the n-gram-based SMT system implemented. Section 4 describes the European Parliament Plenary Session (EPPS) data, as well as the most relevant details about the translation tasks considered. Section 5 presents and discusses the translation experiments and their results. Finally, Section 6 presents some conclusions and intended further work. 2. The Tuple N-gram Model This section describes in detail the tuple n-gram translation model, which constitutes the core model implemented by the n-gram-based SMT system. First, the bilingual unit deﬁnition and model computation are presented in Section 2.1. Then, some important reﬁnements to the basic translation model are provided and discussed in Section 2.2. Finally, Section 2.3 discusses issues related to n-gram-based decoding. 528  Marin˜ o et al.  N-gram-based Machine Translation  2.1 Tuple Extraction and Model Computation  As already mentioned, the translation model implemented by the described SMT system is based on bilingual n-grams. This model actually constitutes a language model of a particular bilanguage composed of bilingual units that are referred to as tuples. In this way, the translation model probabilities at the sentence level are approximated by using n-grams of tuples, such as described by the following equation:  K  p(T, S) ≈ p((t, s)k|(t, s)k−1, (t, s)k−2, . . . , (t, s)k−n+1)  (1)  k=1  where t refers to target, s to source, and (t, s)k to the kth tuple of a given bilingual sentence pair. It is important to note that since both languages are linked up in tuples, the context information provided by this translation model is bilingual. Tuples are extracted from a word-to-word aligned corpus in such a way that a unique segmentation of the bilingual corpus is achieved. Although in principle any Viterbi alignment should allow for tuple extraction, the resulting tuple vocabulary depends highly on the particular alignment set considered, and this impacts the translation results. According to our experience, the best performance is achieved when the union of the source-to-target and target-to-source alignment sets (IBM models; Brown et al. [1993]) is used for tuple extraction (some experimental results regarding this issue are presented in Section 4.2.2). Additionally, the use of the union can also be justiﬁed from a theoretical point of view by considering that the union set typically exhibits higher recall values than do other alignment sets such as the intersection and source-to-target. In this way, as opposed to other implementations, where one-to-one (Bangalore and Riccardi 2000) or one-to-many (Casacuberta and Vidal 2004) alignments are used, tuples are extracted from many-to-many alignments. This implementation produces a monotonic segmentation of bilingual sentence pairs, which allows for simultaneously capturing contextual and reordering information into the bilingual translation unit structures. This segmentation also allows for estimating the n-gram probabilities appearing in (1). In order to guarantee a unique segmentation of the corpus, tuple extraction is performed according to the following constraints (Crego, Marin˜ o, and de Gispert 2004): r a monotonic segmentation of each bilingual sentence pair is produced, r no word inside the tuple is aligned to words outside the tuple, and r no smaller tuples can be extracted without violating the previous constraints.  Notice that, according to this, tuples can be formally deﬁned as the set of shortest phrases that provides a monotonic segmentation of the bilingual corpus. Figure 1 presents a simple example illustrating the unique tuple segmentation for a given pair of sentences, as well as the complete phrase set. The ﬁrst important observation from Figure 1 is related to the possible occurrence of tuples containing unaligned elements on the target side. This is the case for tuple 1. Tuples of this kind should be handled in an alternative way for the system to be able to provide appropriate translations for such unaligned elements. The problem of how  529  Computational Linguistics  Volume 32, Number 4  Figure 1 Example of tuple extraction. Tuples are extracted from Viterbi alignments in such a way that the set of shortest bilingual units that provide a monotonous segmentation of the bilingual sentence pair is achieved. to handle this kind of situation, which we refer to as involving source-nulled tuples, is discussed in detail in Section 2.2.2. Also, as observed from Figure 1, the total number of tuples is signiﬁcantly lower than the total number of phrases, and, in most of the cases, longer phrases can be constructed by considering tuple n-grams, which is the case for phrases 2, 6, 7, 9, 10, and 11. However, phrases 4 and 5 cannot be generated from tuples. In general, the tuple representation is not able to provide translations for individual words that appear tied to other words unless they occur alone in some other tuple. This problem, which we refer to as embedded words, is discussed in detail in Section 2.2.1. Another important observation from Figure 1 is that each tuple length is implicitly deﬁned by the word links in the alignment. As opposed to phrase-extraction procedures, for which a maximum phrase length should be deﬁned to avoid a vocabulary explosion, tuple extraction procedures do not have any control over tuple lengths. According to this, the tuple approach will strongly beneﬁt from the structural similarity between the languages under consideration. Then, for close language pairs, tuples are expected to successfully handle those short reordering patterns that are included in the tuple structure, as in the case of “traducciones perfectas : perfect translations” presented in Figure 1. On the other hand, in the case of distant pairs of languages, for which a large number of long tuples are expected to occur, the approach will more easily fail to provide a good translation model due to tuple sparseness. 2.2 Translation Model Reﬁnements The basic n-gram translation model, as deﬁned in the previous section, exhibits some important limitations that can be easily overcome by incorporating speciﬁc changes in 530  Marin˜ o et al.  N-gram-based Machine Translation  either the tuple vocabulary or the n-gram model. This section describes such limitations and provides a detailed description of the implemented reﬁnements. 2.2.1 Embedded Words. The ﬁrst issue regarding the n-gram translation model is related to the already mentioned problem of embedded words, which refers to the fact that the tuple representation is not able to provide translations for individual words all the time. Embedded words can become a serious drawback when they occur in relatively signiﬁcant numbers in the tuple vocabulary. Consider for example the word translations in Figure 1. As seen from the ﬁgure, this word appears embedded into tuple “traducciones perfectas : perfect translations.” If a similar situation is encountered for all other occurrences of that word in the training corpus, then no translation probability for an independent occurrence of that word will exist. A more relevant example would be the case of the embedded word perfect since this adjective always moves relative to the noun it is modifying. In this case, providing the translation system with a word-to-word translation probability for “perfectas : perfect” only guarantees that the decoder will have a translation option for an isolated occurrence of such words but does not guarantee anything about word order. So, certainly, any adjective–noun combination including the word perfect, which has not been seen during the training stage, will be translated in the wrong order. Accordingly, the problem resulting from embedded words can be partially solved by incorporating a bilingual dictionary able to provide word-to-word translation when required by the translation system. A more complete treatment for this problem must consider the implementation of a word-reordering strategy for the proposed SMT approach (as will be discussed in Section 6, this constitutes one of the main concerns for our further research). In our n-gram-based SMT implementation, the following strategy for handling embedded words is considered. First, one-word tuples for each detected embedded word are extracted from the training data and their corresponding word-to-word translation probabilities are computed by using relative frequencies. Then, the tuple n-gram model is enhanced by including all embedded-word tuples as unigrams into the model. Since a high-precision alignment set is desirable for extracting such one-word tuples and estimating their probabilities, the intersection of both alignments, source to target and target-to-source, is used instead of the union. In the particular case of the EPPS tasks considered in this work, embedded words do not constitute a real problem because of the great amount of training material and the reduced size of the test data set (see Section 4.1 for a detailed description of the EPPS data set). On the contrary, in other translation tasks with less available training material, the embedded-word handling strategy described above has been very useful (de Gispert, Marin˜ o, and Crego 2004). 2.2.2 Tuples with Empty Source Sides. The second important issue regarding the n-gram translation model is related to tuples with empty source sides, hereinafter referred to as source-nulled tuples. In the tuple n-gram model implementation, it frequently happens that some target words linked to NULL end up producing tuples with NULL source sides. Consider, for example, the ﬁrst tuple of the example presented in Figure 1. In this example, “NULL : we” is a source-nulled tuple if Spanish is considered to be the source language. Notice that tuples of this kind cannot be allowed since no NULL is expected to occur in a translation input. The classical solution to this problem in the ﬁnite-state transducer framework is the inclusion of epsilon arcs (Knight and Al-Onaizan 1998; Bangalore and Riccardi 531  Computational Linguistics  Volume 32, Number 4  2000). However, epsilon arcs signiﬁcantly increase decoding complexity. In our n-gram system implementation, this problem is easily solved by preprocessing the union set of alignments before extracting tuples, in such a way that any target word that is linked to NULL is attached to either its preceding word or its following word. In this way, no target word remains linked to NULL, and source-nulled tuples will not occur during tuple extraction. Some different strategies for handling target words aligned to NULL have been considered. In the simplest strategy, which will be referred to as the attach-to-right strategy, target words aligned to NULL are always attached to their following word. This simple strategy happens to provide better results, for English-to-Spanish and Spanishto-English translations, than the opposite one (attachment to the previous word), and also better than a more sophisticated strategy that considers bigram probabilities for deciding whether a given word should be attached to the following or to the previous one. Notice that in the particular cases of Spanish and English, the attach-to-right strategy can be justiﬁed heuristically. Indeed, when translating from Spanish to English, most of the source-nulled tuples result from omitted verbal subjects, which is a very common situation in Spanish. This is the case for the ﬁrst tuple in Figure 1. Suppose, for instance, that the attach-to-right strategy is used in Figure 1; in such a case, the tuple “quisie´ramos : would like” will be replaced by the new tuple “quisie´ramos : we would like,” which actually makes a better translation unit, at least from a grammatical point of view. Similarly, some common situations can be identiﬁed for translations in the English-to-Spanish direction, such as omitted determiners (e.g., “I want information about European countries : quiero informacio´ n sobre los pa´ıses Europeos”). Again, the attach-to-right strategy for the unaligned Spanish determiner los seems to be the best one. Experimental results comparing the attach-to-right strategy to an additional strategy based on a statistical translation lexicon are provided in Section 5.1.3. 2.2.3 Tuple Vocabulary Pruning. The third and last issue regarding the n-gram translation model is related to the computational costs resulting from the tuple vocabulary size during decoding. The idea behind this reﬁnement is to reduce both computation time and storage requirements without degrading translation performance. In our n-grambased SMT system implementation, the tuple vocabulary is pruned by using histogram counts. This pruning is performed by keeping the N most frequent tuples with common source sides. Notice that such a pruning, because it is performed before computing tuple n-gram probabilities, has a direct impact on the translation model probabilities and then on the overall system performance. For this reason, the pruning parameter N is critical for efﬁcient usage of the translation system. While a low value of N will signiﬁcantly decrease translation quality, on the other hand, a large value of N will provide the same translation quality than a more adequate N, but with a signiﬁcant increment in computational costs. The optimal value for this parameter depends on data and should be adjusted empirically for each considered translation task. 2.3 N-gram-based Decoding Decoding for the n-gram-based translation model is slightly different from phrasebased decoding. For this reason, a speciﬁc decoding tool had to be implemented. This 532  Marin˜ o et al.  N-gram-based Machine Translation  section brieﬂy describes MARIE, the n-gram based search engine developed for our SMT system (Crego, Marin˜ o, and de Gispert 2005a). MARIE implements a beam-search strategy based on dynamic programming. The decoding is performed monotonically and is guided by the source. During decoding, partial-translation hypotheses are arranged into different stacks according to the total number of source words they cover. In this way, a given hypothesis only competes with those hypotheses that provide the same source-word coverage. At every translation step, stacks are pruned to keep decoding tractable. MARIE allows for two different pruning methods: r Threshold pruning: for which all partial-translation hypotheses scoring below a predetermined threshold value are eliminated. r Histogram pruning: for which the maximum number of partial-translation hypotheses to be considered is limited to the K-best ranked ones. Additionally, MARIE allows for hypothesis recombination, which provides a more efﬁcient search. In the implemented algorithm, partial-translation hypotheses are recombined if they coincide exactly in both the present tuple and the tuple trigram history. MARIE also allows for considering additional feature functions during decoding. All these models are taken into account simultaneously, along with the n-gram translation model. In our SMT system implementation, four additional feature functions are considered. These functions are described in detail in Section 3.2.  3. Feature Functions for the N-gram-based SMT System This section describes in detail some feature functions that are implemented along with the n-gram translation model for the complete translation system. First, in subsection 3.1, the log-linear combination framework and the implemented optimization procedure are discussed. Then, four speciﬁc feature functions that constitute our SMT system are detailed in Section 3.2.  3.1 Log-linear Combination Framework As mentioned in the Introduction, in recent translation systems the noisy channel approach has been replaced by a more general approach, which is founded on the principles of maximum entropy (Berger, Della Pietra, and Della Pietra 1996). In this approach, the corresponding translation for a given source language sentence S is deﬁned by the target language sentence that maximizes a log-linear combination of multiple feature functions hi(S, T) (Och and Ney 2002), such as described by the following equation:  argmax λmhm(S, T)  (2)  T  m  where λm represents the coefﬁcient of the mth feature function hm(S, T), which actually corresponds to a log-scaled version of the mth-model probabilities. Optimal values for the λm coefﬁcients are estimated via an optimization procedure by using a development data set.  533  Computational Linguistics  Volume 32, Number 4  3.2 Translation System Features In addition to the tuple n-gram translation model, our n-gram-based SMT system implements four feature functions: a target-language model, a word-bonus model, and two lexicon models. These system features are described next.  3.2.1 Target-language Model. This feature provides information about the target language structure and ﬂuency. It favors those partial-translation hypotheses that are more likely to constitute correctly structured target sentences over those that are not. The model is implemented by using a word n-gram model of the target language, which is computed according to the following expression:  K  hTL(T, S) = hTL(T) = log p(wk|wk−1, wk−2, . . . , wk−n+1)  (3)  k=1  where wk refers to the kth word in the considered partial-translation hypothesis. Notice that this model only depends on the target side of the data, and can in fact be trained by including additional information from other available monolingual corpora.  3.2.2 Word-bonus Model. This feature introduces a bonus that depends on the partialtranslation hypothesis length. This is done to compensate for the system preference for short translations over large ones. The model is implemented through a bonus factor that directly depends on the total number of words contained in the partial-translation hypothesis, and it is computed as follows:  hWP(T, S) = hWP(T) = M  (4)  where M is the number of words contained in the partial-translation hypothesis.  3.2.3 Source-to-Target Lexicon Model. This feature actually constitutes a complementary translation model. This model provides, for a given tuple, a translation probability estimate between its source and target sides. This feature is implemented by using the IBM-1 lexical parameters (Brown et al. 1993; Och et al. 2004). Accordingly, the sourceto-target lexicon probability is computed for each tuple according to the following equation:  JI  hLF (T,  S)  =  log  (I  
Since the Web by far represents the largest public repository of natural language texts, recent experiments, methods, and tools in the area of corpus linguistics often use the Web as a corpus. For applications where high accuracy is crucial, the problem has to be faced that a non-negligible number of orthographic and grammatical errors occur in Web documents. In this article we investigate the distribution of orthographic errors of various types in Web pages. As a by-product, methods are developed for efﬁciently detecting erroneous pages and for marking orthographic errors in acceptable Web documents, reducing thus the number of errors in corpora and linguistic knowledge bases automatically retrieved from the Web. 1. Introduction The automated analysis of large corpora has many useful applications (Church and Mercer 1993). Suitable language repositories can be used for deriving models of a given natural language, as needed for speech recognition (Ostendorf, Digalakis, and Kimball 1996; Jelinek 1997; Chelba and Jelinek 2002), language generation (Oh and Rudickny 2000), and text correction (Kukich 1992; Amengual and Vidal 1998; Strohmaier et al. 2003b). Other corpus-based methods determine associations between words (Grefenstette 1992; Dunning 1993; Lin et al. 1998), which yields a basis for computing thesauri, or dictionaries of terminological expressions and multiword lexemes (Gaizauskas, Demetriou, and Humphreys 2000; Grefenstette 2001). From multilingual texts, translation lexica can be generated (Gale and Church 1991; Kupiec 1993; Kumano and Hirakawa 1994; Boutsis, Piperidis, and Demiros 1999; Grefenstette 1999). The analysis of technical texts is used to automatically build dictionaries of acronyms for a given ﬁeld (Taghva and Gilbreth 1999; Yeates, Bainbridge, and Witten 2000), and related methods help to compute dictionaries that cover the special vocabulary of a given thematic area (Strohmaier et al. 2003a). In computer-assisted language learning (CALL), mining techniques for corpora are used to create individualized and user-centric exercises for grammar and text understanding (Schwartz, Aikawa, and Pahud 2004; Brown and Eskenazi 2004; Fletcher 2004a). By Zipf’s law, most words, phrases, and speciﬁc grammatical constructions have a very low frequency. Furthermore, the number of text genres and special thematic ∗ Funded by German Research Foundation (DFG) † Funded by VolkswagenStiftung Submission received: 21 January 2005; revised submission received: 3 August 2005; accepted for publication: 10 December 2005. © 2006 Association for Computational Linguistics  Computational Linguistics  Volume 32, Number 3  areas that come with their own picture of language is large. This explains that most of the aforementioned applications can only work when built on top of huge heterogeneous corpora. Since the Web represents by far the largest public repository for natural language texts, and since Web search engines such as Google offer simple access to pages where language material of a given orthographic, grammatical, or thematic kind is found, many recent experiments and technologies use the Web as a corpus (Kehoe and Renouf 2002; Morley, Renouf, and Kehoe 2003; Kilgarriff and Grefenstette 2003; Resnik and Smith 2003; Way and Gough 2003; Fletcher 2004b). One potential problem for Web-based corpus linguistics is caused by the fact that words and phrases occurring in Web pages are sometimes erroneous. Typing errors represent one widespread phenomenon. Many Web pages, say, in English, are written by non-native speakers, or by persons with very modest language competence. As a consequence, spelling errors and grammatical bugs result. The character sets that are used for writing Web pages are often not fully adequate for the alphabet of a given language, which represents another systematic source for inaccuracies. Furthermore, a small number of texts found in the Web is obtained via optical character recognition (OCR), which may again lead to garbled words. As a consequence of these and other error sources, the Web contains a considerable number of “bad” pages with language material that is inappropriate for corpus construction. In one way or the other, all the aforementioned applications are affected by these inadequacies. While the problem is probably not too serious for approaches that merely collect statistical information about given language items, the construction of dictionaries and related linguistic knowledge bases—which are, after all, meant to be used in different scenarios of automated language processing—becomes problematic if too many erroneous entries are retrieved from Web pages. Obviously, in computer-assisted language learning it is a principal concern that words and phrases from the Web that are presented to the user are error free. In discussions we found that problems resulting from erroneous language material in Web pages for distinct applications are broadly acknowledged (see also Section 4.4 of Kilgarriff and Grefenstette [2003]). Still, to the best of our knowledge, a serious analysis of the frequency and distribution of orthographic errors in the Web is missing, and no general methods have been developed that help to detect and exclude pages with too many erroneous words. In this article we ﬁrst report on a series of experiments that try to answer the following questions: 1. What are important types of orthographic errors found in Web pages? 2. How frequent are errors of a given kind? For a given error level (percentage of erroneous tokens) τ, which percentage of Web pages exceeds error level τ? 3. How do these ﬁgures depend on the language, on the thematic area, and on the genre of the Web pages that are considered? How do these ﬁgures depend on the document format of the Web pages that are considered? We then look at the problem indicated above. 4. Which methods help to automatically detect Web pages with many orthographic errors? Which methods help to mark orthographic errors found in Web pages? 296  Ringlstetter, Schulz, and Mihov  Orthographic Errors in Web Pages  To answer questions 1–3, we retrieved and analyzed a collection of large English and German corpora from the Web, using suitable queries to Web search engines. In our error statistics we wanted to distinguish between (1) “general” Web pages collected without any speciﬁc thematic focus on the one hand and Web pages from speciﬁc thematic areas on the other hand, and (2) between Web pages written in HTML and Web documents written in PDF. To cover the ﬁrst difference, for both languages we retrieved two general corpora as well as a number of corpora for speciﬁc thematic areas. All these corpora only contain HTML pages. A parallel series of general corpora was collected that are composed of PDF documents. Details are provided in Section 2. Special Vocabulary. Web pages often contain tokens that do not belong to the standard vocabulary of the respective language. Typical categories are, for example, special names, slang, archaic language, expressions from foreign languages, and special expressions from computer science/programming. Classiﬁcation and detection of special vocabulary is outside the scope of the present article. Since sometimes a clear separation between special vocabulary and errors is difﬁcult, we brieﬂy come back to this problem in Section 5.4. Proper Errors. Focusing on garbled standard vocabulary, tokens may be seriously damaged in an “unexplainable” way. Most of the remaining errors can be assigned to one of the four classes mentioned above: r typing errors (i.e., errors caused by a confusion of keys when typing a document), r spelling errors (“cognitive” errors resulting from insufﬁcient language competence), r errors resulting from inadequate character encoding, and r OCR errors. In order to estimate the number of errors of a given kind in the corpora, special error dictionaries were built. These dictionaries, which only list garbled words of a given language that do not accidentally represent correct words, try to cover a high number of the conventional errors of each type that are typically found in Web pages and other documents. Section 3 motivates the use of error dictionaries for error detection. Details of the construction of the error dictionaries are discussed in Section 4. In Section 5, we estimate the number of orthographic errors in the corpora that remain undetected because they do not occur in the error dictionaries. We also estimate the percentage of correct tokens of the corpora that are erroneously treated as errors since they appear in the error dictionaries. Our results show that the number of tokens of a text that appear in the error dictionaries can be considered as a lower approximation of the number of real orthographic errors. In Section 6, we describe the distribution of orthographic errors of the types distinguished above in the general test corpora, counting occurrences of entries of the error dictionaries. Section 7 summarizes the most important differences that arise when using PDF corpora, or corpora for special thematic areas. Section 8 presents various 297  Computational Linguistics  Volume 32, Number 3  results that illuminate the relationship between the error rate of a document and its genre. In our experiments we observed in all corpora a rich spectrum of error rates, ranging from perfect documents to a small number of clearly unacceptable pages. This motivates the design of ﬁlters that efﬁciently recognize and reject pages with an error rate beyond a user-speciﬁed threshold. The construction of appropriate ﬁlters is described in Section 9, where we also demonstrate the effect of using these ﬁlters, comparing the ﬁgures obtained in Section 6 with the corresponding ﬁgures for ﬁltered corpora. Filters work surprisingly well due to a Zipf-like distribution of error frequencies in Web pages. In Section 10, we present two experiments that exemplify how the methods developed in the article may in fact help to improve corpus-based methods. The general question of how deeply distinct methods from computational linguistics based on Web corpora are affected by orthographic errors in Web pages and to what extent the methods developed in the article help to remedy these deﬁciencies are too complex to be discussed here. The main insights and contributions are summarized in the Conclusion (Section 11) where we also comment on future work and on some practical difﬁculties one has to face when collecting and analyzing large corpora from the Web.  2. Corpora The basis for the evaluations described below is a collection of corpora, each composed of Web pages retrieved with Web search engines (Google/AllTheWeb). In order to study how speciﬁc features of a language might inﬂuence the distribution of orthographic errors, all corpora were built up in two variants. The English and German variant, respectively, contain Web pages that were classiﬁed as English and German Web pages by the search engine. As described above, for both languages we collected general corpora with Web pages without any thematic focus and, in addition, corpora that cover ﬁve speciﬁc thematic areas to be described below. Statements on the “representativeness” of corpora derived from the Web are notoriously difﬁcult. The composition of corpora retrieved with Web search engines depends on the kind of queries that are used, on the ranking mechanisms of the engine, and on the details of the collection strategy. We mainly concentrated on simple queries and straightforward collection strategies. Still, the large number of subcorpora and pages that were evaluated should guarantee that accidental results are avoided. 2.1 General Web Corpora In a ﬁrst attempt, we tried to obtain a general German HTML corpus using the meaningless query der die das, i.e., the three German deﬁnite articles. However, queries of this and a similar form did not lead to satisfactory results: As a consequence of Google’s ranking mechanism, which prefers “authorities” (Brin and Page 1998), mainly portals of big organizations, companies, and others were retrieved. These pages are often dominated by graphical elements. Portions of text are usually small and carefully edited, which means that orthographic errors are less frequent than in other “less ofﬁcial” pages. To achieve a more realistic scenario we randomly generated quintuples, each collecting ﬁve terms of the 10,000 top frequent German words. We used Google to retrieve 298  Ringlstetter, Schulz, and Mihov  Orthographic Errors in Web Pages  10 pages per query (quintuple) until we obtained 1,000 pages. A considerable number of the URLs were found to be inactive. After conversion to ASCII and a preliminary analysis of error rates with methods described below, some of the remaining pages were found to contain very large lists of general keywords, including many orthographic errors. Apparently these lists and errors were only added to improve the ranking of the page in search engines, even for ill-formed queries. We excluded these pages. The remaining documents represent the “primary” general German HTML corpus. Since we wanted to know how results depend on the peculiarities of the selected set of pages, a second series of queries of the same type was sent to Google to retrieve a “secondary” general German HTML corpus with a completely disjoint set of pages. Similar procedures were used to obtain a primary and a secondary general English HTML corpus, a general German PDF corpus, and a general English PDF corpus. The translation from PDF to ASCII was found to be error prone, in particular for German documents (cf. Gartner 2003). Due to this process, some converted PDF documents were seriously damaged. Since we focus on errors in original Web pages (as opposed to converted versions of such pages), these ﬁles were excluded as well. We found these pages when computing error rates based on error dictionaries as described in Sections 6 and 7. The number of Web pages and the number of normal tokens (i.e., tokens composed of standard letters only) in the resulting six corpora are shown in Table 1. Numbers (1) and (2) stand for the primary and secondary corpora, respectively.  2.2 Web Corpora for Speciﬁc Thematic Areas We looked at the thematic areas “Middle Ages,” “Holocaust,” “Fish,” “Mushrooms,” and “Neurology.” The given selection of topics tries to cover scientiﬁc areas as well as history and hobbies. Simple Crawl. A ﬁrst series of corpora was collected by sending a query with 25 “terminological” keywords mechanically found in a small corpus of the given area to the AllTheWeb search engine and collecting the answer set. For example, the queries mushrooms mushroom pine edible harvesting morels harvested harvesters dried chanterelle matsutake poisonous ﬂavor chanterelles caps fungi drying stufﬁng humidity varieties boletes recipes spores conifers pickers  Table 1 Number of Web pages, number of normal tokens (tokens composed of standard letters only), and sizes in megabytes of the “general” corpora. Numbers (1) and (2) refer to primary and secondary corpora, respectively.  General corpora Web pages Normal tokens Size (MB)  English HTML (1)  829  English HTML (2)  929  German HTML (1)  618  German HTML (2)  857  English PDF  570  German PDF  603  7,900,337  157  7,152,783  188  9,525,484  189  11,539,035  284  2,193,598  393  1,528,914  240  299  Computational Linguistics  Volume 32, Number 3  disorder disorders anxiety self hallucinations delusions anatomy cortex delusion neuroscience disturbance conscious psychotic stimulus hallucination unconsciously receptors cognitive psychoanalytic unconscious consciously stimuli ego schizophrenia impairment were respectively used for collecting the corpora Mushrooms E and Neurology E. The ranking mechanism of AllTheWeb prefers pages containing hits for several keywords of a disjunctive query. Since this form of corpus construction is straightforward, not all pages in the resulting corpora belong to the given thematic area. Reﬁned Crawl. We wanted to see how results are affected when using less naive crawling methods. For the three areas “Fish,” “Mushrooms,” and “Neurology,” the secondary corpora were retrieved using the following reﬁned procedure: Starting from a small tagged seed corpus for the given domain, we mechanically extracted terminological open compounds for English (Sornlertlamvanich and Tanaka 1996; Smadja and McKeown 1990) and compound nouns for German. Examples are amino group, action potential, defense mechanism (English, neurology), trufﬂe species, morel areas, harvesting tips (English, mushrooms), Koffeinstoffwechsel, and Eisenkonzentration (German, neurology). Each of these expressions was sent as a query to Google. From each answer set we collected a maximum of 30 top-ranked hits (many answer sets were smaller). For each document in the resulting corpus, the similarity with the seed corpus was controlled, using a cosine measure (in practice, almost all documents passed the similarity ﬁlter). Our method can be considered as a variant of Baroni and Bernardini’s (2004) and leads to corpora with a strong thematic focus. The statistics for all thematic corpora are summarized in Table 2. Numbers (1) and (2) stand for corpora crawled with the simple and the reﬁned crawling strategy, respectively. The numbers indicate one interesting effect: Documents in the thematic corpora obtained with the reﬁned crawling strategy turned out to be typically rather short. Since we only used the 30 top-ranked documents for each single query, this probably points to a special feature of Google’s ranking mechanism. A manual inspection of hundreds of documents for both the simple and the reﬁned crawl did not lead to additional insights. 3. Error Detection For detecting orthographic errors of a particular type in texts, two naive base methods may be distinguished. 1. A representative list of errors of the respective type is created and manually checked. Each token of the text appearing in the list represents an error (lower approximation). 2. A spell checker or a large-scale dictionary is used to detect “suspicious” words (error candidates). For each such token W we manually check if W really represents an error and we determine its type (upper approximation). For large corpora, both methods have serious deﬁciencies. With Method 1, only a small percentage of all errors is detected. On this basis, it is difﬁcult to estimate the real number of errors. When using Method 2, the number of tokens that have to be manually 300  Ringlstetter, Schulz, and Mihov  Orthographic Errors in Web Pages  Table 2 Selected topics and statistics of English (E) and German (G) corpora for speciﬁc thematic areas. Numbers (1) and (2) refer to corpora crawled with the simple and the reﬁned strategy, respectively.  Topic/Language Web pages Normal tokens Size (MB)  Middle Ages E  710  Fish E (1)  510  Fish E (2)  940  Holocaust E  699  Mushrooms E (1)  676  Mushrooms E (2)  933  Neurology E (1)  624  Neurology E (2)  923  Middle Ages G  614  Fish G (1)  655  Fish G (2)  804  Holocaust G  616  Mushrooms G (1)  527  Mushrooms G (2)  614  Neurology G (1)  486  Neurology G (2)  323  5,069,796  172  10,090,682  266  547,407  22  8,797,882  199  7,876,067  197  734,337  22  8,765,899  197  779,699  24  6,774,794  195  7,621,579  199  688,882  32  5,659,924  160  5,951,305  147  538,575  28  4,322,952  115  345,070  12  checked becomes too large. In practice, a large number of error candidates represent correct tokens. This is mainly due to special names and other types of nonstandard vocabulary found in Web pages, as mentioned in the introduction. We decided to use a third strategy, which can be considered as a synthesis and compromise between the above two approaches. As a starting point, we took standard dictionaries of English, D(English); German, D(German); French, D(French); and Spanish, D(Spanish); and a dictionary of geographic entities, D(Geos); a dictionary of proper names, D(Names); and a dictionary of abbreviations and acronyms, D(Abbreviations).1 The number of entries in the dictionaries is described in Table 3. The German dictionary contains compound nouns, which explains the large number of entries. From these standard dictionaries, we derived special error dictionaries that were used in the experiments described later. First, for each of the four error types mentioned above we manually collected a number of general patterns that “explain” possible mutations from correct words to erroneous entries. In a second step, these patterns were used to garble the words of the given background dictionaries. Third, garbled words that were found to correspond to correct words (entries of the above dictionaries) were excluded (ﬁltering step). Collecting the remaining erroneous strings, we obtained large error dictionaries for each type of orthographic error. Experiments described in Section 5 show that our error dictionaries cover the major part of all orthographic errors occurring in the English and German Web pages. At  
In this article we reﬁne the formulation of the problem of prepositional phrase (PP) attachment as a four-way disambiguation problem. We argue that, in interpreting PPs, both knowledge about the site of the attachment (the traditional noun–verb attachment distinction) and the nature of the attachment (the distinction of arguments from adjuncts) are needed. We introduce a method to learn arguments and adjuncts based on a deﬁnition of arguments as a vector of features. In a series of supervised classiﬁcation experiments, ﬁrst we explore the features that enable us to learn the distinction between arguments and adjuncts. We ﬁnd that both linguistic diagnostics of argumenthood and lexical semantic classes are useful. Second, we investigate the best method to reach the four-way classiﬁcation of potentially ambiguous prepositional phrases. We ﬁnd that whereas it is overall better to solve the problem as a single four-way classiﬁcation task, verb arguments are sometimes more precisely identiﬁed if the classiﬁcation is done as a two-step process, ﬁrst choosing the attachment site and then labeling it as argument or adjunct. 1. Motivation Incorrect attachment of prepositional phrases (PPs) often constitutes the largest single source of errors in current parsing systems. Correct attachment of PPs is necessary to construct a parse tree that will support the proper interpretation of constituents in the sentence. Consider the timeworn example (1) I saw the man with the telescope. It is important to determine if the PP with the telescope is to be attached as a sister to the noun the man, restricting its interpretation, or if it is to be attached to the verb, thereby indicating the instrument of the main action described by the sentence. Based on examples of this sort, recent approaches have formalized the problem of disambiguating PP attachments as a binary choice, distinguishing between attachment of a PP to a given verb or to the verb’s direct object (Hindle and Rooth 1993; Ratnaparkhi, Reynar, and Roukos 1994; Collins and Brooks 1995; Merlo, Crocker, and Berthouzoz 1997; Stetina and Nagao 1997; Ratnaparkhi 1997; Zhao and Lin 2004). This is, however, a simpliﬁcation of the problem, which does not take the nature of the attachment into account. Precisely, it does not distinguish PP arguments from ∗ Linguistics Department, University of Geneva, 2 rue de Candolle, 1211 Gene`ve 4, Switzerland. † Department of Informatics, University of Sussex, Falmer, Brighton BN1 9QH, UK. Submission received: 28 November 2003; revised submission received: 22 June 2005; accepted for publication: 4 November 2005. © 2006 Association for Computational Linguistics  Computational Linguistics  Volume 32, Number 3  PP adjuncts. Consider the following example, which contains two PPs, both modifying the verb. (2) Put the block on the table in the morning. The ﬁrst PP is a locative PP required by the subcategorization frame of the verb put, whereas in the morning is an optional descriptor of the time at which the action was performed. Although both are attached to the verb, the two PPs entertain different relationships with the verb—the ﬁrst is an argument whereas the latter is an adjunct. Analogous examples could be built for attachments to the noun. (See examples 7a, b.) Thus, PPs cannot only vary depending on the site to which they attach in the structure, such as in example (1), but they can fulﬁll different functions in the sentence, such as in example (2). In principle, then, a given PP could be four-way ambiguous. In practice, it is difﬁcult and moderately unnatural to construct examples of four-way ambiguous sentences, sentences that only a good amount of linguistic and extralinguistic knowledge can disambiguate among the noun-attached and verb-attached option, with an argument or adjunct interpretation. It is, however, not impossible. Consider benefactive constructions, such as the sentence below. (3) Darcy baked a cake for Elizabeth. In this case the for is a benefactive, hence an argument of the verb bake. However, the for-PP is optional; thus other non-argument PPs can occur in the same position. (4) Darcy baked a cake for 5 shillings/for an hour. Whereas in sentence (3) the PP is an argument, in (4) the PP is an adjunct, as indicated by the different status of the corresponding passive sentences and by the ordering of the PPs (arguments prefer to come ﬁrst), as shown in (5) and (6). (5a) Elizabeth was baked a cake by Darcy (5b) *5 shillings/an hour were baked a cake by Darcy (6a) Darcy baked a cake for Elizabeth for 5 shillings/for an hour (6b) ??Darcy baked a cake for 5 shillings/for an hour for Elizabeth This kind of ambiguity also occurs in sentences in which the for-PP is modifying the object noun phrase. Depending on the head noun in object position, and under the assumption that a beneﬁciary is an argument, as we have assumed in the sentences above, the PP will be an argument or an adjunct, as in the following examples, respectively. (7a) Darcy baked [cakes for children] (7b) Darcy baked [cakes for 5 shillings] 342  Merlo and Esteve Ferrer  Argument in Prepositional Phrase Attachment  Modeling both the site and the nature of the attachment of a PP into the tree structure is important. Distinguishing arguments from adjuncts is key to identifying the elements that belong to the semantic kernel of a sentence. Extracting the kernel of a sentence or phrase, in turn, is necessary for automatic acquisition of important lexical knowledge, such as subcategorization frames and argument structures, which is used in several natural language processing (NLP) tasks and applications, such as parsing, machine translation, and information extraction (Srinivas and Joshi 1999; Dorr 1997; Phillips and Riloff 2002). This task is fundamentally syntactic in nature and complements the task of assigning thematic role labels (Gildea and Jurafsky 2002; Nielsen and Pradhan 2004; Xue and Palmer 2004; Swier and Stevenson 2005). See also the common task of CoNNL (2004, 2005) and SENSEVAL-3 (2004). Both a distinction of arguments from adjuncts and an appropriate thematic labeling of the complements of a predicate, verb, or noun are necessary, as conﬁrmed by the annotations adopted by current corpora. Framenet makes a distinction between complements and satellites (Baker, Fillmore, and Lowe 1998). The developers of PropBank integrate the difference between arguments and adjuncts directly into the level of speciﬁcity of their annotation. They adopt labels that are common across verbs for adjuncts. They inherit these labels from the Penn Treebank annotation. Arguments are annotated instead with labels speciﬁc to each verb (Xue 2004; Palmer, Gildea, and Kingsbury 2005). From a quantitative point of view, arguments and adjuncts have different statistical properties. For example, Hindle and Rooth (1993) clearly indicate that their lexical association technique performs much better for arguments than for adjuncts, whether the attachment is to the verb or to the noun. Researchers have abstracted away from this distinction, because identifying arguments and adjuncts is a notoriously difﬁcult task, taxing many native speakers’ intuitions. The usual expectation has been that this discrimination is not amenable to a corpus-based treatment. In recent preliminary work, however, we have succeeded in distinguishing arguments from adjuncts using corpus evidence (Merlo and Leybold 2001; Merlo 2003). Our method develops corpus-based statistical correlates for the diagnostics used in linguistics to decide whether a PP is an argument or an adjunct. A numerical vectorial representation of the notion of argumenthood is provided, which supports automatic classiﬁcation. In the current article, we expand and improve on this work, by developing new measures and reﬁning the previous ones. We also extend that work to attachment to nouns. This extension enables us to explore in what way the distinction between argument and adjunct is best integrated in the traditional attachment disambiguation problem. We treat PP attachment as a four-way classiﬁcation of PPs into noun argument PPs, noun adjunct PPs, verb argument PPs, and verb adjunct PPs. We investigate this new approach to PP attachment disambiguation through several sets of experiments, testing different hypotheses on the argument/adjunct distinction of PPs and on its interaction with the disambiguation of the PP attachment site. The two main claims can be formulated as follows. r Hypothesis 1: The argument/adjunct distinction can be performed based on information collected from a minimally annotated corpus, approximating deeper semantic information statistically. r Hypothesis 2: The learning features developed for the notion of argument and adjunct can be usefully integrated in a ﬁner-grained formulation of the problem of PP attachment as a four-way classiﬁcation. 343  Computational Linguistics  Volume 32, Number 3  To test these two hypotheses, we illustrate our technique to distinguish arguments from adjuncts (Section 2), and we report results on this binary classiﬁcation (Sections 3 and 4). The intuition behind the technique is that we do not need to represent the distinction between arguments and adjuncts directly, but that the distinction can be indirectly represented as a numerical vector. The feature values in the vector are corpus-based numerical equivalents of the grammaticality diagnostics used by linguists to decide whether a PP is an argument or an adjunct. For example, one of the values in the vector indicates if the PP is optional, whereas another one indicates if the PP can be iterated. Optionality and iterability are two of the criteria used by linguists to determine whether a PP is an argument or an adjunct. In Section 5, we show how this distinction supports a more reﬁned formulation of the problem of PP attachment. We compare two methods to reach a four-way classiﬁcation. One method is a two-step process that ﬁrst classiﬁes PPs as attached to the noun or to the verb, and then reﬁnes the classiﬁcation by assigning argument or adjunct status to the disambiguated PPs. The other method is a one-step process that performs the four-way classiﬁcation directly. We ﬁnd that the latter has better overall performance, conﬁrming our expectation (Hypothesis 2). In Section 6 we discuss the implications of the results for a deﬁnition of the notion of argument and compare our work to that of the few researchers who have attempted to perform the same distinction.  2. Distinguishing Arguments from Adjuncts Solving the four-way classiﬁcation task described in the introduction crucially relies on the ability to distinguish arguments from adjuncts, using corpus counts. The ability to automatically make this distinction is necessary for the correct automatic acquisition of important lexical knowledge, such as subcategorization frames and argument structures, which is used in parsing, generation, machine translation, and information extraction (Srinivas and Joshi 1999; Stede 1998; Dorr 1997; Riloff and Schmelzenbach 1998). Yet, few attempts have been made to make this distinction automatically. The core difﬁculty in this enterprise is to deﬁne the notion of argument precisely enough that it can be used automatically. There is a consensus in linguistics that arguments and adjuncts are different both with respect to their function in the sentence and in the way they themselves are interpreted (Jackendoff 1977; Marantz 1984; Pollard and Sag 1987; Grimshaw 1990). With respect to their function, an argument ﬁlls a role in the relation described by its associated head, whereas an adjunct predicates a separate property of its associate head or phrase. With respect to their interpretation, a complement is an argument if its interpretation depends exclusively on the head with which it is associated, whereas it is an adjunct if its interpretation remains relatively constant when associating with different heads (Grimshaw 1990, page 108). These semantic differences give rise to some observable distributional consequences: for a given interpretation, an adjunct can co-occur with a relatively broad range of heads, whereas arguments are limited to co-occurrence with a (semantically restricted) class of heads (Pollard and Sag 1987, page 136). Restricting the discussion to PPs, these differences are illustrated in the following examples (PP-argument in bold); see also Schu¨ tze (1995, page 100). (8a) Maria is a student of physics. (8b) Maria is a student from Phoenix. 344  Merlo and Esteve Ferrer  Argument in Prepositional Phrase Attachment  In example (8a), the head student implies that a subject is being studied. The sentence tells us only one property of Maria: that she is a student of physics. In example (8b), the PP instead predicates a different property of the student, namely her geographical origin, which is not implied by the head student. (9a) Kim camps/jogs/meditates on Sunday. (9b) Kim depended/blamed the arson on Sandy. In example (9a) the PP on Sunday can be construed without any reference to the preceding part of the sentence, and it preserves its meaning even when combining with different heads. This is, however, not the case for (9b). Here, the PP can only be properly understood in connection with the rest of the sentence: Sandy is the person on whom someone depends or the person on which the arson is blamed. These semantic distinctions between arguments and adjuncts surface in observable syntactic differences and can be detected automatically both by using general formal features and by speciﬁc lexical semantic features, which group together the arguments of a lexical head. Unfortunately, the linguistic diagnostics that are used to determine whether a PP is an adjunct or an argument are not accurate in all circumstances, they often partition the set of the examples differently, and they give rise to relative, and not absolute, acceptability judgments. We propose a methodology that retains both the linguistic insight of the grammatical tests and the ability to effectively combine several gradient, partial diagnostics, typical of automatic induction methods. Speciﬁcally, we ﬁrst ﬁnd countable diagnostics for the argument–adjunct distinction, which we approximate statistically and estimate using corpus counts. We also augment the feature vector with information encoding the semantic classes of the input words. The diagnostics and the semantic classes are then automatically combined in a decision tree induction algorithm. The diagnostics are presented below. 2.1 The Diagnostics Many diagnostics for argumenthood have been proposed in the literature (Schu¨ tze 1995). Some of them require complex syntactic manipulation of the sentence, such as whextraction, and are therefore too difﬁcult to apply automatically. We choose six formal diagnostics that can be captured by simple corpus counts: head dependence, optionality, iterativity, ordering, copular paraphrase, and deverbal nominalization. These diagnostics tap into the deeper semantic properties that distinguish arguments from adjuncts, without requiring that the distinctions be made explicit. Head Dependence. Arguments depend on their lexical heads because they form an integral part of the phrase. Adjuncts do not. Consequently, PP-arguments can only appear with the speciﬁc verbal head by which they are lexically selected, whereas PP-adjuncts can co-occur with a far greater range of different heads than arguments because they are necessary for the correct interpretation of the semantics of the verb, as illustrated in the following example sentences. (10a) a man/woman/dog/moppet/scarecrow with gray hair (10b) a menu/napkin/glass/waitress/matchbook from Rosie’s 345  Computational Linguistics  Volume 32, Number 3  (11a) a member/*dog/*moppet/*scarecrow of Parliament (11b) a student/*punk/*watermelon/*Martian/*poodle/*VCR of physics  We expect an argument PP to occur with fewer heads, whereas an adjunct PP will occur with more heads, as it is not required by a speciﬁc verb, but it can in principle adjoin to any verb or noun head. We capture this insight by estimating the dispersion of the distribution of the different heads that co-occur with a given PP in a corpus. We expect adjunct PPs to have higher dispersion than argument PPs. We use entropy as a measure of the dispersion of the distribution, as indicated in equation (1) (h indicates the noun or verb head to which the PP is attached, X is the random variable whose outcomes are the values of h).  hdep(PP) ≈ HPP(X) = −Σh∈Xp(h)log2p(h)  (1)  Optionality. In most cases, PP-arguments are obligatory elements of a given sentence whose absence leads to ungrammaticality, while adjuncts do not contribute to the semantics of any particular verb, hence they are optional, as illustrated in the following examples (PP-argument in bold):  (12a) John put the book in the room. (12b) ∗John put the book. (12c) John saw/read the book in the room. (12d) John saw/read the book.  Since arguments are obligatory complements of a verb, whereas adjuncts are not, we expect knowledge of a given verb to be more informative with respect to the probability of existence of an argument than of an adjunct. Thus we expect that the predictive power of a verb with regard to its complements will be greater for arguments than for adjuncts.1 The notion of optionality can be captured by the conditional probability of a PP given a particular verbal head, as indicated in equation (2).  opt(PP) ≈ P(PP|v)  (2)  Iterativity and Ordering. Because they receive a semantic role from the selecting verb, arguments of the same type cannot be iterated because verbs can only assign any given type of role once. Moreover, in English, arguments must be adjacent to the selecting lexical head. Neither of these two restrictions apply to adjuncts, which can be iterated, and follow arguments in a sequence of PPs. Consequently, in a sequence of several PPs  
∗ Institute for Information Technology, National Research Council Canada, M-50 Montreal Road, Ottawa, Ontario, Canada K1A 0R6. E-mail: peter.turney@nrc-cnrc.gc.ca. Submission received: 30 March 2005; revised submission received: 10 November 2005; accepted for publication: 27 February 2006. © 2006 Association for Computational Linguistics  Computational Linguistics  Volume 32, Number 3  relational similarity can be reduced to attributional similarity, since mason and carpenter are attributionally similar, as are stone and wood. In general, this reduction fails. Consider the analogy trafﬁc:street::water:riverbed. Trafﬁc and water are not attributionally similar. Street and riverbed are only moderately attributionally similar. Many algorithms have been proposed for measuring the attributional similarity between two words (Lesk 1969; Resnik 1995; Landauer and Dumais 1997; Jiang and Conrath 1997; Lin 1998b; Turney 2001; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003). Measures of attributional similarity have been studied extensively, due to their applications in problems such as recognizing synonyms (Landauer and Dumais 1997), information retrieval (Deerwester et al. 1990), determining semantic orientation (Turney 2002), grading student essays (Rehder et al. 1998), measuring textual cohesion (Morris and Hirst 1991), and word sense disambiguation (Lesk 1986). On the other hand, since measures of relational similarity are not as well developed as measures of attributional similarity, the potential applications of relational similarity are not as well known. Many problems that involve semantic relations would beneﬁt from an algorithm for measuring relational similarity. We discuss related problems in natural language processing, information retrieval, and information extraction in more detail in Section 3. This article builds on the Vector Space Model (VSM) of information retrieval. Given a query, a search engine produces a ranked list of documents. The documents are ranked in order of decreasing attributional similarity between the query and each document. Almost all modern search engines measure attributional similarity using the VSM (Baeza-Yates and Ribeiro-Neto 1999). Turney and Littman (2005) adapt the VSM approach to measuring relational similarity. They used a vector of frequencies of patterns in a corpus to represent the relation between a pair of words. Section 4 presents the VSM approach to measuring similarity. In Section 5, we present an algorithm for measuring relational similarity, which we call Latent Relational Analysis (LRA). The algorithm learns from a large corpus of unlabeled, unstructured text, without supervision. LRA extends the VSM approach of Turney and Littman (2005) in three ways: (1) The connecting patterns are derived automatically from the corpus, instead of using a ﬁxed set of patterns. (2) Singular Value Decomposition (SVD) is used to smooth the frequency data. (3) Given a word pair such as trafﬁc:street, LRA considers transformations of the word pair, generated by replacing one of the words by synonyms, such as trafﬁc:road or trafﬁc:highway. Section 6 presents our experimental evaluation of LRA with a collection of 374 multiple-choice word analogy questions from the SAT college entrance exam.1 An example of a typical SAT question appears in Table 1. In the educational testing literature, the ﬁrst pair (mason:stone) is called the stem of the analogy. The correct choice is called the solution and the incorrect choices are distractors. We evaluate LRA by testing its ability to select the solution and avoid the distractors. The average performance of collegebound senior high school students on verbal SAT questions corresponds to an accuracy of about 57%. LRA achieves an accuracy of about 56%. On these same questions, the VSM attained 47%.  
Columbia University  This article focuses on the analysis and prediction of corrections, deﬁned as turns where a user tries to correct a prior error made by a spoken dialogue system. We describe our labeling procedure of various corrections types and statistical analyses of their features in a corpus collected from a train information spoken dialogue system. We then present results of machinelearning experiments designed to identify user corrections of speech recognition errors. We investigate the predictive power of features automatically computable from the prosody of the turn, the speech recognition process, experimental conditions, and the dialogue history. Our best-performing features reduce classiﬁcation error from baselines of 25.70–28.99% to 15.72%. 1. Introduction Compared to many other systems, spoken dialogue systems (SDS) tend to have more difﬁculties in correctly interpreting user input. Whereas a car normally goes left if the driver turns the steering wheel in that direction or a vacuum cleaner starts working if one pushes the on button, interactions between a user and a spoken dialogue system are often hampered by mismatches between the action intended by the user and the action executed by the system. Such mismatches are mainly due to errors in the Automatic Speech Recognition (ASR) and/or the Natural Language Understanding (NLU) component of these systems; they can also be due to wrong default assumptions of the system or the fact that a user asks out-of-topic questions for which the system was not designed. To solve these mismatches, users often have to put considerable effort into trying to make it clear to the system that there was a problem, and trying to correct it by reentering misrecognized or misinterpreted information. Previous research has already brought to light that it is not always easy for users to determine whether their intended actions were carried out correctly or not, in particular when the dialogue system does not give appropriate feedback about its internal representation at the right moment. In addition, users’ corrections may miss their goal because corrections themselves are more difﬁcult for the system to recognize and interpret correctly, which may lead to so-called cyclic (or spiral) errors. ∗ E-mail: litman@cs.pitt.edu. † E-mail: julia@cs.columbia.edu. ‡ E-mail: m.g.j.swerts@uvt.nl. Submission received: 12 January 2005; revised submission received: 3 April 2006; accepted for publication: 4 May 2006 © 2006 Association for Computational Linguistics  Computational Linguistics  Volume 32, Number 3  Given that current spoken dialogue systems are not sufﬁciently robust, there is need for sophisticated error-handling strategies to gracefully solve communication problems between the system and the user. Ideally, apart from strategies to prevent errors, error handling would consist of steps to immediately detect an error when it occurs and to interact with the user to correct the error in subsequent exchanges. To date, attempts to improve system performance have largely focused on improving ASR accuracy or simplifying the task, either by further constraining the domain and functionality of the system or by further restricting the vocabulary the system must recognize. Such studies include work on improved acoustic and semantic conﬁdence scores (Ammicht, Potamianos, and Fosler-Lussier 2001; Andorno, Laface, and Gemello 2002; Bouwman, Sturm, and Boves 1999; Falavigna, Gretter, and Riccardi 2002; Guillevic, Gandrabur, and Normandin 2002; Moreno, Logan, and Raj 2001; Wang and Lin 2002; Zhang and Rudnicky 2001), on new system architectures for error handling (McTear et al. 2005; Prodanov and Drygajlo 2005; Torres et al. 2005), on new interfaces that are more user-friendly for error recovery (Bulyko et al. 2005; Karsenty and Botherel 2005; Sturm and Boves 2005), and on the use of error-recovery strategies that are based on analyses of human–human dialogues (Skantze 2005), including the use of facial expressions (Barkhuysen, Krahmer, and Swerts 2005). However, as ASR accuracy improves, dialogue systems will be called upon to handle ever more complex tasks and ever less restricted vocabularies. So, it seems likely that spoken dialogue systems will, for the foreseeable future, always require effective error detection and repair strategies. In previous research (Hirschberg, Litman, and Swerts 1999, 2004), we identiﬁed new procedures to detect recognition errors, which perform well when tested on two different corpora, the TOOT and W99 corpora (train information and conference registration dialogues) collected using two different ASR systems (Sharp et al. 1997; Kamm et al. 1997). We found that prosodic features, in combination with information already available to the recognizer, such as acoustic conﬁdence scores, grammar, and recognized string, can distinguish speaker turns that are misrecognized far better than traditional methods for ASR rejection (the system decision that its hypothesis is so weak that it should reprompt for fresh input), which use acoustic conﬁdence scores alone. Related work has been done by Lendvai (2004) and Batliner et al. (2003). In the current study, we turn to the question of how people try to correct ASR errors in their interactions with machines and the role that prosody may play in identifying user corrections and in helping to analyze them. Understanding how users attempt to correct system failures and why their attempts succeed or fail is important to improve the design of future spoken dialogue systems. For example, knowing whether they are more likely to repeat or rephrase their utterances, add new information or shorten their input, and how system behavior inﬂuences these choices can suggest appropriate on-line modiﬁcations to the system’s interaction strategy or to the recognition procedure it employs. Determining which speaker behaviors are more successful in correcting system errors can also lead to improvements in the help information such systems provide. There is growing evidence that there is much variance in the way people react to system errors and that the variance can be explained on the basis of particular properties of the dialogue system or the dialogue context. In particular, dialogue conﬁrmation strategies may hinder users’ ability to correct system error. For instance, if a system wrongly presents information as being correct, as when it veriﬁes information implicitly, users become confused about how to respond (Krahmer et al. 2001). Other studies have shown that speakers tend to switch to a prosodically “marked” speaking style after communication errors, comparing repetition corrections with the speech being repeated (Wade, Shriberg, and Price 1992; Oviatt et al. 1996; 418  Litman, Hirschberg, and Swerts  Corrections in Spoken Dialogue Systems  Levow 1998; Bell and Gustafson 1999). Although this speaking style may be effective in problematic human–human communicative settings, there is evidence that suggests it leads to further errors in human–machine interactions (Levow 1998; Soltau and Waibel 2000). That corrections are difﬁcult for ASR systems is generally explained by the fact that they tend to be hyperarticulated—higher, louder, longer—than other turns (Wade, Shriberg, and Price 1992; Oviatt et al. 1996; Levow 1998; Bell and Gustafson 1999; Shimojima, et al. 2001; Soltau and Waibel 1998, 2000; Soltau, Metze, and Waibel 2002), where ASR models are not well adapted to handle this special speaking style, although recent studies suggest that ASR systems are becoming less vulnerable to hyperarticulation (Bulyko et al. 2005). So, repair strategies in human–machine interactions can be more or less effective. Therefore, increased knowledge about the efﬁciency of different correction strategies can lead to a number of possible courses of action. System strategy might be chosen to favor the type(s) of correction the system can most easily process. Or, having chosen a particular interaction strategy, the system repair strategy might be tuned to handle the correction types that that strategy is likely to produce. Alternatively, the system’s dialogue manager might use the detection of corrections as a signal that it should modify its interaction strategy, either locally, by beginning a subdialogue for faster error recovery, or globally, by changing its initiative or conﬁrmation strategies, or even directing the user to a human operator. Or, since corrections are often hyperarticulated, detection of a correction could serve as a signal to the ASR engine to run a recognizer trained on hyperarticulated speech in parallel with its normal processor, to better transcribe the speech. All of these possibilities, however, assume that user corrections can be detected by the system reliably during the dialogue. In this article, we describe an analysis of user corrections of system error collected in the TOOT spoken dialogue system. In the next section, we describe the corpus itself and how it was collected and labeled. The corpus is suitable to gain insight into the different correction strategies that speakers exploit in different dialogue contexts and interaction styles. Then, we characterize the nature of corrections in this corpus in terms of when they occur, how well they are handled by the system, what distinguishes their prosody from other utterances, their relationship to the utterances they correct, and how they differ according to dialogue strategy. Then we present results of some machine-learning experiments designed to automatically distinguish corrections from other user input, using features that we derived as potentially useful from our descriptive analyses. 2. The Data 2.1 The TOOT Corpus Our corpus consists of dialogues between human subjects and TOOT, a spoken dialogue system that allows access to train information from the Web via telephone. TOOT was collected to study variations in dialogue strategy and in user-adapted interaction (Litman and Pan 1999). It is implemented using an interactive voice response (IVR) platform developed at AT&T, combining ASR and text-to-speech with a phone interface (Kamm et al. 1997). The system’s speech recognizer is a speaker-independent, hidden Markov model system with context-dependent phone models for telephone speech and constrained (rule-based) grammars deﬁning vocabulary at any dialogue state. Whereas a “universal” grammar specifying all legal utterances was used at some points in the dialogue, seven smaller grammars were also used at many points in the dialogue (e.g., to recognize city names, days of the week, answers to yes/no questions, 419  Computational Linguistics  Volume 32, Number 3  etc.). Grammars were only written for originally expected answers; in other words, no speciﬁc grammar for corrections was built in.1 Conﬁdence scores for recognition were available only at the turn level and were based on acoustic likelihoods; thresholds for rejecting an utterance based on conﬁdence scores were speciﬁed manually by the system designers and were set differently for different grammars. The platform supports barge-in. Subjects performed four tasks with one of several versions of the system that differed in terms of locus of initiative (system, user, or mixed), conﬁrmation strategy (explicit, implicit, or none), and whether these conditions could be changed by the user during the task (adaptive vs. non-adaptive). In the adaptive version of the system, users were allowed to say change strategy at any point(s) in the dialogue. TOOT would then ask the user to specify new initiative and conﬁrmation strategies, for example, You are using the no conﬁrmation strategy. Which conﬁrmation strategy do you want to change to? No conﬁrmation, implicit conﬁrmation, or explicit conﬁrmation? TOOT’s initiative strategy speciﬁes who has control of the dialogue, whereas TOOT’s conﬁrmation strategy speciﬁes how and whether TOOT lets the user know what it just understood. The fragments in Figure 1 provide some illustrations of how dialogues vary with strategy. For example, in user initiative mode, the system allows the user to specify any number of attributes in a single utterance. Thus, the system will let the user ignore speciﬁc questions. In the example in Figure 1, although the system asks for the day of the week, the user answers with the time, which can be recognized due to the use of the “universal grammar.” In contrast, in both mixed and system initiative mode, when a speciﬁc question is asked, one of the restricted grammars is used to recognize the response. Finally, in universal and mixed but not system initiative mode, the system can ask both speciﬁc questions and open-ended questions (e.g., How may I help you?). Subjects were 39 students: 20 native speakers and 19 non-native, 16 women and 23 men. Dialogues were recorded and system and user behavior were logged automatically. The concept accuracy (CA) of each turn was manually labeled. If the ASR correctly captured all task-related information in the turn (e.g., time, departure, and arrival cities), the turn’s CA score was 1 (semantically correct). Otherwise, the CA score reﬂected the percentage of correctly recognized task information in the turn. The dialogues were also transcribed and automatically scored in comparison to the ASR recognized string (the best hypothesis output by the ASR engine) to produce a word error rate (WER) for each turn. For the study described below, we examined 2,328 user turns (all user input between two system inputs) from 152 dialogues. 2.2 Labeling To identify corrections in the corpus two authors independently labeled each turn as to whether or not it constituted a correction of a prior system failure (a rejection or CA error, which were the only system failure subjects were aware of) and subsequently decided upon a consensus label. Note that many of the discrepancies between labels were due to tiredness or incidental sloppiness of individual annotators, rather than true disagreement. Each turn labeled “correction” was further classiﬁed as belonging to one of the following categories: REP (repetition, including repetitions with differences in pronunciation or ﬂuency), PAR (paraphrase), ADD (task-relevant content added), OMIT 
 Computational Linguistics  Volume 32, Number 3  Table 1 List of chapters in New Developments in Parsing Technology. 
Chapter III provides a sketch of computational linguistics (CL) applications. The included discussion serves to illustrate that all of the relevant applications require sophisticated linguistic knowledge. The general lesson of this chapter can be summarized thus: “Most language processing tasks can be considered as special cases of the general task of language understanding, one of the ultimate goals of CL and AI.” Chapter IV describes the MTT theory in more detail and makes interesting comparisons between MTT, HPSG, and Chomskian frameworks. Chapter V introduces the problem of language modeling in CL. The last section of the book includes various exercises, review questions, and multiple-choice problems. This book is a nice addition to the short list of CL textbooks. It is especially helpful for students in Spanish CL, given its numerous Spanish-related examples and  Computational Linguistics  
Moreover, the editors are at pains to provide a coherent intellectual context within which to interpret the collected material. For a start, each of the four parts has an  Computational Linguistics  Volume 32, Number 3  
Chapter 3 reviews other current notions of semantic role representation. It presents in detail Dowty’s (1991) approach on proto-roles, pointing out some of the limitations of this work. It compares this work to Schlesinger (1995) emphasizing potential beneﬁts of the prototypical approach and presents an extensive survey of approaches that extended Dowty’s. This chapter also introduces the approach taken by Van Valin (1990)  Computational Linguistics  Volume 32, Number 3  
© 2006 Association for Computational Linguistics  Computational Linguistics  Volume 32, Number 2  How can we obtain a semantic classiﬁcation of verbs while avoiding tedious manual deﬁnitions of the verbs and the classes? Few resources are semantically annotated and provide semantic information off-the-shelf such as FrameNet (Baker, Fillmore, and Lowe 1998; Fontenelle 2003) and PropBank (Palmer, Gildea, and Kingsbury 2005). Instead, the automatic construction of semantic classes typically beneﬁts from a longstanding linguistic hypothesis that asserts a tight connection between the lexical meaning of a verb and its behavior: To a certain extent, the lexical meaning of a verb determines its behavior, particularly with respect to the choice of its arguments (Pinker 1989; Levin 1993; Dorr and Jones 1996; Siegel and McKeown 2000; Merlo and Stevenson 2001; Schulte im Walde and Brew 2002; Lapata and Brew 2004). Even though the meaning–behavior relationship is not perfect, we can make this prediction: If we induce a verb classiﬁcation on the basis of verb features describing verb behavior, then the resulting behavior classiﬁcation should agree with a semantic classiﬁcation to a certain extent (yet to be determined). The aim of this work is to utilize this prediction for the automatic acquisition of German semantic verb classes. The verb behavior itself is commonly captured by the diathesis alternation of verbs: alternative constructions at the syntax–semantics interface that express the same or a similar conceptual idea of a verb (Lapata 1999; Schulte im Walde 2000; McCarthy 2001; Merlo and Stevenson 2001; Joanis 2002). Consider example (1), where the most common alternations of the Manner of Motion with a Vehicle verb fahren ‘drive’ are illustrated. The conceptual participants are a vehicle, a driver, a passenger, and a direction. In (a), the vehicle is expressed as the subject in a transitive verb construction, with a prepositional phrase indicating the direction. In (b), the driver is expressed as the subject in a transitive verb construction, with a prepositional phrase indicating the direction. In (c), the driver is expressed as the subject in a transitive verb construction, with an accusative noun phrase indicating the vehicle. In (d), the driver is expressed as the subject in a ditransitive verb construction, with an accusative noun phrase indicating the passenger, and a prepositional phrase indicating the direction. Even if a certain participant is not realized within an alternation, its contribution might be implicitly deﬁned by the verb. For example, in the German sentence in (a) the driver is not expressed overtly, but we know that there is a driver, and in (b) and (d) the vehicle is not expressed overtly, but we know that there is a vehicle. Verbs in the same semantic class are expected to overlap in their alternation behavior to a certain extent. For example, the Manner of Motion with a Vehicle verb ﬂiegen ‘ﬂy’ alternates between (a) such as in Der Airbus A380 ﬂiegt nach New York ‘The Airbus A380 ﬂies to New York’, (b) in marked cases as in Der a¨ltere Pilot ﬂiegt nach London ‘The older pilot ﬂies to London’, (c) as in Pilot Schulze ﬂiegt eine Boing 747 ‘Pilot Schulze ﬂies a Boing 747’, and (d) as in Der Pilot ﬂiegt seine Passagiere nach Thailand ‘The pilot ﬂies his passengers to Thailand’; the Manner of Motion with a Vehicle verb rudern ‘row’ alternates between (b) such as in Anna rudert u¨ ber den See ‘Anna rows over the lake’, (c) such as in Anna rudert das blaue Boot ‘Anna rows the blue boat’, and (d) such as in Anna rudert ihren kleinen Bruder u¨ ber den See ‘Anna rows her little brother over the lake’.  Example 1 (a) Der Wagen fa¨hrt in die Innenstadt. ‘The car drives to the city centre.’ (b) Die Frau fa¨hrt nach Hause. ‘The woman drives home.’ 160  Schulte im Walde  Induction of German Semantic Verb Classes  (c) Der Filius fa¨hrt einen blauen Ferrari. ‘The son drives a blue Ferrari.’ (d) Der Junge fa¨hrt seinen Vater zum Zug. ‘The boy drives his father to the train.’ We decided to use diathesis alternations as an approach to characterizing verb behavior, and to use the following verb features to stepwise describe diathesis alternations: (1) syntactic structures, which are relevant for capturing argument functions; (2) prepositions, which are relevant to distinguish, for example, directions from locations; and (3) selectional preferences, which concern participant roles. A statistical grammar model serves as the source for an empirical verb description for the three levels at the syntax– semantics interface. Based on the empirical feature description, we then perform a cluster analysis of the German verbs using k-means, a standard unsupervised hard clustering technique as proposed by Forgy (1965). The clustering outcome cannot be a perfect semantic verb classiﬁcation, since the meaning–behavior relationship on which the clustering relies is not perfect, and the clustering method is not perfect for the ambiguous verb data. However, our primary goal is not necessarily to obtain the optimal clustering result, but rather to assess the linguistic and technical conditions that are crucial for a semantic cluster analysis. More speciﬁcally, (1) we perform an empirical investigation of the relationship between verb meaning and verb behavior (that is, Can we use the meaning–behavior relationship of verbs to induce verb classes, and to what extent does the meaning–behavior relationship hold in the experiments?), and (2) we investigate which technical parameters are suitable for the natural language task. The resulting clustering methodology can then be applied to a larger-scale verb set. The plan of the article is as follows. Section 2 describes the experimental setup with respect to (1) gold standard verb classes for 168 German verbs, (2) the statistical grammar model that provides empirical lexical information for German verbs at the syntax–semantics interface, and (3) the clustering algorithm and evaluation methods. Section 3 performs preliminary clustering experiments on the German gold standard verbs, and Section 4 presents an application of the clustering technique in a large-scale experiment. Section 5 discusses related work, and Section 6 presents the conclusions and outlook for further work. 2. Experimental Setup 2.1 German Semantic Verb Classes A set of 168 German verbs was manually classiﬁed into 43 concise semantic verb classes. The verb class labels refer to the common semantic properties of the verbs in a class at a general conceptual level, and the idiosyncratic lexical semantic properties of the verbs are left underspeciﬁed. The German verbs are provided with a coarse translation into English, given here in brackets; we do not attempt to deﬁne subtle differences in meaning or usage. The translated verb senses only refer to the respective semantic class; if the verb translations in one class are too similar to distinguish among them, a common translation is given. Even though the classiﬁcation is primarily based on semantic intuition and not on facts about syntactic behavior, the verbs grouped in one class share certain aspects of their behavior. (Please note that this overlap does not necessarily transfer to the English translations.) This agreement corresponds to the 161  Computational Linguistics  Volume 32, Number 2  long-standing linguistic hypothesis that asserts a tight connection between the meaning components of a verb and its behavior (Pinker 1989; Levin 1993). The purpose of the manual classiﬁcation is to evaluate the reliability and performance of the clustering experiments. The following facts refer to empirically relevant properties of the classiﬁcation: The class size is between 2 and 7, with an average of 3.9 verbs per class. Eight verbs are ambiguous with respect to class membership and marked by subscripts. The classes include both high- and low-frequency verbs in order to exercise the clustering technology in both data-rich and data-poor situations: The corpus frequencies of the verbs range from 8 to 71,604 (within 35 million words of a German newspaper corpus, cf. Section 2.2). The class labels are given on two semantic levels: coarse labels such as Manner of Motion are subdivided into ﬁner labels, such as Locomotion, Rotation, Rush, Vehicle, Flotation. The ﬁne-grained labels are relevant for the clustering experiments, as the numbering indicates. As mentioned before, the classiﬁcation is primarily based on semantic intuition, not on facts about syntactic behavior. As an extreme example, the Support class (23) contains the verb unterstu¨ tzen, which syntactically requires a direct object, together with the verbs dienen, folgen, and helfen, which dominantly subcategorize for an indirect object. The classiﬁcation was checked to ensure lack of bias, so class membership is not disproportionately made up of highfrequency verbs, low-frequency verbs, strongly ambiguous verbs, verbs from speciﬁc semantic areas, and so forth. The classiﬁcation deliberately sets high standards for the automatic induction process: It would be easier (1) to deﬁne the verb classes on a purely syntactic basis, since syntactic properties are easier to obtain automatically than semantic features, or (2) to deﬁne larger classes of verbs, so that the distinction between the classes is not based on ﬁne-grained verb properties, or (3) to disregard clustering complications such as verb ambiguity and low-frequency verbs. But the overall goal is not to achieve a perfect clustering on the given 168 verbs but to investigate both the potential and the limits of our clustering methodology that combines easily available data with a simple algorithm. The task cannot be solved completely, but we can investigate the bounds. The classiﬁcation is deﬁned as follows: 1. Aspect: anfangen, aufho¨ ren, beenden, beginnen, enden (start, stop, ﬁnish, begin, end) 2. Propositional Attitude: ahnen, denken, glauben, vermuten, wissen (guess, think, believe, assume, know) • Desire 3. Wish: erhoffen, wollen, wu¨ nschen (hope, want, wish) 4. Need: bedu¨ rfen, beno¨ tigen, brauchen (all: need/require) 5. Transfer of Possession (Obtaining): bekommen, erhalten, erlangen, kriegen (all: receive/obtain) • Transfer of Possession (Giving) 6. Gift: geben, leihen, schenken, spenden, stiften, vermachen, u¨ berschreiben (give, borrow, present, donate, donate, bequeath, sign over) 7. Supply: bringen, liefern, schicken, vermitteln1, zustellen (bring, deliver, send, convey, deliver) 162  Schulte im Walde  Induction of German Semantic Verb Classes  • Manner of Motion 8. Locomotion: gehen, klettern, kriechen, laufen, rennen, schleichen, wandern (go, climb, creep, walk, run, sneak, wander) 9. Rotation: drehen, rotieren (turn around, rotate) 10. Rush: eilen, hasten (both: hurry) 11. Vehicle: fahren, ﬂiegen, rudern, segeln (drive, ﬂy, row, sail) 12. Flotation: ﬂießen, gleiten, treiben (ﬂoat, glide, ﬂoat) • Emotion 13. Origin: a¨rgern, freuen (be annoyed, be happy) 14. Expression: heulen1, lachen1, weinen (cry, laugh, cry) 15. Objection: a¨ngstigen, ekeln, fu¨ rchten, scheuen (frighten, disgust, fear, be afraid) 16. Facial Expression: ga¨hnen, grinsen, lachen2, la¨cheln, starren (yawn, grin, laugh, smile, stare) 17. Perception: empﬁnden, erfahren1, fu¨ hlen, ho¨ ren, riechen, sehen, wahrnehmen (feel, experience, feel, hear, smell, see, perceive) 18. Manner of Articulation: ﬂu¨ stern, rufen, schreien (whisper, shout, scream) 19. Moaning: heulen2, jammern, klagen, lamentieren (all: wail/moan/ complain) 20. Communication: kommunizieren, korrespondieren, reden, sprechen, verhandeln (communicate, correspond, talk, talk, negotiate) • Statement 21. Announcement: anku¨ ndigen, bekanntgeben, ero¨ ffnen, verku¨ nden (all: announce) 22. Constitution: anordnen, bestimmen, festlegen (arrange, determine, constitute) 23. Promise: versichern, versprechen, zusagen (ensure, promise, promise) 24. Observation: bemerken, erkennen, erfahren2, feststellen, realisieren, registrieren (notice, realize, get to know, observe, realize, realize) 25. Description: beschreiben, charakterisieren, darstellen1, interpretieren (describe, characterize, describe, interpret) 26. Presentation: darstellen2, demonstrieren, pra¨sentieren, veranschaulichen, vorfu¨ hren (present, demonstrate, present, illustrate, demonstrate) 27. Speculation: gru¨ beln, nachdenken, phantasieren, spekulieren (muse, think about, fantasize, speculate) 28. Insistence: beharren, bestehen1, insistieren, pochen (all: insist) 29. Teaching: beibringen, lehren, unterrichten, vermitteln2 (all: teach) • Position 30. Bring into Position: legen, setzen, stellen (lay, set, put upright) 31. Be in Position: liegen, sitzen, stehen (lie, sit, stand)  163  Computational Linguistics  Volume 32, Number 2  32. Production: bilden, erzeugen, herstellen, hervorbringen, produzieren (all: generate/produce) 33. Renovation: dekorieren, erneuern, renovieren, reparieren (decorate, renew, renovate, repair) 34. Support: dienen, folgen1, helfen, unterstu¨ tzen (serve, follow, help, support) 35. Quantum Change: erho¨ hen, erniedrigen, senken, steigern, vergro¨ ßern, verklenern (increase, decrease, decrease, increase, enlarge, diminish) 36. Opening: o¨ ffnen, schließen1 (open, close) 37. Existence: bestehen2, existieren, leben (exist, exist, live) 38. Consumption: essen, konsumieren, lesen, saufen, trinken (eat, consume, read, booze, drink) 39. Elimination: eliminieren, entfernen, exekutieren, to¨ ten, vernichten (eliminate, delete, execute, kill, destroy) 40. Basis: basieren, beruhen, gru¨ nden, stu¨ tzen (all: be based on) 41. Inference: folgern, schließen2 (conclude, infer) 42. Result: ergeben, erwachsen, folgen2, resultieren (all: follow/result) 43. Weather: blitzen, donnern, da¨mmern, nieseln, regnen, schneien (lightning, thunder, dawn, drizzle, rain, snow) The evidence used in the class creation process—including the choice of the verbs—was provided by subjective conceptual knowledge, monolingual and bilingual dictionary entries and corpus searches. Interannotator agreement has therefore not been addressed, but the classes were created in close relation to the English classiﬁcation by Levin (1993) (as far as the English classes have German counterparts) and agree with the German verb classiﬁcation by Schumacher (1986), as far as the relevant verbs are covered by his semantic ‘ﬁelds’. To overcome the drawback of a subjective class deﬁnition, the classiﬁcation was accompanied by a detailed class description. This characterization is closely related to Fillmore’s scenes-and-frames semantics (Fillmore 1977, 1982), as computationally utilized in FrameNet (Baker, Fillmore, and Lowe 1998; Fontenelle 2003); there is no reference to the German FrameNet version (Erk, Kowalski, and Pinkal 2003)—as one might expect—just because the German version itself had just started to be developed. The frame-semantic class deﬁnition contains a prose scene description, predominant frame participant and modiﬁcation roles, and frame variants describing the scene. The frame roles have been developed on the basis of a large German newspaper corpus from the 1990s (cf. Section 2.2). They capture the scene description with idiosyncratic participant names and demarcate major and minor roles. Since a scene might be activated by a number of frame embeddings, the predominant frame variants from the corpus are listed, marked with participating roles, and at least one example sentence for each verb utilizing the respective frame is given. The corpus examples are annotated and illustrate the idiosyncratic combinations of lexical verb meaning and conceptual constructions to capture variations in verb sense. Example 2 presents a verb class description for the class of Aspect verbs. For further class descrip- 164  Schulte im Walde  Induction of German Semantic Verb Classes  tions, the reader is referred to Schulte im Walde (2003a, pages 27–103). Verbs allowing a frame variant are marked by “+,” verbs allowing the frame variant only in company of an additional adverbial modiﬁer are marked by “+adv,” and verbs not allowing a frame variant are marked by “¬.” In the case of ambiguity, frame variants are only given for the senses of the verbs with respect to the class label. The frame variants with their roles marked represent the alternation potential of the verbs. For example, the causative– inchoative alternation assumes the syntactic embeddings nXaY and nY, indicating that the alternating verbs are realized by a transitive frame type (containing a nominative NP ‘n’ with role X and an accusative NP ‘a’ with role Y) and the corresponding intransitive frame type (with a nominative NP ‘n’ only, indicating the same role Y as for the transitive accusative). Passivization of a verb–frame combination is indicated by [P]. Appendix 6 lists all possible frame variants with illustrative examples. Note that the corpus examples are given in the old German spelling version, before the spelling reform in 1998. Semantic verb classes have been deﬁned for several languages, for example, as the earlier mentioned lexicographic resource FrameNet for English (Baker, Fillmore, and Lowe 1998; Fontenelle 2003) and German (Erk, Kowalski, and Pinkal 2003); the lexical semantic ontology WordNet for English (Miller et al. 1990; Fellbaum 1998); EuroWordNet (Vossen 2004) for Dutch, Italian, Spanish, French, German, Czech, and Estonian, and further languages as listed in WordNets in the World (Global WordNet Association, www.globalwordnet.org); syntax–semantics based verb classes for English (Levin 1993), Spanish (Va´zquez et al. 2000), and French (Saint-Dizier 1998). Example 2 Aspect Verbs: anfangen, aufho¨ren, beenden, beginnen, enden Scene: [E An event] begins or ends, either internally caused or externally caused by [I an initiator]. The event may be speciﬁed with respect to [T tense], [L location], [X an experiencer], or [R a result]. Frame Roles: I(nitiator), E(vent) Modiﬁcation Roles: T(emporal), L(ocal), (e)X(periencer), R(esult)  Frame nE  Participating Verbs and Corpus Examples  + anfangen, aufho¨ ren, beginnen / +adv enden / ¬ beenden Nun aber muß [E der Dialog] anfangen. Now though must the dialog begin  Erst muß [E das Morden] aufho¨ ren. First must the killing stop  [E Der Gottesdienst] beginnt.  The service  begins  [E Das Schuljahr] beginnt [T im Februar]. The school year begins in February  [X Fu¨ r die Flu¨ chtlinge] beginnt nun [E ein Wettlauf gegen die Zeit].  For the fugitives  begins now a race against time  [E Die Ferien] enden [R mit einem großen Fest]. The vacations end with a big party  [E Druckkunst]  ... endet [R beim guten Buch].  The art of typesetting ... ends with a good book  [E Der Informationstag] ... endet [T um 14 Uhr]. The information day ... ﬁnishes at 2pm  165  Computational Linguistics  Volume 32, Number 2  nI nI aE nI aE [P] nI iE nI pE : mit nI pE : mit [P]  + anfangen, aufho¨ ren / ¬ beenden, beginnen, enden  ... daß [I er] [T pu¨ nktlich] anﬁng.  ... that he in time  begins  Jetzt ko¨ nnen [I wir] nicht einfach aufho¨ ren. Now can we not just stop  Vielleicht sollte [I ich] aufho¨ ren und noch studieren. Maybe should I stop and yet study  + anfangen, beenden, beginnen / ¬ aufho¨ ren, enden Nachdem [I wir] [E die Sache] angefangen haben, After we the thing have started  [I Die Polizei] beendete [E die Gewaltta¨tigkeiten]. The police stopped the violence  [T Nach dem Abi] beginnt [I Jens] [L in Frankfurt] [E seine Lehre] ... After the Abitur begins Jens in Frankfurt his apprenticeship ...  + anfangen, beenden, beginnen / ¬ aufho¨ ren, enden  Wenn [E die Arbeiten] [T vor dem Bescheid] angefangen werden ...  If the work  before the notiﬁcation is started  ...  Wa¨hrend [X fu¨ r Senna] [E das Rennen] beendet war ...  While for Senna the race  was ﬁnished ...  ... ehe [E eine milita¨rische Aktion] begonnen wird ...  ... before a military action  is begun  + anfangen, aufho¨ ren, beginnen / ¬ beenden, enden  [I Ich] habe angefangen, [E Hemden zu schneidern].  I have started  shirts to make  ... daß [I der Alkoholiker] aufho¨ rt [E zu trinken].  ... that the alcoholic  stops to drink  In dieser Stimmung begannen [I Ma¨nner] [E Tango zu tanzen] ...  In this mood  began men  tango to dance  + anfangen, aufho¨ ren, beginnen / ¬ beenden, enden Erst als [I der versammelte Hofstaat] [E mit Klatschen] anﬁng, Only when the gathered royal household with applause began  [I Der Athlet] ... kann ... [E mit seinem Sport] aufho¨ ren.  The athlete ... can ... with his sports  stop  [I Man] beginne [E mit eher katharsischen Werken]. One starts with rather catharsic works  +anfangen, aufho¨ ren, beginnen / ¬ beenden, enden Und [E mit den Umbauarbeiten] ko¨ nnte angefangen werden. And with the reconstruction work could be begun  [E Mit diesem ungerechten Krieg] muß sofort  aufgeho¨ rt werden.  With this unjust war  must immediately be stopped  [T Vorher] du¨ rfe [E mit der Auﬂo¨ sung] nicht begonnen werden.  Before must with the closing  not be started  2.2 Empirical Distributions for German Verbs We developed, implemented, and trained a statistical grammar model for German that is based on the framework of head-lexicalized, probabilistic, context-free grammars. The idea originates from Charniak (1997), with this work using an implementation by Schmid (2000) for a training corpus of 35 million words from a collection of large German newspaper corpora from the 1990s, including Frankfurter Rundschau, Stuttgarter Zeitung, VDI-Nachrichten, die tageszeitung, German Law Corpus, Donaukurier, and Computerzeitung. The statistical grammar model provides empirical lexical information, specializing in but not restricted to the subcategorization behavior of verbs. Details of  166  Schulte im Walde  Induction of German Semantic Verb Classes  the implementation, training, and exploitation of the grammar model can be found in Schulte im Walde (2003a, chapter 3). The German verbs are represented by distributional vectors, with features and feature values in the distribution being acquired from the statistical grammar. The distributional description is based on the hypothesis that “each language can be described in terms of a distributional structure, that is, in terms of the occurrence of parts relative to other parts” (cf. Harris 1968). The verbs are described distributionally on three levels at the syntax–semantics interface, each level reﬁning the previous level. The ﬁrst level D1 encodes a purely syntactic deﬁnition of verb subcategorization, the second level D2 encodes a syntactico-semantic deﬁnition of subcategorization with prepositional preferences, and the third level D3 encodes a syntactico-semantic deﬁnition of subcategorization with prepositional and selectional preferences. Thus, the reﬁnement of verb features starts with a purely syntactic deﬁnition and incrementally adds semantic information. The most elaborated description comes close to a deﬁnition of verb alternation behavior. We decided on this three-step procedure of verb descriptions because the resulting clusters and particularly the changes in clusters that result from a change of features should provide insight into the meaning–behavior relationship at the syntax– semantics interface. For D1, the statistical grammar model provides frequency distributions for German verbs over 38 purely syntactic subcategorization frames (cf. Appendix 6). Based on these frequencies, we can also calculate the probabilities. For D2, the grammar provides frequencies for the different kinds of prepositional phrases within a frame type; probabilities are computed by distributing the joint probability of a verb and a PP frame over the prepositional phrases according to their frequencies in the corpus. Prepositional phrases are referred to by case and preposition, such as mitDat, fu¨ rAcc. The statistical grammar model does not learn the distinction between PP arguments and PP adjuncts perfectly. Therefore, we did not restrict the PP features to PP arguments, but to 30 PPs according to ‘reasonable’ appearance in the corpus, as deﬁned by the 30 most frequent PPs that appear with at least 10 different verbs. The subcategorization frame information for D1 and D2 has been evaluated: Schulte im Walde (2002b) describes the induction of a subcategorization lexicon from the grammar model for a total of 14,229 verbs with a frequency between 1 and 255,676 in the training corpus, and Schulte im Walde (2002a) performs an evaluation of the subcategorization data against manually created dictionary entries and shows that the lexical entries have potential for adding to and improving manual verb deﬁnitions. For the reﬁnement of D3, the grammar provides selectional preference information at a ﬁne-grained level: It speciﬁes the possible argument realizations in the form of lexical heads, with reference to a speciﬁc verb–frame–slot combination. Obviously, we would run into a sparse data problem if we tried to incorporate selectional preferences into the verb descriptions at such a speciﬁc level. We are provided with detailed information at the nominal level, but we need a generalization of the selectional preference deﬁnition. A widely used resource for selectional preference information is the semantic ontology WordNet (Miller et al. 1990; Fellbaum 1998); the University of Tu¨ bingen has developed the German version of WordNet, GermaNet (Hamp and Feldweg 1997; Kunze 2000). The hierarchy is realized by means of synsets, sets of synonymous nouns, which are organized by multiple inheritance hyponym/hypernym relationships. A noun can appear in several synsets, according to its number of senses. The German noun hierarchy in GermaNet is utilized for the generalization of selectional preferences: For each noun in a verb–frame–slot combination, the joint frequency is divided over the different senses of the noun and propagated up the hierarchy. In case of multiple hypernym 167  Computational Linguistics  Volume 32, Number 2  synsets, the frequency is divided again. The sum of frequencies over all top synsets equals the total joint frequency. Repeating the frequency assignment and propagation for all nouns appearing in a verb–frame–slot combination, the result deﬁnes a frequency distribution of the verb–frame–slot combination over all GermaNet synsets. To restrict the variety of noun concepts to a general level, only the frequency distributions over the top GermaNet nodes1 are considered: Lebewesen ‘creature’, Sache ‘thing’, Besitz ‘property’, Substanz ‘substance’, Nahrung ‘food’, Mittel ‘means’, Situation ‘situation’, Zustand ‘state’, Struktur ‘structure’, Physis ‘body’, Zeit ‘time’, Ort ‘space’, Attribut ‘attribute’, Kognitives Objekt ‘cognitive object’, Kognitiver Prozess ‘cognitive process’. Since the 15 nodes are mutually exclusive and the node frequencies sum to the total joint verb-frame frequency, we can use their frequencies to deﬁne a probability distribution. Are selectional preferences equally necessary and informative for all frame types? For example, selectional preferences for the direct object are expected to vary strongly with respect to the subcategorizing verb (because the direct object is a highly frequent argument type across all verbs and verb classes), but selectional preferences for a subject in a transitive construction with a nonﬁnite clause are certainly less interesting for reﬁnement (because this frame type is more restricted with respect to the verbs it is subcategorized for). We empirically investigated which of the overall frame roles may be realized by different selectional preferences and are therefore relevant and informative for a selectional preference distinction. As a result, in parts of the clustering experiments we will concentrate on a speciﬁc choice of frame-slot combinations to be reﬁned by selectional preferences (with the relevant slots underlined): ‘n’, ‘na’, ‘nd’, ‘nad’, ‘ns-dass.’ Table 1 presents three verbs from different classes and their 10 most frequent frame types at the three levels of verb deﬁnition and their probabilities. D1 for beginnen ‘begin’ deﬁnes ‘np’ and ‘n’ as the most probable frame types. After splitting the ‘np’ probability over the different PP types in D2, a number of prominent PPs are left, the time indicating umAcc and nachDat, mitDat referring to the begun event, anDat as date, and inDat as place indicator. It is obvious that not all PPs are argument PPs, but adjunct PPs also represent a part of the verb behavior. D3 illustrates that typical selectional preferences for beginner roles are Situation ‘situation’, Zustand ‘state’, Zeit ‘time’, Sache ‘thing’. D3 has the potential to indicate verb alternation behavior, for example, ‘na(Situation)’ refers to the same role for the direct object in a transitive frame as “n(Situation)” in an intransitive frame. essen ‘eat’ as an object-drop verb shows strong preferences for both intransitive and transitive usage. As desired, the argument roles are dominated by Lebewesen ‘creature’ for ‘n’ and ‘na’ and Nahrung ‘food’ for ‘na’. fahren ‘drive’ chooses typical manner of motion frames (‘n,’ ‘np,’ ‘na’) with the reﬁning PPs being directional (inAcc, zuDat, nachDat) or referring to a means of motion (mitDat, inDat, aufDat). The selectional preferences show correct alternation behavior: Lebewesen ‘creature’ in the object drop case for ‘n’ and ‘na,’ Sache ‘thing’ in the inchoative/causative case for ‘n’ and ‘na’. In addition to the absolute verb descriptions above, a simple smoothing technique is applied to the feature values. The goal of smoothing is to create more uniform distributions, especially with regard to adjusting zero values, but also for assimilating high and low frequencies and probabilities. The smoothed distributions are particularly interesting for distributions with a large number of features, since they typically contain  
∗ Computing Science Department, King’s College, University of Aberdeen, United Kingdom, E-mail: kvdeemter@csd.abdn.ac.uk. 
Choosing the wrong word in a machine translation or natural language generation system can convey unwanted connotations, implications, or attitudes. The choice between near-synonyms such as error, mistake, slip, and blunder—words that share the same core meaning, but differ in their nuances—can be made only if knowledge about their differences is available. We present a method to automatically acquire a new type of lexical resource: a knowledge base of near-synonym differences. We develop an unsupervised decision-list algorithm that learns extraction patterns from a special dictionary of synonym differences. The patterns are then used to extract knowledge from the text of the dictionary. The initial knowledge base is later enriched with information from other machine-readable dictionaries. Information about the collocational behavior of the near-synonyms is acquired from free text. The knowledge base is used by Xenon, a natural language generation system that shows how the new lexical resource can be used to choose the best near-synonym in speciﬁc situations. 1. Near-Synonyms Near-synonyms are words that are almost synonyms, but not quite. They are not fully intersubstitutable, but vary in their shades of denotation or connotation, or in the components of meaning they emphasize; they may also vary in grammatical or collocational constraints. For example, the word foe emphasizes active warfare more than enemy does (Gove 1984); the distinction between forest and woods is a complex combination of size, proximity to civilization, and wildness (as determined by the type of animals and plants therein) (Room 1981); among the differences between task and job is their collocational behavior with the word daunting: daunting task is a better collocation than daunting job. More examples are given in Table 1 (Hirst 1995). There are very few absolute synonyms, if they exist at all. So-called dictionaries of synonyms actually contain near-synonyms. This is made clear by dictionaries such as Webster’s New Dictionary of Synonyms (Gove 1984) and Choose the Right Word (hereafter CTRW) (Hayakawa 1994), which list clusters of similar words and explicate the differences between the words in each cluster. An excerpt from CTRW is presented in Figure 1. These dictionaries are in effect dictionaries of near-synonym discrimination.  ∗ School of Information Technology and Engineering, Ottawa, ON, Canada, K1N 6N5; diana@site.uottawa.ca. † Department of Computer Science, Toronto, ON, Canada, M5S 3G4; gh@cs.toronto.edu. Submission received: 5 October 2004; revised submission received: 15 June 2005; accepted for publication: 4 November 2005. © 2006 Association for Computational Linguistics  Computational Linguistics  Volume 32, Number 2  Table 1 Examples of near-synonym variations.  Type of variation  Example  Stylistic, formality Stylistic, force Expressed attitude Emotive Continuousness Emphasis on different aspects of meaning Fuzzy boundary Collocational  pissed : drunk : inebriated ruin : annihilate skinny : thin : slim daddy : dad : father seep : drip enemy : foe woods : forest task : job (in the context of daunting)  Writers often turn to such resources when confronted with a choice between nearsynonyms, because choosing the wrong word can be imprecise or awkward or convey unwanted implications. These dictionaries are made for human use, and they are available only on paper, not in electronic format. Understanding the differences between near-synonyms is important for ﬁnegrained distinctions in machine translation. For example, when translating the French word erreur to English, one of the near-synonyms mistake, blooper, blunder, boner, contretemps, error, faux pas, goof, slip, solecism could be chosen, depending on the context and on the nuances that need to be conveyed. More generally, knowledge of near-synonyms is vital in natural language generation systems that take a nonlinguistic input (semantic representation) and generate text. When more than one word can be used, the choice should be based on some explicit preferences. Another application is an intelligent thesaurus, which would assist writers not only with lists of possible synonyms but also with the nuances they carry (Edmonds 1999).  1.1 Distinctions among Near-Synonyms Near-synonyms can vary in many ways. DiMarco, Hirst, and Stede (1993) analyzed the types of differences adduced in dictionaries of near-synonym discrimination. They found that there was no principled limitation on the types, but a small number of types occurred frequently. A detailed analysis of the types of variation is given by Edmonds (1999). Some of the most relevant types of distinctions, with examples from CTRW, are presented below. Denotational distinctions Near-synonyms can differ in the frequency with which they express a component of their meaning (e.g., Occasionally, invasion suggests a largescale but unplanned incursion), in the latency (or indirectness) of the expression of the component (e.g., Test strongly implies an actual application of these means), and in ﬁnegrained variations of the idea itself (e.g., Paternalistic may suggest either benevolent rule or a style of government determined to keep the governed helpless and dependent). The frequency is signaled in the explanations in CTRW by words such as always, usually, sometimes, seldom, never. The latency is signaled by many words, including the obvious words suggests, denotes, implies, and connotes. The strength of a distinction is signaled by words such as strongly and weakly. Attitudinal distinctions Near-synonyms can convey different attitudes of the speaker toward an entity in the situation. Attitudes can be pejorative, neutral, or favorable. Examples of sentences in CTRW expressing attitudes, in addition to denotational distinctions, are these: Blurb is also used pejoratively to denote the extravagant and insincere  224  Inkpen and Hirst  A Lexical Knowledge Base of Near-Synonym Differences  Figure 1 An excerpt from Choose the Right Word (CTRW) by S. I. Hayakawa. Copyright ©1987. Reprinted by arrangement with HarperCollins Publishers, Inc. praise common in such writing. Placid may have an unfavorable connotation in suggesting an unimaginative, bovine dullness of personality. Stylistic distinctions Stylistic variations of near-synonyms concern their level of formality, concreteness, force, ﬂoridity, and familiarity (Hovy 1990). Only the ﬁrst three of these occur in CTRW. A sentence in CTRW expressing stylistic distinctions is this: Assistant and helper are nearly identical except for the latter’s greater informality. Words that signal the degree of formality include formal, informal, formality, and slang. The degree of concreteness is signaled by words such as abstract, concrete, and concretely. Force can be signaled by words such as emphatic and intensiﬁcation. 1.1.1 The Class Hierarchy of Distinctions. Following the analysis of the distinctions among near-synonyms of Edmonds and Hirst (2002), we derived the class hierarchy of 225  Computational Linguistics  Volume 32, Number 2  distinctions presented in Figure 2. The top-level class DISTINCTIONS consists of DENOTATIONAL DISTINCTIONS, ATTITUDE, and STYLE. The last two are grouped together in a class ATTITUDE-STYLE DISTINCTIONS because they are expressed by similar syntactic constructions in the text of CTRW. Therefore the algorithm to be described in Section 2.2 will treat them together. The leaf classes of DENOTATIONAL DISTINCTIONS are SUGGESTION, IMPLICATION, and DENOTATION; those of ATTITUDE are FAVORABLE, NEUTRAL, and PEJORATIVE; those of STYLE are FORMALITY, CONCRETENESS, and FORCE. All these leaf nodes have the attribute STRENGTH, which takes the values low, medium, and high. All the leaf nodes except those in the class STYLE have the attribute FREQUENCY, which takes the values always, usually, sometimes, seldom, and never. The DENOTATIONAL DISTINCTIONS have an additional attribute: the peripheral concept that is suggested, implied, or denoted. 1.2 The Clustered Model of Lexical Knowledge Hirst (1995) and Edmonds and Hirst (2002) show that current models of lexical knowledge used in computational systems cannot account well for the properties of near-synonyms. The conventional view is that the denotation of a lexical item is represented as a concept or a structure of concepts (i.e., a word sense is linked to the concept it lexicalizes), which are themselves organized into an ontology. The ontology is often language independent, or at least language neutral, so that it can be used in multilingual applications. Words that are nearly synonymous have to be linked to their own slightly different concepts. Hirst (1995) showed that such a model entails an awkward taxonomic proliferation of language-speciﬁc concepts at the fringes, thereby defeating the purpose of a language-independent ontology. Because this model deﬁnes words  Figure 2 The class hierarchy of distinctions: rectangles represent classes, ovals represent attributes that a class and its descendants have. 226  Inkpen and Hirst  A Lexical Knowledge Base of Near-Synonym Differences  in terms of necessary and sufﬁcient truth conditions, it cannot account for indirect expressions of meaning or for fuzzy differences between near-synonyms. Edmonds and Hirst (2002) modiﬁed this model to account for near-synonymy. The meaning of each word arises out of a context-dependent combination of a contextindependent denotation and a set of explicit differences from its near-synonyms, much as in dictionaries of near-synonyms. Thus the meaning of a word consists both of necessary and sufﬁcient conditions that allow the word to be selected by a lexical choice process and a set of nuances of indirect meaning that may be conveyed with different strengths. In this model, a conventional ontology is cut off at a coarse grain and the near-synonyms are clustered under a shared concept, rather than linking each word to a separate concept. The result is a clustered model of lexical knowledge. Thus, each cluster has a core denotation that represents the essential shared denotational meaning of its near-synonyms. The internal structure of a cluster is complex, representing semantic (or denotational), stylistic, and expressive (or attitudinal) differences between the nearsynonyms. The differences or lexical nuances are expressed by means of peripheral concepts (for denotational nuances) or attributes (for nuances of style and attitude). The clustered model has the advantage that it keeps the ontology language neutral by representing language-speciﬁc distinctions inside the cluster of near-synonyms. The near-synonyms of a core denotation in each language do not need to be in separate clusters; they can be part of one larger cross-linguistic cluster. However, building such representations by hand is difﬁcult and time-consuming, and Edmonds and Hirst (2002) completed only nine of them. Our goal in the present work is to build a knowledge base of these representations automatically by extracting the content of all the entries in a dictionary of near-synonym discrimination. Unlike lexical resources such as WordNet (Miller 1995), in which the words in synsets are considered “absolute” synonyms, ignoring any differences between them, and thesauri such as Roget’s (Roget 1852) and Macquarie (Bernard 1987), which contain hierarchical groups of similar words, the knowledge base will include, in addition to the words that are near-synonyms, explicit explanations of differences between these words.  2. Building a Lexical Knowledge Base of Near-Synonym Differences As we saw in Section 1, each entry in a dictionary of near-synonym discrimination lists a set of near-synonyms and describes the differences among them. We will use the term cluster in a broad sense to denote both the near-synonyms from an entry and their differences. Our aim is not only to automatically extract knowledge from one such dictionary in order to create a lexical knowledge base of near-synonyms (LKB of NS), but also to develop a general method that could be applied to any such dictionary with minimal adaptation. We rely on the hypothesis that the language of the entries contains enough regularity to allow automatic extraction of knowledge from them. Earlier versions of our method were described by Inkpen and Hirst (2001). The task can be divided into two phases, treated by two consecutive modules, as shown in Figure 3. The ﬁrst module, the extraction module, will be described in this section. The generic clusters produced by this module contain the concepts that nearsynonyms may involve (the peripheral concepts) as simple strings. This generic LKB of NS can be adapted for use in any Natural Language Processing (NLP) application. The second module customizes the LKB of NS so that it satisﬁes the requirements of the particular system that is to employ it. This customization module transforms the 227  Computational Linguistics  Volume 32, Number 2  Figure 3 The two modules of the task. strings from the generic clusters into concepts in the particular ontology. An example of a customization module will be described in Section 6. The dictionary that we use is Choose the Right Word (Hayakawa 1994) (CTRW),1 which was introduced in Section 1. CTRW contains 909 clusters, which group 5,452 near-synonyms (more precisely, near-synonym senses, because a word can be in more than one cluster) with a total of 14,138 sentences (excluding examples of usage), from which we derive the lexical knowledge base. An example of the results of this phase, corresponding to the second, third, and fourth sentence for the absorb cluster in Figure 1, is presented in Figure 4. This section describes the extraction module, whose architecture is presented in Figure 5. It has two main parts. First, it learns extraction patterns; then it applies the patterns to extract differences between near-synonyms. 2.1 Preprocessing the Dictionary After OCR scanning of CTRW and error correction, we used XML markup to segment the text of the dictionary into cluster name, cluster identiﬁer, members (the nearsynonyms in the cluster), entry (the textual description of the meaning of the nearsynonyms and of the differences among them), cluster’s part of speech, cross-references to other clusters, and antonyms list. Sentence boundaries were detected by using general heuristics, plus heuristics speciﬁc for this particular dictionary; for example, examples appear in square brackets and after a colon. 2.2 The Decision-List Learning Algorithm Before the system can extract differences between near-synonyms, it needs to learn extraction patterns. For each leaf class in the hierarchy (Figure 2) the goal is to learn a set of words and expressions from CTRW—that is, extraction patterns—that characterizes descriptions of the class. Then, during the extraction phase, for each sentence (or fragment of a sentence) in CTRW the program will decide which leaf class is expressed, with what strength, and what frequency. We use a decision-list algorithm to learn sets of words and extraction patterns for the classes DENOTATIONAL DISTINCTIONS and ATTITUDE-STYLE DISTINCTIONS. These are split further for each leaf class, as explained in Section 2.3. The algorithm we implemented is inspired by the work of Yarowsky (1995) on word sense disambiguation. He classiﬁed the senses of a word on the basis of other words that the given word co-occurs with. Collins and Singer (1999) classiﬁed proper names 
There has been a great deal of interest over the past 20 years in developing metrics and frameworks for evaluating and comparing the performance of spoken-language dialogue systems. One of the results of this interest is a potential general methodology, known as the PARADISE framework. This squib highlights some important issues concerning the application of PARADISE that have, up to now, not been sufﬁciently emphasized or have even been neglected by the dialogue-system community. These include considerations regarding the selection of appropriate regression parameters, normalization effects on the accuracy of the prediction, the inﬂuence of speech-recognition errors on the performance function, and the selection of an appropriate user-satisfaction measure. In addition, it gives the results of an evaluation of data from two Wizard-of-Oz experiments. These evaluations include different dependent variables and examination of individual user-satisfaction measures. 1. Introduction A long list of objective dialogue metrics (Danieli and Gerbino 1995; Smith and Gordon 1997) for dialogue evaluation, which can be calculated without recourse to human judgment, and subjective dialogue metrics (Shriberg, Wade, and Price 1992; Danieli and Gerbino 1995), which are based on human judgments, have been proposed. Their wellknown limitations led Walker, Litman, Kamm, and Abella (1997) to propose their paradigm for dialogue system evaluation (PARADISE), a potentially general methodology for evaluating spoken-language dialogue systems, the goal of which was to compare and optimize different dialogue managers and task domains independently. The PARADISE framework maintains that the system’s primary objective is to maximize user satisfaction (Shriberg, Wade, and Price 1992), and it derives a combined performance metric for a dialogue system as a weighted linear combination of tasksuccess measures and dialogue costs. The dialogue costs are of two types: dialogueefﬁciency costs (e.g., number of utterances, dialogue time), which are measures of the system’s efﬁciency in helping the user to complete the task, and dialogue-quality costs (e.g., system-response delay, mean recognition score), which are intended to capture other aspects that can have large effects on the user’s perception of the system’s performance. Applying PARADISE to dialogue data requires dialogue corpora to be collected via controlled experiments during which users subjectively rate their satisfaction. Here, user satisfaction is calculated with a survey (Walker et al. 1998) that asks users to specify the degree to which they agree with several statements about the performance ∗ Faculty of Electrical Engineering, Trzˇasˇka 25, 1000 Ljubljana, Slovenia © 2006 Association for Computational Linguistics  Computational Linguistics  Volume 32, Number 2  of the system. In addition, the other parameters of the model of performance, i.e., the task-success measures and the dialogue costs, must be either automatically logged by the system or be hand-labeled. The PARADISE model of performance posits that a performance function can then be derived by applying multivariate linear regression (MLR) with user satisfaction as the dependent variable and task-success measures and dialogue costs as the independent variables. The squib addresses some PARADISE issues (with most of them arising from the application of MLR) that have, up to now, not been sufﬁciently emphasized or have even been neglected by the dialogue-system community. Moreover, most of the considerations about these issues are supported by the results of applying PARADISE to the data from two Wizard-of-Oz (WOZ) experiments (Hajdinjak and Mihelicˇ 2004a) carried out during the development of a weather-information-providing, naturallanguage spoken dialogue system (Zˇ ibert et al. 2003). In contrast to previous PARADISE applications, we evaluated data that were collected in the early stages of a dialogue system’s design where speech understanding was simulated by a human.  2. PARADISE-Framework Issues  The PARADISE model of performance (Walker et al. 1998) is deﬁned as follows:  n  Performance = (α ∗ N (κ)) − wi ∗ N (ci)  (1)  i=1  Here, α is the weight on the kappa coefﬁcient κ, which is calculated from a confusion matrix that summarizes how well the dialogue system achieves the information require- ments of particular tasks within the dialogue and measures task success, wi are weights on the costs ci, and N is a Z-score normalization function:  N (x)  =  x − x0 σx0  (2)  where x0 and σx0 are the mean value and the standard deviation for x, respectively, computed from the sample set of observations. Normalization guarantees that the weights directly indicate the relative contributions to the performance function, which can be used to predict user satisfaction, i.e., the value of the dependent variable.  2.1 Relationships between MLR Variables  Nevertheless, MLR sets some constraints on the relationships between the parameters (Seber 1977). Most obviously, there must exist at least an approximately linear relationship between the dependent and the independent variables. On the other hand, it can be shown (Patel and Bruce 1995) that dropping the predictors, i.e., the independent variables, that are not statistically signiﬁcant to the dependent variable (i.e., the p value is greater than 0.05) can reduce the average error of the predictions. Furthermore, the predictors must not correlate highly (i.e., the absolute values of the correlation coefﬁcients must not be greater than 0.7). If they do, small errors or variations in the values of sample observations can have a large effect on the weights in the performance function (1). Therefore, the redundant predictors must be removed before applying MLR. Obviously, it is reasonable to remove those predictors that are  264  Hajdinjak and Mihelicˇ  The PARADISE Evaluation Framework  less statistically signiﬁcant (i.e., with greater p values) to the dependent variable. Note, high correlations may exist even between variables that seem unrelated (Section 3.2). 2.2 Normalization Effects MLR is based on the least squares method, where the model minimizes the sum of the squares of the differences between the observed and the predicted values. Hence,  N (US) = N (US) +  (3)  where N (US) is the normalized observed user-satisfaction value, N (US) is the predicted normalized user-satisfaction value, and is the error of the prediction. Further,  US = N (US)σUS0 + US0 + σUS0 = US + σUS0  (4)  is the MLR model for the unnormalized user-satisfaction values, where US0 and σUS0 are the mean value and the standard deviation of the sample set of observed user- satisfaction values, respectively. Note, the initial noise variable is multiplied by σUS0 . There are several different measures of goodness of ﬁt for a regression model, but the most widely used is the coefﬁcient of determination R2. In a regression model with normalized variables, R2 turns out to be the variance of the predicted variable:  R2 =  n i=1  (N  (USi  )  −  N  (US))2  n i=1  (N  (USi  )  −  N  (US))2  =  n i=1  N  (USi  2 )  n  =  var(N (US))  (5)  where USi and N (USi) are the ith component of the vector US of the observed values and the ith component of the vector N (US) of the predicted normalized values, respectively. One noteworthy consequence of the equality (5) is that the absolute values of the weights in the performance function are not greater than 1. Nevertheless, the accuracy of the prediction US for US is indicated by the q ratio:  q(US, US)  =  |US − US| |US|  (6)  The deﬁnitions (2) and (6) and the equality (4) lead to the equality  q(N (US), N (US))  =  |N (US)−N (US)| |N (US)|  =  |US−US0−N (US)σUS0 | |US−US0|  =  |US|  (7)  q(US, US)  |US−US| |US|  |US−N (US)σUS0 −US0| |US|  |US − US0|  which  shows  that  as  soon  as  US  >  US0 2  the  prediction  for  the  normalized  values  is  (usually by a large margin) not as good as the prediction for the unnormalized values.  Therefore, after predicting the normalized user-satisfaction values, these values should  be transformed back to the original scale to guarantee more accurate predictions.  In previous work (Walker et al. 1997, 1998; Walker, Kamm, and Litman 2000; Litman  and Shimei 2002), not only was there no attention paid to these details, it was not  265  Computational Linguistics  Volume 32, Number 2  mentioned that the observed user-satisfaction values need to be normalized as well if one wants an acceptable error in the prediction.  2.3 Choosing the Best Set of Predictors  A major problem in regression analysis is that of deciding which predictors should be in the model. There are two conﬂicting criteria. First, the model chosen should include as many predictors as possible if reliable predictions are to be obtained (also, R2 increases with the number of predictors). Second, because of the costs involved in determining a large number of predictors and the beneﬁt of focusing only on the most signiﬁcant predictors, we would like the equation to include as few predictors as possible. An appropriate method of choosing the best set of predictors is backward elimination (Seber 1977). Here, at each step, a single predictor is eliminated from the current regression model if its removal would increase the sum-of-squares differences between the observed and the predicted values,  n  n  RSS = (N (USi) − N (USi))2 =  2 i  (8)  i=1  i=1  by not more than Fout (some properly chosen constant, typically between 2 and 4)  times  the  residual  mean  square  RSS n−p  ,  where  n  is  the  number  of  observations  and  p  is  the number of predictors. That is, the predictor giving the smallest increase in RSS  is chosen.  2.4 Why Model the Sum of User-Satisfaction Scores Hone and Graham (2000) argued that the items chosen for the user-satisfaction survey (Table 1) introduced within the PARADISE framework were based neither on theory nor on well-conducted empirical research. Moreover, they said that the way that the collected data was used would be inappropriate, i.e., the approach of summing all the scores could only be justiﬁed on the basis of evidence that all of the items are measuring the same construct, otherwise the overall score would be meaningless.  Table 1 The user-satisfaction survey used within the PARADISE framework. 1. Was the system easy to understand? (TTS Performance) 2. Did the system understand what you said? (ASR Performance) 3. Was it easy to ﬁnd the message you wanted? (Task Ease) 4. Was the pace of interaction with the system appropriate? (Interaction Pace) 5. Did you know what you could say at each point of the dialogue? (User Expertise) 6. How often was the system sluggish and slow to reply to you? (System Response) 7. Did the system work the way you expected it? (Expected Behavior) 8. From your current experience with using the system, do you think you’d use the system regularly when you are away from your desk? (Future Use) 266  Hajdinjak and Mihelicˇ  The PARADISE Evaluation Framework  If, in spite of Hone and Graham’s remarks, one wants to evaluate user satisfaction with the survey given in Table 1, the question that arises is whether the target to be predicted should really be the sum of all the user-satisfaction scores. However, our experiments (Section 2.3) showed a remarkable, but expected, difference in the signiﬁcance of the predictors when taking different satisfaction-measure sums or even individual scores as the target to be predicted. Moreover, some individual usersatisfaction measures could not be well modeled. Consequently, we think that it would be more appropriate to take the sum of the user-satisfaction scores that are likely to measure the selected aspect (e.g., dialogue-manager performance) of the system’s performance. 2.5 Speech-recognition Effects It has often been reported (Walker et al. 1998; Kamm, Walker, and Litman 1999; Walker, Kamm, and Litman 2000; Litman and Shimei 2002) that the mean concept accuracy, often referred to as the mean recognition score, is the exceptional predictor of a dialogue system’s performance. Moreover, it has been shown that as recognizer performance improves the signiﬁcance of the predictors can change (Walker, Borland, and Kamm 1999). Thus, we would go so far as to claim that the inﬂuence of automatic speech recognition hinders the other predictors from showing signiﬁcance when evaluating the performance of the dialogue-manager component, excluding the efﬁciency of its clariﬁcation strategies. Only if the users are disencumbered from speech-recognition errors are they able to reliably assess the mutual inﬂuence of the observable less signiﬁcant contributors to their satisfaction with the dialogue manager’s performance. Therefore, in our WOZ experiments (Section 2), which were carried out in order to evaluate the performance of the dialogue manager, speech understanding (i.e., speech recognition and natural-language understanding) was performed by a human. As expected, κ with mean values near 1 (0.94 and 0.98, respectively), which was the only predictor reﬂecting speech-recognition performance (Section 2.1), did not show the usual degree of signiﬁcance in predicting the performance of our WOZ systems (Section 2.3).  3. PARADISE Framework Application With the intention of involving the user in all the stages of the design of the spoken natural language, weather-information-providing dialogue system (Zˇ ibert et al. 2003), WOZ data were collected and evaluated even before the completion of all the system’s components. The aim of the ﬁrst two WOZ experiments (Hajdinjak and Mihelicˇ 2004a) was to evaluate the performance of the dialogue-manager component (Hajdinjak and Mihelicˇ 2004b). Therefore, while the task of the wizard in the ﬁrst WOZ experiment was to perform speech understanding and dialogue management, the task in the second WOZ experiment was to perform only speech understanding, and the dialogue-management task was assigned to the newly implemented dialogue manager. There were 76 and 68 users involved in the ﬁrst and the second WOZ experiment, respectively. The users were given two tasks: The ﬁrst task was to obtain a particular piece of weather-forecast information, and the second task was a given situation, the 267  Computational Linguistics  Volume 32, Number 2  aim of which was to stimulate them to ask context-speciﬁc questions. In addition, the users were given the freedom to ask extra questions. User satisfaction was then evaluated with the user-satisfaction survey given in Table 1, and a comprehensive user satisfaction (US) was computed by summing each question’s score and thus ranged in value from a low of 8 to a high of 40.  3.1 Selection of Regression Parameters The selection of regression parameters is crucial for the quality of the performance equation, and it is usually the result of thorough considerations made during several successive regression analyses. However, following the recommended Cohen’s method (Di Eugenio and Glass 2004), we ﬁrst computed the task-success measure Kappa coefﬁcient (κ), reﬂecting the wizard’s typing errors and unauthorized corrections, and the dialogue-efﬁciency costs Mean elapsed time (MET), i.e., the mean elapsed time for the completion of the tasks that occurred within the interaction, and Mean user moves (MUM), i.e., the mean number of conversational moves that the user needed to either fulﬁl or abandon the initiated information-providing games. Second, the following dialogue-quality costs were selected: Task completion (Comp), i.e., the user’s perception of completing the ﬁrst task; Number of user initiatives (NUI), i.e., the number of user’s moves initiating information-providing games; Mean words per turn (MWT), i.e., the mean number of words in each of the user’s turns; Mean response time (MRT), i.e., the mean system-response time; Number of missing responses (NMR), i.e., the difference between the number of turns by the system and the number of turns by the user; Number of unsuitable requests (NUR) and unsuitable-request ratio (URR), i.e., the number and the ratio of user’s initiating moves that were out of context; Number of inappropriate responses (NIR) and inappropriate-response ratio (IRR), i.e., the number and the ratio of unexpected responses from the system, including pardon moves; Number of errors (Error), i.e., the number of system errors, e.g., interruptions of the telephone connection and unsuitable naturallanguage sentences; 268  Hajdinjak and Mihelicˇ  The PARADISE Evaluation Framework  Number of help messages (NHM) and help-message ratio (HMR), i.e., the number and the ratio of system’s help messages; Number of check moves (NCM) and check-move ratio (CMR), i.e., the number and the ratio of system’s moves checking some information regarding past dialogue events; Number of given data (NGD) and given-data ratio (GDR), i.e., the number and the ratio of system’s information-providing moves; Number of relevant data (NRD) and relevant-data ratio (RDR), i.e., the number and the ratio of system’s moves directing the user to select relevant, available data; Number of no data (NND) and no-data ratio (NDR), i.e., the number and the ratio of system’s moves stating that the requested information is not available; Number of abandoned requests (NAR) and abandoned-request ratio (ARR), i.e., the number and the ratio of the information-providing games abandoned by the user. Note that special attention was given to the parameters NGD, GDR, NRD, RDR, NND, and NDR, which have not so far been reported in the literature as costs for user satisfaction. We will refer to them as database parameters. It has, however, been argued (Walker et al. 1998) that the database size might be a relevant predictor of performance. However, the decision to introduce the database parameters as dialogue costs was based on the extremely sparse and dynamical weather-information source (Hajdinjak and Mihelicˇ 2004b) with a time-dependent data structure that we had at our disposal. 3.2 Correlations between MLR Parameters In the data from both WOZ experiments, some high correlations between the regression parameters were observed. Note that the high correlations were not found only between the newly introduced or the obviously related parameters such as NUT and NGD. In the ﬁrst experiment, not entirely evident high correlations were observed between MET and MRT (0.7), NMR and NHM (0.7), GDR and NDR (−0.8), but in the second experiment between MET and MRT (0.8), NMR and NHM (0.7), GDR and RDR (−0.7). The regression parameters GDR and NDR as well as GDR and RDR were highly correlated only in one of the WOZ experiments, and moderately in the other. 3.3 Performance-function Results We applied PARADISE to the data from both WOZ experiments (Hajdinjak and Mihelicˇ 2004a). As the target to be predicted we ﬁrst took user satisfaction (US) and afterwards the sum of those user-satisfaction values that (in our opinion) measured the dialogue manager’s performance (DM) and could, in addition, be well modeled, i.e., the sum of the user-satisfaction-survey scores assigned to ASR Performance, Task Ease, System Response, and Expected Behavior (Table 1). 269  Computational Linguistics  Volume 32, Number 2  After removing about 10% of the outliers,1 backward elimination for Fout = 2 was performed. Thus, the data from the ﬁrst WOZ experiment gave the following performance equations: N (US) = −0.69N (NND) − 0.16N (NRD) N (DM) = −0.61N (NND) − 0.16N (NRD) + 0.21N (Comp) with 58% (R2 = 0.58) and 59% of the variance explained, respectively. To be able to see the difference between these two equations, note that Comp was signiﬁcant for US (p < 0.02), but removed by backward elimination. In contrast, the data from the second WOZ experiment gave the following performance equations: N (US) = −0.30N (CMR) + 0.18N (κ) − 0.23N (MET) N (DM) = −0.35N (CMR) + 0.35N (κ) + 0.35N (GDR) − 0.17N (ARR) with 26% and 46% of the variance explained, respectively. Note, MET was signiﬁcant for DM (p < 0.02) and that GDR and ARR were signiﬁcant for US (p < 0.04), but they were all removed by backward elimination. Moreover, MET was trivially correlated with GDR and ARR (i.e., the correlation coefﬁcients were lower than 0.1). Let us compare both performance equations predicting DM. The ﬁrst observation we make is that none of the predictors is common to both performance equations. All the predictors from the ﬁrst performance equation (i.e., NND, NRD, Comp) were insigniﬁcant (p > 0.1) for DM in the second experiment. On the other hand, the only predictor from the second performance equation that was signiﬁcant for DM (p < 0.004) in the ﬁrst experiment, but removed by backward elimination, was GDR. Unlike the ﬁrst performance equation with the database parameters NND and NRD as crucial (negative) predictors, the second performance equation clearly shows their insignificance to users’ satisfaction. Hence it follows that the developed dialogue manager (Hajdinjak and Mihelicˇ 2004b) with its rather consistent ﬂexibility in directing the user to select relevant, available data does not (negatively) inﬂuence users’ satisfaction. Moreover, we thought that it would be very interesting to see which parameters are signiﬁcant for individual user-satisfaction measures (Table 1). However, the situation in which the Task Ease question was the only measure of user satisfaction, the aim of which was to maximize the relationship between elapsed time and user satisfaction, was considered before (Walker, Borland, and Kamm 1999). First, we discovered that Future Use could not be well modeled in the ﬁrst WOZ experiment and that User Expertise and Interaction Pace could not be well modeled in the second WOZ experiment, i.e., the corresponding MLR models explained less than 10% of the variance. Second, the parameters that most signiﬁcantly contributed to the remaining, individual user-satisfaction measures are given in Table 2. Surprisingly, the parameters that were most signiﬁcant to an individual user-satisfaction measure in the ﬁrst WOZ  
Strategies like voting for automatic sense annotations and the use of interannotator agreement with adjudication for human sense assignments only partially solve the issue of disagreement. Especially when there is no clear preference towards a certain word sense, the ﬁnal choice made by a judge can be subjective, if not arbitrary. This is a case where analyzing the intrinsic structure of the reference lexicon is essential for producing a consistent decision. A lexicographer is indeed expected to review a number of related dictionary entries in order to adjudicate a sense coherently. This work can be tedious, time-consuming, and often incomplete due to the complex structure of the resource. As a result, inconsistent choices can be made. In this article, we present Valido, a tool for supporting the validation of both manual and automatic sense annotations through the use of semantic graphs, particularly of semantic interconnection patterns (Navigli and Velardi 2005). ∗ Dipartimento di Informatica, Universita` di Roma “La Sapienza,” Via Salaria, 113 - 00198 Roma, Italia. E-mail: navigli@di.uniroma1.it. © 2006 Association for Computational Linguistics  Computational Linguistics  Volume 32, Number 2  2. Semantic Networks and Semantic Interconnection Patterns  Semantic networks are a graphical notation developed to represent knowledge expli-  citly as a set of conceptual entities and their interrelationships. The availability of wide-  coverage computational lexicons like WordNet (Fellbaum 1998), as well as semantically  annotated corpora like SemCor (Miller et al. 1993), has certainly contributed to the  exploration and exploitation of semantic graphs for several tasks like the analysis of  lexical text cohesion (Morris and Hirst 1991), word sense disambiguation (Agirre and  Rigau 1996; Mihalcea and Moldovan 2001), and ontology learning (Navigli and Velardi  2004), etc.  Recently, a knowledge-based algorithm for word sense disambiguation called  structural semantic interconnections (SSI, http://lcl.di.uniroma1.it/ssi) (Navigli and  Velardi 2004, 2005), has been shown to provide interesting insights into the choice of  word senses by providing structural justiﬁcations in terms of semantic graphs. Given  a word context and a lexical knowledge base (LKB), obtained by integrating WordNet  with annotated corpora and collocation resources (Navigli 2005), SSI selects a semantic  graph including those word senses having a higher degree of interconnection, according  to a measure of connectivity.  A semantic interconnection pattern is a relevant sequence of edges selected ac-  cording to a context-free grammar, i.e., a path connecting a pair of word senses  (dark nodes in Figure 1), possibly including a number of intermediate concepts (light  nodes in Figure 1). For example, if the context of words to be disambiguated is [cross-v,  street-n, intersection-n], the senses chosen by SSI with respect to WordNet are [cross-v#1,  street#2, intersection#2],1 supported, among others, by the pattern intersection#2  part−of −→  road#1  kind−of ←−  thoroughfare#1  kind−of ←−  street#2.  Semantic  interconnection  patterns  are inspired by several works on semantic relatedness and similarity (Rada et al. 1989;  Hirst and St-Onge 1998; Mihalcea and Moldovan 2001).  An excerpt of the manually written context-free grammar encoding semantic inter-  connection patterns for the WordNet lexicon is reported in Table 1. For further details  the reader can refer to Velardi 2005.  3. Supporting Validation with Semantic Interconnection Patterns The validation task can be deﬁned as follows: Let w be a word in a sentence σ, previously annotated by a set of annotators A = {a1, a2, ..., an} each providing a sense for w, and let S = {s1, s2, ..., sm} ⊆ Senses(w) be the set of senses chosen for w by the annotators in A, where Senses(w) is the set of senses of w in the reference inventory (e.g., WordNet). A validator is asked to validate, that is, to adjudicate a sense s ∈ Senses(w) for a word w over the others. Notice that s is a word sense for w in the sense inventory, but is not necessarily in S, although it is likely to be. Also note that the annotators in A can be either human or automatic, depending upon the purpose of the exercise. Based on SSI, we developed a visual tool, Valido (http://lcl.di.uniroma1.it/valido), to support the validator in the difﬁcult task of assessing the quality and suitability of sense annotations. The tool takes as input a corpus of documents whose sentences are  
 Computational Linguistics  Volume 32, Number 1  comparing the accuracy of the very same algorithm according to whether or not it takes into account complementary semantic knowledge, they were able to show the beneﬁt derived from such knowledge. However, implications of Choi et al.’s study for text segmentation and for the use of LSA in natural language processing are unclear due to the methodology employed. In their experiments, semantic knowledge was acquired from a corpus containing the materials to be segmented in the test phase. One could speculate whether the largest part of the beneﬁt obtained thanks to the addition of semantic knowledge was not due to this hyper-speciﬁcity of the LSA corpus (i.e., the inclusion of the test materials). If this were the case, it would call into question the possibility of using LSA to acquire generic semantic knowledge that can be used to segment new texts. A priori, the problem does not seem serious for at least two reasons. First, Choi et al.’s segmentation procedure does not rely on supervised learning in which a system learns how to efﬁciently segment a text from training data. The LSA corpus only intervenes in an indirect manner by allowing the extraction of semantic proximities between words that are then used to compute similarities between parts of the text to segment (see Section 2 for details). Second, Choi et al. employed a large number of small test samples to evaluate their algorithm, each making up—on average—0.15% of the LSA corpus. The present study shows, however, that the presence of the test materials in the LSA corpus has an important effect, but also that the generic semantic knowledge derived from large corpora clearly improves the segmentation accuracy. This conclusion is drawn from two experiments in which the presence or absence of the test materials in the LSA corpus is manipulated. The ﬁrst experiment is based on the original materials from Choi et al., which consisted of a small corpus (1,000,000 words). The second experiment is based on a much larger corpus (25,000,000 words). Before reporting these experiments, Choi’s algorithm and the use of LSA within this framework are described.  2. The Two Versions of Choi’s Algorithm The segmentation algorithm proposed by Choi (2000) is made up of the three steps usually found in any segmentation procedure based on lexical cohesion. Firstly, the document to be segmented is divided into minimal textual units, usually sentences. Then, a similarity index between every pair of adjacent units is calculated. Each raw similarity value is cast on an ordinal scale by taking the proportion of neighboring values that are smaller than it. Lastly, the document is segmented recursively according to the boundaries between the units that maximize the sum of the average similarities inside the segments thus comprised (divisive clustering). The step of greatest interest here is the one that calculates the inter-sentence similarities. The procedure initially proposed by Choi (2000), C99, rests exclusively on the information contained in the text to be segmented. According to the vector space model, each sentence is represented by a vector of word frequency count, and the similarity between two sentences is calculated by means of the cosine measure between the corresponding vectors. In a ﬁrst evaluation based on the procedure described below, Choi showed that its algorithm outperforms several other approaches such as TextTiling (Hearst 1997) and Segmenter (Kan, Klavans, and McKeown 1998). Choi et al. (2001) claimed that it was possible to improve the inter-sentence similarities index by taking into account the semantic proximities between words estimated on the basis of Latent Semantic Analysis (LSA). Brieﬂy stated, LSA rests on the thesis that 6  Bestgen  Improving Text Segmentation  analyzing the contexts in which words occur permits an estimation of their similarity in meaning (Deerwester et al. 1990; Landauer and Dumais 1997). The ﬁrst step in the analysis is to construct a lexical table containing an information-theoretic weighting of the frequencies of the words occurrence in each document (i.e. sentence, paragraph, or text) included in the corpus. This frequency table undergoes a Singular Value Decomposition that extracts the most important orthogonal dimensions, and, consequently, discards the small sources of variability in term usage. After this step, every word is represented by a vector of weights indicating its strength of association with each of the dimensions. This makes it possible to measure the semantic proximity between any two words by using, for instance, the cosine measure between the corresponding vectors. Proximity between any two sentences (or any other textual units), even if these sentences were not present in the original corpus, can be estimated by computing a vector for each of these units—which corresponds to the weighted sum of the vectors of the words that compose it—and then by computing the cosine between these vectors (Deerwester et al. 1990). Choi et al. (2001) have shown that using this procedure to compute the inter-sentence similarities results in the previous version of the algorithm (based solely on word repetition) being outperformed. 3. Experiment 1 The aim of this experiment is to determine the impact of the presence of the test materials in the LSA corpus on the results obtained by Choi et al. (2001). Does semantic knowledge acquired from a corpus that does not include the test materials also improve the segmentation accuracy? 3.1 Method This experiment was based on the procedure and test materials designed by Choi (2000), which was also used by several authors as a benchmark for comparing segmentation systems (Brants et al. 2002; Ferret 2002; Kehagias et al. 2003; Utiyama and Isahara 2001). The task consists in ﬁnding the boundaries between concatenated texts. Each test sample is a concatenation of ten text segments. Each segment consisted in the ﬁrst n sentences of a randomly selected text from two sub-sections of the Brown corpus. For the present experiment, I used the most general test materials built by Choi (2000), in which the size of the segments within each sample varies randomly from 3 to 11 sentences. It is composed of 400 samples. The analysis related to the comparison between the accuracy of the algorithm when the test materials were included in the LSA corpus (Within) and when it was not (Without). One Within semantic space, which corresponds to the one used by Choi et al., was built using the entire Brown corpus as the LSA corpus. Four hundred different Without spaces were built, one for each test sample, by each time removing from the Brown corpus only the sentences that make this sample. To extract the LSA space and to apply the segmentation algorithm, a series of parameters had to be set. First of all, paragraphs were used as documents for building the lexical tables because Choi et al. observed that such middle-sized units were more effective than shorter units (i.e., sentences). The words on Choi’s stoplist were removed, as were those that appeared only once in the whole corpus. Words were not stemmed, as in Choi et al. (2001). To build the LSA space, the singular value decomposition was realized using the program SVDPACKC (Berry 1992; Berry et al. 1993), and the ﬁrst 7  Computational Linguistics  Volume 32, Number 1  Table 1 Error rates and variance (in parentheses) for the Within and the Without conditions.  Pk  WindowDiff  Within 0.084 (0.005) 0.090 (0.005) Without 0.120 (0.006) 0.126 (0.006)  300 singular vectors were retained. Concerning the segmentation algorithm, I used the version in which the number of boundaries to be found is imposed, and thus ﬁxed at nine. An 11 × 11 rank mask was used for the ordinal transformation, as recommended by Choi (2000).  3.2 Results The segmentation accuracy was evaluated by means of the index reported by Choi et al. (2001): the Pk measure of segmentation inaccuracy (Beeferman, Berger, and Lafferty 1999), which gives the proportion of sentences that are wrongly predicted to belong to the same segment or wrongly predicted to belong to different segments. I also report, for potential future comparison, Pevzner and Hearst’s (2002) WindowDiff index, which remedies several problems in the Pk measure. Results are provided in Table 1.1 Compared with the Within condition, the performance in the Without condition is deﬁnitely worse, as conﬁrmed by t tests for paired sample (each test sample being used as an observation) that are signiﬁcant for an alpha smaller than 0.0001. The C99 algorithm, which does not employ LSA to estimate the similarities between the sentences, produces a Pk of 0.13 (Choi et al. 2001, Table 3, line 3: No stemming). It appears that although the Without condition is still better than C99, the beneﬁt is very small. Before concluding that the presence of the test materials in the LSA corpus strongly modiﬁed the semantic space, an alternative explanation must be considered. The loss of accuracy in the Without condition could potentially be due to the fact that the words indexed in the corresponding LSA spaces are systematically slightly fewer than those present in the Within space. Removing each test sample led to the loss—on average— of 23 different words out of the 25,847 words that are indexed in the Within space. In the Without spaces, these words are no longer available to estimate the similarity of the sentences, whereas they are employed in the Within space. In order to determine whether this factor can explain the difference in performance, a complementary analysis was carried out on the Within space in which, for each test sample, only the words present in the corresponding Without space were taken into account. In this manner, only the semantic relations can come into play. Compared with the complete Within space, almost no drop in performance was observed: the Pk error rate went from 0.084 to 0.085 in the new analysis. This result indicates that it is not the words selected for the calculation of the proximities that matter, but the semantic relations in the spaces extracted from the word co-occurrences by the Singular Value Decomposition.  
The quantiﬁcation of lexical semantic relatedness has many applications in NLP, and many different measures have been proposed. We evaluate ﬁve of these measures, all of which use WordNet as their central resource, by comparing their performance in detecting and correcting real-word spelling errors. An information-content–based measure proposed by Jiang and Conrath is found superior to those proposed by Hirst and St-Onge, Leacock and Chodorow, Lin, and Resnik. In addition, we explain why distributional similarity is not an adequate proxy for lexical semantic relatedness. 1. Introduction The need to determine semantic relatedness or its inverse, semantic distance, between two lexically expressed concepts is a problem that pervades much of natural language processing. Measures of relatedness or distance are used in such applications as word sense disambiguation, determining the structure of texts, text summarization and annotation, information extraction and retrieval, automatic indexing, lexical selection, and the automatic correction of word errors in text. It’s important to note that semantic relatedness is a more general concept than similarity; similar entities are semantically related by virtue of their similarity (bank–trust company), but dissimilar entities may also be semantically related by lexical relationships such as meronymy (car–wheel) and antonymy (hot–cold), or just by any kind of functional relationship or frequent association (pencil–paper, penguin–Antarctica, rain–ﬂood). Computational applications typically require relatedness rather than just similarity; for example, money and river are cues to the in-context meaning of bank that are just as good as trust company. However, it is frequently unclear how to assess the relative merits of the many competing approaches that have been proposed for determining lexical semantic relatedness. Given a measure of relatedness, how can we tell whether it is a good one or a poor one? Given two measures, how can we tell whether one is better than the other, and under what conditions it is better? And what is it that makes some measures better than others? Our purpose in this paper is to compare the performance of a number of measures of semantic relatedness that have been proposed for use in applications in natural language processing and information retrieval.  ∗ Department of Computer Science, Toronto, Ontario, Canada M5S 3G4; {abm, gh}@cs.toronto.edu. Submission received: 8 July 2004; revised submission received: 30 July 2005; accepted for publication: 20 August 2005. © 2006 Association for Computational Linguistics  Computational Linguistics  Volume 32, Number 1  1.1 Terminology and Notation In the literature related to this topic, at least three different terms are used by different authors or sometimes interchangeably by the same authors: semantic relatedness, similarity, and semantic distance. Resnik (1995) attempts to demonstrate the distinction between the ﬁrst two by way of example. “Cars and gasoline”, he writes, “would seem to be more closely related than, say, cars and bicycles, but the latter pair are certainly more similar.” Similarity is thus a special case of semantic relatedness, and we adopt this perspective in this paper. Among other relationships that the notion of relatedness encompasses are the various kinds of meronymy, antonymy, functional association, and other “non-classical relations” (Morris and Hirst 2004). The term semantic distance may cause even more confusion, as it can be used when talking about either just similarity or relatedness in general. Two concepts are “close” to one another if their similarity or their relatedness is high, and otherwise they are “distant”. Most of the time, these two uses are consistent with one another, but not always; antonymous concepts are dissimilar and hence distant in one sense, and yet are strongly related semantically and hence close in the other sense. We would thus have very much preferred to be able to adhere to the view of semantic distance as the inverse of semantic relatedness, not merely of similarity, in the present paper. Unfortunately, because of the sheer number of methods measuring similarity, as well as those measuring distance as the “opposite” of similarity, this would have made for an awkward presentation. Therefore, we have to ask the reader to rely on context when interpreting what exactly the expressions semantic distance, semantically distant, and semantically close mean in each particular case. Various approaches presented below speak of concepts and words. As a means of acknowledging the polysemy of language, in this paper the term concept will refer to a particular sense of a given word. We want to be very clear that, throughout this paper, when we say that two words are “similar”, this is a short way of saying that they denote similar concepts; we are not talking about similarity of distributional or co-occurrence behavior of the words, for which the term word similarity has also been used (Dagan 2000; Dagan, Lee, and Pereira 1999). While similarity of denotation might be inferred from similarity of distributional or co-occurrence behavior (Dagan 2000; Weeds 2003), the two are distinct ideas. We return to the relationship between them in Section 6.2. When we refer to hierarchies and networks of concepts, we will use both the terms link and edge to refer to the relationships between nodes; we prefer the former term when our view emphasizes the taxonomic aspect or the meaning of the network, and the latter when our view emphasizes algorithmic or graph-theoretic aspects. In running text, examples of concepts are typeset in sans-serif font, whereas examples of words are given in italics; in formulas, concepts and words will usually be denoted by c and w, with various subscripts. For the sake of uniformity of presentation, we have taken the liberty of altering the original notation accordingly in some other authors’ formulas.  2. Lexical Resource–based Approaches to Measuring Semantic Relatedness All approaches to measuring semantic relatedness that use a lexical resource construe the resource, in one way or another, as a network or directed graph, and then base the measure of relatedness on properties of paths in this graph. 14  Budanitsky and Hirst  Lexical Semantic Relatedness  2.1 Dictionary-based Approaches Kozima and Furugori (1993) turned the Longman Dictionary of Contemporary English (LDOCE) (Procter 1978) into a network by creating a node for every headword and linking each node to the nodes for all the words used in its deﬁnition. The 2851word controlled deﬁning vocabulary of LDOCE thus becomes the densest part of the network: the remaining nodes, which represent the headwords outside of the deﬁning vocabulary, can be pictured as being situated at the fringe of the network, as they are linked only to deﬁning-vocabulary nodes and not to each other. In this network, the similarity function simKF between words of the deﬁning vocabulary is computed by means of spreading activation on this network. The function is extended to the rest of LDOCE by representing each word as a list W = {w1, . . . , wr} of the words in its deﬁnition; thus, for instance, simKF(linguistics, stylistics) = simKF({the, study, of, language, in, general, and, of, particular, languages, and, their, structure, and, grammar, and, history}, {the, study, of, style, in, written, or, spoken, language}) Kozima and Ito (1997) built on this work to derive a context-sensitive, or dynamic, measure that takes into account the “associative direction” of a given word pair. For example, the context {car, bus} imposes the associative direction of vehicle (close words are then likely to include taxi, railway, airplane, etc.), whereas the context {car, engine} imposes the direction of components of car (tire, seat, headlight, etc.). 2.2 Approaches Based on Roget-structured Thesauri Roget-structured thesauri, such as Roget’s Thesaurus itself, the Macquarie Thesaurus (Bernard 1986), and others, group words in a structure based on categories within which there are several levels of ﬁner clustering. The categories themselves are grouped into a number of broad, loosely deﬁned classes. However, while the classes and categories are named, the ﬁner divisions are not; the words are clustered without attempting to explicitly indicate how and why they are related. The user’s main access is through the index, which contains category numbers along with labels representative of those categories for each word. Polysemes are implicitly disambiguated, to a certain extent, by the other words in their cluster and in their index entry. Closely related concepts might or might not be physically close in the thesaurus: “Physical closeness has some importance . . . but words in the index of the thesaurus often have widely scattered categories, and each category often points to a widely scattered selection of categories” (Morris and Hirst 1991). Methods of semantic distance that are based on Rogetstructured thesauri therefore rely not only on the category structure but also on the index and on the pointers within categories that cross-reference other categories. In part as a consequence of this, typically no numerical value for semantic distance can be obtained: rather, algorithms using the thesaurus compute a distance implicitly and return a boolean value of ‘close’ or ‘not close’. Working with an abridged version of Roget’s Thesaurus, Morris and Hirst (1991) identiﬁed ﬁve types of semantic relations between words. In their approach, two words 15  Computational Linguistics  Volume 32, Number 1  were deemed to be related to one another, or semantically close, if their base forms satisfy any one of the following conditions: 1. They have a category in common in their index entries. 2. One has a category in its index entry that contains a pointer to a category of the other. 3. One is either a label in the other’s index entry or is in a category of the other. 4. They are both contained in the same subcategory. 5. They both have categories in their index entries that point to a common category. These relations account for such pairings as wife and married, car and driving, blind and see, reality and theoretically, brutal and terriﬁed. (However, different editions of Roget’s Thesaurus yield somewhat different sets of relations.) Of the ﬁve types of relations, perhaps the most intuitively plausible ones — the ﬁrst two in the list above — were found to validate over 90% of the intuitive lexical relationships that the authors used as a benchmark in their experiments. Jarmasz and Szpakowicz (2003) also implemented a similarity measure with Roget’s Thesaurus; but because this measure is based strictly on hierachy rather than the index structure, we discuss it in Section 2.4 below. 2.3 Approaches Using WordNet and Other Semantic Networks Most of the methods discussed in the remainder of Section 2 use WordNet (Fellbaum 1998), a broad coverage lexical network of English words. Nouns, verbs, adjectives, and adverbs are each organized into networks of synonym sets (synsets) that each represent one underlying lexical concept and are interlinked with a variety of relations. (A polysemous word will appear in one synset for each of its senses.) In the ﬁrst versions of WordNet (those numbered 1.x), the networks for the four different parts of speech were not linked to one another.1 The noun network of WordNet was the ﬁrst to be richly developed, and most of the researchers whose work we will discuss below therefore limited themselves to this network.2 The backbone of the noun network is the subsumption hierarchy (hyponymy/ hypernymy), which accounts for close to 80% of the relations. At the top of the hierarchy are 11 abstract concepts, termed unique beginners, such as entity (‘something having concrete existence; living or nonliving’) and psychological feature (‘a feature of the mental life of a living organism’). The maximum depth of the noun hierarchy is 16 nodes. The nine types of relations deﬁned on the noun subnetwork, in addition to the synonymy relation that is implicit in each node are: the hyponymy (IS-A) relation, 
We introduce ﬁnite-state registered automata (FSRAs), a new computational device within the framework of ﬁnite-state technology, speciﬁcally tailored for implementing non-concatenative morphological processes. This model extends and augments existing ﬁnite-state techniques, which are presently not optimized for describing this kind of phenomena. We ﬁrst deﬁne the model and discuss its mathematical and computational properties. Then, we provide an extended regular language whose expressions denote FSRAs. Finally, we exemplify the utility of the model by providing several examples of complex morphological and phonological phenomena, which are elegantly implemented with FSRAs. 1. Introduction Finite-state (FS) technology has been considered adequate for describing the morphological processes of the world’s languages since the pioneering works of Koskenniemi (1983) and Kaplan and Kay (1994). Several toolboxes provide extended regular expression description languages and compilers of the expressions to ﬁnite-state automata (FSAs) and transducers (FSTs) (Karttunen et al. 1996; Mohri 1996; van Noord and Gerdemann 2001a). While FS approaches to most natural languages have generally been very successful, it is widely recognized that they are less suitable for non-concatenative phenomena; in particular, FS techniques are assumed not to be able to efﬁciently account for the non-concatenative word formation processes that Semitic languages exhibit (Lavie et al. 1988). While much of the inﬂectional morphology of Semitic languages can be rather straightforwardly described using concatenation as the primary operation, the main word formation process in such languages is inherently non-concatenative. The standard account describes words in Semitic languages as combinations of two morphemes: a root and a pattern.1 The root consists of consonants only, by default three (although longer roots are known). The pattern is a combination of vowels and, possibly, consonants too, with “slots” into which the root consonants can be inserted. Words are created by interdigitating roots into patterns: The ﬁrst consonant of the root is inserted into the ﬁrst consonantal slot of the pattern, the second root consonant ﬁlls the second slot, and the third ﬁlls the last slot. After the root combines with the pattern, some  ∗ Department of Computer Science, University of Haifa, 31905 Haifa, Israel. E-mail: yaelc@cs.haifa.ac.il. † Department of Computer Science, University of Haifa, 31905 Haifa, Israel. E-mail: shuly@cs.haifa.ac.il. 1 An additional morpheme, vocalization, is used to abstract the pattern further; for the present purposes, this distinction is irrelevant. Submission received: 17 August 2004; revised submission received: 15 June 2005; accepted for publication: 26 September 2005. © 2006 Association for Computational Linguistics  Computational Linguistics  Volume 32, Number 1  Figure 1 Na¨ıve FSA with duplicated paths. morpho-phonological alternations take place, which may be non-trivial but are mostly concatenative. The major problem that we tackle in this work is medium-distance dependencies, whereby some elements that are related to each other in some deep-level representation (e.g., the consonants of the root) are separated on the surface. While these phenomena do not lie outside the descriptive power of FS systems, na¨ıvely implementing them in existing ﬁnite-state calculi is either impossible or, at best, results in large networks that are inefﬁcient to process, as the following examples demonstrate. Example 1 We begin with a simpliﬁed problem, namely accounting for circumﬁxes. Consider three Hebrew patterns: ha a a, hit a a ut, and mi a , where the empty boxes indicate the slots in the patterns into which the consonants of the roots are inserted. Hebrew orthography2 dictates that these patterns be written h a, ht ut, and m , respectively, i.e., the consonants are inserted into the ‘ ’ slots as one unit (i.e., the patterns can be viewed as circumﬁxes). An automaton that accepts all the possible combinations of three-consonant stems and these three circumﬁxes is illustrated in Figure 1.3 Given r stems and p circumﬁxes, the number of its states is (2r + 2)p + 2, i.e., increases linearly with the number of stems and circumﬁxes. The number of arcs in this automaton is 3rp + 2p, i.e, also O(rp). Evidently, the three basic different paths that result from the three circumﬁxes have the same body, which encodes the stems. An attempt to avoid the duplication of paths is represented by the automaton of Figure 2, which accepts the language denoted by the regular expression (ht + h + m)(root)(ut + a + ). The number of states here is 2r + 4, i.e., is independent of the number of circumﬁxes. The number of arcs is (3r + 2p), that is, O(r + p), and thus, the complexity of the number of arcs is also reduced. Obviously, however, such an automaton over-generates by accepting also invalid words such as m ut. In other words, it ignores the dependencies which hold between preﬁxes and sufﬁxes of the same circumﬁx. Since ﬁnite-state devices have no 2 Many of the vowels are not explicitly depicted in the Hebrew script. 3 This is an over-simpliﬁed example; in practice, the process of combining roots with patterns is highly idiosyncratic, like other derivational morphological processes. 50  Cohen-Sygal and Wintner  Non-Concatenative Morphology  Figure 2 Over-generating FSA. memory, save for the states, there is no simple and space-efﬁcient way to account for such dependencies. Example 2 Consider now a representation of Hebrew where all vowels are explicit, e.g., the pattern hit a e . Consider also the roots r.g.z, b.$.l, and g.b.r. The consonants of a given root are inserted into the ‘ ’ slots to obtain bases such as hitragez, hitba$el, and hitgaber. The ﬁnite state automaton of Figure 3 is the minimized automaton accepting the language; it has ﬁfteen states. If the number of three letter roots is r, then a general automaton accepting the combinations of the roots with this pattern will have 4r + 3 states and 5r + 1 arcs. Notice the duplicated arcs which stem from copying the pattern in the different paths. Example 3 Another non-concatenative process is reduplication: The process in which a morpheme or part of it is duplicated. Full reduplication is used as a pluralization process in Malay and Indonesian; partial reduplication is found in Chamorro to indicate intensivity. It can also be found in Hebrew as a diminutive formation of nouns and adjectives: keleb klablab $apan $panpan zaqan zqanqan $axor $xarxar dog puppy rabbit bunny beard goatee black dark qatan qtantan little tiny Let Σ be a ﬁnite alphabet. The language L = {ww | w ∈ Σ∗} is known to be trans-regular, therefore no ﬁnite-state automaton accepts it. However, the language Ln = {ww | w ∈ Σ∗, |w| = n} for some constant n is regular. Recognizing Ln is a ﬁnite approximation of the general problem of recognizing L. The length of the words in natural languages can in most cases be bounded by some n ∈ N, hence the amount of reduplication in natural languages is practically limited. Therefore, the descriptive power of Ln is sufﬁcient for the amount of reduplication in natural languages (by  Figure 3 FSA for the pattern hit a e . 51  Computational Linguistics  Volume 32, Number 1  constructing Ln for a small number of different ns). An automaton that accepts Ln can be constructed by listing a path for each accepted string (since Σ and n are ﬁnite, the number of words in Ln is ﬁnite). The main drawback of such an automaton is the growth in its size as |Σ| and n increase: The number of strings in Ln is |Σ|n. Thus, ﬁnitestate techniques can account for limited reduplication, but the resulting networks are space-inefﬁcient. As a ﬁnal, non-linguistic, motivating example, consider the problem of n-bit incrementation, introduced by Kornai (1996). Example 4 The goal of this example is to construct a transducer over Σ = {0, 1} whose input is a 32 bit binary number and whose output is the result of adding 1 to the input. A transducer that performs addition by 1 on binary numbers has only 5 states and 12 arcs,4 but this transducer is neither sequential nor sequentiable. The problem is that since the input is scanned left to right but the carry moves right to left, the output of the ﬁrst bit has to be delayed, possibly even until the last input bit is scanned. Thus, for an n-bit binary incrementor, 2n disjunctions have to be considered, and therefore a minimized transducer has to assign a separate state to each combination of bits, resulting in 2n states and a similar number of transitions. In this work we propose a novel FS model which facilitates the expression of medium-distance dependencies such as interdigitation and reduplication in an efﬁcient way. Our main motivation is theoretical, i.e., reducing the complexity of the number of states and arcs in the networks; we show that these theoretical contributions result in practical improvements. In Section 3 we deﬁne the model formally, show that it is equivalent to FSAs and deﬁne many closure properties directly.5 We then (Section 4) deﬁne a regular expression language for denoting FSRAs. In Section 5 we provide dedicated regular expression operators for some non-concatenative phenomena and exemplify the usefulness of the model by efﬁciently accounting for the motivating examples. In Section 6 we extend FSRAs to transducers. The model is evaluated through an actual implementation in Section 7. We conclude with suggestions for future research. 2. Related Work In spite of the common view that FS technology is in general inadequate for describing non-concatenative processes, several works address the above-mentioned problems in various ways. We summarize existing approaches in this section. Several works examine the applicability of traditional two-level systems for implementing non-concatenative morphology. Two-Level Morphology was used by Kataja and Koskenniemi (1988) to create a rule system for phonological and morphophonological alternations in Akkadian, accounting for word inﬂection and regular verbal derivation. As this solution effectively deﬁnes lexical representations of word-forms, its main disadvantage is that the ﬁnal network is the na¨ıve one, suffering from the space complexity problems discussed above. Lavie et al. (1988) examine the applicability of Two-  4 A complete explanation of the construction can be found in http://www.xrce.xerox.com/competencies/ content-analysis/fsCompiler/fsexamples.html#Add1. 5 Many of the formal proofs and constructions, especially the ones that are similar to the case of standard FSAs, are suppressed; see Cohen-Sygal (2004) for the complete proofs and constructions. 52  Cohen-Sygal and Wintner  Non-Concatenative Morphology  Level Morphology to the description of Hebrew Morphology, and in particular to verb inﬂection. Their lexicon consists of three parts: verb primary bases (the past tense, third person, singular, masculine), verb preﬁxes, and verb sufﬁxes. They attempt to describe Hebrew verb inﬂection as a concatenation of preﬁx+base+sufﬁx, implementable by the Two-Level model. However, they conclude that “The Two-Level rules are not the natural way to describe . . . verb inﬂection process. The only alternative choice . . . is to keep all bases . . . it seems wasteful to save all the secondary bases of verbs of the same pattern.” Other works deal with non-concatenative morphology by extending ordinary FSAs without extending their expressivity. The traditional two-level model of Koskenniemi (1983) is expanded into n-tape automata by Kiraz (2000), following the insight of Kay (1987) and Kataja and Koskenniemi (1988). The idea is to use more than two levels of expression: The surface level employs one representation, but the lexical form employs multiple representations (e.g., root, pattern) and therefore can be divided into different levels, one for each representation. Elements that are separated on the surface (such as the root’s consonants) are adjacent on a particular lexical level. For example, to describe circumﬁxation using this model, a 4-tape automaton of the form surface, PR pattern, circumﬁx, stem is constructed, so that each word is represented by 4 levels. The surface level represents the ﬁnal form of the word. The PR pattern is the pattern in which the stem and the circumﬁx are combined (P represents the circumﬁx’s position and R the root letter’s position), e.g., PRRRP. The circumﬁx and stem levels represent the circumﬁx and the stem respectively. For example, combining the Hebrew stem pqd with the circumﬁx ht-ut will have the 4-level representation shown in Figure 4. Notice that the symbols representing the circumﬁx in the PR pattern level (i.e., the occurrences of the symbol ‘P’), the circumﬁx symbols in the circumﬁx level, and the circumﬁx symbols in the surface level are located in correlating places in the different levels. The same holds for the stem symbols. In this way, it is clear which symbols of the surface word belong to the circumﬁx, which belong to the stem, and how they combine together to create the ﬁnal form of the word. The 4-tape automaton of Figure 5 accepts all the combinations created by circumﬁxing roots with the three circumﬁxes of Example 1. Each arc is attributed with a quadruplet, consisting of four correlating symbols in the four levels. Notice that as in FSAs, the paths encoding the roots are duplicated for each circumﬁx, so that this automaton is as spaceinefﬁcient as ordinary FSAs. Kiraz (2000) does not discuss the space complexity of this model, but the number of states still seems to increase with the number of roots and patterns. Moreover, the n-tape model requires speciﬁcation of dependencies between symbols in different levels, which may be non-trivial. Walther (2000a) suggests a solution for describing natural language reduplication using ﬁnite-state methods. The idea is to enrich ﬁnite-state automata with three new operations: repeat, skip, and self loops. Repeat arcs allow moving backwards within a string and thus repeat a part of it (to model reduplication). Skip arcs allow moving forwards in a string while suppressing the spell out of some of its letters; self loop arcs model inﬁxation. In Walther (2000b), the above technique is used to describe Temiar  Figure 4 4-tape representation for the Hebrew word htpqdut. 53  Computational Linguistics  Volume 32, Number 1  Figure 5 4-tape automaton for circumﬁxation example. reduplication, but no complexity analysis of the model is given. Moreover, this technique does not seem to be able to describe interdigitation. Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output. The compile-replace algorithm facilitates a compact deﬁnition of nonconcatenative morphological processes, but since such expressions compile to the na¨ıve networks, no space is saved. Furthermore, this is a compile-time mechanism rather than a theoretical and mathematically founded solution. Other works extend the FS model by enabling some sort of context-sensitivity. Blank (1985, 1989) presents a model, called Register Vector Grammar, introducing contextsensitivity by representing the states and transitions of ﬁnite-state automata as ternaryvalued vectors, which need not be fully speciﬁed. No formal properties of this model are discussed. In a similar vein, Kornai (1996) introduces vectorized ﬁnite-state automata, where both the states and the transitions are represented by vectors of elements of a partially ordered set. The vectors are manipulated by operations of uniﬁcation and overwriting. The vectors need not be fully determined, as some of the elements can be unknown (free). In this way information can be moved through the transitions by the overwriting operation and traversing these transitions can be sanctioned through the uniﬁcation operation. As one of the examples of the advantages of this model, Kornai (1996) shows it can efﬁciently solve the problem of 32-bit binary incrementor (Example 4). Using vectorized ﬁnite-state automata, a 32-bit incrementor is constructed where ﬁrst, using overwriting, the input is scanned and stored in the vectors, and then, using uniﬁcation, the result is calculated where the carry can be computed from right to left. We return to this example in example 6, where we show how our own model can solve it efﬁciently. The formalism presented by Kornai (1996) allows a signiﬁcant reduction in the network size, but its main disadvantage lies in the fact that it signiﬁcantly deviates from the standard methodology of developing ﬁnite-state devices, and integration of vectorized automata with standard ones remains a challenge. Moreover, it is unclear how, for a given problem, the corresponding network should be constructed: Programming with vectorized automata seems to be unnatural, and no regular expression language is provided for them. A more general approach to the design of ﬁnite-state machines is introduced by Mohri, Pereira, and Riley (2000). They introduce an object-oriented library for manipu- 54  Cohen-Sygal and Wintner  Non-Concatenative Morphology  lating ﬁnite-state devices that is based on the algebraic concepts of rational power series and semirings. This approach facilitates a high degree of generality as the algorithms are deﬁned for the general algebraic notions, which can then be specialized according to the needs of the user. They exemplify the usefulness of this library by showing how to specialize it for the manipulation of weighted automata and transducers. Our work can be seen as another specialization of this general approach, tailored for ideally dealing with our motivating examples. Several works introduce the notion of registers, whether for solving similar problems or motivated by different considerations. Krauwer and des Tombe (1981) refer to transducers with a ﬁnite number of registers when comparing transducers and context free grammars with respect to their capabilities to describe languages. They sketch a proof showing that such transducers are equivalent to ordinary ﬁnite-state transducers. However, they never formally deﬁne the model and do not discuss its ability to efﬁciently implement non-concatenative natural languages phenomena. Moreover, they do not show how the closure properties can be implemented directly on these registered transducers, and do not provide any regular language denoting such transducers. Motivated by different considerations, Kaminski and Francez (1994) present a computational model which extends ﬁnite state automata to the case of inﬁnite alphabets. This model is limited to recognizing only regular languages over inﬁnite alphabets while maintaining closure under Kleene star and boolean operations, with the exception of closure under complementation. The familiar automaton is augmented with registers, used to store alphabet symbols, whose number is ﬁxed for each automaton and can vary from one automaton to another. The model is designed to deal with inﬁnite alphabets, and therefore it cannot distinguish between different symbols; it can identify different patterns but cannot distinguish between different symbols in the pattern as is often needed in natural languages. Our solution is reminiscent of Kaminski and Francez (1994) in the sense that it augments ﬁnite-state automata with ﬁnite memory (registers) in a restricted way, but we avoid the above-mentioned problem. In addition, our model supports a register alphabet that differs from the language alphabet, allowing the information stored in the registers to be more meaningful. Moreover, our transition relation is a more simpliﬁed extension of the standard one in FSAs, rendering our model a conservative extension of standard FSAs and allowing simple integration of existing networks with networks based on our model. Finally, Beesley (1998) directly addresses medium-distance dependencies between separated morphemes in words. He proposes a method, called ﬂag diacritics, which adds features to symbols in regular expressions to enforce dependencies between separated parts of a string. The dependencies are forced by different kinds of uniﬁcation actions. In this way, a small amount of ﬁnite memory is added, keeping the total size of the network relatively small. Unfortunately, this method is not formally deﬁned, nor are its mathematical and computational properties proved. Furthermore, ﬂag diacritics are manipulated at the level of the extended regular expressions, although it is clear that they are compiled into additional memory and operators in the networks themselves. The presentations of Beesley (1998) and Beesley and Karttunen (2003) do not explicate the implementation of such operators and do not provide an analysis of their complexity. Our approach is similar in spirit, but we provide a complete mathematical and computational analysis of such extended networks, including a proof that the model is indeed regular and constructions of the main closure properties. We also provide dedicated regular expression operations for non-concatenative processes and show  55  Computational Linguistics  Volume 32, Number 1  how they are compiled into extended networks, thereby accounting for the motivating examples. 3. Finite-state Registered Automata We deﬁne a new model, ﬁnite-state registered automata (FSRA), aimed at facilitating the expression of various non-concatenative morphological phenomena in an efﬁcient way. The model augments ﬁnite-state automata with ﬁnite memory (registers) in a restricted way that saves space but does not add expressivity. The number of registers is ﬁnite, usually small, and eliminates the need to duplicate paths as it enables the automaton to “remember” a ﬁnite number of symbols. In addition to being associated with an alphabet symbol, each arc is also associated with an action on the registers. There are two kinds of actions, read and write. The read action, denoted R, allows traversing an arc only if a designated register contains a speciﬁc symbol. The write action, denoted W, allows traversing an arc while writing a speciﬁc symbol into a designated register. In this section we deﬁne FSRAs and show that they are equivalent to standard FSAs (Section 3.1). We then directly deﬁne several closure operations over FSRAs (Section 3.2) and provide some optimizations in Section 3.3. We conclude this section with a discussion of minimization (Section 3.4). 3.1 Deﬁnitions and Examples Deﬁnition A ﬁnite-state registered automaton (FSRA) is a tuple A = Q, q0, Σ, Γ, n, δ, F , where: r Q is a ﬁnite set of states. r q0 ∈ Q is the initial state. r Σ is a ﬁnite alphabet (the language alphabet). r n ∈ N (indicating the number of registers). r Γ is a ﬁnite alphabet including the symbol ‘#’ (the registers alphabet). We use meta-variables ui, vi to range over Γ and u, v to range over Γn. r The initial content of the registers is #n, meaning that the initial value of all the registers is ‘empty’. r δ ⊆ Q × Σ ∪ { } × {R, W} × {0, 1, 2, . . . , n − 1} × Γ × Q is the transition relation. The intuitive meaning of δ is as follows: – (s, σ, R, i, γ, t) ∈ δ where i > 0 implies that if A is in state s, the input symbol is σ, and the content of the i-th register is γ, then A may enter state t. – (s, σ, W, i, γ, t) ∈ δ where i > 0 implies that if A is in state s and the input symbol is σ, then the content of the i-th register is changed into γ (overwriting whatever was there before) and A may enter state t. – (s, σ, R, 0, #, t) implies that if A is in state s and the input symbol is σ, then A may enter state t. Notice that the content of register number 0 is always #. We use the shorthand notation (s, σ, t) for such transitions. r F ⊆ Q is the set of ﬁnal states. 56  Cohen-Sygal and Wintner  Non-Concatenative Morphology  Deﬁnition A conﬁguration of A is a pair (q, u), where q ∈ Q and u ∈ Γn (q is the current state and u represents the registers content). The set of all conﬁgurations of A is denoted by Q c. The pair qc0 = (q0, #n) is called the initial conﬁguration, and conﬁgurations with the ﬁrst component in F are called ﬁnal conﬁgurations. The set of ﬁnal conﬁgurations is denoted by Fc.  Deﬁnition Let u = u0u1 . . . un−1 and v = v0v1 . . . vn−1. Given a symbol α ∈ Σ ∪ { } and an FSRA A, we say that a conﬁguration (s, u) produces a conﬁguration (t, v), denoted (s, u) α,A (t, v), iff either one of the following holds: r There exists i, 0 ≤ i ≤ n − 1, and there exists γ ∈ Γ, such that (s, α, R, i, γ, t) ∈ δ and u = v and ui = vi = γ; or r There exists i, 0 ≤ i ≤ n − 1, and there exists γ ∈ Γ, such that (s, α, W, i, γ, t) ∈ δ and for all k, k ∈ {0, 1, ..., n − 1}, such that k = i, uk = vk and vi = γ.  Informally, a conﬁguration c1 produces a conﬁguration c2 iff the automaton can move from c1 to c2 when scanning the input α (or without any input, when α = ) in one step. If the register operation is R, then the contents of the registers in the two conﬁgurations must be equal, and in particular the contents of the designated register in the two conﬁgurations should be the expected symbol (γ). If the register operation is W, then the contents of the registers in the two conﬁgurations is equal except for the designated register, whose contents in the produced conﬁguration should be the expected symbol (γ).  Deﬁnition A run of A on w is a sequence of conﬁgurations c0, ..., cr such that c0 = qc0, cr ∈ Fc, and for every k, 1 ≤ k ≤ r, ck−1 αk,A ck and w = α1...αr. An FSRA A accepts a word w if there exists a run of A on w. Notice that |w| may be less than r since some of the αi may be . The language recognized by an FSRA A, denoted by L(A), is the set of words over Σ∗ accepted by A.  Example 5 Consider again example 1. We construct an efﬁcient FSRA accepting all and only the possible combinations of stems and circumﬁxes. If the number of stems is r, we deﬁne an FSRA A = Q, q0, Σ, Γ, 2, δ, {qf } where:  r Q = {q0, q1, . . . , q2r+2, qf } r Σ = {a, b, c, . . . , z, ht, ut}  r Γ = {ht  ut, h  a, m  , #}  r δ = {(q0, ht, W, 1, ht  ut, q1), (q0, h, W, 1, h ∪  a, q1)}  {(q0, m, W, 1, m  , q1), (q2r+2, ut, R, 1, ht ∪  ut, qf )}  {(q2r+2, a, R, 1, h  a, qf ), (q2r+2, , R, 1, m ∪  , qf )}  {(q1, α1, qi), (qi, α2, qi+1), (qi+1, α3, q2r+2) | 2 ≤ i ≤ 2r and α1α2α3 is the i-th stem}.  57  Computational Linguistics  Volume 32, Number 1  This automaton is shown in Figure 6. The number of its states is 2r + 4 (like the FSA of Figure 2), that is, O(r), and in particular independent of the number of circumﬁxes. The number of arcs is also reduced from O(r × p), where p indicates the number of circumﬁxes, to O(r + p). Example 6 Consider again example 2. The FSRA of Figure 7 also accepts the same language. This automaton has seven states and will have seven states for any number of roots. The number of arcs is also reduced to 3r + 3. Next, we show that ﬁnite-state registered automata and standard ﬁnite state automata recognize the same class of languages. Trivially, every ﬁnite-state automaton has an equivalent FSRA: Every FSA is also an FSRA since every transition (s, σ, t) in an FSRA is a shorthand notation for (s, σ, R, 0, #, t). The other direction is also simple. Theorem 1 Every FSRA has an equivalent ﬁnite-state automaton. We prove this by constructing an equivalent FSA to a given FSRA. The construction is based on the fact that in FSRAs the number of registers is ﬁnite, as are the sets Γ and Q, the register alphabet and states, respectively. Hence the number of conﬁgurations is ﬁnite. The FSA’s states are the conﬁgurations of the FSRA, and the transition function simulates the ‘produces’ relation. Notice that this relation holds between conﬁgurations depending on Σ only, similarly to the transition function in an FSA. The constructed FSA is non-deterministic, with possible -moves. The formal proof is suppressed. The number of conﬁgurations in A is |Q| × |Γ|n, hence the growth in the number of states when constructing A from A might be in the worst case exponential in the number of registers. In other words, the move from FSAs to FSRAs can yield an exponential reduction in the size of the network. As we show below, the reduction in the number of states can be even more dramatic. The FSRA model deﬁned above allows only one register operation on each transition. We extend it to allow up to k register operations on each transition, where k is determined for each automaton separately. The register operations are deﬁned as a sequence (rather than a set), in order to allow more than one operation on the same  Figure 6 FSRA for circumﬁxation. 58  Cohen-Sygal and Wintner  Non-Concatenative Morphology  Figure 7 FSRA for the pattern hit a e .  register over one transition. This extension allows further reduction of the network size for some automata as well as other advantages that will be discussed presently.  Deﬁnition An order-k ﬁnite-state registered automaton (FSRA-k) is a tuple A = Q, q0, Σ, Γ, n, k, δ, F , where:  r Q, q0, Σ, Γ, n, F and the initial content of the registers are as before. r k ∈ N (indicating the maximum number of register operations allowed  on each arc).  r Let ActionsΓn = {R, W} × {0, 1, 2, . . . , n − 1} × Γ. Then   k δ⊆Q×Σ∪{ }×   a1, ..., aj | for all i, 1 ≤ i ≤ j, ai ∈ ActionsΓn  × Q  j=1  is the transition relation. δ is extended to allow each transition to be associated with a series of up to k operations on the registers. Each operation has the same meaning as before.  The register operations are executed in the order in which they are speciﬁed. Thus, (s, σ, a1, ..., ai , t) ∈ δ where i ≤ k implies that if A is in state s, the input symbol is σ and all the register operations a1, ..., ai are executed successfully, then A may enter state t. Deﬁnition Given a ∈ ActionsΓn we deﬁne a relation over Γn, denoted u a v for u, v ∈ Γn. We deﬁne u a v where u = u0 . . . un−1 and v = v0 . . . vn−1 iff the following holds: r if a = (R, i, γ) for some i, 0 ≤ i ≤ n − 1 and for some γ ∈ Γ, then u = v and ui = vi = γ. r if a = (W, i, γ) for some i, 0 ≤ i ≤ n − 1 and for some γ ∈ Γ, then for all k ∈ {0, 1, . . . , n − 1} such that k = i, uk = vk and vi = γ. This relation is extended to series over ActionsΓn . Given a series a1, ..., ap ∈ (ActionsΓn )p where p ∈ N, we deﬁne a relation over Γn denoted u a1,...,ap v for u, v ∈ Γn. We deﬁne u a1,...,ap v iff the following holds: r if p = 1, then u a1 v. r if p > 1, then there exists w ∈ Γn such that u a1 w and w a2,...,ap v.  59  Computational Linguistics  Volume 32, Number 1  Deﬁnition Let u, v ∈ Γn. Given a symbol α ∈ Σ ∪ { } and an FSRA-k A, we say that a conﬁguration (s, u) produces a conﬁguration (t, v), denoted (s, u) α,A (t, v), iff there exist a1, . . . , ap ∈ (ActionsΓn )p for some p ∈ N such that (s, α, a1, . . . , ap , t) ∈ δ and u a1,...,ap v. Deﬁnition A run of A on w is a sequence of conﬁgurations c0, ..., cr such that c0 = qc0, cr ∈ Fc, and for every l, 1 ≤ l ≤ r, cl−1 αl,A cl and w = α1...αr. An FSRA-k A accepts a word w if there exists a run of A on w. The language recognized by an FSRA-k A, denoted by L(A), is the set of words over Σ∗ accepted by A. Example 7 Consider the Arabic nouns qamar (moon), kitaab (book), $ams (sun), and daftar (notebook). The deﬁnite article in Arabic is the preﬁx al, which is realized as al when preceding most consonants; however, the ‘l’ of the preﬁx assimilates to the ﬁrst consonant of the noun when the latter is ‘d’, ‘$’, etc. Furthermore, Arabic distinguishes between deﬁnite and indeﬁnite case markers. For example, nominative case is realized as the sufﬁx u on deﬁnite nouns, un on indeﬁnite nouns. Examples of the different forms of Arabic nouns are:  word qamar kitaab $ams daftar  nominative deﬁnite ’alqamaru ’alkitaabu ’a$$amsu ’addaftaru  nominative indeﬁnite qamarun kitaabun $amsun daftarun  The FSRA-2 of Figure 8 accepts all the nominative deﬁnite and indeﬁnite forms of the above nouns. In order to account for the assimilation, register 2 stores information about the actual form of the deﬁnite article. Furthermore, to ensure that deﬁnite nouns occur with the correct case ending, register 1 stores information of whether or not a deﬁnite article was seen.  Figure 8 FSRA-2 for Arabic nominative deﬁnite and indeﬁnite nouns. 60  Cohen-Sygal and Wintner  Non-Concatenative Morphology  FSRA-k and FSRAs recognize the same class of languages. Trivially, every FSRA has an equivalent FSRA-k: Every FSRA is an FSRA-k for k = 1. The other direction is also simple.  Theorem 2 Every FSRA-k has an equivalent FSRA. We show how to construct an equivalent FSRA (or FSRA-1) A given an FSRA-k A. Each transition in A is replaced by a series of transitions in A’, each of which performs one operation on the registers. The ﬁrst transition in the series deals with the new input symbol and the rest are -transitions. This construction requires additional states to enable the addition of transitions. Each transition in A that is replaced requires the addition of as many states as the number of register operations performed on this transition minus one. The formal construction is suppressed. In what follows, the term FSRA will be used to denote FSRA-k. Simple FRSA will be referred to as FSRA-1. For the sake of emphasis, however, the term FSRA-k will still be used in some cases. FSRA is a very space-efﬁcient ﬁnite-state device. The next theorem shows how ordinary ﬁnite-state automata can be encoded efﬁciently by the FSRA-2 model. Given a ﬁnite-state automaton A, an equivalent FSRA-2 A is constructed. A has three states and two registers (in fact, only one register is used since register number 0 is never addressed). One state functions as a representative for the ﬁnal states in A, another one functions as a representative for the non-ﬁnal states in A, and the third as an initial state. The register alphabet consists of the states of A and the symbol ‘#’. Each arc in A has an equivalent arc in A with two register operations. The ﬁrst reads the current state of A from the register and the second writes the new state into the register. If the source state of a transition in A is a ﬁnal state, then the source state of the corresponding transition in A will be the ﬁnal states representative; if the source state of a transition in A is a non-ﬁnal state, then the source state of the corresponding transition in A will be the non-ﬁnal states representative. The same holds also for the target states. The purpose of the initial state is to write the start state of A into the register. In this way A simulates the behavior of A. Notice that the number of arcs in A equals the number of arcs in A plus one, i.e., while FSRAs can dramatically reduce the number of states, compared to standard FSAs, a reduction in the number of arcs is not guaranteed.  Theorem 3 Every ﬁnite-state automaton has an equivalent FSRA-2 with three states and two registers.  Proof 1 Let A = Q, q0, Σ, δ, F be an FSA and let f : Q → qf, qnf be a total function deﬁned by  f (q) =  qf qnf  q∈F q ∈/ F  61  Computational Linguistics  Volume 32, Number 1  Construct an FSRA-2 A = Q , q0, Σ , Γ , 2, 2, δ , F , where:  r Q = {q0, qnf, qf}. q0 is the initial state, qf is the ﬁnal states representative,  r  and qnf is the non-ﬁnal states representative Σ =Σ  r Γ = Q ∪ {#}  r F = {qf } r δ = {(f (s), σ, (R, 1, s), (W, 1, t) , f (t)) | (s, σ, t) ∈ δ}  ∪  {(q0, , (W, 1, q0) , f (q0))}  The formal proof that L(A) = L(A ) is suppressed.  3.2 Closure Properties The equivalence shown in the previous section between the classes of languages recognized by ﬁnite-state automata and ﬁnite-state registered automata immediately implies that ﬁnite-state registered automata maintain the closure properties of regular languages. Applying the regular operations to ﬁnite-state registered automata can be easily done by converting them ﬁrst into ﬁnite-state automata. However, as shown above, such a conversion may result in an exponential increase in the size of the automaton, invalidating the advantages of this model. Therefore, we show how some of these operations can be deﬁned directly for FSRAs. The constructions are mostly based on the standard constructions for FSAs with some essential modiﬁcations. In what follows, let A1 = Q1, q10, Σ1, Γ1, n1, k1, δ1, F1 and A2 = Q2, q20, Σ2, Γ2, n2, k2, δ2, F2 be ﬁnite-state registered automata. 3.2.1 Union. Two FSRAs, A1, A2, are unioned into an FSRA A in the same way as in FSAs: by adding a new initial state and connecting it with -arcs to each of the (former) initial states of A1, A2. The number of registers and the maximal number of register operations per arc in A is the maximum of the corresponding values in A1, A2. Notice that in any speciﬁc run of A, the computation goes through just one of the original automata; therefore the same set of registers can be used for strings of L(A1) or L(A2) as needed. 3.2.2 Concatenation. We show two different constructions of an FSRA A = Q, q0, Σ, Γ, n, k, δ, F to recognize L(A1) · L(A2). Concatenation in ﬁnite-state automata is achieved by leaving only the accepting states of the second automaton as accepting states and adding an -arc from every accepting state of the ﬁrst automaton to the initial state of the second automaton. Doing just this in FSRA is insufﬁcient because using the same registers might cause undesired effects: The result might be affected by the content left in the registers after dealing with a substring from L(A1). Thus, this basic construction is used with care. The ﬁrst alternative is to employ more registers in the FSRA. In this way when dealing with a substring from L(A1) the ﬁrst n1 registers are used, and when moving to deal with a substring from L(A2) the next n2 registers are used. The second alternative is to use additional register operations that clear the content of the registers before handling the next substring from L(A2). This solution may be less intuitive but will be instrumental for Kleene closure below.  62  Cohen-Sygal and Wintner  Non-Concatenative Morphology  3.2.3 Kleene Closure. The construction is based on the concatenation construction. Notice that it cannot be based on the ﬁrst alternative (adding registers) due to the fact that the number of iterations in Kleene star is not limited, and therefore the number of registers needed cannot be bounded. Thus, the second alternative is used: Register operations are added to delete the content of registers. The construction is done by turning the initial state into a ﬁnal one (if it is not already ﬁnal) and connecting each of the ﬁnal states to the initial state with an -arc that is associated with a register operation that deletes the contents of the registers, leaving them ready to handle the next substring. 3.2.4 Intersection. For the intersection construction, assume that A1 and A2 are -free (we show an algorithm for removing -arcs in Section 3.3.1). The following construction simulates the runs of A1 and A2 simultaneously. It is based on the basic construction for intersection of ﬁnite-state automata, augmented by a simulation of the registers and their behavior. Each transition is associated with two sequences of operations on the registers, one for each automaton. The number of the registers is the sum of the number of registers in the two automata. In the intersection automaton the ﬁrst n1 registers are designated to simulate the behavior of the registers of A1 and the next n2 registers simulate the behavior of A2. In this way a word can be accepted by the intersection automaton iff it can be accepted by each one of the automata separately. Notice that register operations from δ1 and δ2 cannot be associated with the same register. This guarantees that no information is lost during the simulation of the two intersected automata. 3.2.5 Complementation. Ordinary FSAs are trivially closed under complementation. However, given an FSA A whose language is L(A), the minimal FSA recognizing the complement of L(A) can be exponentially large. More precisely, for any integer n > 2, there exists a non-deterministic ﬁnite-state automaton (NFA) with n states A, such that any NFA that accepts the complement of L(A) needs at least 2n−2 states (Holzer and Kutrib 2002). We have no reason to believe that FSRAs will demonstrate a different behavior; therefore, we maintain that in the worst case, the best approach for complementing an FSRA would be to convert it into FSA and complement the latter. We therefore do not provide a dedicated construction for this operator. 3.3 Optimizations 3.3.1 -removal. An -arc in an FSRA is an arc of the form (s, , a , t) where a is used as a meta-variable over ActionsΓn + (i.e., a represents a vector of register operations). Notice that this kind of arc might occur in an FSRA by its deﬁnition. Given an FSRA that might contain -arcs, an equivalent FSRA without -arcs can be constructed. The construction is based on the algorithm for -removal in ﬁnite-state automata, but the register operations that are associated with the -arc have to be dealt with, and this requires some care. The resulting FSRA has one more state than the original, and some additional arcs may be added, too. The main problem is -loops; while these can be easily removed in standard FSAs, here such loops can be associated with register operations which must be accounted for. The number of possible sequences of register operations along an -loop is unbounded, but it is easy to prove that there are only ﬁnitely many equivalence classes of such sequences: Two sequences are in the same equivalence class if and only if they have the same effect on the state of the machine; since each machine has a ﬁnite number of conﬁgurations (see theorem 1), there are only ﬁnitely many such equivalence classes. Therefore, the basic idea behind the construction is as follows: If there exists an -path from q1 to q2 with the register operations a over its arcs, and an arc (q2, σ, b , q3) 63  Computational Linguistics  Volume 32, Number 1  Figure 9 removal paradigm.  where σ = , and an -path from q3 to q4 with the register operations c over its arcs, then the equivalent -free network will include the arcs (q2, σ, b , q3), (q1, σ, a, b , q3), (q2, σ, b, c , q4), and (q1, σ, a, b, c , q3), with all the -arcs removed. This is illustrated in Figure 9. Notice that if q1 and q2 are the same state, then states q2 and q3 will be connected by two parallel arcs differing in their associated register operations; the same holds for states q2 and q4. Similarly, when q3 and q4 are the same state. In addition to the above changes, special care is needed for the case in which the empty word is accepted by the original automaton. The formal construction is similar in spirit to the -removal paradigm in weighted automata (Mohri 2000), where weights along an -path need to be gathered. Therefore, we suppress the formal construction and the proof of its correctness.  3.3.2 Optimizing Register Operations. In FSRAs, traversing an arc depends not  only on the input symbol but also on satisfying the series of register operations.  Sometimes, a given series of register operations can never be satisﬁed, and thus  the arc to which it is attached cannot be traversed. For example, the series of reg-  ister operations (W, 1, a), (R, 1, b) can never be satisﬁed, hence an arc of the form  (q1, σ, (W, 1, a), (R, 1, b) , q2) is redundant. In addition, the constructions of Sections 3.2  and 3.3.1 might result in redundant states and arcs that can never be reached or can  never lead to a ﬁnal state. Moreover, in many cases a series of register operations can  be minimized into a shorter series with the same effect. For example, the series of  register operations (W, 1, a), (R, 1, a), (W, 1, b) is equal in its effect to the series (W, 1, b) .  Therefore, we show an algorithm for optimizing a given FSRA by minimizing the series  of register operations over its arcs and removing redundant arcs and states.  For a given FSRA A = Q, q0, Σ, Γ, n, δ, F , we construct an equivalent FSRA A = Q, q0, Σ, Γ, n, δ , F = Opt(A), such that δ is created from δ by removing redundant arcs and by optimizing all the series of register operations. We begin by deﬁning  ActionsΓn  + |i  as  the  subset  of  ActionsΓn + that consists only of operations over the  i-th register. Deﬁne a total function sati  :  ActionsΓn  + |i  −→ {true, false} by:  sati(a) =  true false  if there exist u, v ∈ Γn such that u otherwise  av  64  Cohen-Sygal and Wintner  Non-Concatenative Morphology  sati(a) = true iff the series of register operations a is satisﬁable, i.e., there exists a conﬁguration of register contents for which all the operations in the series can be executed successfully. Determining whether sati(a) = true by exhaustively checking all the vectors in Γn may be inefﬁcient. Therefore, we show a necessary and sufﬁcient condition for determining whether sati(a) = true for some a ∈ ActionsΓn +|i, which can be checked efﬁciently. In addition, this condition will be useful in optimizing the series of register operations as will be shown later. A series of register operations over the i-th register is not satisﬁable if either one of the following holds: r A write operation is followed by a read operation expecting a different value. r A read operation is immediately followed by a read operation expecting a different value.  Theorem 4 For all a = (op1, i, γ1), (op2, i, γ2), . . . , (ops, i, γs) ∈ ActionsΓn +|i, sati(a) = false if and only if either one of the following holds: 1. There exists k, 1 ≤ k < s, such that opk = W and there exists m, k < m ≤ s, such that opm = R, γk = γm and for all j, k < j < m, opj = R. 2. There exists k, 1 ≤ k < s, such that opk = opk+1 = R and γk = γk+1. Notice that if i = 0, then by the deﬁnition of FSRAs, all the register operations in the series are the same operation, which is (R, 0, #); and this operation can never fail. In addition, if all the operations in the series are write operations, then again, by the deﬁnition of FSRAs, these operations can never fail. If none of the two conditions of the theorem holds, then the series of register operations is satisﬁable. We now show how to optimize a series of operations over a given register. An optimized series is deﬁned only over satisﬁable series of register operations in the following way: r If all the operations are write operations, then leave only the last one (since it will overwrite all its predecessors). r If all the operations are read operations, then by theorem 4, they are all the same operation, and in this case just leave one of them. r If there are both read and write operations, then distinguish between two cases: – If the ﬁrst operation is a write operation, leave only the last write operation in the series. – If the ﬁrst operation is a read operation, leave the ﬁrst operation (which is read) and the last write operation in the series. If the last write operation writes into the register the same symbol that the read operation required, then the write is redundant; leave only the read operation. 65  Computational Linguistics  Volume 32, Number 1  Deﬁnition  Deﬁne a function mini :  ActionsΓn  + |i  −→  ActionsΓn +|i. Let a =  (op1, i, γ1), . . . , (ops, i,  γs) . If sati(a) = true then:  r If for all k, 1 ≤ k ≤ s, opk = W, deﬁne mini(a) = (W, i, γs) . r If for all k, 1 ≤ k ≤ s, opk = R then deﬁne mini(a) = (R, i, γs) . r If there exists m, 1 ≤ m ≤ s such that opm = W and if there exists t, 
Language Computer Corporation  An important problem in knowledge discovery from text is the automatic extraction of semantic relations. This paper presents a supervised, semantically intensive, domain independent approach for the automatic detection of part–whole relations in text. First an algorithm is described that identiﬁes lexico-syntactic patterns that encode part–whole relations. A difﬁculty is that these patterns also encode other semantic relations, and a learning method is necessary to discriminate whether or not a pattern contains a part–whole relation. A large set of training examples have been annotated and fed into a specialized learning system that learns classiﬁcation rules. The rules are learned through an iterative semantic specialization (ISS) method applied to noun phrase constituents. Classiﬁcation rules have been generated this way for different patterns such as genitives, noun compounds, and noun phrases containing prepositional phrases to extract part–whole relations from them. The applicability of these rules has been tested on a test corpus obtaining an overall average precision of 80.95% and recall of 75.91%. The results demonstrate the importance of word sense disambiguation for this task. They also demonstrate that different lexico-syntactic patterns encode different semantic information and should be treated separately in the sense that different clariﬁcation rules apply to different patterns. 1. Introduction The identiﬁcation of semantic relations in text is at the core of Natural Language Processing and many of its applications. Detecting semantic relations between various text segments, such as phrases, sentences, and discourse spans, is important for automatic text understanding (Rosario, Hearst, and Fillmore 2002; Lapata 2002; Morris and Hirst 2004). Furthermore, semantic relations represent the core elements in the organization of lexical semantic knowledge bases intended for inference purposes. Recently, there has been a renewed interest in text semantics as evidenced by the international  ∗ Computer Science Department, University of Illinois at Urbana-Champaign, Urbana, IL 61801, E-mail: girju@uiuc.edu. † Language Computer Corporation, 1701 N. Collins Blvd. Suite 2000, Richardson, TX 75080, E-mail: adriana@languagecomputer.com. ‡ Language Computer Corporation, 1701 N. Collins Blvd. Suite 2000, Richardson, TX 75080, E-mail: moldovan@languagecomputer.com. Submission received: 21 October 2003; revised submission received: 7 March 2005; accepted for publication: 1 August 2005. © 2006 Association for Computational Linguistics  Computational Linguistics  Volume 32, Number 1  participation in the Senseval 3 Semantic Roles competition,1 the associated workshops,2 and numerous other workshops. An important semantic relation for many applications is the part–whole relation, or meronymy. Let us notate the part–whole relation as PART(X, Y), where X is part of Y. For example, the compound nominal door knob contains the part–whole relation PART(knob, door). Part–whole relations occur frequently in text and are expressed by a variety of lexical constructions as illustrated in the text below. (1) The car’s mail messenger is busy at work in the mail car as the train moves along. Through the open side door of the car, moving scenery can be seen. The worker is alarmed when he hears an unusual sound. He peeks through the door’s keyhole leading to the tender and locomotive cab and sees the two bandits trying to break through the express car door.3 There are several part–whole relations in this text: 1) the mail car is part of the train, 2) the side door is part of the car, 3) the keyhole is part of the door, 4) the cab is part of the locomotive, 5) the tender is part of the train, 6) the locomotive is part of the train, 7) the door is part of the car, and 8) the car is part of the express train (in the compound noun express car door). This paper provides a supervised, knowledge-intensive method for the automatic detection of part–whole relations in English texts. Based on a set of positive (encoding meronymy) and negative (not encoding meronymy) training examples provided and annotated by us, the algorithm creates a decision tree and a set of rules that classify new data. The rules produce semantic conditions that the noun constituents matched by the patterns must satisfy in order to exhibit a part–whole relation. For the discovery of classiﬁcation rules we used C4.5 decision tree learning (Quinlan 1993). The learned function is represented by a decision tree transformed into a set of if–then rules. The decision tree learning searches a complete hypothesis space from simple to complex hypotheses until it ﬁnds a hypothesis consistent with the data. Its bias is a preference for the shorter tree that places high information gain attributes closer to the root. For training purposes we used WordNet, and the LA Times (TREC9)4 and SemCor 1.75 text collections. From these we formed a large corpus of 27,963 negative examples and 29,134 positive examples of well distributed subtypes of part–whole relationships which provided a comprehensive set of classiﬁcation rules. The rules were tested on  
 Computational Linguistics  Volume 32, Number 1  a comprehensive list of topics that are covered from “ﬁrst principles,” provides details about the computational environment that is needed to compile and execute the programs provided on the CD, and a listing of computer skills one would need to get started. Coleman encourages the reader/student (I will use student henceforth) not just to run the programs but to also to “tinker” with them in order to gain a deeper understanding of the way they work. Chapter 1 also lays out the structure of the text graphically in order to depict the dependencies among the chapters. In addition to the book chapters, there is an appendix on ASCII characters, a helpful glossary, a list of references, and a comprehensive index. Importantly, there is also a companion website with errata, solutions to selected exercises, bug reports, software updates, additional programs, links to third-party software, and some nice bibliography links. Presumably, this page will be updated over time. The overall chapter organization of the book is quite nice. Each chapter begins with a preview and a list of key terms (allowing the student an opportunity to look up the deﬁnitions prior to beginning to read the chapter content) and ends with a chapter summary, a set of exercises that are helpful for developing a deeper understanding of the materials discussed in the chapter, suggestions for further reading, and suggestions for readings to prepare for the next chapter. I will discuss chapters 2 through 9 in turn. Chapter 2 discusses issues related to the digital representation of a signal with a focus on the composition of a sound ﬁle and how such a ﬁle can be loaded into a soundediting program for audio display. The chapter starts off by guiding the student through the process of listening to a cosine waveform and then viewing the same ﬁle using a sound editing program such as Cool Edit 2000. The student is asked to ﬁll in a worksheet with values for a cosine function and then plot the values. Coleman then presents important information on the digital representation of sound and on sampling theory. Given this knowledge, the student is walked through the process of generating and playing a cosine wave. The chapter contains a just-in-time introduction to C sufﬁcient for a student to read and comprehend the cosine wave generation program coswave.c. Various computing terms (e.g., bit, compilation, machine code) are deﬁned, followed by a discussion of C numeric data types and differences in representation across architecture. The C code presented in this chapter makes concrete Coleman’s discussion of loops, arrays, calculation of mathematical expressions, overall program layout, and ﬁle output. The chapter ends with several helpful exercises. The ﬁrst provides a very detailed set of instructions for compiling and executing the coswave program and then playing the generated output signal in Cool Edit 2000. It should be noted that Cool Edit 2000 is not a public-domain package and is no longer available through the original developers. Alternatives mentioned on the text’s Web site (e.g., wavesurfer, Praat) can be used instead, although no details are offered about using them for the exercises. Students may face some challenges in opening and playing raw data ﬁles with these alternatives. Chapter 3 introduces methods for modifying the digital representations of sound; in particular, the concept of ﬁltering is introduced, followed by a very brief discussion of how ﬁlters are employed in a Klatt formant synthesizer. The chapter ﬁrst discusses how operations can be applied to number sequences in C to set the stage for discussion of several speech-processing applications. RMS energy is then deﬁned and a corresponding C program is discussed in detail. Next, a moving-average program is presented as an example of a low-pass ﬁlter. The concept of recursion is next introduced in order to pave the way for a discussion of IIR (Inﬁnite Impulse Response) ﬁlters. High-, low-, and band-pass ﬁlters are deﬁned and tables of coefﬁcients for various ﬁlters are provided. An implementation of an IIR ﬁlter is discussed quite brieﬂy; here the author relies on the  138  Book Reviews fact that there is similarity to the earlier moving-average program. Finally, after the basic introduction to ﬁlters, the Klatt synthesizer is discussed and a schematic diagram for the system is presented together with a brief discussion of the control parameters that are used to synthesize sound. IIR ﬁlters are tied in because they are used for modeling the glottal wave and ﬁlter-speciﬁc frequency components in order to obtain the resonant frequencies of the vocal tract required for the sound to be synthesized. A consonant– vowel example is used to demonstrate the synthesizer in action. There is a series of three exercises at the end of the chapter that should help the student get a better sense of ﬁlters and the type of sound generated by the Klatt synthesizer. The synthesizer exercises have a cookbook feel to them and give only a glimpse of what is needed to actually synthesize speech. At the end of the chapter, no further readings on ﬁlters are provided, although readings are recommended for the Klatt synthesizer and methods for estimating its parameters. Chapter 4 discusses several programs to extract acoustic parameters from a speech signal. First up is the fast Fourier transform (FFT), for which a C implementation is presented and described in detail. The student is asked to apply the compiled code to an example speech ﬁle in order to generate its spectrum, which is then plotted in Excel or Matlab for comparison to the spectral analysis obtained using Cool Edit. Given this example, there is a discussion of the types of peaks found in the spectrum, the resonances of the vocal tract, and the harmonics, as a prelude to the discussion of cepstral analysis. Coleman ﬁrst provides a high-level discussion of cepstral analysis, which employs an inverse FFT, followed by the discussion of its C implementation and an example using the executable. Cepstral analysis is then used to build a rudimentary pitch tracker, which is applied to a speech example. This leads to the discussion of a voicing detection algorithm. Next, the autocorrelation method for pitch tracking is presented together with its C implementation. Finally, the chapter discusses linear predictive coding (LPC) and various applications. The chapter ends with exercises to compare the cepstral and autocorrelation pitch trackers, to modify the output of the LPC program, and to analyze the vowels in a speech sample and use the LPC spectrum to estimate their formants. Additional readings are provided on the algorithms presented in this chapter. Chapter 5 offers a change of pace as the book introduces ﬁnite-state machines (FSMs) with a focus initially on symbolic language applications. There is a shift from C to Prolog, although it would have been perhaps more coherent to stick with C. The discussion of the peculiarities of Prolog could be distracting to a novice programmer. Furthermore, the representation of an FSM in Prolog is tedious to read, and it may be difﬁcult for the uninitiated to observe correspondences between the Prolog code and depictions of corresponding models. Simple examples are used to introduce the concept of, rather than a formal deﬁnition of, FSMs. Issues of coverage, over-generation, determinism, and nondeterminism of an FSM are discussed brieﬂy. Although Coleman makes clear that backtracking is an issue for a nondeterministic FSM and notes that there are methods for converting such an FSM to a representationally equivalent deterministic form, existing tools that could be used for carrying out this conversion (e.g., the AT&T FSM library) are not discussed. Coleman next presents a Prolog implementation of an interesting English-like monosyllable FSM. A box is used to introduce a collection of facts about Prolog, and then there is a walk-through of the code. A nice set of exercises follows in which the student loads the FSM program and executes it in various ways, followed by a discussion of some examples of using the FSM to generate strings with particular constraints. A more formal presentation of FSMs is then provided together with a discussion of the state-transition-table representation. The chapter ends by 139  Computational Linguistics  Volume 32, Number 1  introducing the concept of ﬁnite-state transducers and providing several examples from various levels of processing, including phonetics, phonology, orthography, and syntax. Exercises at the end of the chapter build nicely upon the Prolog code already discussed. The suggested additional readings are appropriate, but perhaps too broad, as many are textbooks. 
One of the more important chapters is the second (Roux and du Plessis), which completes the policy section. It describes the South African government’s efforts to foster the development of language and speech technologies and resources, recently expressed as the “Human Language Technologies initiative” of 2004, exactly ten years since the eleven-language policy was established. The authors survey reports and developments over the decade, ﬁnding repeated themes such as an emphasis on government co-ordination and industry partnerships, promotion of multilingualism, and support of “historically marginalized languages” (page 29). The authors describe the general trend of government policy as treating HLT as primarily “a language affair,” that is, as “an instrument of language development and language promotion”; they identify a tension between such aims and “more general expectations” of HLT as a means of enhancing access to information. Generally critical of the present government in ∗ I am grateful to Dr. Nhlanhla Thwala (SOAS/Witwatersrand) for advice about the sociolinguistic and language policy situation in South Africa.  Computational Linguistics  Volume 32, Number 1  its pursuit of “political” aims, the authors prefer to see HLT as an integral part of an “information society” that strives for universal access and consequent social and economic advantages. This polarity, and the wide gap it exposes between positions, represents a weakness of the book as a whole: the absence of language communities as living entities, and the lack of research (and evidence) about their linguistic situations and needs. Chapters 3 to 6 cover automatic translation. Chapter 3 (Weber) describes a continuum of MT (machine translation) technologies, ranging from fully automated to those involving considerable human intervention at various stages. This is an important distinction, not only for the technologies themselves (for example, machine learning can crucially involve human supervision), but also for matching technologies to needs and the resources available, although there is little in the book about the latter. The chapter is a good summary of MT terms and concepts, and phases in its development; most interesting for language planners and policy-makers will be to learn that decades of MT research have been largely unsuccessful, with its ﬁrst paradigm — rule-based translation — largely abandoned as fruitless, and newer statistical methods yet unproven (several later chapters corroborate this conclusion). We ﬁnd here the ﬁrst grounds for doubt about the book’s ability to fulﬁll its aims: while the author provides three examples of translations, not only are they (as he explains) moderately unsuccessful, but they involve translation between major European languages; the language planner will wonder therefore what prospects there are for African languages, and may question why more relevant examples could not have been found. While the chapter offers a good, non-technical background to automated translation, hinting at the difﬁculties of translating meaning and context, unfortunately, as in most chapters, technology rarely gets considered in the context of its users. This may lead to problems; for example, tolerating “less-than-perfect results” may assist implementation but may be precarious in communities who do not have much experience with information technology (IT) or, perhaps as a result of that, expect perfection from it. The strategy of constraining translation to particular domains may not be relevant to community concerns or modes of information-seeking; here the weather-forecast domain is mentioned, in chapter 10 we ﬁnd IT job-postings to a Usenet newsgroup. Chapter 4 (Champollion) moves from technologies to speciﬁc tools and functions, and is another chapter that planners and policy makers should ﬁnd useful. While (nonautomated) translation is of interest to researchers, it remains the ugly duckling of HLT in terms of deliverables (useful tools). Paradoxically, it is widely approached with skepticism while being one of the few areas where HLT has brought productivity beneﬁts and “become part of a language practitioner’s daily life” (page 61). The chapter nicely describes several relatively humble but effective tools, such as a segmenter (assists mechanical tasks of aligning source and target sentences), translation memory (an interactive database of translation segments already made), and terminology banks. Such tools allow the human translator to focus on the language task and to leverage existing work. The reviewer is reminded of a dyslexic friend who many years ago found that even the most basic speech synthesis was a boon — he could hear and check the e-mail he was typing, opening up new possibilities for communication. Voice recognition has considerably improved, according to the author, but is strongly language-dependent; perhaps its adoption will depend on whether keypress or voice input technologies catch the imagination of a new generation. Messerschmidt et al. (chapter 5) provide a digestible account of the mechanisms of a rule-based parser and grammar that learn under human supervision, showing its application to Sesotho. This will be useful for non-specialists. Nevertheless, their  144  Book Reviews argument that HLTs are needed for African languages is based on a belief that HLTs for colonial languages (English and Afrikaans) would be sufﬁcient if only Africans had a better command of them. While colonial languages, especially English, continue to play signiﬁcant, even increasing, roles, especially in business and service delivery in South Africa, what is missing here is the larger picture: evidence about language and technology preferences within particular activities and domains, and acknowledgment that the context for technology development should at least include “capacity building,” including, for example, increasing IT training and employment for speakers of African languages. Short-term projects channeling the richer information currently available in English (e.g., about AIDS) into languages such as Sesotho may not provide a means of addressing future health threats. Given that current MT methods have matured but remain ineffective, Champollion’s second contribution (chapter 6) considers future possibilities for MT. Starting from a transcendental discussion on complexity (and the number pi), he moves between abstract discussion and concrete advice on corpus creation: Don’t attempt to replace “relevance with bulk, or quality with quantity” (page 83) — it is futile to build larger and larger translation databases in the expectation of enabling successful translation. He sees today’s methods as merely “pumps” that work on elements of “how humans think languages are organized” (page 83), and foresees a day when computers produce detailed fractal grammars in their own pattern-matching terms. The third section is more optimistic, with two chapters based on the use of parallel corpora. Marcu shows that statistical machine translation software using such corpora is not only at least as effective as rule-based methods but is cheaper to develop and, more importantly, adaptable to a variety of languages. However, while the software is generic, preparing parallel corpora, especially for pairs of “low density” languages (page 95), is difﬁcult (if not impossible), or may involve reducing content to the trivial or to domains that are usually dealt with in another language anyway. African languages appear for the ﬁrst time as the main focus in chapter 8 (Prinsloo and De Schryver). Parallel corpora can contribute to research in phonetics, linguistics, language change, and lexicography, as well as in practical applications such as language teaching and spelling checkers. The authors used widely available corpus software (WordSmith and ParaConc) to build collections of translation equivalents and extract document keywords, with careful evaluation of the results; however, the techniques have not yet been extended to the production of useful tools such as dictionaries and spelling checkers. Localization — the fourth theme — could have received more attention than a single chapter (van Rooy) classifying and evaluating student spelling errors in order to show that “linguistic resources should be localized for South African English.” Some useful concepts, such as precision and recall, are explained, and a comprehensive typology of errors is presented. The author reaches the unstartling conclusion that a spelling checker would be improved if it was provided with a list of place-names, acronyms, and frequently occurring errors. A reader (especially a planner or policy-maker) surely wants to know about spelling in African languages, about localized change that English is undergoing as it is adapted across different African communities (rather than the increasingly outdated concept of standard South African English), and about localization issues for African software and operating systems. The theme of text mining also receives a single chapter (Mooney and Nahm), which examines the discovery of relationships and patterns in both structured and unstructured documents. The authors developed a system that learns to perform 145  Computational Linguistics 
Computational Linguistics  Volume 32, Number 1  view that “a linguistic system is inherently probabilistic in nature.” Halliday sees no distinction between corpus linguistics and theoretical linguistics, adapting the principle that “frequency in text instantiates probability in the system.” But he criticizes early MT work that gets involved with “the counting of orthographic words in isolation” and believes that corpora should be as useful for grammarians as they have been for lexicologists. But “the grammar is much harder to get at” and “the main limitation on the use of corpuses [sic] for probabilistic grammar is the familiar catch that what is easy to recognize is usually too trivial to be worth recognizing.” So the joint work with James referred to above has to use a number of complex heuristics (in a way that will be familiar to computational linguists) to extract the desired grammatical information from the corpus. Recent developments in wide-coverage parsing and the exploitation of huge corpora should mean that the boundaries are moving very fast in this area, if there are grammarians out there willing to have a go . . . There are a number of intriguing ideas and comments to be found in the papers. For instance, there is the hypothesis (with some empirical support) that binary choice systems in languages tend to have either equal probabilities for the two options (“equiprobable”) or with probabilities around 0.1 and 0.9 (“skew”). Interestingly, the latter case corresponds to where the information-theoretic entropy is about 0.5 (which is also about the ﬁgure for the entropy for English in terms of characters, as calculated by Shannon and Weaver). Also there are some interesting speculations about how language changes that manifest themselves initially through probabilities might eventually lead to changes in the underlying systems themselves. Although I don’t think that computational linguists will ﬁnd any ideas in this book that will directly inspire computational implementation, I found it an interesting read that gave me a number of insights into the history of linguistics and into how Halliday’s thinking relates to computational issues. Halliday’s writing is erudite and clear, but it is also quite dense and uses terminology that has to be mastered. The examples and grammar fragments help a lot to make the ideas precise, but I’d have liked there to be more of these.  References Bateman, John. 1997. Enabling technology for multilingual natural language generation: The KPML development environment. Natural Language Engineering, 3(1):15–55. Brew, Chris. 1991. Systemic classiﬁcation and its efﬁciency. Computational Linguistics, 17(4):375–408. Davey, Anthony. 1978. Discourse Production: A Computer Model of Some Aspects of a Speaker. Edinburgh University Press, Edinburgh. Fawcett, Robin and Gordon Tucker. 1990. Demonstration of GENESYS: A very large, semantically based systemic functional generator. In Proceedings of the 13th International Conference on Computational Linguistics, pages 47–49, Helsinki. Halliday, Michael and Zoe James. 1993. A quantitative study of polarity and primary tense in the English ﬁnite clause.  In John Sinclair, Michael Hoey, and Gwyneth Fox, editors, Techniques of Description: Spoken and Written Discourse. Routledge. Mann, William. 1983. An overview of the PENMAN text generation system. In Proceedings of the National Conference on Artiﬁcial Intelligence, pages 261–265. American Association for Artiﬁcial Intelligence, August. Patten, Terry and Graeme Ritchie. 1987. A formal model of systemic grammar. In Gerard Kempen, editor, Natural Language Generation: New Results in Artiﬁcial Intelligence, Psychology and Linguistics. Martinus Nijhoff. Sugeno, Michio. 1995. Intelligent fuzzy computing. In Proceedings of PACLING II (Second Conference of the Paciﬁc Association of Computational Linguistics), Brisbane. Winograd, Terry. 1972. Understanding Natural Language. Edinburgh University Press, Edinburgh.  150  
The popularity of Aristotelian Logic retarded the advance of physical science throughout the Middle Ages. If only the schoolmen had measured instead of classifying, how much they might have learnt! (page 41)  Computational Linguistics  Volume 32, Number 1  On the other hand, Grassmann was recognized for his foundational work on analytical geometry; for example, Hermann Weyl (1949) comments: Today probably the best approach to analytic geometry is by means of the vector concept, following the procedure of Grassmann’s Ausdehnungslehre. (page 68) Weyl of course contributed extensively to quantum mechanics and already then held the view that “classical logic does not ﬁt in with quantum physics and is to be replaced by a kind of ‘quantum logic’ ” (page 263). More recently Suppes et al. (1989) refer to Grassmann’s ground-breaking work on geometrical structures, which are now called Grassmann Structures. I think the case for Grassmann’s obscurity is unproven. Chapters 1–6 are really an elementary introduction for those without much expertise in mathematics and logic to prepare them for the novel work presented on quantum logic and concept lattices in Chapters 7 and 8 (but see Courant and Robbins [1941] and Marciszewski [1981] for further introductory material). The elementary concepts are well illustrated with examples from search engines and with problems of ambiguity of words. The “killer application” for quantum logic in chapter 7 is the modeling of negation, and in chapter 8 the interpretation of taxonomic structures as non-Boolean lattices. In both cases the applications are convincing. The basis for the representation in quantum logic is the lattice of subspaces of a vector space endowed with a geometry which determines a logic (Birkhoff and von Neumann 1936). It is a pity that Widdows did not complete the story, which was started by Birkhoff and von Neumann, and introduce the probability measure on this lattice of subspaces. One of the striking results in quantum theory is that this can be done consistently and uniquely (Gleason 1957). Von Neumann was well aware of this although he could not prove it; in 1954, he said: In other words, probability corresponds precisely to introducing the angles geometrically. Furthermore, there is only one way to introduce it. The more so because in the quantum mechanical machinery the negation of a statement, so the negation of a statement which is represented by a linear set of vectors, corresponds to the orthogonal complement of this linear space. (von Neumann, 1954, reproduced in Re´dei and Sto¨lzner 2001, page 244) Had probability been introduced, and therefore the role of measurement, it would have helped explain the somewhat tantalizing expression on page 238: “It follows that every ‘experimental proposition’ in a quantum mechanical system corresponds to a subspace of the vector space in which the states of the system are represented mathematically.” It is curious that in the entire book, probability is mentioned only twice in passing. Returning to Whitehead and his beef about Aristotle, when he complains about Aristotle not “measuring,” one needs to take this seriously because the machinery of quantum mechanics and its observables only makes sense when one considers the observation of attributes with a probability of success or failure. Heisenberg’s uncertainty principle, one of the foundation stones of quantum mechanics, was about observables and their probabilistic interaction. A quantum logic without a theory of observation (or interaction) is somewhat empty. Readers might like to pursue this line of reasoning by consulting Beltrametti and Cassinelli (1981), one of the seminal works on quantum logic. Each of Widdows’ chapters ends with a delightful and useful “Wider Reading” section encouraging the reader to explore further aﬁeld; overall, these sections total 15 pages, or about 5% of the book. Below in the appendix to this review, I add a few references that complement some of Widdows’. 156  Book Reviews  In his foreword to the book, Pentti Kanerva suggests that the substance of the book is “the exploration of mathematics that would be appropriate for describing concepts and meaning.” For this reviewer, this is only part of the story; the book goes beyond exploration and applies the relevant mathematics to computational problems in linguistics and information retrieval. It may be the ﬁrst steps along the way to recasting some old problems in terms of some new mathematics.  Appendix: Wider Reading Possibly the most outstanding reference to numerical taxonomy is Sneath and Sokal (1973). For a more up-to-date and appropriate reference to information retrieval, I would propose Belew (2000). The use and application of non-classical logic in IR is well covered by Crestani et al (1998). An extremely relevant set of papers on quantum logic, in that they deal with taxonomy and various conditionals — for example, the Stalnaker conditional — are the papers by Hardegree, especially Hardegree (1976). His 1982 paper on natural kinds covers similar territory to Widdows’ discussion on “extent” and “intent” and its mathematical duality. An excellent source for papers on developments in quantum logic is Beltrametti and van Fraassen (1981). A critical view of the quantum logic enterprise is Gibbins (1987). To plug the gap on measurement in quantum mechanics one can do no better than Wheeler and Zurek (1983). Although Widdows does an excellent job of introducing most of the elementary concepts needed in his book, there is room for some more guidance on where to go next. An excellent introduction to the broad ﬁeld of mathematics, each chapter written by one of the masters of the ﬁeld, is Newman (1988). More speciﬁcally, good introductions to the foundations of vector spaces can be found in Halmos (1958), and Isham (1989). Finally, it may be worth pointing out that the late Jon Barwise, who also worked at CSLI, introduced quantum logic as part of his work on information ﬂow (Barwise and Seligman 1997).  
This paper describes the development of a rule-based computational model that describes how a feature-based representation of shared visual information combines with linguistic cues to enable effective reference resolution. This work explores a language-only model, a visualonly model, and an integrated model of reference resolution and applies them to a corpus of transcribed task-oriented spoken dialogues. Preliminary results from a corpus-based analysis suggest that integrating information from a shared visual environment can improve the performance and quality of existing discoursebased models of reference resolution. 
Many verbal jokes, like garden path sentences, pose difﬁculties to models of discourse since the initially primed interpretation needs to be discarded and a new one created based on subsequent statements. The effect of the joke depends on the fact that the second (correct) interpretation was not visible earlier. Existing models of discourse semantics in principle generate all interpretations of discourse fragments and carry these until contradicted, and thus the dissonance criteria in humour cannot be met. Computationally, maintaining all possible worlds in a discourse is very inefﬁcient, thus computing only the maximum-likelihood interpretation seems to be a more efﬁcient choice on average. In this work we outline a probabilistic lexicon based lexical semantics approach which seems to be a reasonable construct for discourse in general and use some examples from humour to demonstrate its working. 
This paper describes a system that produces extractive summaries of short works of literary fiction. The ultimate purpose of produced summaries is defined as helping a reader to determine whether she would be interested in reading a particular story. To this end, the summary aims to provide a reader with an idea about the settings of a story (such as characters, time and place) without revealing the plot. The approach presented here relies heavily on the notion of aspect. Preliminary results show an improvement over two naïve baselines: a lead baseline and a more sophisticated variant of it. Although modest, the results suggest that using aspectual information may be of help when summarizing fiction. A more thorough evaluation involving human judges is under way. 
Current Named Entity Recognition systems suffer from the lack of hand-tagged data as well as degradation when moving to other domain. This paper explores two aspects: the automatic generation of gazetteer lists from unlabeled data; and the building of a Named Entity Recognition system with labeled and unlabeled data. 
In the current project, we aim at developing an approach for automatically answering why-questions. We created a data collection for research, development and evaluation of a method for automatically answering why-questions (why-QA) The resulting collection comprises 395 why-questions. For each question, the source document and one or two user-formulated answers are available in the data set. The resulting data set is of importance for our research and it will contribute to and stimulate other research in the field of why-QA. We developed a question analysis method for why-questions, based on syntactic categorization and answer type determination. The quality of the output of this module is promising for future development of our method for why-QA. 
This paper addresses the problem of automatically retrieving answers for how-to questions, focusing on those that inquire about the procedure for achieving a specific goal. For such questions, typical information retrieval methods, based on key word matching, are better suited to detecting the content of the goal (e.g., ‘installing a Windows XP server’) than the general nature of the desired information (i.e., procedural, a series of steps for achieving this goal). We suggest dividing the process of retrieving answers for such questions into two stages, with each stage focusing on modeling one aspect of a how-to question. We compare the two-stage approach with two alternative approaches: a baseline approach that only uses the content of the goal to retrieve relevant documents and another approach that explores the potential of automatic query expansion. The result of the experiment shows that the two-stage approach significantly outperforms the baseline but achieves similar result with the systems using automatic query expansion techniques. We analyze the reason and also present some future work. 
This paper presents a way in which a lexicalised HPSG grammar can handle word order constraints in a computational parsing system, without invoking an additional layer of representation for word order, such as Reape’s Word Order Domain. The key proposal is to incorporate into lexical heads the WOC (Word Order Constraints) feature, which is used to constrain the word order of its projection. We also overview our parsing algorithm. 
This paper presents results from experiments in automatic classiﬁcation of animacy for Norwegian nouns using decision-tree classiﬁers. The method makes use of relative frequency measures for linguistically motivated morphosyntactic features extracted from an automatically annotated corpus of Norwegian. The classiﬁers are evaluated using leave-oneout training and testing and the initial results are promising (approaching 90% accuracy) for high frequency nouns, however deteriorate gradually as lower frequency nouns are classiﬁed. Experiments attempting to empirically locate a frequency threshold for the classiﬁcation method indicate that a subset of the chosen morphosyntactic features exhibit a notable resilience to data sparseness. Results will be presented which show that the classiﬁcation accuracy obtained for high frequency nouns (with absolute frequencies >1000) can be maintained for nouns with considerably lower frequencies (∼50) by backing off to a smaller set of features at classiﬁcation. 
Metonymy recognition is generally approached with complex algorithms that rely heavily on the manual annotation of training and test data. This paper will relieve this complexity in two ways. First, it will show that the results of the current learning algorithms can be replicated by the ‘lazy’ algorithm of Memory-Based Learning. This approach simply stores all training instances to its memory and classiﬁes a test instance by comparing it to all training examples. Second, this paper will argue that the number of labelled training examples that is currently used in the literature can be reduced drastically. This ﬁnding can help relieve the knowledge acquisition bottleneck in metonymy recognition, and allow the algorithms to be applied on a wider scale. 
The Web contains vast amounts of linguistic data. One key issue for linguists and language technologists is how to access it. Commercial search engines give highly compromised access. An alternative is to crawl the Web ourselves, which also allows us to remove duplicates and nearduplicates, navigational material, and a range of other kinds of non-linguistic matter. We can also tokenize, lemmatise and part-of-speech tag the corpus, and load the data into a corpus query tool which supports sophisticated linguistic queries. We have now done this for German and Italian, with corpus sizes of over 1 billion words in each case. We provide Web access to the corpora in our query tool, the Sketch Engine. 
This paper describes a multi-lingual phrase-based Statistical Machine Translation system accessible by means of a Web page. The user can issue translation requests from Arabic, Chinese or Spanish into English. The same phrase-based statistical technology is employed to realize the three supported language-pairs. New language-pairs can be easily added to the demonstrator. The Web-based interface allows the use of the translation system to any computer connected to the Internet. 
By presenting the LinguaStream platform, we introduce different methodological principles and analysis models, which make it possible to build hybrid experimental NLP systems by articulating corpus processing tasks. 
We demonstrate a new development environment1 “Information State Update” dialogue systems which allows non-expert developers to produce complete spoken dialogue systems based only on a Business Process Model (BPM) describing their application (e.g. banking, cinema booking, shopping, restaurant information). The environment includes automatic generation of Grammatical Framework (GF) grammars for robust interpretation of spontaneous speech, and uses application databases to generate lexical entries and grammar rules. The GF grammar is compiled to an ATK or Nuance language model for speech recognition. The demonstration system allows users to create and modify spoken dialogue systems, starting with a deﬁnition of a Business Process Model and ending with a working system. This paper describes the environment, its main components, and some of the research issues involved in its development. 
In this paper1 we introduce eXtensible MetaGrammar, a system that facilitates the development of tree based grammars. This system includes both (1) a formal language adapted to the description of linguistic information and (2) a compiler for this language. It applies techniques of logic programming (e.g. Warren’s Abstract Machine), thus providing an efﬁcient and theoretically motivated framework for the processing of linguistic metadescriptions. 
This paper describes an unsupervised knowledge–lean methodology for automatically determining the number of senses in which an ambiguous word is used in a large corpus. It is based on the use of global criterion functions that assess the quality of a clustering solution. 
Many current sentence generators lack the ability to compute elliptical versions of coordinated clauses in accordance with the rules for Gapping, Forward and Backward Conjunction Reduction, and SGF (Subject Gap in clauses with Finite/Fronted verb). We describe a module (implemented in JAVA, with German and Dutch as target languages) that takes non-elliptical coordinated clauses as input and returns all reduced versions licensed by coordinative ellipsis. It is loosely based on a new psycholinguistic theory of coordinative ellipsis proposed by Kempen. In this theory, coordinative ellipsis is not supposed to result from the application of declarative grammar rules for clause formation but from a procedural component that interacts with the sentence generator and may block the overt expression of certain constituents. 
We demonstrate a multimodal dialogue system using reinforcement learning for in-car scenarios, developed at Edinburgh University and Cambridge University for the TALK project1. This prototype is the ﬁrst “Information State Update” (ISU) dialogue system to exhibit reinforcement learning of dialogue strategies, and also has a fragmentary clariﬁcation feature. This paper describes the main components and functionality of the system, as well as the purposes and future use of the system, and surveys the research issues involved in its construction. Evaluation of this system (i.e. comparing the baseline system with handcoded vs. learnt dialogue policies) is ongoing, and the demonstration will show both. 
This demo abstract describes the SmartWeb Ontology-based Annotation system (SOBA). A key feature of SOBA is that all information is extracted and stored with respect to the SmartWeb Integrated Ontology (SWIntO). In this way, other components of the systems, which use the same ontology, can access this information in a straightforward way. We will show how information extracted by SOBA is visualized within its original context, thus enhancing the browsing experience of the end user. 
Esfinge is a general domain Portuguese question answering system. It tries to take advantage of the great amount of information existent in the World Wide Web. Since Portuguese is one of the most used languages in the web and the web itself is a constantly growing source of updated information, this kind of techniques are quite interesting and promising. 
We present a novel application of NLP and text mining to the analysis of financial documents. In particular, we describe an implemented prototype, Maytag, which combines information extraction and subject classification tools in an interactive exploratory framework. We present experimental results on their performance, as tailored to the financial domain, and some forward-looking extensions to the approach that enables users to specify classifications on the fly. 
We describe a system for automatic annotation of English text in the FrameNet standard. In addition to the conventional annotation of frame elements and their semantic roles, we annotate additional semantic information such as support verbs and prepositions, aspectual markers, copular verbs, null arguments, and slot ﬁllers. As far as we are aware, this is the ﬁrst system that ﬁnds this information automatically. 
The problem we address in this paper is that of providing contextual examples of translation equivalents for words from the general lexicon using comparable corpora and semantic annotation that is uniform for the source and target languages. For a sentence, phrase or a query expression in the source language the tool detects the semantic type of the situation in question and gives examples of similar contexts from the target language corpus. 
Extending a machine learning based coreference resolution system with a feature capturing automatically generated information about semantic roles improves its performance. 
GOD (General Ontology Discovery) is an unsupervised system to extract semantic relations among domain speciﬁc entities and concepts from texts. Operationally, it acts as a search engine returning a set of true predicates regarding the query instead of the usual ranked list of relevant documents. Our approach relies on two basic assumptions: (i) paradigmatic relations can be established only among terms in the same Semantic Domain an (ii) they can be inferred from texts by analyzing the Subject-Verb-Object patterns where two domain speciﬁc terms co-occur. A qualitative analysis of the system output shows that GOD provide true, informative and meaningful relations in a very efﬁcient way. 
Term translation probabilities proved an effective method of semantic smoothing in the language modelling approach to information retrieval tasks. In this paper, we use Generalized Latent Semantic Analysis to compute semantically motivated term and document vectors. The normalized cosine similarity between the term vectors is used as term translation probability in the language modelling framework. Our experiments demonstrate that GLSAbased term translation probabilities capture semantic relations between terms and improve performance on document classiﬁcation. 
This paper quantitatively investigates in how far local context is useful to disambiguate the senses of an ambiguous word. This is done by comparing the co-occurrence frequencies of particular context words. First, one context word representing a certain sense is chosen, and then the co-occurrence frequencies with two other context words, one of the same and one of another sense, are compared. As expected, it turns out that context words belonging to the same sense have considerably higher co-occurrence frequencies than words belonging to different senses. In our study, the sense inventory is taken from the University of South Florida homograph norms, and the co-occurrence counts are based on the British National Corpus. 
In this paper, we propose an approach for identifying curatable articles from a large document set. This system considers three parts of an article (title and abstract, MeSH terms, and captions) as its three individual representations and utilizes two domain-specific resources (UMLS and a tumor name list) to reveal the deep knowledge contained in the article. An SVM classifier is trained and cross-validation is employed to find the best combination of representations. The experimental results show overall high performance. 
We describe our initial investigations into generating textual summaries of spatiotemporal data with the help of a prototype Natural Language Generation (NLG) system that produces pollen forecasts for Scotland. 
This paper deals with the problem of recognizing and extracting acronymdeﬁnition pairs in Swedish medical texts. This project applies a rule-based method to solve the acronym recognition task and compares and evaluates the results of different machine learning algorithms on the same task. The method proposed is based on the approach that acronym-deﬁnition pairs follow a set of patterns and other regularities that can be usefully applied for the acronym identiﬁcation task. Supervised machine learning was applied to monitor the performance of the rule-based method, using Memory Based Learning (MBL). The rule-based algorithm was evaluated on a hand tagged acronym corpus and performance was measured using standard measures recall, precision and fscore. The results show that performance could further improve by increasing the training set and modifying the input settings for the machine learning algorithms. An analysis of the errors produced indicates that further improvement of the rulebased method requires the use of syntactic information and textual pre-processing. 
Morphologically complex terms composed from Greek or Latin elements are frequent in scientiﬁc and technical texts. Word forming units are thus relevant cues for the identiﬁcation of terms in domainspeciﬁc texts. This article describes a method for the automatic extraction of terms relying on the detection of classical preﬁxes and word-initial combining forms. Word-forming units are identiﬁed using a regular expression. The system then extracts terms by selecting words which either begin or coalesce with these elements. Next, terms are grouped in families which are displayed as a weighted list in HTML format. 
This paper reports the present results of a research on unsupervised Persian morpheme discovery. In this paper we present a method for discovering the morphemes of Persian language through automatic analysis of corpora. We utilized a Minimum Description Length (MDL) based algorithm with some improvements and applied it to Persian corpus. Our improvements include enhancing the cost function using some heuristics, preventing the split of high frequency chunks, exploiting penalty for first and last letters and distinguishing pre-parts and post-parts. Our improved approach has raised the precision, recall and f-measure of discovery by respectively %32, %17 and %23. 
 In this paper we present LX-Suite, a set of tools for the shallow processing of Portuguese. This suite comprises several modules, namely: a sentence chunker, a tokenizer, a POS tagger, featurizers and lemmatizers.  
We analyze estimation methods for DataOriented Parsing, as well as the theoretical criteria used to evaluate them. We show that all current estimation methods are inconsistent in the “weight-distribution test”, and argue that these results force us to rethink both the methods proposed and the criteria used. 
 In this paper, we present a formalization of grammatical role labeling within the framework of Integer Linear Programming (ILP). We focus on the integration of subcategorization information into the decision making process. We present a ﬁrst empirical evaluation that achieves competitive precision and recall rates.  
This paper describes a study in which a corpus of spoken Danish annotated with focus and topic tags was used to investigate the relation between information structure and pauses. The results show that intra-clausal pauses in the focus domain, tend to precede those words that express the property or semantic type whereby the object in focus is distinguished from other ones in the domain. 
The NLP systems often have low performances because they rely on unreliable and heterogeneous knowledge. We show on the task of non-anaphoric it identiﬁcation how to overcome these handicaps with the Bayesian Network (BN) formalism. The ﬁrst results are very encouraging compared with the state-of-the-art systems. 
Most question answering (QA) and information retrieval (IR) systems are insensitive to different users’ needs and preferences, and also to the existence of multiple, complex or controversial answers. We introduce adaptivity in QA and IR by creating a hybrid system based on a dialogue interface and a user model. Keywords: question answering, information retrieval, user modelling, dialogue interfaces. 
We report work1 in progress on adding affect-detection to an existing program for virtual dramatic improvisation, monitored by a human director. To partially automate the directors’ functions, we have partially implemented the detection of emotions, etc. in users’ text input, by means of pattern-matching, robust parsing and some semantic analysis. The work also involves basic research into how affect is conveyed by metaphor. 
We describe a method for discovering irregularities in temporal mood patterns appearing in a large corpus of blog posts, and labeling them with a natural language explanation. Simple techniques based on comparing corpus frequencies, coupled with large quantities of data, are shown to be effective for identifying the events underlying changes in global moods. 
I propose a uniform approach to the elimination of redundancy in CCG lexicons, where grammars incorporate inheritance hierarchies of lexical types, deﬁned over a simple, feature-based category description language. The resulting formalism is partially ‘constraint-based’, in that the category notation is interpreted against an underlying set of tree-like feature structures. I argue that this version of CCG subsumes a number of other proposed category notations devised to allow for the construction of more efﬁcient lexicons. The formalism retains desirable properties such as tractability and strong competence, and provides a way of approaching the problem of how to generalise CCG lexicons which have been automatically induced from treebanks. 
We present a new method for detecting and disambiguating named entities in open domain text. A disambiguation SVM kernel is trained to exploit the high coverage and rich structure of the knowledge encoded in an online encyclopedia. The resulting model signiﬁcantly outperforms a less informed baseline. 
We present a weakly supervised approach to automatic Ontology Population from text and compare it with other two unsupervised approaches. In our experiments we populate a part of our ontology of Named Entities. We considered two high level categories - geographical locations and person names and ten sub-classes for each category. For each sub-class, from a list of training examples and a syntactically parsed corpus, we automatically learn a syntactic model - a set of weighted syntactic features, i.e. words which typically co-occur in certain syntactic positions with the members of that class. The model is then used to classify the unknown Named Entities in the test set. The method is weakly supervised, since no annotated corpus is used in the learning process. We achieved promising results, i.e. 65% accuracy, outperforming signiﬁcantly previous unsupervised approaches. 
In this paper we study a set of problems that are of considerable importance to Statistical Machine Translation (SMT) but which have not been addressed satisfactorily by the SMT research community. Over the last decade, a variety of SMT algorithms have been built and empirically tested whereas little is known about the computational complexity of some of the fundamental problems of SMT. Our work aims at providing useful insights into the the computational complexity of those problems. We prove that while IBM Models 1-2 are conceptually and computationally simple, computations involving the higher (and more useful) models are hard. Since it is unlikely that there exists a polynomial time solution for any of these hard problems (unless P = NP and P#P = P), our results highlight and justify the need for developing polynomial time approximations for these computations. We also discuss some practical ways of dealing with complexity. 
This paper describes a novel method for computing a consensus translation from the outputs of multiple machine translation (MT) systems. The outputs are combined and a possibly new translation hypothesis can be generated. Similarly to the well-established ROVER approach of (Fiscus, 1997) for combining speech recognition hypotheses, the consensus translation is computed by voting on a confusion network. To create the confusion network, we produce pairwise word alignments of the original machine translation hypotheses with an enhanced statistical alignment algorithm that explicitly models word reordering. The context of a whole document of translations rather than a single sentence is taken into account to produce the alignment. The proposed alignment and voting approach was evaluated on several machine translation tasks, including a large vocabulary task. The method was also tested in the framework of multi-source and speech translation. On all tasks and conditions, we achieved signiﬁcant improvements in translation quality, increasing e. g. the BLEU score by as much as 15% relative. 
 We propose a backoff model for phrasebased machine translation that translates unseen word forms in foreign-language text by hierarchical morphological abstractions at the word and the phrase level. The model is evaluated on the Europarl corpus for German-English and FinnishEnglish translation and shows improvements over state-of-the-art phrase-based models.  
We present an implemented machine learning system for the automatic detection of nonreferential it in spoken dialog. The system builds on shallow features extracted from dialog transcripts. Our experiments indicate a level of performance that makes the system usable as a preprocessing ﬁlter for a coreference resolution system. We also report results of an annotation study dealing with the classiﬁcation of it by naive subjects. 
In this paper, we explore statistical language modelling for a speech-enabled MP3 player application by generating a corpus from the interpretation grammar written for the application with the Grammatical Framework (GF) (Ranta, 2004). We create a statistical language model (SLM) directly from our interpretation grammar and compare recognition performance of this model against a speech recognition grammar compiled from the same GF interpretation grammar. The results show a relative Word Error Rate (WER) reduction of 37% for the SLM derived from the interpretation grammar while maintaining a low in-grammar WER comparable to that associated with the speech recognition grammar. From this starting point we try to improve our artiﬁcially generated model by interpolating it with different corpora achieving great reduction in perplexity and 8% relative recognition improvement. 
To tackle the problem of presenting a large number of options in spoken dialogue systems, we identify compelling options based on a model of user preferences, and present tradeoffs between alternative options explicitly. Multiple attractive options are structured such that the user can gradually reﬁne her request to ﬁnd the optimal tradeoff. We show that our approach presents complex tradeoffs understandably, increases overall user satisfaction, and signiﬁcantly improves the user’s overview of the available options. Moreover, our results suggest that presenting users with a brief summary of the irrelevant options increases users’ conﬁdence in having heard about all relevant options. 
We investigate a series of graph-theoretic constraints on non-projective dependency parsing and their effect on expressivity, i.e. whether they allow naturally occurring syntactic constructions to be adequately represented, and efﬁciency, i.e. whether they reduce the search space for the parser. In particular, we deﬁne a new measure for the degree of non-projectivity in an acyclic dependency graph obeying the single-head constraint. The constraints are evaluated experimentally using data from the Prague Dependency Treebank and the Danish Dependency Treebank. The results indicate that, whereas complete linguistic coverage in principle requires unrestricted non-projective dependency graphs, limiting the degree of non-projectivity to at most 2 can reduce average running time from quadratic to linear, while excluding less than 0.5% of the dependency graphs found in the two treebanks. This is a substantial improvement over the commonly used projective approximation (degree 0), which excludes 15–25% of the graphs. 
In this paper we extend the maximum spanning tree (MST) dependency parsing framework of McDonald et al. (2005c) to incorporate higher-order feature representations and allow dependency structures with multiple parents per word. We show that those extensions can make the MST framework computationally intractable, but that the intractability can be circumvented with new approximate parsing algorithms. We conclude with experiments showing that discriminative online learning using those approximate algorithms achieves the best reported parsing accuracy for Czech and Danish. 
 This paper presents results from the ﬁrst statistical dependency parser for Turkish. Turkish is a free-constituent order language with complex agglutinative inﬂectional and derivational morphology and presents interesting challenges for statistical parsing, as in general, dependency relations are between “portions” of words – called inﬂectional groups. We have explored statistical models that use different representational units for parsing. We have used the Turkish Dependency Treebank to train and test our parser but have limited this initial exploration to that subset of the treebank sentences with only left-to-right non-crossing dependency links. Our results indicate that the best accuracy in terms of the dependency relations between inﬂectional groups is obtained when we use inﬂectional groups as units in parsing, and when contexts around the dependent are employed. 
An algorithm based on the Generalized Hebbian Algorithm is described that allows the singular value decomposition of a dataset to be learned based on single observation pairs presented serially. The algorithm has minimal memory requirements, and is therefore interesting in the natural language domain, where very large datasets are often used, and datasets quickly become intractable. The technique is demonstrated on the task of learning word and letter bigram pairs from text. 
Probabilistic Latent Semantic Analysis (PLSA) models have been shown to provide a better model for capturing polysemy and synonymy than Latent Semantic Analysis (LSA). However, the parameters of a PLSA model are trained using the Expectation Maximization (EM) algorithm, and as a result, the trained model is dependent on the initialization values so that performance can be highly variable. In this paper we present a method for using LSA analysis to initialize a PLSA model. We also investigated the performance of our method for the tasks of text segmentation and retrieval on personal-size corpora, and present results demonstrating the efﬁcacy of our proposed approach. 
In recent years tree kernels have been proposed for the automatic learning of natural language applications. Unfortunately, they show (a) an inherent super linear complexity and (b) a lower accuracy than traditional attribute/value methods. In this paper, we show that tree kernels are very helpful in the processing of natural language as (a) we provide a simple algorithm to compute tree kernels in linear average running time and (b) our study on the classiﬁcation properties of diverse tree kernels show that kernel combinations always improve the traditional methods. Experiments with Support Vector Machines on the predicate argument classiﬁcation task provide empirical support to our thesis. 
The degree of dominance of a sense of a word is the proportion of occurrences of that sense in text. We propose four new methods to accurately determine word sense dominance using raw text and a published thesaurus. Unlike the McCarthy et al. (2004) system, these methods can be used on relatively small target texts, without the need for a similarly-sensedistributed auxiliary text. We perform an extensive evaluation using artiﬁcially generated thesaurus-sense-tagged data. In the process, we create a word–category cooccurrence matrix, which can be used for unsupervised word sense disambiguation and estimating distributional similarity of word senses, as well. 
In this paper a novel solution to automatic and unsupervised word sense induction (WSI) is introduced. It represents an instantiation of the ‘one sense per collocation’ observation (Gale et al., 1992). Like most existing approaches it utilizes clustering of word co-occurrences. This approach differs from other approaches to WSI in that it enhances the effect of the one sense per collocation observation by using triplets of words instead of pairs. The combination with a two-step clustering process using sentence co-occurrences as features allows for accurate results. Additionally, a novel and likewise automatic and unsupervised evaluation method inspired by Schu¨tze’s (1992) idea of evaluation of word sense disambiguation algorithms is employed. Offering advantages like reproducability and independency of a given biased gold standard it also enables automatic parameter optimization of the WSI algorithm. 
This work is concerned with the space of alignments searched by word alignment systems. We focus on situations where word re-ordering is limited by syntax. We present two new alignment spaces that limit an ITG according to a given dependency parse. We provide D-ITG grammars to search these spaces completely and without redundancy. We conduct a careful comparison of ﬁve alignment spaces, and show that limiting search with an ITG reduces error rate by 10%, while a D-ITG produces a 31% reduction. 
We describe a word alignment platform which ensures text pre-processing (tokenization, POS-tagging, lemmatization, chunking, sentence alignment) as required by an accurate word alignment. The platform combines two different methods, producing distinct alignments. The basic word aligners are described in some details and are individually evaluated. The union of the individual alignments is subject to a filtering postprocessing phase. Two different filtering methods are also presented. The evaluation shows that the combined word alignment contains 10.75% less errors than the best individual aligner. 
Aligning sentences belonging to comparable monolingual corpora has been suggested as a ﬁrst step towards training text rewriting algorithms, for tasks such as summarization or paraphrasing. We present here a new monolingual sentence alignment algorithm, combining a sentence-based TF*IDF score, turned into a probability distribution using logistic regression, with a global alignment dynamic programming algorithm. Our approach provides a simpler and more robust solution achieving a substantial improvement in accuracy over existing systems. 
We present results on addressee identiﬁcation in four-participants face-to-face meetings using Bayesian Network and Naive Bayes classiﬁers. First, we investigate how well the addressee of a dialogue act can be predicted based on gaze, utterance and conversational context features. Then, we explore whether information about meeting context can aid classiﬁers’ performances. Both classiﬁers perform the best when conversational context and utterance features are combined with speaker’s gaze information. The classiﬁers show little gain from information about meeting context. 
This paper proposes a method for dealing with repairs in action control dialogue to resolve participants’ misunderstanding. The proposed method identiﬁes the repair target based on common grounding rather than surface expressions. We extend Traum’s grounding act model by introducing degree of groundedness, and partial and mid-discourse unit grounding. This paper contributes to achieving more natural human-machine dialogue and instantaneous and ﬂexible control of agents. 
In this paper, we address the problem of reducing the unpredictability of userinitiated dialogue contributions in humancomputer interaction without explicitly restricting the user’s interactive possibilities. We demonstrate that it is possible to identify conditions under which particular classes of user-initiated contributions will occur and discuss consequences for dialogue system design. 
Opinion mining is a recent subdiscipline of computational linguistics which is concerned not with the topic a document is about, but with the opinion it expresses. To aid the extraction of opinions from text, recent work has tackled the issue of determining the orientation of “subjective” terms contained in text, i.e. deciding whether a term that carries opinionated content has a positive or a negative connotation. This is believed to be of key importance for identifying the orientation of documents, i.e. determining whether a document expresses a positive or negative opinion about its subject matter. We contend that the plain determination of the orientation of terms is not a realistic problem, since it starts from the nonrealistic assumption that we already know whether a term is subjective or not; this would imply that a linguistic resource that marks terms as “subjective” or “objective” is available, which is usually not the case. In this paper we confront the task of deciding whether a given term has a positive connotation, or a negative connotation, or has no subjective connotation at all; this problem thus subsumes the problem of determining subjectivity and the problem of determining orientation. We tackle this problem by testing three different variants of a semi-supervised method previously proposed for orientation detection. Our results show that determining subjectivity and orientation is a much harder problem than determining orientation alone. 
We propose models for semantic orientations of phrases as well as classiﬁcation methods based on the models. Although each phrase consists of multiple words, the semantic orientation of the phrase is not a mere sum of the orientations of the component words. Some words can invert the orientation. In order to capture the property of such phrases, we introduce latent variables into the models. Through experiments, we show that the proposed latent variable models work well in the classiﬁcation of semantic orientations of phrases and achieved nearly 82% classiﬁcation accuracy. 
Many of the tasks required for semantic tagging of phrases and texts rely on a list of words annotated with some semantic features. We present a method for extracting sentiment-bearing adjectives from WordNet using the Sentiment Tag Extraction Program (STEP). We did 58 STEP runs on unique non-intersecting seed lists drawn from manually annotated list of positive and negative adjectives and evaluated the results against other manually annotated lists. The 58 runs were then collapsed into a single set of 7, 813 unique words. For each word we computed a Net Overlap Score by subtracting the total number of runs assigning this word a negative sentiment from the total of the runs that consider it positive. We demonstrate that Net Overlap Score can be used as a measure of the words degree of membership in the fuzzy category of sentiment: the core adjectives, which had the highest Net Overlap scores, were identiﬁed most accurately both by STEP and by human annotators, while the words on the periphery of the category had the lowest scores and were associated with low rates of inter-annotator agreement. 
In this paper, we present an automated, quantitative, knowledge-poor method to evaluate the randomness of a collection of documents (corpus), with respect to a number of biased partitions. The method is based on the comparison of the word frequency distribution of the target corpus to word frequency distributions from corpora built in deliberately biased ways. We apply the method to the task of building a corpus via queries to Google. Our results indicate that this approach can be used, reliably, to discriminate biased and unbiased document collections and to choose the most appropriate query terms. 
We propose a method for compiling bilingual terminologies of multi-word terms (MWTs) for given translation pairs of seed terms. Traditional methods for bilingual terminology compilation exploit parallel texts, while the more recent ones have focused on comparable corpora. We use bilingual corpora collected from the web and tailor made for the seed terms. For each language, we extract from the corpus a set of MWTs pertaining to the seed’s semantic domain, and use a compositional method to align MWTs from both sets. We increase the coverage of our system by using thesauri and by applying a bootstrap method. Experimental results show high precision and indicate promising prospects for future developments. 
Web text has been successfully used as training data for many NLP applications. While most previous work accesses web text through search engine hit counts, we created a Web Corpus by downloading web pages to create a topic-diverse collection of 10 billion words of English. We show that for context-sensitive spelling correction the Web Corpus results are better than using a search engine. For thesaurus extraction, it achieved similar overall results to a corpus of newspaper text. With many more words available on the web, better results can be obtained by collecting much larger web corpora. 
Most state-of-the-art evaluation measures for machine translation assign high costs to movements of word blocks. In many cases though such movements still result in correct or almost correct sentences. In this paper, we will present a new evaluation measure which explicitly models block reordering as an edit operation. Our measure can be exactly calculated in quadratic time. Furthermore, we will show how some evaluation measures can be improved by the introduction of word-dependent substitution costs. The correlation of the new measure with human judgment has been investigated systematically on two different language pairs. The experimental results will show that it signiﬁcantly outperforms state-of-the-art approaches in sentence-level correlation. Results from experiments with word dependent substitution costs will demonstrate an additional increase of correlation between automatic evaluation measures and human judgment. 
We argue that the machine translation community is overly reliant on the Bleu machine translation evaluation metric. We show that an improved Bleu score is neither necessary nor sufﬁcient for achieving an actual improvement in translation quality, and give two signiﬁcant counterexamples to Bleu’s correlation with human judgments of quality. This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores. 
 We present an adaptive technique that enables users to produce a high quality dictionary parsed into its lexicographic components (headwords, pronunciations, parts of speech, translations, etc.) using an extremely small amount of user provided training data. We use transformationbased learning (TBL) as a postprocessor at two points in our system to improve performance. The results using two dictionaries show that the tagging accuracy increases from 83% and 91% to 93% and 94% for individual words or “tokens”, and from 64% and 83% to 90% and 93% for contiguous “phrases” such as deﬁnitions or examples of usage. 
Faced with the problem of annotation errors in part-of-speech (POS) annotated corpora, we develop a method for automatically correcting such errors. Building on top of a successful error detection method, we ﬁrst try correcting a corpus using two off-the-shelf POS taggers, based on the idea that they enforce consistency; with this, we ﬁnd some improvement. After some discussion of the tagging process, we alter the tagging model to better account for problematic tagging distinctions. This modiﬁcation results in signiﬁcantly improved performance, reducing the error rate of the corpus. 
In this paper, we investigate the problem of automatically predicting segment boundaries in spoken multiparty dialogue. We extend prior work in two ways. We ﬁrst apply approaches that have been proposed for predicting top-level topic shifts to the problem of identifying subtopic boundaries. We then explore the impact on performance of using ASR output as opposed to human transcription. Examination of the effect of features shows that predicting top-level and predicting subtopic boundaries are two distinct tasks: (1) for predicting subtopic boundaries, the lexical cohesion-based approach alone can achieve competitive results, (2) for predicting top-level boundaries, the machine learning approach that combines lexical-cohesion and conversational features performs best, and (3) conversational cues, such as cue phrases and overlapping speech, are better indicators for the toplevel prediction task. We also ﬁnd that the transcription errors inevitable in ASR output have a negative impact on models that combine lexical-cohesion and conversational features, but do not change the general preference of approach for the two tasks. 
Detection of discourse structure is crucial in many text-based applications. This paper presents an original framework for describing textual parallelism which allows us to generalize various discourse phenomena and to propose a unique method to recognize them. With this prospect, we discuss several methods in order to identify the most appropriate one for the problem, and evaluate them based on a manually annotated corpus. 
Given the growing complexity of tasks that spoken dialogue systems are trying to handle, Reinforcement Learning (RL) has been increasingly used as a way of automatically learning the best policy for a system to make. While most work has focused on generating better policies for a dialogue manager, very little work has been done in using RL to construct a better dialogue state. This paper presents a RL approach for determining what dialogue features are important to a spoken dialogue tutoring system. Our experiments show that incorporating dialogue factors such as dialogue acts, emotion, repeated concepts and performance play a signiﬁcant role in tutoring and should be taken into account when designing dialogue systems. 
We present a model for sentence compression that uses a discriminative largemargin learning framework coupled with a novel feature set deﬁned on compressed bigrams as well as deep syntactic representations provided by auxiliary dependency and phrase-structure parsers. The parsers are trained out-of-domain and contain a signiﬁcant amount of noise. We argue that the discriminative nature of the learning algorithm allows the model to learn weights relative to any noise in the feature set to optimize compression accuracy directly. This differs from current state-of-the-art models (Knight and Marcu, 2000) that treat noisy parse trees, for both compressed and uncompressed sentences, as gold standard when calculating model parameters. 
We present and compare two approaches to the task of summarizing evaluative arguments. The ﬁrst is a sentence extractionbased approach while the second is a language generation-based approach. We evaluate these approaches in a user study and ﬁnd that they quantitatively perform equally well. Qualitatively, however, we ﬁnd that they perform well for different but complementary reasons. We conclude that an effective method for summarizing evaluative arguments must effectively synthesize the two approaches. 
We consider the evaluation problem in Natural Language Generation (NLG) and present results for evaluating several NLG systems with similar functionality, including a knowledge-based generator and several statistical systems. We compare evaluation results for these systems by human domain experts, human non-experts, and several automatic evaluation metrics, including NIST, BLEU, and ROUGE. We ﬁnd that NIST scores correlate best (> 0.8) with human judgments, but that all automatic metrics we examined are biased in favour of generators that select on the basis of frequency alone. We conclude that automatic evaluation of NLG systems has considerable potential, in particular where high-quality reference texts and only a small number of human evaluators are available. However, in general it is probably best for automatic evaluations to be supported by human-based evaluations, or at least by studies that demonstrate that a particular metric correlates well with human judgments in a given domain. 
This paper discusses two problems that arise in the Generation of Referring Expressions: (a) numeric-valued attributes, such as size or location; (b) perspective-taking in reference. Both problems, it is argued, can be resolved if some structure is imposed on the available knowledge prior to content determination. We describe a clustering algorithm which is sufﬁciently general to be applied to these diverse problems, discuss its application, and evaluate its performance. 
In this paper we present TroFi (Trope Finder), a system for automatically classifying literal and nonliteral usages of verbs through nearly unsupervised word-sense disambiguation and clustering techniques. TroFi uses sentential context instead of selectional constraint violations or paths in semantic hierarchies. It also uses literal and nonliteral seed sets acquired and cleaned without human supervision in order to bootstrap learning. We adapt a word-sense disambiguation algorithm to our task and augment it with multiple seed set learners, a voting schema, and additional features like SuperTags and extrasentential context. Detailed experiments on hand-annotated data show that our enhanced algorithm outperforms the baseline by 24.4%. Using the TroFi algorithm, we also build the TroFi Example Base, an extensible resource of annotated literal/nonliteral examples which is freely available to the NLP research community. 
We investigate the lexical and syntactic ﬂexibility of a class of idiomatic expressions. We develop measures that draw on such linguistic properties, and demonstrate that these statistical, corpus-based measures can be successfully used for distinguishing idiomatic combinations from non-idiomatic ones. We also propose a means for automatically determining which syntactic forms a particular idiom can appear in, and hence should be included in its lexical representation. 
We present the psycholinguistically motivated task of predicting human plausibility judgements for verb-role-argument triples and introduce a probabilistic model that solves it. We also evaluate our model on the related role-labelling task, and compare it with a standard role labeller. For both tasks, our model beneﬁts from classbased smoothing, which allows it to make correct argument-speciﬁc predictions despite a severe sparse data problem. The standard labeller suffers from sparse data and a strong reliance on syntactic cues, especially in the prediction task. 
We describe an implementation of datadriven selection of emphatic facial displays for an embodied conversational agent in a dialogue system. A corpus of sentences in the domain of the target dialogue system was recorded, and the facial displays used by the speaker were annotated. The data from those recordings was used in a range of models for generating facial displays, each model making use of a different amount of context or choosing displays differently within a context. The models were evaluated in two ways: by cross-validation against the corpus, and by asking users to rate the output. The predictions of the cross-validation study differed from the actual user ratings. While the cross-validation gave the highest scores to models making a majority choice within a context, the user study showed a signiﬁcant preference for models that produced more variation. This preference was especially strong among the female subjects. 
Multimodal grammars provide an expressive formalism for multimodal integration and understanding. However, handcrafted multimodal grammars can be brittle with respect to unexpected, erroneous, or disﬂuent inputs. Spoken language (speech-only) understanding systems have addressed this issue of lack of robustness of hand-crafted grammars by exploiting classiﬁcation techniques to extract ﬁllers of a frame representation. In this paper, we illustrate the limitations of such classiﬁcation approaches for multimodal integration and understanding and present an approach based on edit machines that combine the expressiveness of multimodal grammars with the robustness of stochastic language models of speech recognition. We also present an approach where the edit operations are trained from data using a noisy channel model paradigm. We evaluate and compare the performance of the hand-crafted and learned edit machines in the context of a multimodal conversational system (MATCH). 
The Arabic language is a collection of spoken dialects with important phonological, morphological, lexical, and syntactic differences, along with a standard written language, Modern Standard Arabic (MSA). Since the spoken dialects are not ofﬁcially written, it is very costly to obtain adequate corpora to use for training dialect NLP tools such as parsers. In this paper, we address the problem of parsing transcribed spoken Levantine Arabic (LA). We do not assume the existence of any annotated LA corpus (except for development and testing), nor of a parallel corpus LAMSA. Instead, we use explicit knowledge about the relation between LA and MSA. 
We place synchronous tree-adjoining grammars and tree transducers in the single overarching framework of bimorphisms, continuing the uniﬁcation of synchronous grammars and tree transducers initiated by Shieber (2004). Along the way, we present a new deﬁnition of the tree-adjoining grammar derivation relation based on a novel direct inter-reduction of TAG and monadic macro tree transducers. Tree transformation systems such as tree transducers and synchronous grammars have seen renewed interest, based on a perceived relevance to new applications, such as importing syntactic structure into statistical machine translation models or founding a formalism for speech command and control. The exact relationship among a variety of formalisms has been unclear, with a large number of seemingly unrelated formalisms being independently proposed or characterized. An initial step toward unifying the formalisms was taken (Shieber, 2004) in making use of the formallanguage-theoretic device of bimorphisms, previously used to characterize the tree relations deﬁnable by tree transducers. In particular, the tree relations deﬁnable by synchronous tree-substitution grammars (STSG) were shown to be just those deﬁnable by linear complete bimorphisms, thereby providing for the ﬁrst time a clear relationship between synchronous grammars and tree transducers. In this work, we show how the bimorphism framework can be used to capture a more powerful formalism, synchronous tree-adjoining grammars, providing a further uniting of the various and disparate formalisms.  After some preliminaries (Section 1), we begin by recalling the deﬁnition of tree-adjoining grammars and synchronous tree-adjoining grammars (Section 2). We turn then to a set of known results relating context-free languages, tree homomorphisms, tree automata, and tree transducers to extend them for the tree-adjoining languages (Section 3), presenting these in terms of restricted kinds of functional programs over trees, using a simple grammatical notation for describing the programs. This allows us to easily express generalizations of the notions: monadic macro tree homomorphisms, automata, and transducers, which bear (at least some of) the same interrelationships that their traditional simpler counterparts do (Section 4). Finally, we use this characterization to place the synchronous TAG formalism in the bimorphism framework (Section 5), further unifying tree transducers and other synchronous grammar formalisms. We also, in passing, provide a new characterization of the relation between TAG derivation and derived trees, and a new simpler and more direct proof of the equivalence of TALs and the output languages of monadic macro tree transducers. 
Carsim is a program that automatically converts narratives into 3D scenes. Carsim considers authentic texts describing road accidents, generally collected from web sites of Swedish newspapers or transcribed from hand-written accounts by victims of accidents. One of the program’s key features is that it animates the generated scene to visualize events. To create a consistent animation, Carsim extracts the participants mentioned in a text and identiﬁes what they do. In this paper, we focus on the extraction of temporal relations between actions. We ﬁrst describe how we detect time expressions and events. We then present a machine learning technique to order the sequence of events identiﬁed in the narratives. We ﬁnally report the results we obtained. 
All questions are implicitly associated with an expected answer type. Unlike previous approaches that require a predeﬁned set of question types, we present a method for dynamically constructing a probability-based answer type model for each different question. Our model evaluates the appropriateness of a potential answer by the probability that it ﬁts into the question contexts. Evaluation is performed against manual and semiautomatic methods using a ﬁxed set of answer labels. Results show our approach to be superior for those questions classiﬁed as having a miscellaneous answer type. 
We propose an approach for extracting relations between entities from biomedical literature based solely on shallow linguistic information. We use a combination of kernel functions to integrate two different information sources: (i) the whole sentence where the relation appears, and (ii) the local contexts around the interacting entities. We performed experiments on extracting gene and protein interactions from two different data sets. The results show that our approach outperforms most of the previous methods based on syntactic and semantic information. 
Unsupervised paraphrase acquisition has been an active research ﬁeld in recent years, but its effective coverage and performance have rarely been evaluated. We propose a generic paraphrase-based approach for Relation Extraction (RE), aiming at a dual goal: obtaining an applicative evaluation scheme for paraphrase acquisition and obtaining a generic and largely unsupervised conﬁguration for RE. We analyze the potential of our approach and evaluate an implemented prototype of it using an RE dataset. Our ﬁndings reveal a high potential for unsupervised paraphrase acquisition. We also identify the need for novel robust models for matching paraphrases in texts, which should address syntactic complexity and variability. 
Translation Memory (TM) systems have been under the spotlight of translation technology research led by both software developers and academic institutions. Both ends try to find ways to maximize the benefits deriving from the use of these tools, whether those translate into productivity enhancements or cost savings. The involvement of the user in these efforts has always been problematic. It is usually too costly, it delays the development of the product because it takes time, and it requires a well designed mechanism to be in place that facilitates the communication between the user and the developer. Naturally, many developers cannot afford to set up such capability, thus they risk producing TM tools that fail to correspond to the needs of translation professionals. The Translation Memories Survey 2006 (abbr. TM Survey 2006), reported in this paper, was initiated with a view to acting as this very channel of information deriving from users (or potential users) of TM systems. The main purpose behind it is to present the users perspective about TM systems and to supply data on the application domain, that is, information on the procedural aspects of the translation activity, on frequent work practices and on the tasks related to TM systems. It reports on the factors that affect TM use and offers an evaluation of the most commonly used systems according to functional and non-functional criteria. The results also reveal a range of future directions in TM research as those are envisioned by translation professionals. 1. Introduction Fifteen years ago the first commercial Translation Memory tools were made available to translation professionals with the aim of improving their working life. Since then, TM tools have maintained their objective of assisting and accelerating the translation process (and many of its surrounding activities), but the technology employed has passed through various levels of sophistication in a constant quest for optimal solutions to match the translation industry's demands. As the years go by, the technology is not the only thing which has been evolving. The needs and expectations of the users of TM systems have been evolving too, either because the information 
MultiCorpora International Rue de STASSART 117 B-1050 Brussels, Belgium www.multicorpora.com  Copyright ©2006 MultiCorpora R&D Inc. All rights reserved. Revised: Oct-25-05 
Termprofile.com is a tool that helps you to search for terms on the Internet, distinguishing between the countries of origin of the hits, presenting relative frequency and comparative numbers for alternative terms in one language or different languages at a glance, TermProfile is one of the instruments that can significantly enhance interpreters' (and of course translators') workflow. TermProfile shows whether terms or expressions in a certain language are used in the respective countries. It offers an interface to perform three queries in parallel and then displays the search result figures on one screen. For each query it is possible to enter a second term ("control term") in order to obtain a relative frequency of a term. TermProfile offers a web-statistics profile of terms and expressions with dimensions and relations that far exceed a simple Google search. 1. THE SITUATION The Internet is becoming increasingly popular as an instrument for checking the commonness of terms. Translators, interpreters, journalists, technical editors, terminologists and authors all check the frequency of use of terms by entering them in search engines such as Google. The Internet is a huge text corpus of all languages, countries, topics, population groups - free-ofcharge and in real time. An opportunity. Search engines generally provide hits of all possible origins; there is no linguistic (and certainly not an editorial) check. A risk. This is because seemingly common terms can be based on faulty translations and thus be fairly common. 
This paper seeks to present the challenges and obstacles faced by academics from within universities as they attempt to reconcile humanities-based objectives with the pressing need expressed from the profession for translation students adequately prepared in technologies, including for localization projects. The global economy and globalization, in which information and knowledge and their transfer play a fundamental role, would seem to necessitate a more global approach to training and education and continuing professional career development. The exponential rise in the volume of translation worldwide, clearly resulting from the development and dissemination of computer, information and communication technologies, renders the successful introduction and implementation of technologies in translation and localization courses all the more urgent. How can this most effectively be done, particularly when the lines between training, education and business seem to be crossed more substantially and frequently than most academic institutions might initially feel comfortable with? Perhaps the answer lies within a new paradigm of collaboration. 1. Academic environments Academic environments are unique spaces. They thrive on the cultivation and implementation of ideas. They allow for the divulgation of information, circulation of knowledge and mentoring of skills by encouraging and facilitating productive exchanges within a wide community that intersects scholars, researchers, professors, part-time instructors, students, invited speakers, consultants, professional practitioners and many others. They afford participants in this community with an opportunity to step back from the fast-paced environment of the professional world and to contemplate product and the mechanisms of process in a different light, be it through relations of power, gender, race, class, economics, philosophy, cultural studies, and so on. In the best of scenarios, the research undertaken through the community can make a real difference in practice. The practice of translation has been carried out across the globe since the earliest of human times. Yet, translation and translator training and education within the infrastructure of the academy and academic community is a relatively recent phenomenon. The discipline in the humanities we currently know as "Translation Studies" has developed and evolved remarkably over the past 30 years. Significantly interdisciplinary in nature, it studies and teaches theoretical frameworks, conceptual models, practical 
 XML has many built-in capabilities to support the worldwide use of content. Proper use of these capabilities for the purpose of internationalization (i18n) and localization (l10n), however, sometimes requires considerable expertise. This holds especially for developers of XML schemas, and producers of XML instances (such as authors or translators). The Internationalization Tag Set Working Group (ITS WG) of the World Wide Web Consortium (W3C) is working on a standard which makes it easier to create XML which is internationalized and can be localized effectively. The standard has two dimensions: On the one hand, the standard identifies concepts (such as “directionality”) which are important for i18n and l10n. On the other hand, the standard defines implementations of these concepts (termed “ITS data categories”) as a set of elements and attributes called the Internationalization Tag Set (ITS). This paper explains the ITS motivation/context and history, basic ideas and mechanisms. It provides information on how to use ITS with new as well as with existing XML-based content. Furthermore, the paper depicts some processing contexts (such as transformation for localization purposes) in which ITS can be used. 
Denoual (2005) discovered that, contrary to popular belief, an EBMT system trained on heterogeneous data produced significantly better results than a system trained on homogeneous data. Using similar evaluation metrics and a few additional ones, in this paper we show that this does not hold true for the automated translation of subtitles. In fact, our system (when trained on homogeneous data) shows a relative increase of 74% BLEU in the language direction GermanEnglish and 86% BLEU English-German. Furthermore, we show that increasing the amount of heterogeneous data results in ‘bad examples’ being put forward as translation candidates, thus lowering the translation quality. 
 Maite Melero (maite.melero@upf.edu) GLiCom (Universitat Pompeu Fabra) Antoni Oliver (aoliverg@uoc.edu) LPG (Universitat Oberta de Catalunya) Toni Badia (toni.badia@upf.edu) GLiCom (Universitat Pompeu Fabra)  This paper presents the Multilingual Translation Service of eTITLE, a European eContent project, which has produced tools to assist in the multilingual subtitling of audiovisual material through the web. The eTITLE Translation Service combines state-of-the-art Machine translation and Translation memories, which may be tailored to the customer needs. The user can choose to use only Machine Translation, only a certain Translation Memory (or all available) or to use an integration of both approaches, which is the default option. In this latter option, only if no translation above a certain threshold is found in the Translation Memory, the original sentence is Machine Translated. The language pairs that have been developed in the framework of the eTITLE project are: EnglishSpanish, Spanish-English, English-Czech, Catalan-Spanish, Spanish-Catalan English-Catalan and Catalan-English. 1. Introduction  eTITLE is a two-year project that ended in February 2006 and has created web-based solutions that allow media content owners to exploit it internationally, through multilingual and cross-platform localisation. The project builds on a spectrum of newly available technologies for Digital Asset Management, Automated speech-to-text, Machine Translation, Sentence Compression, Subtitling Automation and Metadata Automation to provide a much more cost-effective digital workflow. Subtitling is, together with dubbing, the main form of translation or “language transfer” in television and other audio-visual environments. Language transfer involves more than facilitating the viewer’s comprehension of unfamiliar language. The European Commission has, for example, recommended subtitling as a means of improving knowledge of foreign languages within the European Union. With the continual ramping up of quantities of subtitling produced within the EU as a combination of regulatory changes and recent technological advances creating new channels requiring subtitling, broadcasters and media owners are aggressively seeking lower prices for the subtitle service. In this framework, the eTITLE approach may increase the efficiency of current subtitling by automating various processes within the workflow. The eTITLE project  
Dictionary publishers dealing with millions of bilingual and multilingual data have to find answers to the following two main issues: accommodating the needs of a still large dictionary users community for traditional printed or electronic dictionaries and meeting the need of the professional users beyond Machine Readable Dictionaries on CD-ROM or Online. For this purpose the German specialist dictionary publisher, Langenscheidt Fachverlag (LFG) proposes a global solution together with experts from the University of Rennes 2 and well-known Translation Memory providers. LexTerm is a methodology for reusing lexicographical data and building a bridge between dictionaries and terminology. 1. Meeting the actual professional translators' needs The specialised translation volumes are steadily increasing especially in the technical, economical and juridical fields. In the same time, companies and institutions which are the main order-givers have to cut down costs and in many cases the budget for language communication undergoes a very strict control. As a consequence, a lot of easy and repetitive translation tasks are achieved by non-professionals or translation software and professional translators are given the most difficult texts to translate for which they have to charge at a reasonable rate in order to keep getting orders. In this context, they have to optimise their workflow at a maximum and are looking for time-sparing tools and strategies. In the past decade, terminology management and translation memory providers have equipped them with most valuable translation management tools which enable them to compile their own dictionaries and to store their validated translated text-segments in order to reuse them in future translations. These tools are regularly improved in new versions which offer users a better comfort. On the other hand, dictionary publishers who have been for years the main tool providers for professional translators have digitalized their dictionary data and published several generations of electronic dictionaries, regularly adding new features in order to provide updated bilingual specialist terms.  The next challenge is to propose now a global solution with a unique tool which will ensure translators the reuse of their stored data together with an easy and quick access to the specialist terminology they never had translated before and therefore need. This unique tool: translation memory with “à la carte” integrated specialist dictionaries will save time-consuming internet researches and browsing in print or electronic dictionaries. 2. The proposed solution TMS providers and bilingual specialist dictionary publishers have the same target group but completely different approaches in data management and product marketing. The global solution we propose to translators means some major changes in the dictionary publishers' data processing and in TMS product concept. With the introduction of digital support, dictionary lifecycle has been considerably extended. The original manuscript has now become a unique source that can be accessed many times in order to be reused and even integrated in other language applications. For such data manipulations as integrating lexicographic data in terminological tools, contents have to be structured according to standards recognized by other professionals in order to avoid time-consuming and expensive data manipulations. The revised ISO standard 1951 due to be published next year aims to bridge the traditional methods of dictionary-making with the above mentioned future-oriented ones One of the main challenges of this project has been to convert thousands of lemma-oriented lexicographical data from bilingual specialist dictionaries into concept-oriented terminological data to be integrated in the translator's workbench. A method, concrete XML-models, encoding and finally a converter- described in 3. - have been developed by data-modelling experts from the University of Rennes 2 in France. As TMS providers wish to licence data from different specialist dictionary publishers and specialist dictionary publishers offer their data to different TMS providers, a concept-oriented data representation based on ISO standards has been chosen for smoothly data-exchanges. From now on this final data conversion has been added in the previous editorial work flow which allows publishing specialist dictionaries in all possible publishing devices from a single source. 3. The LexTerm initiative: a methodology for transforming lemma-oriented data into concept-oriented terminological entries Example of lexicographical entries 
Most successful machine translation systems built until now use proprietary software and data, and are either distributed as commercial products or are accessible on the net with some restrictions. This kind of machine translation systems are regarded by most professional translators and researchers as closed and static products which cannot be adapted or enhanced for a particular purpose. In contrast to these systems, we present Opentrad Apertium, an open-source shallow-transfer machine translation engine originally intended for related-language pairs but currently being extended to deal with not so similar pairs. The opportunities offered by open-source software are very interesting in new research projects but they are also promising in business and particularly as a business model for innovative companies. Questions like which is the most suitable license to release open-source software, or how to make it visible are discussed in this paper. Real opportunities for research and business derived from the Apertium machine translation system are presented as well. 
This paper aims at giving an overview of how an Enterprise Content Management system was chosen to support and steer production processes for the European Institutions. The ultimate aim is to include all of the company's production processes under the EMC Documentum hood (not only institutional). A pilot will be presented which has been running for over two years now for the European Parliament. It processes multilingual documents that come with very high turnaround times and feature a highly specialized language. The solution under scrutiny here is called escæpe (short for euroscript advanced production environment for document processing). 1. Introduction Outsourcing Document processing is an evolution in the market that is more and more noticeable. Where in the past, translation agencies were asked to virtually do everything, larger organizations understand that these requests are increasingly unfair. Especially those with a very strict or wellorganized hierarchy start realizing the problems that come with managing large document management projects. It is really about the very managing of the documents: from authoring to managing the whole production chain, to disseminating (publishing online or on paper, using for other purposes, re-using), archiving and destroying the data: they have understood that it is not as easy as meets the eye. In many organizations, the translation process is just considered a 'necessary evil'; something a secretary can do, and if (s)he cannot do, let her/him call a translation agency! The document that took months to write but only counts ten pages can be translated in a day. No? Well... this probably sounds familiar, and it is still true for many organizations. And even those companies that outsource their document related processes believe this, so how does one manage? The more we are involved in the very fabric of the process, the more we see things coming: imagine a process where authoring is done in a controlled environment with translation memories and validated vocabulary to match. What stops our translators from starting work right away? What they do is never lost and the fact that authoring takes place in a controlled environment, can only mean that the end product will not be far from what is being written. But how or why would an organization entrust a translation agency with the whole process? Well, that is easy: it would not. Unless said agency/company does not specialize in translations per se, but in the whole process that comes with it. This company hires technical writers who work very closely with the customer (most of the time onsite). They work exclusively in a controlled environment with a clear definition of validation. They have access to terminology and translation memories. They work in a 
In translation practice, typological differences between languages can pose problems in such a way that the translator has to compensate a source language structure which does not exist in the target language. It is, however, not always easy to find an adequate translation equivalent for such a lacking structure. The aim of this paper is to show how a multiply annotated and aligned corpus can be used as a translation memory for such typologically driven problems, exploiting the linguistic enrichment of the corpus. It is discussed how an existing translation corpus with high-quality translation and alignment is converted into a database and how this database can be exploited as a large on-line resource displaying various translation options for lexico-grammatical problems. 1. Introduction Up to now the annotation of translation corpora, i.e. their linguistic enrichment, has been carried out in order to empirically investigate the properties of translated text. On the other hand, practical translators also work with large amounts of translated texts, the enrichment of these parallel texts, however, being mostly limited to sentence alignment. The use of these aligned texts in translation memories is again limited to string-based queries (see section 2). There are, however, translation problems which are due to typological differences between languages. For translation training and practice it would, consequently, be good to have a database where many examples of these typological characteristics and their translations into other languages can be found. For English and German, raising structures, extractions and deletions are among others problematic constructions (cf. Hawkins 1986). Because here, the search space for a translational choice is rather wide, finding the German translation equivalent for such a construction is therefore a notorious problem for which a parallel concordance can provide help. While with a raw text corpus we can only formulate string searches, an annotated and aligned corpus allows us to query for lexico-grammatical patterns (see section 4). Additionally, we show how our corpus alignment can help to create multilingual term bases for translation practice and training. The advantage of this technique is that the translation candidates are extracted from published translations, i.e. language in use, and are thus more comprehensive and inventive than dictionary entries are.  The research described here is part of a pilot project called KOALA1 for which we use the CroCo corpus. The design of the corpus as well as its multidimensional annotation and alignment are described in section 3. It is shown how the multiply annotated and aligned corpus is imported into a database and how this database can be exploited to solve typical translation problems for English and German. Another crucial issue when dealing with large amounts of source language texts and their translations is the preservation of the meta-information of the texts (i.e. information on the author, translator, nationality and mother tongue of author and translator, publication date, etc.). In order to store and manage this kind of information for each text in the corpus, we developed a graphical user interface, CroCo-Meta. This tool allows the annotation of important meta-information in a user-friendly and fast manner (see section 5). Finally, we conclude the paper with summing up the advantages of our methodology and discussing some directions for future research (see section 6). 2. Corpora in translation studies and translation practice 2.1 Corpora in translation studies In translation studies, parallel corpora are used to analyse characteristic patterns of translations. Within this context Baker (1996) formulates the following hypotheses on the universal features of translations: explicitation (translations are more explicit than originals), simplification (translations are easier to understand and more readable), normalisation (translations strongly adhere to the usage norms of the target language) and levelling out (translations are more alike than the individual texts in a corpus of originals). These hypotheses are tested in several studies using TEC and BNC as comparable corpora: Laviosa-Braithwaite (1996) tests simplification and levelling out by analysing average sentence length, lexical density and type-token ratio. Baker/Olohan (2000) find evidence of explicitation in TEC investigating the use of that-connectives in contrast to zero-connectives of the reporting verbs say and tell (see Figure 1). Furthermore, Olohan tests the hypothesis of explicitation on the basis of the use of contractions (Olohan 2003) and optional syntactic elements in translations (Olohan 2004). Hansen (2003) finds evidence of normalisation in a tagged version of TEC and explains this result with the help of a psycholinguistic test. Based on comparable corpora, translation universals, such as explicitation, simplification or normalisation, are also tested for many other languages (e.g. Bernardini/Zanettin 2004 or Kujamäki/Mauranen 2004). Using a corpus of English source language texts, German translations, and comparable German originals, Hansen/Teich (1999) and Teich/Hansen (2001b) investigate the above mentioned translation features explicitation, simplification, normalisation and levelling out. The use of a combined parallel-comparable corpus allows them to identify the influence of the source language texts on the translations and on the target language. Teich (2003), for instance, finds shining-through (the typical language use of the source language “shines through” in the German translations), thus detecting a tendency contrary to normalisation. Parallel corpora are used for the investigation of information structure in English and German texts (e.g. Doherty 1999), thematic structure (Hasselgard 1998), information packaging (e.g. Steiner 2002, 2004 for English-German and Fabricius-Hansen 1999 for German-English and German-Norwegian) and again for the investigation of explicitation in English and Norwegian parallel texts (Johansson 1995). 
Medtronic is currently in the process of consolidating multiple distributed legacy product databases into one centrally managed SAP database. With a nine-figure budget, the Centerpiece effort is the largest and most visible IT project Medtronic has ever undertaken. One crucial part of this project is the translation - into eight languages - of existing descriptions for 50000 products, as well as approx. 200 new descriptions that are being added to the database every week. With both normalized source text and comprehensive, authoritative terminology in place, Medtronic is in an excellent position to use machine translation to produce translations of these product descriptions at the push of a button. This presentation illustrates the processes that allow Medtronic to produce translations in-house, instantly, at higher quality than previous human translations, and at a fraction of the cost of human translation. 1. Project Background 1.1. Product Database Consolidation With offices in more than 120 countries and annual revenues of more than 11 billion dollars, Medtronic is the world leader in medical technology. Since 2004, Medtronic has been consolidating multiple distributed legacy product databases into one centrally managed SAP database. With a budget in excess of 200 million dollars, the Centerpiece effort is the largest and most visible IT project Medtronic has ever undertaken. One critical part of this project is the translation into multiple languages of the existing descriptions for 50,000 products, plus approximately 200 new descriptions that are added to the product database every week.  2. Existing Translation Environment 2.1. Solution for Technical Literature The workflow for technical literature is highly automated and integrates a proprietary content management system (MAPS) with a customized, off-the-shelf workflow solution (Trados/SDL TeamWorks). This solution is geared towards translating structured XML output that is characterized by the following typical translation memory match rates: • 70% perfect matches • 23% fuzzy matches • 7% no matches This type of translation project is typically a low-volume job that is handled by in-house translators. 2.2. Solution for Marketing Literature The workflow for marketing literature is quality driven and uses a number of labor intensive processes to ensure the highest degree of customer satisfaction possible. Marketing translation documents are translated in industry-standard translation memory systems. This solution is characterized by the following typical translation memory match rates: • 10% perfect matches • 20% fuzzy matches • 70 % no matches This type of translation project is typically a medium-volume job that is handled by a small group of external translators, each of whom receives product training prior to each translation project. 2.3. Product Database Translation Does Not Fit Existing Workflows Translating Medtronic’s large product database is not a good fit for the two existing workflows. The automated workflow for technical literature relies heavily on leveraging matches from existing translation memories. As there would have been no translation memory matches for the initial round of translations, our human technical translators would have been overwhelmed by the sheer volume of work. Our quality-driven workflow for marketing literature was also not usable as marketing translators have a skill set that differs greatly from the one required to perform the monotonous task of translating tens of thousands of product descriptions.  3. Designing the Product Description Translation Environment 3.1. Client Sets Aggressive Goals In the discussions with the owners of the product database, the management team defined the following goals for the new translation process: • Improve translation quality over previous translation efforts • Reduce the cost of translation • Reduce the turnaround time • Reduce the involvement of marketing staff in the translation process • Automate the translation process as much as possible 3.2. Issues with Conventional Translation Tools 
Automatic acquisition of semantics from text has received quite some attention in natural language processing. A lot of research has been done by looking at syntactically similar contexts. For example, semantically related nouns can be clustered by looking at the collocating adjectives. There are, however, two major problems with this approach : computational complexity and data sparseness. This paper describes the application of a mathematical technique called singular value decomposition, which has been succesfully applied in Information Retrieval to counter these problems. It is investigated whether this technique is also able to cluster nouns according to latent semantic dimensions in a reduced adjective space.
To this day, the automatic recognition of metonymies has generally been addressed with supervised approaches. However, these require the annotation of a large number of training instances and hence, hinder the development of a wide-scale metonymy recognition system. This paper investigates if this knowledge acquisition bottleneck in metonymy recognition can be resolved by the application of unsupervised learning. Although the investigated technique, Sch{\"u
This presentation reports on an on-going project aimed at building a large lexical database of corpus-extracted multiword (MW) expressions for the Portuguese language. MW expressions were automatically extracted from a balanced 50 million word corpus compiled for this project, furthermore these were statistically interpreted using lexical association measures, followed by a manual validation process. The lexical database covers different types of MW expressions, from named entities to lexical associations with different degrees of cohesion, ranging from totally frozen idioms to favoured co-occurring forms, such as collocations. We aim to achieve two main objectives with this resource. Firstly to build on the large set of data of different types of MW expressions, thus revising existing typologies of collocations and integrating them in a larger theory of MW units. Secondly, to use the extensive hand-checked data as training data to evaluate existing statistical lexical association measures.
Human-computer interfaces require models of dialogue structure that capture the variability and unpredictability within dialogue. Semantic and pragmatic context are continuously evolving during conversation, especially by the distribution of turns that have a direct effect in dialogue exchanges. In this paper we use a formal language paradigm for modelling multi-agent system conversations. Our computational model combines pragmatic minimal units {--}speech acts{--} for constructing dialogues. In this framework, we show how turn-taking distribution can be ambiguous and propose an algorithm for solving it, considering turn coherence, trajectories and turn pairing. Finally, we suggest overlapping as one of the possible phenomena emerging from an unresolved turn-taking.
This paper suggests a novel Vietnamese segmentation approach for text categorization. Instead of using an annotated training corpus or a lexicon which are still lacking in Vietnamese, we use both statistical information extracted directly from a commercial search engine and a genetic algorithm to find the optimal routes to segmentation. The extracted information includes document frequency and n-gram mutual information. Our experiment results obtained on the segmentation and categorization of online news abstracts are very promising. It matches near 80 {\%} human judgment on segmentation and over 90 {\%} micro-averaging F1 in categorization. The processing time is less than one second per document when statistical information is cached.
Lexicon grammar is a systematic method for the analysis and the representation of the elementary sentence structures of a natural language producing large collections of syntactic electronic dictionaries or lexicongrammar tables (LGTs). In order to describe a language, very long term collaborative work is required. However, the current computer tools for the management of LGTs do not fulfill key requirements including automatic integration of multisource data, data coherence and version control, filtering and sorting, exchange formats, coupled management of data and documentation, dedicated graphical interfaces (GUIs) and user management and access control. In this paper we propose a solution based on PostgreSQL and/or MySQL (open source database management systems), Swing (a GUI toolkit for Java), JDBC (the API for Java database connectivity) and StAX (an API for the analysis and generation of XML documents).
We show that Mutual Information between word pairs can be successfully used to discriminate between word senses in the query translation step of Cross Language Information Retrieval. The experiment is conducted in the context of Amharic to French Cross Language Information Retrieval. We have performed a number of retrieval experiments in which we compare the performance of the sense discriminated and non-discriminated set of query terms against a ranked document collection. The results show an increased performance for the discriminated queries compared to the alternative approach, which uses the fully expanded set of terms.
In this paper, we present an inferential model for text type and genre identification of Web pages, where text types are inferred using a modified form of Bayes{'} theorem, and genres are derived using a few simple if-then rules. As the genre system on the Web is a complex phenomenon, and Web pages are usually more unpredictable and individualized than paper documents, we propose this approach as an alternative to unsupervised and supervised techniques. The inferential model allows a classification that can accommodate genres that are not entirely standardized, and is more capable of reading a Web page, which is mixed, rarely corresponding to an ideal type and often showing a mixture of genres or no genre at all. A proper evaluation of such a model remains an open issue.
Information retrieval (IR) consists in finding all relevant documents for a user query in a collection of documents. These documents are ordered by the probability of being relevant to the user{'}s query. The highest ranked document is considered to be the most likely relevant document. Natural Language Processing (NLP) for IR aims to transform the potentially ambiguous words of queries and documents into unambiguous internal representations on which matching and retrieval can take place. This transformation is generally achieved by several levels of linguistic analysis, morphological, syntactic and so forth. In this paper, we present the Arabic linguistic analyzer used in the LIC2M cross-lingual search engine. We focus on the morphological analyzer and particularly the clitic stemmer which segments the input words into proclitics, simple forms and enclitics. We demonstrate that stemming improves search engine recall and precision.
Natural language analysis systems which combine knowledge-based and corpus-based methods are now becoming accurate enough to be used in various applications. We describe one such parsing system for Dutch, known as Alpino, and we show how corpus-based methods are essential to obtain accurate knowledge-based parsers. In particular we show a variety of cases where large amounts of parser output are used to improve the parser.
This paper studies the impact of automatic sentence segmentation and punctuation prediction on the quality of machine translation of automatically recognized speech. We present a novel sentence segmentation method which is speciﬁcally tailored to the requirements of machine translation algorithms and is competitive with state-of-the-art approaches for detecting sentence-like units. We also describe and compare three strategies for predicting punctuation in a machine translation framework, including the simple and effective implicit punctuation generation by a statistical phrase-based machine translation system. Our experiments show the robust performance of the proposed sentence segmentation and punctuation prediction approaches on the IWSLT Chinese-to-English and TC-STAR English-to-Spanish speech translation tasks in terms of translation quality. 1. Introduction In recent years, machine translation (MT) research groups have increasingly considered translating speech as recognized by an automatic speech recognition (ASR) system. Almost all state-of-the-art ASR systems recognize sequences of words, neither performing a proper segmentation of the output into sentences or sentence-like units (SUs), nor predicting punctuation marks. Usually, only acoustic segmentation into utterances is performed. These utterances may be very long, containing several sentences. Most MT systems are not able to translate such long utterances with an acceptable level of quality because of the constraints of the involved algorithms. Examples of such constraints include reordering strategies with exponential complexity with regard to the length of the input sequence, or parsing techniques which assume the input to be a more or less syntactically correct sentence. The user of an MT system usually expects to see readable sentences as the translation output, with proper punctuation inserted according to the conventions of the target language. Given this situation, algorithms are needed for automatic segmentation of the ASR output into SUs and for punctuation prediction. The latter can be performed either in the source or in the target language. In this paper we present a novel  approach to sentence segmentation and compare three different strategies for punctuation prediction in the framework of statistical MT. In one of these approaches, the punctuation prediction is integrated with the translation process. We also show experimentally that sentence segmentation can be performed automatically without signiﬁcant negative effects on the translation quality. The paper is organized as follows. In section 2, we give a short overview of the published research on SU boundary detection and punctuation prediction. Section 3 presents some details of the statistical phrase-based MT system we use, followed by Section 4 describing the different strategies for punctuation prediction involving this MT system. In Section 5, we describe in detail a novel algorithm for automatic sentence segmentation which was designed especially for the needs of machine translation. Finally, Section 6 describes the experimental results, followed by a summary. 2. Related Work Previous research on sentence boundary detection and punctuation prediction mostly concentrated on annotating the ASR output as the end product delivered to the user. Most authors tried to combine lexical cues (e. g. language model probability) and prosodic cues (pause duration, pitch, etc.) in a single framework in order to improve the quality of sentence boundary prediction [5]. A maximum entropy model [2] or CART-style decision trees [3] are often used to combine the different features. Various levels of performance are achieved depending on the task, but predicting SUs (i. e. complete or incomplete sentences) is reported to be significantly easier than predicting speciﬁc types of punctuation, such as commas and question marks. Recently, [4] performed automatic punctuation restoration in order to translate ASR output for the TC-STAR 2006 evaluation. In this approach, the segments are already known and each segment is assumed to end with a period so that only commas are predicted. A comma is restored only if the bigram or trigram probability of a comma given the context exceeds a certain threshold. We are not aware of any other published work dealing with the detection of SU boundaries and punctuation in the context of machine translation.  158  3. Phrase-based MT system of RWTH In this section we will brieﬂy present the statistical MT system which we use in the experiments for this work. We will denote the (given) source sentence with f1J = f1 . . . fJ , which is to be translated into a target language sentence eI1 = e1 . . . eI . Our baseline system maximizes the translation probability directly using a log-linear model [9]:  p(eI1|f1J ) =  exp  M m=1 
The language model of the target language plays an important role in statistical machine translation systems. In this work, we propose to use a new statistical language model that is based on a continuous representation of the words in the vocabulary. A neural network is used to perform the projection and the probability estimation. This kind of approach is in particular promising for tasks where a very limited amount of resources are available, like the BTEC corpus of tourism related questions. This language model is used in two state-of-the-art statistical machine translation systems that were developed by UPC for the 2006 IWSLT evaluation campaign: a phrase- and an n-gram-based approach. An experimental evaluation for four different language pairs is provided (translation of Mandarin, Japanese, Arabic and Italian to English). The proposed method achieved improvements in the BLEU score of up to 3 points on the development data and of almost 2 points on the ofﬁcial test data. 1. Introduction Speech translation of dedicated tasks like the BTEC corpus of tourism related questions is challenging. Statistical methods have obtained very good performances at the last evaluation campaigns organized by the International Workshop on Spoken Language Translation (IWSLT). However, these techniques rely on representative corpora to train the underlying models: sentence aligned bilingual texts to train the translation models and text in the target language to develop a statistical language model (LM). In the 2006 IWSLT evaluation 40k sentences of bitexts were provided in the Supplied Resources in the “open data track”. The English side of the bitexts is used to train the target language model (326k words). This is a very limited amount of resources in comparison to other tasks like the translation of journal texts (NIST evaluations) or of parliament speeches (TC-STAR evaluations). Therefore, new techniques must be deployed to take the best advantage of the limited resources. For instance, it was proposed to use a translation lexicon that was extracted by applying the Competitive Linking Algorithm on the bilingual training data [1]. By that way, important improvements in the BLEU score were obtained. With respect to language modeling, most of the statistical machine translation  systems (SMT) that participated in the 2005 IWSLT evaluation used 4-gram back-off LM. Some sites reported improvements using 5-gram word or class-based LMs [2, 3], or even 9-gram preﬁx and sufﬁx LMs [4]. Language model adaptation was investigated in [5]. Other interesting approaches include factored [6] or syntax-based language models [7], but to the best of our knowledge, there were not yet applied to the BTEC corpus. In this paper, we investigate if the so-called continuous space language model can be used in a state-of-the-art statistical machine translation system for the IWSLT task. The basic idea of the continuous space LM, also called neural network LM, is to project the word indices onto a continuous space and to use a probability estimator operating on this space [8]. Since the resulting probability functions are smooth functions of the word representation, better generalization to unknown n-grams can be expected. A neural network can be used to simultaneously learn the projection of the words onto the continuous space and to estimate the ngram probabilities. This is still a n-gram approach, but the LM posterior probabilities are ”interpolated” for any possible context of length n-1 instead of backing-off to shorter contexts. This approach was successfully used in large vocabulary continuous speech recognition [9], and initial experiments have shown that it can be used to improve a word-based statistical machine translation system [10]. Here, the continuous space LM is applied the ﬁrst time to a state-of-the-art phrase-based SMT system. Translation of four different languages is considered: Mandarin, Japanese, Arabic and Italian to English. These languages exhibit very different characteristics, e.g. with respect to word order, which may affect the role of the target LM, although a reordering model is used in the SMT systems. We also investigate the use of the continuous space LM in a SMT system based on bilingual n-grams. This paper is organized as follows. In the next section we ﬁrst describe the baseline statistical machine translation systems. Section 3 presents the architecture and training algorithms of the continuous space LM and section 4 summarizes the experimental evaluation. The paper concludes with a discussion of future research directions.  166  2. Baseline systems During the last few years, the use of context in SMT systems has provided great improvements in translation. SMT has evolved from the original word-based approach to phrasebased translation systems. In parallel to the phrase-based approach, the use of bilingual n-grams gives comparable results, as shown by Crego et al. [11]. Two basic issues differentiate the n-gram-based system from the phrase-based: training data are monotonically segmented into bilingual units; and the model considers n-gram probabilities rather than relative frequencies. This translation approach is described in detail by Marin˜o et al. [12]. 
Statistical machine translation systems are usually trained on large amounts of bilingual text and of monolingual text in the target language. In this paper, we will present a self-training approach which additionally explores the use of monolingual source text, namely the documents to be translated, to improve the system performance. An initial version of the translation system is used to translate the source text. Among the generated translations, target sentences of low quality are automatically identiﬁed and discarded. The reliable translations together with their sources are then used as a new bilingual corpus for training an additional phrase translation model. Thus, the translation system can be adapted to the new source data even if no bilingual data in this domain is available. Experimental evaluation was performed on a standard Chinese–English translation task. We focus on settings where the domain and/or the style of the test data is different from that of the training material. We will show a significant improvement in translation quality through the use of the adaptive phrase translation model. BLEU score rises up to 1.1 points, and mWER is reduced by up to 3.1% absolute.  1. Introduction  This paper describes a method for improving an existing sta-  tistical machine translation (SMT) system using monolingual  source language information. Existing statistical machine  translation systems presently beneﬁt from the availability of  bilingual parallel or comparable corpora in the source and  target language, and from monolingual corpora in the target  language. But they do not beneﬁt from the availability of  monolingual corpora in the source language. We will show  how such corpora can be used to improve the translation per-  
This paper proposes the use of rules automatically extracted from word aligned training data to model word reordering phenomena in phrase-based statistical machine translation. Scores computed from matching rules are used as additional feature functions in the rescoring stage of the automatic translation process from various languages to English, in the ambit of a popular traveling domain task. Rules are deﬁned either on Part-of-Speech or words. Part-ofSpeech rules are extracted from and applied to Chinese, while lexicalized rules are extracted from and applied to Chinese, Japanese and Arabic. Both Part-of-Speech and lexicalized rules yield an absolute improvement of the BLEU score of 0.4-0.9 points without affecting the NIST score, on the Chinese-to-English translation task. On other language pairs which differ a lot in the word order, the use of lexicalized rules allows to observe signiﬁcant improvements as well. 1. Introduction In Machine Translation (MT), one of the main problems to handle is word reordering. Informally, a word is “reordered” when it and its translation occupy different positions within the corresponding sentences. In Statistical Machine Translation (SMT) [1], word reordering is faced from two points of view: constraints and modeling. If arbitrary word-reorderings are permitted, the exact decoding problem was shown to be NP-hard [2]; it can be made polynomial-time by introducing proper constraints, such as IBM constraints [3] and Inversion Transduction Grammars (ITG) constraints [4, 5]. It is worth to notice that both types of constraints are linguistically blind, i.e. they are unable to tune the number of allowed word reorderings according to the actual portion of the input sentence under process. Whatever the constraint, among the allowed wordreorderings it is expected that some are more likely than others. The aim of reordering models, known also as distortion models, is just that of providing a measure of the plausibility of reorderings. Most of the distortion models developed so far are unable to exploit linguistic context to score reorder-  ings: they just predict target positions on the basis of other (source and target) positions. Some lexicalized block re-ordering models were presented in [6, 7, 8], where each block is associated with an orientation with respect to its predecessor. During decoding, the probability of a sequence of blocks with the corresponding orientations is computed. In [9] and [10], the aim is to capture particular syntactic phenomena occurring in the source language which are not preserved by the target language. Part-of-Speech (POS) rules are applied for preprocessing the source side both in translation model training and in decoding. In this work we present a novel method for extracting reordering rules from word-aligned training data. The units in the left-hand-side of rules can be plain words or POS’s; moreover, rules can reorder sequences of single units or a pair of unit blocks. In a two-stage SMT system like our one, reordering rules could be exploited directly during decoding in order to focus the search on reordering phenomena observed in training data. Here, we employed them in the rescoring stage, in terms of proper additional feature functions. This paper is organized as follows. Section 2 presents the phrase-based SMT system we have worked with. Section 3 introduces the new reordering method. Sections 4 and 5 present the experimental results and future application, respectively. Some conclusions are drawn in Section 6. 2. The Phrase-based SMT System Given a string f in the source language, the goal of statistical machine translation is to select the string e in the target language which maximizes the posterior distribution Pr(e | f ). In phrase-based translation, words are no longer the only units of translation, but they are complemented by strings of consecutive words, the phrases. By assuming a log-linear model [11, 12] and by introducing the concept of word alignment [1], the optimal translation can be searched for with the criterion: R ˜e∗ = arg max max λrhr(˜e, f , a), ˜e a r=1  182  source string  decoder  WG  : extractor (from WG)  target string N−best Rescoring  Figure 1: Architecture of the ITC-irst SMT system. The decoder produces a word-graph (WG) of translation hypotheses. In single-stage translation the most probable string is output. In two-stage decoding, N-best translations are extracted, re-scored, and re-ranked by applying additional feature functions.  where ˜e represents a string of phrases in the target language, a an alignment from the words in f to the phrases in ˜e, and hr(˜e, f , a) r = 1, . . . , R are feature functions, designed to model different aspects of the translation process. Figure 1 illustrates how the translation of an input string is performed by the ITC-irst SMT system. In the ﬁrst stage, a beam search algorithm (decoder) computes a word graph of translation hypotheses. Hence, either the best translation hypothesis is directly extracted from the word graph and output, or an N-best list of translations is computed by means of an exact algorithm [13]. The N-best translations are then reranked by applying additional feature functions and the top ranking translation is ﬁnally output. The search algorithm [14] exploits dynamic programming, i.e. the optimal solution is computed by expanding and recombining previously computed partial theories. The target string is extended step by step by covering new source positions until all of them are covered. For each added target phrase, a source phrase within the source string is chosen, and the corresponding score is computed on the basis of its position and phrase-to-phrase translation probabilities. The ﬂuency of the added target phrase with respect to its left context is evaluated by a 4-gram language model. Some exceptions are also managed: target phrases might be added which do not translate any source word, and some of the source words can be left untranslated, that is they are translated with a special empty word. To cope with the large number of generated theories, a beam is used to prune out partial theories that are less promising and constraints are set to possible word re-ordering. Word re-ordering constraints are applied during translation each time a new source position is covered, by limiting the number of vacant positions on the left and the distance from the left most vacant position. The log-linear model on which both the search algorithm and the rescoring stage work embeds feature functions whose parameters are either estimated from data or empirically ﬁxed. The scaling factors λ of the log-linear model are instead estimated on a development set, by applying a minimum error training procedure [15, 16]. The language model feature function is estimated on unsegmented monolingual texts.  ab c dbab c acdb Figure 2: Example illustrating the concept of block. The phrase-to-phrase probability feature is estimated from phrase-pair statistics extracted from word-aligned parallel texts. Alignments are computed with the GIZA++ software tool [17]. Phrase pairs are extracted from the segment pairs by means of the algorithm described in [18]. The distortion model feature function is a ﬁxed negative exponential function computed on the distance between the current and the previously translated source phrases. 3. Reordering Rules In order to overcome the limitations of our simple distortion model, we propose to enrich the translation process with reordering rules as deﬁned in the following. Units on which rules work can be words (lexicalized rules) or POS’s (POS rules). Rules can suggest/constraint reorderings at the level of either single units or ngrams of units (blocks). In the following discussion, POS is the unit taken as reference; the extension to words is straightforward. 3.1. Deﬁnition of Block A block is a sequence of source units all occurrences of which are aligned to consecutive positions in an aligned parallel corpus (null alignments are ignored). In the example reported in Figure 2 the sequence “a b c” is a block, while the sequence ”d b” is not, as its last occurrence does not satisfy the contiguity constraint on the target positions. Note that a single unit is also a block. Blocks can be extracted at the level of words or POS by means of a word aligned parallel corpus. In the second case, the source side must be provided with a POS annotation. 3.2. Deﬁnition of Reordering Rule A reordering rule consists of two sides: the left-hand-side (lhs), which is a POS pattern, and the right-hand-side (rhs), which corresponds to a possible reordering of that pattern. Different rules can share the lhs: in such cases, the same pattern can be reordered in more than one way. Rules are weighted, according to statistics extracted from training data. There are two kinds of reordering patterns: unit-based, which deﬁne reorderings at the level of single POS’s (unit reordering rules), and block-based, which deﬁne reorderings between whole blocks of POS’s (block reordering rules). Let us consider the following examples:  183  • Unit reordering rules:  – /rr /vmodal /v # 1 2 3 : 7 (18) – /rr /vmodal /v # 2 1 3 : 4 (18) – /rr /vmodal /v # 1 2 0 : 3 (18) – /rr /vmodal /v # 0 1 2 : 2 (18) – /rr /vmodal /v # 1 0 2 : 1 (18) – /rr /vmodal /v # 2 1 0 : 1 (18) – /v /d /v /m /q # 1 2 3 4 5 : 4 (5) – /v /d /v /m /q # 0 0 1 2 3 : 1 (5)  • Block reordering rules: – [/rr /vmodal /v] [/ng] # 1 2 : 3 (4) – [/rr /vmodal /v] [/ng] # 1 0 : 1 (4)  The tokens “/rr”, “/vmodal”, “/v”, etc. are Chinese POS’s, while the sequences “/rr /vmodal /v” and “/v /d /v /m /q” are POS patterns (pn1 ). The strings of numbers in between the symbols “#” and “:” represent suggested reordering (r1n): each integer ri represents the new position of (the translation of) pi. For example, the rhs of the second unit reordering rule is “2 1 3”. The “2” in the ﬁrst position means that “/rr” goes in the second position; the “1” in the second position means the “/vmodal” goes in the ﬁrst position; ﬁnally, the “3” in the third position means that the “/v” keeps the third position. A “0” means that the corresponding token is deleted, i.e. it is aligned to the “null” word. A pair of square brackets indicates a block. Block reordering ruless are always binary. The two numbers after the colon (:) are collected from training data and are respectively the number of times the rhs (reordering suggestion) of the rule has been observed count(r1n) and (inside brackets) the number of occurrences of the rule pattern count(pn1 ). The probability of each reordering suggestion is computed as:  P (r1n|pn1 )  =  count(r1n) count(pn1 )  (1)  3.3. Extraction of Blocks  Given the above deﬁnition, blocks are extracted from source-to-target (direct) aligned training data by means of the procedure shown in Figure 3. For each source sentence s in the training data, it is assumed that the direct alignment a is available. For each source n-gram (with n up to 20) its counter is updated (lines 2-10). Then, it is checked if the ngram actually corresponds to a block: lines 12-18 check if there are source words on its left that are aligned to indexes within the n-gram translation; lines 19-25 check if there are source words on its right that are aligned to indexes within the n-gram translation. If both checks are passed, a second counter is updated (lines 26-28). Finally, only those n-grams which have been observed as blocks and more than once are returned as actual blocks (lines 29-34).  
Most of statistical machine translation systems are combinations of various models, and tuning of the scaling factors is an important step. However, this optimisation problem is hard because the objective function has many local minima and the available algorithms cannot achieve a global optimum. Consequently, optimisations starting from different initial settings can converge to fairly different solutions. We present tuning experiments with the Simultaneous Perturbation Stochastic Approximation (SPSA) algorithm, and compare them to tuning with the widely used downhill simplex method. With IWSLT 2006 Chinese-English data, both methods showed similar performance, but SPSA was more robust to the choice of initial settings. 1. Introduction Statistical machine translation (SMT) was originally based on the noisy channel approach [1]. In present SMT systems, the noisy channel approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented [2]. Translation quality can be improved by adjusting the weight of each feature function in the log-linear combination. This can be effectively performed by minimising translation error over a development corpus for which manually translated references are available [3]. This minimisation problem in multiple dimensions is difﬁcult because of three main characteristics of the objective function. Firstly, it has no analytic representation, so the gradient cannot be calculated. Secondly, it has many local minima. Finally, its evaluation has a signiﬁcant computational cost (depending on the scheme, it implies translating the development corpus or re-ranking an n-best list for this corpus, and calculating some translation error measure). Gradient may be approximated, but this is costly since it requires typically as many function evaluations as the number of scaling factors. Thus, algorithms based on derivatives are discarded. Algorithms which require many objective functions evaluations, such as simulated annealing or genetic algorithms, are also discarded. Two popular alternatives are Powell’s method [4, 5, 3] and the downhill simplex method [6, 5, 7]. In recent experiments at the 2006 John Hopkins University Summer Workshop on SMT, both meth-  ods achieved similar performance [8]. The simplex method is self-contained and straightforward and thus widely used for SMT tuning [7, 9, 10, 11], although it is not very efﬁcient in terms of number of objective function evaluations for a high number of dimensions [12]. However, from the authors experience, a slight modiﬁcation of initial parameters in the simplex optimisation can result in an appreciable difference in both the value of the local minimum found and the value of the optimal parameters. This difference is transmitted when these parameters are used to translate a test corpus. When a translation system is compared to a baseline, the difference arising only from the tuning process can be even greater than the difference arising from the two systems differences, leading to insigniﬁcant results. In some data sets, some inconsistencies of the tuning method have also been reported [13]. In this paper we compare tuning with the Downhill Simplex method and with the Simultaneous Perturbation Stochastic Approximation [14] (SPSA) method. SPSA has been successfully applied in areas including statistical parameter estimation, simulation-based optimisation, signal and image processing [15]. This paper is structured as follows. First the essential features of the SPSA method are presented. Then in section 3, objectives and details of the experimental work are given. In section 4, results are shown and discussed. Finally, some concluding remarks and perspectives are given.  2. Presentation of SPSA algorithm  The SPSA method is based on a gradient approximation which requires only two evaluations of the objective function, regardless of the dimension of the optimisation problem. This feature makes it especially powerful when the number of dimensions is increased. The SPSA procedure is in the general recursive stochastic approximation form:  λˆk+1 = λˆk − akgˆk(λˆk)  (1)  where gˆk(λˆk) is the estimate of the gradient g(λ) ≡ ∂E/∂λ at the iterate λˆk based on the previous mentioned evalua- tions of the objective function. ak denotes a positive number that usually gets smaller as k gets larger. Two-sided gradient approximations involve evaluations of E(λˆk + perturbation)  190  and E(λˆk − perturbation). In the simultaneous perturbation approximation, all elements of λˆk are randomly perturbed together and the approximated gradient vector is:   1/∆k1   E(λˆk  +  ck∆k) − E(λˆk 2ck  −  ck ∆k )       1/∆k2 ...       (2)  1/∆kN  In equation 2, ∆k is a perturbation vector of same dimension N as λ, whose values ∆i are computed randomly. ck denotes a small positive number that usually gets smaller as k gets larger. Compared to a ﬁnite-difference gradient approximation, involving N times more function evaluations, the simultaneous approximation causes deviations of the search path. These deviations are averaged out in reaching a solution and according to [15], under reasonably general conditions, both gradient approximations achieve the same level of statistical accuracy for a given number of iterations. Notice that in general, SPSA converges to a local minimum. The general form of the algorithm consists of the following steps (see section 3.4 for further implementation details):  Step 1 Calculate gain sequences ak and ck.  Step 2 Generate the simultaneous perturbation vector ∆k. Step 3 Evaluate E(λˆk + ck∆k) and E(λˆk − ck∆k).  Step 4 Approximate the gradient as in equation 2  Step 5 Update λ estimate as in equation 1  Step 6 Iteration or termination. Return to Step 2 with k + 1 replacing k. Terminate if the maximum number of iterations have been reached or if there is little change in several successive iterates.  3. Experimental Settings 3.1. Translation system used Although the following discussion would be valid in many contexts, and in particular for any Empirical MT system, it is convenient here to present brieﬂy the models implemented by our system, and whose respective weights are tuned. For a more complete description see [16]. The SMT approach used here considers a translation model which is based on a 4-grams language model of bilingual units which are referred to as tuples. Tuples are extracted from Viterbi alignments and can be formally deﬁned as the set of shortest phrases that provides a monotonic segmentation of the bilingual corpus. In addition to the bilingual 4-gram translation model, the translation system implements a log linear combination of ﬁve additional feature functions: a 4-gram language model of the target language (denoted TM); a 4-gram language model of target POS-tags (TTM) which helps, along with the target language model, to provide a better concatenation of tuples;  a word bonus feature (WB), which compensates the system preference for short translations over large ones; and ﬁnally, two lexicon models (L1 and L2) that implement, for a given tuple, the IBM-1 translation probability estimate between the source and target (or target and source, respectively) sides of it. 3.2. Objectives Thus we have a translation system whose outcome depends on a set of parameters λ (in this experiment, parameters were restricted to the scaling factors of the various models). We want to minimise a function E(λ), which measures the translation errors over a given development set, made by the system with the parameter vector λ. In this experiment, each evaluation of E(λ) implies computing full translation of the development corpus, which is computationally intensive (the number of evaluations of E(λ) to achieve convergence is in the order of 100). Note that in other setups [3, 8], tuning is performed in two stages. In the ﬁrst stage, full translation of the development corpus is computed, with n-best output. In the second stage, the n-best list is re-ranked. In this second stage, a parameter optimisation is performed with the downhill simplex method or with Powell’s method 1. In this case, an evaluation of the objective function only implies reranking the n-best list. The number of re-rankings necessary to achieve convergence is also in the order of 100. After this optimisation, the optimum parameters are used to translate again the development corpus and generate an updated n-best list. In this setup, convergence (on the ﬁrst stage level) usually occurs after less than 10 full translations. The objective of the experiment was to perform the optimisation of E(λ) with the downhill simplex method and the SPSA method, and to compare the consistency of the results over changes in initial settings. For this, we ran the algorithms from 7 different initial points and for each point, for 10 slightly different realisations. For both algorithms, an evaluation of E(λ) implied a translation of the development corpus by exactly the same system (except the model weights). Thus, an objective function evaluation had the same computational cost for both algorithms. We aimed at choosing initial points well distributed in parameter space, but nevertheless realistic. Notice that in the log-linear combination, weights can be rescaled to set one of the parameters to some value, so the translation model was set to 1 and kept ﬁxed during optimisation. In the ﬁrst initial point, all parameters are also 1, so that all models start with equal weights. In the second initial point, all parameters are equal to 0.5. The other points were chosen in the following way. We collected sets of optimal parameters obtained previously on another development corpus, and noted down in which range the scaling factor of each model behaved. We selected the initial value of each parameter randomly within its corresponding range. Table 1 displays the initial points 1Actually, SPSA could also be used instead  191  used in the experiments.  ID TM TTM WB L1 L2  
In this paper we describe an efﬁcient implementation of a graph search algorithm for phrase-based statistical machine translation. Our goal was to create a decoder that could be used for both our research system and a real-time speechto-speech machine translation demonstration system. The search algorithm is based on a Viterbi graph search with an A* heuristic. We were able to increase the speed of our decoder substantially through the use of on-the-ﬂy beam pruning and other algorithmic enhancements. The decoder supports a variety of reordering constraints as well as arbitrary ngram decoding. In addition, we have implemented disk based translation models and a messaging interface to communicate with other components for use in our real-time speech translation system. 1. Introduction The current state-of-the-art in machine translation uses a phrase-based approach to translate individual sentences from the source language to the target language. This technique [1] gave signiﬁcant improvement over word to word translation originally developed at IBM [2]. However, regardless of the underlying models, the search or decoding process in statistical machine translation is computationally demanding, particularly with regard to word or phrase reordering. Effective pruning and novel reordering techniques have helped reduce the search space to something more manageable, but search still remains a difﬁcult problem in statistical machine translation. Our goal was to create a fast and lightweight decoder that could be used for our research system as well as for a realtime speech-to-speech translation demonstration system. For the research system, the decoder had to support two-pass decoding via n-best list re-ranking or lattice rescoring. The decoder also had to be memory efﬁcient enough to handle search with very large numbers of active nodes. For the 1This work is sponsored by the United States Air Force Research Laboratory under Air Force Contract FA8721-05-C-0002. Opinions, interpretations, conclusions and recommendations are those of the author and are not necessarily endorsed by the United States Government.  real-time demonstration system, we required fast decoding as well as a common API that would allow integration with various other components such as speech recognition and textto-speech systems. Real-time decoding also requires that the translation models, which can approach several gigabytes in size, be read efﬁciently from disk on demand as opposed to the pre-ﬁltering of models typically done during batch processing. 2. System Overview The decoding strategy we implemented is a Viterbi beam search with an A* heuristic based on words not yet translated. The decoder begins by dividing the input sentence into many overlapping word segments up to a given maximum phrase length. Each phrase segment is then cross referenced with the phrase table inventory and the top-N translation candidates for the source phrase are stored in memory. A trigram language model is also loaded into memory, and all partial translations are given an initial language model score. During search, the language model probabilities for the individual words are adjusted given the new context of previously translated words. For our research system, we employ a two-pass decoding strategy. N-best lists are generated from the output phrase lattice, and additional feature functions are added (i.e. higher order language model, word posterior scores, etc.) The resulting N-best lists are then re-ranked according to these new features and the best scoring output is selected [3]. 2.1. Translation and Language Models The basic phrase-translation model we employ is described in detail [4] and [5]. We use GIZA++ [2], [6] and [4] to generate alignments between foreign language and English language words/tokens in both directions (f-to-e and e-to-f). An expanded alignment is then computed using heuristics that interpolate between the intersection and union of these bidirectional word alignments as detailed in [4] and [5]. Then phrases are extracted from the expanded alignments to build the phrase model. We extract translation models for both translation direc-  197  tions (i.e. P (f |e) and P (e|f )). In addition to these models, we add lexical weights extracted from the expanded alignment process in both translation directions [5] and a ﬁxed phrase penalty [7]. We use the SRILM language model toolkit [8] to generate an N-gram language model for use during decoding. These models are trained using modiﬁed Knesser-Ney interpolation. 2.2. Distortion Modeling The distortion model we use is still rather weak and is based on the model used in the Pharaoh decoder [9]. It is simply a distance penalty based on the overall movement of source words: Dp = − |last wordi−1 + 1 − f irst wordi| (1) i That is, the distortion measure between two phrases is the number of words between the last word of the previously translated phrase and the ﬁrst word of the new phrase. In practice, we place limits on the allowed reordering patterns to limit the search space and improve accuracy. This can be through a ﬁxed distance limit or other linguistically inspired constraints. 2.3. Minimum Error Rate Training  Table 1: Translation model parameters.  Parameter p(f |e) p(e|f ) LexW (f |e) LexW (e|f ) Wp Pp plbo(e|f ) Dp p(e)  Description Forward phrase translation probability Backward phrase translation probability Forward lexical weighting Backward lexical weighting Number of output words Fixed penalty for each source phrase Lexical backoff penalty Distortion penalty N-gram language model  Our translation model employs a log-linear combination of many different model parameters. The model parameters used are shown in Table 1. The models are tuned using a development corpus using an algorithm similar to the one described in [10]. The error criterion minimized is 1 − BLEU . N-best lists are generated after decoding on the development corpus and the weights for the model parameters shown in Table 1 are set to minimize the overall error rate. Additional features, such as higher-order language models, can also be added at this stage and their weights adjusted as well.  3. Decoder Implementation The decoder uses a Viterbi beam search with an A* heuristic. Pseudocode for the algorithm is shown in Figure 1. First,  the various data structures are initialized and the models are loaded. The initial node list contains a single node with the begin sentence marker, <s>. The input source sentence is divided into individual phrases up to a maximum phrase length. Each input phrase is looked up in the phrase inventory. For smaller translation tasks with limited training data, the entire phrase inventory is loaded into memory and a chained hash table based on the input phrase is created. This hash table structure allows fast access to the phrase inventory. The hash table entry for a particular source phrase contains a pointer to all possible translations of that phrase sorted by the weighted model parameter scores. This allows for a selection of the top-N best scoring phrases for efﬁcient search. Larger translation tasks have phrase inventories that are too large to be stored in memory during decoding. When the input sentences are known prior to the decoding process, as in batch processing of evaluation corpora, it is reasonable to produce smaller sets of models speciﬁc to particular input sentences. However, for real-time speech translation, this is not practical nor desireable. We do not have prior knowledge of the spoken input and cannot ﬁlter the phrase inventory in advance. Instead, we pre-index the phrase table inventory using Berkeley DB software, and we simply treat the model as a disk based hash table. Since Berkeley DB is designed for very fast access to large amounts of data, there is almost no performance penalty with this approach. The search proceeds according to the number of source words covered at a given time (line 2). For each source phrase in the phrase inventory, possible translation candidates can be linked to previous nodes according to the number of source words in the currently selected phrase. That is, a phrase covering three input words, will link back to nodes from three iterations prior. The distortion is calculated according to equation (1). In addition, the coverage vectors from the left node and this new candidate phrase are checked to make sure they cover different words. The same word cannot be covered more than once on a single path. For the A* heuristic (line 9), we use the highest probability translation options for the remaining words as in the Pharaoh decoder [9]. This includes the translation and language model probabilities for the remaining phrase scores in isolation of one another (i.e. no distortion or language models scores across phrases). These costs can be computed up front with a simple dynamic programming algorithm and stored in a table for later use. We experimented with several extensions to the future cost estimate. We tried augmenting this heuristic with a language model lookahead as well as a best case distortion cost. While both of these were admissible heuristics, we observed no consistent gain in performance across all our experiments. The inner loop of the search (line 10) cycles through all possible translations (up to a predeﬁned limit) for each phrase that meets the distortion criteria. The language model scores are updated only at phrase boundaries. Node expansion is controlled by indexing all created nodes into a hash  198  1. Initialize data structures  2. for cw from 0 to number of words  3. for each src_phrase in src_phrase_list  4.  l_index = cw - len(src_phrase)  5.  for l_node in node_list[l_index]  6.  calculate distortion between l_node and src_phrase  7.  if (dist > dlimit) or (l_node and src_phrase cover the same word)  8.  continue  9.  get future_cost of remaining words  10.  for each translation in src_phrase  11.  update lm scores at phrase boundary using l_node context  12.  find or create curr_node with current coverage and lm context  13.  link curr_node to l_node with current translation  14.  if current_score > curr_node.best_score  15.  set current translation to best path of curr_node  16. beam and histogram pruning  17. back-trace and output best translation  Figure 1: Pseudocode for the decoding algorithm.  table. The hash function is computed using a data structure that contains the word coverage vector and the right most words or language model context for the node. For example, the hash table index might be ”00110 would like”, indicating that the third and fourth input source words were covered and the language model context is ”would like.” If a node already containing the current coverage vector and language model context exists in the hash table (line 12), then the current translation is added to this node and the node is linked back to the left node in the graph. If a proper node does not exist, a new one is created and added to the hash table. The backoff structure of the N-gram language model is also used here to eliminate unnecessary node expansion for language model contexts that have no possible n-gram expansion. This signiﬁcantly reduces the number of nodes created during search. The ﬁnal step in the inner loop is to update the best path for the current node if the current score is better (line 14). This is the Viterbi approximation step. It is important to note that the score used here is the total log probability plus the future cost estimate computed earlier. This total path score plus future cost is also used during beam and histogram pruning. After the main search is complete, all remaining nodes are joined with the end of sentence marker, </s>, and language model scores are updated across phrase boundaries. The remaining search graph can be traced back along the highest scoring path to get the best path and thus the best candidate translation. 3.1. OOV Words The problem of out-of-vocabulary (OOV) words requires some small changes to the above algorithm. We chose to handle the issue with an open vocabulary language model. The SRILM toolkit allows for unknown words by assigning a non-zero probability to word sequences containing the  <unk> tag. In the case of a single unknown word, we simply pass the word through with translation model probabilities of 1.0 and language model probability according to the open vocabulary language model. OOV words are required to remain adjacent to their previously translated source phrases. Their distortion cost must be zero. Since the nodes associated with these OOV input words now have probabilities that are artiﬁcially higher than the remaining nodes. As such, we do not use nodes containing OOV words in order to set our beam threshold during pruning. 3.2. On-the-ﬂy Beam Pruning During some initial proﬁling of the code, we found that a signiﬁcant amount of time was spent looking up various ngrams in the memory-based language model. Individual Ngrams are also stored in a hash table for fast access, but many of these language model lookups are unnecessary as the paths are eventually pruned out. Additionally, there is signiﬁcant node expansion for these unpromising paths, which slows the sorting that takes place prior to pruning. In order to lessen the impact of frivolous language model lookup and node expansion, we employed an active or on-the-ﬂy beam pruning approach. We start by keeping track of the best path score during each iteration of the main outer loop. When a translation option with higher probability occurs, we update the best path score. Then we apply beam pruning right after steps 9 and 10. If the partial score does not fall within the currently selected beam, the translation option is ignored and the loop continues with the next translation option. By checking the score after step 9, we eliminate the inner loop for translations options whose combined partial path costs, distortion, and future cost estimates do not fall within the beam. If at this stage, the probability falls outside the beam, then any  199  further reduction from the application of translation model scores in the inner loop will also be outside the beam. After step 10, we include the translation model scores, which further lowers the probability. Once again, if the hypotheses is outside the beam at this point, then there is no reason for further analysis, and we can skip the costly language model lookup.  Table 2: Results from beam pruning enhancements on the IWSLT06-dev4 data set.  Description  Chars/sec BLEU  No on-the-ﬂy pruning 1.06  prune after step 9  1.08  prune after step 9,10 2.29  Sort phrase list  2.99  20.13 20.08 20.08 20.19  Table 3: Beam width comparison for different pruning techniques on the IWSLT06-dev4 data set.  Beam width 0.0003 0.001 0.005 0.01  On-the-ﬂy pruning Chars/sec BLEU  2.99 7.31 28.21 50.75  20.19 19.44 19.36 18.84  Normal pruning Chars/sec BLEU  1.06  20.13  1.97  19.77  7.29  19.74  13.1  18.91  While this technique works reasonably well, it operates under the assumption that the actual best partial path score is found early on. In the worst case, the best score is found last, and there is no beneﬁt. To increase the chances that the highest probability score is found early, we use a best-ﬁrst approach with respect to the source phrase list. In particular, the source phrases are initially sorted according to their future cost estimate. This helps reduce the average length of time until the best score is found, and even if the best scoring path is not found, there is some beneﬁt to having an active beam width that is at least close to optimal. Using the training and development sets from the 2006 International Workshop on Spoken Language Translation (IWSLT06), we show the BLEU scores decoding times, expressed as characters per second, in Table 2 for a Chinese to English translation. (Our Chinese system uses a combination of word and character segmented alignments to produce a ﬁnal phrase table based on individual Chinese characters.) The IWSLT06 training data consists of approximately 40,000 parallel sentences in the travel expression domain. Supplied with the training data were three development sets of approximately 500 sentences each, with 2 containing 16 references and another with only 7 references. We optimized the model weights on the CSTAR03 development set and decoded on the IWSLT06-dev4 set. The lower cased output was postprocessed to restore mixed-case information, and the result was scored against the mixed-case references.  The biggest reduction in runtime came from removing unnecessary language model lookup after step 10. In the ﬁnal conﬁguration, using a sorted phrase list to quickly arrive at the best scoring hypothesis yields a runtime that is almost 3 times faster than without on-the-ﬂy pruning. The small changes in BLEU scores are due to borderline hypotheses that are pruned out before complete language model expansion. Additionally, we varied the beam width to compare the optimized vs. non-optimized versions. The results are in Table 3. We observe some small reduction in BLEU score when comparing the optimized vs. un-optimized version as the beam becomes tighter. However, the optimized version is still much faster, even if slightly sub-optimal with respect to accuracy. 3.3. Reordering Constraints In this section, we explore the performance of our decoder under different word reordering constraints, including the socalled ITG constraints, IBM constraints, free ordering with a distance limit, as well as an additional approach that provides good accuracy with deﬁnite speed advantages. Additional reordering constraints are implemented in place of or in addition to lines 6,7 in the algorithm shown in Figure 1. In the case of free word order, any source word can follow any other word with respect to the construction of the search graph. Often a hard limit is placed on the maximum distance allowed. A comparison of different word reordering constraints was made in [11], and a simple algorithm for imposing constraints derived from Inversion Transduction Grammars (ITG) [12] during search was introduced. If we assume a simple, 4-word, input sequence of (1,2,3,4), the ITG constraints do not allow certain reordering patterns (i.e. 3,1,4,2 and 2,4,1,3). These “inside-out” patterns are not likely to follow reordering patterns of language. The number of permutations with ITG constraints grows much slower than with free word reordering [13]. Under the IBM constraints, input source words can be chosen out of order so long as they ﬁll one of the ﬁrst k uncovered words [11]. By increasing k, more reordering patterns are possible. This can provide fast decoding when k is small for closely related language pairs, but it can be quite slow when k is large, as for ChineseEnglish. One issue we found when analyzing the performance of our decoder with respect to word reordering was the issue of incomplete paths in the graph. Using a free word order with a hard limit on word movement resulted in graphs with incomplete translation paths, resulting in wasted search effort. For example, consider the translation of 4 input source words using a distortion limit of 2. If we build the graph from left to right using only the distortion limit as a guide, the result is shown in Figure 2. If we initially choose the reordering sequence 2,4,3, we have not violated the constraints placed on the distortion (Using equation (1) the distortion penalty from 2 to 4 is Dp = |2 + 1 − 4| = 1, and from 4 to 3 it is Dp = |4 + 1 − 3| = 2). However, the remaining word, 1,  200  Figure 2: Reordering graph for four input words with a distortion limit of 2. Figure 3: IBM constraints for 4-word input sequence with k = 2. cannot be selected within the distortion limit constraints (in this case, Dp = |3 + 1 − 1| = 3). Even in this simple example, there are many instances of incomplete paths and thus wasted search effort. While this graph could be trimmed using a forward-backward pass, this is not practical for longer sentences with greater limits on distortion. In the future, we would like to address this problem with a better implementation that would detect the incomplete paths in advance. In practice, a hard distortion limit is often placed on the ITG constraints as a way to limit the search time, but this too suffers from the problem of incomplete paths in the search graph. However, the IBM constraints, by design, do not have this problem. Figure 3 shows the same reordering graph under the IBM constraints with k = 2. The IBM style constraints still produce larger numbers of permutations and thus decoding times somewhere between free word order and ITG constraints.  2:2  5  11  3:3  4:4  4:4 6 3:3 12 2:2  
Currently most statistical machine translation systems make use of alignments as a ﬁrst step in the process of training the actual translation models. Several researchers have investigated how to improve the alignment quality, with the (intuitive) assumption that better alignments increase the translation quality. In this paper we will investigate this assumption and show that this is not always the case.  1. Introduction  Alignments are a key concept to statistical machine translation. They represent the correspondence between the words of the source and target sentences. They were introduced in the mathematical context of [1] as a hidden variable and used in the framework of the EM Algorithm to estimate the lexicon probabilities and further parameters of the IBM-1 to IBM-5 translation models. Further development and research in statistical machine translation moved from the original single-word-based models to phrase-based-models, in order to better capture the context dependencies of the words in the translation process. The starting point for the training of these models was however the Viterbi alignment produced as a byproduct of the training of the original IBM models, that is, the alignment with the highest probability given the ﬁnal parameter estimations. Most state-of-the-art machine translation systems, normally based on a phrase-based translation scheme or variations of it, make use of this Viterbi alignment as a ﬁrst step in the training process [2, 3, 4]. Other translation approaches also beneﬁt from the use of alignments [5]. It is then to expect that an increase in quality of the alignment should lead to an increase in translation quality. At least, it is expected that an improvement in the alignments does not hurt translation performance. In [6] the “Alignment Error Rate” (AER) is introduced as a measure of alignment quality. Given a reference alignment, consisting of a set S of “Sure”, unambiguous alignment points and a set P of “Possible”, ambiguous alignment points, with S ⊆ P , the AER of an alignment A = {(j, aj)} is deﬁned to be  AER(S,  P  ;  A)  =  
This paper describes a client-server speech-to-speech translation system developed on a multi-lingual speech communication platform. This platform enables easy assembly of speech communication system from the corresponding software modules (e.g. speech recognition, spoken language machine-translation, speech synthesis). This client-server speech translation system is designed for use at mobile terminals. Terminals and servers are connected via a 3G public mobile phone networks, and speech translation services are available at various places with thin client. This system realizes hands-free communication and robustness for real use of speech translation in noisy environments. A microphone array and new noise suppression technique improves speech recognition performance, and a corpus-based approach enables wide coverage, robustness and portability to new languages and domains. Recent evaluation of the overall system showed that the utterance correctness of speech recognition output achieved 83%, and more than 88% of the utterances are correctly translated for Japanse-English and JapaneseChinese. Index Terms— speech-to-speech translation system, corpus-based, client-server 1. INTRODUCTION Recent progress in corpus-based speech and language processing technology has made it possible to realize speech (speech-to-speech) translation in real situations. A multilingual speech communication platform has been developed, and a client-server speech translation system on this platform is constructed for evaluating the latest speech translation technology in real situations. All component software modules take a corpus-based statistical approach,  where acoustic and language models of each module and corresponding parameters are automatically constructed (tuned) from large-scale corpora[1]. The advantages of the corpus-based approach are that it achieves wide coverage, robustness and portability to new languages and domains. This paper presents a real-time speech translation system with microphone array processing for hands-free speech communication, new techniques for obtaining efficient models, statistical models such as HMMs (Hidden Markov Models) and Ngrams for speech recognition, statistical and example-based translation for machine translation, waveform concatenation for speech synthesis, and a result evaluation test including dialog in a real environment. 2. SPEECH-TO-SPEECH TRANSLATION SYSTEM 2.1. Multi-lingual speech translation system The speech translation system consists of a “module manager” and component modules, i.e., automatic speech recognition (ASR), machine translation (MT), and speech synthesis (SS). The module manager has the function of controlling messages comprising speech data, recognized or translated text data, and system messages according to the predefined message flow. For example, the message flow for speech-to-speech translation is defined as follows; input speech data for each terminal are sent to the ASR,MT, SS, in this order to obtain the recognized text, translated text and its corresponding synthesized speech. Finally, these data are sent back to the terminal. To avoid degradation in the speech recognition performance caused by the loss of data packets between the terminal and the server, TCP protocol is used. Figure 1 shows an overall configuration of the clientserver speech-to-speech translation system. This system consists of several terminals and a speech translation server. The terminals and server are connected via a wireless data network. Instead of a client-server architecture shown in  213  2.3. Component modules 2.3.1. Signal processing and speech recognition  
This paper gives an overview of the evaluation campaign results of the International Workshop on Spoken Language Translation (IWSLT) 20061. In this workshop, we focused on the translation of spontaneous speech. The translation directions were Arabic, Chinese, Italian, or Japanese into English. In total, 21 translation systems from 19 research groups participated in this year’s evaluation campaign. Both automatic and subjective evaluations were carried out in order to investigate the impact of spontaneity aspects on automatic speech recognition (ASR) and machine translation (MT) system performance as well as the robustness of stateof-the-art MT systems towards speech recognition errors. 1. Introduction The International Workshop on Spoken Language Translation (IWSLT) is an evaluation campaign organized by the Consortium for Speech Translation Advanced Research (C-STAR)2, that provides a common framework to compare and improve current state-of-the-art speech-to-speech translation technologies. Previous IWSLT workshops focused on the establishment of evaluation metrics for multilingual speech-to-speech translation [1] and the translation of automatic speech recognition results from read-speech input [2]. The focus of this year’s IWSLT was the translation of spontaneous-speech input. The evaluation campaign was carried out using a multilingual spoken language corpus including Arabic, Chinese, Italian, Japanese, and English sentences from the travel domain. The input to the machine translation (MT) engines was either the output of an automatic speech recognition (ASR) system applied to spontaneous-speech and read-speech input or the correct recognition result (CRR). The translation was carried out from Arabic, Chinese, Italian, or Japanese into English. Participants were supplied with in-domain resources, but were free to use additional resources as well. Depending on the amount of in-domain training data, two different data tracks (OPEN, CSTAR) were distinguished. In total, 21 MT systems from 19 research groups participated in this year’s evaluation campaign. A total of 73 MT engines were built to cover different combinations of language pairs and data 1http://www.slc.atr.jp/IWSLT2006 2http://www.c-star.org/  tracks. The translation quality of all ofﬁcial run submissions was evaluated using automatic evaluation metrics. In addition, human assessments were carried out for the most popular track, i.e., the translation of Chinese ASR output into English. Based on the evaluation results, the impact of the spontaneity aspects of speech on the ASR and MT systems performance as well as the robustness of state-of-the-art MT systems towards speech recognition errors were investigated. 2. IWSLT 2006 Evaluation Campaign 2.1. IWSLT 2006 Spoken Language Corpus The IWSLT 2006 evaluation campaign was carried out using a multilingual spoken language corpus. The Basic Travel Expression Corpus (BTEC ) contains tourism-related sentences similar to those that are usually found in phrase books for tourists going abroad [3]. Parts of this corpus were already used in previous IWSLT evaluation campaigns [1, 2]. In addition to the sentence-aligned training corpus, the evaluation data sets of previous workshops including multiple reference translations were provided to the participants as a development corpus. The evaluation data set of IWSLT 2006 consisted of spontaneous answers to questions in the tourism domain. This “Challenge Task 2006” differed greatly from the translation tasks of previous workshops. In addition to the spontaneous speech data, read-speech recordings of the cleaned transcripts were also used for evaluation purposes. ASR engines provided by the C-STAR partners were applied to the speech input and produced word lattices from which NBEST/1BEST lists were extracted automatically using publicly available tools. Word segmentations according to the output of the ASR engines were also provided for all supplied resources. 2.1.1. Supplied Resources For this year’s evaluation campaign, parts of the Arabic (A), Chinese (C), Italian (I), Japanese (J), and English (E) subsets of the BTEC corpus were used. The participants were supplied with a training corpus of 40K sentence pairs for CE/JE, and 20K sentence pairs for AE/IE and three development data sets (dev1, dev2, dev3; 500 sentences each) consisting of the evaluation data sets of previous IWSLT evaluation campaigns including up to 16 English reference translations for evaluation purposes.  
In this paper, we present our system for statistical machine translation that is based on weighted ﬁnite-state transducers. We describe the construction of the transducer, the estimation of the weights, acquisition of phrases (locally ordered tokens) and the mechanism we use for global reordering. We also present a novel approach to machine translation that uses a maximum entropy model for parameter estimation and contrast its performance to the ﬁnite-state translation model on the IWSLT Chinese-English data sets. 1. Introduction The problem of machine translation can be viewed as consisting of two subproblems: (a) lexical selection, where appropriate target language lexical items are chosen for each source language lexical item and (b) lexical reordering, where the chosen target language lexical items are rearranged to produce a meaningful target language string. In previous work, we have proposed stochastic ﬁnite-state transducer (SFST) models for these two subproblems [1, 2] which can then be composed into a single SFST model for Statistical Machine Translation (SMT). SFST approach to SMT has gained momentum in recent years with several groups following this approach successfully [3, 4, 5, 6] for speech to speech translation. The attractions of this approach are (a) efﬁciently learnable from data (b) generally effective for decoding (c) associated with a calculus for composing models which allows for straightforward integration of constraints from various levels of language processing. In this paper, we present the SFST model for SMT and detail the different phases in training the model and decoding using the model. We also present a novel approach to translation using a discriminatively trained model for lexical choice and a permutation automaton for lexical reordering. This model does not rely on word-level alignments at all which makes it an attractive approach for translating between language pairs that have signiﬁcantly different word orders (English-Japanese, for example) where the stringbased word-alignment methods perform poorly. The outline of the paper is as follows. In Section 2, we  TRAINING PHASE Sentence Aligned Corpus Alignment Word Alignment Bilanguage Transformation Bilanguage Local Phrase Reordering Phrase Segmented Bilanguage Joint Language Modeling Joint Language WFSA FSA to FST Translation model WFST Figure 1: Training phases for our system present an overview of the system. In Section 3, we describe in detail the different stages used to train an SFST translation model. We illustrate the steps in the decoding of a source input using the SFST translation in Section 4. In Section 5, we present a discriminative model for translation which addresses the shortcomings of SFST translation. 2. System Overview In this section, we present an overview of our system. The details of each component of the system are explained in subsequent sections of the paper. The training of the SFST model starts with a sentence  16  DECODING PHASE Source Sentence/Weighted Lattice  Translation Model Target Model Language  FST Composition  Lexical Choice Decoding  Permutation Construction  Lexical Reodering Decoding  Permutation Lattice  FSA Composition  Target Sentence Figure 2: Decoding phases for our system aligned corpus as the input as shown in Figure 1. The sentence aligned corpus is used to infer a word alignment using an alignment training process. Word alignment results in a mapping between the words in the source language sentence and the words in the target language sentence. Words from either the source or target sentence may be left unmapped. Each word aligned sentence is transformed into a bilanguage string where the ﬁrst component is a source word and the second component is a target word. The choice of representing the bilanguage in the order of the source language implies that the target language words would not be in the target language order. In order to alleviate this problem, we use local phrase reordering with a ﬁxed window which retokenizes the bilanguage string so as to align the words appearing in the window to be in the correct source and target language order. This retokenized string is used to build a n-gram language model. The n-gram language model approximates the joint probability model between the source sentence and source-ordered target language sentence. The n-gram language model is represented as a weighted ﬁnitestate acceptor (FSA) and the arcs are interpreted as having two components, thus resulting a weighted ﬁnite-state transducer. In the decoding/translation phase (Figure 2), a given input, a sentence or a weighted lattice from a speech recognizer is represented as a weighted FSA (WFSA). The WFSA is composed with the SFST from the training process and the highest probability path is extracted as the output of lex-  ical selection phase. The resulting target language sentence is expected to have segments of words that would be in the target language word order, however the phrases may have to be reordered globally to form a well formed target language sentence. We use lexical reordering to transform the output of the lexical selection phase to create a well-formed target language sentence.  3. SFST Training In this section, we describe each of the components of our lexical selection system in detail.  3.1. Word Alignment The ﬁrst stage in the process of training a lexical selection model is obtaining an alignment function that given a pair of source (s1s2 . . . sn) and target (t1t2 . . . tm) language sentences, maps source language word subsequences into target language word subsequences, as shown below.  ∀i∃j(f (si) = tj ∨ f (si) = )  (1)  For this purpose, in the past, we have used the tree alignment algorithm described in [7]. For the work reported in this paper, we have used the GIZA++ tool [8] which implements a string-alignment algorithm. GIZA++ alignment however is asymmetric in that the word mappings are different depending on the direction of alignment – source-to-target or target-to-source. Hence in addition to the functions f as shown in Equation 1 we train another alignment function g as shown in Equation 2.  ∀j∃i(g(tj) = si ∨ g(tj) = )  (2)  English: I Japanese:  ÏneHed  tomÃakea  collecÂt ckall  $*d  »^%cW2  Alignment: 1 5 0 3 0 2 4  Figure 3: Example bilingual texts with alignment information  aI::ÏcHollneecetd:»Ã^%ccalWl $2*tod: make:Âk  Figure 4: Bilanguage strings resulting from alignments shown in Figure 1.  3.2. Bilanguage Representation From the alignment information in Figure 3, we construct a bilanguage representation of each sentence in the bilingual  11273  corpus. The bilanguage string consists of source-target symbol pair sequences as shown in Equation 3. Note that the tokens of a bilanguage could be either ordered according to the word order of the source language or ordered according to the word order of the target language.  Bf = bf1 bf2 . . . bfm  (3)  bfi = (si−1; si, f (si)) if f (si−1) =  = (si, f (si−1); f (si)) if si−1 =  = (si, f (si)) otherwise  Figure 4 shows an example alignment and the source- word-ordered bilanguage strings corresponding to the align- ment shown in Figure 3. We also construct a bilanguage using the alignment function g similar to the bilanguage using the alignment function f as shown in Equation 3. Thus, the bilanguage corpus obtained by combining the two alignment functions is B = Bf ∪ Bg.  3.3. Bilingual Phrase Acquisition and Local Reordering While word-to-word translation is only approximating the lexical selection process, phrase-to-phrase mapping can greatly improve the translation of collocations, recurrent strings, etc. Also, the use of phrases allows for reordering of words in the phrase to be in correct target language order, thus solving part of the lexical reordering problem. Moreover, SFSTs can take advantage of the phrasal correlation to improve the computation of the probability P (WS, WT ) [1]. The bilanguage representation could result in multiple words of the source sentence to be mapped to multiple words of the target sentence as a consequence of some words being aligned to . In addition to these phrases, we compute subsequences of a given length k on the bilanguage string and for each subsequence we reorder the target words of the subsequence to be in the same order as they are in the target language sentence corresponding to that bilanguage string. This results in a retokenization of the bilanguage where some tokens are source-target word pairs and others are source-target phrase pairs.  3.4. SFST Representation From the bilanguage corpus B, we train a n-gram language model using language modeling tools [9, 10]. The resulting language model is represented as a weighted ﬁnite-state automaton (S×T → [0, 1]). The symbols on the arcs of this automaton (si ti) are interpreted as having the source and target symbols (si:ti), making it into a weighted ﬁnite-state transducer (S → T × [0, 1]) that provides a weighted string-tostring transduction from S into T (as shown in Equation 4).  T ∗ = argmaxP (si, ti|si−1, ti−1 . . . si−n−1, ti−n−1) (4) T  4. Decoding In this section, we describe the decoding process and the global reordering process in detail. Since we represent the translation model as a weighted ﬁnite state transducer, the decoding process of translating a new source input (sentence or weighted lattice) amounts to a transducer composition and selection of the best probability path resulting from the composition.  T ∗ = π1(BestP ath(Is ◦ T ransF ST ))  (5)  However, we have noticed that on the development corpus, the resulting target sentence is typically shorter than the intended target sentence. This mismatch may be due to the incorrect estimation of the back-off events and their probabilities in the training phase of the transducer. In order to alleviate this mismatch, we introduce a negative word insertion penalty model as a mechanism to produce more words in the target sentence.  4.1. Word Insertion Model The word insertion model is also encoded as a weighted ﬁnite-state automaton and is included in the decoding sequence as shown in Equation 6. The word insertion FST has one state and | T | number of arcs each weighted with a λ weight representing the word insertion cost. The word insertion model penalizes or rewards paths which have more words depending on whether λ is positive or negative value.  T ∗ = π1(BestP ath(Is ◦ T ransF ST ◦ W IP )) (6)  4.2. Global Reordering  Local reordering as described in Section 3.3 is restricted by the window size and accounts only for different word order within phrases. During decoding we apply global reordering by permuting the words of the best translation and weighting the result by an n-gram language model (see also Figure 2):  T ∗ = BestP ath(perm(T ) ◦ LMt)  (7)  Unfortunately, even the size of the minimal permutation automaton grows exponentially with the length of the input sequence. While decoding by composition simply resembles the principle of memoization (i.e. in this case: all state hypotheses necessary to decode a sentence are kept in memory), it is necessary to either use heuristic forward pruning or limit the window of the allowed permutations. Similar to the way described in [11] the latter was used here. Decoding ASR output in combination with global reordering uses either n-best lists or extracts n-best lists from lattices ﬁrst. Decoding using global reordering is performed for each entry of the n-best list separately and the best decoded target sentence is picked from the union of the n intermediate results.  18  5. Discriminant Models for Translation The approach presented in the previous section is a generative model for statistical machine translation. However, discriminant modeling techniques have become the dominant method for resolving ambiguity in speech and natural language processing tasks outperforming generative models for the same task. Discriminative training has been used only for translation model combination [8] but not directly to train the parameters of a model. In recent work [12], we have started to work on discriminatively trained models for lexical selection, which we detail in this section. We expect these models to outperform generative models as well as provide a framework for incorporating rich morpho-syntactic information which result in sparseness for generative models. Statistical machine translation can be formulated as a search for the best target sequence that maximizes P (T |S), where S is the source sentence and T is the target sentence. Ideally, P (T |S) should be estimated directly to maximize the conditional likelihood on the training data (discriminant model). However, T corresponds to a sequence with a exponentially large combination of possible labels, and traditional classiﬁcation approaches cannot be used directly. Although, Conditional Random Fields (CRF) [13] train an exponential model at the sequence level, in translation tasks such as ours the computational requirements of training such models is prohibitively expensive.  5.1. Maximum Entropy-based Sequential Lexical Choice Model We approximate the string level global classiﬁcation problem, using independence assumptions, to a product of local classiﬁcation problems as shown in Equations 8.  N  P (T |S) = P (ti|Φ(S, i))  (8)  i  where Φ(S, i) is a set of features extracted from the source string S (shortened as Φ in the rest of the section). A very general technique to obtain the conditional distribution P (ti|Φ(S, i)) is to choose the distribution with least assumptions (with Maxent) that properly estimates the aver- age of each feature over the training data [14]. This gives us the Gibbs distribution parameterized with the weights λt where t ranges over the label set and V is the total number of target language vocabulary.  P (ti|Φ) =  eλti ·Φ  V t=1  eλt·Φ  (9)  The weights are chosen so as to maximize the conditional likelihood L = i L(si, ti) with  L(S, T ) = log P (ti|Φ) = log  i  i  eλti ·Φ  V t=1  eλt·Φ  (10)  The procedures used to ﬁnd the global maximum of this concave function include two major families of methods: Iterative Scaling (IS) and gradient descent procedures, in particular L-BFGS methods [15], which have been reported to be the fastest. We obtained faster convergence with a new Sequential L1-Regularized Maxent algorithm (SL1-Max) [16], compared to L-BFGS1. We have adapted SL1-Max to conditional distributions for our purposes. Another advantage of the SL1-Max algorithm is that it provides L1-regularization as well as efﬁcient heuristics to estimate the regularization meta-parameters. The computational requirements are O(V ) and as all the classes need to be trained simultaneously, memory requirements are also O(V ). Given that the actual number of non-zero weights is much lower than the total number of features, we use a sparse feature representation which results in a feasible runtime system.  5.1.1. Frame level discriminant model: Binary Maxent  For the machine translation tasks, even allocating O(V · F ) memory (where F denotes the number of features) during training exceeds the memory capacity of current computers. To make learning more manageable we factorize the word- level multi-class classiﬁcation problem into binary classiﬁca- tion sub-problems. This also allows for parallelization during training the parameters. We use here V one-vs-other binary classiﬁers at each word. Each output label t is projected into a bit string, with components bj(t). The probability of each component is estimated independently:  P (bj(t)|Φ)  =  
This is a short report of our participation to IWSLT-06. First, we let 2 commercial systems participate as fairly as possible (SYSTRAN v5.0 for CE, JE, AE, & IE, Atlas-II for JE), taking care of preprocessing and postprocessing tasks, and tuning as many "pairs" as possible by creating "user dictionaries" and finding a good combination of parameters (such as dictionary priority). Second, we took part in the subjective evaluation of CE results (fluency and adequacy). Details on experiments and methodological remarks are provided, with a perspective to introduce less expensive and more objective human- and task-related evaluation methods. Introduction MT evaluation is a hot topic since 1960 or so. The literature on evaluation may even be larger than that on MT techniques proper. MT evaluation may have several goals (i) help buyers buy MT system best suited to their needs (ii) help funders decide on which technology to support (iii) help developers measure various aspects of their systems, and measure progress. The MT evaluation campaign organized by the C-STAR III consortium falls in the latter category. Its aim is to measure the “quality” of various MT systems developed for speechto-speech translation when applied to the BTEC corpus [12]. Another goal is to compare the MT systems developed by the C-STAR partners not only between them, but also with other systems, notably commercial systems. In past similar campaigns, the commercial systems used as a “baseline” were tested in quite unfair ways, shedding serious doubts on the results. According to reports, experimenters submitted input texts to free MT web servers, instead of running a commercial version tunable by various parameter settings and building of "user dictionaries". For example, long ago, IBM CANDIDE system was trained intensively on the Hansard corpus, and then compared with an off-the-shelf version of SYTSRAN, without any tuning. SYSTRAN clearly won, but the margin might have been far bigger (or perhaps not, this should have been studied!), if SYSTRAN had been tuned to this totally unseen corpus, at the level of its dictionaries, of course, but perhaps also of its (procedural) grammars. The Microsoft group also compared its French-English MTS system with SYSTRAN [9]. MTS was highly tuned to their documents (indeed, the transfer component was 100% induced from 150,000 pairs of sentences and their associated “logical forms” or deep syntactic trees). In this case, SYSTRAN was slightly tuned by giving priority to SYSTRAN dictionaries containing computer related terms1. However, MSR apparently did not invest time to produce a user dictionary containing Microsoft computer terminology. Considering that technical terminology varies a lot from firm 
We describe a machine translation approach being designed at HKUST to integrate semantic processing into statistical machine translation, beginning with entity and word sense disambiguation. We show how integrating the semantic modules consistently improves translation quality across several data sets. We report results on ﬁve different IWSLT 2006 speech translation tasks, representing HKUST’s ﬁrst participation in the IWSLT spoken language translation evaluation campaign. We translated both read and spontaneous speech transcriptions from Chinese to English, achieving reasonable performance despite the fact that our system is essentially text-based and therefore not designed and tuned to tackle the challenges of speech translation. We also ﬁnd that the system achieves reasonable results on a wide range of languages, by evaluating on read speech transcriptions from Arabic, Italian, and Japanese into English. 1. Introduction The role and usefulness of semantic processing for Statistical Machine Translation (SMT) has recently been much debated. In previous work, we reported surprisingly disappointing results when using the predictions of a Senseval word sense disambiguation (WSD) system in conjunction with SMT using an IBM-style model (Carpuat and Wu, 2005b). Nevertheless, error analysis leaves little doubt that the performance of SMT systems still suffers from inaccurate lexical choice. Other empirical studies have shown that SMT systems perform much more poorly than dedicated WSD models, both ∗This work was supported in part by DARPA GALE contract HR0011-06-C-0023, and by the Hong Kong Research Grants Council (RGC) research grants RGC6083/99E, RGC6256/00E, and DAG03/04.EG09.  supervised and unsupervised, on Senseval WSD tasks (Carpuat and Wu, 2005a)—also suggesting that WSD still has a role to play in improving SMT. In this paper, we describe ongoing work on an approach being designed at HKUST to investigate the effect of semantic handling on current SMT models, using dedicated word sense and entity disambiguation modules. In particular, we propose a new architecture for integrating WSD into SMT architectures, and show that this additional semantic handling consistently improves translation quality across several data sets. We then turn to the IWSLT 2006 tasks, describing the experimental set-up and evaluation results. This represents a ﬁrst participation by HKUST in the IWSLT spoken language translation evaluation campaign. For this ﬁrst participation, we focused on building a baseline system for Chinese to English translation that could be easily ported to different language pairs. We therefore chose to translate additional input languages from different language families. We submitted translations of read and spontaneous speech in the Chinese to English task, as well as read speech translations from Arabic, Italian and Japanese into English. Despite the fact that the system is essentially text-based, and therefore is not designed and tuned to tackle the challenges of speech translation, the system achieves reasonable performance, yielding a BLEU score of 15.45 and a METEOR score of 44.56 for Chinese to English translation, our main language pair of interest. Results on other language pairs suggest that the system can achieve reasonable results with little modiﬁcation. 2. Machine translation engine The core MT engine as used in the experiments here is an off-the-shelf phrase-based statistical machine trans-  37  lation model. This is a useful engine since the approach has been shown to achieve competitive translation quality and is commonly used. Many state-of-the-art systems employ phrase-based approaches (e.g., Zens et al. (2005), Koehn et al. (2005), Sadat et al. (2005)). All phrase-based models make use of a phrasal bilexicon, but essentially differ in the bilexicon extraction and parameter estimation strategies, and the phrase reordering method. 2.1. Decoder For the experiments here, we used the Pharaoh decoder (Koehn, 2004), which implements a heuristic beam search for phrase based translation. While the phrase reordering model used in Pharaoh is weaker than in other proposed models, Pharaoh was chosen for the advantages of being freely available and widely used, and therefore constitutes an appropriate point of reference. 2.2. Phrasal bilexicon The core phrasal bilexicon is derived from the intersection of bidirectional IBM Model 4 alignments, obtained with GIZA++ (Och and Ney, 2002). The intersection is augmented using growing heuristics proposed by Och and Ney (2002) in order to improve recall. Following Koehn (2003), each entry in the phrasal bilexicon is scored using phrase translation conditional probabilities for both translation directions, as well as lexical weights which combine word translation probabilities according to the word alignment observed within the phrase pair during training. 2.3. Language model The language model is a standard trigram model with Kneser-Ney smoothing trained using the SRI language modeling toolkit (Stolcke, 2002). 3. Word sense disambiguation for translation lexical choice We now present a new architecture integrating a stateof-the-art WSD model into phrase-based SMT, and show that WSD produces small but consistent gains across several test sets.  3.1. WSD classiﬁers The model consists of an ensemble of four voting models combined by majority vote. The ﬁrst voting model is a na¨ıve Bayes model, since Yarowsky and Florian (2002) found this model to be the most accurate classiﬁer in a comparative study on a subset of Senseval-2 English lexical sample data. The second voting model is a maximum entropy model (Jaynes, 1979), since Klein and Manning (2002) found that this model yielded higher accuracy than na¨ıve Bayes in a subsequent comparison of WSD performance. The third voting model is a boosting model (Freund and Schapire, 1997), since has consistently turned in very competitive scores on related tasks such as named entity classiﬁcation, as described in Section 4.1.1. We also use the Adaboost.MH algorithm for WSD, just like for NER. The fourth voting model is a model based on Kernel PCA (Wu et al., 2004). Kernel Principal Component Analysis (KPCA) is a nonlinear kernel method for extracting nonlinear principal components from vector sets where, conceptually, the n-dimensional input vectors are nonlinearly mapped from their original space Rn to a high-dimensional feature space F where linear PCA is performed, yielding a transform by which the input vectors can be mapped nonlinearly to a new set of vectors (Scho¨lkopf et al., 1998). WSD can be performed by a Nearest Neighbor Classiﬁer in the high-dimensional KPCA feature space. We have showed that KPCAbased WSD models achieve close accuracies to the best individual WSD models, while having a signiﬁcantly different bias (Carpuat et al., 2004). All these classiﬁers have the ability to handle large numbers of sparse features, many of which may be irrelevant. Moreover, the maximum entropy and boosting models are known to be well suited to handling features that are highly interdependent. 3.2. WSD features The WSD classiﬁer employs much richer features than IBM-style statistical MT systems. The feature set consists of position-sensitive, syntactic, and local collocational features, since these features yielded the best results when combined in a na¨ıve Bayes model on several Senseval-2 lexical sample tasks (Yarowsky and Florian, 2002). All these WSD models were extensively evaluated on a wide range of monolingual and multilingual lexi-  38  Table 1: Evaluation results: integrating the WSD trans-  lation predictions improves BLEU and NIST scores  across 4 different Chinese-English test sets.  Test Set Experiment  BLEU NIST  DevTest 1 Baseline  40.76 7.9388  + WSD for SMT 41.28 7.9814  DevTest 2 Baseline  39.81 8.1533  + WSD for SMT 39.85 8.1753  DevTest 3 Baseline  49.26 9.1172  + WSD for SMT 49.81 9.1522  DevTest 4 Baseline  16.13 5.7258  + WSD for SMT 16.27 5.7569  Table 2: Translation examples with and without WSD  for SMT Example 1  Input ©  ÜU }  Ref.  May I see the menu ?  Baseline Let me see the menu ?  + WSD May I see the menu ?  Example 2  Input ý Š „ §M Ù  Ref.  Would you show me to my seat ?  Baseline Can you change my seat ﬁnger for me ?  + WSD Can you direct me to my seat ?  cal sample disambiguation tasks both on Senseval-2 and Senseval-3 data (e.g., Carpuat et al. (2004), Wu et al. (2004), Su et al. (2004)). 3.3. Repurposing the WSD models for SMT Table 1 shows that our method of integrating a state-ofthe-art WSD model into phrase-based SMT produces small but consistent gains across all Chinese-English development test sets. The main difference between this approach and our earlier experiments (Carpuat and Wu, 2005b) lies in the fact that we focus on repurposing the WSD system for SMT. Rather than using a generic Senseval WSD model, both the WSD training and the WSD predictions are integrated into the SMT framework. Speciﬁcally: • Instead of using a Senseval system, we redeﬁne the WSD task to be as close as possible to the translation disambiguation task faced by the SMT system. • Instead of using predeﬁned senses drawn from  manually constructed sense inventories such as HowNet (Dong, 1998), our WSD for SMT system directly disambiguates between all translation candidates seen during SMT training. • Instead of learning from manually annotated training data, our WSD system is trained on the same corpora as the SMT system. Thus, in a given SMT input sentence, for every word that was seen in the training data, we have a WSD model and a context-dependent distribution over the possible translation candidates of the word. This distribution is used to augment the baseline bilexicon. With Pharaoh, we use the provided XML markup scheme to speciﬁy translation candidates and their corresponding probabilities. At decoding time, these externally generated translation candidates are considered as if they were additional bilexicon entries, and are used to build translation hypotheses that compete with other translation hypotheses build from within the traditional SMT phrasal translation lexicon. Analysis shows that the WSD translation probabilities give better rankings and are more discriminative than the baseline translation probabilities, yielding improved translations as can be seen in Table 2. 4. Named-entity translation Recognizing, disambiguating, and translating entities is a special case of word sense disambiguation for translation lexical choice, where the words or phrases in question are entities of various sorts. Translating names correctly is particularly important to translation quality and usefulness, but does present some distinct challenges from regular phrase translation. First, the vast majority of names are rare and often never seen in training, and, with the exception of names of well-known persons or other entities, are typically not recorded in lexicons. Second, whether a phrase is a named-entity (NE) depends on context and is therefore ambiguous. Third, names have speciﬁc translation patterns. For instance, the translation of a person name usually cannot be inferred from the translation of each of its components. The ﬁrst step in handling NE translation consists in identifying NE boundaries and their type. In this system, we are focusing on identifying the PERSON, LOCATION and ORGANIZATION entity types. For the purpose of translation, identifying NE boundaries is not sufﬁcient, since the type of a NE affects the translation  39  Table 3: IWSLT-06 Training data statistics computed for the 4 language pairs  Training Data Statistics  Chinese-English Arabic-English Italian-English Japanese-English  Number of bisentences  39953  19972  19972  39953 
We present techniques for improving domainspecific translation quality with a relatively high OOV ratio on test data sets. The key idea is to maximize the vocabulary coverage without degrading the translation quality. We maximize vocabulary coverage by segmenting a word into a sequence of morphemes, prefix*-stem-suffix* and by adding a large amount of out-of-domain training corpora. To preserve the domainspecific meaning of vocabularies occurring in both domain-specific and out-of-domain training corpora, we assign a higher weight to the domain-specific corpus than to the out-ofdomain corpora. IBM Arabic-to-English spoken language translation systems using these techniques have demonstrated the best performances in the Open Data Track of the IWSLT2006 Evaluation Campaign. 1. Introduction Creating a large amount of domain-specific parallel corpus by manual translation is very costly and time consuming. And the sparsity of a domain-specific parallel corpus often leads to a very high OOV ratio on unseen data. For example, the OOV ratio of the Arabic-to-English translation development test data for the IWSLT 2006 Evaluation Campaign with respect to the supplied BTEC corpus (Basic Traveler’s Expression Corpus), consisting of about 20k sentence pairs, is over 10%. It has also been noted in previous evaluation campaigns that improving the translation quality of domain-specific evaluation data by adding out-ofdomain bilingual corpora is quite challenging. In IWSLT 2004 Chinese-to-English translations, [2], training data for the Small Data track was limited to the supplied BTEC corpus, whereas additional out-ofdomain corpora could be used for the Additional Data Track. Tables 1 & 2, illustrate the impact of out-ofdomain bilingual corpora on BTEC translation quality.  BLEU NIST GTM mPER mWER CE-S 0.454 8.55 0.720 0.390 0.455 CE-A 0.351 7.39 0.655 0.420 0.496 Table 1. Comparison of the best performing systems in the small (CE-S) and the additional (CE-A) data tracks Table 1 indicates the performance of the best performing system in the additional (CE-A) data track is consistently worse than that of the best performing system in the small (CE-S) data track. BLEU NIST GTM mPER mWER S1-S 0.374 7.74 0.672 0.425 0.488 S1-A 0.311 5.82 0.632 0.480 0.572 S2-S 0.349 7.09 0.644 0.430 0.507 S2-A 0.351 7.39 0.655 0.420 0.496 Table 2. Comparison of two systems (S1 & S2) in their small (S) and additional (A) data track submissions Table 2 shows that the performance of system S1 is consistently better in the small data track (S1-S) than in the additional data track (S1-A). For system S2, its performance in the additional data track (S2-A) is only marginally better than in the small data track (S2-S). Given the sparsity of a domain-specific parallel corpus in general and the difficulties of improving domain-specific translation quality by adding out-ofdomain corpora which exist in a large amount for many language pairs, e.g. Arabic−English, Chinese−English, we present techniques to improve domain-specific translation quality by maximizing vocabulary coverage, which have proven effective for our Arabic-to-English translation systems. Maximization of vocabulary coverage is achieved by (i) segmentation of a word into a sequence of morphemes, prefix* − stem − suffix* 1 [5], and morphological analysis [6], and (ii) addition of a large amount of out-of-domain corpora. To overcome the problem of performance degradation by adding out-ofdomain corpora, we assign a higher weight to the domain-specific training corpus than to the out-ofdomain corpora for translation model training. This enables the system to choose the domain-specific 
This paper reports on the participation of ITC-irst to the evaluation campaign of the International Workshop on Spoken Language Translation 2006. Our two-pass system is the evolution of the one we employed for the 2005 campaign: in the ﬁrst pass, an N-best list of translations is generated for each source sentence by means of a beam-search decoder; in the second pass, N-best lists are rescored and reranked exploiting additional feature functions. Main updates brought to the 2005 system involve novel additional features which are here described. Results on development sets are analyzed and commented. 1. Introduction In this paper, we report on the participation of ITC-irst to the evaluation campaign of the International Workshop on Spoken Language Translation 2006. We submitted runs under the Open Data conditions for all the language pairs: Arabic-to-English, Chinese-to-English, Japanese-to-English and Italian-to-English. For each language pair, we performed translations of both manual transcriptions and ﬁrst-best transcriptions provided automatically by a speech recognizer. The statistical machine translation (SMT) system developed for the evaluation is an evolution of the one employed for the 2005 IWSLT campaign [1]. The main update is the introduction of some new additional features into the rescoring module: some of them are well known; others, like modeling word reordering phenomena, are new. The paper is organized as follows. Section 2 brieﬂy describes the ITC-irst phrase-based SMT system, spending some more words on new additional features. In Section 3, experimental setups of the evaluation campaign runs and results are presented and discussed. A conclusion section ends the paper. 2. System Description ITC-irst SMT system [1] implements a log-linear model and features a two-step decoding strategy. In the ﬁrst pass, a dynamic programming beam search algorithm generates N-best translation hypotheses for each source sentence. In the sec-  ond pass, a larger set of local and global feature functions is employed to re-score and re-rank the N-best lists. The resulting top-scored entry of each list is ﬁnally returned as best translation hypothesis. 
This paper describes the SMT we built during the 2006 JHU Summer Workshop for the IWSLT 2006 evaluation. Our effort focuses on two parts of the speech translation problem: 1) efﬁcient decoding of word lattices and 2) novel applications of factored translation models to IWSLT-speciﬁc problems. In this paper, we present results from the open-track Chinese-to-English condition. Improvements of 5-10% relative BLEU are obtained over a high performing baseline. We introduce a new open-source decoder that implements the state-of-the-art in statistical machine translation. 1. Introduction With advances in both speech recognition and language translation technologies, speech translation has become more viable for real applications. It is still, however, the case that both ASR and MT technologies are prone to high levels of error. Thus, robust techniques of combining these technologies are needed to make speech translation viable for real applications. Empirically, we know that reduced error rates from upstream ASR systems result in higher translation quality. Our goal is to exploit multiple hypothesis to efﬁctively lower the error rate relative to the 1-best transcription. In this paper, we introduce the notion of confusion network MT decoding for ASR input and discuss novel applications of this technique. We also present a new application of factored translation models for postprocessing MT output. Both confusion network decoding and factored models are implemented in a new, open-source MT decoder called moses that was built as part of the 2006 JHU summer workshop. We introduce moses as a platform for MT research. 1.1. Confusion Networks During decoding, ASR systems can typically generate multiple alternatives in addition to a single-best transcription. It is customary to represent these alternatives using a word graph †This work is sponsored by the Air Force Research Laboratory under Air Force contract FA8721-05-C-0002. Opinions, interpretations, conclusions and recommendations are those of the authors and are not necessarily endorsed by the United States Government.  0  @BG @BG è @BG  256  258  255  257  @BG è  è  259  @BG  è  @BG  260  @BG  261  
This paper describes our Kyoto-U system that attended the IWSLT06 Japanese-English machine translation task. Example-based machine translation is applied in this system to integrate our study on both structural NLP and machine translation. 1. Introduction Machine translation has been actively studied recently, and the major approach is Statistical Machine Translation (SMT). An alternative to SMT is Example-based machine translation (EBMT)[1]. The most important common feature between SMT and EBMT is to use a bilingual corpus, or translation examples, for the translation of new inputs. Both methods exploit translation knowledge implicitly embedded in translation examples, and make MT system maintenance and improvement much easier compared with Rule-Based Machine Translation. On the other hand, EBMT is different from SMT in that SMT hesitates to exploit rich linguistic resources such as a bilingual lexicon and parsers; EBMT does not consider such a constraint. SMT basically combines words or phrases (relatively small pieces) with high probability [2]; EBMT tries to use larger translation examples. When EBMT tries to use larger examples, it can better handle examples which are discontinuous as a word-string, but continuous structurally. Accordingly, though it is not inevitable, EBMT can quite naturally handle syntactic information. Besides that, the difference in the problem setting between EBMT and SMT is also important. SMT is a natural approach when linguistic resources such as parsers and a bilingual lexicon are not available. On the other hand, in case that such linguistic resources are available, it is also natural to see how accurate MT can be achieved using all the available resources. We chose the latter problem setting to conduct EBMT research, and here we would like to mention two reasons we chose this setting. First, we are aiming at the improvement of structural NLP. We have been conducting research on Japanese morphological analyzers, parsers, and anaphora/omission analy-  ੤Ꮕ (cross) ὐ 䈪 䇮(point) ⓭ὼ (suddenly) 䈅䈱 (that) ゞ 䈏 (car) 㘧䈶಴䈚䈩 ᧪䈢 䈱䈪䈜 䇯 (rush out)  the car came at me from the side at the intersection  Figure 1: An example of parallel sentence alignment. (The root of a tree is placed at the extreme left and phrases are placed from top to bottom. Correspondences of underlined words were detected by a bilingual dictionary.)  ses. MT is considered as an application of these fundamental technologies. Amelioration of fundamental NLP technologies naturally improves applications, and applications give some feedback to fundamental NLP, pointing out the shortcomings. Needless to say, MT is not the only NLP application, and monolingual NLP applications such as manmachine interfaces and information retrieval can beneﬁt from the improvement of fundamental NLP. The second point is that, in practice, we often encounter some cases in which the EBMT problem setting is suitable. That is, if there is no huge bilingual corpus which enables SMT, but some very similar translation examples are available, it would be nice if automatic translation or translation assistance can be provided by exploiting the examples. For example, translation of manuals when translations of the old version manuals are available, and patent translation when translations of the related patents are available. Or, in the translation of an article, the translations to a certain point can be used effectively as translation memory step by step, because the same or similar expressions/sentences are often used in an article. In such cases, an EBMT approach which tries to ﬁnd larger translation examples is suitable. This paper describes our Japanese-English EBMT system, Kyoto-U, challenged to IWSLT06, and reports the evaluation results and discussion.  64  2. Alignment of Parallel Sentences Our system consists of two modules: an alignment module for parallel sentences and a translation module retrieving appropriate translation examples and combining them. First, we explain the alignment module. The alignment of Japanese-English parallel sentences is achieved by the following steps, using a Japanese parser, an English parser, and a bilingual dictionary (see Figure 1). 1. Dependency analysis of Japanese and English sentences. 2. Detection of Word/phrase correspondences. 3. Disambiguation of correspondences. 4. Handling of remaining words. Among IWSLT06 39,953-sentence training data, some pairs consist of two or more sentences. We utilized the pairs with the same number of Japanese sentences and English sentences, and separated them into one-to-one Japanese– English sentence pairs. As a result, we utilized 43,318 sentence pairs. We explain these alignment steps in detail. 2.1. Dependency Analysis of Japanese and English Sentences Japanese sentences are converted into dependency structures using a morphological analyzer, JUMAN, and a dependency analyzer, KNP [3]. These tools can detect Japanese sentence structures with high accuracy: for the news article domain, 99% for segmentation and POS-tagging, and 90% for dependency analysis. They are robust enough to handle travel domain conversations and the accuracy is almost the same with news article sentences. Japanese dependency structure consists of nodes which correspond with content words. Function words such as postpositions, afﬁxes, and auxiliary verbs are included in content words’ nodes. For English sentences, Charniak’s nlparser is used to convert them into phrase structures [4], and then they are transformed into dependency structures by rules deﬁning head words for phrases. In the same way as Japanese, each content word composes a node of English dependency tree. The “Self-trained parser” introduced in ACL-COLING06 [5] is used in our system. Since Charniak’s nlparser was trained on the penn Treebank, it is not necessarily suitable for travel domain conversations. In some cases, basic English sentences were wrongly parsed by the parser. 
The MIT-LL/AFRL MT system is a statistical phrasebased translation system that implements many modern SMT training and decoding techniques. Our system was designed with the long-term goal of dealing with corrupted ASR input and limited amounts of training data for speech-to-speech MT applications. This paper will discuss the architecture of the MIT-LL/AFRL MT system, improvements over our 2005 system, and experiments with manual and ASR transcription data that were run as part of the IWSLT-2006 evaluation campaign. 1. Introduction In recent years, the development of statistical methods for machine translation has made usable MT a real possibility. Speciﬁcally, advances in methods to: • Extract word alignments from parallel corpora [1][2] • Learn and model the translation of phrases [3] [4] • Combine and optimize model parameters [5] [6] [7] • Decode and Rescore Test data [8] [9] These advances have helped to dramatically increase the quality of MT output. Our 2006 IWSLT system extends these methods and work we did in 2005 [10]. In subsequent sections, we will discuss the details of the translation system including our alignment and language models and methods we’ve implemented for optimization and decoding. Speciﬁcally, we will highlight improvements and changes made to: 1. Better utilize the larger 2006 training set 2. Coverage of Italian and Japanese 3. Enhance the coverage of extracted phrases †This work is sponsored by the Air Force Research Laboratory under Air Force contract FA8721-05-C-0002. Opinions, interpretations, conclusions and recommendations are those of the authors and are not necessarily endorsed by the United States Government.  Model Training  Ch  En  Training Bitext  GGIIZZAA++++ WWoorrdd AAlliiggnnmmeenntt  AAlliiggnnmmeenntt EExxppaannssiioonn  PPhhrraassee EExxttrraaccttiioonn  MMiinniimmuummEErrrroorr RRaatteeTTrraaiinniinngg  Ch  En  Dev Set  Translation Ch Test Set DDeeccooddee RReessccoorree En Translated Output  Figure 1: Basic Statistical Translation Architecture 4. Better models and better decoding 5. Increase gains from rescoring n-best lists As this year’s evaluation conditions have changed, our basic translation training and decoding processes have been adapted accordingly, as shown in Figure 1. Boxes in grey have not changed substantially since 2005. Refer to [10] for more detail regarding the implementation of these modules. We submitted systems for Chinese, Japanese and Italianto-English language pairs. In each case, we used only the supplied data for each language pair for training and optimization. From these data, we extract word/character alignments. These alignments are then expanded using slightly modiﬁed versions of standard heuristics. This process is described in detail in Section 3. Phrases are then extracted and counted, and the resulting phrase table is then used for de-  71  Improved Training Hybrid Alignment Template Models • Apply Factored Models using Unsupervised Word Classes • Can we use automatic word classes to learn general sequence constraints? • First Experiment: Word Class LMs of varying orders  How much is it? Source Phrase  Target Phrase 总共 多少 钱 ? c1 c22 c3 c55  word class  Surface  MMooddeell  999999-6 WS 9/28/2006  MIT Lincoln Laboratory  Figure 2: A Factor-based Consistency-Checking Model  coding and rescoring. Language models are trained using the English side of each language pair. Using development bitexts separated from the training set, we then employ a minimum error rate training process to optimize model parameters utilizing a held-out development set. These trained parameters and models can then be applied to test data during decoding and rescoring phases of the translation process. 2. Data Preprocessing For Chinese and Japanese texts, we used the supplied UTF8 encodings and converted all roman characters into ASCII. We used Latin-1 encoding for all Italian texts. Source and target side training texts are lower-cased before training. Because this year’s evaluation data (and devset 4) included no source punctuation, we implemented a sourcelanguage repunctuator to better match the training data. 3. Improved Word/Character Alignments In this year’s system we employed multiple word and character alignment strategies, extending the method described in [11]. For all language pairs, we combine alignments from IBM model 5 see [1] and [12] and alignments extracted using the competitive linking algorithm (CLA) described in [13]. We apply a simple χ2 likelihood function, though we found only minor differences between this function and others that have been proposed in the literature [14]. Phrases were extracted from both types of alignments and combined in one phrase table. This was done by summing counts of phrases extracted from alignment types before computing the relative frequency used in the our phrase tables. Additionally, for Chinese-to-English translation, both word and character segmentation were for training CLA and GIZA alignment models. Phrases were then extracted from all four alignments and combined. Word segmented phrases were resegmented into characters before counting. 4. Improved Translation Models Following the 2006 JHU summer workshop we conducted a number of experiments with factored translation models using our training/decoding paradigm. To this end we integrated the moses decoder into minimum error rate training decoding processes. This allowed us to try two different factor-based approaches to the IWSLT Chinese-English translation task.  IWSLT Chinese Alignment Templates for Translation  Analysis  安 Surface c3 Word Class  calm c8 Generation  • Second Experiment: Extend Class-based LM to the translation Figure 3M:oAdelParallel Word Class/Surface Translation Model • Bigram word classes for source and target • Translate alignment templates similar to [Och 98] + surface Factored translation models extend standard phrasebased •staAtpipslytiLcMalto msurofadcee lasndbCylassrepresenting words as vectors of factors. This representation can be used to decompose MIT Lincoln Laboratory 999999-7 WS 9/28/2006 words into constituent parts (e.g. lemma + afﬁx) for the purpose of modeling them separately, or as generalizing words into larger linguistic “classes” (e.g. part-of-speech). From a factored representation, it is possible to train standard statistical models that are then combined using standard log-linear assumptions in which feature functions of the form hF ACT ORk (e1...i, f1...j) represent translation likelihoods that are speciﬁc to factor k and special generation features hgen(F ACT ORk(ei), F ACT ORl(ei)) that represent the likelihood of generating F ACT ORk from F ACT ORl. Because we did not have access to analysis tools in Chinese during the IWSLT evaluation, we chose to create models using automatically derived word classes (as generated by mkcls). In our experiments words are represented both by their surface form and by their associated word classes. Using this representation we trained two different models:  • Consistency-Checking Model – Translate source surface forms to target, generate word classes for each target, then apply a class-based LM.  • A Parallel Translation Model – Translate both source surface forms and word-classes to target word/class pairs, then apply a class-based LM.  These models are shown schematically in Figure 2 and Figure 3, respectively. We note that the parallel approach is quite similar to the alignment template model proposed in [15] with an additional surface-to-surface form translation model. These models were not applied in time for ofﬁcial submission to the 2006 evaluation, but in post-evaluation experiments we found these models to be quite helpful.  5. Improved Decoding For the 2006 evaluation we used a combination of two decoders: our in-house decoder mtdecoder and the moses decoder developed as part of the 2006 JHU summer workshop. For most experiments, both decoders performed on par with each other (though we generally used our own decoder for minimum error rate training, because of it’s speed). For factored experiments, we used moses. With both decoders we found it advantageous to use 4-gram and 5-gram language models in decoding. Our ofﬁcial submissions for Chinese, Japanese and Italian use 4-gram Interpo-  72  Better Decoding Reordering Constraints  • Revisit ReorderinBg CeotntestrraDinetsc[Zoednsin0g3] – IBM ConstraRinetso: rMduestricnovgerCfiorsnt sKturnacionvtesred positions  • RSoevuirsciet Reo安rdering共Const少raints [钱Zens 03]  – IBM Constraints: Must cover first K uncDoivsearelldopwoesidtions  SToaurrgceet c安alm m共oney 少  钱  (k = 2)  – ITG Constraints [Wu 95, 97]  Disallowed  Figure T4a:rgAent eDxciasaamllmlopwl‘einmsoiodfnee-aoyutdsiidsea’ mlloovwemeedntsreo(rkde=r2in)g using IBM  Source constraints  安  共  少  钱  – ITG Constraints [Wu 95, 97]  Disallowed  Disallow ‘inside-outside’ movements  Target Source  small 安  calm 共  money 少  with 钱  • Faster, some BLEU improvement in JapaDniessaellowed  Target 999999-9 WS 10/29/2006  small  calm money with  MIT Lincoln Laboratory  • Faster, some BLEU improvement in Japanese  Figure 999999-9 5: An example of a disallowed reorderinMgIT using ITG Lincoln Laboratory WS 10/29/2006 constraints  lated Knesser-Ney models trained using the SRI Language Modeling Toolkit [16] [17] [18]. Using our decoder we implemented three types of reordering constraints, revisiting work done in [19] with the IWSLT-2006 data. We explored both ITG [20] and IBM constraints, and the results shown in Section 7 indicate that different reordering constraints don’t decrease the BLEU score signiﬁcantly in most language pairs while reducing decoding time by 20-50%. Both constraints disallow certain reording conﬁgurations. Figures 5 and 4 offer examples of these conﬁgurations. Details of these experiments are described in [21].  6. Rescoring N-best Lists As in 2005, we employ minimum error rate training to optimize model scaling factors for both decoding and rescoring features. In this year’s evaluation, we added 5-gram rescoring language models and 6-gram class-based rescoring language models after decoding. After the evaluation we added sentence length posterior features for rescoring. A full list of the feature functions used in our system is shown in Table 1. We approximate sentence length posteriors from the n-best list as:  P (L|f1...J ) ≈  P (e1...L|f1...J )  (1)  {e | |e|=L}  Similarly IBM model 1 scores can be computed for each nbest list entry:  
This paper describes the NiCT-ATR statistical machine translation (SMT) system used for the IWSLT 2006 evaluation compaign. We participated in all four language pair translation tasks (CE, JE, AE and IE) and all two tracks (OPEN and CSTAR). We used a phrase-based SMT in the OPEN track and a hybrid multiple translation engine in the CSTAR track. We also equipped our system with some of new preprocessing and post-processing techniques for Chinese word segmentation, named entity translation, punctuation and capitalization, sentence splitting, and language model adaptation. Our experiments show these features signiﬁcantly improved our system. 1. Introduction Phrase-based statistical machine translation (SMT) has progressed over the years and is the primary approach for SMT research. This approach is used by 80% of the systems participating in the NIST 2006 machine translation evaluation. Our main translation engine for this year’s IWSLT evaluation, TATR, is also a phrase-based SMT. The hybrid multiple engine approach, that was used last year [1], was used again this year. But we replaced the 2005 SMTs (PBHMTM and SAT) with TATR, partly for simpliﬁcation reasons. In addition to TATR, two other engines are included in this year’s hybrid system: HPATR3, a SMT based on syntactic transfer; and EM, the translation memory based on exact match. We employed new approaches for pre-processing, postprocessing, and language modeling. We used subword-based Chinese word segmentation [2]. This word segmentation achieved the highest F-score rate for the second Sighan test data, and can recognize numerical expressions and foreign names. We built a conversion model to implement capitalization and punctuation by using the maximum entropy principle and the conditional random ﬁeld (CRF) approach, which can integrate long-range features to enhance performance. We applied sentence-splitting techniques to all languages. This approach signiﬁcantly improved CE and JE translation.  For language modeling, we used a new language model adap-  tation approach that can divide training data by topic. For  each topic, a topic-dependent language model was built and  applied to input belonging to this topic at the time of transla-  tion. We found this approach also improved translations.  In this year’s evaluation, we participated in all four lan-  guage track.  pair translation tasks and two A list of all tests is shown in  tTraabckles:1O, wPhEeNrean“√d ”CSinTdAi-R  cates we participated in the test and “×” means we did not.  We participated in 14 out of all the 18 tests.  Our translation system ﬂow chart is illustrated in Fig. 1.  Before the input is translated by the MT engines, it is pre-  processed with a series of preprocessing methods: word seg-  mentation and sentence-splitting. We used three translation  engines, TATR, HPATR3 and EM, and used Selector for the  CSTAR track. But we used only one translation engine, TATR,  for the OPEN track. The ﬁnal output is generated after the  post-processing module for the punctuation and capitaliza-  tion.  In the following sections, section 2 describes our word  segmentation methods. Section 3 describes our language  model adaptation. Section 4 describes the subword-based  name entity translation for Chinese. Section 5 describes the  translation engines, TATR and Selector. Section 6 describes  our CRF-based punctuation and capitalization. Section 7  presents our evaluation results. Section 8 provides our con-  clusions and comments.  2. Preprocessing 2.1. Arabic segmentation Of the released data, we threw away all end-of-utterance markers. However, we kept any sentence markers that were in the middle of an utterance, but standardized them to the same unique marker. This punctuation preprocessing was performed on the Arabic data as well as on the English data. The morphological analysis of Arabic was performed using the BAMA as released by the LDC consortium [3]. This implied that we had ﬁrst to convert the encoding from UTF-8 into the Windows-1256 encoding.  83  language track OPEN CSTAR  Table 1: List of IWSLT 2006 tests that we have participated in  CE  JE  AE  IE  spont√aneous re√ad cor√rect re√ad cor√rect re√ad cor√rect re√ad cor√rect  √  √  √  √  √  ×  ×  ×  ×  Preprocessing  input  Segmentation Sentence Split  Translation Engines  MT1  hyp1  TATR  MT2  hyp2  HPATR3  Selector TMi* LMi  Postprocessing output(OPEN track) Punctuation output(CSTAR track) Capitalization  MT3  hyp3  EM  train n multiple pairs of TM and LM  TM 1 LM 1  TM n LM n  subset 1  subset n  parallel text corpus  Figure 1: Overview of our translation system  The Arabic writing system composes the base of a word with a set of proclitics and enclitics: conjunctions, particles, the article and pronouns, etc. Such agglutinated forms are highly ambiguous, and one such inﬂected form yields on average about 7 diﬀerent readings. As the size of the supplied data is small, we followed the conclusions of [4] and opted for a morphological analysis of Arabic. The output of the BAMA is a list of readings of agglutinated forms in their transcribed form. In contrast, our approach diﬀers from the technique used in [5] where the MADA tool is used to select the best hypothesis among the candidate parses, we did not perform any disambiguation but chose to select, in a consistent way, the ﬁrst hypothesis of the BAMA output. The input to the machine translation system was the Buckwalter transcribed Arabic. 2.2. Chinese subword-based word segmentation We used a Chinese subword-based word segmentation [2] that is illustrated in Figure 2. This word segmentation is composed of three steps. The ﬁrst is a dictionary-based word segmentation, similiar to the default word segmentation provided by LDC. The second is a subword-based IOB tagging implemented by a CRF tagging model. The subword-based IOB tagging achieves a better segmentation than characterbased IOB tagging. The third step is conﬁdence dependent disambiguation to combine the previous two results. The subword-based word segmentation was evaluated in  Table 2: Comparison of diﬀerent Chinese word segmentations for the NIST 2005 test data  BLEU NIST WER PER METEOR  LDC default 0.226 7.62 0.895 0.642 0.528  OURs  0.237 7.93 0.867 0.614 0.525  both the Sighan Bakeoﬀ and the NIST machine translation. In the second Sighan Bakeoﬀ, the segmentation gave a higher F-score than the best published results. We also evaluated this in SMT using the NIST evaluation 2005 data, its BLEU score was 1.1% higher than that using the LDC provided default word segmentation. The results are shown in Table 2.2. 2.3. Japanese and Italian word segmentation A Ngram-based (word trigrams + POS trigrams) word segmentation was used for Japanese processing. No Italian word segmentation was required. 2.4. Sentence splitting Sentence splitting is a new technique we applied to this evaluation. We used sentence splitting to cut long sentences into short segments. We did so by automatically adding punctuation to the ASR output without punctuation, and splitting  84  input 咘㣅᯹ԣ೼࣫ҀᏖ +XDQJ<LQJ&KXQOLYHVLQ%HLMLQJFLW\ Dictionary-based word segmentation 咘㣅᯹ԣ೼࣫ҀᏖ +XDQJ<LQJ&KXQOLYHVLQ%HLMLQJFLW\ Subword-based IOB tagging 咘%㣅,᯹,ԣ2೼2࣫Ҁ%Ꮦ, +XDQJ%<LQJ,&KXQ,OLYHV2LQ2%HLMLQJ%FLW\, Confidence-based segmentation 咘%㣅,᯹,ԣ2೼2࣫Ҁ%Ꮦ, +XDQJ%<LQJ,&KXQ,OLYHV2LQ2%HLMLQJ%FLW\, output 咘㣅᯹ԣ೼࣫ҀᏖ +XDQJ<LQJ&KXQOLYHVLQ%HLMLQJFLW\ Figure 2: Subword-based Chinese word segmentation the output in the place of the added punctuation. Each segment was passed to translation engines. The ﬁnal translation of the original ASR output was linearized by all the segments’ translation. From the statistics, there were 1.28 segments for each sentence on average. Sentence splitting was implemented by adding punctuation using the SRI tool, “hidden-ngram.” 3. Topic-dependent language model adaptation A language model plays an important role for SMT. The effectiveness of a language model is signiﬁcant if the test data happen to have the characteristics of the training data for the language models. However, this coincidence is rare in practice. To avoid this performance reduction, a topic adaptation technique is often used. We applied this adaptation technique to machine translation. For this purpose, a “topic” is deﬁned as clusters of bilingual sentence pairs. For bilingual sentence pair clustering, the following process is used: 1. The number of topic is the number of ﬁxed clusters. 2. All of the sentence pairs are randomly assigned to one cluster. 3. For each cluster, language models for e and f are created using the sentence pairs that belong to each cluster. 4. For each cluster, the entropy for e and f is calculated using the language model from each cluster. Total entropy is deﬁned as total sum of the entropies of each cluster.  Table 3: Topic adaptation for J-E translation Model BLEU NIST WER PER Baseline 22.17 6.68 68.09 51.72 Adapted 23.57 6.81 67.16 50.51 Table 4: Topic adaptation for C-E translation Model BLEU NIST WER PER Baseline 21.66 6.78 70.88 51.29 Adapted 22.77 6.96 69.75 50.75 5. Each translated sentence pair in each cluster is moved to other clusters to give the smallest total entropy. 6. Repeat above process, until the entropy reduction is smaller than a given threshold. Topic can be deﬁned according to the above process. In the decoding, for a source input sentence, f , a topic T is determined by maximizing P( f |T ). To maximize P( f |T ) we select cluster T that gives the largest likelihood for a given translation source sentence f . After the topic is found, a topic-dependent language model P(e|T ) is used instead of P(e), the topic-independent language model in the log-linear models. The topic-dependent language models were tested using IWSLT06 data. Experimental results are shown in the tables 3 and 4. Our approach improved the BLEU score by 1.1% ∼ 1.4%. The paper of [6] presents a detailed description for this work. As far as we know, a clustered language model was also used by [7] in this evaluation. Our work and results were similar to theirs. In this evaluation, topic-dependent language model adaptation was used in only the TATR engine and in the translation of JE, CE and IE. 4. Subword-based translation for Chinese It is possible that some words in the test data are not in the translation table extracted from the bilingual training corpus. There are no translations for those words. Some of these words are rare words. Some are out-of-vocabulary (OOV) words recognized by the subword-based word segmentation that can recognize Chinese numerical expressions and named entities such as place name, organization name, and person name. These new generated words cannot ﬁnd corresponding translations in the translation table. For example, “ӉԽਫ਼” is a new word generated by the segmenter if it is labeled as, “Ӊ/B Խ/I ਫ਼/I”. “ӉԽਫ਼” cannot be found in the translation table, thus cannot be translated.  85  As a Chinese word is composed of two or more connected characters, we introduce subwords and segment a nontranslated word into subwords, each of which consists of fewer characters than the original word. Even if the original word is a rare word or OOV, the resulting subwords are not, and are translatable respectively. The subword-based translation model was trained as follows: First, we deﬁned a subword list from the LDC corpus, consisting of the most frequent words. There were 5,000 words in the list. Second, we used an LDC-provided Chinese named entity corpus, LDC2002L27, as the bilingual corpus for training the subword translation model. We segmented Chinese sentences in the corpus into subwords, using a dictionary-based word segmentation approach. Thus, we obtained a training corpus for the translation model with subword sequences on the Chinese side and the corresponding English translation. Third, a phrase-based translation model for translating subwords was trained using the same training approach (described in the next section). Once an OOV is found in the test data, we ﬁrst apply a subword-based word segmenter to segment the OOV into a subword sequence, and then we use the subword translation model to translate the OOV. Finally we append the OOV and its translation into the translation table, so the OOV can be translated using the new translation table. By using this approach, about 95% of the OOVs can be translated. We tested the subword-based OOV translation model using the NIST MT 2005 evaluation data, a 0.4% BLEU score increase was observed. 5. Translation engines We used three translation engines in this evaluation: TATR, a phrase-based SMT system; HPATR3, an SMT system based on syntactic transfer; and EM by exact match. For the OPEN track, only TATR was used; for the CSTAR track, a hybrid system using three engines and Selector was used. See Figure 1. 5.1. TATR TATR is a phrase-based SMT system built within the framework of feature-based exponential models:  Pr(e1I | f1J) =  I  
In this paper we describe a hybrid approach to Chinese-toEnglish spoken language translation system used for the IWSLT 2006 evaluation campaign. In this system, the phrasebased statistical machine translation (SMT) engine is combined with the template-based machine translation (TBMT) engine and a simple way is proposed to select the best translation from the results generated by the two translation engines. The experiments prove that the combination can improve the performance of translation system. As the input sentences are speech recognition results and have no punctuation information, we restore the punctuation in source sentences in the processing and postprocessing. 1. Introduction In this paper, we describe a hybrid approach to Chinese-toEnglish spoken language translation system that combines the TBMT system with the phrase-based SMT system. We use a simple method to select the best translation from the results generated by the two translation system. The template-based translation system is easy to realize and can generate correct translation result if the input sequence is matched properly with a template. So the template-based translator is often employed as one of the translation engines in machine translation systems. However, the template-based method has its weakness. For example, the extraction of templates is difficult and the coverage of the templates is small. In addition, the templates are generally fixed and the flexibility to match the inputs and to generate the target language is often limited. In our system, we use an improved template-based approach which is robust to some extent for the spoken Chinese language translation as described in [1]. In recent years, statistical machine translation method is becoming more and more popular. In the early 90s, IBM developed the candidate system. Since then, many statistical machine translation systems were proposed [2] [3]. These systems are based on the source channel model and apply a translation model to capture the relationship between the source language and the target language, and use a language model to drive the search process. The primary IBM model was purely word-based model and one improvement is phrase-based statistical translation model. The phrase-based SMT model is proposed to incorporate more complex structure and to get better lexical choice and more reliable local reordering. There are many researchers using the phrase-based translation method to improve their systems performance [4][5][6]. In our system, we apply a phrasebased translation model.  This paper is organized as follows: section 2 describes a hybrid approach to Chinese-to-English translation system, which combines the TBMT system and phrase-based SMT system. Section 3 presents the improvements of base system for the evaluation. In section 4, we present a series of experiments of Chinese-to-English translation and the results are analyzed. Section 5 gives the conclusion. 2. System Description Our system combines two translation engines: the templatebased translation engine and phrase-based statistical translation engine. We use a simple method to select the best translation from the results generated by the two translators. Figure 1 shows the architecture of our system. In this following, we will give an overview of the two translation models and introduce the selection method.  TBMT  N Translated Y  SMT  Pre-processing  Post-processing  Source language sentence  Target language sentence  Figure 1: System architecture  2.1. Template-based translation method  The template-based translation use the templates defined  beforehand to translate source language sentence into target  language sentence. If the input sentence is matched properly  with a template, the system can generate a correct translation.  The template is designed as:  C1C 2 " C n ⇒ T  (1)  where n is an integer ( n ≥ i ≥ 1 ), and Ci is a component source language has to meet. A component may be  expressed as the following five types: 1) Key words: a fixed Chinese word such as “ 房 间  (room)”, “有没有(whether to have)”.  2) Parts-of-speech and semantic features: a part-of-  speech, phrase name, a part-of-speech with semantic  features or a phrase name with semantic features.  3) Variable: it means that any word or phrase may  appear at the position, or nothing appears.  91  4) Logical expression of candidate components: for example, ‘C1 | C2’ means the condition is one of C1 and C2. The operator ‘|’ means logical OR. 5) Dispensable components: the components are written in square brackets and it means whether the components appear at the position or not. T is the translation corresponding to the source sentence and it is similar to the format of left side. When an input sentence of source language meets the conditions expressed by the left side, it will be translated to the target language expressed by the right side T. Example 1: #我 想 预定 QP 个 N => I want to reserve !QP !N+s. QP stands for the number phrase. By using the template, the input Chinese utterance ‘我想预定三个单人间’ will be translated into ‘I want to reserve 3 single rooms.’. Example 2: #QP 个 带|有 N1 的 N2 ＝> !QP !N2+s with !N1. With the template, the input Chinese ‘一个带空调的单人 间 ’ will be translated into ‘One single room with air condition.’, and the input Chinese ‘两个有电话的双人间’ will be translated into ‘Two double rooms with telephone’. In our system, four hundred and sixty five templates are summarized from the training data. For more details of the template-based translation system, please refer to the reference [1]. 2.2. Phrase-based translation model  2.2.1. Phrase-based translation model  In our system, the phrase-based translation model is based on  the log-linear model [7] and the phrase we mention here is  composed of a series of words that perhaps possess no syntax  or semantic meanings. In the log-linear model, given the  sentence f (source language), the translating process is  searching the translation e (target language) with the highest  probability:  M  ∑ e* = arg max λmhm (e, f )  (2)  e  m=1  where hm(e,f) is a feature function and λm is the model  parameter.  2.2.2. Phrase extraction Word alignments are first obtained by using the GIZA++ toolkit in both translation directions and then summarizing the two alignments. We use a number of heuristics, which belongs to the refined method [3], to improve the alignment as the IBM model can not map one target (English) word to the multiple source (Chinese) words. Based on the word alignment, we collect all aligned phrase pairs that are consistent with the word alignment: the words in a legal phrase pair are only aligned within the phrase pair and not to the words outside [3].  2.2.3. Feature functions In the phrase-based system, we use the three feature functions: the target language model (LM), the translation model and the distortion model.  z Target 3-gram LM: standard language model with  Kneser-Ney smoothing method by using the ngram-count tool in SRILM toolkit1. z Translation model: we use four different translation  probabilities to calculate a hybrid translation probability.  PT ( fk | ek ) = λ1Plex ( fk | ek ) + λ2Plex (ek | fk )  (3)  + λ3Pfreq ( fk | ek ) + λ4Pfreq (ek | fk )  Where the Plex ( f k | e k ) is based on the lexical probability  of IBM model 4 and Plex ( e k | f k ) is the inversed lexical  probability. The P freq ( f k | e k ) and P freq ( e k | f k ) are  the probabilities of phrase frequency. λi (i=1 to 4) is the  parameter of probability. z Distortion model: in our system, we use a simple  distortion model:  Pd ( ak − bk −1 ) = λ | a k − bk −1 − 1 |  (4)  Where ak denotes the start position of the source phrase that  was translated into the kth English phrase, and bk −1 denotes  the end position of the source phrase translated into the (k-  1)th English phrase.  2.2.4. Decoding strategy In the phrase-based statistical machine translation system, the decoder employs a beam search algorithm that is similar to the Pharaoh decoder [9]. Considering the different expression habits between Chinese and English, some words must be complemented when translating Chinese sentences into English. For example, some frequent words, such as “a, an, of, the”, are difficult to extract because those words have zero fertility and correspond to NULL in IBM model 4. We call them F-zerowords. When decoding, the F-zerowords can be added after each new hypothesis, which means, a NULL is added after each phrase in the source sentences. At the same time, in Chinese sentence there are many auxiliary words and mood words which correspond to NULL in English. In our decoder, we select the final hypothesis of the best translation in the last several stacks instead of those cover all the source words, because not all the words in source language sentence are necessary to be translated. We will describe the two improvements in detail. z Expanding F-zerowords The decoder starts with an initial hypothesis. There are two kinds of initial hypothesis: one is an empty hypothesis that means no source phrase is translated and no target phrase is generated, and the other one is expanded from the empty hypothesis by adding F-zerowords. New hypotheses are expanded from the current existing hypotheses as follow: if the last target phrase generated in the existing hypothesis is an F-zerowords, an un-translated source phrase and its translation options are selected to expand the hypothesis. If the last target phrase is not F-zerowords, the hypothesis can be expanded as described above or by selecting one of the F-zerowords. An example of hypotheses expansion is illustrated in Figure 2. The expansion with cross is unallowable because the F-zerowords can not be added after F-zerowords.  
We present the NTT translation system that is experimented for the evaluation campaign of “International Workshop on Spoken Language Translation (IWSLT).” The system consists of two primary components: a hierarchical phrase-based statistical machine translation system and a reranking system. The former is conceptualized as a synchronous-CFG in which phrases are hierarchically combined using nonterminals. The latter uses a modiﬁed voted perceptron approach with large number of features. Experiments showed that our hierarchical phrase-based model outperformed a conventional phrase-based model. In addition, our reranking algorithm further boosted the performance. 1. Introduction This paper describes the NTT statistical machine translation system which is experimented in the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2006. Our system consists of two parts. A hierarchical phrasebased translation system that generates a large n-best list. The n-best list is further reranked using a variant of voted perceptron algorithm with additional feature functions. This paper is organized as follows: ﬁrst, we will review the framework of statistical machine translation followed by our hierarchical phrase-based approach. In Section 3, a nbest reranking algorithm will be presented. The reranking algorithm is based on a voted perceptron algorithm with a modiﬁed training procedure. Finally, we will discuss the detail of the task description and condition, followed by experimental results in Section 5. 2. Hierarchical Phrase-based Translation 2.1. Statistical Machine Translation We use a log-linear approach [1] in which a foreign language sentence f1J = f1, f2, ...fJ is translated into another language, i.e. English, eI1 = e1, e2, ..., eI by seeking a maxi-  mum likelihood solution:  eˆI1 = argmax P r(eI1|f1J )  (1)  eI1  = argmax eI1  exp  e′  I′ 1  exp  M m=1  λmhm  (eI1  ,  f1J  )  (2)  M m=1  λm  hm  (e′  I′ 1  ,  f1J  )  In this framework, the posterior probability P r(eI1|f1J ) is directly maximized using a log-linear combination of feature functions hm(eI1, f1J ), such as a ngram language model or a translation model. When decoding, the denominator is dropped since it depends only on f1J . Feature function scaling factors λm are optimized based on a maximum likelihood approach [1] or on a direct error minimization approach [2]. This modeling allows the integration of various feature func- tions depending on the scenario of how a translation is con- stituted.  2.2. Hierarchical Phrase-based Approach In the phrase-based translation approach [3], the input foreign sentence is segmented into phrases, f¯1K , mapped into corresponding English-side e¯K1 , then, reordered to form the output English sentence. The approach is able to capture phrase-wise local-reordering, or possibly neighboring phrase reordering, but does not account for long-distance reordering of phrases. In the hierarchical phrase-based translation approach [4], translation is constituted by hierarchically combining phrases with the help of non-terminals embedded in phrases themselves. Each non-terminal represented in each phrase can capture reordering of phrases. Based on the hierarchical phrase-based modeling, we adopted the left-to-right target generation method described in [5]. The method is able to generate translations efﬁciently, ﬁrst, by simplifying the grammar so that the target-side takes a phrase-preﬁxed form, namely target normalized form. Our simpliﬁed grammar drastically reduces the number of rules extracted from a bilingual corpus empirically presented in [5]. Second, translation is generated in a left-to-right manner, similar to a phrase-based approach, using an Earley-style top-down parsing on the source-side. Coupled with the tar-  95  get normalized form, ngram language models are efﬁciently integrated during the search even with a higher order.  2.3. Simpliﬁed Grammar: Target Normalized Form  In [4], each production rule is restricted to a rank-2 or binarized form in which each rule contains at most two nonterminals. Under this restriction, enormous number of rules are still extracted from a bilingual corpus using the algorithm described in Section 2.4. We introduce a target normalized form in which the target-side of the aligned right-hand side is restricted to a Greibach Normal Form like structure:  X ← γ, ¯bβ, ∼  (3)  where X is a non-terminal, γ is a source-side string of arbitrary terminals and/or non-terminals. ¯bβ is a corresponding target-side where ¯b is a string of terminals, or a phrase, and β is a (possibly empty) string of non-terminals. The use of phrase ¯b as a preﬁx keeps the strength of the phrase-base framework. A contiguous English side coupled with a (possibly) discontiguous foreign language side preserves a phrasebounded local word reordering. At the same time, the targetnormalized framework still combines phrases hierarchically in a restricted manner. For instance, it can capture “ne ... pas” and “not ...” translating from French into English, but cannot directly handle the other direction. The target-normalized form can be regarded as a type of rule in which certain non-terminals are always instantiated with phrase translation pairs. Thus, we will be able to reduce the number of rules induced from a bilingual corpus, which, in turn, help reducing the decoding complexity. Note that we do not imply arbitrary synchronous-CFGs are transformed into the target normalized form. The form simply restricts the grammar extracted from a bilingual corpus explained in Section 2.4.  2.4. Training The phrase extraction algorithm is based on those presented by [3]. First, many-to-many word alignments are induced by running a one-to-many word alignment model, such as GIZA++ [6], in both directions and by combining the results based on a heuristic [7]. Second, phrase translation pairs are extracted from the word aligned corpus [3]. This method exhaustively extracts phrase pairs (fjj+m, eii+n) from a sentence pair (f1J , eI1) that do not violate the word alignment constraints a. In the hierarchical phrase-based model, production rules are accumulated by computing “holes” for extracted contiguous phrases [4]: 1. A phrase pair (f¯, e¯) constitutes a rule: X → f¯, e¯  2. A rule X → γ, α and a phrase pair (f¯, e¯) s.t. γ = γ′f¯γ′′ and α = e¯′e¯β constitutes a rule: X → γ′ X k γ′′, e¯′ X k β where the boxed indices indicate non-terminal alignment. One of the major differences to the algorithm presented in [4] is the restriction of the target normalized form in the last step. 2.5. Decoding by Top-down Parsing Decoding is performed by parsing on the source-side and by combining the projected target-side. A conventional method of parsing is a CKY-based method in which ordering is governed by the span-size of the source words [4]. One of the problem is the high computational complexity when integrated with ngram language model of the target-side especially when the ngram’s order is quite high [8]. The complexity lies on the possible “holes” in the target-side. One of the solution is to perform a binarization so that the target-side will not contain holes [9]. We applied an Earley-style top-down parsing approach described in [5] that is similar to [10]. The basic idea is to perform a top-down parsing in order so that the projected target-side is generated in a left-to-right manner. The search is guided with a push-down automaton which keeps track of the span-size of uncovered source word positions. Combined with the rest-cost estimation aggregated in a bottom-up way, our decoder efﬁciently searches for the most-likely translation. Our decoding algorithm can be regarded as an instance of Earley algorithm, but the predicted rule’s “dot” is moved synchronized with the left-to-right ordering of the projected target-side, not the left-to-right ordering on the source-side. The use of target normalized form further simplify the decodig procedure. Since the rule form does not allow any holes for the target-side, the integration with ngram language model is straightforward: the preﬁxed phrases are simply concatenated and intersected. Our decoder is based on an in-house developed phrasebased decoder which uses a bit vector to represent uncovered foreign word positions for each hypothesis [14]. We basically replaced the bit vector structure to the stack structure: Almost no modiﬁcation was required for the word graph structure and the beam search strategy implemented for a phrase-based modeling, since the target-side’s preﬁxed phrases are simply concatenated. The use of a stack structure directly models a synchronous-CFG formalism realized as a push-down automation, while the bit vector implementation is conceptualized as a ﬁnite state transducer. 2.6. Feature Functions Feature functions evaluated during the decoding procedure is summarized as count-based models, lexicon-based models, language model, reordering models and length-based models.  96  2.6.1. Count-based Models Main feature functions hφ(f1J |eI1, D) and hφ(eI1|f1J , D) estimate the likelihood of two sentences f1J and eI1 over a derivation tree D. We assume that the production rules in D are independent of each other:  hφ(f1J |eI1, D) = log  φ(γ|α)  (4)  γ,α ∈D  φ(γ|α) is estimated through the relative frequency on a given bilingual corpus.  φ(γ|α) =  count(γ, α) γ count(γ, α)  (5)  where count(·) represents the cooccurrence frequency of rules γ and α.  2.6.2. Lexicon-based Models We deﬁne lexically weighted feature functions hw(f1J |eI1, D) and hw(eI1|f1J , D) by applying the independence assumption of production rules as in Equation 4.  hw(f1J |eI1, D) = log  pw(γ|α)  (6)  γ,α ∈D  The lexical weight pw(γ|α) is computed from word alignments a inside γ and α [3]:  pw(γ|α, a)  =  |α|  
We give an overview of the RWTH phrase-based statistical machine translation system that was used in the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2006. The system was ranked ﬁrst with respect to the BLEU measure in all language pairs it was used Using a two-pass aproach, we ﬁrst generate the N best translation candidates. The second pass consists of rescoring and reranking these candidates. We will give a description of the search algorithm as well as of the models used in each pass. We will also describe our method for dealing with punctuation restoration, in order to overcome the difﬁculties of spoken language translation. This work also includes a brief description of the system combination done by the partners participating in the European TC-Star project. 1. Introduction We give an overview of the RWTH phrase-based statistical machine translation system that was used in the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2006. We use a two pass approach. First, we generate a list of the N best translation candidates. Then, we apply additional models in a rescoring/reranking approach. This work is structured as follows: ﬁrst, we will review the statistical approach to machine translation and introduce the notation that we will use in the later sections. Then, we will describe the models and algorithms that are used for generating the N -best list, i.e., the ﬁrst pass. In Section 3, we will describe the models that are used to rescore and rerank this N -best list, i.e., the second pass. Afterwards, we will give an overview of the tasks and discuss the experimental results. This paper will also include a section describing the method used for the system combination of the TC-Star project partners. The overall system is similar to the one used in the 2005 IWSLT evaluation [1]. However, it contains novel features for the ﬁrst pass, as well as for the second pass. In the ﬁrst  pass, we use phrase count features (cf. 2.2) to smooth the phrase probabiliies. In the second pass, we used sentence mixture language models 3.2 as a new model for rescoring. 1.1. Source-channel approach to SMT In statistical machine translation, we are given a source language sentence f1J = f1 . . . fj . . . fJ , which is to be translated into a target language sentence eI1 = e1 . . . ei . . . eI . Among all possible target language sentences, we will choose the sentence with the highest probability:  eˆI1ˆ = argmax P r(eI1|f1J )  (1)  I ,eI1  = argmax P r(eI1) · P r(f1J |eI1)  (2)  I ,eI1  This decomposition into two knowledge sources is known as the source-channel approach to statistical machine translation [2]. It allows for an independent modeling of the target language model P r(eI1) and the translation model P r(f1J |eI1)1. The target language model describes the well-formedness of the target language sentence. The translation model links the source language sentence to the target language sentence. The argmax operation denotes the search problem, i.e., the generation of the output sentence in the target language.  1.2. Log-linear model A generalization of the classical source-channel approach is the direct modeling of the posterior probability P r(eI1|f1J ). Using a log-linear model [3], we obtain:  P r(eI1|f1J ) =  exp exp  M m=1  λmhm(eI1  ,  f1J  )  M m=1  λmhm(e  I 1  ,  f1J )  (3)  e  I 1  1The notational convention will be as follows: we use the symbol P r(·) to denote general probability distributions with (nearly) no speciﬁc assumptions. In contrast, for model-based probability distributions, we use the generic symbol p(·).  103  The denominator represents a normalization factor that depends only on the source sentence f1J . Therefore, we can omit it during the search process. As a decision rule, we ob- tain:  M  eˆI1ˆ = argmax  λmhm(eI1, f1J )  (4)  I ,eI1  m=1  This is a generalization of the source-channel approach. It has the advantage that additional models h(·) can be easily integrated into the overall system. The model scaling factors λM 1 are trained according to the maximum entropy principle, e.g., using the GIS algorithm. Alternatively, one can train them with respect to the ﬁnal translation quality measured by an error criterion [4]. For the IWSLT evaluation campaign, we optimized the scaling factors with respect to the BLEU measure, using the Downhill Simplex algorithm from [5].  1.3. Phrase-based approach The basic idea of phrase-based translation is to segment the given source sentence into phrases, then translate each phrase and ﬁnally compose the target sentence from these phrase translations. This idea is illustrated in Figure 1. Formally, we deﬁne a segmentation of a given sentence pair (f1J , eI1) into K blocks:  k → sk := (ik; bk, jk), for k = 1 . . . K. (5)  Here, ik denotes the last position of the kth target phrase; we set i0 := 0. The pair (bk, jk) denotes the start and end positions of the source phrase that is aligned to the kth target phrase; we set j0 := 0. Phrases are deﬁned as nonempty contiguous sequences of words. We constrain the segmenta- tions so that all words in the source and the target sentence are covered by exactly one phrase. Thus, there are no gaps and there is no overlap. For a given sentence pair (f1J , eI1) and a given segmenta- tion sK1 , we deﬁne the bilingual phrases as:  e˜k := eik−1+1 . . . eik  (6)  f˜k := fbk . . . fjk  (7)  Note that the segmentation sK1 contains the information on the phrase-level reordering. The segmentation sK1 is introduced as a hidden variable in the translation model. There- fore, it would be theoretically correct to sum over all possible segmentations. In practice, we use the maximum approximation for this sum. As a result, the models h(·) depend not only on the sentence pair (f1J , eI1), but also on the segmentation sK1 , i.e., we have models h(f1J , eI1, sK1 ).  1.4. Source cardinality synchronous search For single-word based models, this search strategy is described in [6]. The idea is that the search proceeds synchronously with the cardinality of the already translated  I = i4 i3  target positions  i2  i1  0 = i0  0 = j0 j2 j1  j3  b2 b1 b3  b4  source positions  j4 = J  Figure 1: Illustration of the phrase segmentation.  source positions. Here, we use a phrase-based version of this idea. To make the search problem feasible, the reorderings are constrained as in [7].  2. Models used during search When searching for the best translation for a given input sentence, we use a log-linear combination of several models (also called feature functions) as decision criterion. In this section, we will describe the models that are used in the ﬁrst pass, i.e., during N best list generation. More speciﬁcally the models are: a phrase translation model, a word-based translation model, word and phrase penalty, a target language model and a reordering model. We will now describe the models in detail.  2.1. Phrase-based model  The phrase-based translation model is the main component of our translation system. The hypotheses are generated by concatenating target language phrases. The pairs of source and corresponding target phrases are extracted from the wordaligned bilingual training corpus by the phrase extraction algorithm described in detail in [8]. The main idea is to extract phrase pairs that are consistent with the word alignment, meaning that the words of the source phrase are aligned only to words in the target phrase and vice versa. This criterion is identical to the alignment template criterion described in [9]. We use relative frequencies to estimate the phrase translation probabilities:  p(f˜|e˜)  =  N (f˜, e˜) N (e˜)  (8)  Here, the number of co-occurrences of a phrase pair (f˜, e˜) that are consistent with the word alignment is denoted as N (f˜, e˜). If one occurrence of a target phrase e˜ has N > 1 possible translations, each of them contributes to N (f˜, e˜)  104  with 1/N . The marginal count N (e˜) is the number of occurrences of the target phrase e˜ in the training corpus. The resulting feature function is:  K  hPhr(f1J , eI1, sK1 ) = log p(f˜k|e˜k)  (9)  k=1  To obtain a more symmetric model, we use the phrase-based model in both directions p(f˜|e˜) and p(e˜|f˜).  2.2. Phrase Count Features The reliability of the phrase probability estimation is largely dependent on the amount and quality of the training data. Generally, the probability of rare phrases tends to be overestimated, but as they do not occur often, it might be as well errors originating from mistranslations in the training data or erroneous word alignments. Therefore, we also included features based on the actual count of the bilingual phrase pair.  K hC,τ (f1J , eI1, sK1 ) = [N (f˜k, e˜k) ≤ τ ] k=1 We use [·] to denote a true or false statement [10], i.e., the result is 1 if the statement is true, and 0 otherwise. In general, we use the following convention:  [C] =  1, if condition C is true 0, if condition C is false  (10)  The value τ determines the threshold for the phrase count feature. In the evaluation system, we used three phrase count features with τ manually chosen and ranging from 1.0 to 3.0. As that actual phrase count values are fractional, also fractional thresholds can be used.  2.3. Word-based lexicon model We are using relative frequencies to estimate the phrase translation probabilities. Most of the longer phrases occur only once in the training corpus. Therefore, pure relative frequencies overestimate the probability of those phrases. To overcome this problem, we use a word-based lexicon model to smooth the phrase translation probabilities. The score of a phrase pair is computed similar to the IBM model 1, but here, we are summing only within a phrase pair and not over the whole target language sentence:  K jk hLex(f1J , eI1, sK1 ) = log  ik p(fj |ei )  k=1 j=bk i=ik−1+1  (11)  The word translation probabilities p(f |e) are estimated as relative frequencies from the word-aligned training corpus. The word-based lexicon model is also used in both directions p(f |e) and p(e|f ).  2.4. Word and phrase penalty model In addition, we use two simple heuristics, namely word penalty and phrase penalty:  hWP(f1J , eI1, sK1 ) = I  (12)  hPP(f1J , eI1, sK1 ) = K  (13)  These two models affect the average sentence and phrase lengths. The model scaling factors can be adjusted to prefer longer sentences and longer phrases.  2.5. Target language model  We use the SRI language modeling toolkit [11] to train a stan-  dard n-gram language model. The resulting feature function  is:  I  hLM(f1J , eI1, sK1 ) = log p(ei|eii−−1n+1)  (14)  i=1  The smoothing technique we apply is the modiﬁed KneserNey discounting with interpolation. We used a 6-gram language model for all tasks.  2.6. Reordering model We use a very simple reordering model that is also used in, for instance, [9, 12]. It assigns costs based on the jump width: K hRM(f1J , eI1, sK1 ) = |bk − jk−1 − 1| + J − jK (15) k=1 3. Rescoring models In this section, we describe the second pass of our system, the rescoring of N -best lists. N -best lists are suitable for easily applying several rescoring techniques because the hypotheses are already fully generated. In comparison, word graph rescoring techniques need specialized tools which traverse the graph appropriately. Additionally, because a node within a word graph allows for many histories, one can only apply local rescoring techniques, whereas for N -best lists, techniques can be used that consider properties of the whole target sentence. In the next sections, we will present several rescoring models.  3.1. Clustered language models One of the ﬁrst ideas in rescoring is to use additional language models that were not used in the generation procedure. In our system, we use clustered language models based on regular expressions [13]. Each hypothesis is classiﬁed by matching it to regular expressions that identify the type of the sentence. Then, a cluster-speciﬁc (or sentence-type-speciﬁc) language model is interpolated into a global language model  105  Train: IWSLT’05 DEV’06 EVAL’06  Sentences Running Words Vocabulary Singletons Sentences Running Words Vocabulary OOVs (running words) Sentences Running Words Vocabulary OOVs (running words) Sentences Running Words Vocabulary OOVs (running words)  Chinese 295 579 11 170 4 348 3 208 928 67 (2.1%) 5 214 1 137 126 (2.4%) 5 550 1 328 172 (3.1%)  Japanese 40 000 348 103 12 533 5 572 506 3 601 950 46 (1.3%) 489 5 874 1 189 119 (2.0%) 500 6 489 1 330 170 (2.6%)  English 377 355 9 570 3 904 3 767 843 179 (4.7%) 6 362 1 012 296 (4.7%)  Table 1: Corpus Statistics of the IWSLT 2006 training data and development, test and eval corpora after preprocessing  to compute the score of the sentence:  hCLM(f1J , eI1) =  log  Rc(eI1)  c  (16) αcpc(eI1) + (1 − αc)pg(eI1) ,  where pg(eI1) is the global language model, pc(eI1) the cluster-speciﬁc language model, and Rc(eI1) denotes the true-or-false statement (cf. Equation 10) which is 1 if the cth regular expression Rc(·) matches the target sentence eI1 and 0 otherwise.2 Typical examples for clusters are questions and exclamations, which can usually be detected by punctuation marks and/or speciﬁc words (i.e. “what”, “when”, “how”, ... at the beginning of a question sentence. Furthermore, when looking at the training data, speciﬁc sentences and expressions can be spotted occur quite frequently and can be joined into a cluster.  3.2. Sentence-level Mixtures As an additional language model in rescoring, we use sentence level mixture language models, as presented in [14]. The goal is to represent topic dependencies combining M different language models with a global one, corresponding to the index m = 0 in the following equation (for the case of trigram language models)  M  I  p(eI1) =  λm  pm(ei|ei−1, ei−2) . (17)  m=0  i=1  The training sentences are automatically divided into a ﬁxed number M of clusters (representing different topics) using  2The clusters are disjunct, thus only one regular expression matches.  a maximum likelihood approach and the weights λm are trained on the development data. We used 5-grams for this rescoring model.  3.3. IBM model 1  IBM model 1 rescoring rates the quality of a sentence by  using the probabilities of one of the easiest single-word based  translation models:      hIBM1(f1J  ,  eI1 )  =  log    (I  
This paper describes TALPtuples, the 2006 Ngrambased statistical machine translation system developed at the TALP Research Center of the UPC (Universitat Polite`cnica de Catalunya) in Barcelona. Emphasis is put on improvements and extensions of the system of previous years, being highlighted and empirically compared. Mainly, these include a novel and much more efﬁcient word ordering strategy based on reordering patterns, a linguistically-guided tuple segmentation criterion and improved optimization procedures. The paper provides details of this system participation in the third International Workshop on Spoken Language Translation (IWSLT) held in Kyoto, Japan in November 2006. Results on four translation directions are reported, namely from Arabic, Chinese, Italian and Japanese into English for the open data track, thoroughly explaining all language-related preprocessing and optimization schemes. 1. Introduction Rooted in the Finite-State Transducers approach to SMT [1, 2] and estimating a joint-probability model between the source and the target languages, Ngram-based SMT has proved to be a very competitive alternative to phrase-based and other state-of-the-art systems in previous evaluation campaigns, as shown in [3]. This is specially true when dealing with pairs of languages with a relatively similar word order [4, 5]. Given the language pairs involved in this year’s evaluation, efforts have been focused on improving the word reordering strategies for Ngram-based SMT. Speciﬁcally, a novel reordering strategy based on extending the search graph with automatically-extracted reordering patterns is explored. Results are very promising while keeping computational expenses at a similar level of monotone search. Additionally, a novel tuple segmentation strategy based on the entropy of Part-Of-Speech distributions was used with slight improvements in model estimation. This paper is organized as follows. Section 2 brieﬂy reviews last year’s system, including tuple deﬁnition and extraction, translation model and feature functions, decoding  tool and optimization criterion. Section 3 delves into the word ordering problem, by contrasting the constrained reordered search from previous years with the novel strategy based on reordering patterns. Section 4 focuses on tuple segmentation strategies, and contrasts the criterion on IBM model 1 probabilities from 2005 with a novel criterion based on Part-Of-Speech entropy distributions. Later on, Section 5 reports on all experiments carried out from Arabic, Chinese, Italian and Japanese into English for IWSLT 2006. Finally, Section 6 sums up the main conclusions from the paper and discusses future research lines. 2. 2005 system review The TALP Ngram-based SMT system performs a log-linear combination of a translation model and additional feature functions (see further details in [6, 7]). In contrast to phrasebased models, our translation model is estimated as a standard n-gram model of a bilingual language expressed in tuples. This way it approximates the joint probability between source and target languages capturing bilingual context, as described by the following equation: K p(sJ1 , tI1) = p((s, t)i|(s, t)i−N+1, ..., (s, t)i−1) (1) i=1 where (s, t)i refers to the ith tuple of a sentence pair being segmented into K tuples. A detailed comparison between Ngram-based and phrase-based SMT can be found in [8]. 2.1. Tuple extraction Given a certain word-aligned parallel corpus, tuples are extracted according to the following constraints [9]: • a monotonic segmentation of each bilingual sentence pair is produced • no word in a tuple is aligned to words outside of it • no smaller tuples can be extracted without violating the previous constraints  116  2.2. Feature functions As additional feature functions to better guide the translation process, the system incorporates a target language model, a word bonus model and two lexicon models. The target language model is estimated as a standard ngram over the target words, as follows:  k  pLM (tk) ≈ p(wn|wn−N+1, ..., wn−1)  (2)  n=1  where tk refers to the partial hypothesis and wn to the nth word in it. Usually, this feature is accompanied by a word bonus model based on sentence length, compensating the target language model preference for short sentences (in number of target words). This bonus depends on the number of target words in the partial hypothesis, denoted as:  pW P (tk) = exp(number of words in tk)  (3)  where tk refers to the partial hypothesis. Finally, the third and fourth feature functions corre- spond to source-to-target and target-to-source lexicon models. These models use IBM model 1 translation probabilities to compute a lexical weight for each tuple, accounting for the statistical consistency of the pairs of words inside the tuple. These lexicon models are computed according to the following equation:  pIBM1((s, t)n)  =  (I  
This paper describes the TALP phrase-based statistical machine translation system, enriched with the statistical machine reordering technique. We also report the combination of this system and the TALP-tuple, the n-gram-based statistical machine translation system. We report the results for all the tasks (Chinese, Arabic, Italian and Japanese to English) in the framework of the third evaluation campaign of the International Workshop on Spoken Language Translation.  1. Introduction This paper describes the TALP-phrase system for the IWSLT 2006, which is an enhanced version of the system reported in the 2005 evaluation [1]. The main difference is the integration of a new reordering technique called statistical machine reordering, which was presented in [2] in a different framework. Additionally, we report the results of combining the outputs of the two statistical machine translation TALP systems: phrase-based and n-gram-based. The latter of the two also participated in the 2005 evaluation and is described in [3]. Statistical machine translation systems are now usually modelled through a log-linear maximum entropy framework.  M  e˜ = argmax  λmhm(e, f )  (1)  e m=1  The feature functions, hm, and weights, λi, are typically optimized to maximize the scoring function [4]. Two basic issues differentiate the n-gram-based system from the phrase-based system: the bilingual units are extracted from a monotonic segmentation of the training data; the unit probabilities are based on a standard back-off language model rather than directly on relative frequencies. In both systems, the introduction of reordering capabilities is crucial for certain language pairs. This paper is organized as follows. Section 2 describes the TALP-phrase system, with particular emphasis on a new reordering technique: the statistical machine reordering approach. In Section 3, we combine the TALP-phrase and the  TALP-tuple. Finally, in Section 4, we report the results obtained for all the tasks of the evaluation, which include the translations from Chinese, Arabic, Italian and Japanese to English. 2. Description of the TALP-phrase System 2.1. Phrase-based Model The basic idea of phrase-based translation is to segment the given source sentence into units (here called phrases), then translate each phrase and ﬁnally compose the target sentence from these phrase translations. Given a sentence pair and a corresponding word alignment, phrases are extracted following the criterion in [5]. A phrase (or bilingual phrase) is any pair of m source words and n target words that satisﬁes two basic constraints (1) words are consecutive along both sides of the bilingual phrase and (2) no word on either side of the phrase is aligned to a word outside the phrase. 2.2. Feature functions The baseline phrase-based system implements a log-linear combination of four feature functions, which are described as follows. • The translation model is estimated with relative frequencies. Given the collected phrase pairs, we estimate the phrase translation probability distribution by relative frequency in both directions. • The target language model consists of an n-gram model, in which the probability of a translation hypothesis is approximated by the product of word n-gram probabilities. • The forward and backwards lexicon models. These provide lexicon translation probabilities for each phrase based on the word IBM Model 1 probabilities. For computing the forward lexicon model, IBM Model 1 probabilities from GIZA++ source-to-target alignments are used. In the case of the backwards lexicon model, target-to-source alignments are used.  123  • The word bonus model introduces a sentence length bonus in order to compensate the system preference for short output sentences. • The phrase bonus model introduces a constant bonus per produced phrase. All of these models are combined in the decoder. Additionally, the decoder allows for a non-monotonic search with the following distortion model. • A word distance-based distortion model. K P (tK1 ) = exp(− dk) k=1 where dk is the distance between the ﬁrst word of the kth phrase (unit), and the last word +1 of the (k − 1)th phrase. Distance is measured in words referring to the units source side. To reduce the computational cost we place limits on the search using two parameters: the distortion limit (the maximum distance measured in words that a phrase may be reordered, m) and the reordering limit (the maximum number of reordering jumps in a sentence, j). This feature is independent of the reordering approach presented in this paper, so the two can be used simultaneously. In order to combine the models in the decoder suitably, an optimization tool is needed to compute log-linear weights for each model. 2.3. Statistical Machine Reordering The aim of SMR consists of using an SMT system to deal with reordering problems. SMR is a ﬁrst-pass translation performed on the source corpus, converting it into an intermediate representation, in which source-language words are presented in an order that more closely matches that of the target language (see Figure 1). Therefore, the SMR system can be seen as an SMT system which translates from an original source language (S) to a reordered source language (S’), given a target language (T). In this case, the translation task changes from S2T to S’2T (see Figure 2). The main difference between the two tasks is that the latter allows for (1) monotonized word alignment and (2) higher quality monotonized translation. Figure 1: Monotonization of the source language. For the reordering translation, we used an n-gram-based SMT system (and considered only the translation model,  which is detailed below). Additionally, as for the input to the SMR system, in order to be able to infer new reorderings we use word classes instead of words themselves. Figure 2: SMR is applied before SMT. 2.3.1. Description Figure 3 shows the SMR block diagram. The input is the initial source sentence (S) and the output is the reordered source sentence (S’). There are three blocks in the SMR: (1) class replacing ; (2) the decoder, which requires the translation model; and (3) the block which reorders the original sentence using the indexes given by the decoder. The following example speciﬁes the input and output of each block in the SMR. Figure 3: SMR block diagram. 1. Source sentence (S): se ha bisogno di qualcosa altro 2. Source sentence classes (S-c): 49 137 160 189 176 75 3. Decoder output (translation, T ): 49 # 0 | 137 160 189 176 75 # 3 4 0 1 2 where | indicates the segmentation into bilingual units and # indicates the limit between the source and the target part of each bilingual unit. The source part is composed of word classes and the target part is composed of the new positions of the source word classes, starting at 0. 4. SMR output (S’). The reordering information inside each translation unit of the decoder output (T ) is applied to the original source sentence (S): se di qualcosa altro ha bisogno  124  Figure 4: Block diagram of the training process of the SMR translation model.  2.3.2. Training As explained, for the SMR system, we used an n-gram-based SMT system (and considered only the translation model). Figure 4 shows the block diagram of the training process of the SMR translation model, which is a bilingual n-gram-based model. The training process uses the training source and target corpora and consists of the following steps: 1. Determine source and target word classes. 2. Align parallel training sentences at the word level in both translation directions. Compute the union of the two alignments to obtain a symmetrized many-to-many word alignment. 3. Extract reordering tuples, see Figure 5. Figure 5: Example of the extraction of reordering tuples (step 3). (a) From union word alignment, extract bilingual S2T tuples (i.e. source and target fragments) while maintaining the alignment inside the tuple. As an example of a bilingual S2T tuple consider: ha bisogno di qualcosa altro # anything else you need # 0-2 1-3 2-0 3-0 3-1 4-1, as shown in Figure 5, where the different ﬁelds are separated by # and correspond to: (1) the target fragment; (2) the source fragment; and (3) the word alignment (in  this case, the ﬁelds that respectively correspond to a target and source word are separated by −). (b) Modify the many-to-many word alignment from each tuple to many-to-one. If one source word is aligned to two or more target words, the most probable link given IBM Model 1 is chosen, while the other are omitted (i.e. the number of source words is the same before and after the reordering translation). In the above example, the tuple would be changed to: ha bisogno di qualcosa altro # anything you need # 0-2 1-3 2-0 3-0 4-1, as Pibm1(qualcosa, anything) is higher than Pibm1(qualcosa, else). (c) From bilingual S2T tuples (with many-to-one inside alignment), extract bilingual S2S’ tuples (i.e. the source fragment and its reordering). As in the example: ha bisogno di qualcosa altro # 3 4 0 1 2, where the ﬁrst ﬁeld is the source fragment, and the second is the reordering of these source words. (d) Eliminate tuples whose source fragment consists of the NULL word. (e) Replace the words of each tuple source fragment with the classes determined in Step 1. 4. Compute the bilingual language model of the bilingual S2S’ tuple sequence composed of the source fragment (in classes) and its reorder. Once the translation model is built, the original source corpus S is translated into the reordered source corpus S’ with the SMR system, see Figure 3. The reordered training source corpus and the original training target corpus are used to train the SMT system (as explained earlier in this same section). Finally, with this system, the reordered test source corpus is translated. 3. Phrase-based and N -gram-based Combination The aim of the system combination is to select the better translation given the 1-best output of each system: phrasebased and n-gram-based.  125  We perform a log-linear combination, which is computed using the following models: • IBM Model 1 for the sentence in the source to target direction. • IBM Model 1 for the sentence in the target to source direction. • Target language models: 2gram, 3gram and 5gram. • Word bonus. The weights of each model are optimized with the simplex algorithm [6]. 4. Evaluation Framework 4.1. Tools • Word alignments were computed using the GIZA++ tool [7]. During word alignment, we used 50 classes per language. We aligned both translation directions and combined the two alignments with the union operation. • Word classes (which were used to help the aligner and to perform the SMR process) were determined using “mkcls”, a tool freely-available with GIZA++. • The language model was estimated using the SRILM toolkit [8]. • The decoder was MARIE [9]. • The optimization tool used for computing log-linear weights was based on the simplex method [6]. Following the consensus strategy proposed in [10], the objective function was set to 100 · BLEU + 4 · N IST . 4.2. Data Experiments were carried out for all tasks of the IWSLT06 evaluation (Zh2En, Jp2En, Ar2En and It2En) using the BTEC Corpus provided for the open data track1. 4.3. Description of tasks For internal development work, true case and punctuation marks were removed from all parallel corpora (train, develop, test and references), thereby optimizing according to the ’additional’ scoring scheme as deﬁned in IWSLT 2006. For the ﬁnal evaluation test set, punctuation marks and true case were included by using the SRILM ’disambig’ tool as suggested by IWSLT organizers. Given the availability of up to four development sets for all language pairs, our strategy was to use development 4 as the internal development set (dev4), while randomly selecting 500 sentences from developments 1, 2 and 3 (around 160 sentences from each) to build an internal test set (dev123). 1www.slt.atr.jp/IWSLT2006  Finally, the approximately 1k remaining development sentences were included in the training corpus by selecting the ﬁrst English manual reference.  sent. wrds voc. slen. refs.  train  ar en  24.0k  183k 166k  10.5k 7.3k  7.6 6.9  
This paper describes the UKA/CMU statistical machine translation system used in the IWSLT 2006 evaluation campaign. The system is based on phrase-to-phrase translations extracted from a bilingual corpus. We compare two different phrase alignment techniques both based on word alignment probabilities. The system was used for all language pairs and data conditions in the evaluation campaign translating both the ASR output (as 1best) and the correct recognition results. 1. Introduction The UKA/CMU statistical machine translation system that was used for the IWSLT 2006 evaluation campaign is based on phrase to phrase translations. A phrase-to-phrase translation system uses phrases as the general building blocks of the ﬁnal translation. This generally leads to better translation performance than purely word based translation systems. The main reason is that the phrases are able to preserve local context information thus leading to better lexical choice. In addition a phrase translation can already automatically perform a local re-ordering within the translated phrase. Section 3 presents a detailed look at the UKA/CMU translation system and gives an overview over phrase alignment, language models and decoder architecture. Section 4 presents the results of the experiments done during and after IWSLT 2006. We compare our ofﬁcial submission results with contrastive runs using different settings and conditions. The IWSLT 2006 evaluation campaign allowed the use of freely available data for the Open data track submissions and also the additional use of proprietary data for the C-STAR data track submissions. The overview will also list additional data that was added for each condition. The UKA/CMU statistical translation system was used for every language pair for both data conditions. We did not use any speciﬁc techniques for ASR output translation so just the 1-best ASR output was translated and compared to the translation of the correct recognition result.  2. InterACT Partner cooperation The work presented here, is the collaborative effort of researchers at our UKA and CMU laboratories, who combined research systems to test and evaluate approaches on multiple languages. In this manner, we have been able to evaluate on more conditions than any other site and to compare across languages. As shall be seen, however, performance still appears to depend more on the maturity of the effort in any given language than on inherent difﬁculties of that language. The UKA contributions have focused on applying a Chinese → English translation system that is being developed for the TC-STAR project. Also a ﬁrst trial with Italian was carried out to further integration and collaboration with other TCSTAR systems. CMU focused on Arabic, a language that is studied under projects GALE and Transtac and also applied the system to Japanese. The translation system that was used is basically the same in both places. It has to be seen as a combined effort of a close-knit research group [1],[2]. 3. Translation System 3.1. Phrase Alignment 3.1.1. Overview We used two phrase alignment methods for IWSLT 2006, namely PESA and LogLin. The LogLin phrase extraction method is computationally intensive so it was not used for all submitted systems. More extensive experiments with contrastive systems showed that LogLin generally improves the results compared to the PESA phrase extraction method. 3.1.2. PESA: Phrase pair extraction as sentence splitting [3] The PESA phrase extraction method is based on the well known IBM-1 word alignment model [4]. The IBM-1 model assigns a probability to all possible word alignments of respective sentences in the training data. Assuming a sentence in the bilingual corpus contains a phrase from a source sentence eii21 = ei1 ...ei2 we are inter-  130  ested in the sequence of words fjj12 = fj1 ...fj2 from the respective target sentence that is the optimal translation for this source phrase. We can now estimate the quality of a translation candidate by using the IBM-1 word alignment probabilities between the source and target phrases. If the candidate is actually a good translation of the source phrase we expect higher IBM-1 probabilities between the words in the phrases than if the translation candidate was incorrect. If we assume that fjj12 is the optimal translation for eii21 in this sentence pair we can analogously argue that the words from the sentence pair that are not in these phrases must also be translations of each other. This means the optimal translation for the (noncontiguous) source phrase e1...ei1−1 ei2+1...eI is f1...fj1−1 fj2+1...fJ and we also expect high probabilities between the words in these two phrases. Overall the constrained probability for this sentence split can be calculated as:  i1 −1  pj1,j2 (e|f ) =  p(ei|fj) ∗  i=1 j∈/(j1...j2)  i2 j2  I  p(ei|fj )  p(ei|fj )  i=i1 j=j1  i=i2 +1 j∈/(j1...j2)  If we optimize over the target side boundaries j1 and j2 we can determine the optimal sentence splitting and the best translation candidate. The same ideas can be applied if we use the IBM-1 probabilities for the reverse direction thus calculating pj1,j2 (f |e) and we interpolate the two phrase alignment probabilities to get the optimal translation candidate. In the actual system we not only use the top translation candidate but all candidates to a certain threshold. This covers translation alternatives and leaves the ﬁnal decision to other models, mainly the language model.  3.1.3. LogLin: Phrase pair extraction with Log-Linear Features [5],[6] Another phrase extraction method applied was LogLin. LogLin formulates the phrase-extraction problem as a local search, guided by a simple heuristic function. Given the source n-gram eii+k (span from position i to position i+k, with a length of k+1), the local search starts by ﬁrst localizing the projected center of the target phrase, and then it searches the best scored width, corresponding to the left- and right- boundaries (j, j+l) of the target phrase: fjj+l . Previously the heuristic function included phrase-level fertility score, a simple IBM-1 lexicon score, and a phraselevel position distortion score. In this evaluation, we used an extended heuristic function: a log-linear model, in which thirteen feature functions  were computed to predict the qualities of a phrase alignment [6]. The log-linear model allows the utilization of overlapping feature functions. The thirteen feature functions used are summarized as follows. There are four feature functions of phrase-level fertility score: P (l + 1|eii+k) and P (J − l − 1|ei′∈/[i,i+k]), which are the probabilities to generate length of l+1 using the source candidate phrase eii+k and the remaining length of (J-l-1) using the remaining words in the source sentence: ei′∈/[i,i+k] . These probabilities are computed using the word fertility P (φ|ei) via dynamic programming. Similarly, the probabilities are computed in the other direction as P (k + 1|fjj+l) and P (I − k − 1|fj′∈/[j,j+l]) . Four feature functions compute the IBM Model-1 scores for each candidate phrase-pair: P (fjj+l|eii+k) and P (eii+k|fjj+l) . The remaining parts of the sentence pair (e, f ) excluding the candidate phrase-pair is also modeled with P (fj′∈/[j,j+l]|ei′∈/[i,i+k]) and P (ei′∈/[i,i+k]|fj′∈/[j,j+l]) . Another four scores are aimed at bracketing the sentencepair along the diagonal and the inverse-diagonal using the IBM Model-1 lexicon probabilities of p(e | f) and p(f | e). The ﬁnal feature function is the average number of word alignment links per source word in the candidate phrase-pair. We assume that each aligned phrase-pair should contain at least one word alignment link (for details concerning the features please see [6]). To learn the weights for each of the feature function, gold-blocks from human word alignments were extracted, and a log-linear model was trained to optimize the accuracy at the phrase-level for each sentence pair. Details about the training can also be found in [6]. In this evaluation, we simply copied the learned weights from our previous experiments during the NIST evaluation. 3.2. Language Model The UKA/CMU-SMT system supports standard n-gram language models. Here we applied a 6-gram language model using a sufﬁx array implementation . The sufﬁx array language model provides the possibility for histories of arbitrary length but we only used a history of 5 (effectively a 6-gram language model). This language model uses Good-Turing smoothing (see [7]). 3.3. Decoding [8] After the preparation and training of translation and language models is complete all models are used in a decoder to translate the actual source sentences. The UKA/CMU-SMT decoder uses a 2 stage process that ﬁrst builds a translation lattice and then searches for the best path through the lattice. The translation lattice is built by using all available translation pairs from the translation models for the given source sentence and inserting them into a lattice. These translation pairs consist of words or phrases on the source side that cover  131  a part of the source sentence. The decoder inserts an additional edge for each phrase pair and attaches the target side of the translation pair and translations scores to the edge. The translation lattice will now contain a large number of possible paths that cover each source word exactly once (a combination of partial translation of words or phrases). These translation hypotheses will greatly vary in quality and the decoder uses the different knowledge sources and scores to ﬁnd the best path possible translation hypothesis. This step also allows for limited reordering within the found translation hypotheses. The features, which are used to score each phrase-pair for decoding process, are different from the features used for phrase-extraction in section 3.1.3, which are aimed to bracket the sentence-pairs given a phrase-pair block and these features are speciﬁc/relative to each sentence-pair. In our decoding experiments for IWSLT06, we used the following scores1: relative frequencies in both directions, phrase-level fertility scores in both directions (via DP as in section 3.1.3), The IBM-1 Viterbi scores in both directions, un-normalized IBM-1 lexicon scores in both directions P (fjj+l|eii+k) = j′∈[j,j+l] i′∈[i,i+k] P (fj′ |ei′ ) (favoring long phrase-pairs, see [2]), the phrase-level normalized frequency, and the normalized number of alignment links within the phrase-pair (as in section 3.1.3). 4. Translation Results This section gives an overview of all ofﬁcial and contrastive results achieved by the UKA/CMU translation systems. All results will only be given in BLEU([9]) and NIST([10]) scores according to the ofﬁcial evaluation speciﬁcations (mixed case with punctuation marks). For other scores for the submitted systems please refer to the ofﬁcial scoring publication of IWSLT 2006. Submissions were done for all language pairs in the Open and C-STAR data conditions. We always translated ASR output (as 1-best) and as a comparison the correct recognition results (CRR). The development set numbers always refer to the development set that was also provided for the evaluation campaign for IWSLT 2006. 4.1. Data Conditions For each language pair 20,000 or 40,000 sentences from the BTEC corpus [11] were provided to the participants (Supplied data). For the Open data track it was possible to use any freely available data in addition to this. As a C-STAR member UKA/CMU also has access to the whole BTEC training corpus for each language (different sizes per language). The actual C-STAR data track allowed the use of this data and also the use of additional free and proprietary data (Full BTEC + any data). 1which are not necessarily probabilities  Overall we identiﬁed four different data situations that could be investigated.  Data situation Supplied Supplied + free data Extended BTEC Full BTEC + any data  Explanation Supplied data only Supplied data and freely available data (Open data track in Evaluation campaign) Full BTEC corpus Full BTEC corpus and any other data (C-STAR data track in Evaluation campaign)  Table 1: Data sets and conditions  4.2. Input Conditions 
We present the CMU-UKA Syntax Augmented Machine Translation System that was used in the IWSLT-06 evaluation campaign. We participated in the C-Star data track using only the Full BTEC corpus, for Chinese-English translation, focusing on transcript translation. We applied techniques that produce true-cased, punctuated translations from non-punctuated Chinese transcripts, generating translations which score higher against the Ofﬁcial metric than against the lower-cased, punctuation removed metric. Our results demonstrate the impact of syntax and hierarchy based models for speech transcript translation. 1. Introduction As [1] and [2] note, phrase-based models suffer from sparse data effects when required to translate conceptual elements that span or skip across several words, and distortion based re-ordering techniques tend to limit their range of operation for reasons of efﬁciency and model strength [3]. Syntax driven [4], [5] and hierarchical translation models [1] model structured re-ordering constraints and extend the domain of locality in the decoding process. Systems such as [6] and [7] demonstrate effective decoding using these models. For the IWSLT-06 evaluation, we applied syntax augmented translation as per [7] to the speech translation task, using the only Full BTEC corpus to model translational equivalence. We begin by extracting lexical phrases as per [2] as the basis for our syntax augmented translation rules. We annotate and generalize these phrases by parsing the target side of the training data with the Stanford Parser [8] (trained on the Penn Treebank). Our results indicate steady improvements as we introduce hierarchy and syntax into the translation process despite the domain mismatch with the English parser. As deﬁned in the evaluation’s Ofﬁcial Speciﬁcations, translations are evaluated considering case and punctuation markers, artifacts not typically present in Chinese ASR transcripts. We incorporate the generation of punctuation directly into the translation process, learning translation rules that represent case and punctuation decisions. Our Ofﬁcial evaluation results demonstrate the effectiveness of these approaches; our submission achieved higher performance when the system output was evaluated with punctuation and case  than in the lower-cased, punctuation-free evaluation. This result is especially relevant considering the presence of multiple sentences (which should be punctuation-separated in English) within each speech transcript utterance. We begin by summarizing the syntax augmented rule extraction process from [7] and the decoding settings used to train the model parameters to maximize performance [9] on the BLEU metric [10]. We provide a detailed description of the data-processing used to generate our evaluation submission, and demonstrate the impact of syntax for the IWSLT-06 speech translation task. 2. Syntax Augmented Translation Traditional phrase-based translations serves as the lexical foundation for the syntactic synchronous grammar (SynCFG) presented in [7]. Syntactic, since its nonterminals are syntactic categories derived from parsing the target (English) side of the parallel training corpus, and synchronous because they deﬁne operations to derive the source and target language simultaneously. We train a phrase based translation model as in [2] on the bilingual training data, and associate the word alignment graph for each sentence pair with the highest probability parse of the target sentence. We obtain parse trees from the Stanford Parser pre-trained on the Penn Treebank [8]. Using the notation from [1], we aim to construct a synchronous grammar of the form X → λ, α, ∼ where λ = f1 . . . Yi . . . fm is a sequence of nonterminals Yi and source language terminals fk and α = e1 . . . Yj . . . . . . en is a sequence containing the same nonterminals Yi as well as target language terminals ek. The relation ∼ deﬁnes a one-to-one correspondence between nonterminal occurrences Y across λ and α. Target terminals are the English words of the system output, and our nonterminals are target language syntactic categories corresponding to the Penn Treebank nonterminal set along with extensions. Under this notation, phrase table entries represent purely lexical λ and α. To produce rules of the form described above, we annotate the initial lexical rules (the phrase-table) with syntactic productions (left-hand side of the rule) based on the parse  138  trees available for the target side of the corpus. We perform the phrase extraction from [2] for each sentence individually, identifying phrases based on the learned word alignment, and associating the target side of these phrases with constituent labels from the corresponding target parse tree. Details are provided below. 2.1. Annotation We run the freely available phrase extraction system provided by Phillip Koehn for the NAACL-2006 Workshop on Statistical Machine Translation, generating initial word alignments using the grow-diag-ﬁnal method. In order to identify phrases with their corresponding sentences (required to associate the target side parse tree), we insert sentence separator marks between each line of the source, target and alignment ﬁle. Calling phrase-extract (a binary provided in the workshop tools) with these marked ﬁles produces a phrase table with separators between each set of phrases extracted from a sentence. For the set of phrases extracted for each sentence, we consider the corresponding parse tree for the whole target sentence. For each source phrase-pair f1 . . . fm/e1 . . . en, we annotate this phrase-pair with the constituent that spans e1 . . . en in the target side parse tree. As an extension to the nonterminal set provided by the Penn Treebank, we consider the CCG nonterminal set proposed by [11]. Under this approach, rules can be assigned partially formed categories, like DT \N P , indicating a constituent that forms a noun-phrase, but is missing its determiner at the left. [12] demonstrates the importance of considering phrases not corresponding to pure syntactic constituents in translation, and in [13], we demonstrate the value of using extended categories in our translation system. If neither a Penn Treebank or CCG constituent can be found for e1 . . . en, we associate a generic nonterminal symbol _X with this rule, allowing it to still take part in hierarchically motivated synchronous derivations. 2.2. Generalization We generalize the annotated phrases described above to include nonterminals on the target side, which can be instantiated during decoding. We adhere to [1] to recursively generalize each existing rule N → f1 . . . fm/e1 . . . en for which there is an initial rule M → fi . . . fu/ej . . . ev where 1 ≤ i < u ≤ m and 1 ≤ j < v ≤ n, to obtain a new rule N → f1 . . . fi−1Mkfu+1 . . . fm/e1 . . . ej−1Mkev+1 . . . en where k is a new index for the nonterminal M that expresses the one-to-one correspondence between the new occurrence  of M on the source side and the corresponding one on the target side. This approach generates rules comparable to the “Model2C” phrases in [6] assuming consistent initial phrase pairs from [2]. The target side of each rule is ﬂattened before relative frequencies are calculated. 2.3. Rule Preparation The set of rules resulting from the process above is more than 10 times the size of the initial lexical rules / phrase pairs used to create them. As an initial pruning step, we remove rules whose source-conditioned relative frequency is below a speciﬁed threshold, as well as those that include source terminal elements that do not occur in the development or test data. In practice we perform this ﬁltering online during extraction. Also, generalized rules that occur only once are immediately discarded, but lexical rules (those that contain no nonterminal symbols) are never pruned away. To allow derivations that do not comply with the syntactic structure learned from the rule extraction phase, we use “glue” rules as in [1]. The glue rules allow any nonterminal production to participate in sequential combination with other rules, in a left-bracketing manner. Unknown words are assigned production categories based on common occurrence. Speciﬁcally, we add the following rules to the learned grammar. • S → N, N for all nonterminals N in the grammar • G → N, N for all nonterminals N in the grammar • S → S G, S G • {NNP, JJ, RB, NN} → UNK, UNK Glue rule counts as used in the log-linear model (Section 3.2) are incremented for the S → N, N and S → S G, S G rules, thereby counting the number of chunks used in sequential productions. 2.4. Extracted Rules After ﬁltering for development and test set, we are left with 290,325 rules in the syntactic rule set, and 38,576 rules in the hierarchical rule extraction variant. The number of actual occurring nonterminals in our ﬁltered syntactic grammar (before language model intersection) is 1618, due to the CCG extension on top of the 75 original Penn Treebank nonterminals. Table 1 gives statistics and examples of the types of rules occurring in our syntactic rule set. 3. Decoding 3.1. Parsing Strategy We apply the SynCFG to an input source sentence by using a bottom-up, CYK+ [14] style decoder which does not require  139  Rule type Total Abstract (i.e., no terminals) Lexical / initial (i.e., only terminals) Mixed (both terminals and NTs) Regular syntactic (containing only regular NTs) Extended synt. (containing only regular or CCG NTs) X rules (containing the X nonterminal) With reordering no substitution site (i.e., lexical) 
This paper describes the University of Washington’s submission to the IWSLT 2006 evaluation campaign. We present a multi-pass statistical phrase-based machine translation system for the Italian-English open-data track. The focus of our work was on the use of heterogeneous data sources for training translation and language models, the use of several novel rescoring features in the second pass, and exploiting N-best information for translation in the ASR-output condition. Results show mixed beneﬁts of adding out-of-domain data and using N-best information and demonstrate improvements for some of the novel rescoring features. 1. Introduction We present a two-pass statistical phrase-based machine translation system developed for the IWSLT 2006 evaluation. For this task we concentrated on a single language pair, ItalianEnglish, and on the correct transcription and ASR-output conditions. We used the ASR output provided and did not produce our own ASR hypotheses from the raw speech data. Since the BTEC task is a sparse-data task, our focus for this evaluation was on exploring the use of heterogeneous data sources for training. In addition, we investigated several novel features for rescoring and the use of N-best information for the ASR-output condition. This paper is structured as follows: we ﬁrst describe the data sources and preprocessing used. Sections 4 and 5 describe ﬁrst-pass hypothesis generation and second-pass rescoring. Postprocessing and spokenlanguage speciﬁc processing are presented in Sections 6 and 7. We then present experiments and the ofﬁcial evaluation results. Section 10 describes additional analyses performed after the ofﬁcial evaluation and Section 11 concludes. 2. Data The UW system participated in the open data track. For training we used the BTEC Italian-English training data provided for this evaluation campaign, along with the devset1, devset2, and devset3, resulting in approximately 190K words of in-domain training data (including punctuation). In addition, we used the publicly available Europarl corpus of Italian/English [1] for training the translation model. This cor-  pus is very different from BTEC in that it contains edited transcriptions of parliamentary proceedings; thus, the domain differs from that of a travel task, and the style is that of written text. The size of the Europarl corpus is approximately 17M words. We also used the Fisher corpus for training certain second-pass language models. The Fisher corpus is a collection of English conversational telephone speech covering a variety of speakers and topics. It consists of approximately 2.3M word tokens. All development/evaluation was done on devset4, since it was expected to be most similar to the test data. This set was randomly split into a development set of 350 sentences and a held-out set of 139 sentences. 3. Preprocessing The BTEC data was preprocessed by ﬁrst segmenting lines with multiple sentences into single sentences according to the punctuation. Punctuation was then removed and all words were lowercased. The Europarl data was also lowercased and sentence pairs with a length ratio greater than 9 were removed. For the evaluation system, no additional preprocessing was performed. After the ofﬁcial evaluation we attempted to improve the use of the Europarl data by modifying the English side to render it more similar to spoken language style and modifying the Italian side to more closely match the transcription conventions used in the BTEC corpus. All English sentence punctuation was removed, common English contractions were added to 90% of the corpus, and abbreviations or titles were punctuated. For example, the sentence thank you , mr segni , i shall do so gladly . became thank you mr. segni i’ll do so gladly. In the Italian text, sentence punctuation was also removed, apostrophes denoting contractions were joined to the preceding part of the word, and common abbreviations were expanded. For example, the sentence ... condannare l ’ arresto della sig.ra gladys marin ed esigerne l ’ immediata scarcerazione ? became ... condannare l’ arresto della signora gladys marin ed esigerne l’ immediata scarcerazione. However, these modiﬁcation did not affect translation performance signiﬁcantly.  145  4. First-Pass Translation System We use a multi-pass statistical phrase-based translation system based on a log-linear probability model:  K e∗ = argmaxep(e|f ) = argmaxe{ λkφk(e, f )} (1) k=1 where e is and English and f a foreign sentence, φ(e, f ) is a feature function deﬁned on both sentences, and λ is a feature weight. The ﬁrst pass generates up to 2000 translation hypotheses per sentence using the public-domain Pharaoh decoder [2] and a combination of the following nine model scores: • two phrase-based translation scores • two lexical translation scores • data source indicator feature • word transition penalty • phrase penalty • distortion penalty • language model score The ﬁrst two of these are explained below (Section 4.1). The word transition and phrase penalty are constant weights added for each word/phrase used in the translation, thus controlling the length of the translation. The distortion penalty assigns a weight proportional to the distance by which phrases are reordered during decoding; here, the distortion penalty is constant since monotone decoding is used and no reordering is allowed. (Initial experiments show that monotone decoding outperforms non-monotone decoding.) Weights for these scores are optimized using the minimumerror rate training procedure in [3]. The optimization criterion is the BLEU score on the development set as deﬁned above (Section 2). The second pass rescores the ﬁrst-pass output with additional, more advanced model scores. A postprocessing step is then performed to restore true case and punctuation.  4.1. Translation Model The translation model is deﬁned over a segmentation of source and target sentence into phrases: f = f¯1, f¯2, ..., f¯M and e = e¯1, e¯2, ..., e¯M . Phrase pairs of up to length 7 are extracted from the training corpus which was previously wordaligned using GIZA++. The extraction method is the technique described in [4] and implemented in [2]: the corpus is ﬁrst aligned in both translation directions, the intersection of the alignment points is taken, and additional alignment points are added heuristically. For each phrase pair, two phrasal translation probabilities, P (f¯|e¯) and P (e¯|f¯), are computed (one for each direction) from the relative frequency estimate on the training data, e.g.:  P (e¯|f¯)  =  count(e¯, f¯) count(f¯)  (2)  Two analogous lexical scores are computed, e.g.:  Scorelex(f¯|e¯) =  J  
In this paper, an overview of the XMU phrase-based statistical machine translation system for the 2006 IWSLT Speech Translation Evaluation was given. In this year’s evaluation, we participated in the open data track for ASR lattice and Cleaned Transcripts for the Chinese-English translation direction. The system ranked 7th among the 12 participating systems in the Chinese-English spontaneous speech ASR output task, 11th among the 15 participating systems in the Chinese-English read speech ASR output task and 8th among the 15 participating systems in the Cleaned Transcripts task. 1. Introduction This paper describes the system which participated in the 2006 IWSLT Speech Translation Evaluation of Institute of Artificial Intelligence, Xiamen University. The system is a phrase-based Statistical Machine Translation (SMT) system. This paper is organized as follows. Section 2 describes data preparing. Section 3 gives an overview of the translation model. In section 4, experiments and the results are reported. And finally, section 5 concludes. 2. Preparing the Data This section describes how we prepare the training data for our SMT system. Four steps are described in detail, that is., data preprocessing, word alignment, phrase extraction and phrase probabilities estimation.  • Truecasing of the first word of an English sentence: To transform the uppercase version of the beginning words of English sentences into their lowercase version if their lowercase version occur more often. 2.2. Word Alignment To achieve n-to-n word alignment, we first run GIZA++ up to IBM model 4 in both translation directions to get an initial word alignment, and then apply “grow-diag-final” method [1] to refine it. This process could be addressed in detail as followed: • In the initial step, we intersect the two alignments obtained by running GIZA++, i.e., Chinese to English and English to Chinese, and get a high-precision alignment. • Then the intersection alignment grows iteratively by adding potential alignments, which exist in the union of the two alignments. The neighbors of the intersection points in alignment matrix, including left, right, up, bottom and the diagonally directions are checked, if either of the words linked by the potential alignment is not aligned previously, the potential alignment is added. This operator is done until no more neighbors can be added. • In the final step, potential alignments, which exist in the union of two alignments, will be added if all their neighbors do not exist in the union alignment.  2.1. Preprocessing Data preprocessing is not a trivial task for machine translation system. Our experiments showed that good data preprocessing model can result in better translation quality. Two types of preprocessing were performed on the Chinese part of the training data: • Segmentation: To transform Chinese characters into Chinese words. • SBC case to DBC case: To replace numbers, English characters or punctuations in SBC case in Chinese by their DBC case. For instance, "１", "Ａ" and "。" would respectively be replaced by "1", "A" and ".". For the English part of the training data, also two types of preprocessing were performed: • Tokenization: To separate punctuations from words in English sentences.  2.3. Phrase Extraction Bilingual phrases can be learned from word aligned parallel corpus. As is common in most phrase-based SMT systems, we consider bilingual phrase as a pair of source and target words sequences, with the following constrains: • The words should be consecutive in both source and target sentences. • The word level alignment of bilingual phrase should be consistent with the alignment matrix. The consistency means that the words of the bilingual phrase can only be aligned to each other, and not to any other words outside. Our phrase extraction method is very similar to [2]. For a word aligned sentence pair, we enumerate all the consecutive words sequences of English sentence, and for each English phrase, find the corresponding Chinese words according to the alignment matrix, if it satisfies the two constraints above, a bilingual phrase is extracted. In addition, in order to extract  153  more phrases, such a bilingual phrase can be extended at Chinese side since “NULL” alignment is allowed, which means a word aligned to nothing. For the same English phrase, we extend the corresponding Chinese phrase to both left and right, if the added Chinese word is not aligned, and the new phrase satisfies our definition, it is extracted as a bilingual phrase. This is done iteratively until no more words can be added. However, we limited the length of phrases from 1 word to 6 words in our experiment, since it has been showed that longer phrases don’t yield better translation quality [1]. And, to avoid a too large search space in decoding, we also limited the size of the translation table. For a Chinese phrase, only 20-best corresponding bilingual phrases were kept. We used Formula 1 to evaluate and rank the bilingual phrases with the same Chinese phrase.  N  ∑ λi ⋅ hi (e,c)  (1)  i =1  Where, hi (e,c) (1≤ i ≤N) denotes a phrase probability of a given bilingual phrase (e,c) , and λi (1≤ i ≤N) is the corresponding parameter for hi (e,c) . In our system, N is set to be 4, in that there are four phrase probabilities for a given bilingual phrase (see 2.4 for details). Note that, the parameters here should use the same values as their corresponding ones in the translation model (see 3.2 for details). By using the pruned phrase table, our system could translate the test set from this evaluation at the speed of about 0.2 seconds per sentence.  2.4. Phrase Probabilities Four phrase probabilities are defined for a given bilingual phrase in our system: • Phrase translation probability p(e | c) • Inversed phrase translation probability p(c | e) • Phrase lexical weigh lex(e | c) • Inversed phrase lexical weight lex(c | e)  We define the phrase translation probability using relative frequency as in Formula 2:  p(e  |  c)  =  N (e,c) ∑ N (e′,c)  (2)  e′  Where, N (e,c) is the total number of bilingual phrase (e,c) occurred in the training corpus. Additional to p(e | c) , we introduce a lexical weight metric that denotes how well the words of phrase c translate to the words of phrase e . Following the description in [1], given a bilingual phrase (e1I ,c1J ) and its alignment a , the lexical weight is defined as Formula 3:  ∏ ∑ lex(e1I  | c1J , a) =  I i =1  |{  j  |  
We describe a bilingual grammar used for translation of verb frame divergences between Swedish and English. The grammar is used both for analysis and generation with Minimal Recursion Semantics as interlingua. Our grammar is based on the delph-in resources for which semantic transfer is proposed for MT. We show that an interlingua strategy based on a bilingual grammar can handle many cases of verb frame divergences minimising the need of transfer.  
1. Introduction Depending on the purpose of an MT evaluation, getting a ‘first impression’ of the quality of the output may be enough. Fast, on-the-fly evaluations can provide concrete proof of the quality of a system’s translation and save both time and money. As a result of the actual application of MT and MT evaluation methods in a large organisation, the Open University of Catalonia (UOC), we can present in this paper a method that allows for the performing of fast, large-scale on-the-fly evaluations without using Human Translation References (HTR) or large corpora of machine translated and human translated texts. 
One of the bottlenecks of example-based machine translation (EBMT) is to be able to amass automatically quantities of good examples. In our work in EBMT, we are investigating how far one can go by performing example extraction from parallel corpora using Probabilistic Translation Dictionaries to obtain example segmentation points. In fact, the success of EBMT highly depends on examples quality and quantity, but also in their length. Thus, we give special importance on methods to extract diﬀerent size examples from the same translation unit. With this article we show that it is possible to extract quantities for examples from parallel corpora just using probabilistic translation dictionaries extracted from the same corpora.  
The Computer-Assisted Translation (CAT) paradigm tries to integrate human expertise into the automatic translation process. In this paradigm, a human translator interacts with a translation system that dynamically oﬀers a list of translations that best completes the part of the sentence that is being translated. This human-machine sinergy aims at a double goal, to increase translator productivity and ease translators’ work. In this paper, we present a CAT system based on stochastic ﬁnite-state transducer technology. This system has been developed and assessed on two real parallel corpora in the framework of the European project TransType2 (TT2).  
PO Box 88 Manchester M60 1QD, UK  Ana Niño School of Education University of Manchester Oxford Road Manchester M13 9PL, UK  Harold.Somers@manchester.ac.uk,  {F.Gaspari,A.Nino}@postgrad.manchester.ac.uk  Abstract. The ready availability of free online machine translation (MT) systems has given rise to a problem in the world of language teaching in that students – especially weaker ones – use free online MT to do their translation homework. Apart from the pedagogic implications, one question of interest is whether we can devise any techniques for automatically detecting such use. This paper reports an experiment which aims to address this particular problem, using methods from the broader world of computational stylometry, plagiarism detection, text reuse, and MT evaluation. A pilot experiment comparing ‘honest’ and ‘derived’ translations produced by 25 intermediate learners of Spanish, Italian and German is reported.  1. Introduction One of the most important developments in the history of Machine Translation (MT) has been the availability, since about 1994, of free MT online: while initially perhaps a marketing ploy, this has had a profound effect on the perceptions of the general public, as well as shaping the development of the technology. It was CompuServe who first entered into an agreement with Systran to make MT available free online (Flanagan, 1996), though AltaVista’s subsequent development of the Babelfish website is much better known. Some ten years on, numerous sites offer MT between vast numbers of language pairs, although some of them are little more than on-line dictionaries, sometimes of dubious quality. There have been a number of studies of the use of free online MT (FOMT) systems, both from the developers’ and users’ perspectives (Bennett, 1996; Miyazawa et al., 1999; Yang and Lange 1998, 2003). This paper concerns one small group of such users, namely language learners.  1.1 MT in the classroom There is a growing literature on the impact of MT in general on the language classroom (see Somers (2001) for an overview), including a series of Workshops at various conferences.1 Much of the focus is on what trainee translators (or language learners as potential professional translators) should learn about MT, and how MT can be taught to computational linguists. There are also contributions suggesting how MT can be used as a kind of computer-assisted language learning tool. Of interest are approaches which seek to exploit the weaknesses of MT to illustrate the differences between languages, or to heighten learners’ appreciation of matters of grammar and style in both languages 
Translation of discontinuous phrases is a major challenge in Machine Translation. Within METIS-II we developed a dictionary lookup strategy by mapping the items of a dictionary entry on non-adjacent words in an input text. Mapping is controlled through so-called contextual rejection, i.e. inappropriate mappings are discarded if they fail to satisfy a predeﬁned set of constraints. We present various dictionary preprocessing steps to transform the entries into a suitable and more eﬀective format for lookup. Then we describe the dictionary matching of discontinuous phrases. We illustrate this process on German verbs with detachable preﬁxes and support verb constructions.  
The Data-Oriented Translation (DOT) model – originally proposed in (Poutsma, 1998, 2003) and based on Data-Oriented Parsing (DOP) (e.g. (Bod, Scha, & Sima’an, 2003)) – is best described as a hybrid model of translation as it combines examples, linguistic information and a statistical translation model. Although theoretically interesting, it inherits the computational complexity associated with DOP. In this paper, we focus on one computational challenge for this model: eﬃciently selecting the ‘best’ translation to output. We present four diﬀerent disambiguation strategies in terms of how they are implemented in our DOT system, along with experiments which investigate how they compare in terms of accuracy and eﬃciency.  
This paper investigates the use of morphosyntactic information to reduce datasparseness in statistical machine translation from Spanish to English. In particular, word-alignment training is performed by applying different word transformations using lemmas and stems. It has been observed that stem-based training is better than lemma-based training when up to 1 million running words of data are used. In this paper a new word-alignment training technique is proposed by exploiting syntactically motivated constraints to the parallel data. Preliminary experimental results show that stem-based training with syntactically motivated constraints gives signiﬁcant improvement in translation performance. Finally, a technique to reduce the impact of out-of-vocabulary words is discussed. The considered task is the translation of Plenary Sessions of the European Parliament. 
We present an intuitive technical framework for making Computer Assisted Translation (CAT) adaptable and more suitable for rapid application development. The framework is a client-server-based architecture that uses an approach similar to “message passing”, a technique widely used in computer science. We deﬁne a “translation object”, a structure holding all necessary data, that is passed to server-like processes via sockets. This method can be easily enhanced in a modular manner where several recipients build a chain, one passing the processed object to the next one. We enhance a state-of-the-art phrase-based translation system with server and interactive generation capabilities and evaluate this prototype on diﬀerent language pairs.  
1. Introduction Evaluating machine translation hypotheses is a very important part of the ongoing research. Automatic scoring metrics allow a fast evaluation of translations and a quick turnaround for experiments. Researchers rely on the evaluation metrics to measure performance improvements gained by new approaches. The well-known automatic scores for machine translation are BLEU and NIST but a variety of other scores is available (Papineni, Roukos, Ward, and Zhu, 2002; Doddington, 2001; Banerjee and Lavie, 2005). Most of the scores rely on software programs or scripts that expect a variety of parameters including the hypothesis and reference files. The software then calculates the appropriate score. For most applications the files have to be in a special SGML file format that tags the different parts of the hypothesis or reference file. It is especially difficult for newcomers or for people who just want to get a glimpse of the possibilities to use these software programs. An experienced developer will most probably have a sophisticated setup for translation scoring but this will take a while for a beginner. The web server application presented here tries to circumvent some of the difficulties of scoring machine translation output. The online user interface offers an interactive environment in which test sets and experiments can be defined and hypotheses can be scored. The server stores the submitted translations for later  review. It also offers directly accessible web services that allow score calculation in scripts and software programs based on the defined test sets. 2. Related Work Online Servers for Competitive Evaluations Different online servers have been used to evaluate translations for a variety of competitive evaluations. Especially notable are the evaluation servers for the NIST MT Evaluations (NIST, 2001-2006) and for the Evaluations in the International Workshops for Spoken Language Translation (IWSLT) in the years 2004 and 20051 (Akiba, Federico, Kando, Nakaiwa, Paul, and Tsuji, 2004; Eck and Hori, 2005). All of these evaluation servers were geared towards the needs of a competitive evaluation. The main goal was to make it easier for the organizers to handle a large amount of translation submissions and not necessarily to support the research of the participants. The servers did for example not show any scores during the actual evaluation period so that tuning the systems was impossible. The servers also did not provide any possibility to the participants to set up their own test sets. 
Grammatical Framework (GF) is a meta-language for multilingual linguistic descriptions, which can be used to build rule-based interlingua MT applications in natural sublanguage domains. The GF open-source package contains linguistic and computational resources to facilitate language engineering including: a resource grammar library for ten languages, a user interface for multilingual authoring and a grammar development environment.  
1. Introduction Low translatability indicators, such as included and parallel structures, ambiguous PP attachments, etc., (Underwood and  Jongejan, 2001) characteristic of long sentences, are the reasons that currently no commercial MT system can translate patent claims adequately.  A device for producing a spray of electrically charged particles comprising means defining a location from which the spray is generated, and a voltage generator for producing high voltage between said location and the surroundings, characterized in that said voltage generator comprises a large solid state array of radiation sensitive voltage producing elements interconnected to produce high voltage... Fig. 1. A Fragment of a US patent claim. Predicates of included clauses are bold-faced.  In many MT systems long sentences are broken up along punctuation, and the segments are parsed separately (Kim and Ehara, 1994). This method can be incorrect due to the punctuation ambiguity. The long sentence problem is sometimes approached by being very selective about which sentences to parse as in (Hobbs and Bear, 1995), where statistical filter is used to preprocess a text. In the Pattern-based English-  Korean MT (Roh et al., 2003) long sentences are handled by using chunking information on the phrase-level of the parse result to which a sentence pattern is applied directly. A patent specific research in MT where the problem of low translatable sentences is addressed by suggesting an interactive analysis module has been done for Russian to English by (Sheremetyeva and Nirenburg, 1999). The most recent  attempt to cope with low translatability of a patent claim is a Japanese-English patent MT system, which merges the English claim authoring system AutoPat (Sheremetyeva, 2003) and a Japanese PC-Transfer application (Neumann, 2005). The APTrans system presented in the current paper integrates some of the transfer and generation techniques described in the last cited works but relies on an automatic analyzer bypassing some of low translatability problems. Although the correlation between the sentence length and ambiguity is clear, the great portion of ambiguities occurs in treatment the higher (clause) nodes in the syntactic tree, on the contrary, processing on the phrase (NP, PP, etc.) level does not usually generate more ambiguity as a sentence becomes longer (Abney, 1996). The specificity of our approach is that parsing is not required to produce the structural information of higher levels than a simple clause in the syntactic tree of a complex claim structure. The parser carries out the analysis on a phrase level and a level of individual simple clauses, which results in an interlingual content representation. The load of detecting a clause hierarchy is shifted to the generator. The system is augmented with domain tuned proofing tools: spelling and grammar checkers. APTrans draws heavily on patent data that, as our research showed, feature great similarity across many languages, i.e., sublanguages of different national languages in patent domain are much closer than these languages as such. The linguistic knowledge of the system currently covers English and Danish, but the methodology, engine programs and developer tools make APTrans easily extendable to any other language pair. 2. Lexicon APTrans lexicon contains rather deep knowledge crucial for all components of  APTrans. It includes corpus-based crossreferenced monolingual lexicons. Every monolingual lexicon consists of a set of single sense entries maximally defined as a tree of the following features: Every entry is maximally defined as a tree of features: SEM-CL[Language[POS RANK [MORPH CASE_ROLE FILLER PATTERN] SEM_Cl - semantic class; CASE_ROLEs, - a set of case roles associated with a lexeme, if any; FILLERs - sets of most probable fillers of case-roles in terms of types of phrases and lexical preferences (field “case-role syntax” in Figure 2). PATTERNs - patterns that code both the knowledge about co-occurrences of lexemes with their case-roles and the knowledge about their linear order (local word order) (linking features), e.g., the pattern (13 x 2 4) for the predicate “connected” (see Figure 2) means that this predicate can have case-roles 1(subject), 2(indirect-object), 3(manner) and 4(purpose) realized simultaneously and in such a case the order of the words should be: “1: wires 3:electrically x: connected 2:to the lamp 4: to switch it off” POS - part of speech out of the set of 14 POS defined for the domain. To simplify processing we consider passive and active predicates as belonging to different parts of speech. MORPH - explicitly listed domain relevant wordforms, number, gender, etc., (morphological features). The beauty of the claim domain is that verb (predicate) paradigms are very much restricted and we can save acquisition effort on listing only those wordforms, which can occur in the claim text and skip those which do not (see more on claim sublanguage analysis in and its representation in TransDict in (Sheremetyeva, 2005).  Figure 2. A screenshot of the developer TransDict interface with a typical maximal entry for the predicate.  Clicking on language bookmarks over the “Word forms” field will open entries equivalent to “connected” in  different languages. Every word form is associated with a supertag (shown on the right of the word form field),  which will be assigned to the word during text tagging.  semantic class “connection” in a finite form,  Figure 2 shows a self-explanatory  present, singular, passive voice. In general, a  screenshot of the TransDict developer  supertag in our system is a typed feature  interface with the (maximal) entry for the  structure; the set of features assigned by  predicate “connected”. All seven domain  every supertag is application and domain  relevant wordforms of the lexeme is  dependent. This allows us both to provide  associated with a specific supertag1 coding  for the adequate disambiguation power of  deep linguistic knowledge.  the analyzer, and to avoid a situation when  For example, the supertag “Pdcs” of the  too fine grain size of features in tags as well  wordform “is connected” in Figure 2 means  as a large number of tags may lead to  that this wordform is a verb from the  computational problems (see e.g., Church,  1988). We currently use 23 supertags that  
(Way & Gough, 2005) demonstrate that their Marker-based EBMT system is capable of outperforming a word-based SMT system trained on reasonably large data sets. (Groves & Way, 2005) take this a stage further and demonstrate that while the EBMT system also outperforms a phrase-based SMT (PBSMT) system, a hybrid ‘example-based SMT’ system incorporating marker chunks and SMT sub-sentential alignments is capable of outperforming both baseline translation models for French–English translation. In this paper, we show that similar gains are to be had from constructing a hybrid ‘statistical EBMT’ system capable of outperforming the baseline system of (Way & Gough, 2005). Using the Europarl (Koehn, 2005) training and test sets we show that this time around, although all ‘hybrid’ variants of the EBMT system fall short of the quality achieved by the baseline PBSMT system, merging elements of the marker-based and SMT data, as in (Groves & Way, 2005), to create a hybrid ‘example-based SMT’ system, outperforms the baseline SMT and EBMT systems from which it is derived. Furthermore, we provide further evidence in favour of hybrid systems by adding an SMT target language model to all EBMT system variants and demonstrate that this too has a positive eﬀect on translation quality. 
 1. Introduction and background Machine translation (MT) is a serious business with a wide range of useful real-life practical applications. It is, however, quite common for MT to unintentionally become the source of hilarity, particularly when “howlers” produced by MT systems are publicised, often as a result of carefully concocted experiments or tests designed to push the reasonable capabilities of translation technology to the limit and expose some inevitable weaknesses. Some of the most commonly cited examples of such howlers have become classics in spite of being apocryphal (Hutchins, 1995). However, others are certainly genuine and are reported on a fairly regular basis in newspapers, magazines, articles on the Internet, postings of discussion forums, etc. Ironically, this way of having fun with (or making fun of) machine translation echoes the humorous results of human translations (whether written or oral) of dubious quality that give rise  to target-language texts taking on unintended meanings, becoming nonsensical or having ambiguous readings, as can happen when translators and interpreters work into a language with which they are not thoroughly familiar. This paper provides an overview of websites that capitalise on the potential of machine translation and MT-like applications to amuse by offering an end product without any real use beyond its entertainment value. After the introduction, which provides a background to this topic, sections 2 and 3 present a description of the two major categories of these fun translation websites with a list of salient examples, outlining some of the main linguistic and design issues behind their implementation. Finally, section 4 provides a conclusion with some considerations on the impact of these amusing websites on the reputation and trustworthiness of real web-based MT systems that showcase and promote commercial translation software.  1.1. Embarrassment and (unwanted) humour with MT on the Internet In 2001 the Italian government published on its official website the biographies of cabinet ministers that had been machine translated from Italian into English using freely available webbased MT services. Since no attention had been paid to drafting the source texts in a way that would increase the likelihood of successful MT processing or to post-editing the output, the translated biographies were a mix of unintelligible, nonsensical and hilarious information – suffice it to mention that for example the proper names of ministers and senior civil servants that were homographs of general-language vocabulary were translated literally (more details on this rather embarrassing incident are reported in Gaspari, 2002: 116). The government’s official response was that those badly translated texts were merely a test that was meant to remain hidden from external viewers of the website, and in fact the incriminated biographies written in poor English were promptly removed. However, the damage had already been done by then, and the embarrassment of the government went hand in hand with the mockery and attacks of the opposition, which seized on the opportunity to poke fun at the ineptitude of the governing coalition, bizarrely exposed and perhaps epitomised by some of the most shameful phrases and hilarious mistranslations taken from the ministerial biographies. (Smith, 2001: 38) discusses the “laughable results” produced by free web-based MT services translating from English into other major languages the contents of an official report into the “intimate dealings” of a former US President with a young female intern that was published in 1998, and argues that given the “potent combination of technical and colloquial English” in the source document that would have been a challenge for a capable human translator, “MT applications were quite out of their depth”. (Smith, 2001) points out that the unedited raw MT output hastily produced in a number of languages using free online MT software was widely disseminated to satisfy the morbid curiosity of Internet users for the contents of the report, and given the poor quality of the results some people were quick to dismiss  MT software as useless. These high-profile incidents – which exemplify a rich repertoire of similar instances, particularly well-publicised in the popular press – beg the question of whether in such circumstances machine translation technology is at fault or, rather, those making use of it without understanding its limitations and being unable to estimate the pitfalls that it might involve. Be that as it may, it seems undeniable that MT output that might be hurriedly branded as a poor-quality or unacceptable translation turns out to possess an entertainment value that people appreciate – and that some might conversely find annoying or embarrassing, if this reflects negatively on their reputation. 1.2. Entertainment and (deliberate) fun with MT on the Internet Until the mid-1990s usage of machine translation systems was, by and large, limited to individuals who worked in corporate or institutional environments where translation tools were available, or who had purchased licences for personal use. However, this situation has dramatically changed in the last ten years due to the popularity of machine translation services that have become available free of charge on the Internet for a variety of language pairs, which has spurred the usage of MT technology among a vast population of nonexpert Internet users, as discussed in (Macklovitch, 2001: 27). In a report on the usage of Babelfish, the pioneer online MT service based on Systran’s core technology that was launched in late 1997, (Yang and Lange, 2003: 201-202) mention that this free MT service is used “as an entertainment tool”, for example by getting it to translate poems, jokes and idioms between languages, which represent notoriously tricky tests for MT software. It is fair to say that many users may not have a deep understanding of the challenges involved in translating with computers (cf. Arnold, 2003) and they might make naive assumptions about how machine translation works (cf. Somers, 2005: 127). It seems, however, reasonable to suggest that even a fairly basic knowledge of the source and/or target languages puts lay-users in a position to realise whether the MT processing of idioms or other  challenging language phenomena (e.g. homographs or polysemous words in a sentence) gives rise to funny mistakes which are often accompanied by spectacular communication failures, and accordingly to enjoy the hilarious results that can ensue. In particular, people with a limited bilingual knowledge may be able to identify, albeit intuitively, where the problems in the MT processing lie, and therefore to understand why misinterpretations or mistranslations occur, often with surprisingly funny results. In summary, the point can be made that although MT software, particularly the services available for free on the web, might produce poor translations, an added bonus of this admittedly low quality is that it tends to provide entertainment at zero cost for well-disposed Internet users. In other words, even though MT software fails in its basic primary objective, which is to provide a good or useful translation into the target language (no matter according to what specific criteria this is defined or measured), an unanticipated side-effect is that there can be a surplus of fun which some users might enjoy. 1.3. MT as we have never known it before In a very thought-provoking paper questioning some widely-held assumptions about appropriate localisation practice which draws on examples taken from fields as diverse as the advertising, entertainment and travel industries, (Schäler, 2005) shows that today localisation professionals deliberately flout some of the key principles in their trade, by preserving or even inserting some elements of “linguistic and cultural strangeness” in products, websites and promotional material of various kinds (ads for the radio and TV, on the covers and packaging of audio-visual products, home pages of websites promoting well-known brands, etc.). He calls this approach “reverse localisation” and argues that maintaining a certain level of strangeness is perceived as attractive, therefore boosting the chances of success of a product, company or service. He discusses a number of instances in which the appeal of a commercial message results from a corrupted localisation process, where “foreign” elements that do not  conform with the norms and expectations of consumers in the target locale are emphasised, rather than concealed or adapted, as would be needed to give the impression that a product completely suits their needs. This argument is based on current practice in localisation and hinges on some deliberate linguistic misunderstandings, awkward (mis)translations and hilarious cultural clashes, illustrating the point that a partially failed localisation (if judged according to strictly functional criteria) is compensated for by the appreciation of an appealing sense of “strangeness” that the public in the target locale seems to enjoy and that therefore is worth promoting, in apparent violation of the basic tenets of proper localisation. In a similar vein, but considering free online MT rather than localisation as such, this paper explores fun translation websites that fail to serve any of the utilitarian goals usually associated with MT software (and commonly expected by MT users), but offer free entertainment and fun. The assumption underlying this paper is that such fun translation websites deserve to be looked at as a new phenomenon grown out of the popularity and success of genuine online MT services, paying particular attention to their impact on the reputation of, and users’ trust in, real Internet-based MT tools. 1.4. Overview of fun translation websites This paper does not consider websites that contain lists or reports of MT howlers, or webpages that discuss how to get funny results out of MT systems, but only looks at online services, i.e. websites with an interface requiring users to provide some sort of input which is then manipulated and returned in a different form, so as to reproduce a machine translation process. Furthermore, due to space limitations, this overview focuses exclusively on websites that are designed to process input written in standard English, and does not claim to cover all existing services, but only aims to provide some salient examples to illustrate typical websites offering fun translations – certainly many more of them exist, and it would be impossible to provide an accurate and comprehensive listing here. The websites considered here can be broadly divided into two major categories, which are  presented and discussed separately in the two following sections in the interest of clarity: the “impersonation” websites are covered in section 2, whilst section 3 is devoted to the “Chinese whispers” websites. Some examples of the main kinds of spoof machine translation websites belonging to these two categories are described, discussing some of the key linguistic and design issues related to their implementation. 2. “Impersonation” websites The “impersonation” websites provide fun translations by modifying input written in standard English into output in non-standard and idiosyncratic varieties of English or invented languages. Section 2.1 looks in particular at the websites providing output whose form tends to reproduce in writing sociolects strongly associated with certain groups of speakers, local dialects or accents typical of certain sections of the community as well as invented languages. Section 2.2 focuses more specifically on mock MT services covering the language pair EnglishPig Latin, which is particularly popular. Finally, section 2.3 contains some examples of websites that convert standard written English into the alleged speaking style of celebrities or funny fictional characters, whilst 2.4 concludes with some remarks on some of the key linguistic issues of the implementation of these impersonation websites. 2.1. Translations into sociolects, local dialects, accents or invented languages This section devoted to “impersonation” websites focuses on nine examples of spoof online machine translation services supporting between 1 and 16 “target languages”, revealing the breadth of their coverage and the creativity of their developers. Table 1 in the Appendix shows more detailed information, including the URLs of these services, whether they accept input in the form of plain text and/or entire webpages, as well as the number and names of the translation options offered. Most of the “target languages” are self-explanatory, but additional information provided by the websites themselves is supplied between square brackets in the table to explain the most obscure names.  There is some overlap for certain nonstandard varieties that are supported by more than one fun translation website with exactly the same name (e.g. “Redneck” and “Jive” are found four times each) or with similar ones (e.g. “Valley Girl” and “Valspeak”, “Cockney London/Rhyming Slang” and “Cockney”). In addition, some invented languages are defined by means of different labels, although they essentially refer to very similar manipulations of the input at the surface spelling level. This is the case, for instance, for “Lame HAcKer”, “simple hax0r”, “Ultra Leet”, “L33T-SP34K” and “hAck3r”, all of which cause the substitution of the spelling of the input in standard English to create strings of symbols (including digits and special characters as well as letters) that are more or less unintelligible to the untrained eye, and that mirror the cryptic spelling that is typical of some Internet users, especially hackers. This idiosyncratic form of writing is usually called “Leet”, “Leetspeek” or “1337”, and since the spelling variations giving rise to it do not follow a set convention, it can be characterised by more or less complex levels of adaptation with an impact on the difficulty in decoding the underlying message1. This range of different “flavours” is reflected by the options offered in the websites covered in this section. A number of tests revealed that some of the translation options enabling users to convert input from standard English into sociolects, local dialects, non-standard accents or invented languages offered by some of these impersonation websites mimicking MT are exactly identical. In fact, some of the software powering these fun translation services is based on the same or very similar code, and as a result the output they provide for the same input may be identical or only display minor differences. Also, some of these parody MT websites are clearly related, as they show identical sets of instructions and information for the users. Although these similarities are significant in some cases, they are not pointed out and discussed in detail here for reasons of space. Finally, it is also worth noting that the listing provided in Table 1 in the Appendix is somewhat simplified, as for example the 
Mixture modelling is a standard pattern classiﬁcation technique. However, in statistical machine translation, the use of mixture modelling is still unexplored. Two main advantages of the mixture approach are ﬁrst, its ﬂexibility to ﬁnd an appropriate tradeoﬀ between model complexity and the amount of training data available and second, its capability to learn speciﬁc probability distributions that better ﬁt subsets of the training dataset. This latter advantage is even more important in statistical machine translation, since it is well known that most of the current translation models proposed have limited application to restricted semantic domains. In this paper, we describe a mixture extension of the IBM model 2 along with the maximum likelihood estimation of its parameters through the EM algorithm and a dynamic-programming decoding algorithm for this mixture model. Preliminary experiments carried out on the Tourist task show that the mixture extension conveys a decrease in word-error rate of up to 15%.  
We present a novel approach for the automatic translation of written text into sign language. A new corpus focussing on the weather report domain for the language pair German and German Sign Language is introduced. We apply phrase-based statistical machine translation, enhanced by pre- and post-processing steps based on the morpho-syntactical analysis of German. Detailed results are given based on automatic and manual evaluation.  
Phrase-based statistical translation systems are currently providing excellent results in real machine translation tasks. In phrase-based statistical translation systems, the basic translation units are word phrases. An important problem that is related to the estimation of phrase-based statistical models is the obtaining of word phrases from an aligned bilingual training corpus. In this work, we propose obtaining word phrases by means of a Stochastic Inversion Transduction Grammar. Preliminary experiments have been carried out on real tasks and promising results have been obtained.  
We present a method for improving statistical machine translation performance by using linguistically motivated syntactic information. Our algorithm recursively decomposes source language sentences into syntactically simpler and shorter chunks, and recomposes their translation to form target language sentences. This improves both the word order and lexical selection of the translation. We report statistically signiﬁcant relative improvements of 3.3% BLEU score in an experiment (English→Spanish) carried out on an 800-sentence test set extracted from the Europarl corpus.  
How effectively can people perform the text-handling task of extracting information from the output of MT engines? When is the output of one MT engine more likely than the output of another engine to support people performing an extraction task? This paper reports on the results of a one-of-akind, large-scale, MT evaluation experiment where nearly sixty subjects extracted who, when, and where-type elements of information (EIs) from output generated by three types of Arabic-English MT engines. Our hypothesis was that, in an end-to-end (MT engine and user) evaluation, the best extraction results would come from subjects working with output from MT engines that reordered Arabic input to generate English word order, rather than from an engine that did not, i.e., from a statistical or a rule-based, rather than from a substitution-based MT engine. The results of the experiment were not as straight-forward as expected: (1) non-response rates were statistically comparable across all three evaluated MT engines, while (2) correct response rates were statistically comparable on two engines, the statistical and substitution-based engines that yielded better (higher) rates than the rule-based engine did, and (3) incorrect response rates were statistically comparable on a different pair of two engines, the rule- and substitution-based engines that yielded signiﬁcantly worse (higher) rates than the statistical engine. While these results do indicate that the statistical engine yielded signiﬁcantly better rates than at least one of the other two engines on two of the three metrics, the lack of uniform results pre-empts an across-the-board ranking of the engines. Our next step is to incorporate the collected data in statistical models and test for their adequacy in predicting these task results from faster and less expensive, automatic metrics. The long-term goal is to understand which metrics accurately predict MT users’ task effectiveness with different MT engines on text-handling tasks of varying levels of difﬁculty.  
 1. Introduction The paper gives an overview of the evaluation methods of memory-based translation systems: Translation Memories (TM) and Example Based Machine Translation (EBMT) systems. After a short comparison with the well-discussed methods of evaluation of Machine Translation (MT) Systems we give a brief overview of current methodology on memory-based applications. We propose a new aspect, which takes the content of memory into account: a measure to describe the correspondence between the memory and the current segment to translate. We also offer a brief survey of a linguistically enriched translation memory on which these new methods will be tested. The literature of MT Systems discusses the theme of evaluation exhaustively and plenty of methods and measures arose in the past decades. Whilst in the case of translation memories and (in many respects very similar) EBMT systems the methods are less comprehensive. The evaluation of a machine translation system aims to measure a distance (or similarity) between the output of the system and a human translation used as a gold standard. Whereas the evaluation 
We present a pilot study for using transformation-based learning for automatic correction of rule-based machine translation. Correction rules are learned based on a parallel corpus of machine translations from a commercial machine translation system and a human-corrected version of these translations. The correction rules exploit information on word forms and part of speech. The experiment results in a relative increase in translation quality of 4.6% measured using the BLEU metric.  
While it is generally agreed that Word Sense Disambiguation (WSD) is an application-dependent task, the great majority of systems pursue application-independent approaches. We propose a strategy to support WSD for Machine Translation which is designed specifically for this application. It relies on the analysis of co-occurrences in the context that refer to words which have already been translated. Experiments on the English-Portuguese translation of 10 verbs using just this knowledge yielded an accuracy of 51%, which outperforms the baseline using the most frequent translation (37%). A less strict evaluation criterion considering the 10 best ranked translations proved the potential for this approach to be used as extra knowledge source for WSD: the correct translation was among the top 10 results in 92% of the cases.  1. Introduction Word Sense Disambiguation (WSD) in Machine Translation (MT) is concerned with the choice of the most appropriate translation of an ambiguous word given its context. The question of whether WSD is useful for MT has recently been debated again. Vickrey et al. (2005), e.g., showed that a WSD module significantly improves the performance of their statistical MT system. Conversely, Carpuat and Wu (2005) claim that WSD does not yield significantly better translation quality than a statistical MT system alone. In this latter work, however, the WSD module was not specifically designed for MT. In fact, although it has been agreed that multilingual WSD differs from monolingual WSD (Hutchins and Somers, 1992) and that WSD is ultimately a task which is only relevant in the context of a specific application (Wilks and Stevenson, 1998), little has been done on the development of WSD modules for particular applications. WSD models in general are application independent, and focus on monolingual contexts, particularly English. In the case of MT, the application we are dealing with, WSD approaches usually apply traditional sense repositories, such as the one provided by WordNet (Miller, 1990), to identify the monolingual senses, which are then mapped into the target language translations.  However, mapping senses between languages is a very complex issue. One of the reasons for this complexity is the difference in the sense inventories of the languages, as already discussed in (Hutchins and Somers, 1992) and recently evidenced by studies with certain pairs of languages. For example, Bentivogli et al. (2004) investigate the sense inventory discrepancies for English-Italian, Miháltz (2005), for English-Hungarian, Chatterjee et al. (2005), for English-Hindi, and Specia et al. (2006), for English-Portuguese. They show that there is not a one-to-one relation between the number of senses in the source language and their translations into another language. More specifically, they show that many source language senses are translated into a unique target language word, while some senses need to be split into different translations, conveying sense distinctions that only exist in the target language. In addition to the differences in the sense inventory, the disambiguation process can vary according to the application. For instance, in monolingual WSD the main information is the context of the ambiguous word, that is, the surrounding words in a sentence or text. For MT purposes, however, the context may also include the translation in the target language, i.e., words in the text which have already been translated. Although intuitively plausible, this strategy has not been explored specifically for WSD. On the other hand, some related approaches have exploited similar strategies for other purposes. For example, some approaches for MT, especially the statistics-based ap-  proaches, make use of the words which have already been translated as context, implicitly accomplishing basic WSD during the translation process (Egedi et al., 1994; Dagan and Itai, 1994). Some approaches for monolingual WSD use techniques which are similar to ours to gather cooccurrence evidence from bilingual corpus in order either to carry out WSD (Mihalcea and Moldovan, 1999), or to create monolingual sense tagged corpora (Fernández et al., 2004). Other monolingual related approaches somehow explore the already disambiguated or unambiguous words by taking into account the senses of the other words in the sentence in order to disambiguate a given word (Lesk, 1986; Hirst, 1987; Cowie et al., 1992). In this paper we investigate the use of the translation context, that is, the surrounding words which have already been translated, as knowledge source for multilingual WSD. We present experiments on the disambiguation of 10 ambiguous verbs in English-Portuguese translation. The target language contextual information is applied by analysing the frequency of occurrence of the possible translations of the ambiguous word in text fragments found on the web by Google®, using ngrams and bags-of-words queries made of each translation along with other words in the target language context. This investigation is part of an on-going project on WSD for MT, which makes use of resources and strategies specific to this application (Specia, 2005): parallel corpora (providing translations), bilingual dictionaries (as sense repositories), and this contextual information referring to target-words. The proposed approach also employs several knowledge sources exploring the source-language context, such as part-of-speech, syntactic relations and collocations. It learns a model in the form of a set of ordered rules from examples of translations, which are described by means of these knowledge sources. Knowledge provided by the strategy presented in this paper will be used to reinforce the available evidence. In what follows we present our experimental settings (Section 2), the experiments carried out (Section 3) and the results achieved (Section 4). 2. Experimental setting We limited our experiments to 10 verbs: seven highly frequent and ambiguous verbs previously identified as problematic for English-Portuguese MT systems: “to come”, “to get”, “to give”, “to look”, “to make”, and “to take”, along with other  three also frequent but not so ambiguous verbs: “to ask”, “to live”, and “to tell”. This set of verbs was also used in other experiments on WSD (Specia et al., 2005; Specia et al., 2006). In order to experiment with these verbs we could use an MT system to produce the translation context. However, that would require adapting an MT system to our purposes. Since our intention is to investigate the feasibility of the strategy, we chose to use sentence aligned parallel corpora containing the ambiguous words to supply the translation context. This choice makes the experiment independent of translation methods and systems; in particular, it allows an evaluation which is not biased by the accuracy of an MT system. We assume the translations in our parallel corpus to be correct, since they were produced by human translators. The parallel corpus consists of 100 English sentences for each of the 10 verbs (a total of 1,000 sentences) extracted from the corpus Compara (FrankenbergGarcia and Santos, 2003), which is comprised of fiction books. In order to make the evaluation automatic, we use a version of this parallel corpus in which the translation of the ambiguous verbs has already been (automatically) annotated (Specia et al., 2005). For each occurrence of a certain verb, this corpus contains the English sentence, annotated with such translation, and the corresponding Portuguese sentence, as the example shown in Figure 1. The strategy requires a list with all the possible translations of each verb. This list was extracted from bilingual dictionaries (e.g.: DIC Pratico Michaelis® and Password®), amounting to the numbers shown in Table 1. Our sample corpus includes translations of both phrasal and non-phrasal usages of the verbs.  Verb # translations Verb # translations  ask  16 live  15  come  226 look  63  get  242 make  239  give  128 take  331  go  197 tell  28  Table 1. Verbs and their possible senses in the corpus  3. Obtaining information from the web Our experiments explore translation information through the analysis of co-occurrence frequencies of n-grams and bags-of-words, created from the Portuguese words which have already been translated from the English source sentence and one possible translation of the verb under consideration. This is obtained by querying the web (via Google’s API) with such ngrams or bag-of-words. In essence, an n-gram (or collocation) is a sub-sequence of n words from a  tomar#I will take my medicines tonight, as prescribed by the doctor. Vou tomar meus remédios hoje à noite, conforme indicado pelo médico. Figure 1. Example of parallel sentence in our corpus  given sequence of words in the translation context, including one possible translation of the ambiguous word. A bag-of-words is a subset of m words from the set of words in the translation context, including one possible translation of the ambiguous word, regardless of the order these words appear in the sentence (cf. examples in Table 2). In both cases, the relevant information is the number of hits (retrieved documents) provided by Google. The web was chosen to be used as corpus to provide the statistical co-occurrence information because it is potentially the most representative corpus for Portuguese. However, the strategy could be applied using any monolingual corpus of the target language. Since we are using the parallel corpus to provide translation context, all words in the target sentence, except the ambiguous one, can be used as context in the query. However, as previously mentioned, the parallel corpus is used here to simulate the environment that would be provided by an MT system. If we consider the use the approach in an MT system, the formulation of queries will vary depending on the translation approach. For example, if the MT system translates word-by-word, in the order they appear in the source sentence, the translation context can be constituted by the translations of the words in preceding positions in the source language sentence. Alternatively, if the system first translates all the possible words but the ambiguous one, any subset of the already translated words can be used as context. Other variations are possible for MT approaches translating chunks, translating all the words simultaneously, and translating the sentence based on the identification of the its main structure, usually given by the verb, for example. In our experiments we consider a hypothetical rule-based transfer MT system which first translates all the non-ambiguous words in the sentence and then each ambiguous word - in the order they appear in the sentence - using the WSD module. In order to make it possible to use all the words in the sentence (except the ambiguous verb) as context, we assume that all these words will have already been translated, remaining only the verb to be disambiguated. Thus, our translation context could consist either of non-ambiguous words, which would have been already straightforwardly translated, or previously ambiguous words, which  would have been already disambiguated. Although any combination of words in that context could be used as a query, we limited the types of queries to the followings, each including one of the possible translations of the ambiguous verb (examples in Table 2): (a) bigrams with the first word to the right of the verb; (b) trigrams with the first two words to the right of the verb; (c) trigrams with the first word to the right and to the left of the verb; (d) n-grams with the first two words to the right and the first word to the left of the verb; (e) bags-of-words with all the content words al- ready translated in the sentence, requiring all of them to be included in the results; (f) bags-of-words with all the content words already translated in the sentence, requiring any subset of the words to be in the results. Two types of context are covered by the query sentences: (1) local context, give by n-grams, which consider a small window of surrounding words; and (2) topic context, given by bags-of-words, which consist of all the content words in the sentence. Given the parallel corpus as exemplified in Figure 1, the automatic procedure to determine the queries is explained as follows (taking sentence in Figure 1): 1) Identify the ambiguous target word in the Por- tuguese side of the parallel corpus. Eg.: “Eu vou target=tomar meus remédios hoje à noite, conforme indicado pelo médico”. 2) For every sentence, compose a set of queries for each type of bag-of-words and n-gram. Each query will contain one possible translation of the ambiguous verb. Stop words and other non-content words were eliminated from bags-of-words. Assuming that the verb “to take” has only three possible translations, “tomar” (consume, ingest), “pegar” (buy, select), and “levar” (take someone to some place), e.g., the queries that will be built for the sentence in Figure 1 are shown in Table 2. 3) Search every set of queries for a given sentence in Google, extracting the numbers of hits from the information provided by the search engine. The number of hits for our example is presented in the last column in Table 21. 
This paper reports on experiences with fully automatic lexical transfer rule acquisition in the domain of rule-based machine translation with deep syntactic and semantic processing. We demonstrate that if comprehensive grammars with broad lexical coverage exist for the source and target languages, then open POS class lexical transfer rules can be derived automatically if access to some bilingual dictionary is provided. We show the results in terms of extended coverage and BLEU scores. 
We present a web service to aid translators by quickly producing corpora for specialist areas, in any of a range of languages, from the web. The underlying BootCaT tools have already been extensively used: here, we present a version which is easy for non-technical people to use as all they need do is ﬁll in a web form. The corpus, once produced, can be either downloaded or loaded into the Sketch Engine, a corpus query tool, for further exploration. Reference corpora are used to identify the key terms in the specialist domain.  
A. Timeline of SE/STE (1) Simplified English (SE) started in the late 1970s and was named Simplified English by the sponsoring agency, AECMA (Association of European Manufacturers), to improve aerospace maintenance instructions. (2) SE was made part of the Air Transport Association Publications Spec (ATA-100) in 1987 to control the language level in commercial aerospace technical writing. (3) The SE Guide was revised into its final form in 1995. (4) In 2004 AECMA joined other European rule-making organizations and was re-named AeroSpace Defence Industries of Europe (ASD). SE was re-named to Simplified Technical English (STE) and the SE Guide became the ASD STE-100 Specification.. B. Current Developments In STE (1) ASD’S S1000D Spec for current-technology publications has STE-100 as its default language control (2) S1000D will eventually replace ATA-i2200 (formerly ATA-100) as publications spec for both commercial and military technical writing. (3) ASD S1000D Spec compliance will motivate adoption of ASD STE-100 as the technical writing spec for land/sea/air programs (not only aerospace). 
Abstract: The trend to globalization and “outsourcing” presents a major linguistic challenge. This paper presents a proven methodology to use SMART Controlled English to write technical documentation for global communications. Today, large corporations must adjust their business practices to communicate more effectively across all time zones and 80 languages. The use of SMART Controlled English, when coupled with Statistical Machine Translation (SMT), will become an ideal method to cross the language barrier. 
Controlled Language checking has traditionally been applied largely on a sentence level by placing restrictions on permissible vocabulary and permissible syntactic constructions, including proper punctuation. Only little attention has been paid to document or discourse level checking. In this paper, we report on work on EasyEnglishAnalyzer to handle certain discourse and document level checks. This work helps take Controlled Language checkers from the sentence level to the discourse and document level. Deep semantic analysis provided by a discourse understanding system assists with the semantically more challenging tasks such as proper paragraph structure. For checks related to overall style and organization, document structure is recognized by enhanced interpretation and use of document structure tags so that appropriate checks can be applied in a context-sensitive manner. Parsing of combined segments is applied in checking correctness of list environments. 
In this paper, we present PENG Online, a web-based authoring tool which allows authors to write RSS feeds in a machine-processable controlled natural language. The authoring tool is implemented as a Java applet and communicates in near real-time with a remote server which runs a controlled natural language processor and an inference engine. The authoring tool provides browser functionality for viewing web pages which can then be summarised and augmented in controlled natural language. The writing process is guided via predictive interface techniques which guarantee that the text conforms to the rules of the controlled natural language. The resulting descriptions in RSS format look informal at first glance and are easy to read and understand by humans, but they have the same formal properties as the underlying logic-based representation language. These properties build an ideal starting point for making inferences over the represented knowledge and for using the RSS feeds as a source for question answering. 1. Introduction It has been convincingly argued that the current architecture for the Semantic Web, with its strong emphasis on RDF for syntactic and semantic compatibility, has severe problems when expressive Semantic Web (reasoning) languages are incorporated [Patel-Schneider, 2005]. An alternative approach is to use conventional first-order logic as the semantic underpinning for the Semantic Web. First-order logic is well understood and well established subsets of first-order logic offer tradeoffs with respect to expressive power, complexity and computability [Horrocks and Patel-Schneider, 2003]. For example, the direct mapping of description logic-based ontology languages and Horn rule languages into subsets of first-order logic provides immediate semantic interoperability and builds the prerequisite for efficient reasoning [Grosof et al., 2003]. Instead of relying on RDF, we suggest using a machine-oriented controlled natural language which is based on first-order logic as interface language to the Semantic Web. To promote our approach, we show how such a controlled natural language can directly be embedded into RSS and used to summarise and augment web pages with information which is easy to understand by humans and easy to process by machines. RSS is a family of XML-based web feed formats designed for sharing and aggregating web content [RSS, 2002]. RSS feeds provide summaries of web content together with links to the full versions of the content. RSS feeds can be created automatically from existing websites with the help of specific markup tags or manually using feed creation software. Probably the 
In a multilingual Semantic Web, authors might write in precise, expressive varieties of diverse languages. Do such controlled languages exist? Of 41 candidates, just 4 were (1) designed for multiple domains and genres and (2) documented enough for evaluation. A sample of Web statements on health and human rights revealed limited expressivity or precision in each language. The most expressive one avoided structural ambiguity but allowed semantic ambiguity that could frustrate human and machine comprehension. The possibility of a practical Web-scale controlled language remains undemonstrated but unrefuted.  Introduction Of various Semantic Web visions (Marshall 2003), the most prominent (Berners-Lee 2001) imagines authors using "off-the-shelf software for writing Semantic Web pages" that machines can reason with. So that authors need not be knowledge engineers (Marshall 2003, Shirky 2003), formal but "seemingly informal" controlled natural languages might make semantic precision practical for them (Bernth 1998a; Clark 2005; Schwitter 2005). If so, equivalent controlled varieties of all languages could make the Semantic Web panlingual. To evaluate the costs, benefits, and feasibility of a controlled-language Semantic Web, including the usability of controlled languages for humans and their tractability for machines, we need appropriate languages. Their documentation must show authors how to represent diverse illocutionary forces, evidentialities, probabilities, times, aspects, moods, numbers, persons, discourse references, entities, and relations. Unlike natural languages, however, such controlled languages must not merely permit but require authors to avoid structural and semantic ambiguities that frustrate automated natural-language processing. Is any controlled natural language Web-ready? I considered projects in the last 25 years, whether their languages were aimed at the Web, machine reasoning, machine translation, or human intelligibility. I found 41 attempts to define (written) controlled varieties of English, Esperanto, French, German, Greek, Japanese, Mandarin, Spanish, or Swedish. A "controlled variety of X" licenses some but not all sentences of X, may require annotations, and may license sentences only resembling X. It may be formalistic (a language-like formal notation) or naturalistic (a language with restrictions), roughly equivalent to the "machine-oriented"/ "human-oriented" distinction (O'Brien 2003, p. 1; Reuther 2003; Schwitter 2006, p. 2). I did not consider controlled editing systems (e.g., Power 2004), natural-language-like programming languages (e.g., Apple 1999), and (3) natural-language-based designs for universal, philosophical, and exploratory languages (e.g., Harrison 2002, Langmaker 2006).  
In 1990, Ford Body & Assembly Operations introduced Standard Language as a requirement for writing process sheet assembly instructions in North America. Standard Language was created to standardize the process sheet assembly instructions and introduce consistency across the entire manufacturing spectrum at Ford Motor Company. Standard Language is a Ford-specific, restricted subset of English that is used to describe the vehicle assembly process at Ford Motor Company. This language is used as the input to the Artificial Intelligence (AI) component of the Global Study Process Allocation System (GSPAS). Process sheets written in Standard Language are read by the AI system and used to generate work assembly instructions. Since its introduction in North America, Standard Language has been deployed to Ford's assembly plants in Europe, South America and Asia. In this paper, we will discuss our implementation of Standard Language at Ford Motor Company and show how a controlled language, such as Standard Language, can be used to provide a competitive advantage in business. 
Processing of Colloquial Arabic is a relatively new area of research, and a number of interesting challenges pertaining to spoken Arabic dialects arise. On the one hand, a whole continuum of Arabic dialects exists, with linguistic differences on phonological, morphological, syntactic, and lexical levels. On the other hand, there are inter-dialectal similarities that need be explored. Furthermore, due to scarcity of dialect-specific linguistic resources and availability of a wide range of resources for Modern Standard Arabic (MSA), it is desirable to explore the possibility of exploiting MSA tools when working on dialects. This paper describes challenges in processing of Colloquial Arabic in the context of language modeling for Automatic Speech Recognition. Using data from Egyptian Colloquial Arabic and MSA, we investigate the question of improving language modeling of Egyptian Arabic with MSA data and resources. As part of the project, we address the problem of linguistic variation between Egyptian Arabic and MSA. To account for differences between MSA and Colloquial Arabic, we experiment with the following techniques of data transformation: morphological simplification (stemming), lexical transductions, and syntactic transformations. While the best performing model remains the one built using only dialectal data, these techniques allow us to obtain an improvement over the baseline MSA model. More specifically, while the effect on perplexity of syntactic transformations is not very significant, stemming of the training and testing data improves the baseline perplexity of the MSA model trained on words by 51{\%}, and lexical transductions yield an 82{\%} perplexity reduction. Although the focus of the present work is on language modeling, we believe the findings of the study will be useful for researchers involved in other areas of processing Arabic dialects, such as parsing and machine translation.
Arabic WordNet is a lexical resource for Modern Standard Arabic based on the widely used Princeton WordNet for English (Fellbaum, 1998). Arabic WordNet (AWN) is based on the design and contents of the universally accepted Princeton WordNet (PWN) and will be mappable straightforwardly onto PWN 2.0 and EuroWordNet (EWN), enabling translation on the lexical level to English and dozens of other languages. We have developed and linked the AWN with the Suggested Upper Merged Ontology (SUMO), where concepts are defined with machine interpretable semantics in first order logic (Niles and Pease, 2001). We have greatly extended the ontology and its set of mappings to provide formal terms and definitions for each synset. The end product would be a linguistic resource with a deep formal semantic foundation that is able to capture the richness of Arabic as described in Elkateb (2005). Tools we have developed as part of this effort include a lexicographer's interface modeled on that used for EuroWordNet, with added facilities for Arabic script, following Black and Elkateb's earlier work (2004). In this paper we describe our methodology for building a lexical resource in Arabic and the challenge of Arabic for lexical resources.
In this paper, we report on several software implementations that we have developed within Prague Arabic Dependency Treebank or some other projects concerned with Arabic Natural Language Processing. We try to guide the reader through some essential tasks and note the solutions that we have designed and used. We as well point to third-party computational systems that the research community might exploit in the future work in this field.
Arabic diacritization (referred to sometimes as vocalization or vowelling), defined as the full or partial representation of short vowels, shadda (consonantal length or germination), tanween (nunation or definiteness), and hamza (the glottal stop and its support letters), is still largely understudied in the current NLP literature. In this paper, the lack of diacritics in standard Arabic texts is presented as a major challenge to most Arabic natural language processing tasks, including parsing. Recent studies (Messaoudi, et al. 2004; Vergyri {\&} Kirchhoff 2004; Zitouni, et al. 2006 and Maamouri, et al. forthcoming) about the place and impact of diacritization in text-based NLP research are presented along with an analysis of the weight of the missing diacritics on Treebank morphological and syntactic analyses and the impact on parser development.
Morphological ambiguity is a major concern for syntactic parsers, POS taggers and other NLP tools. For example, the greater the number of morphological analyses given for a lexical entry, the longer a parser takes in analyzing a sentence, and the greater the number of parses it produces. Xerox Arabic Finite State Morphology and Buckwalter Arabic Morphological Analyzer are two of the best known, well documented, morphological analyzers for Modern Standard Arabic (MSA). Yet there are significant problems with both systems in design as well as coverage that increase the ambiguity rate. This paper shows how an ambiguity-controlled morphological analyzer for Arabic is built in a rule-based system that takes the stem as the base form using finite state technology. The paper also points out sources of legal and illegal ambiguities in MSA, and how ambiguity in the new system is reduced without compromising precision. At the end, an evaluation of Xerox, Buckwalter, and our system is conducted, and the performance is compared and analyzed.
Arabic has a very rich and complex morphology. Its appropriate morphological processing is very important for Information Retrieval (IR). In this paper, we propose a new stemming technique that tries to determine the stem of a word representing the semantic core of this word according to Arabic morphology. This method is compared to a commonly used light stemming technique which truncates a word by simple rules. Our tests on TREC collections show that the new stemming technique is more effective than the light stemming.
After providing a brief introduction to the transliteration problem, and highlighting some issues specific to Arabic to English translation, a three phase algorithm is introduced as a computational solution to the problem. The algorithm is based on a Hidden Markov Model approach, but also leverages information available in on-line databases. The algorithm is then evaluated, and shown to achieve accuracy approaching .80{\%}
This article describes the construction of a lexicon and a morphological description for standard Arabic. This system uses finite state technology to parse vowelled texts, as well as partially and not vowelled ones. It is based on large-coverage morphological grammars covering all grammatical rules.
Cross-language information retrieval consists in providing a query in one language and searching documents in different languages. Retrieved documents are ordered by the probability of being relevant to the user's request with the highest ranked being considered the most relevant document. The LIC2M cross-language information retrieval system is a weighted Boolean search engine based on a deep linguistic analysis of the query and the documents to be indexed. This system, designed to work on Arabic, Chinese, English, French, German and Spanish, is composed of a multilingual linguistic analyzer, a statistical analyzer, a reformulator, a comparator and a search engine. The multilingual linguistic analyzer includes a morphological analyzer, a part-of-speech tagger and a syntactic analyzer. In the case of Arabic, a clitic stemmer is added to the morphological analyzer to segment the input words into proclitics, simple forms and enclitics. The linguistic analyzer processes both documents to be indexed and queries to produce a set of normalized lemmas, a set of named entities and a set of nominal compounds with their morpho-syntactic tags. The statistical analyzer computes for documents to be indexed concept weights based on concept database frequencies. The comparator computes intersections between queries and documents and provides a relevance weight for each intersection. Before this comparison, the reformulator expands queries during the search. The expansion is used to infer from the original query words other words expressing the same concepts. The expansion can be in the same language or in different languages. The search engine retrieves the ranked, relevant documents from the indexes according to the corresponding reformulated query and then merges the results obtained for each language, taking into account the original words of the query and their weights in order to score the documents. Sentence alignment consists in estimating which sentence or sentences in the source language correspond with which sentence or sentences in a target language. We present in this paper a new approach to aligning sentences from a parallel corpora based on the LIC2M cross-language information retrieval system. This approach consists in building a database of sentences of the target text and considering each sentence of the source text as a ``query'' to that database. The aligned bilingual parallel corpora can be used as a translation memory in a computer-aided translation tool.
This paper describes the construction of a dependency bank gold standard for Arabic, DCU 250 Arabic Dependency Bank (DCU 250), based on the Arabic Penn Treebank Corpus (ATB) (Bies and Maamouri, 2003; Maamouri and Bies, 2004) within the theoretical framework of Lexical Functional Grammar (LFG). For parsing and automatically extracting grammatical and lexical resources from treebanks, it is necessary to evaluate against established gold standard resources. Gold standards for various languages have been developed, but to our knowledge, such a resource has not yet been constructed for Arabic. The construction of the DCU 250 marks the first step towards the creation of an automatic LFG f-structure annotation algorithm for the ATB, and for the extraction of Arabic grammatical and lexical resources.
The paper describes the translations of three online systems: Google, Sakhr, and Systran, using two sets of texts (Arabic and English) as input. It diagnoses the faults and attempts to detect the reasons, trying to shed light on the areas where the right translation solution is missed. Flaws and translation problems are categorized and analyzed, and recommendations are given. The two modes of translation (from and into Arabic) face a wide range of common linguistic problems as well as mode-specific problems. These problems are discussed and examples of output are given. The paper raises questions whose answers should help in the improvement of MT systems. The questions deal with establishing equivalents, lexical environment, and collocation. Cases that triggered these questions are illustrated and discussed.
The interlingua approach to Machine Translation (MT) aims to achieve the translation task in two independent steps. First, the meanings of source language sentences are represented in an intermediate (interlingua) representation. Then, sentences of the target language are generated from those meaning representations. In the generation of the target sentence, determining sentence structures becomes more difficult, especially when the interlingua does not contain any syntactic information. Hence, the sentence structures cannot be transferred exactly from the interlingua representations. In this paper, we present a mapping approach for task- oriented interlingua-based spoken dialogue that transforms an interlingua representation, so-called Interchange Format (IF), into a feature structure (FS) that reflects the syntactic structure of the target Arabic sentence. This approach addresses the handling of the problem of Arabic syntactic structure determination in the interlingua approach. A mapper is developed primarily within the framework of the NESPOLE! (NEgotiating through SPOken Language in E-commerce) multilingual speech-to-speech MT project. The IF-to-Arabic FS mapper is implemented in SICStus Prolog. Examples of Arabic syntactic mapping, using the output from the English analyzer provided by Carnegie Mellon University (CMU), will illustrate how the system works.
This paper reports on an experiment investigating how effective free online machine translation (MT) is in helping Internet users to access the contents of websites written only in languages they do not know. This study explores the extent to which using Internet-based MT tools affects the confidence of web-surfers in the reliability of the information they find on websites available only in languages unfamiliar to them. The results of a case study for the language pair Italian-English involving 101 participants show that the chances of identifying correctly basic information (i.e. understanding the nature of websites and finding contact telephone numbers from their web-pages) are consistently enhanced to varying degrees (up to nearly 20%) by translating online content into a familiar language. In addition, confidence ratings given by users to the reliability and accuracy of the information they find are significantly higher (with increases between 5 and 11%) when they translate websites into their preferred language with free online MT services. 
 New Optimism in MT Community 2006 June 30: http://businessnetwork.smh.com.au/articles/2006/06/30/5104.html • Within the next few years there will be an explosion in translation technologies, says Alex Waibel, director of the International Centre for Advanced Communication Technology… • How far can machine translators be taken? "There is no reason why they should not become as good, if not better, than humans," Dr Waibel says.  Part 1: Challenges Ahead for Data-driven Machine Translation • a: Comparison with human qualifications • b: Avoidance of compositionality assumption • c: Using relevant co-text (beyond sentence) • d: Using relevant "extra-text" (real world info) • e: Displaying "second-order creativity" (creating novel solutions and detecting need)  Challenge 1: Comparison with Human Qualifications  Challenge 1: Comparison with Human Qualifications • Display same qualifications required of human translators or explain why some are not needed for data-driven machine translation systems  Human Translation Project Phases (ASTM)  SPECIFICATIONS PHASE Ter minology Management  PRODUCTION PHASE Requester  POST-PROJECT REVIEW  Pro j ect Manager  Sp eci fi cati o n s Agreement  Pro o fi n g , Veri fi cati o n, QC, Del i very  T ransl ati o n  Ed i ti ng  F i nal F o rmatti n g , Co mp i l ati o n Optiona l 3 rd Pa r ty R e v ie w (c a n oc c ur a t a n a gr e e d upon point in the proc e s s )  Specifications Phase • Begin with: – Source test – Target language – Target audience – Purpose of translation • Negotiate: – Specifications for this project  Production Phase • Specifications Agreement (mode adjustment) • Translation (actual translation) • Editing (source- vs. target-text comparison) • Formatting (e.g. integrate source format) • Proofing (monolingual target-text check)  Some qualifications needed for human translators • Ability to understand source text • Ability to write in target language • Ability to adjust to audience and purpose, when translating and evaluating whether source and target texts correspond  Audience and Purpose • Same source text may be translated very differently, depending on audience and purpose – A story could be translated for easy reading and the storyline (adjusted for target culture) – Same story could be translated for access to the source culture by those who can't read original  Data-driven Comments on Challenge 1 Airplanes don’t bat their wings, but they still fly.  Chinese Room Experiment  new Chinese document  English translation  Chinese texts with English translations Chinese word or phrase => sentence pairs containing it  HIGH ACCURACY DOES THIS PERSON KNOW CHINESE?  Chinese Room Experiment  170k sentence pairs of bilingual training data (3.5m words translated)  14 1 245  3 3 5  2 56  test subsequence “c6 c7” has been observed 56 times in training data 3  47  1016  110  
The DARPA Challenge DARPA can develop technology that –  Removes language barriers  Enables soldiers to: ¾ Communicate with allies, enemies, local populations ¾ Understand huge amounts of information available in foreign languages ¾ Decipher captured documents ¾ Learn foreign languages Distribution Statement “A” (Approved for Public Release, Distribution Unlimited).  Human - Machine Interaction  Historical Perspective ¾ Early Greeks ¾ Late 18th Century  Scientific Approaches ¾ Helmholtz ¾ Electronics  Digital Computers Distribution Statement “A” (Approved for Public Release, Distribution Unlimited).  Human – Computer Communication  Science fiction ¾ HAL ¾ C3PO  Reality ¾ Little knowledge ¾ No experience ¾ No deductive reasoning ¾ No language comprehension Distribution Statement “A” (Approved for Public Release, Distribution Unlimited).  Speech Recognition Late ’60s  Small vocabulary (10 words)  Speaker dependent  Low accuracy (approx. 85%) “General purpose speech recognition seems far away. Special purpose speech recognition is severely limited. It would seem appropriate for people to ask themselves why they are working in the field and what they expect to accomplish.” -- John Pierce, 1969 Distribution Statement “A” (Approved for Public Release, Distribution Unlimited).  Advances in Speech Recognition  Speaker independence  Large vocabularies ¾ Word spotting ¾ Concept spotting • Robotic operators • Call centers • Travel reservations  Unconstrained vocabularies ¾ Dictation systems ¾ Closed captioning Distribution Statement “A” (Approved for Public Release, Distribution Unlimited).  COMPUTING  DARPA’s SPOKEN LANGUAGE PROGRAMS 1983-1996: SOME SIGNIFICANT EVENTS  83 84 85 86 87 88 89 90 91 92 93 94 95 96  SPEECH RECOGNITION  SPOKEN LANGUAGE  HUMAN LANGUAGE  88  TOPIC/SPKR/LANGID  TECHNOLOGY SYSTEMS  PLAN  T  STRATEGIC  SU R 7176 KAHN, ADAMS, OHLANDER 83 84  ISA  NEURAL NETS  NUS  92  AFTI F16 AR DE N IEE E85PA LO AL TO  FIRST BENCH MARKS  SLCC  BYBLO S DEMO SPHIN X DEMO  ATIS CHOICE  ARRANGE D MARRIAGE  ISA  T 
In this paper we describe a set of processes for the acquisition of re{\-}sources for quick ramp{\-}up machine translation (MT) from any language lacking significant machine tracta{\-}ble resources into English, using the Paraguayan indigenous lan{\-}guage Guarani as well as Amharic and Chechen, as examples. Our task is to develop a 250,000 mono{\-}lingual corpus, a 250,000 bilingual parallel corpus, and smaller corpora tagged with part of speech, named entity, and morphological annota{\-}tions.
The Joint Probability Model proposed by Marcu and Wong (2002) provides a probabilistic framework for modeling phrase-based statistical machine transla- tion (SMT). The model{'}s usefulness is, however, limited by the computational complexity of estimating parameters at the phrase level. We present a method of constraining the search space of the Joint Probability Model based on statistically and linguistically motivated word align- ments. This method reduces the complexity and size of the Joint Model and allows it to display performance superior to the standard phrase-based models for small amounts of training material.
Context-Based Machine TranslationTM (CBMT) is a new paradigm for corpus-based translation that requires no parallel text. Instead, CBMT relies on a lightweight translation model utilizing a fullform bilingual dictionary and a sophisticated decoder using long-range context via long n-grams and cascaded overlapping. The translation process is enhanced via in-language substitution of tokens and phrases, both for source and target, when top candidates cannot be confirmed or resolved in decoding. Substitution utilizes a synonym and near-synonym generator implemented as a corpus-based unsupervised learning process. Decoding requires a very large target-language-only corpus, and while substitution in target can be performed using that same corpus, substitution in source requires a separate (and smaller) source monolingual corpus. Spanish-to-English CBMT was tested on Spanish newswire text, achieving a BLEU score of 0.6462 in June 2006, the highest BLEU reported for any language pair. Further testing also shows that quality increases above the reported score as the target corpus size increases and as dictionary coverage of source words and phrases becomes more complete.
This paper presents a reordering framework for statistical machine translation (SMT) where source-side reorderings are integrated into SMT decoding, allowing for a highly constrained reordered search graph. The monotone search is extended by means of a set of reordering patterns (linguistically motivated rewrite patterns). Patterns are automatically learnt in training from word-to-word alignments and source-side Part-Of-Speech (POS) tags. Traversing the extended search graph, the decoder evaluates every hypothesis making use of a group of widely used SMT models and helped by an additional Ngram language model of source-side POS tags. Experiments are reported on the Euparl task (Spanish-to-English and English-to- Spanish). Results are presented regarding translation accuracy (using human and automatic evaluations) and computational efficiency, showing significant improvements in translation quality for both translation directions at a very low computational cost.
As an approach to syntax based statistical machine translation (SMT), Probabilistic Synchronous Dependency Insertion Grammars (PSDIG), introduced in (Ding and Palmer, 2005), are a version of synchronous grammars defined on dependency trees. In this paper we discuss better learning and decoding algorithms for a PSDIG MT system. We introduce two new grammar learners: (1) an exhaustive learner combining different heuristics, (2) an n-gram based grammar learner. Combining the grammar rules learned from the two learners improved the performance. We introduce a better decoding algorithm which incorporates a tri-gram language model. According to the Bleu metric, the PSDIG MT system performance is significantly better than IBM Model 4, while on par with the state-of-the-art phrase based system Pharaoh (Koehn, 2004). The improved integration of syntax on both source and target languages opens door to more sophisticated SMT processes.
This paper reports on an experiment investigating how effective free online machine translation (MT) is in helping Internet users to access the contents of websites written only in languages they do not know. This study explores the extent to which using Internet-based MT tools affects the confidence of web-surfers in the reliability of the information they find on websites available only in languages unfamiliar to them. The results of a case study for the language pair Italian-English involving 101 participants show that the chances of identifying correctly basic information (i.e. understanding the nature of websites and finding contact telephone numbers from their web-pages) are consistently enhanced to varying degrees (up to nearly 20{\%}) by translating online content into a familiar language. In addition, confidence ratings given by users to the reliability and accuracy of the information they find are significantly higher (with increases between 5 and 11{\%}) when they translate websites into their preferred language with free online MT services.
The research context of this paper is developing hybrid machine translation (MT) systems that exploit the advantages of linguistic rule-based and statistical MT systems. Arabic, as a morphologically rich language, is especially challenging even without addressing the hybridization question. In this paper, we describe the challenges in building an Arabic-English generation-heavy machine translation (GHMT) system and boosting it with statistical machine translation (SMT) components. We present an extensive evaluation of multiple system variants and report positive results on the advantages of hybridization.
In syntax-directed translation, the source-language input is first parsed into a parse-tree, which is then recursively converted into a string in the target-language. We model this conversion by an extended tree-to-string transducer that has multi-level trees on the source-side, which gives our system more expressive power and flexibility. We also define a direct probability model and use a linear-time dynamic programming algorithm to search for the best derivation. The model is then extended to the general log-linear frame-work in order to incorporate other features like n-gram language models. We devise a simple-yet-effective algorithm to generate non-duplicate k-best translations for n-gram rescoring. Preliminary experiments on English-to-Chinese translation show a significant improvement in terms of translation quality compared to a state-of-the- art phrase-based system.
Lexical mappings (word translations) between languages are an invaluable resource for multilingual processing. While the problem of extracting lexical mappings from parallel corpora is well-studied, the task is more challenging when the language samples are from non-parallel corpora. The goal of this work is to investigate one such scenario: finding lexical mappings between dialects of a diglossic language, in which people conduct their written communications in a prestigious formal dialect, but they communicate verbally in a colloquial dialect. Because the two dialects serve different socio-linguistic functions, parallel corpora do not naturally exist between them. An example of a diglossic dialect pair is Modern Standard Arabic (MSA) and Levantine Arabic. In this paper, we evaluate the applicability of a standard algorithm for inducing lexical mappings between comparable corpora (Rapp, 1999) to such diglossic corpora pairs. The focus of the paper is an in-depth error analysis, exploring the notion of relatedness in diglossic corpora and scrutinizing the effects of various dimensions of relatedness (such as mode, topic, style, and statistics) on the quality of the resulting translation lexicon.
We present observations from three exercises designed to map the effective listening and speaking skills of an operator of a speech-to-speech translation system (S2S) to the Interagency Language Roundtable (ILR) scale. Such a mapping is non-trivial, but will be useful for government and military decision makers in managing expectations of S2S technology. We observed domain-dependent S2S capabilities in the ILR range of Level 0+ to Level 1, and interactive text-based machine translation in the Level 3 range.
State-of-the-art statistical machine translation is based on alignments between phrases {--} sequences of words in the source and target sentences. The learning step in these systems often relies on alignments between words. It is often assumed that the quality of this word alignment is critical for translation. However, recent results suggest that the relationship between alignment quality and translation quality is weaker than previously thought. We investigate this question directly, comparing the impact of high-quality alignments with a carefully constructed set of degraded alignments. In order to tease apart various interactions, we report experiments investigating the impact of alignments on different aspects of the system. Our results confirm a weak correlation, but they also illustrate that more data and better feature engineering may be more beneficial than better alignment.
Accurately translating multiword expressions is important to obtain good performance in machine translation, cross-language information retrieval, and other multilingual tasks in human language technology. Existing approaches to inducing translation equivalents of multiword units have focused on agglomerating individual words or on aligning words in a statistical machine translation system. We present a different approach based upon information theoretic heuristics and the exact counting of frequencies of occurrence of multiword strings in aligned parallel corpora. We are applying a technique introduced by Yamamoto and Church that uses suffix arrays and longest common prefix arrays. Evaluation of the method in multiple language pairs was performed using bilingual lexicons of domain-specific terminology as a gold standard. We found that performance of 50-70{\%}, as measured by mean reciprocal rank, can be obtained for terms that occur more than 10 or so times.
In this paper, we present a novel approach to combine the outputs of multiple MT engines into a consensus translation. In contrast to previous Multi-Engine Machine Translation (MEMT) techniques, we do not rely on word alignments of output hypotheses, but prepare the input sentence for multi-engine processing. We do this by using a recursive decomposition algorithm that produces simple chunks as input to the MT engines. A consensus translation is produced by combining the best chunk translations, selected through majority voting, a trigram language model score and a confidence score assigned to each MT engine. We report statistically significant relative improvements of up to 9{\%} BLEU score in experiments (English→Spanish) carried out on an 800-sentence test set extracted from the Penn-II Treebank.
This paper evaluates the hypothesis that pictorial representations can be used to effectively convey simple sentences across language barriers. Comparative evaluations show that a considerable amount of understanding can be achieved using visual descriptions of information, with evaluation figures within a comparable range of those obtained with linguistic representations produced by an automatic machine translation system.
The more expressive and flexible a base formalism for machine translation is, the less efficient parsing of it will be. However, even among formalisms with the same parse complexity, some formalisms better realize the desired characteristics for machine translation formalisms than others. We introduce a particular formalism, probabilistic synchronous tree-insertion grammar (PSTIG) that we argue satisfies the desiderata optimally within the class of formalisms that can be parsed no less efficiently than context-free grammars and demonstrate that it outperforms state-of-the-art word-based and phrase-based finite-state translation models on training and test data taken from the EuroParl corpus (Koehn, 2005). We then argue that a higher level of translation quality can be achieved by hybridizing our in- duced model with elementary structures produced using supervised techniques such as those of Groves et al. (2004).
This paper presents our study of exploiting morpho-syntactic information for phrase-based statistical machine translation (SMT). For morphological transformation, we use hand-crafted transformational rules. For syntactic transformation, we propose a transformational model based on Bayes{'} formula. The model is trained using a bilingual corpus and a broad coverage parser of the source language. The morphological and syntactic transformations are used in the preprocessing phase of a SMT system. This preprocessing method is applicable to language pairs in which the target language is poor in resources. We applied the proposed method to translation from English to Vietnamese. Our experiments showed a BLEU-score improvement of more than 3.28{\%} in comparison with Pharaoh, a state-of-the-art phrase-based SMT system.
TransBooster is a wrapper technology designed to improve the performance of wide-coverage machine translation systems. Using linguistically motivated syntactic information, it automatically decomposes source language sentences into shorter and syntactically simpler chunks, and recomposes their translation to form target language sentences. This generally improves both the word order and lexical selection of the translation. To date, TransBooster has been successfully applied to rule-based MT, statistical MT, and multi-engine MT. This paper presents the application of TransBooster to Example-Based Machine Translation. In an experiment conducted on test sets extracted from Europarl and the Penn II Treebank we show that our method can raise the BLEU score up to 3.8{\%} relative to the EBMT baseline. We also conduct a manual evaluation, showing that TransBooster-enhanced EBMT produces a better output in terms of fluency than the baseline EBMT in 55{\%} of the cases and in terms of accuracy in 53{\%} of the cases.
We propose and evaluate a new paradigm for machine translation of low resource languages via the learned surface transduction and paraphrase of multilingual glosses.
This paper shows the applicability of language testing techniques to machine translation (MT) evaluation through one of a set of related experiments. One straightforward experiment is to use language testing exams and scoring on MT output with little or no adaptation. This paper describes one such experiment, the first in a set. After an initial test (Vanni and Reeder, 2000), we expanded the experiment to include multiple raters and a more detailed analysis of the surprising results. Namely that unlike with humans, MT systems perform more poorly at both level zero and one than at level two and three. This paper presents these results as an illustration of both the applicability of language testing techniques and also the caution that needs to be applied.
Translation adequacy is defined as the amount of semantic content from the source language document that is conveyed in the target language document. As such, it is more difficult to measure than intelligibility since semantic content must be measured in two documents and then compared. Latent Semantic Analysis is a content measurement technique used in language learner evaluation that exhibits characteristics attractive for re-use in machine translation evaluation (MTE). This experiment, which is a series of applications of the LSA algorithm in various configurations, demonstrates its usefulness as an MTE metric for adequacy. In addition, this experiment lays the groundwork for using LSA as a method to measure the accuracy of a translation without reliance on reference translations.
Inflected languages in a low-resource setting present a data sparsity problem for statistical machine translation. In this paper, we present a minimally supervised algorithm for morpheme segmentation on Arabic dialects which reduces unknown words at translation time by over 50{\%}, total vocabulary size by over 40{\%}, and yields a significant increase in BLEU score over a previous state-of-the-art phrase-based statistical MT system.
Statistical Machine Translation (SMT) accuracy degrades when there is only a limited amount of training, or when the training is not from the same domain or genre of text as the target application. However, cross-domain applications are typical of many real world tasks. We demonstrate that SMT accuracy can be improved in a cross-domain application by using a controlled language (CL) interface to help reduce lexical ambiguity in the input text. Our system, CL-MT, presents a monolingual user with a choice of word senses for each content word in the input text. CL-MT temporarily adjusts the underlying SMT system's phrase table, boosting the scores of translations that include the word senses preferred by the user and lowering scores for disfavored translations. We demonstrate that this improves translation adequacy in 33.8{\%} of the sentences in Spanish to English translation of news stories, where the SMT system was trained on proceedings of the European Parliament.
We present and empirically compare a range of novel probabilistic finite-state transducer (PFST) models targeted at two major natural language string transduction tasks, transliteration selection and cognate translation selection. Evaluation is performed on 10 distinct language pair data sets, and in each case novel models consistently and substantially outperform a well-established standard reference algorithm.
In this paper, we discuss techniques to combine an interlingua translation framework with phrase-based statistical methods, for translation from Chinese into English. Our goal is to achieve high-quality translation, suitable for use in language tutoring applications. We explore these ideas in the context of a flight domain, for which we have a large corpus of English queries, obtained from users interacting with a dialogue system. Our techniques exploit a pre-existing English-to-Chinese translation system to automatically produce a synthetic bilingual corpus. Several experiments were conducted combining linguistic and statistical methods, and manual evaluation was conducted for a set of 460 Chinese sentences. The best performance achieved an {``}adequate{''} or better analysis (3 or above rating) on nearly 94{\%} of the 409 parsable subset. Using a Rover scheme to combine four systems resulted in an {``}adequate or better{''} rating for 88{\%} of all the utterances.
We examine a new, intuitive measure for evaluating machine-translation output that avoids the knowledge intensiveness of more meaning-based approaches, and the labor-intensiveness of human judgments. Translation Edit Rate (TER) measures the amount of editing that a human would have to perform to change a system output so it exactly matches a reference translation. We show that the single-reference variant of TER correlates as well with human judgments of MT quality as the four-reference variant of BLEU. We also define a human-targeted TER (or HTER) and show that it yields higher correlations with human judgments than BLEU{---}even when BLEU is given human-targeted references. Our results indicate that HTER correlates with human judgments better than HMETEOR and that the four-reference variants of TER and HTER correlate with human judgments as well as{---}or better than{---}a second human judgment does.
Basque is both a minority and a highly inflected language with free order of sentence constituents. Machine Translation of Basque is thus both a real need and a test bed for MT techniques. In this paper, we present a modular Data-Driven MT system which includes different chunkers as well as chunk aligners which can deal with the free order of sentence constituents of Basque. We conducted Basque to English translation experiments, evaluated on a large corpus (270,000 sentence pairs). The experimental results show that our system significantly outperforms state-of-the-art approaches according to several common automatic evaluation metrics.
When response metrics for evaluating the utility of machine translation (MT) output on a given task do not yield a single ranking of MT engines, how are MT users to decide which engine best supports their task? When the cost of different types of response errors vary, how are MT users to factor that information into their rankings? What impact do different costs have on response-based rankings? Starting with data from an extraction experiment detailed in Voss and Tate (2006), this paper describes three response-rate metrics developed to quantify different aspects of MT users{'} performance identifying who/when/where-items in MT output, and then presents a loss function analysis over these rates to derive a single customizable metric, applying a range of values to correct responses and costs to different error types. For the given experimental dataset, loss function analyses provided a clearer characterization of the engines{'} relative strength than did comparing the response rates to each other. For one MT engine, varying the costs had no impact: the engine consistently ranked best. By contrast, cost variations did impact the ranking of the other two engines: a rank reversal occurred on who-item extractions when incorrect responses were penalized more than non-responses. Future work with loss analysis, developing operational cost ratios of error rates to correct response rates, will require user studies and expert document-screening personnel to establish baseline values for effective MT engine support on wh-item extraction.
Discriminative training methods have recently led to significant advances in the state of the art of machine translation (MT). Another promising trend is the incorporation of syntactic information into MT systems. Combining these trends is difficult for reasons of system complexity and computational complexity. The present study makes progress towards a syntax-aware MT system whose every component is trained discriminatively. Our main innovation is an approach to discriminative learning that is computationally efficient enough for large statistical MT systems, yet whose accuracy on translation sub-tasks is near the state of the art. Our source code is downloadable from http://nlp.cs.nyu.edu/GenPar/.
 Hallo!  Presenter: Nizar Habash (Columbia)  Moderators: Violetta Cavalli-Sforza & Alon Lavie (CMU) Fellow Panelists: Jaime Carbonell (CMU), Philipp Koehn (Edinburgh), Stephanie Seneff (MIT), John White and Jean Senellart (Systran)  MT Strategies (1954-2004) Shallow/ Simple  Original statistical  Electronic dictionaries  Word-based  MT  only  Knowledge Acquisition Strategy Hand-built by All manual experts Original direct approach  Hand-built by non-experts  Phrase tables  Example-based MT  Learn from annotated data Syntactic Constituent  Learn from unannotated data Fully automated  Typical transfer  Structure  system Classic interlingual  Semantic analysis Interlingua  New Research Goes Here!  system  Knowledge Deep/ Complex Representation Strategy  Slide courtesy of Laurie Gerber  What is a Hybrid MT system? • “Hybrid” is a moving target – StatMT systems use some rule-based components • orthographic normalization, number/date translation, etc. – RuleMT systems nowadays use statistical n-gram language modeling • Hybrid continuum – Different mixes of statistical/rule-based components – Richly annotated corpora created by linguists – Used for statistical POS tagging » As part of a symbolic interlingual systems » Preprocessing for phrase-based MT – With statistical language modeling component – Every component can be done in either approach – Typically developers use what is available  MT Approaches Statistical vs. Symbolic vs. Hybrid  Source meaning  Target meaning  Source syntax  Target syntax  Source word Analysis  Target word Generation  Why Hybridize? • The Intuition – StatMT and RuleMT have complementary advantages • Syntactic structure produces better global target linguistic structure • Statistical phrase-based translation is more robust locally • The Resource Challenge – Parallel corpora as models of performance vs. Dictionaries/analyzers as models of competence – “More is better” is true for both approaches • Parallel corpora are domain/genre specific • Dictionaries and parsers can be domain/genre specific – Hybrids may need more data 
– Common meaning representation for dialogue manager – Many different languages: English, French, Spanish, German, Italian, Chinese, Japanese, Korean – Many different domains: flights, weather, restaurants, hotels, calendar management, etc. 
 Known practical and theoretical limits to RBMT and SMT  RBMT { Overtly expresses definition of infinite set of phenomena 
• Integrated: – SMT + EBMT (phrase tables, decoders) ≈ SMT++ – Syntax “SMT” (e.g. ISI, UMD, …)  
– Common meaning representation for dialogue manager – Many different languages: English, French, Spanish, German, Italian, Chinese, Japanese, Korean – Many different domains: flights, weather, restaurants, hotels, calendar management, etc. 
 Known practical and theoretical limits to RBMT and SMT  RBMT { Overtly expresses definition of infinite set of phenomena 
This paper compares two techniques for robust parsing of extragrammatical natural language. Both are based on well-known approaches; one selects the optimal combination of partial analyses, the other relaxes grammar rules. Both techniques use a stochastic parser to select the “best” solution among multiple analyses. Experimental results show that regardless of the grammar, the best results are obtained by sequentially combining the two techniques, by ﬁrst relaxing the rules and only when that fails by then selecting a combination of partial analyses. 
In this paper, a new conceptual hierarchy based semantic similarity measure is presented, and it is evaluated in word sense disambiguation using a well known algorithm which is called Maximum Relatedness Disambiguation. In this study, WordNet’s conceptual hierarchy is utilized as the data source, but the methods presented are suitable to other resources. 
This paper presents a series of experiments that were performed with a dataset of Finnish question reformulations. The goal of the experiments was to determine whether some reformulations are easier for a question answering system to deal with than others, and if so, how easy it is to transform a question into that form. A question answering system typically consists of several independent modules that are arranged into a pipeline architecture. In order to determine if some reformulations are easier for a question answering system to deal with, the performance of the question classiﬁer component was analyzed. The experiments show that different question reformulations do affect the performance of the classiﬁer signiﬁcantly. However, the automatic transformation of a question into another form seems difﬁcult. 
We present a simple and efﬁcient approach for deriving bilingual dictionaries from sentence-aligned parallel text by extending the notion of co-occurrences to a cross-lingual setting. Dictionaries are evaluated against gold standards and manually; the analysis accounts for frequency and corpus size effects. 
In this paper we investigate an assertion made by Richards and Underwood (1985), who claim that people interacting with a spoken information retrieval system, structure their information in such a uniform manner that this regularity can be used to enhance the performance of the dialog system. We put forward the possibility that this uniform ordering of information might be due to the design of the written task descriptions used in Wizard of Oz experiments. 
In this paper we have investigated 128 high frequent Swedish compound queries (6.2 per thousand) with no search results among 1.6 million searches carried out at nine public web sites containing all together 100,000 web pages in Swedish. To these compound queries we added a compound splitter as a pre-processor and we found that after decompounding these queries they gave relevant results in 64 percent of the cases instead of zero percent hits. We give also examples on some rules for optimal compound splitting in a search situation. 
In the construction of a computational lexicon, one of the problems is how to handle cases where words have a partial morphological paradigm. In this paper we will describe this problem and sketch how we implemented a system for capturing the degree to which forms should be considered improbable. Also, we will describe how our results can be used in language applications. 
We present a software architecture for data-driven dependency parsing of unrestricted natural language text, which achieves a strict modularization of parsing algorithm, feature model and learning method such that these parameters can be varied independently. The design has been realized in MaltParser, which supports several parsing algorithms and learning methods, for which complex feature models can be deﬁned in a special description language. 
We present a principled background for the adoption of a category ‘Speciﬁer’ in the analysis of noun phrases, and show how, under certain constraining assumptions, it can be successfully employed in the implementation of an HPSG grammar of noun phrases for Norwegian. How widely the assumptions can be applied on empirical grounds cross-linguistically, is still a matter for further investigation. 
This paper presents LiSa, a system for morphological analysis, designed to meet the needs of the Information Retrieval (IR) community. LiSa is an acronym for Linguistic and Statistical Analysis. The system is lexicon- and rule based and developed in Java. It performs lemmatization, part of speech categorization, decompounding and compound disambiguation for German, Spanish, French and English, with the other major European languages under development. The lessons learned when developing the rules for disambiguation of German compounds are also applicable to other compounding languages, such as the Nordic languages. Since compounding is much more common and far more complex in German than in the other languages currently handled by LiSa, this paper will deal mainly with German. A comparative evaluation of LiSa with the GERTWOL system1, combined with a ﬁlter for disambiguation developed by Volk (Volk, 1999) has been performed. (The combination of GERTWOL and ﬁlter will from here on be referred to as Filtered GERTWOL.) The focus of the evaluation has been to measure how suitable the respective systems are for query processing and for building indices for IRsystems. Special attention has been paid to their abilities to select the correct analysis of compounds. 1http://www.lingsoft.ﬁ/doc/gertwol/  LiSa is developed by Intraﬁnd Software AG, on whose homepage an online demo of LiSa can be found2. It is used in Intraﬁnd’s iFinder and also exists as an add-on to the open source free text indexing tool Lucene3. 
This paper proposes a novel method for automatically acquiring multilingual lexica from non-parallel data and reports some initial experiments to prove the viability of the approach. Using established techniques for building mono-lingual vector spaces two independent semantic vector spaces are built from textual data. These vector spaces are related to each other using a small reference word list of manually chosen reference points taken from available bi-lingual dictionaries. Other words can then be related to these reference points ﬁrst in the one language and then in the other. In the present experiments, we apply the proposed method to comparable but non-parallel English-German data. The resulting bi-lingual lexicon is evaluated using an online EnglishGerman lexicon as gold standard. The results clearly demonstrate the viability of the proposed methodology. 
This paper explores the use of the naive Bayes classiﬁer as the basis for personalised spam ﬁlters. Several machine learning algorithms, including variants of naive Bayes, have previously been used for this purpose, but the author’s implementation using word-position-based attribute vectors gave very good results when tested on several publicly available corpora. The effects of various forms of attribute selection – removal of frequent and infrequent words, respectively, and by using mutual information – are investigated. It is also shown how n-grams, with n > 1, may be used to boost classiﬁcation performance. Finally, an efﬁcient weighting scheme for costsensitive classiﬁcation is introduced. 
Analogical modeling (AM) is a memory based model. Known algorithms implementing AM depend on investigating all combinations of matching features, which in the worst case is exponential (O(2n)). We formulate a representation theorem on analogical modeling which is used for implementing a range of approximations to AM with a much lower complexity. We will demonstrate how our model can be modiﬁed to reach better performance than the original AM model a popular categorization task (chunk tagging). 
In this paper, current dependencybased treebanks are introduced and analyzed. The methods used for building the resources, the annotation schemes applied, and the tools used (such as POS taggers, parsers and annotation software) are discussed. 
Building a large dictionary of synonyms for a language is a very tedious task. Hence there exist very few synonym dictionaries for most languages, and those that exist are generally not freely available due to the amount of work that have been put into them. The Lexin on-line dictionary1 is a very popular web-site for translations of Swedish words to about ten different languages. By letting users on this site grade automatically generated possible synonym pairs a free dictionary of Swedish synonyms has been created. The lexicon reﬂects the users intuitive deﬁnition of synonymity and the amount of work put into the project is only as much as the participants want to. Keywords: Synonyms, dictionary construction, multi-user collaboration, random indexing. 
Compounds, especially in languages where compounds are formed by concatenation without intervening whitespace between elements, pose challenges to simple text retrieval algorithms. Search queries that include compounds may not retrieve texts where elements of those compounds occur in uncompounded form; search queries that lack compounds will not retrieve texts where the salient elements are buried inside compounds. This study explores the distributional characteristics of compounds and their constituent elements using Swedish, a compounding language, as a test case. The compounds studied are taken from experimental search topics given for CLEF, the Cross-Language Evaluation Forum and their distributions are related to relevance assessments made on the collection under study and evaluated in terms of divergence from expected random distribution over documents. The observations made have direct ramiﬁcations on e.g. query analysis and term weighting approaches in information retrieval system design. 
The speech technological scene in Denmark is slowly gaining public and commercial attention, and today there is a call for more diverse products than just recognition and synthesis of standard Danish (i.e. the Copenhagen regiolect). In this working paper, we demonstrate how an existing synthesis engine may be tuned to provide a regiolect speaking voice, involving - in its simplest form - prosodic alterations only. We present the ﬁrst publicly accessible Danish synthetic voice for a non-standard regiolect - in casu Århusian. 
ReCompounder is a working prototype for automatic translation of compounds by using the web as language model and a comprehensive bilingual dictionary as translation model. Evaluation shows this strategy is viable also with current search engine technologies. In addition to a detailed overview of the system, there is a discussion of the current capabilities of the search engines used (Google and Yahoo! Search) and some tendencies relevant to lexicography. As the system currently translates from Norwegian to English, a brief introduction to compounding in Norwegian is included. 
This paper proposes modeling the semantics of natural-language calendar expressions as extended regular expressions (XREs). The approach covers expressions ranging from plain dates to such ones as the second Tuesday following Easter. The paper presents basic calendar XRE constructs, sample calendar expressions with their representations as XREs, and possible applications in reasoning and natural-language generation. 
In this paper we present SUiS - Stockholm University Information System. SUiS tries to ﬁnd answers to a predeﬁned set of question types: who, what, when and where. A domainspeciﬁc ontology is used for query expansion and translation, for answer generation, and for document analysis together with Named Entity Recognition modules for recognizing people, locations, organizations, and date expressions. Qualitative user evaluations show satisfactory results as well as indications of areas where the system could be improved. 
This paper describes the ﬁrst stage of research towards automatic recognition of brand names (trademarks, product names and service names) in Swedish economic texts. The ﬁndings of an exploratory study of brand names in economic texts by Malmgren (2004) are summarized, and the work of compiling a corpus annotated with named entities based on these ﬁndings is described. A Named Entity Recognition experiment using transformationbased learning on this data shows that what is problematic to the annotator is difﬁcult also to the recognizer; company names and brand names are closely connected and thus hard to disambiguate. 
This article describes a memory-based mechanism for anaphora resolution and coreference. A program labels the words in the text with part-ofspeech tags, functional roles, and lemma forms. This information is used for generating a representation of each anaphor and antecedent candidate. Each potential anaphorantecedent pair has a match vector calculated, which is used to select similar cases from a database of labeled cases. We are currently testing many feature combinations to ﬁnd an optimal set for the task. The most recent results show an overall F-measure (combined precision and recall) of 62, with an F-measure of 40 for those cases where anaphor and antecedent are non-identical, and 81 for identical ones. The coreference chains are restricted so that an anaphor is only allowed to link to the last item in a chain. 
In this paper we describe an information retrieval approach for mapping online business information texts to concepts in a large ontology. We adopt the traditional vector space model by representing the texts as queries and the concept labels in the ontology as documents. Because of the size of the ontology and the fact that concept labels are very sparse and generic, we conducted additional experiments for reducing the set of concepts, as well as the enrichment and enlargement of concept labels. The documents in our collection were of too poor quality for this task, and although we show that our enrichment technique did provide us with an ontology with good overall similarity to our query collection, individual concepts did not include enough terms for our method to achieve good results. 
We have investigated the impact of using phrases in the vector space model for clustering documents in Swedish in different ways. The investigation is carried out on two text sets from different domains: one set of newspaper articles and one set of medical papers. The use of phrases do not improve results relative the ordinary use of words. The results differ signiﬁcantly between the text types. This indicates that one could beneﬁt from different text representations for different domains although a fundamentally different approach probably would be needed. 
We describe a method to use a chunker for grammar checking. Once a chunker is available the method is fully unsupervised, only unannotated text is required for training. The method is very simple, compare the output of the chunker on new texts to the output on known correct text. Rare chunk sequences that occur in the new texts are reported as suspected errors. By automatically modifying the chunk set to be more detailed for common verbs or prepositions more error types can be detected. The method is evaluated on Swedish texts from a few different genres. Our method can be used without modiﬁcations on any language, as long as a chunker is available for that language. 
We present a new strategy for the creation of phonetic lexicons. As we argue, lexical resources for speech technology integration should be informed by transcriptions of spontaneous speech. We illustrate our strategy with examples from the dictionary DanPO (Danish Phonetic-Orthographic Dictionary) which is developed at the Center for Computational Modelling of Language (CMOL). For reference corpus we used DanPASS consisting of 57 recordings of task-oriented monologs, transcribed by professional and MAlevel phoneticians using the Danish SAMPA phonetic alphabet. From the transcriptions, dictionaries and concordances were compiled, and these resources were merged with the (prescriptive) phonetic renderings of a standard Danish word dictionary of 87,000 lemmata. As an effect of the “transcription informed” strategy, DanPO is expected to signiﬁcantly improve the success rate of automatic speech recognizers, as well as the naturalness of artiﬁcial voices. Furthermore, we devise an experimental strategy in order to evaluate the dictionary and further improve later versions. 
The implementation of the Scandinavian Grammar Matrix gave rise to a number of methodological and theoretical discussions about the desiderata for a formal theory of natural language grammar. In this paper, a strong hypothesis of the functionality of grammar is presented. Functionality is imposed on constraintbased grammars, on their lexicons and on the grammars themselves. It is demonstrated how this radically reduces complexity, i.e. the recognition problem is in ptime. Certain aspects of the hypothesis are motivated by methodological and psycholinguistic considerations too. The implementation employs underspeciﬁcation techniques, type inference and some amount of constructional speciﬁcation. 
The research on modeling the Estonian morphology by ﬁnite state devices has been inﬂuenced mostly by (Koskenniemi, 1983), (Lauri Karttunen and Zaenen, 1992) and (Beesley and Karttunen, 2000). We have used lexical transducer combined with twolevel rules as a general model for describing Estonian morphology. As a novel approach we can emphasize the application of the rules to the both sides of the lexical transducer – both to the lexical representation and to the lemma. In the paper the criteria of optimality of the ﬁnite-state description of a natural language morphology and the means of fulﬁlling these criteria are discussed on the example of Estonian – a language with very rich and complex morphology. Other builders of ﬁnite-state morphological transducers may proﬁt from the ideas proposed. 
This paper introduces a new linguistically motivated feature selection technique for text categorization based on morphological analysis. It will be shown that compound parts that are constituents of many (different) noun compounds throughout a text are good and general indicators of this text’s content; they are more general in meaning than the compounds they are part of, but nevertheless have good domain-speciﬁcity so that they distinguish between categories. Experiments with categorizing German newspaper texts show that this feature selection technique is superior to other popular ones, especially when dimensionality is reduced substantially. Additionally, a new compound splitting method based on compact patricia tries is introduced. 
