Annotated linguistic databases are widely used in linguistic research and in language technology development. These annotations are typically hierarchical, and represent the nested structure of syntactic and prosodic constituents. Recently, the LPath language has been proposed as a convenient path-based language for querying linguistic trees. We establish the formal expressiveness of LPath relative to the XPath family of languages. We also extend LPath to permit simple closures, resulting in a ﬁrst-order complete language which we believe is sufﬁciently expressive for the majority of linguistic tree query needs. 1. Introduction 
2. Prosody and pragmatics Before describing terminal contours and question-answer types, a brief exposition on Korean  sentence types and levels is in order.  2.1. Sentence types and sentence levels The following table shows the morphological conflation of the two dimensions, sentence type and sentence level in Korean (Chang 2001).  (1) ST \ SL Declarative Interrogative Imperative Propositive  Deferential p -(su)pnita -(su)pnikka -psio -(u)psita  Plain Familiar  a  ey  -ta -ney  -nya -na  -(e)la -key  -ca -sey  Blunt o -o -o -o -o  Intimate e -e -e -e -e  Polite yo -yo -yo -yo -yo  Three sentence levels (SLs)—blunt, intimate and polite-- are morphologically nondistinctive as to their sentence types (STs); the distinction is partially made by the terminal contour of falling (↘) or rising (↗), as shown in (2).  (2) Mia-ka wa.ss.e SM come.PST.IMT a. Mia-ka wa.ss.e ↘ b. Mia-ka wa.ss.e? ↗  % PST = PAST IMT=INTIMATE  ‘Mia came. / Mia is here.’  (declarative)  ‘Did Mia come? / Is Mia here?’ (interrogative)  2.2. Nonstandard questions and terminal contour Depending on the terminal contour (TC) and the lexical meaning of nwuka (‘who’ and ‘someone’), (3) has four readings: a statement, two standard questions (yes/no-question (ynQ) and wh-question (whQ)) and an echo question (echoQ).  (3) Nwuka  wa.ss.e  who/someone come.PST.IMT  a. nwu.ka wa.ss.e. ↘  b. nwu.ka wa.ss.e ? ↗  c. nwu.ka wa.ss.e? ↘  d. nwu.ka wa.ss.e? ↗  ‘Somebody came.’ ‘Did anybody come?’ ‘Who came?’ ‘Who did you say came?’  (statement) (ynQ) (whO) (echoQ)  2.2.1. Echo utterances An echo utterance is the speaker’s reprise or retortion of the whole or part of a previous utterance: that is, the speaker questioning the hearer’s utterance or asserting his own previous utterance.  (4) A: Mia-ka wa.ss.e ↘ (=2a) ‘Mia came./ Mia is here.’  Proceedings of PACLIC 19, the 19th Asia-Pacific Conference on Language, Information and Computation  B: Mia-ka wa.ss.ta.ko ?↗ come.PST.DEC.QM ‘Did you say Mia came?’  % DEC=DECLARATIVE QM=QUOTATIVE MARKER  In the echo question (4B) the quotative connective ko (colloquially, kwu) marks A’s utterance (4A) as an embedded declarative sentence linked to the unexpressed predicate such as malhay.ss.nya? ‘did you say?’. In the dialogue exchange (5), the speaker repeats his earlier utterance in the form of echo statement (5c). (5d) is another type of echo questions, called ‘incredulity question’ (incr-Q) (Jun and Oh 1996).  (5) a. A: Mia-ka wa.ss.e ↗  ‘Did Mia come?’  b. B: Mia-ka wa.ss.nya.ko ↗  ‘Did Mia come—did you ask?  c. A: Ung. Mia-ka wa.ss.nya.ko ↘ ‘Yeah, did Mia come? I asked.‘  d. Mia-ka wa.ss.e↗↗  ‘Did you say Mia came? It’s surprising/unbelievable/….’  2.2.2. Tag questions A typical Korean tag question is what I call ‘negative tag question (neg-tagQ)’,1 which is in the form: an ‘not’ + ha? ‘do/be’, analogous to the English isn’t it? or the French n’est ce pas?  (6)  [S …(tense) ci] an.ha?  |  |  PACK TAG  …, isn’t it?  (English)  …, n’est ce pas? (French)  It has two terminal contours, rising (↗) and falling (↘), each distinct in speech-act meaning. The tag question with a falling contour generally shows the speaker’s attitude of seeking a confirmation from the hearer and the one with a rising contour is seeking an agreement. For ease of reference they are called here: confirmative tag question (conf-tagQ) and agreeing tag question (agr-tagQ), respectively. They are biased questions in that the speaker is not simply asking a question but (s)he is biased in favor of the proposition expressed in the pack.  (7) Mia-ka wa. ss. ci an. h.a ↗ (or ↘) come.PST.SUP.NEG.be/do.IMT  ‘Mia came, didn’t she ↗ (or ↘)’  (8) Mia-ka o.ci  an.ha.ss.e ↗  come.SUP NEG.be/do.PST.IMT  ‘Didn’t Mia come?’ (SUP = suppositive)  A constraint on the two types of interrogatives, negative tag question (neg-tagQ) and negative  
Human interpretation of natural language relies heavily on cognitive processes involving metaphorical and idiomatic meanings. One area of computational linguistics in which such processes play an important, but largely unaddressed, role is the determination of the properties of multiword predicates (MWPs). MWPs such as give a groan and cut taxes involve metaphorical meaning extensions of highly frequent, and highly polysemous, verbs. Tools for automatically identifying such MWPs, and extracting their lexical and syntactic properties, are crucial to the adequate treatment of text in a computational system, due to the productive nature of MWPs across many languages. This paper gives an overview of our work addressing these issues. We begin by relating linguistic properties of metaphorical uses of verbs to their distributional properties. We devise automatic methods for assessing whether a verb phrase is literal, metaphorical, or idiomatic. Since metaphorical MWPs are generally semi-productive, we also develop computational measures of their individual acceptability and of their productivity over semantically related combinations. Our results demonstrate that combining statistical approaches with linguistic information is beneﬁcial, both for the acquisition of knowledge about metaphorical and idiomatic MWPs, and for the organization of such knowledge in a computational lexicon.  1. Metaphorical Multiword Predicates Metaphor is a powerful aspect of language, enabling creative expression in terms of familiar concepts, usually ones which are easily visualizable (Lakoff and Johnson, 1980; Johnson, 1987; Nunberg et al., 1994). Indeed, metaphor is such a central part of linguistic competence that many terms, especially multiword expressions, that are currently accepted as “regular” language have their origin in metaphorical uses (Newman, 1996). Some of these expressions are viewed as meaning extensions of their component words, which at least partly contribute their semantics, or a ﬁgurative version of their semantics. Others have become idioms with idiosyncratic semantics whose relation to their component words is not obvious (except possibly historically). In particular, it is common across languages for multiword predicates (MWPs) to form around certain high frequency verbs that easily undergo a process of metaphorization (Pauwels, 2000; Newman and Rice, 2004). In their literal uses, these so-called “basic” verbs typically refer to states or acts that are central to human experience (e.g., cut, give, put, sit). Their metaphorical uses yield a range of meaning extensions, exhibited in MWPs such as those in 1(a–d):  1. (a) cut taxes, cut in line, cut a dash (b) give permission, give a toss, give a groan, give sb. their due (c) put sth. to rest, put one’s ﬁnger (on), put a gloss (on) (d) sit in judgment, sit on sth., sit tight, sit on the fence As seen in the examples above, a wide variety of MWPs result from basic verbs in combination with various types of complements, lying along a continuum of less to more metaphorical usage, culminating in idiomatic expressions. Multiword predicates of this type are widespread in many different languages, including, but not limited to, English, German, French, Spanish, Persian, Urdu, Hindi, Chinese, and Japanese (Seaton and Macaulay, 2002; Kearns, 2002; Fellbaum, 2002; Desbiens and Simon, 2003; AlbaSalas, 2002; Karimi, 1997; Butt, 2003; Lin, 2001; Butt and Scott, 2002; Miyamoto, 2000). Such MWPs contrast with verb-particle constructions (e.g., give up and ﬁgure out), which form a restricted set of MWPs that are common in English but not as widely attested crosslinguistically. Nonetheless, MWPs with basic verbs have been granted relatively little attention in the computational linguistics community. Consequently, fundamental issues about such expressions are only beginning to be addressed, such as: automatic extraction and acquisition of relevant properties; adequate representation in a computational lexicon; and appropriate treatment in various natural language processing tasks. Because expressions with basic verbs fall on a continuum of literal to metaphorical to idiomatic, they cannot be treated uniformly in computational systems. Metaphorical expressions vary in the degree to which the meaning of the basic verb differs from its literal semantics, and even expressions considered idiomatic can vary in their level of semantic transparency. Such expressions also exhibit varying levels of tolerance for morphosyntactic variations, so that their appropriate syntactic treatment is also not uniform. We address these issues through a careful linguistic analysis of the properties of MWPs and their relation to statistical behaviour, to support the automatic acquisition of semantic and syntactic knowledge about MWPs. 2. Automatic Lexical Acquisition of Properties of MWPs In this article, we give an overview of our research on the automatic acquisition of semantic and syntactic properties of English MWPs involving a basic verb. For simplicity, we use the general term MWP to refer to such expressions in the remainder of the paper. Because many MWPs are comprised of the verb and a noun in its direct object position (Cowie et al., 1983; Nunberg et al., 1994), we particularly focus on verb+noun combinations, or VNCs for short. In our recent body of work, we have addressed two overall problems in lexical acquisition of the properties of VNCs: ﬁrst, of determining where on the continuum of literal to metaphorical to idiomatic a particular VNC lies; and second, of inferring the possible complements a basic verb can combine with to form an acceptable MWP. Recognizing VNCs as literal, metaphorical, or idiomatic is complicated by the fact that nonliteral VNCs conform to the grammar rules for verb phrases. Hence they are indistinguishable on the surface from regular compositional verb phrases. For example, the three expressions take a lunch, take a powder, and take a walk appear similar at ﬁrst glance, but a closer look at their semantics reveals signiﬁcant differences. Each of these three represents a different class of expressions using a basic verb. Take a lunch is a compositional combination of a verb and  Proceedings of PACLIC 19, the 19th Asia-Pacific Conference on Language, Information and Computation a noun, both contributing their literal semantics. Take a powder has an idiomatic meaning (“to leave abruptly”) that has nothing to do with either take or powder. Take a walk stands somewhere between the idiomatic and compositional expressions. It is not completely idiomatic because the noun constituent determines the primary meaning of the expression—that is, take a walk can be roughly paraphrased by the verb walk. However, it is not fully compositional either, since the verb constituent does not contribute its literal meaning, but rather a metaphorical extension of it. Because the verb is “semantically bleached” in such expressions (Butt, 2003), it is referred to as a light verb. Take a walk is one of a general class of VNCs known as light verb constructions (LVCs). It is evident that the distinction between idiomatic and literal expressions is essential to NLP applications that require some degree of semantic interpretation. A machine translation system, for example, should translate the idiom kick the bucket as a single unit of meaning, while this is not the case for the literal phrase kick the pail. It is thus necessary to develop means for automatically distinguishing idiomatic VNCs from literal ones. This distinction is also important for LVCs. For example, give a groan is translated to the French verb ge´mir (“to groan”), while the literal give a present has a word-for-word translation. However, LVCs differ from idiomatic phrases in that, in most cases, their semantics (and hence their translation) is more predictable. Speciﬁcally, the meaning of an LVC typically corresponds to a verb related to the noun complement, as with the French translation ge´mir related to the English noun complement groan. In Section 3, we describe our work on the ﬁrst step in the appropriate handling of VNCs: to automatically distinguish expressions that are literal, metaphorical (as in LVCs), and idiomatic. The second issue regarding lexical acquisition of VNCs concerns the determination of which complements are semantically compatible with a basic verb in forming a multiword predicate. While idiomatic combinations are by deﬁnition semantically idiosyncratic, LVCs show a greater degree of compositionality and hence also semantic predictability. However, it is still extremely difﬁcult to determine which nouns can combine with a given light verb to form an LVC. This depends on the semantic properties of both the noun and the light verb; for example, one can take a walk and give a groan, but it is less natural to ?take a groan or ?give a walk. Interestingly, light verbs often tend to combine with semantically similar complements to form families of semantically-related LVCs (Wierzbicka, 1982; Nunberg et al., 1994; Sag et al., 2002). For example, one can take a walk, take a stroll, or take a run; similarly it is acceptable to give a groan, give a smile, and give a wink. It is important in a computational system to recognize the allowable patterns of combination for a light verb, so that previously unseen LVCs can be handled appropriately. In Section 4, we describe our two-pronged approach to this problem, which involves computational measures for the individual acceptability of potential LVCs, as well as for the assessment of productivity over a class of semantically related potential complements. 3. Literal, Metaphorical, or Idiomatic VNCs Here we summarize our work on the determination of whether a VNC is literal, metaphorical, or idiomatic. We have thus far broken down the problem into two parts. First, we address the endpoints on the scale of metaphoricity by distinguishing idiomatic expressions from literal ones (Section 3.1). Then we tackle the distinction of degree of metaphoricity of a potential LVC, which also helps to distinguish LVCs from literal expressions (Section 3.2). Our current work focuses on combining these approaches into one measure that places a VNC on a scale of literal to metaphorical to idiomatic.  3.1. Literal vs. Idiomatic Compared to literal expressions, idiomatic VNCs are notable for their semantic idiosyncrasy. For example, kick the bucket has a meaning (“to die”) that is relevant neither to the independent meaning of kick nor to that of bucket (Cacciari, 1993; Sag et al., 2002). Semantic idiosyncrasy is a matter of degree, however: the idiom spill the beans (“to reveal a secret”) is often argued to be less idiomatic than kick the bucket, because it can be analyzed as spill corresponding to “reveal” and beans referring to “secret(s)”. Such idioms are referred to as semantically analyzable.1 There is evidence in the linguistic literature that the idiosyncrasy of idiomatic combinations is not limited to their semantics, but extends to their lexical and/or syntactic behaviour. Idiomatic VNCs are known to be lexically ﬁxed (non-productive) to a large extent (Gibbs, 1993; Glucksberg, 1993). Neither kick the pail nor hit the bucket have meanings related to that of kick the bucket. Similarly, while spill the beans has an idiomatic interpretation, spill the peas and spread the beans are literal phrases. Interestingly, when an idiomatic VNC does show some (limited) lexical productivity, it is typically with respect to the verb constituent—that is, in some cases, a few semantically similar basic verbs can be used in a particular idiom (e.g., keep one’s cool, lose one’s cool). Idiomatic combinations are also syntactically ﬁxed to some extent, i.e., they typically cannot appear in syntactic variations while at the same time retaining their idiomatic interpretations (Stock et al., 1993; Fellbaum, 1993; Nunberg et al., 1994). The idiom kick the bucket, for example, is generally unacceptable in other syntactic forms: 2. (a) John kicked the bucket. (b) ?? John kicked a bucket. (c) ?? John kicked the buckets. (d) ?? John kicked the fast bucket. (e) ?? The bucket was kicked by John. Syntactic ﬂexibility of an idiom is argued to be strongly related to its semantic analyzability. The idiom spill the beans, as noted above, is considered to be semantically more transparent, and is correspondingly more ﬂexible: 3. (a) John spilled the beans. (b) ? John spilled some beans. (c) ?? John spilled the bean. (d) John spilled the ofﬁcial beans. (e) The beans were spilled by John. As sentences in 2 and 3 show, some idioms (e.g., those that are semantically analyzable) may be syntactically more ﬂexible than others. Nonetheless, in general they are expected to appear in restricted syntactic constructions. The lexical and syntactic ﬁxedness of idiomatic VNCs contrasts with the behaviour of regular compositional verb phrases that tend to be more productive and appear in a wider range 1Semantic analyzability is also referred to as decomposability or compositionality in the linguistic literature.  Proceedings of PACLIC 19, the 19th Asia-Pacific Conference on Language, Information and Computation of syntactic constructions. Examining the degree of ﬁxedness of a verb+noun combination can thus be regarded as a way of determining its idiomaticity, i.e., the degree to which it is idiomatic. In our recent work on idioms (Fazly and Stevenson, 2005), we propose statistical measures for quantifying the lexical, syntactic, and overall ﬁxedness of a VNC. Each measure brings together aspects of the above-mentioned linguistic properties of idioms and their distributional behaviour in a corpus. We assume that a target VNC is lexically ﬁxed, and hence idiomatic, if it is not open to lexical substitution. This means that we expect to see a notable difference between the idiomaticity level of the target VNC and that of the variants generated by replacing one of its constituents (the verb or noun) with a semantically (and syntactically) similar word. The paradox of this assumption is that it requires knowledge about the idiomaticity of the variant combinations. Inspired by Lin (1999), we moderate this assumption by examining the association strengths of the target combination and its variants, as an indirect cue to their idiomaticity. Target VNCs that are signiﬁcantly greater in association strength than their variants are assumed to be more lexically ﬁxed, and therefore more idiomatic. Our contribution is a novel technique for incorporating these association strengths into a single measure of lexical ﬁxedness for the target expression. We assume a target VNC is syntactically ﬁxed, and hence idiomatic, if it mainly appears in restricted syntactic constructions. We thus extract, from the linguistic literature, a set of syntactic patterns expected to capture the difference in behaviour of idiomatic and literal VNCs (Nunberg et al., 1994; Fellbaum, 1993). Syntactic ﬁxedness of a VNC is measured by comparing its probability distribution over the selected set of patterns, to that of a “typical” verb+noun combination. The more the distributional behaviour of the target VNC deviates from that of the typical VNC, the more likely it is to be idiomatic. We evaluate our ﬁxedness measures by applying them to the task of separating idiomatic from literal combinations, and comparing their performance to that of a collocation-based measure, pointwise mutual information (PMI) (Church et al., 1991). We ﬁnd in our results that both the lexical and syntactic ﬁxedness measures have good performance. While the lexical ﬁxedness measure works comparably to PMI, the syntactic ﬁxedness measure substantially outperforms both. Moreover, we show that combining the lexical and syntactic ﬁxedness measures into a measure of overall ﬁxedness results in a notable gain in performance. Our measures are also less sensitive to the frequency of the expressions than PMI; this is important since many idioms have low frequency of occurrence in traditional corpora. 3.2. Literal vs. Metaphorical Compared to idiomatic VNCs, light verb constructions are more semantically analyzable, and thus lie between idioms and literal phrases on the continuum of metaphoricity. In an LVC, the noun constituent generally contributes its literal meaning to the expression, while the light verb contributes a more or less metaphorical meaning. A light verb can also be used in a literal expression with its core meaning. Hence the challenge is to determine the level of metaphoricity of a use of a light verb. For example, give in give sb. a present has a literal meaning, i.e., “transfer of possession” of a THING to a RECIPIENT. In an LVC such as give a speech, give has a metaphorical meaning, while at the same time keeping aspects of its literal semantics: an abstract entity (a speech) is “transferred” to the audience, but no “possession” is involved. In other LVCs such as give a groan, give has a highly metaphorical meaning: here the notions of  “transfer” and “possession” are almost completely diminished. As with idiomatic VNCs, syntactic ﬁxedness plays an important role in identifying degree of semantic analyzability. The noun complement of a literal use of a light verb (e.g., present in give sb. a present) acts as a direct object and hence can move around freely. LVCs whose noun constituent can be treated, possibly metaphorically, as the direct object of the light verb also exhibit syntactic ﬂexibility to a large extent. In these, the noun may be introduced by a deﬁnite article, pluralized, passivized, or relativized, as in 4(b–e): 4. (a) Azin gave a speech to a few students. (b) Azin gave the speech just now. (c) Azin gave a couple of speeches last night. (d) A speech was given by Azin just now. (e) The speech that Azin gave was brilliant. In contrast, LVCs involving highly metaphorical uses of the light verb enforce certain restrictions on the syntactic freedom of their noun constituents (Kearns, 2002), as in 5(b–e): 5. (a) Azin gave a groan just now. (b) ?? Azin gave the groan just now. (c) ? Azin gave a couple of groans last night. (d) ?? A groan was given by Azin just now. (e) ?? The groan that Azin gave was very long. In general, the degree to which the light verb retains aspects of its literal meaning—and contributes them compositionally to the LVC—is reﬂected in the degree of freedom exhibited by the noun component. We have proposed a statistical measure that uses this insight to situate an LV+N combination (a potential LVC) on a scale of literal to metaphorical usage of the light verb (metaphoricity continuum) (Fazly et al., 2005). Our measure assigns a score to each potential LVC, reﬂecting its syntactic ﬁxedness as determined by the degree of freedom of the noun component. This is approximated as the difference between the strength of association of the potential LVC with two types of syntactic patterns: those that are preferred by LVCs (as in 5(a)), and those that are less preferred by more metaphorical LVCs (as in 5(b–e)). We evaluate our measure of metaphoricity by comparing the ratings it assigns to a set of candidate LV+N combinations with those assigned by human judges. Using the Spearman rank correlation coefﬁcient, we show that the ratings of our measure achieve signiﬁcant high correlations with the human judgments. Moreover, when the measure is applied to a restricted subset of LVCs, i.e., those of the form “LV a/an N”, the correlation scores are particularly high. Since the more metaphorical LVCs are similar in some respects to idioms, we also compare our results to correlations attained by PMI, as a simple measure of collocation. Our measure outperforms PMI (on all the data, as well as on the restricted subset), indicating that degree of metaphoricity cannot be simply treated as degree of collocation.  Proceedings of PACLIC 19, the 19th Asia-Pacific Conference on Language, Information and Computation 4. Semantic Patterns in MWP Formation Here we give an overview of our work on determining the semantic class of complements that can combine with a basic verb to form multiword predicates. Because LVCs (in contrast to idiomatic expressions) exhibit some predictability in terms of the allowable complements of a light verb, we have focused on this subclass of MWPs in our research on this topic. As mentioned above, we have developed computational measures for addressing two related aspects of this problem: individual acceptability of LV+N combinations, and assessment of productivity of a light verb with respect to a semantic class of potential complements. 4.1. Individual LVC Acceptability First, we focus on the individual acceptability of potential LVCs. Although light verbs tend to have similar patterns of cooccurrence with semantically similar complements, they fail to exhibit full generality in their combinations with a semantic class of nouns. Hence, deciding precisely which light verbs combine with which nouns to form acceptable LVCs is a difﬁcult task. Our ﬁrst solution to this problem treated LV+N combinations as syntactically-dependent collocations, using a PMI-based measure (Stevenson et al., 2004). This measure outperformed the standard PMI measure because it incorporated some knowledge of the preferred LVC pattern (cf. the examples in 5 above). However, while common LVCs typically appear as good collocations, our collocation-based measure failed to take into account other important properties of LVCs. We have devised an improved acceptability measure that brings together some of the linguistic properties of LVCs into a probability formula to determine the likelihood of a given light verb and noun forming an acceptable LVC (Fazly et al., 2005, 2006). The measure captures the general tendency of the noun to form LVCs with any light verb, as well as its speciﬁc inclination towards the particular light verb. We evaluate this measure by comparing its ratings on a set of expressions formed from combining candidate light verbs and nouns, with human judgments of acceptability on these LV+N combinations. Using the Spearman rank correlation coefﬁcient, we show that our probabilistic acceptability measure generally achieves good (and signiﬁcant) correlations with the human judgments. Moreover, we compare this measure to our earlier PMI-based measure, as well as to standard PMI. We ﬁnd that our new measure performs better and more consistently across expressions formed from different light verbs and candidate nouns from different semantic classes. 4.2. Class-based LVC Acceptability Our probabilistic measure achieves good performance in determining the level of acceptability of an LV+N combination. Still, a further goal is to devise statistical indicators of the productivity of LVC formation over a class of semantically related nouns with a given light verb. This is required for the adequate treatment of LV+N combinations in a computational system. Knowledge about the collective tendency of a semantic class in forming LVCs with a given light verb can be extended to unattested, semantically similar nouns. For example, if the class of sound emission nouns (e.g., groan, moan) is known to productively form LVCs with give, the assessed acceptability of an unseen or low frequency LVC such as give a rasp should be promoted. Moreover, each group of semantically similar nouns combining with a particular light verb is often deemed to distinguish a possible meaning extension for the light verb (Newman, 1996).  For example, in give advice, give orders, give a speech, etc., give contributes a notion of “abstract transfer”, while in give a groan, give a moan, give a rasp, etc., give contributes a notion of “emission”. Capturing the class-based tendency of light verbs in forming LVCs is thus essential to reﬁning the semantic space of the metaphorical usages of these (and other) basic verbs, which are all highly polysemous (Fazly et al., 2005). We examine the class-based tendency of LVC formation by focusing on potential LVCs formed from the combination of candidate light verbs with nouns from a number of selected semantic classes (Fazly et al., 2006). We draw on two different classiﬁcations, that of Levin (1993), and that of WordNet (Fellbaum, 1998). We deﬁne the productivity level of each semantic class with respect to a particular light verb to be the proportion of class members that form acceptable LVCs with the light verb. To extend our acceptability measure for assessing the productivity of a class, we set a threshold on the ratings assigned by the measure; LV+N combinations with ratings higher than the threshold are considered to be acceptable LVCs. A good acceptability measure should accurately predict the individual acceptability level of an LV+N combination, as well as the collective acceptability (productivity level) of a semantic class with respect to a particular light verb. We compare, for each semantic class and light verb, the proportion of nouns that form acceptable LVCs according to our probabilistic measure (see Section 4.1), to the same ﬁgure as determined by the human judges. The divergence of the former from the latter is estimated by calculating the sum of squared errors between the two sets of numbers, averaged across all light verbs and semantic classes. The results show that our linguistically-motivated measure of acceptability has smaller divergence when compared with the collocation-based PMI measures. 5. General Discussion Recently there has been a growing understanding both of the need for the appropriate handling of multiword expressions, and of the complexities involved in the task (Sag et al., 2002). Most research, however, has concentrated on the automatic extraction of these expressions (Grefenstette and Teufel, 1995; Dras and Johnson, 1996; Melamed, 1997; Baldwin and Villavicencio, 2002; Seretan et al., 2003; Moiro´n, 2004). Previous studies that focus on learning about semantic properties of multiword expressions, such as their compositionality, have mainly covered compound nouns (Wermter and Hahn, 2005), and verb-particle constructions (McCarthy et al., 2003; Bannard et al., 2003; Baldwin et al., 2003). Our work differs in focusing on those multiword predicates (MWPs) that involve a “basic” verb, a broadly-documented class of expressions that has received relatively little attention within the computational linguistics community. Most previous work on compositionality of multiword expressions either treats them as collocations (Smadja, 1993), or examines the distributional similarity between an expression and its constituents (McCarthy et al., 2003; Baldwin et al., 2003; Bannard et al., 2003). Lin (1999) and Wermter and Hahn (2005) go one step further and look into a linguistic property of non-compositional compounds—their lexical ﬁxedness—to identify them. Venkatapathy and Joshi (2005) combine aspects of the above-mentioned work by incorporating lexical ﬁxedness, collocation-based, and distributional similarity measures into a set of features which are used to rank verb+noun combinations (VNCs) according to their compositionality. Our work differs from such approaches in presenting an alternative view on compositionality. More speciﬁcally, we carefully examine several linguistic properties of metaphorical and idiomatic MWPs, those that distinguish them from literal (compositional) combinations. These  Proceedings of PACLIC 19, the 19th Asia-Pacific Conference on Language, Information and Computation include characteristics related not only to lexical ﬁxedness, but to syntactic ﬁxedness as well. Widdows and Dorow (2005) also draw on the notion of syntactic ﬁxedness for idiom detection, though their method is speciﬁc to a highly constrained type of idiom. Our work examines a broader range of syntactic patterns associated with a large class of MWPs. We then suggest novel techniques for translating these lexical and syntactic characteristics into measures that predict the level of metaphoricity or idiomaticity of MWPs. Work indicating acceptability of multiword expressions is largely limited to collocational analysis using PMI-based measures (Lin, 1999; Stevenson et al., 2004). In our recent work, we have proposed a linguistically-motivated acceptability measure for light verb constructions that enables ﬂexible integration of LVC-speciﬁc properties. In addition to distinguishing literal from metaphorical usages of a light verb, we aim to determine ﬁner-grained distinctions among the identiﬁed metaphorical usages. In most cases, the ﬁner-grained distinctions appear to relate to the semantic properties of the complement that combines with the light verb. Not only does a light verb tend to combine with semantically similar complements, it tends to contribute similar metaphorical meaning to the resulting LVC. Semantic class knowledge may thus enable us to further reﬁne the semantic space of a light verb by elucidating its relation with complements from different classes. Wanner (2004) attempts to perform a similar task by classifying VNCs into predeﬁned groups, each corresponding to a particular semantic relation between the verb and the noun. However, his approach requires manually labelled training data. Villavicencio (2003) uses class-based knowledge to extend a lexicon of verb-particle constructions, but assumes that an unobserved expression is not acceptable. We instead propose that more robust application of class-based knowledge can be achieved with a better estimate of the acceptability of various expressions. While we focus on light verb constructions, we believe that similar techniques can be useful in dealing with other semi-productive MWPs, such as verb-particle constructions. The signiﬁcance of the role metaphor plays in language has long been recognized. However, due to the peculiarities in the behaviour of metaphorical and idiomatic expressions, they have been mostly overlooked by researchers in computational linguistics. Previous studies recognize the challenges these constructions impose on NLP systems (see, e.g., Fellbaum, 2005), but often lack proposals for robust and wide-coverage mechanisms to handle them. Some work has relied on the existence of expensive resources such as manually-built knowledge bases (Fass, 1991; Villavicencio et al., 2004). Mason (2004) incorporates automatically-induced knowledge about the domain of use of a verb to help identify different metaphorical meanings. However, highly polysemous verbs cannot be easily associated with particular domains. Hence such an approach overlooks the great potential of basic verbs in forming metaphorical and idiomatic MWPs. Our work demonstrates that combining statistical approaches with linguistic information is beneﬁcial in devising reliable techniques, both for the acquisition of knowledge about metaphorical and idiomatic MWPs, and for the organization of such knowledge in a computational lexicon. Given the crosslinguistic prominence of MWPs, our future work aims to extend these techniques to similar constructions in languages other than English. Moreover, while we have focused here on LVCs and idiomatic VNCs, we believe that similar techniques can be useful in dealing with other MWPs, and possibly other types of multiword expressions in general.  6. Acknowledgements Thanks especially to our colleague, Ryan North, for his contributions toward a fruitful and enjoyable collaboration. We are also grateful to the rest of the computational linguistics research group at the University of Toronto for helpful comments and discussion on this and earlier papers on this topic. This work is supported by the Natural Sciences and Engineering Research Council of Canada (NSERC) under a grant to the second author, and by Ontario Graduate Scholarships to the ﬁrst author. 7. References Alba-Salas, J. (2002). Light Verb Constructions in Romance: A Syntactic Analysis. PhD thesis, Cornell University. Baldwin, T., Bannard, C., Tanaka, T., and Widdows, D. (2003). An empirical model of multiword expression decomposability. In Proceedings of the ACL-SIGLEX Workshop on Multiword Expressions: Analysis, Acquisition and Treatment, pages 89–96. Baldwin, T. and Villavicencio, A. (2002). Extracting the unextractable: A case study on verbparticles. In Proceedings of CoNLL’02, pages 98–104, Taipei, Taiwan. Bannard, C., Baldwin, T., and Lascarides, A. (2003). A statistical approach to the semantics of verb-particles. In Proceedings of the ACL-SIGLEX Workshop on Multiword Expressions: Analysis, Acquisition and Treatment, pages 65–72, Sapporo, Japan. Butt, M. (2003). The light verb jungle. Workshop on Multi-Verb Constructions. Butt, M. and Scott, B. (2002). Chinese directionals. Talk held as part of the Workshop on Complex Predicates, Particles and Subevents. Cacciari, C. (1993). The place of idioms in a literal and metaphorical world. In Cacciari and Tabossi (1993), pages 27–53. Cacciari, C. and Tabossi, P., editors (1993). Idioms: Processing, Structure, and Interpretation. Lawrence Erlbaum Associates, Publishers. Church, K., Gale, W., Hanks, P., and Hindle, D. (1991). Using statistics in lexical analysis. In Zernik, U., editor, Lexical Acquisition: Exploiting On-Line Resources to Build a Lexicon, pages 115–164. Lawrence Erlbaum. Cowie, A. P., Macking, P., and McCaig, I. R. (1983). Oxford Dictionary of Current Idiomatic English, volume 2. Oxford University Press. Desbiens, M. C. and Simon, M. (2003). De´terminants et locutions verbales. Manuscript. Dras, M. and Johnson, M. (1996). Death and lightness: Using a demographic model to ﬁnd support verbs. In Proceedings of the 5th International Conference on the Cognitive Science of Natural Language Processing. Fass, D. (1991). met*: A method for discriminating metonymy and metaphor by computer. Computational Linguistics, 17(1):49–90. Fazly, A., North, R., and Stevenson, S. (2005). Automatically distinguishing literal and ﬁgurative usages of highly polysemous verbs. In Proceedings of the ACL’05 Workshop on Deep Lexical Acquisition, pages 38–47, Ann Arbor, USA. Fazly, A., North, R., and Stevenson, S. (2006). Automatically determining allowable combinations of a class of ﬂexible multiword expressions. Submitted. Fazly, A. and Stevenson, S. (2005). Automatically constructing a lexicon of verb phrase idioms. In preparation.  Proceedings of PACLIC 19, the 19th Asia-Pacific Conference on Language, Information and Computation Fellbaum, C. (1993). The determiner in English idioms. In Cacciari and Tabossi (1993), pages 271–.395. Fellbaum, C., editor (1998). WordNet, An Electronic Lexical Database. MIT Press. Fellbaum, C. (2002). VP idioms in the lexicon: Topics for research using a very large cor- pus. In Busemann, S., editor, Proceedings of the KONVENS 2002 Conference, Saarbruecken, Germany. Fellbaum, C. (2005). The ontological loneliness of verb phrase idioms. In Schalley, A. and Zaefferer, D., editors, Ontolinguistics. Mouton de Gruyter. Forthcomming. Gibbs, Jr., R. W. (1993). Why idioms are not dead metaphors. In Cacciari and Tabossi (1993), pages 57–77. Glucksberg, S. (1993). Idiom meanings and allusional content. In Cacciari and Tabossi (1993), pages 3–.36. Grefenstette, G. and Teufel, S. (1995). Corpus-based method for automatic identiﬁcation of support verbs for nominalization. In Proceedings of the 7th Meeting of the European Chapter of the Association for Computational Linguistics (EACL’95). Johnson, M. (1987). The body in the mind: The bodily basis of meaning, imagination, and reason. The University of Chicago Press. Karimi, S. (1997). Persian complex verbs: Idiomatic or compositional? Lexicology, 3(1):273– 318. Kearns, K. (2002). Light verbs in English. Manuscript. Lakoff, G. and Johnson, M. (1980). Metaphors we live by. The University of Chicago Press. Levin, B. (1993). English Verb Classes and Alternations: A Preliminary Investigation. The University of Chicago Press. Lin, D. (1999). Automatic identiﬁcation of non-compositional phrases. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL’99), pages 317– 324, Maryland, USA. Lin, T.-H. (2001). Light Verb Syntax and the Theory of Phrase Structure. PhD thesis, University of California, Irvine. Mason, Z. J. (2004). CorMet: A computational, corpus-based conventional metaphor extraction system. Computational Linguistics, 30(1):23–44. McCarthy, D., Keller, B., and Carroll, J. (2003). Detecting a continuum of compositionality in phrasal verbs. In Proceedings of the ACL-SIGLEX Workshop on Multiword Expressions: Analysis, Acquisition and Treatment, Sapporo, Japan. Melamed, I. D. (1997). Automatic discovery of non-compositional compounds in parallel data. In Proceedings of the 2nd Conference on Empirical Methods for Natural Language Processing (EMNLP’97), Providence, USA. Miyamoto, T. (2000). The Light Verb Construction in Japanese: the Role of the Verbal Noun. John Benjamins. Moiro´n, M. B. V. (2004). Discarding noise in an automatically acquired lexicon of support verb constructions. In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC 2004), Lisbon, Portugal. Newman, J. (1996). Give: A Cognitive Linguistic Study. Mouton de Gruyter. Newman, J. and Rice, S. (2004). Patterns of usage for English SIT, STAND, and LIE: A cognitively inspired exploration in corpus linguistics. Cognitive Linguistics, 15(3):351–396. Nunberg, G., Sag, I. A., and Wasow, T. (1994). Idioms. Language, 70(3):491–538.  Pauwels, P. (2000). Put, Set, Lay and Place: A Cognitive Linguistic Approach to Verbal Meaning. LINCOM EUROPA. Sag, I. A., Baldwin, T., Bond, F., Copestake, A., and Flickinger, D. (2002). Multiword expressions: A pain in the neck for NLP. In Proceedings of the 3rd International Conference on Intelligent Text Processing and Computational Linguistics (CICLING’02), pages 1–15, Mexico City. Seaton, M. and Macaulay, A., editors (2002). Collins COBUILD Idioms Dictionary. HarperCollins Publishers, second edition. Seretan, V., Nerima, L., and Wehrli, E. (2003). Extraction of multi-word collocations using syntactic bigram composition. In Proceedings of the International Conference RANLP’03, Bulgaria. Smadja, F. (1993). Retrieving collocations from text: Xtract. Computational Linguistics, 19(1):143–177. Stevenson, S., Fazly, A., and North, R. (2004). Statistical measures of the semi-productivity of light verb constructions. In Proceedings of the ACL’04 Workshop on Multiword Expressions: Integrating Processing, pages 1–8, Barcelona, Spain. Stock, O., Slack, J., and Ortony, A. (1993). Building castles in the air. some computational and theoretical issues in idiom comprehension. In Cacciari and Tabossi (1993), pages 229–.347. Venkatapathy, S. and Joshi, A. (2005). Measuring the relative compositionality of verb-noun (V-N) collocations by integrating features. In Proceedings of HLT-EMNLP’05, pages 899– 906. Villavicencio, A. (2003). Verb-particle constructions and lexical resources. In Proceedings of the ACL-SIGLEX Workshop on Multiword Expressions: Analysis, Acquisition and Treatment, pages 57–64, Sapporo, Japan. Villavicencio, A., Baldwin, T., and Waldron, B. (2004). A multilingual database of idioms. In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC’04), pages 1127–30, Lisbon, Portugal. 
 Table 1: State of the Union Speeches included in current corpora  Name  Year  Number Political Party Word  Count  Truman  1945-1951 7  Democrat  Eisenhower 1953-1960 8  Republican  Kennedy 1961-1963 3  Democrat  Johnson  1963-1969 8*  Democrat  Nixon  1970-1974 5  Republican  Ford  1975-1977 3  Republican  Carter  1978-1980 3  Democrat  Reagan  1981-1988 8  Republican  G.H.W.  1989-1992 5*  Republican  Bush  Clinton  1993-2000 8  Democrat  G. W. Bush 2001-2005 6*  Republican  Total  64  *Presidents gave more than on SOU in a given year.  53,066 54,145 13,970 33,463 19,567 13,867 11,298 36,822 20,668 60,751 27,437 345,054  Avg.  #  Words/Spee  ch  7581  6768  4657  4182  3913  4622  3766  4603  4134  7594 4573 5391  Since some presidents gave more than one SOU address in a given year (i.e. Bush gave two addresses in 2001, one on 2/27 and one on 9/20), there are a total of 64 speeches in this corpora.1 Clinton, is, as noted by many pundits, the most prolix speaker on in terms of total number of words (Figure 1), although Truman is a close second, and Eisenhower is  
A new framework for a system that aids online volunteer translators is proposed. As regards this proposal, first, the current status and conditions of online volunteer translators and their translation environments are examined, and general requirements for a system that would aid these translators are given. Our proposed approach for dealing with heterogeneous data, which involves providing a new XML structure that we have developed for maximizing efficiency and functionalities, is then described. 1. Introduction For many years, specialists and researchers have become pessimistic in regards to the prospects for fully automatic translation capable of producing high quality translations equal to human translator. It is well known that the ALPAC report in 1966 evaluated the performances of Machine Translation (MT) systems negatively. Martin Kay stated: “...this happens when the attempt is made to mechanize the non-mechanical or something whose mechanistic substructure science has not yet been revealed...” [6] This situation has promoted a shift in emphasis of research from fully automated machine translation to computer aided human translation, which exploits the potential of computers to support human skills and intelligence [5]. Many industries have made a large investment in developing useful translation-aid tools, which has resulted in commercial Computer-Aided Translation (CAT) systems such as Translation Memory (TM) in various forms, dictionaries and terminology database techniques. However, these systems are not designed to be used by all translators. On the one hand, the commercial feature is a barrier for many translators. On the other hand, these tools do not provide content and functions that fully satisfy some translators. Online volunteer translators, to whom we specifically address our system, are among those excluded from commercial CAT systems. There is thus a real need to aid online volunteer translators and their communities by providing them with a free environment with a rich linguistic content and improved process and data management. In section 2, we first outline the status and conditions of online volunteer translators and how they work in translation. Section 3 outlines the framework and technical modules we have defined on the basis of analyzing online translators' requirements and, in the process clarified the basic requirements for the data management module. In section 4, we present the XML structures that we defined for our data management module.  2. Aiding Online Volunteer Translators We notice recently an important grow of volunteer translators who are translating thousands of documents in different fields and, thereby, showing the true way to break the language barrier. This is mainly due to the important role that the internet plays in allowing translators to join volunteer translation activities. For example, in the W3C consortium, there are 301 volunteer translators involved in translating thousands of specification documents covering approximately 41 languages. Documentation in the Mozilla project exists in 70 languages, and they are also translated by hundred of volunteer translators located in different countries. Other volunteer translator communities are involved in translation in non-identified projects; they, however, form translator groups without any orientation in advance. According to our analysis of existing communities, volunteer translator communities consist mainly of two types: Mission-oriented translators communities: mission-oriented, strongly-coordinated group of volunteers involved in translating clearly defined sets of documents. These communities cover loosely technical documentation like translation of Linux documentation [15], W3C specifications [17], and open source Mozilla localization software [9]. Subject-oriented translators network communities: individual translators who translate online documents such as news, analysis, and reports and make translations available in personal or group web pages [4] [11]. They form groups of translators without any orientation in advance, and they share similar opinions about events (antiwar humanitarian communities, report translation, news translation, humanitarian help, etc.). For instance, almost all online translators show similar behavior. As for the first communities (henceforth “Linux communities”), volunteer translators in both the Traduc and Mozilla projects are invited to translate a list of documents available on web sites (of each project) in different formats (XML, SGML, HTML, HLP, plain text, etc.). Firstly, they check whether the relevant document has been translated; if not, they make a reservation and announce the beginning of a translation to other translators via a discussion list or email. To obtain the document to be translated, they download it directly from the CVS (Concurrent Version System) or ask the coordinator to send it via mail. Once the relevant document is obtained, each translator has their individual translation environment, which is not similar to those of other translators (Figure 1). Indeed, the translation process is carried out in environments, which consist loosely of a set of tools: textual editor, dictionaries (electronic, paper version, or online), glossaries, terminology, and sometimes TM. In addition, we note the importance of the internet, which has become a precious linguistic resource for translators, who use it for recovering existing translation segments (quotations, collocations, technical terms, etc.). Documents are translated in the respect of the original format. For example in the Traduc project, documents are structured in XML DocBook1; translators operate the translation process between XML markers. After the translation finished, they send the whole target document in the same structure to the coordinators. 
In this paper, we address the problem of automatically converting information in the Korean language to one in a sign language as used in Korea. First, we discuss the differences between sign language and natural language, and in particular between the sign language in Korea and the Korean language. Then, we focus on issues that are relevant to the process of converting expressions in Korean into their counterparts in the sign language, including: 1) making explicit elided subjects of expressions in Korean, 2) omitting some expressions in Korean, and 3) reordering some expressions. We argue that it is important to utilize the spatial and motioning dimensionality of a sign language in order to minimize information loss and distortion. We also argue that the right decision to omit, or to merge some expressions in Korean plays a key role in exploiting this dimensionality. Finally, we present a system that converts sentences in Korean into corresponding animations in the sign language as proof of evidence for our claim.  1. Introduction The abundance of information in the present day, mostly in the form of written and spoken languages, has inevitably deepened the information divide among the people with different backgrounds. In this paper, we address the problems of automatically converting information in natural language to one in sign language, so as to lessen the information divide that people with different levels of hearing difficulty experience. For concreteness, we study the differences between the Korean language and the sign language as used in Korea. Earlier work in the field has mostly defined one-to-one relations between unit expressions in spoken and sign languages, along with simple translation rules. However, they suffer from the problem of not really reflecting the differences in the information space that the two types of language have. It is thus not surprising that an expression in spoken language becomes either unnecessarily verbose or severely restricted in its counterpart in sign language. In this paper, we argue that it is important to utilize the spatial and motioning dimensionality of a sign language to minimize information loss and distortion. For this reason, we also present a system that converts sentences in Korean into corresponding animations in the sign language, as proof of evidence for our claim. The rest of this paper is organized as follows. Section 2 describes other approaches to converting a natural language sentence into the corresponding sign language expression. Section 3 discusses the  differences between sign language and natural language, Section 4 explores issues that arise when natural language sentences are converted into sign language expressions, using characteristics of the sign language as identified in Section 3. Then, Section 5 presents an automatic translation system. 2. Related Work 
3. Methodology For Chinese, Malay and English, they are all spoken by the author and the data were analyzed manually by the author. Even though these three languages are distant in their origins (in terms of language family), all these three languages share similar word order of SVO, as shown in (1) below, in which the sentences with the same meaning of ‘the market enters the century of war.’ (1) Chinese:市場 Subject 進入 Verb 戰國時代 Object Malay: pasaran Subject memasuki Verb masa perjuangan Object English: the market Subject enters Verb the century of war Object  Proceedings of PACLIC 19, the 19th Asia-Pacific Conference on Language, Information and Computation. Among all the structures examined in this paper, the Malay has a different noun-modifier position in which the noun comes before the modifier as in kehendak pasaran ‘the needs of the market (or market’s needs)’ and strategi pasaran ‘market strategy. ’ Three sets of data were extracted for the analysis of this paper. The Chinese data was taken from the Academia Sinica Balanced Corpus of Mandarin Chinese (available at http://www.sinica.edu.tw/ SinicaCorpus/). Before the search was carried out, the setting was set to search for the keyword 市場 shichang ‘market’ in newspapers and magazines only. The search yielded 1775 instances and the only the first 300 were analyzed for the purpose of this work. For the Malay data, a corpus was compiled based on selected articles from the Malay newspaper Utusan Malaysia ‘Malaysian Messenger’ (available through http://www.utusan.com. my/). Using the webpage search archives system, the keyword pasaran ‘market’ was entered and this yielded 200 news articles from the systems. The first 139 news articles were collected and a concordance was made using Wordsmith Tool (Scott, 1999). This produced 285 concordance lines with the keyword pasaran ‘market’ and all these instances are analyzed. For the English data, 300 instances were taken from the total of 2726 instances of ‘market’ from the New York Times corpus complied by the American National Corpus. The data comprises over 4000 articles collected for each of the odd-nunbered days in July 2002. From these 2726 instances, only the first 300 were taken. This is to control the same amount of data across three languages (with Malay slightly lower in number). The analysis of these three sets of data involved three steps. First, all metaphorical instances were extracted from the corpora and the respective source domains are determined manually (as in Ahrens, Chung and Huang 2003 and Chung, Ahrens and Huang 2003). For instance, the following phrases were found to contain metaphorical uses. (2) (a) Chinese: 被市場淘汰出局 ‘to be kicked out by the market’ (b) Malay: apabila pasaran bermaharajalela ‘when the market rules (as King)’ (c) English: ‘as market plunges’ In 2(a), the Chinese metaphor was accorded the source domain of COMPETITION manually; the Malay metaphor in 2(b) was accorded KINGSHIP and English (2(c)) SUBMARINE. For discussion on how to determine source domains, see Chung et al. (In Press). To answer the first research question (i.e., whether the conceptual metaphors shared by these languages are similar), the frequency and percentages of the different metaphors were collected. In step one, the literal meanings of the ‘market’ will not be considered. Examples such as he entered the market to check on the hawkers will not be considered as MARKET IS A CONTAINER. This is because this market refers to the literal marketplace as a building (though a metonymic one) not the abstract concept of ‘market’ (as in the abstract ‘place’ for business exchanges to take place). Second, once all the instances in both corpora were analyzed, the instances were categorized according to the syntactic positions of the target term “market.” The types of syntactic positions identified are as (3) below: (3) (a) Subject (including grammatical subject of passive, i.e., the patient of the passive form) (b) Adverbial (only those that indicate location are found) (c) Modifier (d) Noun phrase (especially the use of noun phrase in the dependent clause such as ‘during the bull market,’ which is part of a complex sentence) (e) Object (including grammatical object, i.e, the agent, of the passive form) (f) Others (including oblique uses of ‘market’ other than the locative ones such as berpandukan kepada pasaran ‘with the guidance of the market’)  All instances were then grouped according to these syntactical categories. In order to see the terms used with these syntactic categories, the collocates of each syntactic category were constructed. This way of treating the collocates is in some way similar to Kilgarriff and Tugwell’s (2001) WORD SKETCH, which is a collocation-based resource that can tally the collocation for different grammatical relations. This part of the analysis answers the second research question (i.e., whether the syntactical positions of the word “market” are similar across these languages). The frequency and percentages of collocations for the different syntactic positions were compared across the three languages. The third research question is an interpretative question in which differences of the three speech communities are discussed in order to interpret the differences in terms of metaphor types and syntactic positions (of the target word ‘market’). It also tries to find out whether these say something about the three speech communities in terms of the attitudes and the cognitive motivations behind the use of conceptual metaphors. 4. Results 4.1.Metaphor Types In terms of metaphor types, the results in Table 1 were obtained. Table 1: Frequencies of Metaphor Types in Chinese, Malay and English MARKET Metaphors Chinese and Malay seem to show similar patterns in terms of the most prototypical metaphors found. The metaphors that occurred frequently in both these languages are MARKET IS COMPETITION and MARKET IS A PERSON (METONYMY). The metonymic use of MARKET was included in the PERSON domain because some of the instances such as 在藥材市場需求飽和後 ‘after the needs of the medicine market is filled,’ the ‘needs’ is a metonymic extension of the needs of the people rather than the market being a person. However, in other examples such as 市場正在 衰退 ‘the market is deteriorating (physically),’ the MARKET can be interpreted as a PERSON rather than a metonymic extension of a PERSON. Nevertheless, these two kinds of uses were hard to separate from one another. Therefore, they are combined under PERSON and METONYMY in Table 1 for the three languages. In English, MARKET IS A PERSON is as frequent as MARKET IS AN ANIMAL (20.18% each). Compared to Chinese and Malay, English has a slightly different pattern. MARKET IS COMPETITION in English does not appear as frequent as does the ANIMAL metaphor (such as the bull market). Comparatively, Charteris-Black and Ennis (2001) cannot outline the differences between MARKET IS A COMPETITION (PHYSICAL CONFLICT) and MARKET IS A PERSON (ORGANISM) as such demonstrated in Table 1 because their MARKET IS AN ORGANISM encompassed these two metaphors, resulting in an over-general source domain.  Proceedings of PACLIC 19, the 19th Asia-Pacific Conference on Language, Information and Computation. Table 1 also shows image metaphors such as market swings are more only found in English. Chinese does not see the market as something that ‘swings’ but one that ‘keeps changing’ as in 瞬 息萬變的市場. Both Malay and English see market as MOVEABLE ENTITY as in mengerakkan pasaran ‘move the market’ (means ‘encourage the market’) and the fickle market. On the other hand, only the Malay data show to have metaphors such as MARKET IS A FOREST (menerokai pasaran ‘exploit the market’). This metaphor (although not frequent) are particularly referring to nature of the ancient Malay society, which might involve moving to new areas of the forest as a way to exploit new land in the forest. In the Chinese data, the similar view is expressed through MARKET IS A LAND in which examples such as 開拓國內的市場 ‘exploit the local market,’ 開拓 kaituo ‘exploit’ is mapped from the LAND domain to the MARKET domain. Interestingly, none of the English MARKET metaphors show the need to exploit new market and this is interpreted as a way the Western society conceptualizes their power to control the world economy. In fact, most of the English MARKET metaphors refer to the market as something that is negative (being crushed, falling, plunges and crash). This again may show the market of the States which had been established before and most conceptual metaphors are used to refer to describe its falling and rather than its ability to compete. This is one way to explain why MARKET IS COMPETITION is not as frequent in the English data as in the Chinese and Malay data. In addition, Malay also uses the metaphor MARKET IS KINGSHIP (pasaran bermaharajarela ‘the market rules’) in which the ‘kingship’ concept originates from the sultanate system of the Malay society. Another example that shows the ancient Malay origin is that of MARKET IS OCEAN in which its use is different from that of English (in the market bottomed out and the market hit bottom). The Malay instance melayari pasaran ‘sail through the market’ is also another indication that refers to the ancient Malay society which depends on the ocean for trading and for earning a living. Comparatively, the Chinese society shows a preference for the metaphor MARKET IS A POSSESSION in which occupying a place in the market is seen as the major activity of the people in the market (佔有市場 ‘to possess the market’). The Chinese also talks about the market in terms of a pie chart and to possess a big portion of the pie chart is success. This conceptualization is not seen in Malay and there is only one instance in the English data (who had the market all to themselves). The reason behind the high frequency of the POSSESSION metaphors in Chinese may be attributed to the attitudes of the different communities toward competition in the market. As discussed earlier, the controlling power of economy is in the hand of the Western society and therefore they do not see possession is as important as in the newly emerged Chinese market (especially that of Taiwan). The lack of such instances in the Malay data may also due to its economic status which is at the stage of competing for possession in the pie chart yet. However, these are only personal interpretations of the data collected. In addition to the above differences, both Chinese and English data have metaphors related to TRANSPORTATION (and SUBMARINE and AIRPLANE for English). However, this use of transportation is not seen in the Malay data. English, especially, depends heavily on the ‘transportation’ domain (including forms such as ‘submarine’ and ‘airplane’) to refer to the movement of the market (crash, turmoil, reeled, plunges, put brakes, and turned around). The movement of the MARKET in Malay is not represented using the same metaphors. The movement in Malay often appears when the market is a person (when it is a subject) and when the market is a moveable entity (when it is an object). Due to this phenomenon observed that might have contributed to the differences in the three languages observed, this paper further analyzes the syntactic positions of the target word ‘market’ in the three languages. The following section will deal with this issue.  4.2.Types of Metaphors versus Syntactic Positions The interaction between the most frequently occurring metaphor types and the syntactic positions of ‘market’ for the three languages are given in Tables 2 to 4. These tables show percentages according to the syntactic positions of ‘market.’ Comparing Tables 2 and 3, MARKET IS COMPETITION in both Chinese and Malay has preferences for modifiers and objects. Examples are given in (4) below. (4) Chinese (COMPETITION x Modifier) 增強市場競爭力 ‘to improve the market’s competitive power’ Chinese (COMPETITION x Object) 進軍大陸及國際市場 ‘to lead the army into the Mainland China and international market’ Malay (COMPETITION x Location) boleh bersaing dalam pasaran ‘can compete in the market’ Malay (COMPETITION x Object) memonopoli pasaran ‘to monopolize the market’ COMPETITION in English (see Table 4) does not have preferences for any particular syntactic position because its occurrence is low. All three languages show similar patterns in terms of MARKET IS A PERSON (METONYMY) – this metaphor has a preference for the subject positions for the target word ‘market.’ Examples of the three languages are given in (5) below. (5) PERSON(METONYMY) x Subject Chinese 市場、銀行看在有錢可賺的份上 ‘markets and banks saw the changes of gaining profits..’ Malay pasaran menyaksikan dua penyenaraian baru ‘the market saw two new listings’ English American market was not going to rally Table 2: Most Frequently Occurring Chinese MARKET Metaphors and Syntactic Positions (Percentages According to Syntactic Positions) Table 3: Most Frequently Occurring Malay MARKET Metaphors and Syntactic Positions (Percentages According to Syntactic Positions) Comparatively, only Chinese and English data have more instances of MARKET IS A PERSON (METONYMY) as a noun phrase. (6)PERSON (METONYMY x Noun Phrase) Chinese 今天臺灣資金市場的病症是多發性的 ‘Today, the symptoms of Taiwan’s investment market are multiple.’ English Buyers’ sentiment holds fate of market  Proceedings of PACLIC 19, the 19th Asia-Pacific Conference on Language, Information and Computation. Table 4: Most Frequently Occurring English MARKET Metaphors and the Syntactical Positions for MARKET (Percentages According to Syntactic Positions) The results in this section shows one important point – the relationship between the types of metaphors and syntax is inseparable. When the source domain PERSON is chosen, most of the times the target term will appear at the subject position than at the object position. This appears true for all three languages investigated. In other words, when personification takes place, it is likely that ‘market’ is the personified subject rather than the object. On the other hand, when COMPETITION is concerned, ‘market’ in this domain usually takes the locative position, followed by the object position. This is, however, found only in Chinese and Malay, as COMPETITION is not as frequently found in English as in the other two languages. The overall frequency for each syntactic positions shows that different languages show preferences for different syntactic positions. Chinese prefers to place ‘market’ at the subject position (24.18%) and modifier (24.18%) and object (21.57%). Malay prefers the object position (42.86%) and English prefers the subject position (38.60%). This, again, indicates that even if the two languages share similar metaphors, their structuring of the MARKET metaphors may differ. For instance, Chinese treats ‘market’ equally as the doer of the action, the modifier to a noun and the receiver of the action. Malay, on the other hand, prefers to treat ‘market’ as the receivers of the action while English provides more ‘doer’ meaning to ‘market,’ as if the ‘market’ is able to carry out an action by itself. This may explain the attitudes of the English speaker as seeing the market as something that can fall and rise by itself. This can explains examples such as the market plunges, the market swings, and the market falls. In Malay, in particular, these uses are rare and most of the time, people are the ‘doer,’ as in menceburi pasaran ‘to dive (into) the market’ and menembusi pasaran ‘to break through the (wall of the) market.’ Chinese, contrastively, uses both of these structure in referring to market, as in 市場缺乏競爭性 ‘the market lacks competition’ and 掌握市 場 ‘to handle the market well.’ To look at the data differently, Table 5 shows the analysis according to the selectional preferences of the syntactic positions. Table 5: MARKET Metaphor and Syntactic Positions (Percentages according to Types of Metaphors) From this table, one can see that ‘’subject’ in Chinese and Malay is highest in the PERSON (METONYMY) metaphor than the other metaphors, but in English, ‘subject’ in PERSON is at the second place (after the ANIMAL metaphor). ‘Locative adverbial’ is found highest in the COMPETITION metaphor in both Chinese and Malay. In fact, all the syntactic categories in Chinese data seem to have reached a ceiling effect where COMPETITION is concerned, i.e., almost all  categories under COMPETITION show the highest percentages in Chinese. ‘Modifier’ in Chinese is highest in the COMPETITION metaphor; in Malay for both COMPETITION and PERSON; and in English it is highest in the IMAGE metaphor. Examples of the modifiers are given in (7) below.  (7) Chinese 市場競爭力的保障 ‘the safeguard of the market’s competitive power’  Malay strategi pasaran masing-masing ‘the respective markets’ strategy’ memenuhi kehendak pasaran pekerjaanF ‘to fulfill the needs of the job market’  English ‘All Weather’ funds are not always impervious to market swings  In order to see the patterns of the instances more clearly, the collocations according to the different syntactic positions were created, as shown in Tables 6 to 8 below. In these tables, the patterns of words that collocate with each syntactic position are presented. In Table 6, the collocations when ‘market’ is a subject are presented in all three languages. In Chinese, 37 verbs that take shichang ‘market.’.Compared to Malay and English, the patterns of each language differ. Chinese often describes shichang ‘market’ as appearing, becoming, lacking of something, and happening (middle voice), etc. Malay often describes ‘market’ as seeing something, and grow. Interestingly, in Ahrens, Chung and Huang (2003) and Chung, Ahrens and Huang (2003), they found that the Chinese and English often refer to the ‘economy’ as growing, but not the ‘market’ in this case. The Malay, however, can describe the market as something that grows. The English subject ‘market’ often takes the verb plunges, rally, and crashes. These uses of the verbs to describe the market action are not seen in Chinese and Malay.  Table 6 Collocations for ‘market’ as Subjects  Chinese  Malay  English  Collocates T Collocates T  Collocates  T Collocates T Collocates T  出 ‘materialize’  現  2  取向 ‘have tendency’  
We present a two-level model of Malagasy nominal and verbal morphology (Beesley and Karttunen, 2003), based primarily on the discussion of Malagasy morphology in Keenan and Polinsky (1998) and Randriamasimanana (1986). Words in Malagasy are built from roots by means of a variety of morphological operations such as afﬁxation and reduplication. The present paper analyzes productive patterns of nominal and verbal morphology, describing genitive compounding and sufﬁxation for nouns, and various derivational processes involving compounding and afﬁxation for verbs. 
This paper describes how to perform syntactic parsing and semantic analysis using the contextual information as a language understanding component in a dialog system. Although syntactic parsing and semantic analysis are often conducted independently of each other, correct parsing of a sentence often requires the semantic information on the input and/or the contextual information prior to the input. We therefore merge syntactic parsing with semantic analysis, which enables syntactic parsing to take advantage of the semantic content of an input and its contextual information. To use contextual information, the semantic representation of an input should have a comparable form to the semantic content of the preceding context. Accordingly, we employ a framework for semantic representations that achieves such comparison. We take dialogs of hotel search and reservation for example, and demonstrate the effectiveness of the proposed method. The experimental results confirm that the proposed system achieves high accuracy in parsing and generation of semantic representations. 1. Introduction When we have a talk, we presuppose the situation and particular context.1 The situation/context enables us to understand utterances without talking in great detail. The situation/context also makes it possible to convey a wide variety of contents with a certain linguistic expression. This means that a language understanding component in a dialog system should not interpret a given sentence alone; rather, it should interpret a sentence using the context information. In our previous work, we have developed a Japanese dialog system for hotel search and reservation (Noguchi et al. 2002). The system accepts free input from the keyboard. We are planning to build an audio input module into the system. Audio input raises the importance of the context information because the effective use of the context information may circumvent possible recognition errors. In this paper, we focus on how a language understanding component can perform syntactic parsing and semantic analysis using the contextual information. It may be possible to integrate syntactic parsing with semantic analysis by taking advantage of LFG (Kaplan et al. 1982) and HPSG (Pollard et al. 1994). However, they do not consider the use of the context information. In this paper, we propose the method to integrate dependency analysis with semantic analysis referring to the context. Recently, many researches have adopted a statistical approach toward parsing, and it has been shown that the statistical approach is effective in obtaining correct dependency structure. There are 
In the past two hundred years or so, a sizable corpus of Taiwanese text in Latin script has been accumulated. However, due to the political and historical situation of Taiwan, few people can read these materials at present. It is regrettable that the utilization of these plentiful materials is very low. This paper addresses problems raised by the Taiwanese tone sandhi system by describing a set of computational rules to approximate this system, as well as the results obtained from our implementation. Using the Taiwanese Latinization text as source, we take the sentence as the unit, translate every word into Chinese via a Taiwanese-Chinese dictionary, and obtain the POS information from the CKIP dictionary made by the CKIP group of the Academia Sinica. Using the POS data and tone sandhi rules we formulated based on linguistics, we then tag each syllable with its post-sandhi tone marker. Finally we implemented a Taiwanese tone sandhi processing system which takes a Latinized sentence as input and outputs the tone markers. We were able to obtain an accuracy rate of 97.56% and 88.90% with training and testing data, respectively. We analyze the sources of error for the purpose of future improvement. Keywords: written Taiwanese, tone sandhi system, Taiwanese latinization 1. Introduction 1.1.Background Taiwanese is often used in daily life in Taiwan, but written Taiwanese is less common by far; even so, the history of written Taiwanese stands at well over a century. (Tiun 2001) At present there are several dozen if not more than a hundred proposed phonetic and writing systems for Taiwanese. (Iun&Tiun 1999) The orthography adopted by this article is Peh-oe-ji (POJ, also known as Latinized Taiwanese or Church Latinized Taiwanese). Under the auspices of the National Museum of Taiwanese Literature, the Department of Taiwanese Literature of Cheng Kung University carried out a project titled 'The Collection and Cataloging of Peh-oe-ji Literature Data'. Although a lot of texts have already been lost due to the changing political situation, this project nevertheless revealed nearly 2,000 Peh-oe-ji books and periodicals, with publication sites spread over Taiwan, Xiamen, Shanghai, Guangzhou, Hong Kong, Singapore, Philippine, London, Japan, etc. The amount of publishing peaked in the 1950’s and 1960’s. (Iun&Tan-Ten 2005) The scope covers not only formally published books and periodicals but also non-published items such as personal letters and medical charts. Later on the government, citing supposed detrimental effects of Peh-oe-ji on Mandarin promotion as a reason, banned its use and thus contributed to the rapid decline of this practice. We hope that applied information science might enable more people to access the extant materials collected by the above-mentioned project, as well as contribute to basic and applied research in Taiwanese. As most people nowadays are not familiar with Latinized written Taiwanese, use of state-of-the-art text-to-speech technology would enhance the value of these materials to the general  public. Tone sandhi represents a challenging problem to be solved before we can successfully transform the written Taiwanese text to its natural speech-like tonal contour. This is because the written form of Latinized Taiwanese represents the tones as "base tones", or the tones of syllables when they are read in isolation. At the level of the word, all syllables but the last one are usually read differently (that is, they manifest tone sandhi). At the level of a whole sentence, under most situations only the last syllables next to the boundary of the phrases or structural markers are read as base tones, the others being read as sandhi tones. In fact, besides the "regular tone sandhi" mentioned above, there exist several other kinds of situations which will be discussed one by one later. This paper will first formulate the sandhi rules, which are the key to a correct pronunciation. The inputs of our experiment are mainly the data collected by the above-mentioned project; these data are processed by our sandhi system and the final outputs are these data with sandhi markers. Due to the lack of a set of well-marked data, we do not adopt the statistical model in this experiment but the rule-based model. Two of the authors who are long-time experienced Taiwanese users evaluated the outputs for their accuracy. 1.2. Tone Sandhi Problem Tones in Taiwanese are traditionally analyzed as consisting of piâⁿ, siáng, khì, ji̍p, each having im and iâng but for siáng, so there are seven tones in all. Following the sequence of im-piâⁿ, siáng, imkhì, im-ji̍p, iâng-piâⁿ, iâng-khì, iâng-ji̍p, they are numbered 1 (high flat), 2 (high to low), 3 (low), 4 (middle short), 5 (low rising), 7 (middle flat), and 8 (high short). The descriptions of tone pitch are within the parentheses. For tone diacritics please refer to the following examples. Tone sandhi is a very important characteristic of Taiwanese. At the level of the word, the last syllable is usually read as base tone and the others, as sandhi tones. For example in (1) the underlined syllables are read as base tones, while the others are read as sandhi tones, and dialect difference is represented in parenthesis : (1) tâį (“palteform”) / Tâi-gí(gú)̨Ⴇ (“Taiwanese language”) Tâi-gí(gú)-bûn̨Ⴇ˖ (“written Taiwanese”) Tâi-gí(gú) bûn-ha̍k̨Ⴇ˖ኪ (“Taiwanese literature”) Tâi-gí(gú) bûn-ha̍k-sų́Ⴇ˖ኪ̦ (“history of Taiwanese literature”) In fact, at the level of the syllable or the word, tone sandhi may manifest in at least the following several ways: (a) Normal sandhi: using reduplicated syllables as examples (the numbers within parentheses are reading tones). (2) (i) tone 1 → tone 7: “chheng-chheng૶૶” (7,1) (“water-white”) (ii) tone 7 → tone 3: “chēng-chēng᎑᎑” (3,7) (“quiet”) (iii) tone 3 → tone 2: “chhiò-chhiò३३” (2,3) (“smily”) (iv) tone 2 → tone 1: “léng-léng冷冷” (1,2) (“cold”) (v) tone 5 → tone 7 or 3 (northern Taiwan): “âng-ângߎߎ” (7/3,5) (“red”) (vi) tone 4 → tone 8 (-p/t/k) or 2 (-h): like “sip-sip᐀᐀” (8,4); (“moisty”) “phah-phah͂͂” (2,4) (“beat”) (vii)tone 8 → tone 4 (-p/t/k) or 3 (-h): like “ٜٜti̍t-ti̍t” (4,8) (“straight”); “ᆠᆠjo̍ah-jo̍ah” (3,8) (“hot”)  Proceedings of PACLIC 19, the 19th Asia-Pacific Conference on Language, Information and Computation. (b) Following sandhi: this pattern generally occurs on pronouns or the suffix of names. The tone pitch depends on that of the immediately preceding syllable and is either tone 1, 3, or 7. (3) (i) “A-eng--a‫ڛ‬຃a” (7,1,1) (the second "a" is a suffix) (“a person name”) (ii) “góa lâi khòaⁿ -- iҢ來޶ͺ” (1,7/3,3,3) (the base tone of “iͺ” is tone 1) (“I come to see him/her”) (iii) “hō͘--lí [ഗ]Ы” (7,7) (the base tone of “líЫ” is tone 2) (“give you”) (c) Neutral sandhi: the previous syllable is read as base tone, and the tones of the neutral sandhi are read softly as if they were tone 3 or tone 4. (4) (i) “Tân--sian-siⁿ(sin-seⁿ) ௓΋͛” (5,3,3) (the original tones of “sian-siⁿ (sin-seⁿ) ΋͛” are tone 7 and tone 1) (“Mr. Tân”) (ii) “kiâⁿ--chhut-lâi行̈來” (5,4,3) (the original tones of “chhut-lâï來 ” are tone 8 and tone 5) (“walk out”) (d) Double sandhi: this pattern mostly appears in syllables endng in the glottal stop (-h) and having tone 4. The normal sandhi rules are applied twice in sequence (i.e. tone 4 → tone 2 → tone 1): (5) (i) “beh tha̍k-chu[ࠅ]讀ࣣ” (1,4,1) (“beh ࠅ” is tone 4, but rather than becoming tone 2, it becomes tone 1) (“want to read books”) (ii) “khì gōa-kháu̘̮ɹ” (1,3,2) (“khì̘” is tone 3, but rather than becoming tone 2, it becomes tone 1) (“go outside”) (e) Pre-á sandhi: the syllables before á are different from the normal sandhi unless they are tone 1 or tone 2. (6) (i) tone 1 → tone 7: “sun-á࢑˺” (7,2) (“nephew”) (ii) tone 2 → tone 1: “chháu-áণ˺” (1,2) (“grass”) (iii) tone 3 → tone 1: “tàⁿ-áዄ˺” (1,2) (“stall”) (iv) tone 4 → tone 8 (-p/t/k) or tone 1 (-h): “tek-á϶˺” (8,2) (“bamboo”) “thih-á᚛˺” (1,2) (“iron”) (v) tone 5 → tone 7: “lô͘-á爐˺” (7,2) (“oven”) (vi) tone 7 does not change: “phō͘-áᖤ˺” (7,2) (“tablet”) (vii) tone 8 → tone 4 (-p/t/k) or tone 7 (-h): “chha̍t-á༞˺” (4,2) (“thief”) “hio̍h-á葉˺” (7,2) (“leaf”) 
For speech synthesis systems that transform text materials into voice data, correctness and naturalness are the crucial measures of performance, the latter gaining more emphasis recently. In order to make synthesized voices natural, we must take into account pronunciation-related linguistic phenomena such as homograph, among others. The syntax certainly provides an important clue to disambiguating such homographs, but the relatively free word order in the Korean language makes it hard to utilize such information. In this paper, we describe a computational generation of contextually appropriate vowel lengths for the words in Korean by utilizing a higher level of linguistic information in a Combinatory Categorial Grammar framework. We consider parts-ofspeech information, the possibility of conjunction with a suffix, case information, unconjugated adjectives, numerals, numerical adjectives with related nouns, and the relationship between a noun and its predicate as syntactic and semantic clues for vowel sound disambiguation. The results are expressed in Speech Synthesis Markup Language (SSML) for a target system neutral application. The proposed system with correctly predicted vowel sound can be used not only as an educational tool, but also as a plug-in for enhancing the intelligibility of a general purpose Text-to-Speech (TTS) system.  1. Introduction Speech plays an important role as a communication method from birth to death, and it has also been regarded as the most user-friendly interface to machines thanks to the great development of the Information Technology such as the technologies for computers and mobile phones. For speech synthesis systems that transform text sources into voice data, correctness and naturalness are considered the crucial measures of performance. The naturalness enhances usability of speech synthesis systems with adding intelligibility to synthesized results. In order to make synthesized voices more natural, we must take into account pronunciation-related linguistic phenomena, such as homograph, among others. While most homographs in English have different vowel sound such as ‘read [ri:d]’ versus ‘read [red]’ and ‘bass [beis]’ versus ‘bass [bæs]’, homographs in Korean share nearly the same pronunciation with one another, except for the lengths of their vowel sound. This has apparently caused many people even including television news announcers, to suffer from the confusion between short vowel sound and long vowel sound in homographs. Disambiguation of short and long vowel sound for a homographic word in Korean represents not only the standard pronunciation, but also a mark of different meanings of that word. For instance, the word ‘밤(pam)’ has short vowel sound ‘[bam]’ to mean ‘night’, and long vowel sound ‘[ba:m]’ to mean ‘chestnut’. The following examples also show two different meaning of the word ‘눈(nwun)’ that means ‘eyes’ with short vowel sound ‘[nun]’, and ‘snow’ with long vowel sound ‘[nu:n]’, respectively.  (1) 철수가 눈이 아프대. (chelswuka nwuni aphutay: Chelswu has pain in his eyes.) (2) 오늘은 눈이 올 것 같아. (onulun nwuni ol kes katha: It is likely to snow today.)  The phenomenon that involves short and long vowel sound of Korean homographic words is hard to explain with simple phonetic rules alone. When a word is broken into several lexical units for analysis of its reaction with one another, vowel sound tends to be characterized either short or long even with the same morpheme and similar lexical item sequence. For further details, the reader is referred to Lee and Park (2005). It is thus more reasonable to assume that short and long vowel sound of homographic words can be addressed by lexical characteristics rather than by simple phonetic rules. The correct disambiguation of short and long vowel sound not only gives rise to a standard usage for the word pronunciation, but also gives a chance to distinguish different meanings of a word that can not be inferred simply from the written sentence. For example, we can not identify the intended meaning and vowel sound length with following sentence alone. (3) 나는 밤이 좋아. (nanun pami coha: I love night / chestnuts.)  But when the above sentence is pronounced correctly by a speaker, the intended meaning of the involved word can be identified with proper short or long vowel length as in ‘[bam]’ for ‘night’ and ‘[ba:m]’ for ‘chestnut’. More examples are shown in Table 1.1  Table 1: Word senses that depend on vowel lengths  Word  Long vowel sound  Short vowel sound  밤(pam)  chestnut  night  눈(nwun)  snow  eyes  말(mal)  speech  horse  굴(kwul)  tunnel  oyster  배(pay)  twice or more  abdomen, ship, pear  모자(moca)  mother and son  hat  부자(pwuca)  rich person  father and son  부채(pwuchay)  debt  fan  성인(sengin)  saint  adult  새집(saycip)  birdhouse  new house  유리(ywuli)  profitable  glass  천직(chencik)  lowly occupation  vocation as calling  During the disambiguation of homographs in written form, the syntax certainly provides an important clue, but the relatively free word order in the Korean language makes it quite hard to utilize such information. Note that the Japanese language, whose word order is nearly as free as Korean, has certain words with the same sound but with different Kanji characters. In this case, their meanings can be distinguished by the use of intervening characters such as ‘あ’, ‘い’, ‘う’, ‘え’, ‘お’, or ‘-’. For example, ‘歡呼(かんこ)’ and ‘觀光(かんこう)’ mean ‘acclamation’ and 
This article explores the construction of filler-gap dependencies in Chinese possessor relative clauses (PRCs), which are different from typical relative clauses (RCs) considered in the literature because Chinese PRCs contain no overt missing arguments (i.e. gaps). As Chinese RCs are prenominal, the gaps precede the head noun fillers. It has been suggested that when the gaps are close to the filler, the dependency is easier to construct; there is, thus, a processing advantage for object RCs over subject RCs (Hsiao & Gibson, 2003). The PRC data presented show that even in Chinese, a language with RCs that are head-final, it is possible to have a subject gap preference (over object) despite the fact that the subject is further away from the filler. Three experiments confirmed this subject preference with respect to naturalness and grammaticality ratings (Experiment 1), paraphrasing tasks (Experiment 2), and self-paced reading tasks (Experiment 3). The results support a theory of gap-searching which operates top down. Issues regarding locality and canonicity will also be discussed. 1. Introduction As human languages are replete with dependency relations within and across sentences, one crucial task of the language parser concerns the efficient recovery of such relations and their correct interpretation. This process involves various factors, including the required on-line processing load (often discussed in terms of working memory), the complexity of the processed materials, and human syntactic knowledge, etc.1 Most previous research focused on the first two factors. When a sentence consumes more processing resources, it is assumed to be more difficult and thus takes longer to understand. Similarly, when a sentence is more complicated or less usual, it requires longer processing time. However, the third factor, i.e. the structural properties of different syntactic positions in strategic on-line processing, has been less studied. Studies of focus positions and their salience for processing are both examples of this vein of inquiry (see for example Birch et al., 2000, and Frazier et al., 2005). In this paper, we present data that supports structural knowledge as a prominent factor in sentence processing. By structural or syntactic knowledge, we mean knowledge about the function of specific syntactic positions. This knowledge allows the human parser to recover structure in efficient fashions. For example, in a probe-goal model such as Chomsky (2001), this might mean specific and direct access to certain syntactic positions (e.g. a probe to spec-TP), followed by more general top-down search for matching goals. As the experimental data with possessor relative clauses (PRCs) will suggest, surface subject positions are the most likely candidates for direct gap-probing. In what follows, section 2 reviews findings on processing Chinese relative clauses (RCs), and classical effects such as locality and canonicity. Section 3 introduces Chinese PRCs and issues regarding filler-gap dependencies. Section 4 presents three experiments on sentence comprehension. Section 5 discusses the implications of the  
Errors of the translation of tense, aspect, and modality by machine translation systems were analyzed for six translation systems on the market and our new systems for translating tense, aspect, and modality. The results showed that our systems outperformed the other systems. They also showed that the other systems often produced progressive forms rather than the correct present forms. Our systems rarely made this mistake. Translation systems on the market could thus be improved by incorporating the methods used in our systems. Moreover, error analysis of the translation systems on the market identified information that would be useful for improving them. 1. Introduction Tense, aspect, and modality are difficult to translate appropriately using machines (Shirai et al., 1990; Kume et al., 1990; Dale et al., 2000; Nirenburg et al., 2002). We investigated the error patterns produced by translation systems when translating Japanese tense, aspect, and modality expressions into English. We compared the performance of six translation systems on the market and our new translation systems for tense, aspect, and modality. We found that our systems outperformed the other systems, and we detected error patterns that the other systems often made and our systems rarely made. These results can be used to improve translation systems on the market. Moreover, we extracted error patterns peculiar to each translation system. These errors can be corrected easily because their corresponding sentences can be translated correctly by other systems. These results are useful for improving each translation system.  2. Method In our investigation, we considered that the translation of tense, aspect, and modality from Japanese to English means the production of the surface expressions of tense, aspect, and modality of the main verb phrase in the English translated sentence. We calculated the accuracy rates and extracted the error patterns in the translations. We used combinations of the following categories as the surface expressions of tense, aspect, and modality. We refer to the categories as the categories of tense, aspect, and modality. 1. all combinations of {present tense, past tense}, {progressive, non-progressive}, and {perfect, non-perfect} (8 categories) 2. imperative mood (1 category) 3. auxiliary verbs ({present tense, past tense} of “be able to”, {present tense, past tense} of “be going to”, {present tense, past tense} of “be to”, “can”, “could”, and {present tense, past tense} of “have to”, “had better”, “may”, “might”, “must”, “need”, “ought”, “shall”, “should”, “used to”, “will”, and “would”) (21 categories) 4. noun phrases (one category) 5. participial constructions (one category) 6. verb ellipses (one category) 7. interjections or greeting sentences (one category) We used 800 sentences extracted from a corpus1 containing 40,198 sentences for the evaluation. We calculated the accuracy rates of six translation systems on the market and our new translation systems and examined the error patterns in the results. The six translation systems were the latest of leading translation system companies as of October 2003. Our systems for translating tense, aspect, and modality are based on support vector machines (SVMs) (Murata et al., 2001).2 They translate Japanese tense, aspect, and modality expressions into English. They detect categories of tense, aspect, and modality previously defined from English expressions. The categories are detected as a categorization problem by SVMs (Cristianini and Shawe-Taylor, 2000; Kudoh, 2000). However, an SVM can handle only two categories at a time. Therefore, we used a pairwise method in addition to the SVM to handle more than two categories (Moreira and Mayoraz, 1998). As training sentences, we used the sentences remaining after eliminating the 800 evaluation sentences from the 40,198-sentence corpus. We used two feature sets for the machine learning. • Feature Set 1 This set consisted of 1- to 10-gram strings at the ends of the input Japanese sentences, e.g., shinai (do not), shinakatta (did not). • Feature Set 2 This set consisted of all of the morphemes in each of the input sentences, e.g., kyou (today), watashi (I), wa (topic-marker particle), hashiru (run). 
This paper presents a novel method of semantic parsing that maps a natural language (NL) sentence to a logical form. We propose a semantic parsing method by conducting separately two steps as follows; 1) The first step is to predict semantic tags for a given input sentence. 2) The second step is to build a semantic representation structure for the sentence using the sequence of semantic tags. We formulate the problem of semantic tagging as a sequence learning using a conditional random field models (CRFs). We then represent a tree structure of a given sentence in which syntactic and semantic information are integrated in that tree. The learning problem is to map a given input sentence to a tree structure using a structure support vector model. Experimental results on the CLANG corpus show that the semantic tagging performance achieved a sufficiently high result. In addition, the precision and recall of mapping NL sentences to logical forms i.e. the meaning representation in CLANG show an improvement in comparison with the previous work. 1. Introduction Semantic parsing is an interesting problem in NLP as it would very likely be part of any interesting NLP applications. For example, the ability to map natural language to a formal query or command language is critical to developing more user-friendly interfaces. Traditional approaches to constructing database interfaces require an expert to hand-craft an appropriate semantic parser (Allen, 95). However, such hand-crafted parsers are time consuming to develop and suffer from problems with robustness and incompleteness, even for domain specific applications. Recent approaches have focused on using machine learning methods on the corpus of sentences and semantic annotations to map natural language sentences (NL) to a complete formal language including cite (Miller, 96)(Zelle,96)(L.R.Tang, 2003). These machine learning methods are the application of inductive logic programming (Zelle, 96) (L.R. Tang, 2003), the adaptation of transformation based learning (R.J. Kate, et.al. 2005) to semantic parsing, and the modification of a head-driven parsing model (R.Ge and R.J. Mooney 2005). This paper addresses a novel approach to the problem of semantic parsing by considering it as a structure classification problem with a support vector machine model (SVM). We describe a tree structure in which semantic information and syntactic information are incorporated and the semantic representation of a sentence, such as first order logic or CLANG, can be interpreted by the tree. With this flexible structure, the problem of semantic parsing is converted to the problem of how we can parse a sentence to a tree. Furthermore, we can easily incorporate many useful features by mapping a NL sentence to the formal representation under the framework of a structured support vector machine (Tsochantaridis、  et.al., 2004). We also utilize the advantages of kernel methods as well as discriminate learning to the semantic parsing problem. In addition, we propose a semantic tagging problem that maps each word in NL sentence to a semantic category by formulating it in a sequence learning mode. We then exploit a Conditional Random Field (CRF) model with a set of template features for that problem. Our semantic tagging proved to be very accurate showing that it can provide preprocessing in mapping NL sentences to a formal language. Although the proposed method can be applied to any task of mapping NL sentence to a formal language, in this paper we focus on experimenting on the problem of mapping NL sentence to its representation in Coach Language (CLANG corpus) (Chen, et.al., 2003). The rest of this paper is organized as follows: Section 2 gives some background in which we briefly present the previous work and the conditional random field model, as well as the structured support vector machine model. Section 3 proposes our method, which uses a conditional random filed for semantic tagging and augment it to the structured support vector machine for transforming a NL sentence to the coach language. Section 4 shows experimental results and Section 5 discusses the advantage of our methods and describes future works. 2. Background 2.1 Related works Zelle and Mooney initially proposed the empirically based method using a corpus of NL sentences and their formal representation for learning by inductive logic programming (ILP) (Zelle, 1996). Several extensions for mapping a NL sentence to its logical form have been addressed by (L.R. Tang, 2003). The task of transforming a natural language sentence to a logical form was formulated as the task of determining a sequence of actions that would transform the given input sentence to a logical form (L.R. Tang, 2003) (J.R. Mooney, 2004). The main problem is how to learn a set of rules from the corpus using the ILP method. The advantage of the ILP method is that we do not need to design features for learning a set of rules from corpus. The disadvantage is that it is quite complex and slow to acquire parsers for mapping sentences to logical forms. Kate et al. presented a method (R.J.Kate, et.al. 2005) that used transformation rules to transform NL sentences to logical forms. Those transformation rules are learnt using the corpus of sentences and their logical forms. This method is much simpler than the ILP method, while it can achieve comparable result on the CLANG (Coach Language) and Query corpus. The transformation based method has the condition that the formal language should be in the form of LR grammar. Ge and Mooney also presented a statistical method (R.Ge and J.R. Mooney 2005) by merging syntactic and semantic information. Their method relaxed the condition in (R.J. Kate et.al. 2005) and achieved a state-of the art performance on the CLANG and query database corpus. The recent work proposed by (Zettlemoyer, et.al. 2005) that maps a NL sentence to its logical form by structured classification, using a log-linear model that represents a distribution over syntactic and semantic analyses conditioned on the input sentence. This work is quite similar to our work in considering the structured classification problem. The difference is that we used the kernel based method instead of a log-linear model in order to utilize the advantage of handling a very large number of features by maximizing the margin in the learning process. In addition, the learning process in our method is performed on the corpus of NL sentences and their tree structures, while their method used probabilistic categorical grammar.  Proceedings of PACLIC 19, the 19th Asia-Pacific Conference on Language, Information and Computation.  2.2 Conditional Random Fields Let o = o1o2...oT be some observed data sequence. Let S be a set of FSM states, each of which is associated with a label, l ∈ L . Let s = (s1, s2,..., sT ) be some state sequence, CRFs (Lafferty et.al., 2001) define the conditional probability of a state sequence given an observation sequence as  ∑ ∑ pθ  (s  |  o)  =  
This paper presents an analysis of Internally Headed Relative Clause (IHRC) construction in Japanese within the framework of Combinatory Categorial Grammar (Steedman 2000). Shimoyama (1999) argues that when an IHRC appears within the scope of a universal quantifier, the interpretation of the IHRC exemplifies E-type anaphora and that the LF representation of the IHRC should have a variable bound by the quantifier in the matrix clause. To accommodate this argument Shimoyama posits a free variable of functional type to which the bound variable is applied, and whose denotation is determined by the context-dependent assignment function. However since there is no limit to the number of quantifiers in the matrix clause (and accordingly that of bound variables in the IHRC), the semantic type of the free variable would be highly ambiguous if the IHRC occurs within the scope of multiple quantifiers. The current analysis assumes that the interpretation of IHRCs exhibits an instance of generalized Skolem term (Steedman 1999, 2004), a term whose denotation varies with the value of bound variables introduced by scope-taking operators, but which is interpreted as a constant in the absence of such operators. This paper provides a straightforward account for the semantics of the construction without invoking the complexities of the type ambiguity of free variables.  1. Introduction This paper presents an analysis of Internally Headed Relative Clause (IHRC) construction in Japanese paying particular attention to the effect of quantification on its interpretation. (1) illustrates the basic form of the construction:1 (1) Taroo-ga [Hanako-ga ringo-o muita] no-o tabeta. Taro-NOM Hanako-NOM apple-ACC peeled NML-ACC ate ‘Taro ate the apple that Hanako peeled.’ The bracketed string Hanako-ga ringo-o muita ‘Hanako peeled an apple’ is a clause followed by nominalizer no. The whole NP obtained is marked with accusative particle -o, and is construed as the object of the matrix verb tabeta ‘ate’. Since the verb requires as its semantic restriction that the object be an edible thing, it anaphorically picks up the referent of ringo ‘apple’ from the embedded clause. A brief note on terminology is in order. If we say the antecedent of an IHRC, we mean the referent of the IHRC which functions as the argument of the matrix predicate. And the head of the IHRC refers to the linguistic element in the IHRC which describes the antecedent. So for example, in (1) the antecedent is the apple that Hanako peeled, and the head is the noun ringo ‘apple’.  
Shigeru Sato Graduate School of International Cultural Studies, Tohoku University Kawauchi 41, Aoba-ku, Sendai-shi, Miyagi 980-8576, Japan satos@mail.tains.tohoku.ac.jp  Abstract This paper is concerned with frequency adverbs in Japanese. Many linguists have pointed out that frequency statements and generic sentences have multiple readings when they co-occur with a kind of adjuncts. More often than not, the interpretation is complicated by the ambiguity of temporal adverbs and the context-dependent nature of the way operator works. In fact, frequency adverbs in Japanese cause at least four readings. We resolve this ambiguity by taking into account two factors. The first one is the syntactic structure at which a temporal adverb is located: it can stand for either an interval during which events are iterated or a temporal point for a single event. Second, we illustrate how the restriction and scope, the two arguments of the frequency operator, are decided depending on the context. Lastly, we show how semantic formulas for multiple interpretations are derived as a result of combining these factors. 1. Introduction Given that the frequency adverb can be taken as a quantifier over an event, it can easily be seen that many of sentences with this kind of adjunct are ambiguous. As shown in the following section, however, the frequency adverb in Japanese sentences cause much more ambiguity than one can expect. We address this issue which has evaded researchers’ attention so far. The approach we adopt is two-fold. First, we investigate the ambiguity of temporal adverbs in terms of modification of a duration during which events are repeated or a temporal point corresponding to a single event. Second, we try to account for the remaining part of the ambiguity by assuming that the restriction and scope of the frequency operator is introduced by context, more specifically by the focus structure. We show how these factors combine to produce semantic formulas assigned to multiply ambiguous sentences with frequency adverbs. 2. Multiply ambiguous sentences with frequency adverbs In this section, we take up examples from Japanese sentences with frequency adverbs and show how they are multiply ambiguous. Let us see sentence (1). This is ambiguous in four ways.  (1) Kyuzitu-ni Taro-ga  tokidoki  eiga-o  holiday-on Taro-NOM sometimes movie-ACC  “On holiday(s), Taro sometimes saw a movie.”  mi-ta. see-PAST  The first distinction in meanings involves the ambiguity in number (singular/plural) attributed to Japanese nouns. The noun kyuzitu is ambiguous in this sense, meaning either a series of holidays or a single holiday. In the first case, the adverbial phrase kyuzitu-ni provides a duration of time during which events of Taro’s seeing a movie is repeated. This is the interpretation shown as (2a). We interpret this sentence as having no specific restriction, as shown by `φ’. The meaning of the sentence is diagrammatically shown in Figure 1. The restriction of the quantified sentence is taken as a set of relevant situations provided by the context.  A:Nuclear Scope B:Restriction C: Holidays  [A]: A set of events Taro saw a movie [B]: A set of felicity situations with respect to Taro saw a movie Figure 1  A:Nuclear Scope B:Restriction C: Felicity situations with respect to iteration-event Taro saw a movie  [A]: A set of events X's occur on holiday [B]: A set of events Taro saw a movie  Figure 2  A:Nuclear Scope B:Restriction C: Felicity situations with respect to iteration-event Taro saw a movie  [A]: A set of events Taro saw a movie [B]: A set of events X's occur on holiday  Figure 3  Proceedings of PACLIC 19, the 19th Asia-Pacific Conference on Language, Information and Computation.  A:Nuclear Scope B:Restriction C: Felicity situations with respect to iteration-event Taro saw a movie  [A]: A set of events Taro saw a movie on holiday [B]: A set of felicity situations with respect to Taro saw a movie  Figure 4  (2) Four interpretations in (1)  a) holiday (sometimes {φ} [Taro saw a movie] )  interval  Restriction Nuclear Scope  b) φ (sometimes {event X occurs holidays} [Taro saw a movie])  interval  Restriction  Nuclear Scope  c) φ (sometimes{Taro saw a movie} [Taro saw a movie on holiday] )  interval  Restriction  Nuclear Scope  d) φ (sometimes {φ} [Taro saw a movie on holidays]  interval  Restriction  Nuclear Scope  In the remaining cases (2b, c, d), there is no specific refernce to the interval during which events are repeated, as shown by `φ’ as the value for the interval. In these three sentences, the AdvP kyuzitu-ni modifies events within the restriction or nuclear scope. (2b) is an interpretation according to which some proportion of the events which occur on holiday are Taro’s seeing a movie. In this reading, as shown in Figure 2 and (2b), the sentence refers to the ratio between two sets of events, those occurring on holiday and those of Taro’s seeing a movie. Interestingly, (2c) mentions a reversed relation between the two sets of events given above; it says that some proportion of the events “Taro’s seeing a movie” occurs on holiday, as is also shown in Figure 3. (2d) has no specific restriction, either. It says that in relation to a set of relevant situations established by the context (this is shown by `a set of felicity conditions’ in Figure 4), Taro’s seeing a movie on holiday happens occasionally.  3. Structural ambiguity of time adverbial adjuncts This section discusses where the difference in modification by the temporal adverbial kyuzitu-ni explained in the previous section derives from. What we rely on is Nakamura’s (2001) observation that the tense/aspect information of the Japanese clause is composed of three strata. (3) 1st Stratum: the assignment of aspect and a temporal point to a single eventuality 2nd Stratum: the assignment of aspect and a temporal point to a set of repeated eventualities 3rd Stratum: the assignment of (relative or absolute) tense  While Nakamura regards the hierarchical structure primarily as a semantic one, we take this as being reflected as a syntactic structure. (4) is the syntactic structure we pose for the interpretation (2a) of sentence (1). The temporal AdvP kyuzitu-ni, appearing at PP2, the uppermost position of the sentence structure and the place for the 2nd Stratum accounted for in (3), provides a duration of time during which the events of Taro’s seeing a movie are repeated. Note again that the noun kyuzitu is not marked for plurality. (4) The syntactic structure for (2a) (5) is the syntactic structure for other interpretations (2b, c, d) for sentence (1). (5) The syntactic structure for (2b-d) Note here that the position PP2 is left unspecified, and PP1 is assigned to the temporal AdvP kyuzitu-ni. This is the position for the 1st Stratum in (8), specifying the temporal information of only a single event. The AdvP is moved to the sentence-initial locus as a result of scrambling.  Proceedings of PACLIC 19, the 19th Asia-Pacific Conference on Language, Information and Computation. The difference should originate from that in lexical definitions for the two usages of the noun kyuzitu. After combining with the postposition ni, the syntactic and semantic information assigned to the AdvP looks like below: (6) kyuzitu-ni-2 Syntax：PP-2 Semantics：λP$E(holiday(E) ⁄ P(E)) (7) kyuzitu-ni-1 Syntax：PP-1 Semantics：λP$e(holiday(e) ⁄ P(e)) (6) is for the usage of the AdvP as a `2nd Stratum’ temporal AdvP which stands for an interval during which events are repeated. Here, holiday is predicated of a set of events E. (7) is for the `1st Stratum’ temporal AdvP which indicates the time of a single event, as represented by e. 4. Analysis based on Focus Structure This section aims at accounting for why one and the same syntactic structure (5) leads to the three way ambiguity as shown in (2b, c, d). We attribute this to different ways of focus placement. Rooth (1985, 1992) discusses that a different focus placement in the same sentence can change truth values. He pointed out that under the circumstance in (8), (8a) is true but (8b) is false. (Subscript F stands for focus phrase.) (8) John introduced Bill and Tom to Sue. a. John only introduced Bill to [Sue]F. b. John only introduced [Bill]F to Sue. We assume that the restriction and nuclear scope in sentences with frequency adverbs are decided depending on the information structure of the sentence. While the restriction is provided by the presupposition of the sentence, the nuclear scope is identified with its focus. See the different manners in focus placement in (9b, c, d), which each correspond to (2b, c, d). In (9b, c, d), [ ]1 stands for a focused phrase and [ ]0 for one without focus. (9) a. [Kyuzitu-ni tokidoki [Taro-ga eiga-o mita]F ]1 b. [[Kyuzitu-ni] F tokidoki Taro-ga eiga-o mita]]1 c. [Kyuzitu-ni tokidoki Taro-ga eiga-o mita]0 Rooth (1985,1992) proposed "P-set"(the set of preposition) and "alternative semantics". "P-sets" indicates the set of the type which corresponds to the type of focus phrase. In our case, "P-set" is the set of events. "Alternatives" means a set of presuppositions including P-sets. "Alternatives" can be represented as open sentence which is composed by the abstraction of P-sets. For (2b), "Alternatives" are calculated as follows; 1) Specify the type of the focus phrase and determine "Psets" ("P-sets" of our example (2b) is a set of events). 2) Using "P-sets", compose the sentence (in example (2b), "event e occurs on holiday"). 3) Formulate "alternative" ({event e occurs on holiday| e  œ a set of events}). 4) Map alternatives on syntactic structure, after syntactic structure converge (see (10))1. 5) Specify the variable x from the uttered sentence. (10) "alternatives" structure in (2b) Presupposition in (2b) indicates ; {event e occurs on holiday| e œ a set of events} According to this notation, we can determine the presupposition in (2c) as follows, whereas (2d) has no presupposition; in other words, all arguments are new information. (11) Presupposition in (2c) a. P-sets: a set of events. b. Alternatives:{taro saw a movie on e| e œ a set of events} From the analysis in alternative semantics, it is clear that the sentence with association with focus has two arguments. One is "alternative"(Presupposition) and the other is "the uttered sentence" (Proposition). Based on the discussion above, we can now specify the meaning of the frequency ADVP tokidoki as follows: (12) Given that P = [X [Y]F]1 and [[P]] = $eΠ, Tokidoki(E, [[P]]) = Sometimes-When(E, λeΠ, Presupposition ([[P]])), where Presupposition ([[P]]) is obtained by the procedure explained above. 5. Analysis for multiple meanings FAS in Japanese From the discussions we have made so far, it is now apparent how the multiple ambiguity in sentences with a frequency adverb is derived; while there exists ambiguity as to the interval of repetition vs. temporal point of a single event denoted by the temporal AdvP, different focus interpretation further adds to ambiguity. The last task left to us is how to deal with the `frequency’ meant by the adverbs involved. Åqvist et al. (1980) regards these adverbs, including always, sometimes, never, often, and seldom, as an operator which takes as its arguments two predicates. Each operator specifies a proportion between two sets of relevant temporal units. Based on this framework, we formulate the semantics of some representative frequency adverbs as follows: 
Centering theory (Grosz et al., 1995) tries to explain relations among attention, anaphora, and cohesion. It has two theoretical limitations. The first is the lack of a principle behind these discourse phenomena. The second is that the salience of discourse entities has not been quantitatively defined, although it plays a critical role in this theory. Hasida et al. (1995, 1996) propose the meaning game as a more principled model of intentional communication based on game theory, and claim that it can derive centering theory. This claim, however, has not yet been verified on the basis of substantial linguistic data. In this paper, we formulate salience as a measurable quantity in terms of a reference probability. We also formulate preferences subsuming centering theory under this quantitative formulation of salience. The preferences are derived from the meaning game and entail more general predictions than those of conventional centering theory. These formulations overcome the above limitations of centering theory. By following them, we empirically verify our generalization with a large Japanese corpus. The experimental results show that there is positive correlation between the salience (reference probability) of an entity and the simplicity (utility) of a noun phrase which refers to the entity. They also indicate correspondence between the values of expected utility and the ranking of the transition states. These results indicate that our generalization is appropriate.  1. Introduction Principled and quantitative modeling of discourse is important for analyzing and generating discourse. Centering theory (CT) is a model of discourse structures. It explains the relations among attention, anaphora, and cohesionç(Iida, 1997). However, CT has had two theoretical limitations. The first is the lack of a general principle behind the discourse phenomena. Although some studies on CT have focused on analyzing surficial linguistic features without general principles, we consider that the principle of discourse phenomena must be addressed based on measurable quantities. The second is that “salience”, which plays a critical role in CT, cannot be verified based on large linguistic data because it is not formulated as a measurable quantity, but as heuristic rules. We have investigated the general principle of CT. We adopted the meaning game (MG) (Hasida et al., 1995, 1996) framework because it gives a more principled explanation of the discourse phenomena than CTçdoes. MG is a model of intentional communication (e.g., anaphora) based on game theory. Game players in game theory correspond to interlocutors in MG, and they decide their intentions and interpretations at the Pareto-optimum. Although Hasida et al.ç(1995) claimed that CT could be derived from the MG by formulating salience in terms of a reference probability, their  claim has yet to be verified on the basis of substantial linguistic data. In this paper, we formulate the MG-based generalization of CT and verify it with a large corpus of Japanese newspaper articles. Furthermore, we quantitatively define salience by using multiple regression with a corpus for the MG-based generalization and for its verification. 2. Centering Theory and Its Two Issues  2.1 Centering Theory In CT, a discourse is represented as a sequence of utterances [U1, U2, … ,Un]. The “center” is a discourse entity which draws attention. The center is likely to be pronominalised. The “salience” represents the degree of attention to a discourse entity. The salience also represents the likelihood of pronominalization. The salience has been defined as a heuristic ranking in previous studies (see Section 2.2). Centers are categorized as follows:  ¾ Cb(Ui): The backward-looking center of the utterance Ui, which denotes the most salient discourse entity referenced in both the previous context and the current utterance Ui. ¾ Cf(Ui): The forward-looking centers of Ui, which denote a list of entities sorted by their salience. ¾ Cp(Ui): The preferred center of Ui, which is the most salient discourse entity in Cf(Ui).  CT embodies as the following rules (preferences) based on the heuristics definition of salience.  ¾ Rule 1 (pronominalization): If any element in Cf(Ui) is pronominalized, the Cb(Ui) is also pronominalized. ¾ Rule 2 (topic continuity): The transition states of centers between utterances (Table 1) are  preferred in the following order: CONTINUE > RETAIN > SMOOTH-SHIFT > ROUGH-  SHIFT.  Table 1: Transition states of centers between utterances  Cb(U i ) = Cb(U i−1 ) Cb(U i ) ≠ Cb(U i−1 )  Cb(U i ) = Cp(U i )  CONTINUE  SMOOTH-SHIFT  Cb(U i ) ≠ Cp(U i )  RETAIN  ROUGH-SHIFT  Rule 1 means that pronouns are more likely to refer to Cb than non-pronouns. Rule 2 represents the preference order among transition states according to the strength of topic continuity. 2.2 Two Issues Conventional CT studies face two limitations: 1. Lack of principles behind the rules. CT does not explain why the two rules occur in discourse phenomena. 2. Salience is formalized neither objectively nor quantitatively, but heuristically (e.g., Cf-ranking). Such ranking is non-falsifiable (unscientific) and cannot be verified against real linguistic data. The first limitation means that CT should have a hypothesis about the mechanisms behind discourse phenomena. The second limitation means that CT should be based on the quantitative definition of salience. Salience in CT is approximated by a heuristic ranking, called “Cf-ranking” (Walker et al., 1994), as follows:  English Cf-ranking: subject > object > indirect object > complement > adjunct Japanese Cf-ranking: topic (zero or grammatical) > subject > indirect object > object > others  Proceedings of PACLIC 19, the 19th Asia-Pacific Conference on Language, Information and Computation.  The above Cf-ranking depends on only grammatical function. While Strube et al. (1999) proposed an extended Cf-ranking integrated with information status and Nariyama (2001) proposed an extended ranking integrated with contextual information, these rankings are based on surficial observations without sufficient theoretical grounds. Although Poesio et al. (2004) discussed the parameters settings in CT, their discussion was also based on heuristic ranking. Besides this second limitation, we also note that heuristic ranking is difficult to integrate with other features that influence salience (e.g., distance between the current utterance and the latest expression referring to the target entity). We address the above two issues in the following sections. 3. Generalization of Centering Theory based on the Meaning Game The meaning game (MG) is a hypothesis about a model of intentional communication based on game theory (Hasida, 1996). We adopted MG to give CT a general principle. The MG-based account of anaphora is a more principled hypothesis than that of CT, because MG is based on the general principle of decision-making. In MG, the interlocutors’ expected utility is represented as: ∑ Pr(e)Ut(w). w refers to e Here, the Pr(e) is the reference probability of a discourse entity e, which is the probability that e will be referenced in the next utterance. Ut(w) is the utility of expression w that refers to e. The lower the cost of speaking or hearing w is, the higher Ut(w) becomes. Here, we assume that the value of Pr(e) is shared by interlocutors. Under this assumption, the solution which provides the maximum expected utility is the interlocutor’s Pareto-optimum because the expected utility is shared by them. We leave miscommunication out of consideration under that assumption. Hasida et al. (1995) suggested that Rule 1 and 2 of CT can be derived from MG only in a few particular cases. 3.1 Derivation of Preference 1a and 1b Rule 1 of CT is a preference about pronominalization. Hasida et al. derived Rule 1 from MG in the following case which involves little semantic bias, “he” tends to refer to “Fred”, and “the man” to “Max”.  U1: Fred scolded Max. U2: He was angry with the man.  They assumed the following inequations in this case.  Pr(“Fred”) > Pr(“Max”)  (Q A subject is more salient than an object )  Ut(“he”) > Ut(“the man”) (Q A pronoun costs less than a non-pronoun )  In this case, the interlocutors have two choices of anaphora. Hasida et al. indicated the above  Table 2: Correspondence between MG and CT concepts  MG  CT  Pr: Reference probability  Salience (Cf-ranking)  High-Pr discourse entity  Center  Ut: Utility of noun phrase  Simplicity of noun phrase  High-Ut noun phrase  Pronoun  Low-Ut noun phrase  Non-pronoun  Choice (A)  Choice (B)  Fred Max  Fred  Max  “he” “the man”  “he” “the man”  Pr(“Fred”)Ut(“he”) + Pr(“Max”)Ut(“the man”) > Pr(“Fred”)Ut(“the man”) + Pr(“Max”)Ut(“he”) Q (Pr(“Fred”) − Pr(“Max”)) (Ut(“he”) − Ut(“the man”)) > 0 Figure 1: Comparison of the expected utilities of two choices  Choice (A)  Candidates of  Anaphors  Antecedent  high a1  high  a2  c1  Choice (B)  Candidates of  Anaphors  Antecedent  high a1  high  a2  c1  low  c2  low  c2  Ut  Ut  low  low  Pr  Pr  Pr(c1)Ut(a1)+Pr(c2)Ut(a2) > Pr(c2)Ut(a1)+Pr(c1)Ut(a2) Q(Pr(c1) − Pr(c2)) (Ut(a1) − Ut(a2)) > 0  Figure 2: Preference 1a  semantic bias by comparing the expected utilities of the two choices. In other words, a solution of their MG model is that choice (A) is preferred over choice (B) in Figure 1. This solution conforms to a prediction using Rule 1. Thus, they claimed this thought experiment proves that rule 1 of CT can be derived from MG. Table 2 shows correspondence between MG and CT concepts. The above derivation, however, has neither been given a general formulation nor been verified on the basis of substantial linguistic data. An utterance in general examples possibly has a few anaphors and a lot of candidates of antecedent, whereas the above example case has only two anaphors and only two candidates of antecedent. Thus, general formulation is required before one can apply this model to real linguistic data. We generalize the above derivation to the following preference.  Preference 1a: When an utterance has multiple anaphors, an anaphor with a higher utility among them tends to refer to an entity with a higher reference probability.  Figure 2 illustrates Preference 1a we propose. In this example, choice (A) is preferred over choice (B). This is the preference in the cases of multiple anaphors in an utterance. Below, we generalize it to cases that do not depend on the number of anaphors in an utterance as follows:  Preference 1b: There is a positive correlation between the utility and the reference probability.  These preferences are based on a general principle. Moreover, the coverage of these preferences is wider and more general than that of Rule 1 of CT. Accordingly, these preferences we propose are generalizations of Rule 1.  Proceedings of PACLIC 19, the 19th Asia-Pacific Conference on Language, Information and Computation. 3.2 Derivation of Preference 2 Rule 2 of CT is a preference about local cohesion that indicates the level of topic continuity. Transition states are categorized into four types with respect to two conditions (Table 1). These four types have been heuristically ranked by local cohesion or topic continuity. The first condition, Cb(Ui) =Cb(Ui−1), means that the current utterance Ui inherits Cb from the previous utterance Ui−1. This condition corresponds to cohesion between Ui−1 and Ui. The second condition, Cb(Ui) =Cp(Ui), means that Cb(Ui) is the most salient entity in Ui . This condition corresponds to the prediction of cohesion between Ui and Ui+1 because Cp(Ui), the most salient entity in Ui, is the most likely one to be pronominalized in the following utterance Ui+1. We consider that the preference order of Rule 2 is attributed to expected utility. When the first condition holds, the reference probability of Cb is higher than when it does not hold. In this case, the utility of the anaphor referring to Cb also tends to become high because of Preference 1b, so that the expected utility is high. Similarly, when the second condition holds, the reference probability of Cb and the utility of Cb are high, thus, the expected utility is also high. Furthermore, the first condition has stronger influence than the second, because the first one represents the cohesion between the previous and the current utterances, whereas the second merely predicts the cohesion between the current and the next utterances. Consequently, RETAIN has a larger expected utility than SMOOTH-SHIFT. Rule 2 of CT can thus be derived from the general principle of maximum expected utility, which is stated as follows: Preference 2: The interlocutors prefer an interpretation of anaphora with higher expected utility. This preference is a generalization of Rule 2. We will verify the above preferences and provide evidence of the existence of the MG principle behind Rules 1 and 2 of CT in Section 5. 4. Definition and Measurement of Salience Salience represents the likelihood of a discourse entity to be pronominalized or its degree of attention. In CT, it is not quantitatively defined despite that it plays critical roles in the theory. Therefore, we formulate salience in terms of a reference probability — a measurable quantity. The salience of an entity can be defined based on how probable the entity will also be referenced in the following utterances. In other words, if an entity seems to be referenced in the following discourse, the entity tends to draw attention and its salience can be considered to be high. This formulation resolves the issues of CT that were discussed in Section 2. The salience of an entity e at the target utterance Ui is empirically defined as follows. Definition of Salience: The salience of e at Ui is defined as the reference probability Pr(e, Ui), which is the probability of e being referenced in the next utterance Ui+1. Given a large amount of linguistic data, Pr(e, Ui) can be calculated as follows: 1. Find the latest reference to e in the previous discourse [U1, …, Ui]. Let it be we. 2. Compose the feature vector feat(we,Ui) from we and [U1, …, Ui] . For example, the features we used in this study are listed in Table 3. 3. Extract samples (wx,Uj) whose feature vectors feat(wx,Uj) equal feat(we,Ui) from a large amount of linguistic data. 4. Using the extracted samples, calculate Pr(wx, Uj), the probability that the referent of wx is also referenced in Uj+1 (in other words, calculate the relative frequency of samples that the referent of wx is also referenced in Uj+1). 5. Take Pr(wx, Uj) to be Pr(e, Ui).  Dist (1) Gram Chain Exp last_topic (2) last_sbj p1 Pos  Table 3: Features used in regression analysis of Pr(e,Ui) log ( (# utterances between Ui and the latest reference to e ) + 1) grammatical function of the latest reference to e (wa/ga/no/o/ni/mo/de/kara/to) log ( (# references to e in the previous context of Ui) + 1) expression type of the latest reference to e (pronoun/non-pronoun) whether the latest reference to e was the last topic (yes/no) whether the latest reference to e was the last subject (yes/no) whether e was in the first person (yes/no) part of speech of the latest reference to e We used (1) for MLR, and (1) and (2) for SVR.  This definition is expressed by the following equation.  (Salience  of  e  at  Ui)  :=  Pr(e,U i  )  ≈  Pr(wx ,U  j  )  =  #{(wx ,U j ) ;C ∧ D} #{(wx ,U j );C}  Condition C: feat(wx,Uj) = feat(we, Ui) Condition D: The referent of wx is also referenced in Uj+1.  Hereafter, we explain the measurement of the salience of “Tom” at Ui for the following example.  U i-2: I saw Tom a little while ago.  U i-1: He seems to be sleepy.  U i : It was so hot last night,  U i+1:  .  Pr(“Tom”,Ui)  In this example, the anaphor “he” refers to “Tom” and it appears in the last position among expressions referring to “Tom” in the previous discourse. We call it the latest reference to “Tom”. To simplify the explanation, if the following three features are used, feat(“Tom”,Ui) is defined as (dist = 2, gram =subject, chain = 2). ¾ dist : Utterances between Ui and the latest reference to e. ¾ gram : Grammatical function of the latest reference to e. ¾ chain : References to e in the previous context of Ui We extract samples (wx,Uj) that have the same feature vector as feat(“Tom”,Ui) from a corpus and calculate the reference probability from these samples.  U j-k: …. U j-1: wx Uj : U j+1:  .  Condition C: feat(wx,Uj) = feat(“Tom”,Ui)  .  Condition D: The referent of wx is also referenced in Uj+1.  .  .  Pr(“Tom”,Ui)  ≈  Pr(wx ,U  j)  =  #{(wx ,U j ) ;C ∧ D} #{(wx ,U j );C}  Notice that interpolation and extrapolation are necessary because of data sparseness in the corpus. To this end, we used regression analysis for the measurements. We measured the reference probability with two regression algorithms: MLR (multiple logistic regression) and SVR (support vector regression). Table 3 lists the features for regression in this study.  Proceedings of PACLIC 19, the 19th Asia-Pacific Conference on Language, Information and Computation.  5. Empirical Verification of Meaning-Game-based Generalization We statistically verified Preference 1a, 1b, and 2 derived from MG. We used 1,356 articles taken from Japanese newspapers annotated with Global Document Annotation (GDA) (Hasida, 1998). These articles contained 63,562 utterances (predicate clauses). Table 4 shows the distribution of anaphora types in the corpus. To measure the reference probability, we extracted 1,073,781 samples of previously referenced entities for each utterance. Table 5 shows that there were 16,728 pairs of an utterance Ui and a previously referenced entity e that is also referenced in Ui (namely, that corpus contains 16,728 anaphors).  Table 4: Distribution of Japanese anaphora types  Anaphora Types  # Sample Ratio  Zero Pronoun  5876 35.1%  Pronoun  843 5.0%  Noun Phrase with Demonstrative  1011 6.0%  Other Noun Phrase  8998 53.8%  Total  16728 100.0%  Table 5: Percentage of samples in which previously referenced e is also referenced in Ui  # Sample (e, Ui) Percentage  e is referenced in Ui  16,728  1.6%  e is not referenced in Ui  1,057,053  98.4%  Total  1,073,781 100.0%  We assumed that the utility of pronouns is greater than that of non-pronouns; i.e., the utility of pronouns equals 2, and that of non-pronouns equals 1. This assumption is equivalent to distinguishing between pronouns and non-pronouns in CT.  5.1 Measurement of Salience as Reference Probability We measured reference probability, which is required in the verification of our MG-based generalization in Section 5.2. We used two regression algorithms for the measurement: MLR and SVR, which will be mentioned in Section 5.1.2 and 5.1.3, respectively. These regressions, especially MLR, require that their features must be numeric values. However, a grammatical function is not defined as numeric values. Therefore, we assigned numeric values to the  Table 6: Reference probabilities by only  Japanese grammatical functions (by particles)  Particle (Grammatical Function)  # Sample  Referenced in Ui+1  Reference Probability  wa (topic)  35,329  1,908 0.0540  ga (subject)  38,450  1,107 0.0288  no (of)  88,695  1,755 0.0198  o (direct object)  50,217  898 0.0179  ni (indirect object)  46,058  569 0.0124  mo  8,710  105 0.0121  de  24,142  267 0.0111  kara  7,963  76 0.00954  to  19,383  129 0.00666  Other particles  512,006  8,027 0.0157  No particle  153,197  1,315 0.00858  grammatical functions as a preparation for the multiple regressions in Section 5.1.1. After these regressions, we will also reconsider the conventional Japanese Cf-ranking based on the assigned values for grammatical functions in Section 5.1.4. 5.1.1 Assigning Numeric Values to Grammatical Functions We measured the reference probabilities by using only grammatical functions for enabling the regression. This preparative measurement involves not regression but counting samples. Table 6 shows the reference probabilities calculated by from only the grammatical functions existing in the corpus. We used these values as gram in the multiple regression of the reference probability. 5.1.2 Measurement with MLR MLR model is based on an assumption that the log odds of probability, log(P (1 − P)) , of some kind of event can be expressed as a linear expression of the explanatory variables. The regression function with three features in Table 3 is Pr = (1 + exp(−λ))−1 = (1 + exp(−(b0 + b1dist + b2 gram + b3chain)))−1 .  It takes a huge amount of time to perform MLR on 1,073,781 samples. Thus, we made five regression models using 12,000 subsamples per model. We used statistical software called R (R Development Core Team, 2002) for the MLR analysis. Table 7 shows the parameters of the five regression models. We used average of probabilities predicted by the five models as the reference probability, which is represented as follows:  ∑ Prç  =  
 Seed Rule Generator Seed rules Rule Generalization Generalized Mapping Rule  Figure 1: Architecture for Learning Module.  Each sentence pair goes through the process of seed rule generation. First, the sentence pair is automatically aligned word by word based on the lexicon. The alignment algorithm uses the parse tree and the Filipino sentence. The parse tree is traversed in order to reach the leaf nodes containing the English word. The English word is compared to each word in the Filipino sentence. If a match is found, the Filipino word is mapped and tagged with the information attached in the English word. Figure 2 illustrates the alignment.  IC (1) (2) (3) (4) (5) (6) English Sentence: The girl went to the market  NP  V  Filipino Sentence: Pumunta ang babae sa palengke  (1) (2) (3) (4) (5)  det The: det  verb NPh go: verb Form:past NL noun girl: noun  PP  prep  NP  to: prep NPh  det  NL  Filipino Word Pumunta ang babae sa palengke  Index English Filipino  3  
 This paper explores the semantic properties and extension paths of modern Mandarin Chinese spatial terms nei4 內‘in, inside’ and wai4 外‘out, outside’. Their symmetric and asymmetric contrasts are accounted for with idealized cognitive models (ICMs) and metonymic models. These models are used to delineate the core meaning and main meaning extension routes of nei4 and wai4: from the area of a region to the length of the radius or side, and then to distance. Locations of the observer, marked foci, container metaphors and social backgrounds also contribute to the asymmetry of the two terms. The synchronic difference between nei4 and wai4 can also be attributed to diachronic changes.  1. Introduction 1.1 Symmetry and Asymmetry of nei4 and wai41 Nei4 內 ‘in’ and wai4 外 ‘out’ are two Mandarin location terms that represent speakers’ conceptualization of space. It is shown that they behave similarly and share an extensive set of collocation patterns as in (1).  1a. wu1nei2 屋內 ‘in the house/room’ b. nei4ce4 內側 ‘inside’ c. nei4xiao1 內銷 ‘sell domestically’ d. zai4 huang2gong1 zhi1 nei4 在皇宮之內 ‘in the palace’ e. yi4bai3 kong1chi3 yi3nei4 一百公尺以內 ‘within one hundred meters’  wu1wai4 屋外 ‘out of the house/room’ wai4ce4 外側 ‘outside’ wai4xiao1 外銷 ‘export’ zai4 huang2gong1 zhi1 wai4 在皇宮之外 ‘out of the palace’. yi4bai3 kong1chi3 yi3wai4 一百公尺以外 ‘beyond one hundred meters’  
Bare NP Demonstrative Demonstrative Zero NP  Determiner +N Pronouns  sono N ’that N’ sore ‘that’  (%)  (%)  soko ’there’ (%)  (%)  2 (100.0)  0  0  0  5 (83.3)  
where P∂ (S |W ) is the probability of input speech given a word sequence W.  A language model is regarded as the probability distribution over events or token sequences  (texts) that models how often each sequence occurs as a sentence. Chain rule is used to  decompose probability prediction:  * Correspondence author.  P(w1m)=P(w1w2..w. m)=P(w1)P(w2 | w11)P(w3 | w12)..P.(wm| w1m−1)  m  ∏ =P(w1) P(wi | w1i−1).  (2)  i =2  where w1m denotes the word sequence with m words.  Speech S features selecting  word-match W  selected words w sentence-match  α acoustic model  P( ) languagemodel  Fig. 1: LMs in speech recognition system.  1.2 n-gram Model  Because of the finite training corpora in real world and to reduce the parameter space of word feature in languages, the approximate probability of a given word by using the (n-1)th preceding words is used to estimate sequence W.  The probability model with various n can be written:  m  ∏ P( w1m ) ≅ P( w1 ) P( wi | wi−n+1).  (3)  where wi-n+1 deni=o1 tes the history of n-1 word for word wi.  In many applications, the models for n=1, 2 and 3 are called unigram, bigram and trigram  models [1], [8] and [16], respectively.  In Eq. (3), the probability for each event or token can be obtained by training the bigram  model (for clarity, bigram model is illustrated). Therefore the probability of a word bigram will  be written as:  ∑ P(wi | wi−1 ) =  C(wi−1wi ) , w C(wi −1w)  (4)  where C(wi) is the count of word wi appeared in training corpus. The probability P of Eq. (4) refers to the relative frequency and such method is called maximum likelihood estimation (MLE).  1.3 Smoothing Issue in Language Models As shown in Eq. (4), C( ) of a novel word, which don’t occur in the corpus, may be zero because of the limited training data and infinite language. It is always hard for us to collect sufficient datum. The potential issue of MLE is that the probability for unseen events is exactly  Proceedings of PACLIC 19, the 19th Asia-Pacific Conference on Language, Information and Computation. zero. This is so-called the zero-count problem. It is obvious that zero count will lead to the zero probability of P( ) in Eqs. (3) and (4). The prediction of zero probability of certain event is unreliable and unfeasible for most applications, especially for language models. The smoothing techniques [3], [4], [11] and [19], are essential and employed by language mode to overcome the issue zero count of traditional language models, as described above. There are many smoothing methods, such as Add-1, Good-Turing [6], deleted interpolation [7], Katz [13], etc. There are several literatures discussing about smoothing methods [3], [4], [12], [14], [15], [16] and [18]. 2. Smoothing Processes in LMs The adjustment of smoothed probability for all possibly occurred events involves discounting and redistributing processes: 2.1 Discounting Process The probability of all seen and unseen events is summed to be one (unity). First operation of smoothing method is the discounting process, which discount the probability of all seen events. It means that the probability of seen events will be decreased a bit. The adjustment can be divided into two types: static and dynamic. Static smoothing methods, as most smoothing methods, discount the probability based on the frequenc y of events in trained corpus. However, dynamic smoothing method, i.e., cached-based language, discounts the probability based on the frequency of seen events in cache and trained corpus. 2.2 Redistributing Process In this operation of smoothing algorithm, the escape probability Pesc obtained from all seen events will be redistributed to all unseen events. Pesc is usually shared by all the unseen events. That is, Pesc is redistributed uniformly to each unseen event, Pesc/U, where U is the number of unseen events of a language model. In other hand, each unseen event obtains same probability based on the uniform distribution. The redistribution process of most well known smoothing methods, such as Add-1, Absolute discounting, Good-Turing, Delete interpolation, Back-off and Witten-Bell, and so on. The escape probability Pesc (or called probability mass assigned to all unseen events) is uniformly shared by all unseen events. It is a possible factor that affects the performance of smoothing algorithm. There are little previous papers discussing how to redistribute the escape probability Pesc, and how the different redistribution can improve the smoothing methods for language models. 3. Improving the Smoothing Process 3.1 Interval Behavior of Seen Events Count As described in the section 2, there are two main processes for smoothing methods; discounting and redistributing. In the redistributed process, the escape probability Pesc is shared uniformly by all unseen events for most smoothing methods, each event obtain same smoothed probability Pesc/U. Based on the observation of behaviors for seen events, each event has its probability relying on the event frequency in the training corpus. It is obvious that the probability distribution  for each event is quite different. Therefore, It is unreasonable to assign same probability to each incoming unseen events. As shown in Fig. 2, the figures draw the frequency interval (offset) between two new successive events for two models; Chinese character word unigrams and bigrams. There are 100M (108) Chinese characters for source training data. The sentences in source are segmented into words and 65M (65*106) words are obtained. The length of word is 1.45 Chinese characters per word in average. The recourse files are randomly selected and we obtain the offset diagrams. More than 100 training processes are implemented and then the final curve can be obtained in average. The regression curves Y1 and Y2 for Chinese word unigram and character bigram models can be described as follows: Y1 = 1E-10x3 - 4E-06x2 + 0.0307x - 39.825 Y2 = -1E-16x4 + 2E-11x3 - 6E-07x2 + 0.0058x - 3.7502 where x and y denotes the data size the offset. An idea for redistributing escape probability Pesc is that how many tokens read- in while the next new event will occur? It means the count interval between two successive events, which vary usually with the training data N. Basically, the larger the training data N, the larger, and the interval. In the beginning of training phase, next new events will occur in short interval of count. It means that next new event will occur rapidly at smaller N while slowly at larger N. The larger the training data N is, the larger the offset (interval) is. It is apparent that the. The regression curves present the general interval of original intervals and its trend increased gradually. Note that the regression curves varied with N and flatter at the beginning and steeply at end of curves. Fig. 2: the interval (offset) between two successive events varied with training data; (left) word unigrams, (right) character bigrams.  Proceedings of PACLIC 19, the 19th Asia-Pacific Conference on Language, Information and Computation.  3.2 Redistributing Process for Unseen Events  As described above, the regression curves from seen events can be used to demonstrate the  interval of unseen events. Based on the cur ves derived from the seen events occurrence, we can  furthermore derive the behaviors for estimating the probability assigning to the next incoming  unseen event. Note that all the probability for seen and unseen events should be unity; which  must satisfy the basic statistical condition.  Supposed that the interval yi on training data Ni, the distribution for all unseen events can be  as follows:  
This paper describes a text retrieval system for cellular phones with Web browsing capability, which accepts spoken queries over the cellular phone and provides the search result on the cellular phone screen. This system recognizes spoken queries by large vocabulary continuous speech recognition (LVCSR), retrieves relevant document by text retrieval, and provides the search result on the World Wide Web by the integration of the Web and the voice systems. The text retrieval in this system improves the performance for spoken short queries by: 1) utilizing word pairs with dependency relations, 2) distinguishing affirmative and negative expressions, and 3) converging synonyms. The LVCSR in this system shows enough performance level for speech over the cellular phone with acoustic and language models derived from a query corpus with target contents. The system constructed for user’s manual for a cellular phone navigates users to relevant passages for 81.4% of spoken queries.  1. Introduction Cellular phones are now widely used and those with Web browsing capability are becoming very popular. Users can easily browse information provided on the World Wide Web such as news, weather, and traffic report with the cellular phone screen in mobile environment. However, obtaining necessary information from large database such as user’s manual or travelers’ guide is quite a task for users since searching for appropriate information from seas of data requires cumbersome key operations. In most cases, users have to carefully navigate through deep hierarchical structures of menus or have to type in complex combination of keys to enter some keywords. Text retrieval by voice input is one of the solutions for this problem. This paper presents a telephone-based voice query retrieval system in Japanese which enables cellular phone users to search through the user’s manual. This system accepts spoken queries over the cellular phone with large vocabulary continuous speech recognition (LVCSR) and retrieves relevant parts from the user’s manual with text retrieval. The results are provided to the user as a Web page by synchronously activating the Web and the voice systems (Yoshida et al., 2002). Users can input queries without complicated keystrokes and can view the list of results on the cellular phone screen. With respect to voice input systems, a large number of interactive voice responses (IVR) systems and spoken dialogue systems has been designed and developed over the years (Zue, 1997). As for user’s manual retrieval systems which accept voice input, Kawahara et al. (2003) has developed a spoken dialogue system for appliance manuals. However, they mainly focus on the dialogue strategy to select the appropriate result on screen-less systems such as VTR and FAX. On the other hand, retrieval methods for voice input have been examined on a TREC query set (Barnett et al., 1997; Crestani, 2000).  The Internet  Web Service Module  Telephone Network  The Internet  Telephone Service Module  Text Retrieval Module LVCSR Module  User’s Manual Index  Figure 1: The configuration of the prototype system.  Figure 2: The screen of the cellular phone displaying the search result.  . However, text retrieval in TREC mainly aims to search open domain documents from long queries, while our system is required to search closed domain documents such as user’s manuals based on short queries spoken over the cellular phone. In order to apply text retrieval technique to speech-activated user’s manual retrieval, we have investigated queries for searching manuals in addition to the text of the manuals from a linguistic viewpoint. We found that text retrieval for a user’s manual has the following three difficulties. 1) The difficulty of identifying passages in a user’s manual based on an individual word. 2) The difficulty of distinguishing affirmative and negative sentences which mean two different features in the manual. 3) The difficulty of retrieving appropriate passages for a query using words not appearing in the manual. This paper presents how we overcome these difficulties using three techniques: 1) utilizing word pairs with dependency relations, 2) distinguishing affirmative and negative expressions by auxiliary verbs, and 3) converging synonyms with synonym dictionary. The rest of the paper is organized as follows. Section 2 describes the system configuration of our speech-activated text retrieval system and how it works. Section 3 discusses the difficulties in text retrieval in our system and presents our proposed techniques in detail. Section 4 shows the developed prototype system and Section 5 reports its evaluation results. Finally Section 6 concludes the paper. 2. Speech-Activated Text Retrieval System Our system receives spoken queries on the usage of the cellular phone and provides the list of relevant passages in the user’s manual. In this paper, a passage denotes a part of the document corresponding to a feature in the user’s manual. 2.1. System Configuration Figure 1 shows the configuration of our retrieval system. The telephone service module receives a phone call from the user. This module prepares the search operation by calling the LVCSR module, which recognizes the query spoken over the phone, and the text retrieval module, which provides the search result for the query.  Proceedings of PACLIC 19, the 19th Asia-Pacific Conference on Language, Information and Computation.  Figure 3: The main page of our system.  Figure 4: The result page displaying the title list of top ten results for the query.  Figure 5: The body of the passage displayed when the user selects the title in Figure 4.  The telephone service module sends the list of the relevant passages to the Web service module, and then hangs up the phone. The Web service module provides the result to the user according to the user’s request via the internet. We assume that the cellular phone screen displays about 30 letters per line and 15 lines of text according to the specifications of recent popular cellular phones in Japan. We assign top ten potential passages as the search result and display the title of them in order for the user to see with ease. Figure 2 shows the screen of the cellular phone displaying the search result.  2.2. Example of Using the System This section describes how our system works. Our system works in Japanese, but in the following section, English translation is provided for the reader’s convenience. In our system, the user obtains the relevant passage in the user’s manual with the voice query according to the following steps. Step 1: The user first accesses the system’s main page of our system with the cellular phone (Figure 3). The page contains two hyperlinks along with brief instructions and query examples. Step 2: The user follows the first link labeled “Input query by voice.” It is linked to the telephone service module, allowing the user to call the telephone service module. Step 3: The user inputs a query following the voice guidance from the system. The LVCSR module recognizes it and outputs the result text. The text retrieval module searches the user’s manual from recognized text and outputs the top ten results. The user goes back to the main page after the telephone service module hangs up the phone. Step 4: The user follows the second link labeled “Show search results,” which is linked to our Web service module. Then the user views the result page which contains the title list of top ten results (each passage consists of a title and a body). Figure 4 shows the example of the result page responding to the voice query “How to change my email address.” Step 5: By selecting a title of a passage from the result list, the user retrieves the corresponding body of the passage (Figure 5). If the result list contains no relevant passages, the user can go back to the homepage and re-enter a query by speech.  3. Text Retrieval for a User’s Manual 3.1. The Problems on User’s Manual Retrieval In general, user’s manual of equipment explains all functions extensively. Since the phrasing used in a user’s manual is often similar, expressions with small difference might appear in completely different entries. We have investigated queries for searching manuals in addition to the text of the manuals from a linguistic viewpoint and found that text retrieval for user’s manual has the following three difficulties. 1) It is difficult to identify passages in a user’s manual based on an individual word. For example, a word “mail” shows up in passages explaining various functions such as sending mails, receiving mails, composing mails, and many others. In order to overcome this difficulty, we need to use relations between words. 2) It is difficult to distinguish affirmative and negative sentences based on independent words. Sentences with the same set of content words can mean two different features depending on whether the sentence is in the affirmative or in the negative. This is often true in manual writings where each function is described in pair: one activating and the other deactivating the function (ex. “Sending the caller number” and “Not sending the caller number”). In order to overcome this difficulty, we need to handle polarity indicated by auxiliary verbs. 3) It is difficult to retrieve appropriate passages for a query using words not appearing in the manual. While the expression denoting an object is generally standardized in a user’s manual, users often indicate the object with other expressions. In order to overcome this difficulty, we need to assimilate difference of various synonymous expressions. 3.2. The Approaches for User’s Manual Retrieval The system retrieves relevant passages from the user’s manual with a word-based text retrieval method. The system generates indexes for content words in passages and obtains relevant passages from the words in the query based on Okapi BM25 probabilistic retrieval model without relevance feedback in principle (Robertson et al., 1993). In this model, the weight W of a passage P for a query Q is defined as follows: W   TW (T ) TQ TW (T )  w  (k1  1)  tf  (k2  1)  qtf k1  K  tf k2  qtf w  log N  n  0.5 n  0.5 K  (1  b)  b  PL AVPL Here T denotes a term in the query Q, N denotes the number of passages in the whole text, n denotes the number of passages containing the term T, tf denotes the frequency of occurrence of the term T within the passage P, qtf denotes the frequency of occurrence of the term T within the query Q, PL denotes the length of the passage P, and AVPL denotes the average length of all passages. k1, k2, and b are predefined constants.  Proceedings of PACLIC 19, the 19th Asia-Pacific Conference on Language, Information and Computation.  Table 1: An example of a synonym dictionary  Standard Expression saito (site) chakushin’on (ringtone) ridaiaru (redial)  Synonymous Expressions  webu  hômupêji  (web)  (homepage)  chakushinmerodî  yobidashion  (ring melody)  (phone beep)  môichido  kakeru  (again  call)  In order to overcome the difficulties stated previously, we have expanded the retrieval model with the following three techniques.  1) Utilization of word pairs with dependency relations This technique assigns larger weight for passages including the same word pairs with dependency relations as in the query. The system uses the following weight Wwp, which is simple extension of W:  Wwp    k NP wp    W  where NP denotes the number of word pairs which appear both in the passage P and the query Q with dependency relations. kwp is predefined constants. We detect the dependency between words by shallow dependency analysis without parsing. The system assigns depend-to and depend-from attributes to each word based on its part of speech and connects them according to the surrounding relationship (Satoh et al., 2003).  2) Distinction between the negative and the affirmative phrases by auxiliary verbs This technique assigns the different weight on the term according to the condition whether an auxiliary verb indicating negative polarity follows after the term. The system adds this condition to each word after morphological analysis, and distinguishes words with different conditions. The system uses the following weight Waux instead of W:    Waux  TW (T  )  kaux TW (T  ) T Q     TW (T  )  kaux TW (T  ) T Q  where T+ denotes the term T with this condition and T-denotes the term T without this condition. kaux is predefined constants. 3) Converging synonyms This technique assumes the occurrence of synonymous expressions for a word as the occurrence of the word itself in calculating the weight. The system converges various synonymous expressions into the standard expression by using predefined synonym dictionary. The system accepts a set of words with dependency relations as a synonymous expression in order to converge complex synonymous expressions. Table 1 shows an example of a synonym dictionary. An arrow sign denotes a dependency relation between words.  4. Prototype System We have constructed a prototype system to search through the manuals for cellular phone users (Ishikawa et al., 2004). The user’s manual contains about 14,000 passages and consists of about 4,000 unique words. The prototype system works in real time according to the user’s operation. 4.1. LVCSR Module 4.1.1. Language Model A statistical language model (LM) with word and class n-gram estimates is used in our system. Word 3-gram is backed off to word 2-gram, and word 2-gram is backed off to class 2-gram. Partof-speech patterns are used as the classes of each word. The LM is trained on a text corpus of query samples for our target user’s manual. Nouns in the manual document are added to the recognition dictionary apart from the training. A total of 15,000 queries were manually constructed and used for training the LM. The final LM for the prototype system has about 4,000 words in the recognition vocabulary, about 20,000 word 2gram entries, and about 40,000 word 3-gram entries. 4.1.2. Acoustic Model A speech signal is sampled at 8kHz, with MFCC analysis frame rate of 10ms. Spectral subtraction (SS) is applied to remove stationary additive noises. The feature set includes MFCC, pitch, and energy with their time derivatives. The LVCSR decoder supports triphone HMMs with tree-based state clustering on phonetic contexts. The state emission probability is represented by Gaussian mixtures with diagonal covariance matrices. For the prototype system, Gender-dependent acoustic models were prepared by the training on the speech corpus with 200,000 sentences read by 1,385 speakers collected through telephone line. 4.1.3. LVCSR Decoder The LVCSR decoder recognizes the query utterances with the triphone acoustic model, the statistical language model, and a tree-structured word dictionary. It performs two-stage processing. On the first stage, input speech is decoded by frame-synchronous beam search to generate a word candidate graph using the acoustic model, 2-gram language model, and the word dictionary. On the second stage, the graph is searched to find the optimal word sequence using the 3-gram language model. Both male and female acoustic models are used and decoding is performed independently for each model except for the common beam pruning in every frame. Recognition results by male and female acoustic models are compared and the one with better score is used as the result. Gender-dependent models improve the recognition accuracy while curbing the increase of the computational amount by common beam pruning. 4.2. Text Retrieval Module All the techniques described in Section 3.2 are implemented on the text retrieval module in the system. We fixed the constants as follows according to the preliminary experiments using query samples developed for training the LM: k1  100, k2  1000, b  0.3, kwp  1.3, kaux  0.3 We developed the synonym dictionary with about 500 entries to converge synonymous expressions used to describe cellular phone functions.  Proceedings of PACLIC 19, the 19th Asia-Pacific Conference on Language, Information and Computation.  Table 2: Examples of queries used for evaluation. Shashin-o mêru-de okuritai (I want to send a picture via email) Aikon-o desukutoppu-ni tôroku shitai (I want to register a new icon on the desktop) Jushin-shita mêru-o minagara henshin mêru-o sakusei-suru hôhô (How to write a reply mail while looking at the incoming mail)  Table 3: The retrieval success rate for the transcriptions of queries.  Number of Result Passages 
In this paper, we propose a constrained finite-state model, named cfsm, for Korean morphotactics and attempt to show how it can successfully treat some major morphological problems in Korean. As a preliminary descriptive framework, this model adopts the Korean morphological system Komor by Lee (1999) to lay out some basic problems in Korean morphotactics and describe linear approaches to their possible solutions. This descriptive step is then followed by various testing steps executed by using Xerox’s finite-state development tools, namely xfst for creating finite-state networks and lexc specifying natural language lexicons. With Komor’s constraints represented in feature structures and appropriately implmented into xfst and by making Komor run on xfst, the proposed cfsm is expected to fully benefit from the descriptive groundwork of Komor and the finite-state processing power of xfst.  1. Aim and Approach This work aims at developing a constraint-based finite-state model named cfsm for Korean morphology. For this purpose, it re-implements Lee’s (1999) Korean morphology system Komor, which had been implemented with a C-augmented grammar tools named MALAGA1, by using Xerox’s finite-state development tools, namely xfst for creating finite-state networks and lexc specifying natural language lexicons. With Komor’s constraints represented in feature structures and appropriately implmented into xfst and by making Komor run on xfst, the proposed cfsm is expected to fully benefit from the descriptive groundwork of Komor and the finite-state processing power of xfst. In designing and implementing our proposed model cfsm, we strictly adhere to the principle of possible continuation that has been advocated by Hausser (1989) and Beesley and Karttunen (2003) in recent years. The main operation in executing cfsm is concatenation (without backtracking), but strictly constrained by some requirement conditions on feature-value matching. In Korean, for instance, noun and verbal stems concatenate with a sequence of their suffixal particles or endings to form well-formed word forms. But this concatenation is often constrained by their particular syllable structure or degree of regularity. For example, a verbal stem ‘먹 mek’ may simply concatenate with a tense marker to form another stem, but its syllable structure may restrict the choice of its ending, thus allowing the stem ‘먹었 mek.ess’ only. This linear approach with necessary constraints is, however, considered adequate especially in treating the morphology of agglutinative languages like Korean. 
2.2. IE Data Anomaly Problem Ideally, data cleansing for IE results should be able to detect and remove all four types of data errors. However, the textual nature of IE results presents much more obstacles for data analysis than numeric data. Among the four types of data anomalies, duplicates are easier to detect with information of key attributes. Missing values are mostly dependent on the availability of integrity constraint. If an attribute is known to be required, an empty slot provides a straightforward indication of a missing value. Otherwise, it is very hard to detect anything as missing when there is no information on its existence. The same applies to missing entities, which is not possible to detect by analyzing IE results alone. Our primary focus will be on detecting invalid values. Given an initial textual database loaded by an IE task, we address the problem of analyzing the actual data content and detecting values that are either syntactically or semantically invalid. We consider a simplified database in the form of a table where each row is a subject entity and each column corresponds to an attribute. Each attribute may  Proceedings of PACLIC 19, the 19th Asia-Pacific Conference on Language, Information and Computation. or may not associate with integrity constraints defined by a-prior knowledge of the task domain. The goal is to detect each and every invalid value in the textual database. Such a detection capability offers significant help to ensuring high-quality IE applications and is the beginning step towards more complete data cleansing with anomaly removal and correction. 3. Featured-based Detection on Invalid Values We propose a set of generic string features that can be used to classify textual data into groups with different feature values. Each string feature describes a certain characteristics of a string. Ideally, invalid data will be exclusively separated out from normal data based on their unique string feature values. We observe that attribute values of different entities are only various instantiations of the same concept. Values in each column can be considered as the same class of textual string that shares certain characteristics. String features provide a basis to find the common characteristics of a class of textual strings. Groups with atypical string feature values indicate their possible anomaly. 3.1. String Features A textual string, as an attribute value in IE results, is usually a word or a sequence of words that provides a unit of information pertaining to a subject entity. In linguistics, morphology concerns rules of word formation. A word has a form at the surface level and a meaning at the content level. Word meaning classification requires dictionary, which is subject dependent and does not work for entity name. On the other hand, word forms allow direct processing to determine its surface properties. Therefore, we propose to detect anomaly of attribute textual values based on surface forms of words. Our basic assumption is that certain surface properties of words would help provide differentiable characteristics between normal and abnormal values. Invalid attribute values would be revealed by their uncharacteristic surface features. We define a set of string features that characterize a textual string’s surface form. Each string feature describes a surface property of a string without concerning its literal meaning. Depending on the text language and the subject domain, string features can be defined at the character level or the word level. We will focus on the character-leveled features and their applications on Chinese IE. The same principles should be applicable to word-leveled features and other languages. We define a set of six string features, SF = (Sc, Sp, Ss, Se, Sl, Sn), that are used to evaluate each value vi in an n-tuple. Given a set of n-tuple, r = {t1, t2, …, tm}, in the database initially loaded by an IE task, we will apply the set of string features to the set of n-tuple such that each value vi in the database is evaluated by six string features. In other words, SF(r) = (Sc (vi), Sp(vi), Ss(vi), Se(vi), Sl(vi), Sn(vi)), for each and every vi in the database. The set of string features are specified as follows. z String cardinality, denoted by Sc: the number of characters in the string. z String k-prefix, denoted by Sp: the first k characters of a string, where k is a design parameter. z String k-suffix, denoted by Ss: the last k characters of a string, where k is a design parameter. z String entity, denoted by Se: the full range of characters of a string. z String lexicon, denoted by Sl: a true/false evaluation on whether the string matches with any of the known lexicon associated with a corresponding attribute. z String numeral, denoted by Sn: a true/false evaluation on whether the string contains any numeral symbol. 3.2. Attribute Constraints Assuming that domain knowledge of IE task subject is given, constraints on attribute values may be specified based on string features. Each string feature provides a potential classifying and screening criterion on data validity for an attribute. Recall that each attribute represents a concept of IE subject and attribute values are instantiations of the same concept in various symbolic forms. Whether the various valid symbolic forms of an attribute converge on certain string feature values depends on the nature of the corresponding concept. Some attributes may be associated with  effective criteria on a single string feature, while others may need logical combinations of multiple string features to enable appropriate screening. Still other attributes may be difficult to find reliable string features at all. Nevertheless, string feature constraints, when available, provide a convenient and potentially effective way to discriminate invalid data. The approach to examining string features of an attribute value against its known constrain for validity discrimination is both simple and straightforward. In addition, string feature constraints may be effective in detecting obvious errors due to IE’s mistaken selection and insertion operations. However, the approach is not applicable to all attributes. It also requires human intervention in specifying known constraints. Finally, the approach may not be able to detect errors that are semantically incorrect but appear to have the right surface forms. 3.3. Majority Rule Another approach is to adopt common string features of attributes based on group majority as classification standards. All the values of an attribute (a column of the database table) are considered as a population to be discriminated based on majority and minority. Each string feature provides some differentiating characteristics that divide the whole population into one larger group and one smaller group. Attribute values in the smaller group, as minority, are discriminated as invalid data based on majority rule. It is expected that some string features work better than others on certain attributes. The accuracy of detecting invalid data also depends on the boundary between majority and minority. There are a number of strategies to draw the line between majority and minority. The most straightforward way is to calculate the occurrence percentage of each feature value for each string feature and rank them from the least to the most. Based on the ordered percentage distribution and a given threshold on accumulated percentages, a line can be drawn to separate minority from majority. For example, if a threshold of 20% is given, the subset of less significant groups with accumulated percentages less than 20% will be discriminated as minority. All other more significant groups are regarded as majority. Overall, the approach to derive classification standards based on majority rule requires no prior knowledge on subject domains and allows fully automatic processing. However, the assumption here is the percentage of invalid data is significantly less than that of valid data. Otherwise, it would be difficult to identify anomaly based on the notion of uncharacteristic features as minority. From the standpoint of data cleansing, the assumption is reasonable since it may not be suitable, in the first place, to apply data cleansing to a database with very high percentages of incorrect data. Another potential problem is the performance variations of pairs of string feature and attribute. For each attribute, the best string feature for detecting invalid data may be different. 4. Experimental Evaluation We apply the feature-based detection approach to a set of IE task results and evaluate its performance. The IE task domain is government personnel changes. The original documents are government gazettes publishing government personnel directives issued by the reigning President. Each government personnel directive authorizes government post appointment and dismissal of the named officials. The IE task is to transform government personnel directives into a set of structured information on the subject, such as person name, government unit name, position title, rank, type of changes, date, etc. 4.1. Performance Measures With string features as the basis of error detection, we are essentially constructing binary classifiers that indicate the validity/anomaly of data. Each string feature can be used directly as a simple binary classifier. Or we can develop more enhanced classifiers by combining multiple string features. The standard performance measures for binary classifiers are summarized in a 2 x 2 confusion matrix, shown in Table 1.  Proceedings of PACLIC 19, the 19th Asia-Pacific Conference on Language, Information and Computation.  Actual Yes Class Actual No Class  Table 1: 2 x 2 confusion matrix  Classified as Yes  Classified as No  Number of True Positive (TP) Number of False Negative (FN)  Number of False Positive (FP) Number of True Negative (TN)  From this matrix a number of standard metrics are specified to measure classification performance. Theses metrics are true positive rate, true negative rate, false positive rate, and false negative rate. zTrue positive rate: TP-rate = TP/(TP+FN) is the percentage of positive cases correctly classified as belonging to the positive class; zTrue negative rate: TN-rate = TN/(FP+TN) is the percentage of negative cases correctly classified as belonging to the negative class; zFalse positive rate: FP-rate = FP/(FP+TN) = 1 – TN is the percentage of negative cases misclassified as belonging to the positive class; zFalse negative rate: FN-rate = FN/(TP+FN) = 1 – TP is the percentage of positive cases misclassified as belonging to the negative class; These four metrics measure the classification performance on the positive and negative classes independently, therefore, support objective evaluation even under skewed class distributions, as in our data set. The main objective of a classifier is to maximize the true positive and true negative rates. A perfect classifier will have true positive and true negative rates of 1. However, for most real world applications, there is always a tradeoff between TP-rate and TN-rate. The ROC (Receiver Operating Characteristic) graph can be used to characterize the tradeoff relationship between TP-rate and TN-rate of a classifier. On a ROC graph, TP-rate is plotted on the Y-axis and FP-rate is plotted on the X-axis. The performance of a binary classifier, depicted by a (TP-rate, FP-rate) pair, corresponds to a point on the graph. The lower left point (0, 0) represents a classifier that never classifies a case as positive class. The opposite classifier, one that always classifies a case as positive class, is represented by the upper right point (1, 1). The upper left point (0, 1) represents a perfect classifier. The diagonal line x = y represents a classifier with a random guessing strategy. Any useful classifier must produce a point above the diagonal line. In general, a classifier is better than another if it produces a point closer to the northwest corner. For classifiers with parameters, different settings produce different ROC points. Plotting all the ROC points that can be produced by varying the parameters produces a ROC curve, representing the overall performance of the classifier. 4.2. Experimental Design In order to evaluate our approach to feature-based detection for invalid textual data, we use a sampled subset of the initial database as our test data. The test data contains a total of 150,752 textual string values, with approximately 2.2% error rate. Our primary goal of the experiment is to assess the utility of each string feature and the overall performance of the approach in detecting invalid textual data from IE results. We observe that values of an attribute in IE results are symbolic form variations of a concept in the IE task subject. Our fundamental hypothesis is that certain string features are shared by string variations representing the same concept. These common string features may be found at the string’s surface form level. As a result, effective detection of IE invalid values can be achieved by using the surface-form string features. The experiments are intended to verify the hypothesis and derive information on the overall performance of the approach. We conducted two sets of experiments. In the first set of experiments, we apply subject domain knowledge to specify attribute constraints with selected string features. Classification performance measures are collected. The results provide information for us to analyze the applicability relationships between string feature types and attribute types. In the second set of experiments, it is assumed that no prior domain knowledge is available. Majority rule is applied for classification. We examine its performance and analyze the conditions of applicability. The combined results of the  two sets of experiments provide empirical evidence for us to suggest the overall utility of string feature based detection for IE results.  4.3. Performance of Attribute Constraints There are a total of 16 attributes in the domain of government personnel directives. Although the entire database was tested for anomaly detection, we only report performance measures on four representative attributes based on their distinguished content characteristics. In general, each attribute can be examined by any of the string features we defined. However, not all pairs provide useful anomaly detection. With subject domain knowledge, we can select testing pairs that are meaningful and avoid useless, even noisy, classification results. We summarize the selected combinations of attribute and string feature pair and their corresponding performance measures in Table 2. Table 2: Performance metrics by attribute constraints  ROC point A B C D E F G H  attribute  string feature constraint  person name  Sc ( = 2 ~ 4)  person name  Sp + Sl (first character ∈ known)  person name  Sc + Sp + Sl  rank  Sn (= true)  rank  Sn + Sl (= true)  issue number  Sn (= true)  cause of changes  Sl (= true)  government unit name  Sc ( = 2 ~ 10)  TP rate .9999 .9867 .9926 .6549 .6579 1.0 1.0 .1536  FP rate .8672 .1719 .1016 .2900 .0933 .3276 .2353 .2857  Figure 1 shows the ROC graph of the selected testing pairs. The ROC points fall into three groups based on their locations. The first group is {A, B, C, F, G}, all of which have extremely high TP rates. This indicates that the string feature constraints specified for the particular attributes are highly successful in correctly classifying valid values. In addition, we can generally enhance their performance by adding more string feature constraints, as shown by the improvement from A to B to C. The second group includes D and E, which are only moderately effective in correctly classifying valid values. Note that the results were achieved by using binary string features only. Again, with more restricted validity constraints, more invalid values are detected correctly, as shown by the improvement from D to E. The third group contains H only, which is worse than random guessing classification. In fact, H is intended to show the possibility of a meaningless testing pair. Based on these performance results, we make the following observations. zSome attributes are strongly restricted by special types of value domains, which can be mostly represented by certain string features. Such a string feature is called the primary string feature of the corresponding attribute. zFor many attributes, there is a good chance of finding a primary string feature that provides satisfactory or even highly successful positive classification. zThe classification performance on an attribute can be enhanced by complementing its primary string feature with some secondary string features. zGiven subject domain knowledge, string feature based detection by attribute constraints may provide more than satisfactory performance. The overall results seem to show that string features as attribute constraints are capable of effective data validity classification. We only show partial results in Table 2 to highlight some major points. However, we did confirm that by specifying more restrictive constraints with multiple string features ROC points of the attributes in our task domains all fall into area near the upper left  Proceedings of PACLIC 19, the 19th Asia-Pacific Conference on Language, Information and Computation.  corner. Finally, constraint specifications for attribute values involve only surface-form string features. Shallow domain knowledge is sufficed to provide the classification rules.  4.4. Performance of Majority Rule The second set of experiments concerns with the use of string features in invalid value discrimination based on statistical majority. It is hypothesized that invalid values in IE results will be indicated by their unusual string feature values, which may be exhibited when comparing to other valid values of the same attribute. We conducted pair-wise testing on every combination of string feature and attribute. When classifying values of an attribute, frequencies of occurrence of string feature values are ordered from least significant to most significant. A threshold of accumulated percentage is used as the discrimination line between minority and majority. Based on majority rule, attribute values with minority string feature values are considered as invalid data. The parameter of accumulated percentage threshold is set at 1%, 3%, 5%, 10%, 15%, 20%, and 25%, respectively. This variation of parameter values produces a ROC curve, representing the overall behavior of the classifier.  CBG F 
Language identification has been an interesting and fascinating issue in natural language processing for decades, and there have been many researches on it. However, most of the researches are for documents, and though the possibility of high accuracy for shorter strings of characters, language identification for words or phrases has not been discussed much. In this paper we propose a statistical method of language identification for phrases, and show the empirical results for person names of 9 languages (12 areas). Our simple method based on n-gram and phrase length obtained more than 90% of accuracy for Japanese, Korean and Russian, and fair results for other languages except English. This result indicated the possibility of language identification for person names based on statistics, which is useful in multi-language person name detection and also let us expect the possibility of language identification for phrases with simple statistics-based methods.  1. Introduction The technology of language identification has become more important with the growth of the WWW. As Grefenstette reported in their paper (Grefenstette 2000) non-English languages are growing in recent years on the WWW, and the need for automatic language identification for both documents and phrases are increasing. An easy method for language identification must be a key for better accuracy rate in natural language processing, such as information retrieval and machine translation. Language identification is not a new topic in natural language processing. Bastrup proposed a unigram-based decision-tree method for language identification (Bastrup 2003). Dunning reported that 20 bytes are enough to obtain 92% accuracy in language identification (Dunning 1994). His method was based on statistical information, and it did not use any accented characters which would be a great help in identifying. This result is very encouraging for applying statistical language identification to proper nouns. Language identification is also well examined as speech recognition task (Matrouf 1998, Schultz 1996, Hazen 1994a, Hazen 1994b, Lamel 1994, Berkling 1994). Caseiro and Trancoso introduced a method using one language phone recognizer and less linguistic information for the language identification of speech (Caseiro 1998). 2. Language Identification for Person Names Person name is one of the most frequent foreign phrases in texts, and it often causes noise as unknown words. Thus language identification of person names can be a help for better accuracy in text processing. However, there are several difficulties in identifying the language of a person name. First, the language of a person name may not match the official language of the area. Second, person names are international and not as language-specific as other words. Third, person names are often not long enough for analysis, and it is even possible to have name words in different languages in a full name. There can be found names in non-official languages of the area  for many reasons such as international naming, international marriage or migration. And it is also difficult to identify a single language to a name, as there are common names used in several languages. So it is not practical to identify the language of a person name, however, to identify the area to where a person name belong should be possible with statistical data.  3. Statistics in Person Names It is well known that each language has its own n-gram frequency (Dunning 1994). Our person name list corpora also have language-specific n-gram frequency in person names. Thus we examined the possibility of language identification for person names based only on automatically-extractable statistical information. Nobesawa et al. proposed methods on obtaining domain-specific phrases using automatically-extractable statistical information only (Nobesawa 2002, Nobesawa 2000). Thus we examined the possibility of language identification for person names based only on automatically-extractable statistical information.  3.1. Corpora We use person name lists for both training corpora and test corpora. Our system requires statistical data extracted automatically from training corpora, and estimates the likelihood of each name belonging to each language.  3.1.1. Name Corpora by Areas We prepared person name lists in 9 languages and made 12 person name corpora by areas (English (United Kingdom, United States), Chinese (mainland China, Hong Kong, and Taiwan), German, French, Greek, Japanese, Korean, Russian, and Thai. As for person names, it is almost impossible to decide a single nationality to each name. Thus we work on area identification rather than language identification for person names, according to sets of names found in academic organization websites on the WWW. We can not avoid foreign names being noise with these data. The size of the corpora varies, from at most 18,119 full names for Germany corpus to at least 5,764 full names for Greece corpora. Average number of full names in a corpus is 10,903 (Table 1). The basic data of the name corpora are shown in Table 1. “Name” in Table 1 means full names, “word” is for name words such as a single family name or a single given name, including initials. Table 1 also includes average number of words in a full name and average number of characters in a name word in each name corpus. Table 1: Name Corpora for Experiments on Language Identification  name corpus China Taiwan Hong Kong Korea Japan Thailand Russia Greece France Germany U.K. U.S.A. average  #name 7,388 7,676 9,049 9,284 13,680 6,774 5,891 5,764 14,405 18,119 17,149 15,661 10,903  #word 15,004 16,486 25,389 21,258 27,339 13,693 14,564 11,749 30,005 37,406 39,142 34,212 23,853  #char 86,336 105,010 122,244 118,338 207,618 129,415 122,663 100,705 232,667 277,188 254,194 231,259 165,636  #word/#name 2.03 2.15 2.81 2.29 2.00 2.02 2.47 2.04 2.08 2.06 2.28 2.18 2.20  #char/#word 5.75 6.37 4.81 5.57 7.59 9.45 8.42 8.57 7.75 7.41 6.49 6.76 7.08  Proceedings of PACLIC 19, the 19th Asia-Pacific Conference on Language, Information and Computation. 3.1.2. Notation Our system does not use the advantage of language-specified accented characters. We excluded names with accented characters from the corpora. Thus the corpora do not include, for example, “Uta Müller,” but they may include “Uta Muller” and “Uta Mueller.” The corpora contain names with initials, such as “A.B. Chan” and “Ann B. Chan.” This has influence on the numbers per word/name in Table 1 and Figure 2, for the ratio of names with initials differ between the corpora. Some areas such as Hong Kong and Russia show tendency to have more initials. 3.2. Statistics in Person Names Our system uses the statistical data of person names in the name corpora in Section 3.1. 3.2.1. Length Names have small differences in their names according to the areas. The overall average number of characters in a full name is 16.19. Chinese and Korean is shorter in average, and Russian and Thai are longer. Full-Name Length (Number of Words in a Full-Name) A Hong Kong name has 2.81 words on average, which is a unique feature (Table 1). A Chinese name is basically made of three characters, one for family name and succeeding two for a given name. But the basic notations of the names in Latin alphabet are different in China, Hong Kong and Taiwan. So-called English names are more common in Hong Kong, and many people put English names to/instead of Chinese given names. Thus there is variety of notation of names in Hong Kong. In Korea, person names are made of three words like in Chinese, and some people separate their given names in two words when writing in Latin alphabet. This is the main reason of having name-length average rather longer in Hong Kong and Korea comparing to other Asian areas. In Russia the most frequent full-name length is three, where two is common in most of languages. Russian names have longer words than other languages, and this length information helps in distinguishing Russian from others, as Thai and Greek which also have longer words contain only two words in a name. Word Length (Number of Characters in a Name Word) The average number of characters in a name word was 7.08 and the average number of name words in a full name was 2.20. Figure 1: Average number of characters per word/full name of each corpus As for the word length, Russian, Thai and Greece have slightly longer words. Figure 1 shows that there can be three groups according to the length; shorter-word group for 3 Chinese Areas and Korea, middle-length group for Europe, U.S.A. and Japan, and longer-word group for Thailand, Greece and Russia. Shorter-word group is tending to have approximately 6 characters. Japanese  seems to be unique in Asian languages, as it has 8 letters in a word on average, which is more alike to middle-length group (Figure 2).  Figure 2: Ratio of Characters per Word (%)  3.2.2. N-gram  Unigram Table 2 shows the frequency of characters in each name corpus. Upper-case characters and lower-case characters are counted as the same.  corpus China Taiwan H.K. Korea Japan Thailand Russia Greece France Germany U.K. U.S.A.  Table 2: Unigram Ratio Ranking of Each Corpus 10% & more / 5% & more / 1% & more /  less than 1%  N I G / A U H E / O Z Y L X J WS C Q D M/ F T P B R K V  N H / I G A E U C / S L O Y WT J MR K / P F Z D B X V Q  N / A I G H E U O / C L K WY S MT R P / F D J Z B V X Q  N O / G E H U K A I / Y S MJ L C WR P / B D T Z V X F Q  A I / O K H S U T / MR N Y E D G C WJ / Z B F L P V X Q  A N / T I R O H S / P U E K C MG WL D Y J / V B F Z X Q  A / I V O N E L R / K S H T MD Y G C U B P / Z F X J WQ  A I O S | N T R / L E K U P D G MH V C / Z F Y B J X WQ  E A / I R N L O / S T U C D H MB G P V F Y J / K Z Q WX  E / A R N I S L H T / O MC U K D G B F P WJ / Z V Y X Q  A / E R N I O L S / T H D MC G B U P WY K J F V / Z X Q  A E / N R I L O S / H T MD C G U K Y B J P WV F / Z X Q  As shown in Table 2, the ratio of characters differs with the areas. Basic five vowels are emphasized in Table 2. ‘A’ and ‘I’ are frequent in almost all the corpora. However, for the Korea corpus ‘A’ and ‘I’ are the least frequent vowels, which is very different from other corpora. ‘U’ has high frequency in Asian corpora, but not in European corpora. ‘E’ is also frequent in most corpora, but the frequency rate differ comparing European corpora and Asian corpora. ‘Z’ and ‘Q’ are almost not used in 11 corpora, but are found in the China corpus, which is even different from the Hong Kong corpus and the Taiwan corpus. ‘V’ is also almost not used, but it is the most frequent consonant in the Russia corpus. ‘K’ is not a frequent character in most of the corpora, but it is the most frequent consonant in the Japan corpus. ‘T’ has low frequency ratio in Chinese corpora and the Korea corpus, but the ratio is more than 5% in Japan, Thailand, Greece and Germany.  Proceedings of PACLIC 19, the 19th Asia-Pacific Conference on Language, Information and Computation. Bigram There are language-specific features in bigram data as well. Bigrams mostly show the features of areas in vowel cooccurrences and consonant cooccurrences. Sometimes they are more area-specific rather than language-specific. Trigram Trigram provides the specific features of the areas. The frequency rankings of trigrams differ between the areas, and high-ranked trigrams are the keys of high accuracy rate in area identification. 4. Identification Based on Statistical Information Our system is fully based on statistical information. The system takes one full name as an input and outputs the possibility value for the input full name belonging to each name corpus. 4.1. Corpora We used the name corpora explained in Section 3.1 as the test corpora for the experiments. Though the experiments were on closed corpora, the system may not be able to obtain 100% accuracy because of the foreign names included in the corpora. 4.2. Language Identification Methods Based on Statistical Data The algorithm is the same for all the methods. The system calculates the possibility to belong to an area using the statistical data extracted from the name corpus. The estimation is calculated independently for each name corpus, thus the summation of the possibility ratio for each area is not 100%. This is because one name may have more than one area to belong, or no area, according to the listing up of the test areas. After examining all the areas, the system outputs the best-scored area as the result of the area identification of the input full name. A Method Based on Length The system uses full-name length and word length as statistical data for estimation. This is rather a negative method to filter out areas with low possibility. A Method Based on N-gram The system uses n-gram data for estimation. We had four patterns: unigram only, bigram only, trigram only, and interpolated trigram. Interpolated trigram is to avoid the problem of sparseness with trigram, and the system introduces bigrams for the gap. If the system finds lack of both trigram and bigram for a string, then unigram is used for interpolation. For bigrams and unigrams, the possibility value is reduced with certain weights. A Method Based on N-gram and Length The system uses both interpolated trigram and length data for estimation. The weights for the two data should be controlled, and for the experiments in this paper interpolated trigram is more weighted so that length data is used as a support to raise the accuracy. 4.3. Empirical Results We had experiments on area identification for the 12 name corpora. Table 3 shows the ratio of full names successfully estimated with our statistical methods. The results showed at most around 90% accuracy for four corpora (Japan, Russia, Korea and Thailand). Another four corpora showed 70% to 80% accuracy (Greece, Taiwan, Germany and China), and two corpora showed more than 60% accuracy (France and Hong Kong). The accuracy rate for U.K. was 59.18% and 50.88% for U.S.A. The accuracy rates were mostly good for most areas except English-speaking areas. Table 3 indicated that interpolated n-gram was efficient for most of the corpora, and the system raised its accuracy rate slightly by combining length data and n-gram data. Length data were not efficient enough for an independent use, but they never decreased the accuracy when used combined with n-gram data. From the results we recognized two groups of areas; the European-American group and the Chinese group. The European-American group includes U.K., U.S.A., France and Germany. Chinese group includes China, Taiwan and Hong Kong. The name corpora which belong to these groups showed lower accuracy rates, as they are often confused and mistaken between the areas in  the same group. Obviously it is natural to have such area groups, and it can be said that we succeeded in recognition of area groups only with statistical information. Name corpora not included in the two area groups showed higher accuracy rates.  Table 3: Accuracy Rates on Language (Area) Identification (%)  name corpus  length unigram bigram  trigram  
Various kinds of IT and computer technology have enabled language tests to be delivered online or on computer, and to have much faster scoring time. Such computer-based tests can well assess three skill areas such as reading, writing, and listening comprehension. However, in many cases, speaking ability is still inferred by the scores obtained for those three skill areas. Ordinate Corporation and the Institute for Digital Enhancement of COgnitive DEvelopment (the Institute for DECODE) at Waseda University have been jointly working to develop a completely automated test of spoken Japanese, Spoken Japanese Test (SJT)1. SJT is intended to provide automated test administration and scoring service by delivering the test over the telephone and by scoring the test using speech recognition technology and other computerized systems. Ordinate and the Institute for DECODE are currently collecting speech data from native and non-native speakers of Japanese to develop a speech recognizer optimized for the Japanese language and for non-native speakers of Japanese. 1. Introduction Multimedia technology, IT, computers, and voice recognition systems have started to be widely used in foreign language education as instructional aids and as self-learning tools. Within foreign language education, language testing has begun to use these types of technology as well. The most common way to utilize technology in language testing is to develop Computer-Based Tests (CBT). One of the advantages of CBT is to be able to collect test-takers’ responses almost immediately and to return scores to the test-takers in a shorter period of time than ever before. Most CBTs assess learners’ Listening, Reading, and Writing skills, but they rarely test Speaking. It is still very challenging to test speaking remotely via technology. Therefore, the common format of testing one’s speaking ability is still a face-to-face interview format. More and more foreign nationals are coming to Japan for academic and business purposes. The Japan Foundation (2005) reported that the Japanese Language Proficiency Test was administered to more than 300,000 test-takers in 2004. As more people learn Japanese or as more people come to Japan to study or to work, demand for speaking tests that can measure Japanese learners’ speaking skills quickly and reliably is growing. However, few speaking tests are available in the field of teaching Japanese, and, not to mention, there is no speaking test that has utilized computer technology to efficiently test learners’ speaking ability. Ordinate Corporation has developed a language testing system using speech recognition and computerized scoring systems that can evaluate the spoken language skills of non-native speakers. The Spoken English Test (SET) and Spoken Spanish Test (SST) are currently delivered on this 
The issue for this study is to examine whether the structures described are applicable to conversation taking place in the Web and if there is any difference in repair structures between ordinary conversation and the Web-based conversation. 3. Data collection and analysis This study is based on naturally occurring written interaction on the Web-based discussion boards for two education courses for in-service teachers at the Open University of Hong Kong. The discussion boards are presented in the written form in Chinese. A total of 400 participants, including students, tutors, and the Course Coordinator (CC), took part in the two boards. These participants created more than 4,000 postings containing nearly half million Chinese characters. 351 repair cases have been identified from the main body of the data, which contains 1525 postings of two public groups amongst total seventeen groups in the two boards. The data presented in this paper is in English, which is translated by  Proceedings of PACLIC 19, the 19th Asia-Pacific Conference on Language, Information and Computation  the author, but original Chinese texts are followed to facilitate presentation and proofreading of the data. Some notations used in the transcription assist presenting the data analysis.1  4. Possible structures for repair in Web-based conversation Four possible structures for successful repair and two possible structures for failure of repair as identified by Schegloff et al. (1977) in ordinary conversation conducted in English have also been found in Web-based conversation in Chinese.  4.1 Self-initiation self-repair  Excerpt 1: Data code Date/time P019 2002/07/23 01:17PM P020 2002/07/23 01:20PM P021 2002/07/23 01:21PM P023 2002/07/23 03:09PM  Sender Ms Chan (S) Ms Chan (S) Ms Chan (S) Ms Chan (S)  Title Re: (3) Curriculum development Re: (3) Curriculum development Re: (4) Curriculum development Re: (5) Curriculum development  Content ((The screen shows all Chinese characters in an illegible code)) Let me make a response. 讓我來回應。 Let me make a response. ‘Please see attachment’ ((with no attachment)) 讓我來回應。<請看附件 ((並無附件)) Let me make a response. ‘Please see attachment’. ((with an attachment)) 讓我來回應。<請看附件. ((有附件))  In this extract, Ms Chan, a student in the discussion group, responded to the discussion topic  ‘Curriculum development’. As her first response (P019) was typed straight on to the Web in  Chinese using a special code, the screen showed all the Chinese characters she typed as being  illegible when she posted them on the discussion board at 01:17PM, 23 July, 2002. Three  minutes later, 01:20PM, Ms Chan sent a second posting saying ‘let me make a response’. This  was obviously an attempt to make her response again to repair the trouble source of her prior  posting. Then, after one minute, 01:21PM, Ms Chan found that the second response was  incomplete, because she should have attached a file. So, she sent a third posting, which added  ‘Please see attachment’ (P021). However, as in P020, while the third posting was a repair to  prior postings, it also became a trouble source, as there was no file attached, even though the  
(1) f-structure: PRED ‘cause-to-run < SUBJ, OBJ>’ SUBJ [PRED ‘John’] OBJ [PRED ‘Bill’]  c-structure: i) [Jon ga [Bill o hashir-]ase-ta] ii) [Jon ga Bill o [hasher-ase-ta]]  (biclausal c-structure) (monoclausal c-structure)  A-structures are assumed to project skeletal f-structures through lexical mapping principles (Bresnan 2001). Following Butt, Isoda, and Sells (1990), we assume all arguments in upper clause ‘cause’ and the SUBEVENT clause in (2) are mapped onto entities in a simplex f-structure (1).  (2) a-structure:  REL  ‘cause < AGENT, PATIENT, SUBEVENT >’  AGENT  [REL ‘John’]  PATIENT  [REL ‘Bill’] i  SUBEVENT REL ‘run < AGENT >’  AGENT i  Based on the notion of the complex-predicate formation, the next section provides evidence to support this view. 3. Syntactic/semantic integrity of Japanese morphological causatives 3.1. Evidence for f-structure monoclausality F-structure models the internal (or covert) structure of language where grammatical relations are represented. In this subsection, we provide three pieces of evidence in support of functional monoclausality, indicating that the Japanese morphological causative is a complex predicate rather than a control construction which has biclausal f-structures. -Kata ‘way of-’ nominalization The -kata ‘way’ nominalization supports the monoclausal approach. Suffixation of kata creates a nominal meaning “way of”, and can apply to causatives. (3) a. kodomo ni hon o yom-ase-ta. child Dat book Acc read-Caus-Past ‘(I) caused the child read a book’  Proceedings of PACLIC 19, the 19th Asia-Pacific Conference on Language, Information and Computation  b. (?kodomo e no) hon no yom-ase-kata child Dat Gen book Gen read-Caus-way ‘the way to cause (the child) to read a book’  (Long-distance) passivization It is well-known that causatives of transitive verbs allow passivization of the -ni marked phrase (Kuno 1973).  (4) Sono rinyuu-shoku wa mada dono akachan-ni-mo tabe-sase-rare-te inai. that babay food Top yet any baby Dat even eat-Caus-Pass Asp-Neg ‘The baby food has not yet been given to feed any child.’ (Matsumoto 2000:148)  Semantic scope by shika---na ‘only --- Neg’ The split quantificational phrase shika-na must be, in general, in the same clause.  (5) a. Ken ga kinoo shika Naomi ga ki-ta to iw-anakat-ta.  Ken Nom yesterday only Naomi Nom come-Past Comp say-Neg-Past.  ‘It was only yesterday that Ken said Naomi came.’  b. Ken ga kinoo shika Naomi ga ko-nakat-ta to  it-ta.  Ken Nom yesterday only Naomi Nom come-Neg-Past Comp say-Past  ‘Ken said that it was only yesterday that Naomi came.’  These structures would look like the following.  (6) a. Ken ga kinoo shika [Naomi ga ki-ta] to iw-anakat-ta. (=5a) b. Ken ga [kinoo shika Naomi ga ko-nakat-ta] to it-ta. (=5b)  Consider now the causative pattern.  (7) Ken ga Naomi ni TV shika mi-sase-nakat-ta. Ken Nom Naomi Dat TV only see-Cause-Neg-Past (a) (Wide scope) Ken didn’t cause Naomi to watch other things. (b) (Narrow scope) Ken didn’t cause Naomi to do other things.  The wide-scope reading (7a) is obtained if shika---nakat has mi-sase “cause to see” within its scope. On the other hand, the narrow scope reading (7b) can only be obtained if shika --nakat is assumed to have only mi in its scope. These contrasts can be explained TV shika misase is considered to have a “biclausal” structure.  (8) a. Ken ga Naomi ni TV shika [mi]-sase-nakat-ta. (= 7a) b. Ken ga Naomi ni [TV shika mi]-sase-nakat-ta. (= 7b)  In the construction shown in (8a) TV shika is outside of the inner “clause” headed by mi ‘watch’ via Argument Transfer (Grimshaw and Mester 1988). In light of these examples, any  “lexical” account of causatives makes clear how it can deal with such ambiguous scope assignments. Under the assumption that the causative is a single lexical entry, the problem posed by such examples is basically the problem of how to assign “word-internal” scope to a quantified NP that appears external to the lexical causative.1 3.2. Evidence for c-structure monoclausality C-structure is the level where the surface syntactic form, including categorial information, word order and phrasal grouping of constituent, is encoded and is expressed through phrase structure rules, such as S → NP, VP. Concerning constituent (or category) monoclausality of the Japanese causatives, it is quite easy to give evidence (see Manning et al. 1999). We will discuss just a piece of evidence for c-structure monoclausality. Reduplication Reduplication process is assumed to be a lexical process (Maranz 1982), which is irrelevant to syntactic and semantic information and is construed so as to respect the morphophonological integrity between the stem of the head verb and the causative. (9) a. gohan o tabe tabe rice Acc eat eat ‘eating rice repeatedly’ b. ?gohan o tabe-sase tabe-sase rice Acc eat-Caus eat-Caus ‘causing someone to eat rice repeatedly’ c. *gohan o tabe-sase sase rice Acc eat-Caus Caus ‘causing someone to eat rice repeatedly’ The LFG conception of “word” is a purely c-structural concept. Not only does the Lexical Integrity Principle say nothing about semantics and phonology, it does not even apply to the functional aspect of syntax. The relations between the smallest element of c-structure (the word), the smallest unit of semantics (the semantic word), and prosodic constituent often described as the “phonological word” will certainly be a question for the theories of correspondence between c-structure and these other modules of the grammar (see Matsumoto 1996). 3.3. Evidence for c-structure biclausality We provide three pieces of evidence for c-structure biclausality. Particle intervention (10) Sono eiga wa kankyaku o naki mo sase-ta shi, warai mo sase-ta. that movie Top audience Acc cry also Caus-Past and laugh also Caus-Past 
Statistical NLG has largely meant n-gram modelling which has the considerable advantages of lending robustness to NLG systems, and of making automatic adaptation to new domains from raw corpora possible. On the downside, n-gram models are expensive to use as selection mechanisms and have a built-in bias towards shorter realisations. This paper looks at treebank-training of generators, an alternative method for building statistical models for NLG from raw corpora, and two different ways of using treebank-trained models during generation. Results show that the treebank-trained generators achieve improvements similar to a 2-gram generator over a baseline of random selection. However, the treebank-trained generators achieve this at a much lower cost than the 2-gram generator, and without its strong preference for shorter realisations. 
We present an authoring system for logical forms encoded as conceptual graphs (CG). The system belongs to the family of WYSIWYM (What You See Is What You Mean) text generation systems: logical forms are entered interactively and the corresponding linguistic realization of the expressions is generated in several languages. The system maintains a model of the discourse context corresponding to the authored documents. The system helps users author documents formulated in the CG format. In a ﬁrst stage, a domainspeciﬁc ontology is acquired by learning from example texts in the domain. The ontology acquisition module builds a typed hierarchy of concepts and relations derived from the WordNet and Verbnet. The user can then edit a speciﬁc document, by entering utterances in sequence, and maintaining a representation of the context. While the user enters data, the system performs the standard steps of text generation on the basis of the authored logical forms: reference planning, aggregation, lexical choice and syntactic realization – in several languages (we have implemented English and Hebrew - and are exploring an implementation using the Bliss graphical language). The feedback in natural language is produced in real-time for every single modiﬁcation performed by the author. We perform a cost-beneﬁt analysis of the application of NLG techniques in the context of authoring cooking recipes in English and Hebrew. By combining existing large-scale knowledge resources (WordNet, Verbnet, the SURGE and HUGG realization grammars) and techniques from modern integrated software development environment (such as the Eclipse IDE), we obtain an efﬁcient tool for the generation of logical forms, in domains where content is not available in the form of databases. ∗Research supported by the Israel Ministry of Science - Knowledge Center for Hebrew Computational Linguistics and by the Frankel Fund  
Since its ﬁrst implementation in 1995, the shallow NLG system TG/2 has been used as a component in many NLG applications that range from very shallow template systems to in-depth realization engines. TG/2 has continuously been reﬁned, the Java brother implementation XtraGen has become available, and the grammar development environment eGram today allows for designing grammars on a more abstract level. Besides a better understanding of the usability of shallow systems like TG/2 has emerged. Time has come to summarize the developments and look forward to new borders. 
This paper describes Acorn, a sentence planner and surface realizer for dialogue systems. Improvements to previous stochastic word-forest based approaches are described, countering recent criticism of this class of algorithms for their slow speed. An evaluation of the approach with semantic input shows runtimes of a fraction of a second and presents results that suggest it is also portable across domains. 
Natural language has a high paraphrastic power yet not all paraphrases are appropriate for all contexts. In this paper, we present a TAG based surface realiser which supports both the generation and the selection of paraphrases. To deal with the combinatorial explosion typical of such an NP-complete task, we introduce a number of new optimisations in a tabular, bottom-up surface realisation algorithm. We then show that one of these optimisations supports paraphrase selection. 
Algorithms for generating referring expressions typically assume that an object in a scenary can be identified through a set of commonly agreed properties. This is a strong assumption, since in reality properties of objects may be perceived differently among people, due to a number of factors including vagueness, knowledge discrepancies, and limited perception capabilities. Taking these discrepancies into account, we reinterpret concepts of algorithms generating referring expressions in view of uncertainties about the appearance of objects. Our model includes two complementary measures of likelihood in object identification, and adapted property selection and termination criteria. The approach is relevant for situations with potential perception problems and for scenarios with knowledge discrepancies between conversants. 
This paper presents a framework for generating locative expressions. The framework addresses the issue of combinatorial explosion inherent in the construction of relational context models by: (a) contextually deﬁning the set of objects in the context that may function as a landmark, and (b) sequencing the order in which spatial relations are considered using a cognitively motivated hierarchy of relations. 
This paper describes an approach for the generation of multimodal deixis to be uttered by an anthropomorphic agent in virtual reality. The proposed algorithm integrates pointing and deﬁnite description. Doing so, the context-dependent discriminatory power of the gesture determines the contentselection for the verbal constituent. The concept of a pointing cone is used to model the region singled out by a pointing gesture and to distinguish two referential functions called object-pointing and region-pointing.  [Dale and Reiter, 1995]. This algorithm for the generation of verbal referring expressions was adapted in that the spatial property location, which can be expressed either absolutely by pointing or relationally by verbal expressions (e.g. ”the left object”), is evaluated besides other object properties in content-selection. Taking account of the inherent impreciseness of pointing gestures, two referential functions of pointing are distinguished, object-pointing and region-pointing. While object-pointing refers on its own, region-pointing is used to narrow down the set of objects from which the referent has to be distinguished by a deﬁnite description.  
The paper presents an approach to utterance planning, which can dynamically use context information about the environment in which a dialogue is situated. The approach is functional in nature, using systemic networks to specify its planning grammar. The planner takes a description of a communicative goal as input, and produces one or more logical forms that can express that goal in a contextually appropriate way. Both the goal and the resulting logical forms are expressed in a single formalism as ontologically rich, relational structures. To realize the logical forms, OpenCCG is used. The paper focuses primarily on the implementation, but also discusses how the planning grammar can be based on the grammar used in OpenCCG, and trained on (parseable) data. 
The paper proposes an architecture for advanced NLG systems that handle narratives. Special attention is paid to document planning. Domain modelling and meta-knowledge modelling for a narratological structurer are exemplified. 
We present an NLG system that uses Integer Linear Programming to integrate different decisions involved in the generation process. Our approach provides an alternative to pipeline-based sequential processing which has become prevalent in today’s NLG applications. 
Sentence fusion is a text-to-text (revision-like) generation task which takes related sentences as input and merges these into a single output sentence. In this paper we describe our ongoing work on developing a sentence fusion module for Dutch. We propose a generalized version of alignment which not only indicates which words and phrases should be aligned but also labels these in terms of a small set of primitive semantic relations, indicating how words and phrases from the two input sentences relate to each other. It is shown that human labelers can perform this task with a high agreement (Fscore of .95). We then describe and evaluate our adaptation of an existing automatic alignment algorithm, and use the resulting alignments, plus the semantic labels, in a generalized fusion and generation algorithm. A small-scale evaluation study reveals that most of the resulting sentences are adequate to good. 
It is hard to come up with a general formalisation of the problem of content determination in natural language generation because of the degree of domaindependence that is involved. This paper presents a novel way of looking at a class of content determination problems in terms of a non-standard kind of inference, which we call natural language directed inference. This is illustrated through examples from a system under development to present parts of ontologies in natural language. Natural language directed inference represents an interesting challenge to research in automated reasoning and natural language processing.  emulate a museum curator telling a good story to link together a sequence of selected exhibits. It was argued that a more opportunistic approach to content determination was needed for this sort of application [Mellish et al., 1998]. In this paper, we concentrate primarily on the “bottom-up” type of content determination problem. But what makes content determination hard in either case is largely the fact that two different “worlds” are involved – the domain model and the linguistic world. Content determination is selecting material from the (not necessarily very linguistic) domain model, e.g. facts, rules and numbers, in the hope that it will permit a coherent realisation as a text. In between the domain model Θ and the set of possible texts T ext sits a possibly non-trivial mapping ρ (“realisation”): ρ : {θ|θ is content selected from Θ} → 2T ext  
Computer pun-generators have so far relied on arbitrary semantic content, not linked to the immediate context. The mechanisms used, although tractable, may be of limited applicability. Integrating puns into normal text may involve complex search. 
Post-editing is commonly performed on computergenerated texts, whether from Machine Translation (MT) or NLG systems, to make the texts acceptable to end users. MT systems are often evaluated using post-edit data. In this paper we describe our experience of using post-edit data to evaluate SUMTIME-MOUSAM, an NLG system that produces marine weather forecasts. 
Most NLG systems generate texts for readers with good reading ability, but SkillSum adapts its output for readers with poor literacy. Evaluation with lowskilled readers confirms that SkillSum’s knowledge-based microplanning choices enhance readability. We also discuss future readability improvements. 
We present three ways in which a natural language generator that produces textual descriptions of objects from symbolic information can exploit OWL ontologies, using M-PIRO’s multilingual generation system as a concrete example. 
Instructional texts consist of sequences of instructions designed in order to reach an objective. The author or the generator of instructional texts must follow a number of principles to guarantee that the text is of any use. Similarly, a user must follow step by step the instructions in order to reach the results expected. In this paper, we explore facets of instructional texts: general prototypical structures, rhetorical structure and natural argumentation. Our study is based on an extensive corpus study with the aim of generating such texts. 
Recent empirical experiments on surface realizers have shown that grammars for generation can be effectively evaluated using large corpora. Evaluation metrics are usually reported as single averages across all possible types of errors and syntactic forms. But the causes of these errors are diverse, and the extent to which the accuracy of generation over individual syntactic phenomena is unknown. This article explores the types of errors, both computational and linguistic, inherent in the evaluation of a surface realizer when using large corpora. We analyze data from an earlier wide coverage experiment on the FUF/SURGE surface realizer with the Penn TreeBank in order to empirically classify the sources of errors and describe their frequency and distribution. This both provides a baseline for future evaluations and allows designers of NLG applications needing off-the-shelf surface realizers to choose on a quantitative basis. 
The work presented here is intended as an evolutionary task-speciﬁc module for referring expression generation and aggregation to be enclosed in a generic ﬂexible architecture. Appearances of concepts are considered as genes, each one encoding the type of reference used. Three genetic operators are used: classic crossover and mutation, plus a speciﬁc operator dealing with aggregation. Fitness functions are deﬁned to achieve elementary coherence and stylistic validity. Experiments are described and discussed. 
This paper addresses two previously unresolved issues in the automatic evaluation of Text Structuring (TS) in Natural Language Generation (NLG). First, we describe how to verify the generality of an existing collection of sentence orderings deﬁned by one domain expert using data provided by additional experts. Second, a general evaluation methodology is outlined which investigates the previously unaddressed possibility that there may exist many optimal solutions for TS in the employed domain. This methodology is implemented in a set of experiments which identify the most promising candidate for TS among several metrics of coherence previously suggested in the literature.1 
This paper describes ongoing work on the choice of modal expressions in the generation of recommendations about courses of study within the B.A.program of the Ruhr-Universität Bochum. We focus our work on the German modal verbs müssen (must), können (can), the subjunctive form sollte (should), and dürfen (may). The idea is to combine insights from formal semantics into the meaning of modal verbs with requirements for their choice in NLG-systems in order to achieve a linguistically satisfying model of their choice. The overall model is implemented in the CAN-system that plans courses of study and generates recommendations about them. 
Reversibility is a key to eﬃcient and maintainable NLG systems. In this paper, we present a formal deﬁnition of reversible NLG systems and develop a classiﬁcation of existing natural language dialog systems in this framework. 
We describe a Wizard-of-Oz experiment setup for the collection of multimodal interaction data for a Music Player application. This setup was developed and used to collect experimental data as part of a project aimed at building a ﬂexible multimodal dialogue system which provides an interface to an MP3 player, combining speech and screen input and output. Besides the usual goal of WOZ data collection to get realistic examples of the behavior and expectations of the users, an equally important goal for us was to observe natural behavior of multiple wizards in order to guide our system development. The wizards’ responses were therefore not constrained by a script. One of the challenges we had to address was to allow the wizards to produce varied screen output a in real time. Our setup includes a preliminary screen output planning module, which prepares several versions of possible screen output. The wizards were free to speak, and/or to select a screen output. 
In this paper, we propose an approach for content determination and surface generation of answers in a question-answering system on the web. The content determination is based on a coherence rate which takes into account coherence with other potential answers. Answer generation is made through the use of classical techniques and templates and is based on a certainty degree. 
Productions systems, traditionally mainly used for developing expert systems, can also be employed for implementing chart generators. Focusing on bottom-up chart generation, we describe how the notions of chart algorithms relate to the knowledge base and Rete network of production systems. We draw on experience gained in two research projects on natural language generation (NLG), one involving surface realization, the other involving both a content determination task (referring expression generation) and surface realization. The projects centered around the idea of ‘overgeneration’, i.e. of generating large numbers of output candidates which served as input to a ranking component. The purpose of this paper is to extend the range of implementation options available to the NLG practitioner by detailing the speciﬁc advantages and disadvantages of using production systems for NLG. 
We discuss work-in-progress on a hybrid approach to the generation of spatial descriptions, using the maps of the Map Task dialogue corpus as domain models. We treat spatial descriptions as referring expressions that distinguish particular points on the maps from all other points (potential ‘distractors’). Our approach is based on rule-based overgeneration of spatial descriptions combined with ranking which currently is based on explicit goodness criteria but will ultimately be corpus-based. Ranking for content determination tasks such as referring expression generation raises a number of deep and vexing questions about the role of corpora in NLG, the kind of knowledge they can provide and how it is used. 
In many text-to-text generation scenarios (for instance, summarisation), we encounter humanauthored sentences that could be composed by recycling portions of related sentences to form new sentences. In this paper, we couch the generation of such sentences as a search problem. We investigate a statistical sentence generation method which recombines words to form new sentences. We propose an extension to the Viterbi algorithm designed to improve the grammaticality of generated sentences. Within a statistical framework, the extension favours those partially generated strings with a probable dependency tree structure. Our preliminary evaluations show that our approach generates less fragmented text than a bigram baseline. 
In this paper, we introduce a new parser, called SXLFG, based on the LexicalFunctional Grammars formalism (LFG). We describe the underlying context-free parser and how functional structures are efﬁciently computed on top of the CFG shared forest thanks to computation sharing, lazy evaluation, and compact data representation. We then present various error recovery techniques we implemented in order to build a robust parser. Finally, we offer concrete results when SXLFG is used with an existing grammar for French. We show that our parser is both efﬁcient and robust, although the grammar is very ambiguous. 
We describe four different parsing algorithms for Linear Context-Free Rewriting Systems (Vijay-Shanker et al., 1987). The algorithms are described as deduction systems, and possible optimizations are discussed. The only parsing algorithms presented for linear contextfree rewriting systems (LCFRS; Vijay-Shanker et al., 1987) and the equivalent formalism multiple context-free grammar (MCFG; Seki et al., 1991) are extensions of the CKY algorithm (Younger, 1967), more designed for their theoretical interest, and not for practical purposes. The reason for this could be that there are not many implementations of these grammar formalisms. However, since a very important subclass of the Grammatical Framework (Ranta, 2004) is equivalent to LCFRS/MCFG (Ljunglöf, 2004a; Ljunglöf, 2004b), there is a need for practical parsing algorithms. In this paper we describe four different parsing algorithms for Linear Context-Free Rewriting Systems. The algorithms are described as deduction systems, and possible optimizations are discussed. 
Parsing in type logical grammars amounts to theorem proving in a substructural logic. This paper takes the proof net presentation of Lambek’s associative calculus as a case study. It introduces switch graphs for online maintenance of the Danos-Regnier acyclicity condition on proof nets. Early detection of Danos-Regnier acyclicity violations supports early failure in shift-reduce parsers. Normalized switch graphs represent the combinatorial potential of a set of analyses derived from lexical and structural ambiguities. Packing these subanalyses and memoizing the results leads directly to a dynamic programming algorithm for Lambek grammars. 
In lexicalized phrase-structure or dependency parses, a word’s modiﬁers tend to fall near it in the string. We show that a crude way to use dependency length as a parsing feature can substantially improve parsing speed and accuracy in English and Chinese, with more mixed results on German. We then show similar improvements by imposing hard bounds on dependency length and (additionally) modeling the resulting sequence of parse fragments. This simple “vine grammar” formalism has only ﬁnite-state power, but a context-free parameterization with some extra parameters for stringing fragments together. We exhibit a linear-time chart parsing algorithm with a low grammar constant. 
We present a corrective model for recovering non-projective dependency structures from trees generated by state-of-theart constituency-based parsers. The continuity constraint of these constituencybased parsers makes it impossible for them to posit non-projective dependency trees. Analysis of the types of dependency errors made by these parsers on a Czech corpus show that the correct governor is likely to be found within a local neighborhood of the governor proposed by the parser. Our model, based on a MaxEnt classiﬁer, improves overall dependency accuracy by .7% (a 4.5% reduction in error) with over 50% accuracy for non-projective structures. 
We discuss the relevance of k-best parsing to recent applications in natural language processing, and develop eﬃcient algorithms for k-best trees in the framework of hypergraph parsing. To demonstrate the eﬃciency, scalability and accuracy of these algorithms, we present experiments on Bikel’s implementation of Collins’ lexicalized PCFG model, and on Chiang’s CFG-based decoder for hierarchical phrase-based translation. We show in particular how the improved output of our algorithms has the potential to improve results from parse reranking systems and other applications. 
We adapt the “hook” trick for speeding up bilexical parsing to the decoding problem for machine translation models that are based on combining a synchronous context free grammar as the translation model with an n-gram language model. This dynamic programming technique yields lower complexity algorithms than have previously been described for an important class of translation models. 
We introduce a method for transferring annotation from a syntactically annotated corpus in a source language to a target language. Our approach assumes only that an (unannotated) text corpus exists for the target language, and does not require that the parameters of the mapping between the two languages are known. We outline a general probabilistic approach based on Data Augmentation, discuss the algorithmic challenges, and present a novel algorithm for sampling from a posterior distribution over trees. 
In this paper, we explore two extensions to an existing statistical parsing model to produce richer parse trees, annotated with function labels. We achieve signiﬁcant improvements in parsing by modelling directly the speciﬁc nature of function labels, as both expressions of the lexical semantics properties of a constituent and as syntactic elements whose distribution is subject to structural locality constraints. We also reach state-of-the-art accuracy on function labelling. Our results suggest that current statistical parsing methods are sufﬁciently robust to produce accurate shallow functional or semantic annotation, if appropriately biased. 
We describe probabilistic models for a chart generator based on HPSG. Within the research ﬁeld of parsing with lexicalized grammars such as HPSG, recent developments have achieved efﬁcient estimation of probabilistic models and high-speed parsing guided by probabilistic models. The focus of this paper is to show that two essential techniques – model estimation on packed parse forests and beam search during parsing – are successfully exported to the task of natural language generation. Additionally, we report empirical evaluation of the performance of several disambiguation models and how the performance changes according to the feature set used in the models and the size of training data. 
We investigated the performance eﬃcacy of beam search parsing and deep parsing techniques in probabilistic HPSG parsing using the Penn treebank. We ﬁrst tested the beam thresholding and iterative parsing developed for PCFG parsing with an HPSG. Next, we tested three techniques originally developed for deep parsing: quick check, large constituent inhibition, and hybrid parsing with a CFG chunk parser. The contributions of the large constituent inhibition and global thresholding were not signiﬁcant, while the quick check and chunk parser greatly contributed to total parsing performance. The precision, recall and average parsing time for the Penn treebank (Section 23) were 87.85%, 86.85%, and 360 ms, respectively. 
Although state-of-the-art parsers for natural language are lexicalized, it was recently shown that an accurate unlexicalized parser for the Penn tree-bank can be simply read off a manually reﬁned treebank. While lexicalized parsers often suffer from sparse data, manual mark-up is costly and largely based on individual linguistic intuition. Thus, across domains, languages, and tree-bank annotations, a fundamental question arises: Is it possible to automatically induce an accurate parser from a tree-bank without resorting to full lexicalization? In this paper, we show how to induce head-driven probabilistic parsers with latent heads from a tree-bank. Our automatically trained parser has a performance of 85.7% (LP/LR F1), which is already better than that of early lexicalized ones. 
We present a classifier-based parser that produces constituent trees in linear time. The parser uses a basic bottom-up shiftreduce algorithm, but employs a classifier to determine parser actions instead of a grammar. This can be seen as an extension of the deterministic dependency parser of Nivre and Scholz (2004) to full constituent parsing. We show that, with an appropriate feature set used in classification, a very simple one-path greedy parser can perform at the same level of accuracy as more complex parsers. We evaluate our parser on section 23 of the WSJ section of the Penn Treebank, and obtain precision and recall of 87.54% and 87.61%, respectively. 
Chunk parsing is conceptually appealing but its performance has not been satisfactory for practical use. In this paper we show that chunk parsing can perform signiﬁcantly better than previously reported by using a simple slidingwindow method and maximum entropy classiﬁers for phrase recognition in each level of chunking. Experimental results with the Penn Treebank corpus show that our chunk parser can give high-precision parsing outputs with very high speed (14 msec/sentence). We also present a parsing method for searching the best parse by considering the probabilities output by the maximum entropy classiﬁers, and show that the search method can further improve the parsing accuracy. 
Ordinary classiﬁcation techniques can drive a conceptually simple constituent parser that achieves near state-of-the-art accuracy on standard test sets. Here we present such a parser, which avoids some of the limitations of other discriminative parsers. In particular, it does not place any restrictions upon which types of features are allowed. We also present several innovations for faster training of discriminative parsers: we show how training can be parallelized, how examples can be generated prior to training without a working parser, and how independently trained sub-classiﬁers that have never done any parsing can be eﬀectively combined into a working parser. Finally, we propose a new ﬁgure-of-merit for bestﬁrst parsing with conﬁdence-rated inferences. Our implementation is freely available at: http://cs.nyu.edu/˜turian/ software/parser/ 
We present a strictly lexical parsing model where all the parameters are based on the words. This model does not rely on part-of-speech tags or grammatical categories. It maximizes the conditional probability of the parse tree given the sentence. This is in contrast with most previous models that compute the joint probability of the parse tree and the sentence. Although the maximization of joint and conditional probabilities are theoretically equivalent, the conditional model allows us to use distributional word similarity to generalize the observed frequency counts in the training corpus. Our experiments with the Chinese Treebank show that the accuracy of the conditional model is 13.6% higher than the joint model and that the strictly lexicalized conditional model outperforms the corresponding unlexicalized model based on part-of-speech tags. 
We present a novel approach for applying the Inside-Outside Algorithm to a packed parse forest produced by a uniﬁcationbased parser. The approach allows a node in the forest to be assigned multiple inside and outside probabilities, enabling a set of ‘weighted GRs’ to be computed directly from the forest. The approach improves on previous work which either loses efﬁciency by unpacking the parse forest before extracting weighted GRs, or places extra constraints on which nodes can be packed, leading to less compact forests. Our experiments demonstrate substantial increases in parser accuracy and throughput for weighted GR output. 
This paper explores the possibilities of improving parsing results by combining outputs of several parsers. To some extent, we are porting the ideas of Henderson and Brill (1999) to the world of dependency structures. We differ from them in exploring context features more deeply. All our experiments were conducted on Czech but the method is language-independent. We were able to significantly improve over the best parsing result for the given setting, known so far. Moreover, our experiments show that even parsers far below the state of the art can contribute to the total improvement. 
This paper describes our effort on the task of edited region identification for parsing disfluent sentences in the Switchboard corpus. We focus our attention on exploring feature spaces and selecting good features and start with analyzing the distributions of the edited regions and their components in the targeted corpus. We explore new feature spaces of a partof-speech (POS) hierarchy and relaxed for rough copy in the experiments. These steps result in an improvement of 43.98% percent relative error reduction in F-score over an earlier best result in edited detection when punctuation is included in both training and testing data [Charniak and Johnson 2001], and 20.44% percent relative error reduction in F-score over the latest best result where punctuation is excluded from the training and testing data [Johnson and Charniak 2004]. 
Institute University of Southern California Marina del Rey, CA, 90292, USA hovy@isi.edu  
 2 Generic factorization operators  This document shows how the factorized syntactic descriptions provided by MetaGrammars coupled with factorization operators may be used to derive compact large coverage tree adjoining grammars. 
mdzikovs@inf.ed.ac.uk  Carolyn P. Rose Carnegie Mellon University Language Technologies Institute Pittsburgh PA 15213, USA cprose@cs.cmu.edu  
 2 Experiments  We describe a method for augmenting uniﬁcation-based deep parsing with statistical methods. We extend and adapt the Bikel parser, which uses head-driven lexical statistics, to dialogue. We show that our augmented parser produces signiﬁcantly fewer constituents than the baseline system and achieves comparable bracketing accuracy, even yielding slight improvements for longer sentences. 
We describe SUPPLE, a freely-available, open source natural language parsing system, implemented in Prolog, and designed for practical use in language engineering (LE) applications. SUPPLE can be run as a stand-alone application, or as a component within the GATE General Architecture for Text Engineering. SUPPLE is distributed with an example grammar that has been developed over a number of years across several LE projects. This paper describes the key characteristics of the parser and the distributed grammar. 
 We describe a history-based generative  parsing model which uses a k-nearest  neighbour (k-NN) technique to estimate  the model’s parameters. Taking the  output of a base n-best parser we use our  model to re-estimate the log probability of  each parse tree in the n-best list for  sentences from the Penn Wall Street  Journal treebank.  By further  decomposing the local probability  distributions of the base model, enriching  the set of conditioning features used to  estimate the model’s parameters, and  using k-NN as opposed to the Witten-Bell  estimation of the base model, we achieve  an f-score of 89.2%, representing a 4%  relative decrease in f-score error over the  1-best output of the base parser.  
Department of Computer Science & Engineering The Ohio State University, USA fosler@cse.ohio-state.edu  
A pervasive problem facing many biomedical text mining applications is that of correctly associating mentions of entities in the literature with corresponding concepts in a database or ontology. Attempts to build systems for automating this process have shown promise as demonstrated by the recent BioCreAtIvE Task 1B evaluation. A significant obstacle to improved performance for this task, however, is a lack of high quality training data. In this work, we explore methods for improving the quality of (noisy) Task 1B training data using variants of weakly supervised learning methods. We present positive results demonstrating that these methods result in an improvement in training data quality as measured by improved system performance over the same system using the originally labeled data. 
In this paper we present the evaluation of a set of string similarity metrics used to resolve the mapping from strings to concepts in the UMLS MetaThesaurus. String similarity is conceived as a single component in a full Reference Resolution System that would resolve such a mapping. Given this qualiﬁcation, we obtain positive results achieving 73.6 F-measure (76.1 precision and 71.4 recall) for the task of assigning the correct UMLS concept to a given string. Our results demonstrate that adaptive string similarity methods based on Conditional Random Fields outperform standard metrics in this domain. 
Gene and protein named-entity recognition (NER) and normalization is often treated as a two-step process. While the first step, NER, has received considerable attention over the last few years, normalization has received much less attention. We have built a dictionary based gene and protein NER and normalization system that requires no supervised training and no human intervention to build the dictionaries from online genomics resources. We have tested our system on the Genia corpus and the BioCreative Task 1B mouse and yeast corpora and achieved a level of performance comparable to state-of-the-art systems that require supervised learning and manual dictionary creation. Our technique should also work for organisms following similar naming conventions as mouse, such as human. Further evaluation and improvement of gene/protein NER and normalization systems is somewhat hampered by the lack of larger test collections and collections for additional organisms, such as human. 
This paper presents a machine learning approach to acronym generation. We formalize the generation process as a sequence labeling problem on the letters in the deﬁnition (expanded form) so that a variety of Markov modeling approaches can be applied to this task. To construct the data for training and testing, we extracted acronym-deﬁnition pairs from MEDLINE abstracts and manually annotated each pair with positional information about the letters in the acronym. We have built an MEMM-based tagger using this training data set and evaluated the performance of acronym generation. Experimental results show that our machine learning method gives signiﬁcantly better performance than that achieved by the standard heuristic rule for acronym generation and enables us to obtain multiple candidate acronyms together with their likelihoods represented in probability values. 
We present a database of annotated biomedical text corpora merged into a portable data structure with uniform conventions. MedTag combines three corpora, MedPost, ABGene and GENETAG, within a common relational database data model. The GENETAG corpus has been modiﬁed to reﬂect new deﬁnitions of genes and proteins. The MedPost corpus has been updated to include 1,000 additional sentences from the clinical medicine domain. All data have been updated with original MEDLINE text excerpts, PubMed identiﬁers, and tokenization independence to facilitate data accuracy, consistency and usability. The data are available in ﬂat ﬁles along with software to facilitate loading the data into a relational SQL database from ftp://ftp.ncbi.nlm.nih.gov/pub/lsmith /MedTag/medtag.tar.gz. 
This paper classiﬁes six publicly available biomedical corpora according to various corpus design features and characteristics. We then present usage data for the six corpora. We show that corpora that are carefully annotated with respect to structural and linguistic characteristics and that are distributed in standard formats are more widely used than corpora that are not. These ﬁndings have implications for the design of the next generation of biomedical corpora. 
In this paper, we present a fully automated extraction system, named IntEx, to identify gene and protein interactions in biomedical text. Our approach is based on first splitting complex sentences into simple clausal structures made up of syntactic roles. Then, tagging biological entities with the help of biomedical and linguistic ontologies. Finally, extracting complete interactions by analyzing the matching contents of syntactic roles and their linguistically significant combinations. Our extraction system handles complex sentences and extracts multiple and nested interactions specified in a sentence. Experimental evaluations with two other state of the art extraction systems indicate that the IntEx system achieves better performance without the labor intensive pattern engineering requirement. ∗ 
This paper addresses the classiﬁcation of semantic relations between pairs of sentences extracted from a Dutch parallel corpus at the word, phrase and sentence level. We ﬁrst investigate the performance of human annotators on the task of manually aligning dependency analyses of the respective sentences and of assigning one of ﬁve semantic relations to the aligned phrases (equals, generalizes, speciﬁes, restates and intersects). Results indicate that humans can perform this task well, with an F-score of .98 on alignment and an Fscore of .95 on semantic relations (after correction). We then describe and evaluate a combined alignment and classiﬁcation algorithm, which achieves an F-score on alignment of .85 (using EuroWordNet) and an F-score of .80 on semantic relation classiﬁcation. 
This work explores computing distributional similarity between sub-parses, i.e., fragments of a parse tree, as an extension to general lexical distributional similarity techniques. In the same way that lexical distributional similarity is used to estimate lexical semantic similarity, we propose using distributional similarity between subparses to estimate the semantic similarity of phrases. Such a technique will allow us to identify paraphrases where the component words are not semantically similar. We demonstrate the potential of the method by applying it to a small number of examples and showing that the paraphrases are more similar than the non-paraphrases. 
This paper presents a knowledge-based method for measuring the semanticsimilarity of texts. While there is a large body of previous work focused on ﬁnding the semantic similarity of concepts and words, the application of these wordoriented methods to text similarity has not been yet explored. In this paper, we introduce a method that combines wordto-word similarity metrics into a text-totext metric, and we show that this method outperforms the traditional text similarity metrics based on lexical matching. 
Generally speaking, statistical machine translation systems would be able to attain better performance with more training sets. Unfortunately, well-organized training sets are rarely available in the real world. Consequently, it is necessary to focus on modifying the training set to obtain high accuracy for an SMT system. If the SMT system trained the translation model, the translation pair would have a low probability when there are many variations for target sentences from a single source sentence. If we decreased the number of variations for the translation pair, we could construct a superior translation model. This paper describes the effects of modification on the training corpus when consideration is given to synonymous sentence groups. We attempt three types of modification: compression of the training set, replacement of source and target sentences with a selected sentence from the synonymous sentence group, and replacement of the sentence on only one side with the selected sentence from the synonymous sentence group. As a result, we achieve improved performance with the replacement of source-side sentences. 
We present ﬁrst results using paraphrase as well as textual entailment data to test the language universal constraint posited by Wu’s (1995, 1997) Inversion Transduction Grammar (ITG) hypothesis. In machine translation and alignment, the ITG Hypothesis provides a strong inductive bias, and has been shown empirically across numerous language pairs and corpora to yield both efﬁciency and accuracy gains for various language acquisition tasks. Monolingual paraphrase and textual entailment recognition datasets, however, potentially facilitate closer tests of certain aspects of the hypothesis than bilingual parallel corpora, which simultaneously exhibit many irrelevant dimensions of cross-lingual variation. We investigate this using simple generic Bracketing ITGs containing no language-speciﬁc linguistic knowledge. Experimental results on the MSR Paraphrase Corpus show that, even in the absence of any thesaurus to accommodate lexical variation between the paraphrases, an uninterpolated average precision of at least 76% is obtainable from the Bracketing ITG’s structure matching bias alone. This is consistent with experimental results on the Pascal Recognising Textual Entailment Challenge Corpus, which show surpisingly strong results for a number of the task subsets. 
This paper argues that local textual inferences come in three well-deﬁned varieties (entailments, conventional implicatures/presuppositions, and conversational implicatures) and one less clearly deﬁned one, generally available world knowledge. Based on this taxonomy, it discusses some of the examples in the PASCAL text suite and shows that these examples do not fall into any of them. It proposes to enlarge the test suite with examples that are more directly related to the inference patterns discussed. 
In this work we investigate methods to enable the detection of a speciﬁc type of textual entailment (strict entailment), starting from the preliminary assumption that these relations are often clearly expressed in texts. Our method is a statistical approach based on what we call textual entailment patterns, prototypical sentences hiding entailment relations among two activities. We experimented the proposed method using the entailment relations of WordNet as test case and the web as corpus where to estimate the probabilities; obtained results will be shown. 
This paper proposes a general probabilistic setting that formalizes a probabilistic notion of textual entailment. We further describe a particular preliminary model for lexical-level entailment, based on document cooccurrence probabilities, which follows the general setting. The model was evaluated on two application independent datasets, suggesting the relevance of such probabilistic approaches for entailment modeling. 
We describe our efforts to generate a large (100,000 instance) corpus of textual entailment pairs from the lead paragraph and headline of news articles. We manually inspected a small set of news stories in order to locate the most productive source of entailments, then built an annotation interface for rapid manual evaluation of further exemplars. With this training data we built an SVM-based document classifier, which we used for corpus refinement purposes—we believe that roughly three-quarters of the resulting corpus are genuine entailment pairs. We also discuss the difficulties inherent in manual entailment judgment, and suggest ways to ameliorate some of these. 
In this paper we deﬁne two intermediate models of textual entailment, which correspond to lexical and lexical-syntactic levels of representation. We manually annotated a sample from the RTE dataset according to each model, compared the outcome for the two models, and explored how well they approximate the notion of entailment. We show that the lexicalsyntactic model outperforms the lexical model, mainly due to a much lower rate of false-positives, but both models fail to achieve high recall. Our analysis also shows that paraphrases stand out as a dominant contributor to the entailment task. We suggest that our models and annotation methods can serve as an evaluation scheme for entailment at these levels. 
TextTrees, introduced in (Newman, 2005), are skeletal representations formed by systematically converting parser output trees into unlabeled indented strings with minimal bracketing. Files of TextTrees can be read rapidly to evaluate the results of parsing long documents, and are easily edited to allow limited-cost treebank development. This paper reviews the TextTree concept, and then describes the implementation of the almost parser- and grammar-independent TextTree generator, as well as auxiliary methods for producing parser review files and inputs to bracket scoring tools. The results of some limited experiments in TextTree usage are also provided. 
It is not clear a priori how well parsers trained on the Penn Treebank will parse signiﬁcantly different corpora without retraining. We carried out a competitive evaluation of three leading treebank parsers on an annotated corpus from the human molecular biology domain, and on an extract from the Penn Treebank for comparison, performing a detailed analysis of the kinds of errors each parser made, along with a quantitative comparison of syntax usage between the two corpora. Our results suggest that these tools are becoming somewhat over-specialised on their training domain at the expense of portability, but also indicate that some of the errors encountered are of doubtful importance for information extraction tasks. Furthermore, our inital experiments with unsupervised parse combination techniques showed that integrating the output of several parsers can ameliorate some of the performance problems they encounter on unfamiliar text, providing accuracy and coverage improvements, and a novel measure of trustworthiness. Supplementary materials are available at http://textmining.cryst.bbk. ac.uk/acl05/.  
We give a technical description of the ﬁssion module of the COMIC multimodal dialogue system, which both plans the multimodal content of the system turns and controls the execution of those plans. We emphasise the parts of the implementation that allow the system to begin producing output as soon as possible by preparing and outputting the content in parallel. We also demonstrate how the module was designed to ensure robustness and conﬁgurability, and describe how the module has performed successfully as part of the overall system. Finally, we discuss how the techniques used in this module can be applied to other similar dialogue systems. 
We present an extensible API for integrating language modeling and realization, describing its design and efﬁcient implementation in the OpenCCG surface realizer. With OpenCCG, language models may be used to select realizations with preferred word orders, promote alignment with a conversational partner, avoid repetitive language use, and increase the speed of the best-ﬁrst anytime search. The API enables a variety of n-gram models to be easily combined and used in conjunction with appropriate edge pruning strategies. The n-gram models may be of any order, operate in reverse (“right-to-left”), and selectively replace certain words with their semantic classes. Factored language models with generalized backoff may also be employed, over words represented as bundles of factors such as form, pitch accent, stem, part of speech, supertag, and semantic class. 
We describe the evolution of solvers for dominance constraints, a formalism used in underspeciﬁed semantics, and present a new graph-based solver using charts. An evaluation on real-world data shows that each solver (including the new one) is signiﬁcantly faster than its predecessors. We believe that our strategy of successively tailoring a powerful formalism to the actual inputs is more generally applicable. 
Common tasks involving orthographic words include spellchecking, stemming, morphological analysis, and morphological synthesis. To enable signiﬁcant reuse of the language-speciﬁc resources across all such tasks, we have extended the functionality of the open source spellchecker MySpell, yielding a generic word analysis library, the runtime layer of the hunmorph toolkit. We added an offline resource management component, hunlex, which complements the efﬁciency of our runtime layer with a high-level description language and a conﬁgurable precompiler.  The C/C++ runtime layer of our toolkit, called hunmorph, was developed by extending the codebase of MySpell, a reimplementation of the wellknown Ispell spellchecker. Our technology, like the Ispell family of spellcheckers it descends from, enforces a strict separation between the language-speciﬁc resources (known as dictionary and affix ﬁles), and the runtime environment, which is independent of the target natural language.  0 Introduction  Word-level analysis and synthesis problems range from strict recognition and approximate matching to full morphological analysis and generation. Our technology is predicated on the observation that all of these problems are, when viewed algorithmically, very similar: the central problem is to dynamically analyze complex structures derived from some lexicon of base forms. Viewing word analysis routines as a uniﬁed problem means sharing the same codebase for a wider range of tasks, a design goal carried out by ﬁnding the parameters which optimize each of the analysis modes independently of the language-speciﬁc resources.  Figure 1: Architecture Compiling accurate wide coverage machinereadable dictionaries and coding the morphology of a language can be an extremely labor-intensive task, so the beneﬁt expected from reusing the language-speciﬁc input database across tasks can hardly be overestimated. To facilitate this resource sharing and to enable systematic task-dependent optimizations from a central lexical knowledge base, we designed and implemented a powerful ofﬂine layer we call hunlex. Hunlex offers an easy  to use general framework for describing the lexicon and morphology of any language. Using this description it can generate the language-speciﬁc aff/dic resources, optimized for the task at hand. The architecture of our toolkit is depicted in Figure 1. Our toolkit is released under a permissive LGPL-style license and can be freely downloaded from mokk.bme.hu/resources/hunmorph. The rest of this paper is organized as follows. Section 1 is about the runtime layer of our toolkit. We discuss the algorithmic extensions and implementational enhancements in the C/C++ runtime layer over MySpell, and also describe the newly created Java port jmorph. Section 2 gives an overview of the ofﬂine layer hunlex. In Section 3 we consider the free open source software alternatives and offer our conclusions. 
We describe the implementation steps required to scale high-order character language models to gigabytes of training data without pruning. Our online models build character-level PAT trie structures on the ﬂy using heavily data-unfolded implementations of an mutable daughter maps with a long integer count interface. Terminal nodes are shared. Character 8-gram training runs at 200,000 characters per second and allows online tuning of hyperparameters. Our compiled models precompute all probability estimates for observed n-grams and all interpolation parameters, along with sufﬁx pointers to speedup context computations from proportional to n-gram length to a constant. The result is compiled models that are larger than the training models, but execute at 2 million characters per second on a desktop PC. Cross-entropy on held-out data shows these models to be state of the art in terms of performance. 
This paper introduces xfst2fsa, a compiler which translates grammars expressed in the syntax of the XFST ﬁnite-state toolbox to grammars in the language of the FSA Utilities package. Compilation to FSA facilitates the use of grammars developed with the proprietary XFST toolbox on a publicly available platform. The paper describes the non-trivial issues of the compilation process, highlighting several shortcomings of some published algorithms, especially where replace rules are concerned. The compiler augments FSA with most of the operators supported by XFST. Furthermore, it provides a means for comparing the two systems on comparable grammars. The paper presents the results of such a comparison. 
This paper reports on a study of semantic role tagging in Chinese in the absence of a parser. We tackle the task by identifying the relevant headwords in a sentence as a first step to partially locate the corresponding constituents to be labelled. We also explore the effect of data homogeneity by experimenting with a textbook corpus and a news corpus, representing simple data and complex data respectively. Results suggest that while the headword location method remains to be improved, the homogeneity between the training and testing data is important especially in view of the characteristic syntaxsemantics interface in Chinese. We also plan to explore some class-based techniques for the task with reference to existing semantic lexicons, and to modify the method and augment the feature set with more linguistic input. 
Recently, many researches in natural language learning have considered the representation of complex linguistic phenomena by means of structural kernels. In particular, tree kernels have been used to represent verbal subcategorization frame (SCF) information for predicate argument classiﬁcation. As the SCF is a relevant clue to learn the relation between syntax and semantic, the classiﬁcation algorithm accuracy was remarkable enhanced. In this article, we extend such work by studying the impact of the SCF tree kernel on both PropBank and FrameNet semantic roles. The experiments with Support Vector Machines (SVMs) conﬁrm a strong link between the SCF and the semantics of the verbal predicates as well as the beneﬁt of using kernels in diverse and complex test conditions, e.g. classiﬁcation of unseen verbs. 
We developed a novel classification of concept attributes and two supervised classifiers using this classification to identify concept attributes from candidate attributes extracted from the Web. Our binary (attribute / non-attribute) classifier achieves an accuracy of 81.82% whereas our 5-way classifier achieves 80.35%. 
Qualia Structures have many applications within computational linguistics, but currently there are no corresponding lexical resources such as WordNet or FrameNet. This paper presents an approach to automatically learn qualia structures for nominals from the World Wide Web and thus opens the possibility to explore the impact of qualia structures for natural language processing at a larger scale. Furthermore, our approach can be also used support a lexicographer in the task of manually creating a lexicon of qualia structures. The approach is based on the idea of matching certain lexicosyntactic patterns conveying a certain semantic relation on the World Wide Web using standard search engines. We evaluate our approach qualitatively by comparing our automatically learned qualia structures with the ones from the literature, but also quantitatively by presenting results of a human evaluation. 
We investigate the meaning extensions of very frequent and highly polysemous verbs, both in terms of their compositional contribution to a light verb construction (LVC), and the patterns of acceptability of the resulting LVC. We develop compositionality and acceptability measures that draw on linguistic properties speciﬁc to LVCs, and demonstrate that these statistical, corpus-based measures correlate well with human judgments of each property. 
This paper describes a technique for extracting idioms from text. The technique works by ﬁnding patterns such as “thrills and spills”, whose reversals (such as “spills and thrills”) are never encountered. This method collects not only idioms, but also many phrases that exhibit a strong tendency to occur in one particular order, due apparently to underlying semantic issues. These include hierarchical relationships, gender differences, temporal ordering, and prototype-variant effects. 
SemFrame generates FrameNet-like frames, complete with semantic roles and evoking lexical units. This output can enhance FrameNet by suggesting new frames, as well as additional lexical units that evoke existing frames. SemFrame output can also support the addition of frame semantic relationships to WordNet. 
We propose a range of deep lexical acquisition methods which make use of morphological, syntactic and ontological language resources to model word similarity and bootstrap from a seed lexicon. The different methods are deployed in learning lexical items for a precision grammar, and shown to each have strengths and weaknesses over different word classes. A particular focus of this paper is the relative accessibility of different language resource types, and predicted “bang for the buck” associated with each in deep lexical acquisition applications. 
This paper discusses the role of morphological and syntactic information in the automatic acquisition of semantic classes for Catalan adjectives, using decision trees as a tool for exploratory data analysis. We show that a simple mapping from the derivational type to the semantic class achieves 70.1% accuracy; syntactic function reaches a slightly higher accuracy of 73.5%. Although the accuracy scores are quite similar with the two resulting classiﬁcations, the kinds of mistakes are qualitatively very different. Morphology can be used as a baseline classiﬁcation, and syntax can be used as a clue when there are mismatches between morphology and semantics. 
Distributional similarity requires large volumes of data to accurately represent infrequent words. However, the nearestneighbour approach to ﬁnding synonyms suffers from poor scalability. The Spatial Approximation Sample Hierarchy (SASH), proposed by Houle (2003b), is a data structure for approximate nearestneighbour queries that balances the efﬁciency/approximation trade-off. We have intergrated this into an existing distributional similarity system, tripling efﬁciency with a minor accuracy penalty. 
This paper demonstrates the usefulness of summaries in an extrinsic task of relevance judgment based on a new method for measuring agreement, Relevance-Prediction, which compares subjects’ judgments on summaries with their own judgments on full text documents. We demonstrate that, because this measure is more reliable than previous gold-standard measures, we are able to make stronger statistical statements about the beneﬁts of summarization. We found positive correlations between ROUGE scores and two different summary types, where only weak or negative correlations were found using other agreement measures. However, we show that ROUGE may be sensitive to the choice of summarization style. We discuss the importance of these results and the implications for future summarization evaluations. 
We address the issue of human subjectivity when authoring summaries, aiming at a simple, robust evaluation of machine generated summaries. Applying a cross comprehension test on human authored short summaries from broadcast news, the level of subjectivity is gauged among four authors. The instruction set is simple, thus there is enough room for subjectivity. However the approach is robust because the test does not use the absolute score, relying instead on relative comparison, effectively alleviating the subjectivity. Finally we illustrate the application of the above scheme when evaluating the informativeness of machine generated summaries. 
Evaluation measures for machine translation depend on several common methods, such as preprocessing, tokenization, handling of sentence boundaries, and the choice of a reference length. In this paper, we describe and review some new approaches to them and compare these to state-of-the-art methods. We experimentally look into their impact on four established evaluation measures. For this purpose, we study the correlation between automatic and human evaluation scores on three MT evaluation corpora. These experiments conﬁrm that the tokenization method, the reference length selection scheme, and the use of sentence boundaries we introduce will increase the correlation between automatic and human evaluation scores. We ﬁnd that ignoring case information and normalizing evaluator scores has a positive effect on the sentence level correlation as well. 
Automatic evaluation of machine translation, based on computing n-gram similarity between system output and human reference translations, has revolutionized the development of MT systems. We explore the use of syntactic information, including constituent labels and head-modiﬁer dependencies, in computing similarity between output and reference. Our results show that adding syntactic information to the evaluation metric improves both sentence-level and corpus-level correlation with human judgments. 
The research below explores schemes for evaluating automatic summaries of business meetings, using the ICSI Meeting Corpus (Janin et al., 2003). Both automatic and subjective evaluations were carried out, with a central interest being whether or not the two types of evaluations correlate with each other. The evaluation metrics were used to compare and contrast differing approaches to automatic summarization, the deterioration of summary quality on ASR output versus manual transcripts, and to determine whether manual extracts are rated signiﬁcantly higher than automatic extracts. 
This paper discusses the convergence between question answering and multidocument summarization, pointing out implications and opportunities for knowledge transfer in both directions. As a case study in one direction, we discuss the recent development of an automatic method for evaluating deﬁnition questions based on n-gram overlap, a commonlyused technique in summarization evaluation. In the other direction, the move towards topic-oriented summaries requires an understanding of relevance and topicality, issues which have received attention in the question answering literature. It is our opinion that question answering and multi-document summarization represent two complementary approaches to the same problem of satisfying complex user information needs. Although this points to many exciting opportunities for systembuilding, here we primarily focus on implications for system evaluation. 
This papers reports the application of the QARLA evaluation framework to the DUC 2004 testbed (tasks 2 and 5). Our experiment addresses two issues: how well QARLA evaluation measures correlate with human judgements, and what additional insights can be provided by the QARLA framework to the DUC evaluation exercises. 
We investigate some pitfalls regarding the discriminatory power of MT evaluation metrics and the accuracy of statistical signiﬁcance tests. In a discriminative reranking experiment for phrase-based SMT we show that the NIST metric is more sensitive than BLEU or F-score despite their incorporation of aspects of ﬂuency or meaning adequacy into MT evaluation. In an experimental comparison of two statistical signiﬁcance tests we show that p-values are estimated more conservatively by approximate randomization than by bootstrap tests, thus increasing the likelihood of type-I error for the latter. We point out a pitfall of randomly assessing significance in multiple pairwise comparisons, and conclude with a recommendation to combine NIST with approximate randomization, at more stringent rejection levels than is currently standard. 
We describe METEOR, an automatic metric for machine translation evaluation that is based on a generalized concept of unigram matching between the machineproduced translation and human-produced reference translations. Unigrams can be matched based on their surface forms, stemmed forms, and meanings; furthermore, METEOR can be easily extended to include more advanced matching strategies. Once all generalized unigram matches between the two strings have been found, METEOR computes a score for this matching using a combination of unigram-precision, unigram-recall, and a measure of fragmentation that is designed to directly capture how well-ordered the matched words in the machine translation are in relation to the reference. We evaluate METEOR by measuring the correlation between the metric scores and human judgments of translation quality. We compute the Pearson R correlation value between its scores and human quality assessments of the LDC TIDES 2003 Arabic-to-English and Chinese-to-English datasets. We perform segment-bysegment correlation, and show that METEOR gets an R correlation value of 0.347 on the Arabic data and 0.331 on the Chinese data. This is shown to be an improvement on using simply unigramprecision, unigram-recall and their harmonic F1 combination. We also perform experiments to show the relative contributions of the various mapping modules.  Alon Lavie Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 alavie@cs.cmu.edu 
Bilingual word alignment forms the foundation of current work on statistical machine translation. Standard wordalignment methods involve the use of probabilistic generative models that are complex to implement and slow to train. In this paper we show that it is possible to approach the alignment accuracy of the standard models using algorithms that are much faster, and in some ways simpler, based on basic word-association statistics. 
In a multilingual scenario, the classical monolingual text categorization problem can be reformulated as a cross language TC task, in which we have to cope with two or more languages (e.g. English and Italian). In this setting, the system is trained using labeled examples in a source language (e.g. English), and it classiﬁes documents in a different target language (e.g. Italian). In this paper we propose a novel approach to solve the cross language text categorization problem based on acquiring Multilingual Domain Models from comparable corpora in a totally unsupervised way and without using any external knowledge source (e.g. bilingual dictionaries). These Multilingual Domain Models are exploited to deﬁne a generalized similarity function (i.e. a kernel function) among documents in different languages, which is used inside a Support Vector Machines classiﬁcation framework. The results show that our approach is a feasible and cheap solution that largely outperforms a baseline. 
We present an Earley-style dynamic programming algorithm for parsing sentence pairs from a parallel corpus simultaneously, building up two phrase structure trees and a correspondence mapping between the nodes. The intended use of the algorithm is in bootstrapping grammars for less studied languages by using implicit grammatical information in parallel corpora. Therefore, we presuppose a given (statistical) word alignment underlying in the synchronous parsing task; this leads to a signiﬁcant reduction of the parsing complexity. The theoretical complexity results are corroborated by a quantitative evaluation in which we ran an implementation of the algorithm on a suite of test sentences from the Europarl parallel corpus. 
In this paper, a variant of a spectral clustering algorithm is proposed for bilingual word clustering. The proposed algorithm generates the two sets of clusters for both languages efﬁciently with high semantic correlation within monolingual clusters, and high translation quality across the clusters between two languages. Each cluster level translation is considered as a bilingual concept, which generalizes words in bilingual clusters. This scheme improves the robustness for statistical machine translation models. Two HMMbased translation models are tested to use these bilingual clusters. Improved perplexity, word alignment accuracy, and translation quality are observed in our experiments. 
In this paper, we present an approach to automatically revealing phonological correspondences within historically related languages. We create two bilingual pronunciation dictionaries for the language pairs German-Dutch and GermanEnglish. The data is used for automatically learning phonological similarities between the two language pairs via EMbased clustering. We apply our models to predict from a phonological German word the phonemes of a Dutch and an English cognate. The similarity scores show that German and Dutch phonemes are more similar than German and English phonemes, which supplies statistical evidence of the common knowledge that German is more closely related to Dutch than to English. We assess our approach qualitatively, ﬁnding meaningful classes caused by historical sound changes. The classes can be used for language learning. 
In this work, we examine the quality of several statistical machine translation systems constructed on a small amount of parallel Serbian-English text. The main bilingual parallel corpus consists of about 3k sentences and 20k running words from an unrestricted domain. The translation systems are built on the full corpus as well as on a reduced corpus containing only 200 parallel sentences. A small set of about 350 short phrases from the web is used as additional bilingual knowledge. In addition, we investigate the use of monolingual morpho-syntactic knowledge i.e. base forms and POS tags. 
This paper presents an original approach to part-of-speech tagging of ﬁne-grained features (such as case, aspect, and adjective person/number) in languages such as English where these properties are generally not morphologically marked. The goals of such rich lexical tagging in English are to provide additional features for word alignment models in bilingual corpora (for statistical machine translation), and to provide an information source for part-of-speech tagger induction in new languages via tag projection across bilingual corpora. First, we present a classiﬁer-combination approach to tagging English bitext with very ﬁne-grained part-of-speech tags necessary for annotating morphologically richer languages such as Czech and French, combining the extracted features of three major English parsers, and achieve ﬁne-grained-tag-level syntactic analysis accuracy higher than any individual parser. Second, we present experimental results for the cross-language projection of partof-speech taggers in Czech and French via word-aligned bitext, achieving successful ﬁne-grained part-of-speech tagging of these languages without any Czech or French training data of any kind.  
In this paper we describe an alignment system that aligns English-Hindi texts at the sentence and word level in parallel corpora. We describe a simple sentence length approach to sentence alignment and a hybrid, multi-feature approach to perform word alignment. We use regression techniques in order to learn parameters which characterise the relationship between the lengths of two sentences in parallel text. We use a multi-feature approach with dictionary lookup as a primary technique and other methods such as local word grouping, transliteration similarity (edit-distance) and a nearest aligned neighbours approach to deal with many-to-many word alignment. Our experiments are based on the EMILLE (Enabling Minority Language Engineering) corpus. We obtained 99.09% accuracy for many-to-many sentence alignment and 77% precision and 67.79% recall for many-to-many word alignment. 
This paper presents a set of techniques for bitext word align- ment, optimized for a language pair with the characteristics of Inuktitut-English. The resulting systems exploit cross-lingual afﬁnities at the sublexical level of syllables and substrings, as well as regular patterns of transliteration and the tendency to- wards monotonicity of alignment. Our most successful systems were based on classiﬁer combination, and we found different combination methods performed best under the target evalua- tion metrics of F-measure and alignment error rate. 
We introduce improvements to statistical word alignment based on the Hidden Markov Model. One improvement incorporates syntactic knowledge. Results on the workshop data show that alignment performance exceeds that of a state-of-the art system based on more complex models, resulting in over a 5.5% absolute reduction in error on Romanian-English. 
Several algorithms are available for sentence alignment, but there is a lack of systematic evaluation and comparison of these algorithms under different conditions. In most cases, the factors which can signiﬁcantly affect the performance of a sentence alignment algorithm have not been considered while evaluating. We have used a method for evaluation that can give a better estimate about a sentence alignment algorithm’s performance, so that the best one can be selected. We have compared four approaches using this method. These have mostly been tried on European language pairs. We have evaluated manually-checked and validated English-Hindi aligned parallel corpora under different conditions. We also suggest some guidelines on actual alignment. 
In this paper we describe LIHLA, a lexical aligner which uses bilingual probabilistic lexicons generated by a freely available set of tools (NATools) and languageindependent heuristics to ﬁnd links between single words and multiword units in sentence-aligned parallel texts. The method has achieved an alignment error rate of 22.72% and 44.49% on English– Inuktitut and Romanian–English parallel sentences, respectively. 
Statistical machine translation systems use a combination of one or more translation models and a language model. While there is a signiﬁcant body of research addressing the improvement of translation models, the problem of optimizing language models for a speciﬁc translation task has not received much attention. Typically, standard word trigram models are used as an out-of-the-box component in a statistical machine translation system. In this paper we apply language modeling techniques that have proved beneﬁcial in automatic speech recognition to the ACL05 machine translation shared data task and demonstrate improvements over a baseline system with a standard language model. 
Thanks to the profusion of freely available tools, it recently became fairly easy to built a statistical machine translation (SMT) engine given a bitext. The expectations we can have on the quality of such a system may however greatly vary from one pair of languages to another. We report on our experiments in building phrase-based translation engines for the four pairs of languages we had to consider for the SMT sharedtask. 
We motivate our contribution to the shared MT task as a ﬁrst step towards an integrated architecture that combines advantages of statistical and knowledge-based approaches. Translations were generated using the Pharaoh decoder with tables derived from the provided alignments for all four languages, and for three of them using web-based and locally installed commercial systems. We then applied statistical and heuristic algorithms to select the most promising translation out of each set of candidates obtained from a source sentence. Results and possible reﬁnements are discussed. 
Part-of-Speech patterns extracted from parallel corpora have been used to enhance a translation resource for statistical phrase-based machine translation. 
This paper presents novel approaches to reordering in phrase-based statistical machine translation. We perform consistent reordering of source sentences in training and estimate a statistical translation model. Using this model, we follow a phrase-based monotonic machine translation approach, for which we develop an efﬁcient and ﬂexible reordering framework that allows to easily introduce different reordering constraints. In translation, we apply source sentence reordering on word level and use a reordering automaton as input. We show how to compute reordering automata on-demand using IBM or ITG constraints, and also introduce two new types of reordering constraints. We further add weights to the reordering automata. We present detailed experimental results and show that reordering signiﬁcantly improves translation quality. 
Translation memories provide assistance to human translators in production settings, and are sometimes used as ﬁrst-pass machine translation in assimilation settings because they produce highly ﬂuent output very rapidly. In this paper, we describe and evaluate a simple whole-segment translation memory, placing it as a new lower bound in the well-populated space of machine translation systems. The result is a new way to gauge how far machine translation has progressed compared to an easily understood baseline system. The evaluation also sheds light on the evaluation metric and gives evidence showing that gaming translation with perfect ﬂuency does not fool bleu the way it fools people. 
(Way and Gough, 2005) provide an indepth comparison of their Example-Based Machine Translation (EBMT) system with a Statistical Machine Translation (SMT) system constructed from freely available tools. According to a wide variety of automatic evaluation metrics, they demonstrated that their EBMT system outperformed the SMT system by a factor of two to one. Nevertheless, they did not test their EBMT system against a phrase-based SMT system. Obtaining their training and test data for English–French, we carry out a number of experiments using the Pharaoh SMT Decoder. While better results are seen when Pharaoh is seeded with Giza++ word- and phrase-based data compared to EBMT sub-sentential alignments, in general better results are obtained when combinations of this ‘hybrid’ data is used to construct the translation and probability models. While for the most part the EBMT system of (Gough & Way, 2004b) outperforms any ﬂavour of the phrasebased SMT systems constructed in our experiments, combining the data sets automatically induced by both Giza++ and their EBMT system leads to a hybrid system which improves on the EBMT system per se for French–English.  
Word graphs have various applications in the ﬁeld of machine translation. Therefore it is important for machine translation systems to produce compact word graphs of high quality. We will describe the generation of word graphs for state of the art phrase-based statistical machine translation. We will use these word graph to provide an analysis of the search process. We will evaluate the quality of the word graphs using the well-known graph word error rate. Additionally, we introduce the two novel graph-to-string criteria: the position-independent graph word error rate and the graph BLEU score. Experimental results are presented for two Chinese–English tasks: the small IWSLT task and the NIST large data track task. For both tasks, we achieve signiﬁcant reductions of the graph error rate already with compact word graphs.  hundred up to a few thousand candidate translations per source sentence. If we want to use larger N -best lists the processing time gets very soon infeasible. Word graphs are a much more compact representation that avoid these redundancies as much as possible. The number of alternatives in a word graph is usually an order of magnitude larger than in an N best list. The graph representation avoids the combinatorial explosion that make large N -best lists infeasible. Word graphs are an important data structure with various applications: • Word Filter. The word graph is used as a compact representation of a large number of sentences. The score information is not contained. • Rescoring. We can use word graphs for rescoring with more sophisticated models, e.g. higher-order language models.  
Decision rules that explicitly account for non-probabilistic evaluation metrics in machine translation typically require special training, often to estimate parameters in exponential models that govern the search space and the selection of candidate translations. While the traditional Maximum A Posteriori (MAP) decision rule can be optimized as a piecewise linear function in a greedy search of the parameter space, the Minimum Bayes Risk (MBR) decision rule is not well suited to this technique, a condition that makes past results difﬁcult to compare. We present a novel training approach for non-tractable decision rules, allowing us to compare and evaluate these and other decision rules on a large scale translation task, taking advantage of the high dimensional parameter space available to the phrase based Pharaoh decoder. This comparison is timely, and important, as decoders evolve to represent more complex search space decisions and are evaluated against innovative evaluation metrics of translation quality. 
We explore the application of memorybased learning to morphological analysis and part-of-speech tagging of written Arabic, based on data from the Arabic Treebank. Morphological analysis – the construction of all possible analyses of isolated unvoweled wordforms – is performed as a letter-by-letter operation prediction task, where the operation encodes segmentation, part-of-speech, character changes, and vocalization. Part-of-speech tagging is carried out by a bi-modular tagger that has a subtagger for known words and one for unknown words. We report on the performance of the morphological analyzer and part-of-speech tagger. We observe that the tagger, which has an accuracy of 91.9% on new data, can be used to select the appropriate morphological analysis of words in context at a precision of 64.0 and a recall of 89.7. 
Morphological analysis is a crucial component of several natural language processing tasks, especially for languages with a highly productive morphology, where stipulating a full lexicon of surface forms is not feasible. We describe HAMSAH (HAifa Morphological System for Analyzing Hebrew), a morphological processor for Modern Hebrew, based on ﬁnite-state linguistically motivated rules and a broad coverage lexicon. The set of rules comprehensively covers the morphological, morpho-phonological and orthographic phenomena that are observable in contemporary Hebrew texts. Reliance on ﬁnite-state technology facilitates the construction of a highly efﬁcient, completely bidirectional system for analysis and generation. HAMSAH is currently the broadest-coverage and most accurate freely-available system for Hebrew. 
We present MAGEAD, a morphological analyzer and generator for the Arabic language family. Our work is novel in that it explicitly addresses the need for processing the morphology of the dialects. MAGEAD provides an analysis to a root+pattern representation, it has separate phonological and orthographic representations, and it allows for combining morphemes from different dialects. 
This paper explores the effect of improved morphological analysis, particularly context sensitive morphology, on monolingual Arabic Information Retrieval (IR). It also compares the effect of context sensitive morphology to noncontext sensitive morphology. The results show that better coverage and improved correctness have a dramatic effect on IR effectiveness and that context sensitive morphology further improves retrieval effectiveness, but the improvement is not statistically significant. Furthermore, the improvement obtained by the use of context sensitive morphology over the use of light stemming was not significantly significant. 
The goal of many natural language processing platforms is to be able to someday correctly treat all languages. Each new language, especially one from a new language family, provokes some modification and design changes. Here we present the changes that we had to introduce into our platform designed for European languages in order to handle a Semitic language. Treatment of Arabic was successfully integrated into our cross language information retrieval system, which is visible online. 
 results on Modern Standard Arabic.  A major architectural decision in designing a disambiguation model for segmentation and Part-of-Speech (POS) tagging in Semitic languages concerns the choice of the input-output terminal symbols over which the probability distributions are deﬁned. In this paper we develop a segmenter and a tagger for Hebrew based on Hidden Markov Models (HMMs). We start out from a morphological analyzer and a very small morphologically annotated corpus. We show that a model whose terminal symbols are word segments (=morphemes), is advantageous over a word-level model for the task of POS tagging. However, for segmentation alone, the morpheme-level model has no signiﬁcant advantage over the word-level model. Error analysis shows that both models are not adequate for resolving a common type of segmentation ambiguity in Hebrew – whether or not a word in a written text is preﬁxed by a deﬁniteness marker. Hence, we propose a morphemelevel model where the deﬁniteness morpheme is treated as a possible feature of morpheme terminals. This model exhibits the best overall performance, both in POS tagging and in segmentation. Despite the small size of the annotated corpus available for Hebrew, the results achieved using our best model are on par with recent  
We applied Conditional Random Fields (CRFs) to the tasks of Amharic word segmentation and POS tagging using a small annotated corpus of 1000 words. Given the size of the data and the large number of unknown words in the test corpus (80%), an accuracy of 84% for Amharic word segmentation and 74% for POS tagging is encouraging, indicating the applicability of CRFs for a morphologically complex language like Amharic. 
Natural language processing technology for the dialects of Arabic is still in its infancy, due to the problem of obtaining large amounts of text data for spoken Arabic. In this paper we describe the development of a part-of-speech (POS) tagger for Egyptian Colloquial Arabic. We adopt a minimally supervised approach that only requires raw text data from several varieties of Arabic and a morphological analyzer for Modern Standard Arabic. No dialect-speciﬁc tools are used. We present several statistical modeling and cross-dialectal data sharing techniques to enhance the performance of the baseline tagger and compare the results to those obtained by a supervised tagger trained on hand-annotated data and, by a state-ofthe-art Modern Standard Arabic tagger applied to Egyptian Arabic. 
Arabic presents an interesting challenge to natural language processing, being a highly inﬂected and agglutinative language. In particular, this paper presents an in-depth investigation of the entity detection and recognition (EDR) task for Arabic. We start by highlighting why segmentation is a necessary prerequisite for EDR, continue by presenting a ﬁnite-state statistical segmenter, and then examine how the resulting segments can be better included into a mention detection system and an entity recognition system; both systems are statistical, build around the maximum entropy principle. Experiments on a clearly stated partition of the ACE 2004 data show that stem-based features can signiﬁcantly improve the performance of the EDT system by 2 absolute F-measure points. The system presented here had a competitive performance in the ACE 2004 evaluation. 
The paper addresses using artiﬁcial neural networks for classiﬁcation of Amharic news items. Amharic is the language for countrywide communication in Ethiopia and has its own writing system containing extensive systematic redundancy. It is quite dialectally diversiﬁed and probably representative of the languages of a continent that so far has received little attention within the language processing ﬁeld. The experiments investigated document clustering around user queries using SelfOrganizing Maps, an unsupervised learning neural network strategy. The best ANN model showed a precision of 60.0% when trying to cluster unseen data, and a 69.5% precision when trying to classify it. 
Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, pages 79–86, Ann Arbor, June 2005. c 2005 Association for Computational Linguistics  
Translation of named entities (NEs), such as person names, organization names and location names is crucial for cross lingual information retrieval, machine translation, and many other natural language processing applications. Newly named entities are introduced on daily basis in newswire and this greatly complicates the translation task. Also, while some names can be translated, others must be transliterated, and, still, others are mixed. In this paper we introduce an integrated approach for named entity translation deploying phrase-based translation, word-based translation, and transliteration modules into a single framework. While Arabic based, the approach introduced here is a unified approach that can be applied to NE translation for any language pair. 
Research on document similarity has shown that complex representations are not more accurate than the simple bag-ofwords. Term clustering, e.g. using latent semantic indexing, word co-occurrences or synonym relations using a word ontology have been shown not very effective. In particular, when to extend the similarity function external prior knowledge is used, e.g. WordNet, the retrieval system decreases its performance. The critical issues here are methods and conditions to integrate such knowledge. In this paper we propose kernel functions to add prior knowledge to learning algorithms for document classiﬁcation. Such kernels use a term similarity measure based on the WordNet hierarchy. The kernel trick is used to implement such space in a balanced and statistically coherent way. Cross-validation results show the beneﬁt of the approach for the Support Vector Machines when few training data is available. 
We introduce a learning semantic parser, SCISSOR, that maps natural-language sentences to a detailed, formal, meaningrepresentation language. It ﬁrst uses an integrated statistical parser to produce a semantically augmented parse tree, in which each non-terminal node has both a syntactic and a semantic label. A compositional-semantics procedure is then used to map the augmented parse tree into a ﬁnal meaning representation. We evaluate the system in two domains, a natural-language database interface and an interpreter for coaching instructions in robotic soccer. We present experimental results demonstrating that SCISSOR produces more accurate semantic representations than several previous approaches. 
In order to achieve the long-range goal of semantic interpretation of noun compounds, it is often necessary to ﬁrst determine their syntactic structure. This paper describes an unsupervised method for noun compound bracketing which extracts statistics from Web search engines using a χ2 measure, a new set of surface features, and paraphrases. On a gold standard, the system achieves results of 89.34% (baseline 66.80%), which is a sizable improvement over the state of the art (80.70%). 
Recent work on the problem of detecting synonymy through corpus analysis has used the Test of English as a Foreign Language (TOEFL) as a benchmark. However, this test involves as few as 80 questions, prompting questions regarding the statistical signiﬁcance of reported results. We overcome this limitation by generating a TOEFL-like test using WordNet, containing thousands of questions and composed only of words occurring with sufﬁcient corpus frequency to support sound distributional comparisons. Experiments with this test lead us to a similarity measure which signiﬁcantly outperforms the best proposed to date. Analysis suggests that a strength of this measure is its relative robustness against polysemy. 
Traditionally, word sense disambiguation (WSD) involves a different context classification model for each individual word. This paper presents a weakly supervised learning approach to WSD based on learning a word independent context pair classification model. Statistical models are not trained for classifying the word contexts, but for classifying a pair of contexts, i.e. determining if a pair of contexts of the same ambiguous word refers to the same or different senses. Using this approach, annotated corpus of a target word A can be explored to disambiguate senses of a different word B. Hence, only a limited amount of existing annotated corpus is required in order to disambiguate the entire vocabulary. In this research, maximum entropy modeling is used to train the word independent context pair classification model. Then based on the context pair classification results, clustering is performed on word mentions extracted from a large raw corpus. The resulting context clusters are mapped onto the external thesaurus WordNet. This approach shows great flexibility to efficiently integrate heterogeneous knowledge sources, e.g. trigger words and parsing structures. Based on Senseval-3 Lexical Sample standards, this approach achieves state-of-the-art performance in the unsupervised learning category, and performs comparably with the supervised Naïve Bayes system. 
We present a system for computing similarity between pairs of words. Our system is based on Pair Hidden Markov Models, a variation on Hidden Markov Models that has been used successfully for the alignment of biological sequences. The parameters of the model are automatically learned from training data that consists of word pairs known to be similar. Our tests focus on the identiﬁcation of cognates — words of common origin in related languages. The results show that our system outperforms previously proposed techniques. 
This paper proposes a model for term reoccurrence in a text collection based on the gaps between successive occurrences of a term. These gaps are modeled using a mixture of exponential distributions. Parameter estimation is based on a Bayesian framework that allows us to ﬁt a ﬂexible model. The model provides measures of a term’s re-occurrence rate and withindocument burstiness. The model works for all kinds of terms, be it rare content word, medium frequency term or frequent function word. A measure is proposed to account for the term’s importance based on its distribution pattern in the corpus. 
In this paper we propose and evaluate a technique to perform semi-supervised learning for Text Categorization. In particular we deﬁned a kernel function, namely the Domain Kernel, that allowed us to plug “external knowledge” into the supervised learning process. External knowledge is acquired from unlabeled data in a totally unsupervised way, and it is represented by means of Domain Models. We evaluated the Domain Kernel in two standard benchmarks for Text Categorization with good results, and we compared its performance with a kernel function that exploits a standard bag-of-words feature representation. The learning curves show that the Domain Kernel allows us to reduce drastically the amount of training data required for learning. 
Clustering is an optimization procedure that partitions a set of elements to optimize some criteria, based on a ﬁxed distance metric deﬁned between the elements. Clustering approaches have been widely applied in natural language processing and it has been shown repeatedly that their success depends on deﬁning a good distance metric, one that is appropriate for the task and the clustering algorithm used. This paper develops a framework in which clustering is viewed as a learning task, and proposes a way to train a distance metric that is appropriate for the chosen clustering algorithm in the context of the given task. Experiments in the context of the entity identiﬁcation problem exhibit signiﬁcant performance improvements over state-of-the-art clustering approaches developed for this problem. 
The classiﬁcation problem derived from information extraction (IE) has an imbalanced training set. This is particularly true when learning from smaller datasets which often have a few positive training examples and many negative ones. This paper takes two popular IE algorithms – SVM and Perceptron – and demonstrates how the introduction of an uneven margins parameter can improve the results on imbalanced training data in IE. Our experiments demonstrate that the uneven margin was indeed helpful, especially when learning from few examples. Essentially, the smaller the training set is, the more beneﬁcial the uneven margin can be. We also compare our systems to other state-of-theart algorithms on several benchmarking corpora for IE. 
 §  ¨  ©                          !  ¥  ¥  ¡  ¥  Symbolic machine-learning classiﬁers are known to suffer from near-sightedness when performing sequence segmentation (chunking) tasks in natural language processing: without special architectural additions they are oblivious of the decisions they made earlier when making new ones. We introduce a new pointwise-prediction single-classiﬁer method that predicts trigrams of class labels on the basis of windowed input sequences, and uses a simple voting mechanism to decide on the labels in the ﬁnal output sequence. We apply the method to maximum-entropy, sparsewinnow, and memory-based classiﬁers using three different sentence-level chunking tasks, and show that the method is able to boost generalization performance in most experiments, attaining error reductions of up to 51%. We compare and combine the method with two known alternative methods to combat near-sightedness, viz. a feedback-loop method and a stacking method, using the memory-based classiﬁer. The combination with a feedback loop suffers from the label bias problem, while the combination with a stacking method produces the best overall results. 
We propose an unsupervised Expectation Maximization approach to pronoun resolution. The system learns from a ﬁxed list of potential antecedents for each pronoun. We show that unsupervised learning is possible in this context, as the performance of our system is comparable to supervised methods. Our results indicate that a probabilistic gender/number model, determined automatically from unlabeled text, is a powerful feature for this task. 
We describe a data-driven approach to building interpretable discourse structures for appointment scheduling dialogues. We represent discourse structures as headed trees and model them with probabilistic head-driven parsing techniques. We show that dialogue-based features regarding turn-taking and domain speciﬁc goals have a large positive impact on performance. Our best model achieves an f score of 43.2% for labelled discourse relations and 67.9% for unlabelled ones, signiﬁcantly beating a right-branching baseline that uses the most frequent relations. 
Natural language interfaces designed for situationally embedded domains (e.g. cars, videogames) must incorporate knowledge about the users’ context to address the many ambiguities of situated language use. We introduce a model of situated language acquisition that operates in two phases. First, intentional context is represented and inferred from user actions using probabilistic context free grammars. Then, utterances are mapped onto this representation in a noisy channel framework. The acquisition model is trained on unconstrained speech collected from subjects playing an interactive game, and tested on an understanding task. 
Unsupervised learning algorithms based on Expectation Maximization (EM) are often straightforward to implement and provably converge on a local likelihood maximum. However, these algorithms often do not perform well in practice. Common wisdom holds that they yield poor results because they are overly sensitive to initial parameter values and easily get stuck in local (but not global) maxima. We present a series of experiments indicating that for the task of learning syllable structure, the initial parameter weights are not crucial. Rather, it is the choice of model class itself that makes the difference between successful and unsuccessful learning. We use a language-universal rule-based algorithm to ﬁnd a good set of parameters, and then train the parameter weights using EM. We achieve word accuracy of 95.9% on German and 97.1% on English, as compared to 97.4% and 98.1% respectively for supervised training. 
Analogical learning is based on a twostep inference process: (i) computation of a structural mapping between a new and a memorized situation; (ii) transfer of knowledge from the known to the unknown situation. This approach requires the ability to search for and exploit such mappings, hence the need to properly deﬁne analogical relationships, and to efﬁciently implement their computation. In this paper, we propose a uniﬁed deﬁnition for the notion of (formal) analogical proportion, which applies to a wide range of algebraic structures. We show that this deﬁnition is suitable for learning in domains involving large databases of structured data, as is especially the case in Natural Language Processing (NLP). We then present experimental results obtained on two morphological analysis tasks which demonstrate the ﬂexibility and accuracy of this approach. 
We address the problem of learning a morphological automaton directly from a monolingual text corpus without recourse to additional resources. Like previous work in this area, our approach exploits orthographic regularities in a search for possible morphological segmentation points. Instead of afﬁxes, however, we search for afﬁx transformation rules that express correspondences between term clusters induced from the data. This focuses the system on substrings having syntactic function, and yields clusterto-cluster transformation rules which enable the system to process unknown morphological forms of known words accurately. A stem-weighting algorithm based on Hubs and Authorities is used to clarify ambiguous segmentation points. We evaluate our approach using the CELEX database. 
We present a discrete optimization model based on a linear programming formulation as an alternative to the cascade of classiﬁers implemented in many language processing systems. Since NLP tasks are correlated with one another, sequential processing does not guarantee optimal solutions. We apply our model in an NLG application and show that it performs better than a pipeline-based system. 
We report on an active learning experiment for named entity recognition in the astronomy domain. Active learning has been shown to reduce the amount of labelled data required to train a supervised learner by selectively sampling more informative data points for human annotation. We inspect double annotation data from the same domain and quantify potential problems concerning annotators’ performance. For data selectively sampled according to different selection metrics, we ﬁnd lower inter-annotator agreement and higher per token annotation times. However, overall results conﬁrm the utility of active learning. 
We present an application of Sparse Bayesian Learning to the task of semantic role labeling, and we demonstrate that this method produces smaller classiﬁers than the popular Support Vector approach. We describe the classiﬁcation strategy and the features used by the classiﬁer. In particular, the contribution of six parse tree path features is investigated. 
We present an approach to semantic role labeling (SRL) that takes the output of multiple argument classiﬁers and combines them into a coherent predicateargument output by solving an optimization problem. The optimization stage, which is solved via integer linear programming, takes into account both the recommendation of the classiﬁers and a set of problem speciﬁc constraints, and is thus used both to clean the classiﬁcation results and to ensure structural integrity of the ﬁnal role labeling. We illustrate a signiﬁcant improvement in overall SRL performance through this inference.  ploits the heuristic introduced by (Xue and Palmer, 2004) to ﬁlter out very unlikely constituents. The heuristic is a recursive process starting from the verb whose arguments are to be identiﬁed. It ﬁrst returns the siblings of the verb; then it moves to the parent of the verb, and collects the siblings again. The process goes on until it reaches the root. In addition, if a constituent is a PP (propositional phrase), its children are also collected. Candidates consisting of only a single punctuation mark are not considered. This heuristic works well with the correct parse trees. However, one of the errors by automatic parsers is due to incorrect PP attachment leading to missing arguments. To attempt to ﬁx this, we consider as arguments the combination of any consecutive NP and PP, and the split of NP and PP inside the NP that was chosen by the previous heuristics.  
A maximum entropy classiﬁer is used in our semantic role labeling system, which takes syntactic constituents as the labeling units. The maximum entropy classiﬁer is trained to identify and classify the predicates’ semantic arguments together. Only the constituents with the largest probability among embedding ones are kept. After predicting all arguments which have matching constituents in full parsing trees, a simple rule-based post-processing is applied to correct the arguments which have no matching constituents in these trees. Some useful features and their combinations are evaluated. 
We present a four-step hierarchical SRL strategy which generalizes the classical two-level approach (boundary detection and classiﬁcation). To achieve this, we have split the classiﬁcation step by grouping together roles which share linguistic properties (e.g. Core Roles versus Adjuncts). The results show that the nonoptimized hierarchical approach is computationally more efﬁcient than the traditional systems and it preserves their accuracy. 
Our system for semantic role labeling is multi-stage in nature, being based on tree pruning techniques, statistical methods for lexicalised feature encoding, and a C4.5 decision tree classiﬁer. We use both shallow and deep syntactic information from automatically generated chunks and parse trees, and develop a model for learning the semantic arguments of predicates as a multi-class decision problem. We evaluate the performance on a set of relatively ‘cheap’ features and report an F1 score of 68.13% on the overall test set. 
This paper describes a semantic role labeling system that uses features derived from different syntactic views, and combines them within a phrase-based chunking paradigm. For an input sentence, syntactic constituent structure parses are generated by a Charniak parser and a Collins parser. Semantic role labels are assigned to the constituents of each parse using Support Vector Machine classiﬁers. The resulting semantic role labels are converted to an IOB representation. These IOB representations are used as additional features, along with ﬂat syntactic chunks, by a chunking SVM classiﬁer that produces the ﬁnal SRL output. This strategy for combining features from three different syntactic views gives a signiﬁcant improvement in performance over roles produced by using any one of the syntactic views individually. 
In this paper we introduce a semantic role labeling system constructed on top of the full syntactic analysis of text. The labeling problem is modeled using a rich set of lexical, syntactic, and semantic attributes and learned using one-versus-all AdaBoost classiﬁers. Our results indicate that even a simple approach that assumes that each semantic argument maps into exactly one syntactic phrase obtains encouraging performance, surpassing the best system that uses partial syntax by almost 6%. 
A striking feature of human syntactic processing is that it is context-dependent, that is, it seems to take into account semantic information from the discourse context and world knowledge. In this paper, we attempt to use this insight to bridge the gap between SRL results from gold parses and from automatically-generated parses. To do this, we jointly perform parsing and semantic role labeling, using a probabilistic SRL system to rerank the results of a probabilistic parser. Our current results are negative, because a locallytrained SRL model can return inaccurate probability estimates. 
 Informatics Institute  ILK / Computational Linguistics and AI  University of Amsterdam, Kruislaan 403  Tilburg University, P.O. Box 90153,  NL-1098 SJ Amsterdam, The Netherlands  NL-5000 LE Tilburg, The Netherlands  erikt@science.uva.nl  {S.V.M.Canisius,Antal.vdnBosch,  A.M.Bogers}@uvt.nl  
In this paper, we propose a method that exploits full parsing information by representing it as features of argument classification models and as constraints in integer linear learning programs. In addition, to take advantage of SVM-based and Maximum Entropy-based argument classification models, we incorporate their scoring matrices, and use the combined matrix in the above-mentioned integer linear programs. The experimental results show that full parsing information not only increases the F-score of argument classification models by 0.7%, but also effectively removes all labeling inconsistencies, which increases the F-score by 0.64%. The ensemble of SVM and ME also boosts the F-score by 0.77%. Our system achieves an F-score of 76.53% in the development set and 76.38% in Test WSJ. 
Sparse data is a well-known problem for any probabilistic model. However, recent language acquisition proposals suggest that the data children learn from is heavily restricted - children learn only from unambiguous triggers (Fodor 1998, Dresher 1999, Lightfoot 1999) and degree-0 data (Lightfoot 1991). Surprisingly, we show that these conditions are a necessary feature of an accurate language acquisition model. We test these predictions indirectly by developing a mathematical learning and language change model inspired by Yang’s (2003, 2000) insights. Our logic is that, besides accounting for how children acquire the adult grammar so quickly, a viable acquisition proposal must also be able to account for how populations change their grammars over time. The language change we examine is the shift in Old English from a strongly Object-Verb (OV) distribution to a strongly Verb-Object (VO) distribution between 1000 A.D. and 1200 A.D., based on data from the YCOE Corpus (Taylor et al. 2003) and the PPCME2 Corpus (Kroch & Taylor 2000). Grounding our simulated population with these historical data, we demonstrate that these acquisition restrictions seem to be both sufficient and necessary for an Old English population to shift its distribution from strongly OV to strongly VO at the right time. 
Both Middle English and Old French had a syntactic property called verb-second or V2 that disappeared. In this paper describes a simulation being developed to shed light on the question of why V2 is stable in some languages, but not others. The simulation, based on a Markov chain, uses fuzzy grammars where speakers can use an arbitrary mixture of idealized grammars. Thus, it can mimic the variable syntax observed in Middle English manuscripts. The simulation supports the hypotheses that children use the topic of a sentence for word order acquisition, that acquisition takes into account the ambiguity of grammatical information available from sample sentences, and that speakers prefer to speak with more regularity than they observe in the primary linguistic data. 
Unsupervised learning of grammar is a problem that can be important in many areas ranging from text preprocessing for information retrieval and classification to machine translation. We describe an MDL based grammar of a language that contains morphology and lexical categories. We use an unsupervised learner of morphology to bootstrap the acquisition of lexical categories and use these two learning processes iteratively to help and constrain each other. To be able to do so, we need to make our existing morphological analysis less fine grained. We present an algorithm for collapsing morphological classes (signatures) by using syntactic context. Our experiments demonstrate that this collapse preserves the relation between morphology and lexical categories within new signatures, and thereby minimizes the description length of the model. 
This paper describes a heuristic for morpheme- and morphology-learning based on string edit distance. Experiments with a 7,000 word corpus of Swahili, a language with a rich morphology, support the effectiveness of this approach. 
Recent “visual worlds” studies, wherein researchers study language in context by monitoring eye-movements in a visual scene during sentence processing, have revealed much about the interaction of diverse information sources and the time course of their inﬂuence on comprehension. In this study, ﬁve experiments that trade off scene context with a variety of linguistic factors are modelled with a Simple Recurrent Network modiﬁed to integrate a scene representation with the standard incremental input of a sentence. The results show that the model captures the qualitative behavior observed during the experiments, while retaining the ability to develop the correct interpretation in the absence of visual input. 
We present a computational model of acquiring a second language from example sentences. Our learning algorithms build a construction grammar language model, and generalize using form-based patterns and the learner’s conceptual system. We use a unique professional language learning corpus, and show that substantial reliable learning can be achieved even though the corpus is very small. The model is applied to assisting the authoring of Japanese language learning corpora. 
The logical problem of language is grounded on arguments from poverty of positive evidence and arguments from poverty of negative evidence. Careful analysis of child language corpora shows that, if one assumes that children learn through item-based constructions, there is an abundance of positive evidence. Arguments regarding the poverty of negative evidence can also be addressed by the mechanism of conservative item-based learning. When conservativism is abandoned, children can rely on competition, cue construction, monitoring and probabilistic identification to derive information from positive data to recover from overgeneralization. 1. The Logical Problem Chomsky (1957, 1980) has argued that the child’s acquisition of grammar is ‘hopelessly underdetermined by the fragmentary evidence available.’ He attributed this indeterminacy to two major sources. The first is the degenerate nature of the input. According to Chomsky, the sentences heard by the child are so full of retracing, error, and incompletion that they provide no clear indication of the possible sentences of the language. Coupled with this problem of input degeneracy is the problem of unavailability of negative evidence. According to this view, children have a hard time knowing which forms of their language are acceptable and which are unacceptable, because parents fail to provide consistent evidence regarding the ungrammaticality of unacceptable sentences. Worse  still, when such evidence is provided, children appear to ignore it. Chomsky’s (1957) views about the degeneracy of the input did not stand up well to the test of time. As Newport, Gleitman & Gleitman (1977) reported, ‘the speech of mothers to children is unswervingly well-formed.’ More recently, Sagae, Lavie & MacWhinney (2004) examined several of the corpora in the CHILDES database and found that adult input to children can be parsed with an accuracy level parallel to that for corpora such as the Wall Street Journal database. 
Iglika Stoyneshka PhD Program in Linguistics The Graduate Center, City University of New York idst_r@yahoo.com  Lidiya Tornyova PhD Program in Linguistics The Graduate Center, 
In this paper, we discuss an applic ation of Maximum Entropy to modeling the acquisition of subject and object processing in Italian. The model is able to learn from corpus data a set of experimentally and theoretically well-motivated linguistic constraints, as well as their relative salience in Italian grammar development and processing. The model is also shown to acquire robust syntactic generalizations by relying on the evidence provided by a small number of high token frequency verbs only. These results are consistent with current research focusing on the role of high frequency verbs in allowing children to converge on the most salient constraints in the grammar. 
I describe steps toward “deep lexical acquisition” based on naive theories, motivated by modern results of developmental psychology. I argue that today’s machine learning paradigm is inappropriate to take these steps. Instead we must develop computational accounts of naive theory representations, mechanisms of theory acquisition, and the mapping of naive theories to lexicalizable concepts. This will enable our theories to describe the ﬂexibility of the human conceptual apparatus. 
In this paper, a novel machine learning approach for the identification of named entity relations (NERs) called positive and negative case-based learning (PNCBL) is proposed. It pursues the improvement of the identification performance for NERs through simultaneously learning two opposite cases and automatically selecting effective multi-level linguistic features for NERs and nonNERs. This approach has been applied to the identification of domain-specific and cross-sentence NERs for Chinese texts. The experimental results have shown that the overall average recall, precision, and F-measure for 14 NERs are 78.50%, 63.92% and 70.46% respectively. In addition, the above F-measure has been enhanced from 63.61% to 70.46% due to adoption of both positive and negative cases. 
We present the results of feature engineering and post-processing experiments conducted on a temporal expression recognition task. The former explores the use of different kinds of tagging schemes and of exploiting a list of core temporal expressions during training. The latter is concerned with the use of this list for postprocessing the output of a system based on conditional random ﬁelds. We ﬁnd that the incorporation of knowledge sources both for training and postprocessing improves recall, while the use of extended tagging schemes may help to offset the (mildly) negative impact on precision. Each of these approaches addresses a different aspect of the overall recognition performance. Taken separately, the impact on the overall performance is low, but by combining the approaches we achieve both high precision and high recall scores. 
We show that the intelligent use of one small piece of contextual information–a document’s publication date–can improve the performance of classiﬁers trained on a text categorization task. We focus on academic research documents, where the date of publication undoubtedly has an effect on an author’s choice of words. To exploit this contextual feature, we propose the technique of temporal feature modiﬁcation, which takes various sources of lexical change into account, including changes in term frequency, associative strength between terms and categories, and dynamic categorization systems. We present results of classiﬁcation experiments using both full text papers and abstracts of conference proceedings, showing improved classiﬁcation accuracy across the whole collection, with performance increases of greater than 40% when temporal features are exploited. The technique is fast, classiﬁerindependent, and works well even when making only a few modiﬁcations. 
In this paper, we introduce a new data representation format for language processing, the syntactic and semantic graphs (SSGs), and show its use for call classiﬁcation in spoken dialog systems. For each sentence or utterance, these graphs include lexical information (words), syntactic information (such as the part of speech tags of the words and the syntactic parse of the utterance), and semantic information (such as the named entities and semantic role labels). In our experiments, we used written language as the training data while computing SSGs and tested on spoken language. In spite of this mismatch, we have shown that this is a very promising approach for classifying complex examples, and by using SSGs it is possible to reduce the call classiﬁcation error rate by 4.74% relative. 
In this paper we examine topic segmentation of narrative documents, which are characterized by long passages of text with few headings. We ﬁrst present results suggesting that previous topic segmentation approaches are not appropriate for narrative text. We then present a featurebased method that combines features from diverse sources as well as learned features. Applied to narrative books and encyclopedia articles, our method shows results that are signiﬁcantly better than previous segmentation approaches. An analysis of individual features is also provided and the beneﬁt of generalization using outside resources is shown. 
In this paper, we present a machine learning system for identifying non-referential it. Types of non-referential it are examined to determine relevant linguistic patterns. The patterns are incorporated as features in a machine learning system which performs a binary classiﬁcation of it as referential or non-referential in a POS-tagged corpus. The selection of relevant, generalized patterns leads to a signiﬁcant improvement in performance. 
Recent natural language learning research has shown that structural kernels can be effectively used to induce accurate models of linguistic phenomena. In this paper, we show that the above properties hold on a novel task related to predicate argument classiﬁcation. A tree kernel for selecting the subtrees which encodes argument structures is applied. Experiments with Support Vector Machines on large data sets (i.e. the PropBank collection) show that such kernel improves the recognition of argument boundaries. 
We describe an extension to the technique for the automatic identification and labeling of sentiment terms described in Turney (2002) and Turney and Littman (2002). Their basic assumption is that sentiment terms of similar orientation tend to co-occur at the document level. We add a second assumption, namely that sentiment terms of opposite orientation tend not to co-occur at the sentence level. This additional assumption allows us to identify sentiment-bearing terms very reliably. We then use these newly identified terms in various scenarios for the sentiment classification of sentences. We show that our approach outperforms Turney’s original approach. Combining our approach with a Naive Bayes bootstrapping method yields a further small improvement of classifier performance. We finally compare our results to precision and recall figures that can be obtained on the same data set with labeled data. 
In this paper, we study how to generate features from various data representations, such as surface texts and parse trees, for answer extraction. Besides the features generated from the surface texts, we mainly discuss the feature generation in the parse trees. We propose and compare three methods, including feature vector, string kernel and tree kernel, to represent the syntactic features in Support Vector Machines. The experiment on the TREC question answering task shows that the features generated from the more structured data representations significantly improve the performance based on the features generated from the surface texts. Furthermore, the contribution of the individual feature will be discussed in detail. 
3.1 The Initial Pie in the Sky Example The following two consecutive sentences have been annotated for Pie in the Sky. Two Sentences From ACE Corpus File NBC20001019.1830.0181 • but Yemen's president says the FBI has told him the explosive material could only have come from the U.S., Israel or two Arab countries. • and to a former federal bomb investigator, that description suggests a powerful military-style plastic explosive c-4 that can be cut or molded into different shapes. Although the full Pie-in-the-Sky analysis includes information from many different annotation projects, the Dependency Structure in Figure 1 includes only those components that relate to PropBank, NomBank, Discourse annotation, coreference and TimeBank. Several parts of this representation require further explanation. Most of these are signified by the special arcs, arc labels, and nodes. Dashed lines represent transparent arcs, such as the transparent 7  dependency between the argument (ARG1) of modal can and the or. Or is transparent in that it allows this dependency to pass through it to cut and mold. There are two small arc loops -investigator is its own ARG0 and description is its own ARG1. Investigator is a relational noun in NomBank. There is assumed to be an underlying relation between the Investigator (ARG0), the beneficiary or employer (the ARG2) and the item investigated (ARG1). Similarly, description acts as its own ARG1 (the thing described). There are four special coreference arc labels: ARG0-CF, ARG-ANAPH, EVENTANAPH and ARG1-SBJ-CF. At the target of these arcs are pointers referring to phrases from the previous sentence or previous discourse. The first three of these labels are on arcs with the noun description as their source. The ARG0-CF label indicates that the phrase Yemen's president (**1**) is the ARG0, the one who is doing the describing. The EVENT-ANAPH label points to a previous mention of the describing event, namely the clause: The FBI told him the explosive material… (**3**). However, as noted above, the NP headed by description represents the thing described in addition to the action. The ARG-ANAPH label points to the thing that the FBI told him the explosive material can only come from … (**2**). The ARG1-SBJ-CF label links the NP from the discourse what the bomb was made from as the subject with the NP headed by explosive as its predicate, much the same as it would in a copular construction such as: What the bomb was made from is the explosive C-4. Similarly, the arc ARG1-APP marks C-4 as an apposite, also predicated to the NP headed by explosive. Finally, the thick arcs labeled SLINK-MOD represent TimeML SLINK relations between eventuality variables, i.e., the cut and molded events are modally subordinate to the suggests proposition. The merged representation aims to be compatible with the projects from which it derives, each of which analyzes a different aspect of linguistic analysis. Indeed most of the dependency labels are based on the annotation schemes of those projects. We have also provided the individual PropBank, NomBank and TimeBank annotations below in textual form, in order to highlight potential points of interaction. PropBank: and [Arg2 to a former federal bomb investigator], [Arg0 that description] [Rel_suggest.01 suggests] [Arg1 [Arg1 a powerful military-style plastic explosive c-4] that  [ArgM-MOD can] be [Rel_cut.01 cut] or [Rel_mold.01 molded] [ArgM-RESULT into different shapes]]. NomBank: and to a former [Arg2 federal] [Arg1 bomb] [Rel investigator], that description suggests a powerful [Arg2 military] - [Rel style] plastic [Arg1 explosive] c-4 that can be cut or molded into different shapes. TimeML: and to a former federal bomb investigator, that description [Event = ei1 suggests] a powerful military-style plastic explosive c-4 that can be [Event = ei2 modal=’can’ cut] or [Event = ei3 modal=’can’ molded] into different shapes. <SLINK eventInstanceID = ei1 subordinatedEventID = ei2 relType = ‘Modal’/> <SLINK eventInstanceID = ei1 subordinatedEventID = ei3 relType = ‘Modal’/>  3.2 A More Complex Example To better illustrate the interaction between annotation levels, and the importance of merging information resident in one level but not necessarily in another, consider the sentence below which has more complex temporal properties than the Pie-in-the-Sky sentences and its dependency analysis (Figure 2). According to reports, sea trials for a patrol boat developed by Kazakhstan are being conducted and the formal launch is planned for the beginning of April this year.  Figure 1. Dependency Analysis of Sentence 2 Note that the subordinating Events indicated by the TimeML SLINKS refer to the predicate argument structures labeled by PropBank, and that the ArgM-MODal also labeled by PropBank contains modality information also crucial to the SLINKS. While the grammatical modal on cut and mold is captured as an attribute value on the event tag, the governing event predicate suggest introduces a modal subordination to its internal argument, along with its relative clause. While this markup is possible in TimeML, it is difficult to standardize (or automate, algorithmically) since arguments are not marked up unless they are event denoting. 8  Figure 2. Dependency Analysis of a Sentence with Interesting Temporal Properties The graph above incorporates these distinct annotations into a merged representation, much like the previous analysis. This sentence has more TimeML annotation than the previous sentence. Note the loops of arcs which show that According to plays two roles in the sentence: (1) it heads a constituent that is the ARGM-ADV of the verbs conducted and planned; (2) it indicates that the information in this entire sentence is attributed to the reports. This loop is problematic in some sense because the adverbial appears to modify a constituent that includes itself. In actuality, however, one would expect that the ARGM-ADV role modifies the sentence minus the adverbial, the constituent that you would get if you ignore the transparent arc from ARGM-  ADV to the rest of the sentence. Alternatively, a merging decision may elect to delete the ARGMADV arcs, once the more specific predicate argument structure of the sentence adverbial annotation is available. The PropBank annotation for this sentence would label arguments for develop, conduct and plan, as given below. [ArgM-ADV According to reports], [Arg1sea trials for [Arg1 a patrol boat] [Rel_develop.02 developed] [Arg0 by Kazakhstan]] are being [Rel_conduct.01 conducted] and [Arg1 the formal launch] is [Rel_plan.01 planned] [ArgM-TMP for the beginning of April this year]. NomBank would add arguments for report, trial, launch and beginning as follows: According to [Rel_report.01 reports], [Arg1 [ArgM-LOC sea [Rel_trial.01 trials] [Arg1 for [Arg1-CF_launch.01 a patrol boat] developed by Kazakhstan] are being conducted and the [ArgM-MNR formal] [Rel_launch.01 launch] is planned for the [[REL_beginning.01 beginning] [ARG1 of April this year]]. TimeML, however, focuses on the anchoring of events to explicit temporal expressions (or document creation dates) through TLINKs, as well as subordinating relations, such as those introduced by modals, intensional predicates, and other event-selecting predicates, through SLINKs. For discussion, only part of the complete annotation is shown below. According to [Event = ei1 reports], sea [Event = ei3 trials] for a boat [Event = ei4 developed] by Kazakhstan are being [Event = ei5 conducted] and the formal [Event = ei6 launch] is [Event = ei7 planned] for the [Timex3= t1 beginning of April] [Timex3= t2 this year]. <SLINK eventID=”ei1” subordinatedEvent=”ei5, ei7” relType=EVIDENTIAL/> <TLINK eventID=”ei4” relatedToEvent =”ei3” relType=BEFORE/> <TLINK eventID=”ei6” relatedToTime=”t1” relType=IS_INCLUDED /> <SLINK eventID=”ei7” subordinatedEvent=”ei6” relType=”MODAL”/> <TLINK eventID=”ei5” relatedToEvent=”ei3” relType=IDENTITY/> Predicates such as plan and nominals such as report are lexically encoded to introduce SLINKs with a specific semantic relation, in this  case, a “MODAL” relType,. This effectively introduces an intensional context over the subordinated events. These examples illustrate the type of semantic representation we are trying to achieve. It is clear that our various layers already capture many of the intended relationships, but they do not do so in a unified, coherent fashion. Our goal is to develop both a framework and a process for annotation that allows the individual pieces to be automatically assembled into a coherent whole. 4.0 Merging Annotations 4.1 First Order Merging of Annotation We begin by discussing issues that arise in defining a single format for a merged representation of PropBank, NomBank and Coreference, the core predicate argument structures and referents for the arguments. One possible representation format would be to convert each annotation into features and values to be added to a larger feature structure. 1 The resulting feature structure would combine stand alone and offset annotation – it would include actual words and features from the text as well as special features that point to the actual text (character offsets) and, perhaps, syntactic trees (offsets along the lines of PropBank/NomBank). Alternative global annotation schemes include annotation graphs (Cieri & Bird, 2001), and MATE (Carletta, et. al., 1999). There are many areas in which the boundaries between these annotations have not been clearly defined, such as the treatment of support constructions and light verbs, as discussed below. Determining the most suitable format for the merged representation should be a top priority. 4.2 Resolving Annotation Overlap There are many possible interactions between different types of annotation: aspectual verbs have argument labels in PropBank, but are also important roles for temporal relations. Support 
This paper reports on the SYN-RA (SYNtax-based Reference Annotation) project, an on-going project of annotating German newspaper texts with referential relations. The project has developed an inventory of anaphoric and coreference relations for German in the context of a uniﬁed, XML-based annotation scheme for combining morphological, syntactic, semantic, and anaphoric information. The paper discusses how this uniﬁed annotation scheme relates to other formats currently discussed in the literature, in particular the annotation graph model of Bird and Liberman (2001) and the pie-in-thesky scheme for semantic annotation. 
We describe a parallel annotation approach for PubMed abstracts. It includes both entity/relation annotation and a treebank containing syntactic structure, with a goal of mapping entities to constituents in the treebank. Crucial to this approach is a modiﬁcation of the Penn Treebank guidelines and the characterization of entities as relation components, which allows the integration of the entity annotation with the syntactic structure while retaining the capacity to annotate and extract more complex events. 
The annotations of the Penn Discourse Treebank (PDTB) include (1) discourse connectives and their arguments, and (2) attribution of each argument of each connective and of the relation it denotes. Because the PDTB covers the same text as the Penn TreeBank WSJ corpus, syntactic and discourse annotation can be compared. This has revealed signiﬁcant differences between syntactic structure and discourse structure, in terms of the arguments of connectives, due in large part to attribution. We describe these differences, an algorithm for detecting them, and ﬁnally some experimental results. These results have implications for automating discourse annotation based on syntactic annotation. 
We investigated of the characteristics of in-text causal relations. We designed causal relation tags. With our designed tag set, three annotators annotated 750 Japanese newspaper articles. Then, using the annotated corpus, we investigated the causal relation instances from some viewpoints. Our quantitative study shows that what amount of causal relation instances are present, where these relation instances are present, and which types of linguistic expressions are used for expressing these relation instances in text. 
We present a framework for the integrated analysis of the textual and prosodic characteristics of information structure in the Switchboard corpus of conversational English. Information structure describes the availability, organisation and salience of entities in a discourse model. We present standards for the annotation of information status (old, mediated and new), and give guidelines for annotating information structure, i.e. theme/rheme and background/kontrast. We show that information structure in English can only be analysed concurrently with prosodic prominence and phrasing. This annotation, using stand-off XML in NXT, can help establish standards for the annotation of information structure in discourse. 
This paper describes extensions to a corpus annotation scheme for the manual annotation of attributions, as well as opinions, emotions, sentiments, speculations, evaluations and other private states in language. It discusses the scheme with respect to the “Pie in the Sky” Check List of Desirable Semantic Information for Annotation. We believe that the scheme is a good foundation for adding private state annotations to other layers of semantic meaning. 
The Proposition Bank (PropBank) project is aimed at creating a corpus of text annotated with information about semantic propositions. The second phase of the project, PropBank II adds additional levels of semantic annotation which include eventuality variables, co-reference, coarse-grained sense tags, and discourse connectives. This paper presents the results of the parallel PropBank II project, which adds these richer layers of semantic annotation to the ﬁrst 100K of the Chinese Treebank and its English translation. Our preliminary analysis supports the hypothesis that this additional annotation reconciles many of the surface differences between the two languages. 
This paper describes a semantically rich, human-aided machine annotation system created within the Ontological Semantics (OntoSem) environment using the DEKADE toolset. In contrast to mainstream annotation efforts, this method of annotation provides more information at a lower cost and, for the most part, shifts the maintenance of consistency to the system itself. In addition, each tagging effort not only produces knowledge resources for that corpus, but also leads to improvements in the knowledge environment that will better support subsequent tagging efforts. 
We report the results of a study of the reliability of anaphoric annotation which (i) involved a substantial number of naive subjects, (ii) used Krippendorff’s α instead of K to measure agreement, as recently proposed by Passonneau, and (iii) allowed annotators to mark anaphoric expressions as ambiguous. 
 year old students take up to 10 of these in different subjects in the UK school system.  Our aim is to investigate computational linguistics (CL) techniques in marking short free text responses automatically. Successful automatic marking of free text answers would seem to presuppose an advanced level of performance in automated natural language understanding. However, recent advances in CL techniques have opened up the possibility of being able to automate the marking of free text responses typed into a computer without having to create systems that fully understand the answers. This paper describes some of the techniques we have tried so far vis-à-vis this problem with results, discussion and description of the main issues encountered.1 1. Introduction Our aim is to investigate computational linguistics techniques in marking short free text responses automatically. The free text responses we are dealing with are answers ranging from a few words up to 5 lines. These answers are for factual science questions that typically ask candidates to state, describe, suggest, explain, etc. and where there is an objective criterion for right and wrong. These questions are from an exam known as GCSE (General Certificate of Secondary Education): most 16  2. The Data  Consider the following GCSE biology question:  Statement of the question The blood vessels help to maintain normal body temperature. Explain how the blood vessels reduce heat loss if the body temperature falls below normal.  Marking Scheme (full mark 3)2 any three: vasoconstriction; explanation (of vasoconstriction); less blood flows to / through the skin / close to the surface; less heat loss to air/surrounding/from the blood / less radiation / conduction / convection;  Here is a sample of real answers:  1. all the blood move faster and dose not go near the top of your skin they stay close to the moses 2. The blood vessels stops a large ammount of blood going to the blood capillary and sweat gland. This prents the presonne from sweating and loosing heat. 3. When the body falls below normal the blood ves- sels 'vasoconstrict' where the blood supply to the skin is cut off, increasing the metabolism of the  
An automatic generation of multiplechoice questions is one of the promising examples of educational applications of NLP techniques. A machine learning approach seems to be useful for this purpose because some of the processes can be done by classiﬁcation. Using basic machine learning algorithms as Naive Bayes and K-Nearest Neighbors, we have developed a real-time system which generates questions on English grammar and vocabulary from on-line news articles. This paper describes the current version of our system and discusses some of the issues on constructing this kind of system.  has been made to generate this kind of questions in a totally automatic way. This paper presents a novel approach to generate multiple-choice questions using machine learning techniques. The questions generated are those of ﬁllin-the-blank type, so it does not involve transforming declarative sentences into question sentences as in Mitkov’s work. This simplicity makes the method to be language independent. Although this application can be very versatile, in that it can be used to test any kind of knowledge as in history exams, as a purpose of this research we limit ourselves to testing student’s proﬁciency in a foreign language. One of the purposes of this research is to automatically extract important words or phrases in a text for a learner of the language. 2 System Design  
A Landscape Model analysis, adopted from the text processing literature, was run on transcripts of tutoring sessions, and a technique developed to count the occurrence of key physics points in the resulting connection matrices. This point-count measure was found to be well correlated with learning. 
In Online Inquiry-Based Learning (OIBL) learners search for information to answer driving questions. While learners conduct sequential related searches, the search engines interpret each query in isolation, and thus are unable to utilize task context. Consequently, learners usually get less relevant search results. We are developing a NLP-based search agent to bridge the gap between learners and search engines. Our algorithms utilize contextual features to provide user with search term suggestions and results re-ranking. Our pilot study indicates that our method can effectively enhance the quality of OIBL.  are usually unable to develop effective search terms. Many search keywords students generate are either too broad or too narrow. Although learners have specific search purposes, many times they are unable to express the purposes in keywordbased queries. In fact, by analyzing the search logs, we found that the average query length is only about 2 words. In such typical cases in OIBL, informative contexts are not presented in queries, and thus the requests become ambiguous. As a result, the search engines may not interpret the query as the learners intended to. Therefore, the results are usually not satisfactory. Given the selfregulated nature of OIBL and limited self-control skills of K-12 students, the problem is even more serious, as students may shift their focus off the task if they constantly fail to find relevant information for their DQ.  
Probabilistic Latent Semantic Analysis (PLSA) is an information retrieval technique proposed to improve the problems found in Latent Semantic Analysis (LSA). We have applied both LSA and PLSA in our system for grading essays written in Finnish, called Automatic Essay Assessor (AEA). We report the results comparing PLSA and LSA with three essay sets from various subjects. The methods were found to be almost equal in the accuracy measured by Spearman correlation between the grades given by the system and a human. Furthermore, we propose methods for improving the usage of PLSA in essay grading. 
Using keyword overlaps to identify plagiarism can result in many false negatives and positives: substitution of synonyms for each other reduces the similarity between works, making it difﬁcult to recognize plagiarism; overlap in ambiguous keywords can falsely inﬂate the similarity of works that are in fact different in content. Plagiarism detection based on verbatim similarity of works can be rendered ineffective when works are paraphrased even in superﬁcial and immaterial ways. Considering linguistic information related to creative aspects of writing can improve identiﬁcation of plagiarism by adding a crucial dimension to evaluation of similarity: documents that share linguistic elements in addition to content are more likely to be copied from each other. In this paper, we present a set of low-level syntactic structures that capture creative aspects of writing and show that information about linguistic similarities of works improves recognition of plagiarism (over tﬁdf-weighted keywords alone) when combined with similarity measurements based on tﬁdf-weighted keywords. 
Our goal is to develop tools for facilitating the authoring of conversational agents for educational applications, and in particular to enable noncomputational linguists to accomplish this task efficiently. Such a tool would benefit both learning researchers, allowing them to study dialogue in new ways, and educational technology researchers, allowing them to quickly build dialogue based help systems for tutoring systems. We argue in favor of a user-centered design methodology. We present our work-in-progress design for authoring, which is motivated by our previous tool development experiences and preliminary contextual interviews and then refined through user testing and iterative design. 
Direkt Proﬁl is an automatic analyzer of texts written in French as a second language. Its objective is to produce an evaluation of the developmental stage of students under the form of a grammatical learner proﬁle. Direkt Proﬁl carries out a sentence analysis based on developmental sequences, i.e. local morphosyntactic phenomena linked to a development in the acquisition of French. The paper presents the corpus that we use to develop the system and brieﬂy, the developmental sequences. Then, it describes the annotation that we have deﬁned, the parser, and the user interface. We conclude by the results obtained so far: on the test corpus the systems obtains a recall of 83% and a precision of 83%. 
This paper proposes the automatic generation of Fill-in-the-Blank Questions (FBQs) together with testing based on Item Response Theory (IRT) to measure English proficiency. First, the proposal generates an FBQ from a given sentence in English. The position of a blank in the sentence is determined, and the word at that position is considered as the correct choice. The candidates for incorrect choices for the blank are hypothesized through a thesaurus. Then, each of the candidates is verified by using the Web. Finally, the blanked sentence, the correct choice and the incorrect choices surviving the verification are together laid out to form the FBQ. Second, the proficiency of nonnative speakers who took the test consisting of such FBQs is estimated through IRT. Our experimental results suggest that: (1) the generated questions plus IRT estimate the non-native speakers’ English proficiency; (2) while on the other hand, the test can be completed almost perfectly by English native speakers; and (3) the number of questions can be reduced by using item information in IRT. The proposed method provides teachers and testers with a tool that reduces time and expenditure for testing English proficiency. 
This paper evaluates a series of freely available, state-of-the-art parsers on a standard benchmark as well as with respect to a set of data relevant for measuring text cohesion. We outline advantages and disadvantages of existing technologies and make recommendations. Our performance report uses traditional measures based on a gold standard as well as novel dimensions for parsing evaluation. To our knowledge this is the ﬁrst attempt to evaluate parsers accross genres and grade levels for the implementation in learning technology. 
Native speakers of English who are good readers can “sound out” words or names from printed text, even if they have never seen them before, although they may not be conscious of the strategies they use. No tools are available today that can convey that knowledge to learners, showing them the rules that apply in English text. We have adapted the letter-to-phoneme component of a text-to-speech synthesizer to a web-based software system that can teach word decoding to non-native speakers of English, English-speaking children, and adult learners. 
This paper presents the lessons learned in experimenting with Thetis1, an EC project focusing on the creation and localization of enhanced on-line pedagogical content for language learning in tourism industry. It is based on a general innovative approach to language learning that allows employees to acquire practical oral and written skills while navigating a relevant professional scenario. The approach is enabled by an underlying platform (EXILLS) that integrates virtual reality with a set of linguistic, technologies to create a new form of dynamic, extensible, goal-directed e-content. 
 foundations and core NLP algorithms. Several com-  puter science students took both courses, and thus  In Fall 2004 I introduced a new course  learned both the theoretical and the applied sides of  called Applied Natural Language Process-  NLP. Dan and I discussed the goals and content of  ing, in which students acquire an under-  our respective courses in advance, but developed the  standing of which text analysis techniques  courses independently.  are currently feasible for practical applications. The class was intended for in-  2 Course Role within the SIMS Program  terdisciplinary students with a somewhat technical background. This paper describes the topics covered and the programming exercises, emphasizing which aspects were successful and which problematic, and makes recommendations for future versions of the course.  The primary target audience of the Applied NLP course were masters students, and to a lesser extent, PhD students, in the School of Information Management and Systems. (Nevertheless, PhD students in computer science and other ﬁelds also took the course.) MIMS students (as the SIMS masters students are known) pursue a professional de-  gree studying information at the intersection of tech-  
We present some lessons we have learned from using software infrastructure to support coursework in natural language dialogue and embodied conversational agents. We have a new appreciation for the differences between coursework and research infrastructure—supporting teaching may be harder, because students require a broader spectrum of implementation, a faster learning curve and the ability to explore mistaken ideas as well as promising ones. We outline the collaborative discussion and effort we think is required to create better teaching infrastructure in the future. 
 course because it is an alternative to calculus,  This paper describes the creation of Language and Computers, a new course at the Ohio State University designed to be a broad overview of topics in computational linguistics, focusing on applications which have the most immediate relevance to students. This course satisﬁes the mathematical and  others because of curiosity about the subject matter. The course was ﬁrst taught in Winter 2004, drawing a wide range of majors, and has since expanded to three sections of up to 35 students each. In this paper we will discuss the design of the course, focusing on the success we have had in oﬀering it, as well as some of the diﬃculties we have faced.  logical analysis requirement at Ohio State by using natural language sys-  2 General Context  tems to motivate students to exercise and develop a range of basic skills in formal and computational analysis. In this paper we discuss the design of the course, focusing on the success we have had in oﬀering it, as well as some of the diﬃculties we have faced.  The Linguistics Department at OSU is the home of a leading graduate program in which 17 graduate students are currently specializing in computational linguistics. From the perspective of the graduate program, the goal of the new course development was to create more appropriate teaching opportunities for the graduate students  
 jump right into research projects in my group,  In the fall term of 2004, I taught a new statistical NLP course focusing on core tools and machine-learning algorithms. The course work was organized around four substantial programming assignments in which the students implemented the important parts of several core tools, including language models (for speech reranking), a maximum entropy classiﬁer, a part-of-speech tagger, a PCFG parser, and a word-alignment system. Using provided scaﬀolding, students built realistic tools with nearly state-of-theart performance in most cases. This paper brieﬂy outlines the coverage of the course, the scope of the assignments, and some of the lessons learned in teaching the course in this way.  there’s no question that the broad accessibility of the course, especially for non-CS students, was limited. As with any NLP course, there were several fundamental choice points. First, it’s not possible to cover both core tools and end-to-end applications in detail in a single term. Since Marti Hearst was teaching an applied NLP course during the same term, I chose to cover tools and algorithms almost exclusively (see ﬁgure 1 for a syllabus). The second choice point was whether to organize the course primarily around linguistic topics or primarily around statistical methods. I chose to follow linguistic topics because that order seemed much easier to motivate to the students (comments on this choice in section 3). The ﬁnal fundamental choice I made in deciding how to target this class was to require both substantial coding and substantial math. This choice narrowed the audience of the class, but allowed the students to build realistic systems  
 goal of understanding how a technology works and  how best to employ it for their interests.  We have built web interfaces to a number of Natural Language Processing technologies. These interfaces allow students to experiment with different inputs and view corresponding output and inner workings of the systems. When possible, the interfaces also enable the student to modify the knowledge bases of the systems and view the resulting change in behavior. Such interfaces are important because they allow students without computer science background to learn by doing. Web interfaces also sidestep issues of platform dependency in software packages, available computer lab times, etc. We discuss our basic approach and lessons learned.  In addition, getting a technology to run on a set lab machines can be problematic: the programs may be developed for a different platform, e.g., a program was developed for Linux but the lab machines run MSWindows. Another hurdle is that machine administrators are often loath to install applications that they perceive as non-standard. Finally, lab times can be restrictive and thus it is preferable to enable students to use computers to which they have easy access. Our Solution: We built web interfaces to many core NLP modules. These interfaces not only allow students to use a technology but also allow students to modify and extend the technology. This enables experimentation. We used server-side scripting languages to build such web interfaces. These  programs take input from a web browser, feed it to  
 misconceptions. For example, we have found that  Understanding the decoding algorithm for hidden Markov models is a difﬁcult task for many students. A comprehensive understanding is difﬁcult to gain from static state transition diagrams and tables of observation production probabilities. We have built a number of visualizations depicting a hidden Markov model for partof-speech tagging and the operation of the Viterbi algorithm. The visualizations are designed to help students grasp the operation of the HMM. In addition, we have found that the displays are useful as debugging tools for experienced researchers.  students often believe that as the Viterbi algorithm calculates joint state sequence observation sequence probabilities, the best state sequence so far is always a preﬁx of global best path. This is of course false. Working a long example to show this is very tedious and thus text books seldom provide such examples. Even for practitioners, HMMs are often opaque in that the cause of a mis-tagging error is often left uncharacterized. A display would be helpful to pinpoint why an HMM chose an incorrect state sequence instead of the correct one. Below we describe two displays that attempt to remedy the above mentioned problems and we discuss a Java implementation of these displays in the context of a part-of-speech tagging HMM (Kupiec,  1992). The system is freely available and has an  
The professionally oriented computer science M.S. students at Northern Illinois University are intelligent, interested in new ideas, and have good programming skills and a good math background. However, they have no linguistics background, find traditional academic prose difficult and uninteresting, and have had no exposure to research. Given this population, the assignments I have found most successful in teaching Introduction to NLP involve concrete projects where students could see for themselves the phenomena discussed in class. This paper describes three of my most successful assignments: duplicating Kernighan et al.’s Bayesian approach to spelling correction, a study of Greenberg’s universals in the student’s native language, and a dialogue generation project. For each assignment I discuss what the students learned and why the assignment was successful. 
This paper describes the cooperation of four European Universities aiming at attracting more students to European master studies in Language and Communication Technologies. The cooperation has been formally approved within the framework of the new European program “Erasmus Mundus” as a Specific Support Action in 2004. The consortium also aims at creating a sound basis for a joint master program in the field of language technology and computer science. 
 But who will develop those systems? A prerequisite  The lack of persons trained in computational linguistic methods is a severe obstacle to making the Internet and computers accessible to people all over the world in their own languages. The paper discusses the experiences of designing and teaching an introductory course in Natural Language Processing to graduate computer science students at Addis Ababa University, Ethiopia, in order to initiate the education of computational linguists in the Horn of Africa region.  to the creation of NLP applications is the education and training of computer professionals skilled in localisation and development of language processing resources. To this end, the authors were invited to conduct a course in Natural Language Processing at the School of Information Studies for Africa, Addis Ababa University, Ethiopia. As far as we know, this was the ﬁrst computational linguistics course given in Ethiopia and in the entire Horn of Africa region. There are several obstacles to progress in language processing for new languages. Firstly, the particulars of a language itself might force new strategies to be developed. Secondly, the lack of already  
The BA Language Technology program was recently introduced at the North-West University and is, to date, the only of its kind in South Africa. This paper gives an overview of the program, which consists of computational linguistic subjects as well as subjects from languages, computer science, mathematics, and statistics. A brief discussion of the content of the program and specifically the computational linguistics subjects, illustrates that the BA Language Technology program is a vocationally directed, future oriented teaching program, preparing students for both future graduate studies and a career in language technology. By means of an example, it is then illustrated how students and researchers alike benefit from working side by side on research and development projects by using a problembased, project-organized approach to curriculum design and teaching. 
 2 A Course Within Multiple Curricula  The need for a single NLP offering for a diverse mix of graduate students (including computer scientists, information scientists, and linguists) has motivated us to develop a course that provides students with a breadth of understanding of the scope of real world applications, as well as depth of knowledge of the computational techniques on which to build in later experiences. We describe the three hands-on tasks for the course that have proven successful, namely: 1) in-class group simulations of computational processes; 2) team posters and public presentations on state-of-the-art commercial NLP applications, and; 3) team projects implementing various levels of human language processing using open-source software on large textual collections. Methods of evaluation and indicators of success are also described. 
In order to arrive at a more disciplined approach to the sustained development of linguistically rich grammars, I present a methodology for grammar validation, identifying principal dimensions of the task, and illustrating the application of the method for one release cycle of the open-source English Resource Grammar. 
In our recent research, we proposed a language interpretation model to deal with an input text as a byte sequence rather than a sequence of words. It is an approach to unify the language processing model to cope with the ambiguities in word determination problem. The approach takes an input text in the early stage of language processing when the exhaustive recognition of total word identity is not necessary. In our research, we present the achievements in language identification, indexing for full text retrieval, and word candidate extraction based on the unified input byte sequence. Our experiments show comparable results with the existing word-based approaches. 
The recognition of named entities is now a welldeveloped area, with a range of symbolic and machine learning techniques that deliver high accuracy identiﬁcation and categorisation of a variety of entity types. However, there are still some named entity phenomena that present problems for existing techniques; in particular, relatively little work has explored the disambiguation of conjunctions appearing in candidate named entity strings. We demonstrate that there are in fact four distinct uses of conjunctions in the context of named entities; we present the results of some experiments using machine-learned classiﬁers to disambiguate the diﬀerent uses of the conjunction, with 81.73% of test examples being correctly classiﬁed. We provide some discussion and analysis of the problem of conjunction in named entities, and we show that there are some cases which are ambiguous even for humans. 
AnswerFinder is a framework for the development of question-answering systems. AnswerFinder is currently being used to test the applicability of graph representations for the detection and extraction of answers. In this paper we brieﬂy describe AnswerFinder and introduce our method to learn graph patterns that link questions with their corresponding answers in arbitrary sentences. The method is based on the translation of the logical forms of questions and answer sentences into graphs, and the application of operations based on graph overlaps and the construction of paths within graphs. The method is general and can be applied to any graph-based representation of the contents of questions and answers. 
This paper presents a statistical approach to unknown word type prediction for a deep HPSG grammar. Our motivation is to enhance robustness in deep processing. With a predictor which predicts lexical types for unknown words according to the context, new lexical entries can be generated on the ﬂy. The predictor is a maximum entropy based classiﬁer trained on a HPSG treebank. By exploring various feature templates and the feedback from parse disambiguation results, the predictor achieves precision over 60%. The models are general enough to be applied to other constraint-based grammar formalisms. 
Processing unknown words is disproportionately important because of their high information content. It is crucial in domains with specialist vocabularies where relevant training material is scarce, for example: biological text. Unknown word processing often begins with Part of Speech (POS) tagging, where accuracy is typically 10% worse than on known words. We demonstrate that features extracted from large raw text corpora can signiﬁcantly increase accuracy on unknown words. These features supply a large part of what we are missing with unknown words: context information about how the word is used. We describe a Maximum Entropy modelling approach which uses real-valued features to represent unannotated contextual information. Our initial experiments with real-valued features have resulted in an increased accuracy from 87.39% to 88.85% on unknown words. 
We investigate the impact of introducing ﬁner distinctions into the tagset on the accuracy of partof-speech tagging. This is a tangential approach to most recent research in the ﬁeld, which has focussed on applying diﬀerent algorithms using a very similar set of features. We outline the basic approach to tagset reﬁnement and describe preliminary ﬁndings. 
Accurately representing synonymy using distributional similarity requires large volumes of data to reliably represent infrequent words. However, the na¨ıve nearest-neighbour approach to compare context vectors extracted from large corpora scales poorly. The Spatial Approximation Sample Hierarchy (SASH) is a data-structure for performing approximate nearest-neighbour queries, and has been previously used to improve the scalability of distributional similarity searches. We add lexical semantic information from WordNet to the SASH in an attempt to improve the accuracy and efﬁciency of similarity searches. 
Word prediction is the problem of guessing which words are likely to follow in a given segment of a text to help a user with disabilities. As the user enters each letters of the required word, the system displays a list of the most probable words that could appear in that position. In our research we designed and implemented a word predictor for the Persian language. Three standard performance metrics were used to evaluate the system including keystroke saving, the most important one. The system achieved 57.57% saving in keystrokes. 1. Introduction  prediction system saves the user's energy by reducing his physical effort and also the system assists the user in the composition of the well-formed text qualitatively and quantitatively (Fazly, 2002). Moreover, the system increases user’s concentration (Klund and Novak, 2001). Traditionally, word predictors have been built based on statistical language modeling (SLM) (Gustavii and Pederssen, 2003). SLM is based on the probability of a sequence of n given words (n-gram). A number of word prediction systems are available today for English, Swedish, and other European languages. Most of these systems have used n-gram language modeling. The current research deals with the design and implementation of a word prediction system based on SLM for the Persian language.  A word prediction system facilitates typing of a text for a user with physical or cognitive disabilities. As the user enters each letter of the required word, the system displays a list of the most likely completions of the partially typed word. As the user continues typing more letters, the system updates the suggestion list accordingly based on the new context. If the required word is in the list, the user can select it with a single keystroke. Then, the system tries to predict the next word. It displays a list of suggestions to the user. If he finds the next intended word, he selects it; otherwise he enters the first letter of the next word to restrict the suggestions. The process continues to complete the text. For someone with physical disabilities, each keystroke is an effort; as a result, the  2. Related Works By looking back, early prediction systems mostly were developed in the 1980s. They were used as a writing assistance system for the one with disabilities. In the early systems, they only suggested the high frequency words that matched the partially typed word and ignored the entire previous context (Swiffin et al, 1985). SoothSayer is such a system. To make suggestions more appropriate, some systems look at a larger context by exploiting word bigram language model beside the word unigram. WordQ (Nantais et al, 2001; Shein et al, 2001) is a system which is developed for English. Profet (Carlberger et al, 1997a; Carlberger et al, 1997b) is a system developed in four  Proceedings of the Australasian Language Technology Workshop 2005, pages 57–63, Sydney, Australia, December 2005. 57  languages: English, Norwegian, Polish and French. PAL (Predicative Adaptive Lexicon) is one of the major projects at ACSD (Applied Computer Studies Division) at Dundee University, Scotland (Booth et al, 1990). These systems have used word unigrams and bigrams; also, the systems try being adapted to the user’s typing behavior by employing information on the user’s recency and frequency of use. Since there are no previous works of any developed word prediction systems for Persian, what we have done is the first attempt to design and implement a word predictor for this language. We have used the experience of the developed systems for the English and Swedish languages in our research. Details are presented in Ghayoomi (2004).  influenced by Arabic and to some extent by French, and a great amount of words are borrowed from these languages. Talking about Persian syntax, only verbs are inflected in the language. The subjective mood is widely used in it. It is an SOV language, and also a free word order language. The language does not make use of gender; not even the third person of he or she distinctions that exists in English (Assi, 2004). 4. N-gram Word Model The task of predicting the next word can be stated as attempting to estimate the probability function P: P(Wn|W1,…, Wn-1)  3. Some Facts about the Persian Language Persian is a member of the Indo-European languages and has many features in common with them in morphology, syntax, the sound system, and the lexicon. Arabic is from the Semitic family and differs from Persian in many respects. The Persian alphabet is a modified version of the Arabic alphabet. Hence it is more appropriate to the Arabic sound system and less suitable for Persian. For instance ‘‫’ز‬, ‘‫’ذ‬, ‘‫ ’ض‬and ‘‫ ’ظ‬are four alphabets both in Persian and Arabic, but all pronounced the same /z/ in Persian and differently in Arabic. So there is a little correspondence between Persian letters and sounds. Although some alphabets are written differently and there is no difference in their pronunciations, they make differentiations in the meanings of words. Letters have joined or disjoined forms; i.e. based on the position that the letters appear in a word, they have different forms. Persian writing system is right to left, the same as Arabic; but quite contrary to the European languages that have left to right writing system. The vocabularies have been greatly  In such a stochastic problem, we use the previous word(s), the history, to predict the next word. To give reasonable prediction to the words which appear together, we try to use Markov assumption that only the last few words affect the next word. So if we construct a model where all histories restrict the word that would appear in the next position, we have then an (n-1)th order Markov model or an n-gram word model. (Manning and Schüdze, 1999; Jurafsky and Martin, 2000) The aim of our study is to design a word predictor that uses a unigram (n=1), bigram (n=2), and trigram (n=3) word model for Persian. 4.1. Word Prediction Algorithm Suppose the user is typing a sentence and the following sequence has been entered so far from right to left based on Persian writing system: CWi Wi-1 Wi-2 … where Wi-2 and Wi-1 are the most recently completed words and CWi is the current word that is going to be predicted or completed. Let W be the set of all words in the lexicon that likely would appear in that  58  position. A statistical word prediction algorithm attempts to select the N most appropriate words from W that are likely to be the user’s intended words, where N is usually between 1, 5, 9 or 10 based on the experiment done by Soede and Foulds (1986). The general approach is to estimate the probability of each candidate word, wi ∈ W, being the user’s required word in that context. 5. Methodology 5.1.Corpus To do our research, we made a balanced corpus in different genres from 8 months of the on-line Hamshahri newspaper archive on the web. Although the corpus was small, it was a good representative for the Persian language. The corpus contained approximately 8 million tokens. After downloading the web pages, HTML pages were converted to their plain text equivalents. 5.2. Annotation The plain text corpus was annotated. One of the annotations was replacing various spellings of a word by a selected spelling. In Persian, some words have various spellings without any changes in the meaning. To choose one spelling among various ones, the highest frequency of use was used to consider the word as the default spelling, and the various spellings were replaced by the selected one. Replacing was done manually. By doing so, the distribution of frequencies of a word with different spellings would be gathered together to assign a single frequency to the selected spelling; because of the smallness of the corpus. For example, these four words were found in the corpus: “‫ اﻣﺮﻳﮑﺎﻳﯽ‬/?emrikāyi/”, “‫ اﻣﺮﻳﮑﺎﺋﯽ‬/?emrikā?i/”, “‫ ﺁﻣﺮﻳﮑﺎﻳﯽ‬/?āmrikāyi/”, “‫ ﺁﻣﺮﻳﮑﺎﺋﯽ‬/?āmrikā?i/”. All the words mean “American”. Between them, only the spelling “‫ ”ﺁﻣﺮﻳﮑﺎﻳﯽ‬with the highest frequency of use was selected as default and the other spellings were replaced to that.  The other annotation was removing words or phrases in the corpus from other languages or other Persian dialects comparing to the standard language that do not belong to Persian at all and not be used by native speakers of the language. Email or internet addresses were removed from the corpus. Headlines, footnotes and references in the articles were also removed. 5.3.Tokenization After annotation, the corpus was divided into three sections: one was the training corpus that contained 6258000 tokens, and 72494 types; the other section was used as the developing corpus which contained 872450 tokens, and the last section was used as the test corpus which contained 11960 tokens. To do the tokenization process, the training corpus was ran on NSP (N-gram Statistic Package), a program which was written in Perl in Linux (Banerjee and Pedersen, 2003), and uni-, bi-, and trigram statistics were extracted. Words with frequency of one and two regarded as Out-Of-Vocabulary (OOV) and only the most common sequence of words with the frequency of three and more were taken into account and the statistics of word uni-, bi-, and trigrams were extracted. In NSP a token is defined as a continuance sequence of characters to be space delimited alphanumeric strings or individual characters. 5.4. Solving Sparseness Since a big corpus includes only a fraction of n-grams, increasing n makes the distribution of the events rarer. We have used the Simple Linear Interpolation (SLI) method (Manning and Schüdze, 1999) to smooth the probability distribution. 6. Implementation 6.1. The Algorithm The architecture of our algorithm is shown in figure 1. The system we developed has four  59  Training Corpus Extracting Statistics  Updating, Adding new word  N-gram Statistics Computing Probability  Developing Corpus Computing Lambda Value  Prediction  Setting  Simulated Typist  Test Corpus  Test Result Figure 1: The architecture of our algorithm  major components: a) the statistical information extracted from the training corpus for the prediction algorithm; b) the predictive program that tries to suggest words to the simulated user. This component has two parts: one is word completion and the other one is word prediction. The prediction algorithm first completes the partially spelled word and then it predicts the probable words and present them in the suggestion list; c) a simulate user that types the test text. The simulated typist is a perfect user who always chooses the desired word when it is available in the prediction list and does not miss it; d) the component of updating the statistics of the words’ recency of use and adding new words along with their frequency of use. To get the system adaptive to the user, two processes will be done. One is extracting word uni-, bi-, and trigrams from the current text that is being entered. The other process is saving and updating the recent extracted statistical information in a dynamic file. The recent information is related to the static file which keeps the statistical information resulted from the training corpus. When the predictor tries to predict words, first it searches the dynamic file and gives more  weight to the words that are recently used; then, it uses the statistical information of the static file. Gradually as the user enters more texts, the system saves and updates the information and gets adapted to the user’s style of writing and brings up more appropriate suggestions in the prediction list. 6.2. Conditions In addition to the word prediction algorithm themselves, the parameter that varied in our experiments was the number of suggestions in the prediction list. It is assumed that the higher number of words in the suggestion list, the greater the chance of having the intended word among the suggestions; but it imposes a cognitive load on the user, because it takes the search time for the desired word longer and it is more likely that the user would miss the word they are looking for. Different users of word prediction systems may prefer different values for this parameter according to their type and level of disabilities. As it has been stated in section 4.1, Soede and Foulds (1986) experimentally identified the number of suggestions. In our work, we selected the values 1, 5 and 9 for the number of  60  suggestions. In our system, the sorting order of words in the list is based on the frequency of use in which the most probable words would appear on the top of the list. Also, in our research we designed a word processor to be compatible with the Persian specifications such as having a right to left writing system to have the cursor in its right direction. 6.3. Performance Measures  reading each letters, it determines what the correct prediction for the current position is. The prediction program then is called and a list of suggestions is returned to the user. The user searches the prediction list for the correct prediction. If it is found in the list, the user increases the amount of correct predictions by the predictor. The correctly predicted word is then completed and the user continues to read the rest of the text. The gained results are presented in table 1 for 1, 5 and 9 numbers of suggestions:  To evaluate our system, three standard performance metrics have been used in our research (Woods, 1996; Fazly, 2002): Keystroke Saving (KSS): The percentage of keystrokes that the user saves by using the word prediction system. A higher value for keystroke saving implies a better performance. Hit Rate (HR): The percentage of correct words that appear in the suggestion list without entering any letters of the next word. A higher hit rate implies a better performance. Keystroke until Prediction (KuP): The average number of keystrokes that the user enters for each word before it appears in the prediction list. A lower value for this measure implies a better performance. 7. Results To test our system, test corpus was given to the simulated typist. The length of the test corpus was 11960 words and contained 46637 characters without considering space as a character. The reason of not considering space is that after selecting any words a space will be entered automatically and the result is having a keystroke saving. On the other hand, to select a word from the list one of the Function Keys, F1 to F9, are required to be pressed to drag and drop the intended word to the text being typed. The result is that the keystroke which is saved by entering the automatic space would be lost. The virtual typist is a Visual C++ program that reads in each text letter by letter. After  
Language samples are useful as an object of study for a diverse range of people. Samples of low-density languages in particular are often valuable in their own right, yet it is these samples which are most diﬃcult to locate, especially in a vast repository of information such as the World Wide Web. We identify here some shortcomings to the more obvious approaches to locating such samples and present an alternative technique based on a search query using publicly available wordlists augmented with geospatial evidence, and show that the technique is successful for a number of languages. 
Speech synthesis or text-to-speech (TTS) systems are currently available for a number of the world’s major languages, but for thousands of the world’s ‘minor’ languages no such technology is available. While awaiting the development of such technology, we would like to try the stop-gap solution of using an existing TTS system for a major language (the base language) to ‘fake’ TTS for a minor language (the target language). This paper describes the design for an experiment which involves finding a suitable base language for the Australian Aboriginal language Pitjantjajara as a target language, and evaluating its usability in the real-life situation of providing language technology support for speakers of the target language whose understanding of the local majority language is limited, for example in the scenario of going to the doctor. 1. Introduction Speech synthesis systems, in particular text-tospeech (TTS) systems which ‘read out’ ordinary text on the computer, are now fairly widespread and are sufficiently reliable and of a suitable quality for wide acceptance and use. However, this is only true for the ‘major’ languages. For example, Microsoft’s Agent includes American and British English, Dutch, French, German, Italian, Japanese, Korean, Portuguese, Russian and Spanish. Scansoft’s RealSpeak provides for all of the above, plus Basque, Cantonese, Mandarin, Danish, two varieties of Dutch, Australian and Indian English, Canadian French, Norwegian, Polish, and two varieties of Portuguese. The list is impressive, but there are still thousands of languages not covered. Our interest is in providing language technology-based support for speakers of ‘minor’ languages1 when they find themselves in situations 
A Dual-Type automatic speech recogniser (ASR) is a multi-pass ASR system that incorporates both a speaker-independent (SI) and a speaker-dependent (SD) ASR. The purpose of this approach is to improve the robustness of spoken dialogue systems for a broader range of applications. This paper identifies feasible Dual-Type multi-pass ASR system designs that are intended to overcome limitations arising from the use of a single type of ASR. Implementation issues are also discussed. 1.Introduction Current implementations of Spoken Dialogue Systems (SDS) are developed around a single ASR. This design limits the overall system’s recognition accuracy to the performance of the installed ASR, while the useability is determined by the individual ASR type. ASRs can be categorised into two types, either SD or SI, with each having their own strengths and weaknesses. SI ASRs have the advantage of not requiring a prior enrolment or customised training session for their end users, thereby allowing any user of a given regional dialect to effectively use the system. These systems rely on an underlying grammar that typically needs to be relatively small, or at least, only have a small portion of the grammar active at any point in time. Due to requiring limited grammar size for optimum recognition accuracy, these systems are often used in a system led interaction, whereby the machine asks questions of the user that elicit simple responses. These responses may be single words, or small continuous strings. SD ASRs, on the other hand, require training for each individual user. This training is relatively short, generally less than ten minutes, and involves the speaker reading aloud a prepared text, which is analysed by the ASR for generation of the  speaker’s acoustic model. A speaker profile is created by combining this acoustic model with a vocabulary and a regional language model. SD systems are adaptive and have the advantage of being able to recognise free speech or constrained grammar speech, although extracting semantic content from the free speech is more difficult than with a formalised grammar. In addition to the fundamental differences between the two ASR types, there are also individual differences within each ASR type. The various commercial and research implementations are developed from different algorithms and techniques, typically providing varying output for the same paragraph of spoken text. ASRs are also further classified by their speech continuity as well as grammar and vocabulary size. Speech continuity describes whether words are spoken in isolation, as connected speech or as continuous speech (Zue et al., 1997). Connected speech ASRs require pauses between multiple word phrases, whereas continuous speech ASRs do not. The grammar and vocabulary size refers to the number of phrases and words that can be spoken and recognized. A grammar can be characterised by the number of plausible alternatives (perplexity), the number of rules and the number of words (Gibbon et al., 1997). A prototype Multimodal Dialogue System, incorporating an SDS, has been developed for the Future Operations Centre Analysis Laboratory (FOCAL) at Australia’s Defence Science and Technology Organisation (DSTO). FOCAL is a collaborative environment that is exploring new paradigms for situation awareness and command and control in military command centres (Wark et al. 2004). An SDS was initially implemented for FOCAL to enable natural dialogue with its Virtual Advisers (Broughton et al. 2002) using an SD ASR and later using an SI ASR (refer section 3). These Virtual Advisers are real-time animated talking heads that can deliver briefs or be queried for additional information. Figure 1 shows some of FOCAL’s Virtual Advisers on the main display during an interactive briefing session.  Proceedings of the Australasian Language Technology Workshop 2005, pages 78–86, Sydney, Australia, December 2005. 78  SDSs rely on accurate speech-to-text transcription (spoken utterance decoding) from their ASR to perform well. State-of-the-art ASRs perform optimally in quiet environments but are sensitive to interference from ambient noise, overlapping speech and reverberation (Littlefield et al., 2002). Due to the 3.6 metre radius 150° spherical screen, the reverberation characteristics of FOCAL are less than ideal, particularly near the focal point of the screen. This causes degradation in performance of ASRs used in this environment. Figure 1: Photograph of FOCAL with screen. To overcome these limitations, we are interested in the development of multi-pass systems, those requiring two or more ASR engines to improve robustness and overcome deficiencies in single ASR based SDSs. The ASR engines can either be of the same or different type, with the overall aim of improving recognition accuracy in a broader range of applications, by utilising the best features of each ASR in the SDS system. An example of an existing multi-pass system is SpeechMAX™ (Custom Speech USA, 2005), a dual-engine system that utilises two SD ASRs, in this case Scansoft’s Dragon NaturallySpeaking and IBM’s ViaVoice (ScanSoft, 2005). Pellom and Hacioglu (2003) incorporated two passes in the University of Colorado’s SONIC ASR to improve robustness in noisy environments. Furthermore, Pérez-Piñar López and García Mateo (2005) use a multiplepass ASR system where the ASRs have language models adapted from distinct topics. We are interested in a new area of research that incorporates a Dual-Type ASR to improve SDS robustness. A Dual-Type ASR is a multi-pass ASR that incorporates both a SI and a SD ASR. As discussed, SD and SI ASRs have differing advantages and disadvantages to each other, and the aim of this proposed research is to exploit the benefits of these systems to improve recognition accuracy in situations that would normally be  detrimental to these systems if used in a traditional single-pass design. Hockey et al. (2003) have developed an SDS that uses two ASRs, a grammarbased SI primary ASR a Statistical Language Model ASR Section 2 introduces components of an SDS while section 3 describes the past and present SDS in FOCAL. Components of a Dual-Type ASR are identified and explained in section 4. Section 5 describes the alternative designs for a Dual-Type ASR and issues common to all the designs are examined in section 6. Section 7 describes future implementation and experimentation. Finally, the conclusion is provided in section 8. 2.Components of an SDS The components of an SDS include a microphone, an ASR, a grammar and a Dialogue Manager. The microphone physical design, directionality, frequency response and electrical output are characteristics that help describe different microphone types and aid in the correct microphone selection for specific applications. The microphones that are used in the FOCAL environment include analogue and digital supercardioid headset microphones, and analogue supercardioid gooseneck microphones. An ASR decodes audio from spoken utterances into one or more recognition results in the form of text. By default ASRs often display only one speech-to-text interpretation, the most probable interpretation. However, ASRs can produce a list of alternative interpretations, each with a confidence score expressed as a probability or percentage. It is useful to use more than one interpretation in an SDS when another component, such as the Dialogue Manager, has more contextual information than the ASR to select the most likely recognition result. A speech recognition grammar is a list of rules and symbols that can be spoken and recognised by an ASR, often represented as a context free grammar (CFG). The format of the CFG used by an ASR is usually a standard format, such as Nuance Grammar Specification Language (GSL) (Nuance, 2001) or Java Speech Grammar Format (JSGF) (Sun Microsystems, 1998). The Dialogue Manager controls the flow of dialogue with the user and coordinates system responses. The Dialogue Manager, as implemented within FOCAL, can receive one or more recognition results from the ASR system. The additional recognition results are compared within the current dialogue context to improve likelihood of correct recognition.  79  3.FOCAL’s Current SDS FOCAL's initial SDS (Broughton et al. 2002) was developed around the SD ASR Dragon NaturallySpeaking™, chosen because of its high recognition accuracy, availability and developer support. Additional software for natural language understanding and dialogue management was developed using Natlink (Gould, 2001). Natlink enabled the development of macros and grammars for Dragon NaturallySpeaking. This initial concept system demonstrated the ability to interact with FOCAL’s Virtual Advisers. However, the major limiting factor of SD ASRs meant that only those trained with the ASR could use the system. More sophisticated grammars also needed to be implemented to enable scalability of the system. To address these issues, a second SDS was developed using a SI ASR and a more sophisticated Dialogue Manager based on an agent-based architecture (Estival et al., 2003). In this system, Nuance 8.0 (Nuance, 2001) was chosen as the SI ASR as it provided high reliability, a developer’s toolkit, and an AustralianNew Zealand acoustic language model. Regulus (Rayner et al., 2001, Regulus, 2005) was incorporated for language processing, enabling the development of typed unification grammars and their compiling into Nuance compatible contextfree grammar language models. The agent-based dialogue management system was incorporated into the larger FOCAL agent architecture (Wark et al., 2004) to enable broader application within FOCAL. Currently this system has been implemented to enable users to dialogue with the Virtual Advisers during their presentation of a brief. It enables any one of four Virtual Advisers to be asked questions relevant to their presented information. The SI ASR in FOCAL’s current SDS has two functions. Firstly, it detects, records and saves the audio from spoken utterances as wavefiles. Secondly, it decodes the audio input from the spoken utterance into recognition results. The recognition results are a set of text strings that most closely match rules in the ASR’s small CFG. The Queensland University of Technology Universal Background Model (QUT-UBM) Speaker Identification System (SID) (Pelecanos and Sridharan, 2001) which recognises a person from the sound of their voice, has also been integrated into FOCAL. The SID performs acoustic analysis of the audio from a spoken utterance and tries to match the pattern with that of a trained target user model. The system’s response is either the name of the matched target user model or “unknown”. In addition to our current SDS with the Virtual  Advisers, we are also exploring multimodal input with an immersive geospatial application (Wark et al., 2005). This system builds on our current SDS, to enable deictic referencing from pointing devices. 4.Components of a Dual-Type ASR The components of a Dual-Type ASR include a microphone, spoken Utterance Recorder, speech recognition grammar, SI ASR, SD ASR coupled with an SID, and recognition result Error Detector and Reconciler. Configurations of these components are described in section 5. The Utterance Recorder is used to detect, record and save the audio from spoken utterances as wavefiles. Although ASRs are capable of recording spoken utterances, we propose that the use of an independent spoken utterance recorder will lead to a more scalable and flexible system. This is important because more than one of the components requires the audio from spoken utterances at the same time. However, this incurs a delay, since the ASRs cannot begin to decode a spoken utterance until that utterance has finished and has been saved as a wavefile. It takes roughly as long as the duration of an utterance to decode an utterance from a wavefile. Because there is an independent Utterance Recorder, the SI ASR is only required to decode the audio input from spoken utterances into recognition results. The SD ASR also decodes the audio input from the spoken utterance into a set of recognition results. However, because this ASR has a more accurate model of a speaker’s voice pattern than the SI ASR, it can use larger grammars. Hence, SD ASRs can operate in at least two different modes: large vocabulary continuous speech (dictation mode) or small vocabulary connected speech (command mode). The dictation mode uses a large vocabulary of 20000 words or more (Zue et al., 1997). The command mode employs a userdefined CFG in a standard format. Since the SD ASR needs to know the speaker’s identity, we couple a SID system with the SD ASR in an attempt to automate this process. The Error Detector will select the best recognition result interpretations, measure agreement between the best interpretations, and identify erroneous segments of interpretations. The recognition results from each ASR include an ordered list of possible interpretations within the grammar, with each interpretation having a confidence score associated with it. The best interpretations will be selected by examining the confidence scores and choosing those above a  80  predefined threshold. The interpretation with the highest confidence score from each ASR will be compared for agreement. The assumption here is that if the ASRs produce the same recognition result and this recognition result receives a high confidence score, then it is likely to be correct. In this case a second ASR reinforces the best result of the first ASR. This comparison will be accomplished using Sclite (NIST, 2001), a software tool from the US National Institute of Standards and Technology, that provides word error rate between the two strings, a reference and a hypothesis. If the word error rate is zero, the Error Detector will flag agreement. Since there is only one recognition result in this case, the Reconciler is not required, and is bypassed. If the word error rate is greater than zero, the recognition results are aligned and compared again using Sclite. Sclite aligns the strings and identifies substitution, insertion and deletion misalignments. Part of an example report from Sclite for insertion, substitution and deletion misalignments follows.  REF: the brown ** fox JUMPED over THE lazy dog  HYP: the brown IN fox LUMPED over *** lazy dog  Eval:  I  S  D  Note that the reference (REF) is only the best recognition result based in confidence scores, not necessarily a correct recognition result. Hence, the substitution, deletion and insertion misalignments are only possible sources of errors. The degree of agreement (word error rate produced by Sclite) and the location and type of misalignments will be passed on to the Dialogue Manager which will lead to a response to query the user about the error. The best recognition results will be passed on to the Reconciler to process. It is expected that the Error Detector will require minimal processing for smaller grammars due to the high recognition accuracy achievable with them. The high recognition accuracy will provide identical outputs from both the SD and SI systems and therefore minimal work for the error detection system. As the grammars become more complex however, variation between the two ASRs is expected and providing the correct output in this situation is one of the aims of this research. The Reconciler will receive a set of recognition results from more than one ASR and produce the most probable interpretation. The Reconciler will use an existing system in the speech and language technology domain that makes a selection from multiple output strings. Multi-engine machine translation systems require a component similar to the recognition result Reconciler presented here. DEMOCRAT is an example of such a component  for deciding between multiple outputs created by automatic translation (van Zaanen and Somers, 2005). The best two or three interpretations from each of the ASRs will be sorted in order of confidence and passed on to DEMOCRAT. However, the relationship between the confidence scores from one ASR to another is unknown. The Reconciler will need to take this into account when selecting the best candidate interpretations. DEMOCRAT will produce a consensus interpretation by taking the best segments of each interpretation (van Zaanen and Somers, 2005). 5.Proposed Dual-Type ASR Designs The following Dual-Type ASR designs we have proposed incorporate one or more ASR to decode the audio input from spoken utterances into recognition results. Each iteration through an ASR is a recognition pass, and therefore, a design using one ASR is a single-pass system, and a design using two ASRs is a two-pass system and so on. The four proposed Dual-Type ASR designs are: 1. Single-pass ASR 2. Two-pass ASR in parallel 3. Two-pass ASR in parallel with error detection 4. Three-pass ASR in parallel with error detection. The first Dual-Type ASR system design being proposed is a single-pass ASR which includes a Utterance Recorder followed by a SID system where the speaker’s identity is decoded. This design is illustrated in figure 2. Figure 2: System design of the Dual-Type singlepass ASR. If the speaker is identified, then the wavefile for the utterance is passed to the SD ASR for decoding into a recognition result. If the speaker is not identified then the wavefile for the utterance is  81  passed to the SI ASR for decoding. This assumes that the SD ASR is at least as accurate as the SI ASR for large vocabularies as referred to by Merino (2001). This system does not require a Reconciler or Error Detector component. The second system design, shown in figure 3, proposes a two-pass ASR in parallel where two ASRs decode all spoken utterances concurrently. A spoken utterance recorder detects and records utterances as wavefiles, which are then decoded simultaneously using a SI ASR and a SD ASR incorporating a SID system. The Reconciler compares the recognition results and provides a reconciled result for the SDS Dialogue Manager.  Figure 3: System design for the Dual-Type twopass ASR in parallel. The third Dual-Type ASR system design being proposed is an extension of the second system. It is a two-pass system, where two ASRs decode all spoken utterances in parallel, with the addition of an Error Detector. In an effort to be more efficient, a first-pass using a SI ASR will be used every time, whereas the second-pass using a SD ASR will be used only if the Error Detector decides it is required. As before, a spoken utterance recorder detects and records utterances as wavefiles and the spoken utterances are decoded by the SI ASR and the SD ASR incorporating a SID system. However, the SI ASR recognition result is assessed for errors. If an error is detected, then the result is passed to the Reconciler and compared to the SD ASR recognition result. The Reconciler then passes a reconciled result to the SDS Dialogue Manager. If no errors are found, then the Reconciler is bypassed, and the result from the SI ASR is passed on directly to the SDS Dialogue Manager. Figure 4 illustrates the Dual-Type two-pass ASR in Parallel with Error Detection system design.  Figure 4: System design of the Dual-Type twopass ASR in Parallel with Error Detection. The last proposed design shown in figure 5 incorporates three recognition passes. As in proposal 3 (figure 4), the SI and SD ASR will be used in parallel with error detection. The third pass in this proposal is another SD ASR in dictation mode without a constrained grammar. This would be a useful approach in situations where there are out of vocabulary errors using constrained grammars. The SD ASR in dictation mode has a much larger vocabulary, which could help overcome out of vocabulary errors with constrained grammars and potentially provide a more accurate recognition result. Figure 5: System design of the Dual-Type threepass ASR in Parallel with Error Detection.  82  6.Pro and Cons of Each Proposed Design The speed, accuracy and complexity of each proposed design and its effect on SDS robustness will be compared to determine the most promising approach. A breakdown of the time delay overall is discussed in section 7.1. In the single-speaker situation, the speed of each proposed Dual-Type ASR is estimated to be 2t + 2 seconds, where t is the length of the spoken utterance in seconds. This assumes the Error Detector and Reconciler incur a negligible time delay. The accuracy and robustness of each proposed design will be determined through experimentation. Proposed designs 3 and 4 are expected to perform better due to the use of the Error Detector component. This is due to the agreement of recognition results with high confidence scores between ASRs. Also, the additional alignment data enables the Dialogue Manager to query the user when conflicting recognition results occur. That is the ability to query the user for clarification of an utterance segment when required. The complexity of each of the proposed designs can be described in terms of the number of ASR passes and the number and type of required components. Table 1 shows these terms for each of the proposed designs, 1 though 4. The more complex the design, the more effort required to build and maintain.  Design No. of SI  SID SD Rec. Err.  Passes ASR  ASR  Det.  
We present KAFTIE – an incremental knowledge acquisition framework which utilizes expert knowledge to build high quality knowledge base annotators. Using KAFTIE, a knowledge base was built based on a small data set that outperforms machine learning algorithms trained on a much bigger data set for the task of recognizing temporal relations. In particular, this can be incorporated to bootstrap the process of labeling data for domains where annotated data is not available. Furthermore, we demonstrate how machine learning can be utilized to reduce the knowledge acquisition effort. 
There has been recent interest in looking at what is required for a tree query language for linguistic corpora. One approach is to start from existing formal machinery, such as tree grammars and automata, to see what kind of machine is an appropriate underlying one for the query language. The goal of the paper is then to examine what is an appropriate machine for a linguistic tree query language, with a view to future work deﬁning a query language based on it. In this paper we review work relating XPath to regular tree grammars, and as the paper’s ﬁrst contribution show how regular tree grammars can also be a basis for extensions proposed for XPath for common linguistic corpus querying. As the paper’s second contribution we demonstrate that, on the other hand, regular tree grammars cannot describe a number of structures of interest; we then show that, instead, a slightly more powerful machine is appropriate, and indicate how linguistic tree query languages might be augmented to include this extra power. 
This work concerns a question answering tool that uses multiple Web search engines and Web question answering systems to retrieve snippets of text that may contain an exact answer for a natural language question. The method described here treats each Web information retrieval system in a unique manner in order to extract the best results they can provide. The results obtained suggest that our method is comparable with some of today’s state-of-the-art systems. 
Documents are typically marked up to enable rendering and to facilitate reuse. However, retargetting a document often requires pervasive changes to the markup. Power et al. have proposed a new level of representation called document structure which captures just those aspects of graphical organisation that are signiﬁcant for conveying meaning. These document structures can be generated automatically from rhetorical structures, abstract representations of the meaning of a text. The mapping is highly indeterminate, being governed by a large number of interacting constraints. We present a constraint programming approach to the problem, and report on early experiments with an implementation in Prolog. 
This paper considers the popular but questionable technique of ‘round-trip translation’ (RTT) as a means of evaluating free on-line Machine Translation systems. Two experiments are reported, both relating to common requirements of lay-users of MT on the web. In the first we see whether RTT can accurately predict the overall quality of the MT system. In the second, we ask whether RTT can predict the translatability of a given text. In both cases, we find RTT to be a poor predictor of quality, with high BLEU and F-scores for RTTs when the forward translation was poor. We discuss why this is the case, and conclude that, even if it seemed obvious that RTT was good for nothing, at least we now have some tangible evidence. 
Recent studies of sentiment classification (determining whether a text is “positive” or “negative”) using Appraisal theory have provided mixed results. While some good results have been obtained, it is difficult to tell what aspects of Appraisal are particularly useful for this task. In this paper, we present a series of experiments to isolate features of Appraisal, in order to compare which parts aid the task of sentiment classification on movie reviews. We report results which on the surface challenge the utility of Appraisal Hierarchies for this task, when modelled using systemic features. However in the context of making a trade-off between coverage and scale of feature space, our results appear promising. We hence discuss the need for a balance between the size of a classifier’s structure and the overall accuracy. 1. Introduction Sentiment classification is a field of growing interest in the computational linguistics world, as researchers see the need for what has been termed non-topical text analysis. Sentiment classification deals with the problem of determining whether a document is positive or negative. This task has wide-ranging applications, notably market research, and customer feedback. This paper sets about to determine the usefulness of the linguistic theory of Appraisal for Sentiment Classification. Appraisal theory describes how opinion is expressed in text. Its description is in the form of system networks denoted by a taxonomy of expressions. In this work we rely on the description of these taxonomies in Martin and White’s The Language of Evaluation: Appraisal in English (2005), for both our linguistically guided hierarchies and realisations of features1. 
This paper presents a method for detecting compound nominalisations from open data, and providing a semantic intepretation. It uses a statistical model based on conﬁdence intervals over frequencies extracted from a large, balanced corpus. Using three paraphrases of the given compound nominalisation, and interpretation preferences of its components, the algorithm achieves about 70% accuracy in classifying the semantic relationship as one of subject, and object, and 57% between subject, direct object, and prepositional object. 
This paper proposes an approach to sentencelevel paraphrase identiﬁcation by text canonicalization. The source sentence pairs are ﬁrst converted into surface text that approximates canonical forms. A decision tree learning module which employs simple lexical matching features then takes the output canonicalized texts as its input for a supervised learning process. Experiments on the Microsoft Research (MSR) Paraphrase Corpus give comparable performance to other systems that are equipped with more sophisticated lexical semantic and syntactic matching components, with a Conﬁdence-weighted Score of 0.791. An ancillary experiment using the occurrence of nominalizations suggests that the MSR Paraphrase Corpus might not be a rich source for learning paraphrasing patterns. 
This paper explores the diﬀerences in words and word usage in two corpora – one derived from newspaper text and the other from the web. A corpus of web pages is compiled from a controlled traversal of the web, producing a topicdiverse collection of 2 billion words of web text1. We compare this Web Corpus with the Gigaword Corpus, a 2 billion word collection of news articles. The Web Corpus is applied to the task of automatic thesaurus extraction, obtaining similar overall results to using the Gigaword. The quality of synonyms extracted for each target word is dependent on the word’s usage in the corpus. With many more words available on the web, a much larger Web Corpus can be created to obtain better results in diﬀerent nlp tasks. 
In this paper we present work in progress on the PoS annotation of an Italian Corpus (CORIS) developed at CILTA (University of Bologna). We aim to automatically induce the PoS tagset by analysing the distributional behaviour of Italian words by relying only on theory-neutral linguistic knowledge. To this end, we propose an algorithm that derives a possible tagset to be further interpreted and deﬁned by the linguist. The algorithm extracts information from loosely labelled dependency structures that encode only basic and broadly accepted syntactic relations, namely Head/Dependent, and the distinction of dependents into Argument vs. Adjunct. 
competition and inclusion competition.  Concept word acquisition is an important research in Nagao and Mori (1994) proposed a rapid n-gram  knowledge acquisition from text (KAT) (Cao and Sui, extraction method to extract adjacent substrings with  2003), and it is also the foundation of ontology same prefix in an ordered prefix table. It was noted  learning (Maedche, 2002). Its main purpose is to that it was an affix method intrinsically.  acquire plentiful concept words from text corpora. It From these above works, we can summarize that  is very similar to unknown word recognition (Chen there are two kinds of method to identify or acquire  and Bai, 1998), (Feng, Chen, et al., 2004) and term unknown words, that is, the non-iterative statistical  extraction (Bourigault and Jacquemin, 1999). method and the affix method. The non-iterative  However, there are subtle distinctions among these unknown word recognition (Fu and Luke, 2003),  three researches. Generally, concept word can be (Yang and Li, 2003) , (Peng and Schuurmans, 2001) ,  classified into three types: proper name, compound (Lai and Wu, 2000, 2002), (Zhang, Lv, et al., 2003 )  word and derived word. Except for these three word usually adopts n-gram statistical model that is  types, unknown word recognition also identifies combined with segmentation and combination  numeric-type compounds, and it does not concern operation to identify unknown words. It can deal  known words listed in a dictionary. Term extraction with over-segmentation, but can not tackle  (Bourigault and Jacquemin, 1999) mainly processes over-combination. In addition, the length of  domain texts, and often extracts commonly used unknown word must be restricted in order to ensure  professional terms from a specific domain text system performance. The acquired unknown words  corpus.  are often 2-grams, 3-grams and 4-grams. The affix  Fu and Luke (2003) proposed a two-stage Chinese method (Nagao and Mori, 1994) has even more  Proceedings of the Australasian Language Technology Workshop 2005, pages 184–190, Sydney, Australia, December 2005. 184  limits. For example, it can not deal with unknown identifies their types in a large corpus.  words that have not obvious affix features, and it can  not use contextual information of unknown words,  either.  This paper, motivated by the work of Chang and  Concept Word Generation  Su (1997) and Liu, Zhang, et al. (2004) presents a  hierarchical inner and outer iteration method to  OST  acquire concept words from a large-scale,  EM-CLS  CT NT  un-segmented Chinese text corpus. It has two levels  OCT  of iteration which involves concept word generation  and validation. It makes some extension on EM  algorithm and Viterbi algorithm which make up the  CAS  concept word generation. The concept word  NS SC  Viterbi-C/S  validation combines mutual information and context  OAS  entropy into a validation criterion. These two levels  of iteration can simultaneously increase the precision  and recall rate of concept word acquisition. The main contribution of this paper is that it  Concept Word Validation  proposes a HIO method for concept word acquisition.  The HIO method unifies concept word generation and validation into a consecutively iterative process so that it can increase precision and recall  SC: segmented corpus CAS: combination-ambiguity sentence NS: normal sentence OAS: overlap-ambiguity sentence  CT: candidate term OST: over-segmented term NT: normal term OCT: over-combined term  simultaneously. The rest of this paper is organized as follows: Section 2 presents the HIO method. Concept  Fig.1. The Structure of HIO Method  word generation is discussed in section 2.1, and  concept word validation is discussed in section 2.2. EM algorithm (Figueiredo, 2004), (Prescher, 2003)  The whole HIO algorithm is presented in Section 2.3. is a common method for estimating  The experiment result and error analysis are provided maximum-likelihood when missing data are present.  in section 3. Section 4 concludes this paper and It has two steps: E-step (expectation step) and M-step  outlines the future work.  (maximization step). Given the observed data x and  the  current  parameter  estimation  ^( θ  t  )  ,  E-step  computes  2. The HIO Method  the conditional expectation (with respect to the  missing data y) of the logarithm of a complete  The HIO method (a Hierarchical Inner and Outer posteriori probability function, logp(y,θ|x). Usually  iteration method) has two levels of iteration, that is, E-step is called as Q function, as illustrated in (1).  the inner iteration and the outer iteration. The Equation (2) shows the M-step of EM algorithm.  alternation of EM-CLS and Viterbi-C/S algorithm M-step chooses the parameters which can maximize  constitutes the inner iteration of the HIO – concept Q function as the estimated parameters. Through  word generation, and concept word validation consecutive iterations of E-Step and M-Step, EM  constitutes the outer iteration of the HIO. The basic algorithm can get stabilized parameters.  structure of the HIO method is illustrated in Fig. 1. The HIO method can cope with the two primary  E-Step:  problems in concept word acquisition: over-segmentation and over-combination. Data  ^ (t)  ^ (t)  Q(θ | θ ) ≡ E[log p( y,θ | x) | x,θ ]  sparseness is one of common problems in statistical language processing. Concept word acquisition is not  ^ (t) ∝ log p(θ ) + E[log p( y, x | θ ) | x,θ ] (1)  the exception. In the acquisition process, it may produce the sparse data. Katz smoothing is applied in  ^ (t) ∫ = log p(θ ) + p( y | x,θ ) log p( y, x | θ )dy  the HIO method to smooth sparse data and reduce  their effect on concept word acquisition.  M-Step:  2.1 Concept Word Generation 2.1.1 EM-CLS algorithm The EM-CLS algorithm, which is based on EM (expectation maximization) algorithm, estimates generated terms’ probability distribution and  ^ (t +1)  ^ (t)  θ = arg max Q(θ | θ ) (2)  θ  An un-segmented corpus is denoted as C={C1, C2, … ,Cn} where Ci(1 ≤ i ≤ n) represents an  185  un-segmented sentence. After segmentation, C is  converted into the segmented corpus denoted as S={S1, S2, …, Sn} where Si (1 ≤ i ≤ n) is a segmentation of Ci. The generated candidate terms1 are grouped into a set denoted as T={t1, t2, …, tm}, where tj (1≤j≤m) is the generated candidate term. If Ci is taken as the observed data, Si as the missing data, we can estimate the  maximum-likelihood of term tj with the EM algorithm which is deemed as its probability  distribution in the corpus C. Equation (3) shows the probability estimation of term tj. In (3), Si* denotes the optimal segmentation of sentence Ci, which can be achieved by the Viterbi-C/S algorithm (to be  discussed in the next section). f(tj, Si) denotes the  frequency of term tj in sentence Si.  ^  ^  t j = p(t j ),T = { p(t j ) | t j ∈ T} .  After estimating the probability of term tj, we still have to judge to which type it belongs. The candidate  term has three types that are normal term,  over-segmented term and over-combined term.  ∑ ^ ( t + 1) ∑ ∑ t j =  n  ^ (t)  f (t j , S i ) ×  p  (  S  * i  |  Ci,T  )  i =1  mn  ^ (t)  f (t j , S i ) ×  p  (  S  * i  | Ci,T  )  j =1 i=1  (3)  ∑n  ^ (t)  f (t j , S i ) ×  p  (  S  * i  ,  C  i  |T  )  =  i =1  ∑ ∑ m n  ^ (t)  f (t j , S i ) ×  p  (  S  * i  ,  C  i  |T  )  j =1 i=1  If a concept word (or meaningful word) is segmented into several components, it is called over-segmented term. For example, 高 血 糖 (hyperglycemia) is possibly spitted into 高 (high) and 血糖 (blood sugar).  If a word is combined with another word, but their combination is not a concept word (or meaningful word), it is called over-combined term, such as 但也 (but also). Equation (4) can assign a type label to tj, which is denoted by CLS(tj).  CLS (t j )  =  arg max H (i)  | {Sk  |  Sk  ∈  Sen(t j ) ∧ Sk | Sen(t j ) |  ∈  H  (i)} |  (4)  In (4), Sen(tj)={Si| tj∈Si}, H(i) denotes the sentence type label.  2.1.2 Viterbi-C/S algorithm  The Viterbi-C/S algorithm dynamically segments a  corpus using the estimated probability of candidate terms and executes combination and segmentation operations on ambiguous terms in order to achieve the optimal segmentation. After completing corpus segmentation, it judges if a sentence contains overlap ambiguity or combination ambiguity. Given a segmented sentence Si, Si=t1t2…tk (1≤j≤ k,tj∈T), it is assumed that terms are independent each other, the likelihood of sentence Si is defined as:  k ∏ p(Si ) = p(t j ) (5) j =1  Definition 1: It is the optimal segmentation that its likelihood is maximal among all segmentations of a sentence. The optimal segmentation is denoted as Si*  ^  ^  Si* = arg max p(Si | Ci ,T ) = arg max p(Si ,Ci| T ) (6)  Si  Si  Like candidate terms, segmented sentences are also classified into three types: normal sentence (N-Sen), overlap-ambiguity sentence (OA-Sen), and combination-ambiguity sentence (CA-Sen).  If a segmented sentence contains over-combined terms, it is considered as an OA-Sen.  If a segmented sentence contains over-segmented terms, it is considered as a CA-Sen.  It is observed that there is a direct correspondence between the type of candidate term and segmented sentence: normal term – N-Sen, over-segmented term – CA-Sen and over-combined term – OA-Sen.  Definition 2: Segmented Density is defined as the number of segmented term in each length unit. For a sentence Si,  SD(Si ) =  p(Si ) × NT (Si ) (7). length(Si )  For a corpus S,  ∑ ∑ p(Si ) × NT (Si )  p(Si ) × NT (Si )  SD(S ) = Si∈S length(S )  ∑ = Si∈S  (8).  length(Si )  Si∈S  In (7)-(8), NT(X) denotes the number of terms in sentence X, and length(Y) denotes the length of sentence Y. The type of segmented sentence is measured by (9). Setting a threshold range [r1, r2] (r1<r2), if CLS(Si)<r1, Si is a OA-Sen, if CLS(Si)>r2, Si is a CA-Sen, if r1≤CLS(Si)≤r2, Si is a N-Sen.  
We argue it is better to program in a natural language such as English, instead of a programming language like Java. A natural language interface for programming should result in greater readability, as well as making possible a more intuitive way of writing code. In contrast to previous controlled language systems, we allow unrestricted syntax, using wide-coverage syntactic and semantic methods to extract information from the user’s instructions. We also look at how people actually give programming instructions in English, collecting and annotating a corpus of such statements. We identify differences between sentences in this corpus and in typical newspaper text, and the effect they have on how we process the natural language input. Finally, we demonstrate a prototype system, that is capable of translating some English instructions into executable code. 
Previous systems that automatically tag text with FrameNet labels have been trained from the FrameNet example data, as there is no FrameNet annotated corpus. The FrameNet data is systematically biased by the criteria for the examples’ selection, as annotators attempt to select simple sentences that include the target word. Instead of using the FrameNet examples, we train a maximum entropy model classiﬁer to identify verb frames on text from the Penn Treebank. We use examples of verbs with only one entry in FrameNet as training data, and evaluate the system on human annotated text from the Wall Street Journal. We accurately identify the frame used by 76% of ﬁnite verbs. We also investigate how well the system performs on verbs it has not encountered before. This task examines the feasibility of using the system to automatically extend the coverage of FrameNet by classifying verbs with no FrameNet entries. The classiﬁer accurately assigns a frame to 55% of instances of verbs it has not been trained on. 
In this paper we describe a modular system architecture for distributed parse annotation using interactive correction. This involves interactively adding constraints to an existing parse until the returned parse is correct. Using a mixed initiative approach, human annotators interact live with distributed ccg parser servers through an annotation gui. The examples presented to each annotator are selected by an active learning framework to maximise the value of the annotated corpus for machine learners. We report on an initial implementation based on a distributed workﬂow architecture. 
A fundamental problem for systems that require natural language understanding capabilities is the identiﬁcation of instances of semantic equivalence and paraphrase in text. The PASCAL Recognising Textual Entailment (RTE) challenge is a recently proposed research initiative that addressed this problem by providing an evaluation framework for the development of generic “semantic engines” that can be used to identify language variability in a variety of applications such as Information Retrieval, Machine Translation and Question Answering. This paper discusses the suitability of the RTE evaluation datasets as a framework for evaluating the problem of redundancy recognition in multi-document summarisation, i.e. the identiﬁcation of repetitive information across documents. This paper also reports on the development of an additional dataset containing examples of informationally equivalent sentence pairs that are typically found in machine generated summaries. The performance of a competitive entailment recognition system on this dataset is also reported. 
The paper outlines the development and design of a speech driven control for a personal in-car navigation system which runs on a standard Pocket PC. The modified system enables speech driven menu navigation, speech shortcut commands and interactive dialogues. The speech recognition method is presented, sources of inaccurate recognition are identified, and solutions are presented. Speech recognition accuracies of 96% and 88%, depending on the task, are achieved in an in-car environment. One draw back is the time taken to perform the recognition. The speech driven control module which interfaces with the in-car navigator is designed to be flexible. These features are discussed. 
We present an approach to multi-device dialogue that evaluates and selects amongst candidate dialogue moves based on features at multiple levels. Multiple sources of information can be combined, multiple speech recognition and parsing hypotheses tested, and multiple devices and moves considered to choose the highest scoring hypothesis overall. The approach has the added beneﬁt of potentially re-ordering n-best lists of inputs, eﬀectively correcting errors in speech recognition or parsing. A current application includes conversational interaction with a collection of in-car devices. 
Instant Messaging (IM) chat sessions are real-time, text-based conversations which can be analyzed using dialogue-act models. Dialogue acts represent the semantic information of an utterance, however, messages must be segmented into utterances before classiﬁcation can take place. We describe and compare two statistical methods for automatic utterance segmentation and dialogue-act classiﬁcation in task-based IM dialogue. It is shown that IM messages can be automatically segmented and classiﬁed to a very high accuracy using statistical machine learning. 
We describe a dialogue system that works with its interlocutor to identify objects. Our contributions include a concise, modular architecture with reversible processes of understanding and generation, an information-state model of reference, and ﬂexible links between semantics and collaborative problem solving. 
We present an API developed to access GermaNet, a lexical semantic database for German represented in XML. The API provides a set of software functions for parsing and retrieving information from GermaNet. Then, we present a case study which builds upon the GermaNet API and implements an application for computing semantic relatedness according to ﬁve different metrics. The package can, again, serve as a software library to be deployed in natural language processing applications. A graphical user interface allows to interactively experiment with the system. 
We present the currently most efﬁcient solver for scope underspeciﬁcation; it also converts between different underspeciﬁcation formalisms and counts readings. Our tool makes the practical use of large-scale grammars with (underspeciﬁed) semantic output more feasible, and can be used in grammar debugging. 
CL Research began experimenting with massive XML tagging of texts to answer questions in TREC 2002. In DUC 2003, the experiments were extended into text summarization. Based on these experiments, The Knowledge Management System (KMS) was developed to combine these two capabilities and to serve as a unified basis for other types of document exploration. KMS has been extended to include web question answering, both general and topic-based summarization, information extraction, and document exploration. The document exploration functionality includes identification of semantically similar concepts and dynamic ontology creation. As development of KMS has continued, user modeling has become a key research issue: how will different users want to use the information they identify. 
Zhangzhi Hu  Cathy Wu  Department of Biochemistry and Molecular Biology  Georgetown University Medical Center  3900 Reservoir Road, NW, Washington, DC 20057  {zh9,wuc}@georgetown.edu  Abstract: With the overwhelming amount of biological knowledge stored in free text, natural language processing (NLP) has received much attention recently to make the task of managing information recorded in free text more feasible. One requirement for most NLP systems is the ability to accurately recognize biological entity terms in free text and the ability to map these terms to corresponding records in databases. Such task is called biological named entity tagging. In this paper, we present a system that automatically constructs a protein entity dictionary, which contains gene or protein names associated with UniProt identifiers using online resources. The system can run periodically to always keep up-to-date with these online resources. Using online resources that were available on Dec. 25, 2004, we obtained 4,046,733 terms for 1,640,082 entities. The dictionary can be accessed from the following website: http://biocreative.ifsm.umbc.edu/biothesauru s/. Contact: hfliu@umbc.edu 
Recently there is a need for a QA system to answer not only factoid questions but also descriptive questions. Descriptive questions are questions which need answers that contain definitional information about the search term or describe some special events. We have proposed a new descriptive QA model and presented the result of a system which we have built to answer descriptive questions. We defined 10 Descriptive Answer Type(DAT)s as answer types for descriptive questions. We discussed how our proposed model was applied to the descriptive question with some experiments. 
This paper presents the results of the development of a high throughput, real time modularized text analysis and information retrieval system that identifies clinically relevant entities in clinical notes, maps the entities to several standardized nomenclatures and makes them available for subsequent information retrieval and data mining. The performance of the system was validated on a small collection of 351 documents partitioned into 4 query topics and manually examined by 3 physicians and 3 nurse abstractors for relevance to the query topics. We find that simple key phrase searching results in 73% recall and 77% precision. A combination of NLP approaches to indexing improve the recall to 92%, while lowering the precision to 67%.  ability into account. The Mayo Clinic’s repository of clinical notes contains over 16 million documents growing at the rate of 50K documents per week. The time and space required for processing these large amounts of data impose constraints on the complexity of NLP systems. Another engineering challenge is to make the NLP systems work in real time. This is particularly important in a clinical environment for patient recruitment or patient identification for clinical research use cases. In order to satisfy this requirement, a text processing system has to interface with the Electronic Health Record (EHR) system in real time and process documents immediately after they become available electronically. All of these are non-trivial issues and are currently being addressed in the community. In this poster we present the design and architecture of a large-scale, highly modularized, real-time enabled text analysis system as well as experimental validation results. 2 System Description  
Clarissa, an experimental voice enabled procedure browser that has recently been deployed on the International Space Station (ISS), is to the best of our knowledge the ﬁrst spoken dialog system in space. This paper gives background on the system and the ISS procedures, then discusses the research developed to address three key problems: grammarbased speech recognition using the Regulus toolkit; SVM based methods for open microphone speech recognition; and robust side-effect free dialogue management for handling undos, corrections and conﬁrmations. 
The Linguist’s Search Engine (LSE) was designed to provide an intuitive, easy-touse interface that enables language researchers to seek linguistically interesting examples on the Web, based on syntactic and lexical criteria. We briefly describe its user interface and architecture, as well as recent developments that include LSE search capabilities for Chinese. 
This paper introduces a method for learning to find translation of a given source term on the Web. In the approach, the source term is used as a query and part of patterns to retrieve and extract translations in Web pages. The method involves using a bilingual term list to learn sourcetarget surface patterns. At runtime, the given term is submitted to a search engine then the candidate translations are extracted from the returned summaries and subsequently ranked based on the surface patterns, occurrence counts, and transliteration knowledge. We present a prototype called TermMine that applies the method to translate terms. Evaluation on a set of encyclopedia terms shows that the method significantly outperforms the state-of-the-art online machine translation systems. 
The paper presents the Position Speciﬁc Posterior Lattice (PSPL), a novel lossy representation of automatic speech recognition lattices that naturally lends itself to efﬁcient indexing and subsequent relevance ranking of spoken documents. In experiments performed on a collection of lecture recordings — MIT iCampus data — the spoken document ranking accuracy was improved by 20% relative over the commonly used baseline of indexing the 1-best output from an automatic speech recognizer. The inverted index built from PSPL lattices is compact — about 20% of the size of 3-gram ASR lattices and 3% of the size of the uncompressed speech — and it allows for extremely fast retrieval. Furthermore, little degradation in performance is observed when pruning PSPL lattices, resulting in even smaller indexes — 5% of the size of 3-gram ASR lattices. 
We describe how context-sensitive, usertailored output is speciﬁed and produced in the COMIC multimodal dialogue system. At the conference, we will demonstrate the user-adapted features of the dialogue manager and text planner. 
We demonstrate TextRank – a system for unsupervised extractive summarization that relies on the application of iterative graphbased ranking algorithms to graphs encoding the cohesive structure of a text. An important characteristic of the system is that it does not rely on any language-speciﬁc knowledge resources or any manually constructed training data, and thus it is highly portable to new languages or domains.  cally designed to address this problem, by using an extractive summarization technique that does not require any training data or any language-speciﬁc knowledge sources. TextRank can be effectively applied to the summarization of documents in different languages without any modiﬁcations of the algorithm and without any requirements for additional data. Moreover, results from experiments performed on standard data sets have demonstrated that the performance of TextRank is competitive with that of some of the best summarization systems available today. 2 Extractive Summarization  
This paper describes SENSELEARNER – a minimally supervised word sense disambiguation system that attempts to disambiguate all content words in a text using WordNet senses. We evaluate the accuracy of SENSELEARNER on several standard sense-annotated data sets, and show that it compares favorably with the best results reported during the recent SENSEVAL evaluations.  exceeded by their supervised lexical-sample alternatives, they have however the advantage of providing larger coverage. In this paper, we present a method for solving the semantic ambiguity of all content words in a text. The algorithm can be thought of as a minimally supervised word sense disambiguation algorithm, in that it uses a relatively small data set for training purposes, and generalizes the concepts learned from the training data to disambiguate the words in the test data set. As a result, the algorithm does not need a separate classiﬁer for each word to be disambiguated, but instead it learns global models for general word categories.  
We report an empirical study on the role of syntactic features in building a semisupervised named entity (NE) tagger. Our study addresses two questions: What types of syntactic features are suitable for extracting potential NEs to train a classifier in a semi-supervised setting? How good is the resulting NE classifier on testing instances dissimilar from its training data? Our study shows that constituency and dependency parsing constraints are both suitable features to extract NEs and train the classifier. Moreover, the classifier showed significant accuracy improvement when constituency features are combined with new dependency feature. Furthermore, the degradation in accuracy on unfamiliar test cases is low, suggesting that the trained classifier generalizes well. 
We present a portable translator that recognizes and translates phrases on signboards and menus as captured by a builtin camera. This system can be used on PDAs or mobile phones and resolves the difﬁculty of inputting some character sets such as Japanese and Chinese if the user doesn’t know their readings. Through the high speed mobile network, small images of signboards can be quickly sent to the recognition and translation server. Since the server runs state of the art recognition and translation technology and huge dictionaries, the proposed system offers more accurate character recognition and machine translation. 
We demonstrate a system for ﬂexible querying against text that has been annotated with the results of NLP processing. The system supports self-overlapping and parallel layers, integration of syntactic and ontological hierarchies, ﬂexibility in the format of returned results, and tight integration with SQL. We present a query language and its use on examples taken from the NLP literature. 
Annotated corpora are valuable resources for developing Natural Language Processing applications. This work focuses on acquiring annotated data for multilingual processing applications. We present an annotation environment that supports a web-based user-interface for acquiring word alignments between English and Chinese as well as a visualization tool for researchers to explore the annotated data. 
We have previously introduced a method of word sense disambiguation that computes the intended sense of a target word, using WordNet-based measures of semantic relatedness (Patwardhan et al., 2003). SenseRelate::TargetWord is a Perl package that implements this algorithm. The disambiguation process is carried out by selecting that sense of the target word which is most related to the context words. Relatedness between word senses is measured using the WordNet::Similarity Perl modules. 
The problem of part-of-speech induction from text involves two aspects: Firstly, a set of word classes is to be derived automatically. Secondly, each word of a vocabulary is to be assigned to one or several of these word classes. In this paper we present a method that solves both problems with good accuracy. Our approach adopts a mixture of statistical methods that have been successfully applied in word sense induction. Its main advantage over previous attempts is that it reduces the syntactic space to only the most important dimensions, thereby almost eliminating the otherwise omnipresent problem of data sparseness. 
We present an overview of TARSQI, a modular system for automatic temporal annotation that adds time expressions, events and temporal relations to news texts. 
This paper describes recent progress on the TRIPS architecture for developing spoken-language dialogue systems. The interactive poster session will include demonstrations of two systems built using TRIPS: a computer purchasing assistant, and an object placement (and manipulation) task. 
We briefly describe a two-way speech-tospeech English-Farsi translation system prototype developed for use in doctorpatient interactions. The overarching philosophy of the developers has been to create a system that enables effective communication, rather than focusing on maximizing component-level performance. The discussion focuses on the general approach and evaluation of the system by an independent government evaluation team. 
Suppose you are on a mobile device with no keyboard (e.g., a cell or PDA). How can you enter text quickly? T9? Graffiti? This demo will show how language modeling can be used to speed up data entry, both in the mobile context, as well as the desktop. The Wild Thing encourages users to use wildcards (*). A language model finds the k-best expansions. Users quickly figure out when they can get away with wildcards. General purpose trigram language models are effective for the general case (unrestricted text), but there are important special cases like searching over popular web queries, where more restricted language models are even more effective. 
This paper describes a method of interactively visualizing and directing the process of translating a sentence. The method allows a user to explore a model of syntax-based statistical machine translation (MT), to understand the model’s strengths and weaknesses, and to compare it to other MT systems. Using this visualization method, we can ﬁnd and address conceptual and practical problems in an MT system. In our demonstration at ACL, new users of our tool will drive a syntaxbased decoder for themselves. 
We describe a new approach for synthetically combining the output of several different Machine Translation (MT) engines operating on the same input. The goal is to produce a synthetic combination that surpasses all of the original systems in translation quality. Our approach uses the individual MT engines as “black boxes” and does not require any explicit cooperation from the original MT systems. A decoding algorithm uses explicit word matches, in conjunction with confidence estimates for the various engines and a trigram language model in order to score and rank a collection of sentence hypotheses that are synthetic combinations of words from the various original engines. The highest scoring sentence hypothesis is selected as the final output of our system. Experiments, using several Arabicto-English systems of similar quality, show a substantial improvement in the quality of the translation output. 
SenseClusters is a freely available system that identiﬁes similar contexts in text. It relies on lexical features to build ﬁrst and second order representations of contexts, which are then clustered using unsupervised methods. It was originally developed to discriminate among contexts centered around a given target word, but can now be applied more generally. It also supports methods that create descriptive and discriminating labels for the discovered clusters. 
We present an implemented XML data model and a new, simpliﬁed query language for multi-level annotated corpora. The new query language involves automatic conversion of queries into the underlying, more complicated MMAXQL query language. It supports queries for sequential and hierarchical, but also associative (e.g. coreferential) relations. The simpliﬁed query language has been designed with non-expert users in mind. 
Computational humor will be needed in interfaces, no less than other cognitive capabilities. There are many practical settings where computational humor will add value. Among them there are: business world applications (such as advertisement, e-commerce, etc.), general computer-mediated communication and human-computer interaction, increase in the friendliness of natural language interfaces, educational and edutainment systems. In particular in the educational ﬁeld it is an important resource for getting selective attention, help in memorizing names and situations etc. And we all know how well it works with children. Automated humor production in general is a very difﬁcult task but we wanted to prove that some results can be achieved even in short time. We have worked at a concrete limited problem, as the core of the European Project HAHAcronym. The main goal of HAHAcronym has been the realization of an acronym ironic reanalyzer and generator as a proof of concept in a focalized but non restricted context. To implement this system some general tools have been adapted, or developed for the humorous context. Systems output has been submitted to evaluation by human subjects, with a very positive result.  
We propose a method of organizing reading materials for vocabulary learning. It enables us to select a concise set of reading texts (from a target corpus) that contains all the target vocabulary to be learned. We used a specialized vocabulary for an English certiﬁcation test as the target vocabulary and used English Wikipedia, a free-content encyclopedia, as the target corpus. The organized reading materials would enable learners not only to study the target vocabulary efﬁciently but also to gain a variety of knowledge through reading. The reading materials are available on our web site. 
We propose a new method for reformatting web documents by extracting semantic structures from web pages. Our approach is to extract trees that describe hierarchical relations in documents. We developed an algorithm for this task by employing the EM algorithm and clustering techniques. Preliminary experiments showed that our approach was more effective than baseline methods. 
This paper describes a hybrid model that combines a rule-based model with two statistical models for the task of POS guessing of Chinese unknown words. The rule-based model is sensitive to the type, length, and internal structure of unknown words, and the two statistical models utilize contextual information and the likelihood for a character to appear in a particular position of words of a particular length and POS category. By combining models that use different sources of information, the hybrid model achieves a precision of 89%, a signiﬁcant improvement over the best result reported in previous studies, which was 69%. 
In this paper, we develop a methodology for discovering the thematic structure of the Qur’an based on a fundamental idea in data mining and related disciplines: that, with respect to some collection of texts, the lexical frequency profiles of the individual texts are a good indicator of their conceptual content, and thus provide a reliable criterion for their classification relative to one another. This idea is applied to the discovery of thematic interrelationships among the suras (chapters) of the Qur’an by abstracting lexical frequency data from them and then applying hierarchical cluster analysis to that data. The results reported here indicate that the proposed methodology yields usable results in understanding the Qur’an on the basis of its lexical semantics. 
This paper presents a status quo of an ongoing research study of collocations – an essential linguistic phenomenon having a wide spectrum of applications in the ﬁeld of natural language processing. The core of the work is an empirical evaluation of a comprehensive list of automatic collocation extraction methods using precision-recall measures and a proposal of a new approach integrating multiple basic methods and statistical classiﬁcation. We demonstrate that combining multiple independent techniques leads to a signiﬁcant performance improvement in comparison with individual basic methods. 
We present new statistical models for jointly labeling multiple sequences and apply them to the combined task of partof-speech tagging and noun phrase chunking. The model is based on the Factorial Hidden Markov Model (FHMM) with distributed hidden states representing partof-speech and noun phrase sequences. We demonstrate that this joint labeling approach, by enabling information sharing between tagging/chunking subtasks, outperforms the traditional method of tagging and chunking in succession. Further, we extend this into a novel model, Switching FHMM, to allow for explicit modeling of cross-sequence dependencies based on linguistic knowledge. We report tagging/chunking accuracies for varying dataset sizes and show that our approach is relatively robust to data sparsity. 
In this work we present a method for Named Entity Recognition (NER). Our method does not rely on complex linguistic resources, and apart from a hand coded system, we do not use any languagedependent tools. The only information we use is automatically extracted from the documents, without human intervention. Moreover, the method performs well even without the use of the hand coded system. The experimental results are very encouraging. Our approach even outperformed the hand coded system on NER in Spanish, and it achieved high accuracies in Portuguese. 
Semantic relations between text concepts denote the core elements of lexical semantics. This paper presents a model for the automatic detection of INTENTION semantic relation. Our approach ﬁrst identiﬁes the syntactic patterns that encode intentions, then we select syntactic and semantic features for a SVM learning classiﬁer. In conclusion, we discuss the application of INTENTION relations to Q&A. 
Software to translate English text into American Sign Language (ASL) animation can improve information accessibility for the majority of deaf adults with limited English literacy. ASL natural language generation (NLG) is a special form of multimodal NLG that uses multiple linguistic output channels. ASL NLG technology has applications for the generation of gesture animation and other communication signals that are not easily encoded as text strings. 
Sentiment Classiﬁcation seeks to identify a piece of text according to its author’s general feeling toward their subject, be it positive or negative. Traditional machine learning techniques have been applied to this problem with reasonable success, but they have been shown to work well only when there is a good match between the training and test data with respect to topic. This paper demonstrates that match with respect to domain and time is also important, and presents preliminary experiments with training data labeled with emoticons, which has the potential of being independent of domain, topic and time. 
The part-whole relation is of special importance in biomedicine: structure and process are organised along partitive axes. Anatomy, for example, is rich in partwhole relations. This paper reports preliminary experiments on part-whole extraction from a corpus of anatomy deﬁnitions, using a fully automatic iterative algorithm to learn simple lexico-syntactic patterns from multiword terms. The experiments show that meronyms can be extracted using these patterns. A failure analysis points out factors that could contribute to improvements in both precision and recall, including pattern generalisation, pattern pruning, and term matching. The analysis gives insights into the relationship between domain terminology and lexical relations, and into evaluation strategies for relation learning. 
 2 From Lexical Cohesion to Anchoring  This paper describes a reader-based experiment on lexical cohesion, detailing the task given to readers and the analysis of the experimental data. We conclude with discussion of the usefulness of the data in future research on lexical cohesion. 
In contrast to the latest progress in speech recognition, the state-of-the-art in natural language generation for spoken language dialog systems is lagging behind. The core dialog managers are now more sophisticated; and natural-sounding and flexible output is expected, but not achieved with current simple techniques such as template-based systems. Portability of systems across subject domains and languages is another increasingly important requirement in dialog systems. This paper presents an outline of LEGEND, a system that is both portable and generates natural-sounding output. This goal is achieved through the novel use of existing lexical resources such as FrameNet and WordNet. 
In this paper a method to incorporate linguistic information regarding single-word and compound verbs is proposed, as a ﬁrst step towards an SMT model based on linguistically-classiﬁed phrases. By substituting these verb structures by the base form of the head verb, we achieve a better statistical word alignment performance, and are able to better estimate the translation model and generalize to unseen verb forms during translation. Preliminary experiments for the English - Spanish language pair are performed, and future research lines are detailed. 
This paper presents the results of automatically inducing a Combinatory Categorial Grammar (CCG) lexicon from a Turkish dependency treebank. The fact that Turkish is an agglutinating free wordorder language presents a challenge for language theories. We explored possible ways to obtain a compact lexicon, consistent with CCG principles, from a treebank which is an order of magnitude smaller than Penn WSJ.  The rest of this section contains an overview of the underlying formalism (1.1). This is followed by a review of the relevant work (1.2). In Section 2, the properties of the data are explained. Section 3 then gives a brief sketch of the algorithm used to induce a CCG lexicon, with some examples of how certain phenomena in Turkish are handled. As is likely to be the case for most languages for the foreseeable future, the Turkish treebank is quite small (less than 60K words). A major emphasis in the project is on generalising the induced lexicon to improve coverage. Results and future work are discussed in the last two sections.  
Instant Messaging chat sessions are realtime text-based conversations which can be analyzed using dialogue-act models. We describe a statistical approach for modelling and detecting dialogue acts in Instant Messaging dialogue. This involved the collection of a small set of task-based dialogues and annotating them with a revised tag set. We then dealt with segmentation and synchronisation issues which do not arise in spoken dialogue. The model we developed combines naive Bayes and dialogue-act n-grams to obtain better than 80% accuracy in our tagging experiment. 
This work presents a model for learning inference procedures for story comprehension through inductive generalization and reinforcement learning, based on classified examples. The learned inference procedures (or strategies) are represented as of sequences of transformation rules. The approach is compared to three prior systems, and experimental results are presented demonstrating the efficacy of the model. 
We present a Czech-English statistical machine translation system which performs tree-to-tree translation of dependency structures. The only bilingual resource required is a sentence-aligned parallel corpus. All other resources are monolingual. We also refer to an evaluation method and plan to compare our system’s output with a benchmark system.  
In Sayeed and Szpakowicz (2004), we proposed a parser inspired by some aspects of the Minimalist Program. This incremental parser was designed speciﬁcally to handle discontinuous constituency phenomena for NPs in Latin. We take a look at the application of this parser to a speciﬁc kind of apparent island violation in Latin involving the extraction of constituents, including subjects, from tensed embedded clauses. We make use of ideas about the left periphery from Rizzi (1997) to modify our parser in order to handle apparently violated subject islands and similar phenomena. 
In this paper, we study different centrality measures being used in predicting noun phrases appearing in the abstracts of scientific articles. Our experimental results show that centrality measures improve the accuracy of the prediction in terms of both precision and recall. We also found that the method of constructing Noun Phrase Network significantly influences the accuracy when using the centrality heuristics itself, but is negligible when it is used together with other text features in decision trees. 
We report on an investigation of the pragmatic category of topic in Danish dialog and its correlation to surface features of NPs. Using a corpus of 444 utterances, we trained a decision tree system on 16 features. The system achieved nearhuman performance with success rates of 84–89% and F1-scores of 0.63–0.72 in 10fold cross validation tests (human performance: 89% and 0.78). The most important features turned out to be preverbal position, deﬁniteness, pronominalisation, and non-subordination. We discovered that NPs in epistemic matrix clauses (e.g. “I think . . . ”) were seldom topics and we suspect that this holds for other interpersonal matrix clauses as well. 
This paper investigates the automatic identiﬁcation of aspects of Information Structure (IS) in texts. The experiments use the Prague Dependency Treebank which is annotated with IS following the Praguian approach of Topic Focus Articulation. We automatically detect t(opic) and f(ocus), using node attributes from the treebank as basic features and derived features inspired by the annotation guidelines. We show the performance of C4.5, Bagging, and Ripper classiﬁers on several classes of instances such as nouns and pronouns, only nouns, only pronouns. A baseline system assigning always f(ocus) has an F-score of 42.5%. Our best system obtains 82.04%. 
Large vocabulary continuous speech recognition of inﬂective languages, such as Czech, Russian or Serbo-Croatian, is heavily deteriorated by excessive out of vocabulary rate. In this paper, we tackle the problem of vocabulary selection, language modeling and pruning for inﬂective languages. We show that by explicit reduction of out of vocabulary rate we can achieve signiﬁcant improvements in recognition accuracy while almost preserving the model size. Reported results are on Czech speech corpora. 
This paper describes a word and phrase alignment approach based on a dependency analysis of French/English parallel corpora, referred to as alignment by “syntax-based propagation.” Both corpora are analysed with a deep and robust dependency parser. Starting with an anchor pair consisting of two words that are translations of one another within aligned sentences, the alignment link is propagated to syntactically connected words. 
We present an unsupervised system that exploits linguistic knowledge resources, namely English and German lexical databases and the World Wide Web, to identify English inclusions in German text. We describe experiments with this system and the corpus which was developed for this task. We report the classiﬁcation results of our system and compare them to the performance of a trained machine learner in a series of in- and crossdomain experiments. 
This paper reports the corpus-oriented development of a wide-coverage Japanese HPSG parser. We ﬁrst created an HPSG treebank from the EDR corpus by using heuristic conversion rules, and then extracted lexical entries from the treebank. The grammar developed using this method attained wide coverage that could hardly be obtained by conventional manual development. We also trained a statistical parser for the grammar on the treebank, and evaluated the parser in terms of the accuracy of semantic-role identiﬁcation and dependency analysis.  of a target grammar is constructed ﬁrst, and various grammatical constraints are extracted from the treebank. Previous studies reported that wide-coverage grammars can be obtained at low cost by using this method. (Hockenmaier and Steedman, 2002; Miyao et al., 2004) The treebank can also be used for training statistical disambiguation models, and hence we can construct a statistical parser for the extracted grammar. The corpus-oriented method enabled us to develop a Japanese HPSG parser with semantic information, whose coverage on real-world sentences is 95.3%. This high coverage allowed us to evaluate the parser in terms of the accuracy of dependency analysis on real-world texts, the evaluation measure that is previously used for more statistically-oriented parsers.  
This paper describes adaptations of unsupervised word sense discrimination techniques to the problem of name discrimination. These methods cluster the contexts containing an ambiguous name, such that each cluster refers to a unique underlying person or place. We also present new techniques to assign meaningful labels to the discovered clusters. 
We present a search-based approach to automatic surface realization given a corpus of domain sentences. Using heuristic search based on a statistical language model and a structure we introduce called an inheritance table we overgenerate a set of complete syntactic-semantic trees that are consistent with the given semantic structure and have high likelihood relative to the language model. These trees are then lexicalized, linearized, scored, and ranked. This model is being developed to generate real-time navigation instructions. 
In machine learning, whether one can build a more accurate classiﬁer by using unlabeled data (semi-supervised learning) is an important issue. Although a number of semi-supervised methods have been proposed, their effectiveness on NLP tasks is not always clear. This paper presents a novel semi-supervised method that employs a learning paradigm which we call structural learning. The idea is to ﬁnd “what good classiﬁers are like” by learning from thousands of automatically generated auxiliary classiﬁcation problems on unlabeled data. By doing so, the common predictive structure shared by the multiple classiﬁcation problems can be discovered, which can then be used to improve performance on the target problem. The method produces performance higher than the previous best results on CoNLL’00 syntactic chunking and CoNLL’03 named entity chunking (English and German). 
Conditional Random Fields (CRFs) have been applied with considerable success to a number of natural language processing tasks. However, these tasks have mostly involved very small label sets. When deployed on tasks with larger label sets, the requirements for computational resources mean that training becomes intractable. This paper describes a method for training CRFs on such tasks, using error correcting output codes (ECOC). A number of CRFs are independently trained on the separate binary labelling tasks of distinguishing between a subset of the labels and its complement. During decoding, these models are combined to produce a predicted label sequence which is resilient to errors by individual models. Error-correcting CRF training is much less resource intensive and has a much faster training time than a standardly formulated CRF, while decoding performance remains quite comparable. This allows us to scale CRFs to previously impossible tasks, as demonstrated by our experiments with large label sets. 
Recent work on Conditional Random Fields (CRFs) has demonstrated the need for regularisation to counter the tendency of these models to overﬁt. The standard approach to regularising CRFs involves a prior distribution over the model parameters, typically requiring search over a hyperparameter space. In this paper we address the overﬁtting problem from a different perspective, by factoring the CRF distribution into a weighted product of individual “expert” CRF distributions. We call this model a logarithmic opinion pool (LOP) of CRFs (LOP-CRFs). We apply the LOP-CRF to two sequencing tasks. Our results show that unregularised expert CRFs with an unregularised CRF under a LOP can outperform the unregularised CRF, and attain a performance level close to the regularised CRF. LOP-CRFs therefore provide a viable alternative to CRF regularisation without the need for hyperparameter search. 
The limited coverage of lexical-semantic resources is a signiﬁcant problem for NLP systems which can be alleviated by automatically classifying the unknown words. Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET. Ciaramita and Johnson (2003) present a tagger which uses synonym set glosses as annotated training examples. We describe an unsupervised approach, based on vector-space similarity, which does not require annotated examples but signiﬁcantly outperforms their tagger. We also demonstrate the use of an extremely large shallow-parsed corpus for calculating vector-space semantic similarity. 
Word Sense Disambiguation suffers from a long-standing problem of knowledge acquisition bottleneck. Although state of the art supervised systems report good accuracies for selected words, they have not been shown to be promising in terms of scalability. In this paper, we present an approach for learning coarser and more general set of concepts from a sense tagged corpus, in order to alleviate the knowledge acquisition bottleneck. We show that these general concepts can be transformed to ﬁne grained word senses using simple heuristics, and applying the technique for recent SENSEVAL data sets shows that our approach can yield state of the art performance. 
We describe an automatic Word Sense Disambiguation (WSD) system that disambiguates verb senses using syntactic and semantic features that encode information about predicate arguments and semantic classes. Our system performs at the best published accuracy on the English verbs of Senseval-2. We also experiment with using the gold-standard predicateargument labels from PropBank for disambiguating ﬁne-grained WordNet senses and course-grained PropBank framesets, and show that disambiguation of verb senses can be further improved with better extraction of semantic roles. 
To improve the interaction between students and an intelligent tutoring system, we developed two Natural Language generators, that we systematically evaluated in a three way comparison that included the original system as well. We found that the generator which intuitively produces the best language does engender the most learning. Speciﬁcally, it appears that functional aggregation is responsible for the improvement. 
In this paper we present a new approach to controlling the behaviour of a natural language generation system by correlating internal decisions taken during free generation of a wide range of texts with the surface stylistic characteristics of the resulting outputs, and using the correlation to control the generator. This contrasts with the generate-andtest architecture adopted by most previous empirically-based generation approaches, offering a more efficient, generic and holistic method of generator control. We illustrate the approach by describing a system in which stylistic variation (in the sense of Biber (1988)) can be effectively controlled during the generation of short medical information texts. 
We describe a new sentence realization framework for text-to-text applications. This framework uses IDL-expressions as a representation formalism, and a generation mechanism based on algorithms for intersecting IDL-expressions with probabilistic language models. We present both theoretical and empirical results concerning the correctness and efﬁciency of these algorithms. 
This paper deﬁnes a generative probabilistic model of parse trees, which we call PCFG-LA. This model is an extension of PCFG in which non-terminal symbols are augmented with latent variables. Finegrained CFG rules are automatically induced from a parsed corpus by training a PCFG-LA model using an EM-algorithm. Because exact parsing with a PCFG-LA is NP-hard, several approximations are described and empirically compared. In experiments using the Penn WSJ corpus, our automatically trained model gave a performance of 86.6% (F¥ , sentences ¦ 40 words), which is comparable to that of an unlexicalized PCFG parser created using extensive manual feature selection. 
This paper reports the development of loglinear models for the disambiguation in wide-coverage HPSG parsing. The estimation of log-linear models requires high computational cost, especially with widecoverage grammars. Using techniques to reduce the estimation cost, we trained the models using 20 sections of Penn Treebank. A series of experiments empirically evaluated the estimation techniques, and also examined the performance of the disambiguation models on the parsing of real-world sentences. 
We present an effective training algorithm for linearly-scored dependency parsers that implements online largemargin multi-class training (Crammer and Singer, 2003; Crammer et al., 2003) on top of efﬁcient parsing techniques for dependency trees (Eisner, 1996). The trained parsers achieve a competitive dependency accuracy for both English and Czech with no language speciﬁc enhancements. 
In order to realize the full potential of dependency-based syntactic parsing, it is desirable to allow non-projective dependency structures. We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures. Experiments using data from the Prague Dependency Treebank show that the combined system can handle nonprojective constructions with a precision sufﬁcient to yield a signiﬁcant improvement in overall parsing accuracy. This leads to the best reported performance for robust non-projective parsing of Czech. 
This paper suggests refinements for the Distributional Similarity Hypothesis. Our proposed hypotheses relate the distributional behavior of pairs of words to lexical entailment – a tighter notion of semantic similarity that is required by many NLP applications. To automatically explore the validity of the defined hypotheses we developed an inclusion testing algorithm for characteristic features of two words, which incorporates corpus and web-based feature sampling to overcome data sparseness. The degree of hypotheses validity was then empirically tested and manually analyzed with respect to the word sense level. In addition, the above testing algorithm was exploited to improve lexical entailment acquisition. 
We address the rating-inference problem, wherein rather than simply decide whether a review is “thumbs up” or “thumbs down”, as in previous sentiment analysis work, one must determine an author’s evaluation with respect to a multi-point scale (e.g., one to ﬁve “stars”). This task represents an interesting twist on standard multi-class text categorization because there are several different degrees of similarity between class labels; for example, “three stars” is intuitively closer to “four stars” than to “one star”. We ﬁrst evaluate human performance at the task. Then, we apply a metaalgorithm, based on a metric labeling formulation of the problem, that alters a given ¦ -ary classiﬁer’s output in an explicit attempt to ensure that similar items receive similar labels. We show that the meta-algorithm can provide signiﬁcant improvements over both multi-class and regression versions of SVMs when we employ a novel similarity measure appropriate to the problem. 
In this paper, we present an unsupervised methodology for propagating lexical cooccurrence vectors into an ontology such as WordNet. We evaluate the framework on the task of automatically attaching new concepts into the ontology. Experimental results show 73.9% attachment accuracy in the first position and 81.3% accuracy in the top-5 positions. This framework could potentially serve as a foundation for ontologizing lexical-semantic resources and assist the development of other largescale and internally consistent collections of semantic information. 
We propose a method for extracting semantic orientations of words: desirable or undesirable. Regarding semantic orientations as spins of electrons, we use the mean ﬁeld approximation to compute the approximate probability function of the system instead of the intractable actual probability function. We also propose a criterion for parameter selection on the basis of magnetization. Given only a small number of seed words, the proposed method extracts semantic orientations with high accuracy in the experiments on English lexicon. The result is comparable to the best value ever reported.  have a positive attitude on the topic. The goal of this paper is to propose a method for automatically creating such a word list from glosses (i.e., deﬁnition or explanation sentences ) in a dictionary, as well as from a thesaurus and a corpus. For this purpose, we use spin model, which is a model for a set of electrons with spins. Just as each electron has a direction of spin (up or down), each word has a semantic orientation (positive or negative). We therefore regard words as a set of electrons and apply the mean ﬁeld approximation to compute the average orientation of each word. We also propose a criterion for parameter selection on the basis of magnetization, a notion in statistical physics. Magnetization indicates the global tendency of polarization. We empirically show that the proposed method works well even with a small number of seed words. 2 Related Work  
This paper considers the problem of automatic assessment of local coherence. We present a novel entity-based representation of discourse which is inspired by Centering Theory and can be computed automatically from raw text. We view coherence assessment as a ranking learning problem and show that the proposed discourse representation supports the effective learning of a ranking function. Our experiments demonstrate that the induced model achieves signiﬁcantly higher accuracy than a state-of-the-art coherence model. 
Processing discourse connectives is important for tasks such as discourse parsing and generation. For these tasks, it is useful to know which connectives can signal the same coherence relations. This paper presents experiments into modelling the substitutability of discourse connectives. It shows that substitutability effects distributional similarity. A novel variancebased function for comparing probability distributions is found to assist in predicting substitutability. 
In this paper, we view coreference resolution as a problem of ranking candidate partitions generated by different coreference systems. We propose a set of partition-based features to learn a ranking model for distinguishing good and bad partitions. Our approach compares favorably to two state-of-the-art coreference systems when evaluated on three standard coreference data sets. 
In this paper we focus on how to improve pronoun resolution using the statisticsbased semantic compatibility information. We investigate two unexplored issues that inﬂuence the effectiveness of such information: statistics source and learning framework. Speciﬁcally, we for the ﬁrst time propose to utilize the web and the twin-candidate model, in addition to the previous combination of the corpus and the single-candidate model, to compute and apply the semantic information. Our study shows that the semantic compatibility obtained from the web can be effectively incorporated in the twin-candidate learning model and signiﬁcantly improve the resolution of neutral pronouns. 
Discriminative reranking is one method for constructing high-performance statistical parsers (Collins, 2000). A discriminative reranker requires a source of candidate parses for each sentence. This paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse-to-ﬁne generative parser (Charniak, 2000). This method generates 50-best lists that are of substantially higher quality than previously obtainable. We used these parses as the input to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91.0% on sentences of length 100 or less. 
Previous research applying kernel methods to natural language parsing have focussed on proposing kernels over parse trees, which are hand-crafted based on domain knowledge and computational considerations. In this paper we propose a method for deﬁning kernels in terms of a probabilistic model of parsing. This model is then trained, so that the parameters of the probabilistic model reﬂect the generalizations in the training data. The method we propose then uses these trained parameters to deﬁne a kernel for reranking parse trees. In experiments, we use a neural network based statistical parser as the probabilistic model, and use the resulting kernel with the Voted Perceptron algorithm to rerank the top 20 parses from the probabilistic model. This method achieves a signiﬁcant improvement over the accuracy of the probabilistic model. 
This paper introduces a new application of boosting for parse reranking. Several parsers have been proposed that utilize the all-subtrees representation (e.g., tree kernel and data oriented parsing). This paper argues that such an all-subtrees representation is extremely redundant and a comparable accuracy can be achieved using just a small set of subtrees. We show how the boosting algorithm can be applied to the all-subtrees representation and how it selects a small and relevant feature set efﬁciently. Two experiments on parse reranking show that our method achieves comparable or even better performance than kernel methods and also improves the testing efﬁciency. 
To facilitate the use of syntactic information in the study of child language acquisition, a coding scheme for Grammatical Relations (GRs) in transcripts of parent-child dialogs has been proposed by Sagae, MacWhinney and Lavie (2004). We discuss the use of current NLP techniques to produce the GRs in this annotation scheme. By using a statistical parser (Charniak, 2000) and memorybased learning tools for classiﬁcation (Daelemans et al., 2004), we obtain high precision and recall of several GRs. We demonstrate the usefulness of this approach by performing automatic measurements of syntactic development with the Index of Productive Syntax (Scarborough, 1990) at similar levels to what child language researchers compute manually. 
This paper describes a novel framework for interactive question-answering (Q/A) based on predictive questioning. Generated off-line from topic representations of complex scenarios, predictive questions represent requests for information that capture the most salient (and diverse) aspects of a topic. We present experimental results from large user studies (featuring a fully-implemented interactive Q/A system named FERRET) that demonstrates that surprising performance is achieved by integrating predictive questions into the context of a Q/A dialogue. 
This paper regards Question Answering (QA) as Question-Biased Term Extraction (QBTE). This new QBTE approach liberates QA systems from the heavy burden imposed by question types (or answer types). In conventional approaches, a QA system analyzes a given question and determines the question type, and then it selects answers from among answer candidates that match the question type. Consequently, the output of a QA system is restricted by the design of the question types. The QBTE directly extracts answers as terms biased by the question. To conﬁrm the feasibility of our QBTE approach, we conducted experiments on the CRL QA Data based on 10-fold cross validation, using Maximum Entropy Models (MEMs) as an ML technique. Experimental results showed that the trained system achieved 0.36 in MRR and 0.47 in Top5 accuracy. 
This paper presents a corpus study that explores the extent to which captions contribute to recognizing the intended message of an information graphic. It then presents an implemented graphic interpretation system that takes into account a variety of communicative signals, and an evaluation study showing that evidence obtained from shallow processing of the graphic’s caption has a signiﬁcant impact on the system’s success. This work is part of a larger project whose goal is to provide sight-impaired users with effective access to information graphics.  
The paper considers how to scale up dialogue protocols to multilogue, settings with multiple conversationalists. We extract two benchmarks to evaluate scaled up protocols based on the long distance resolution possibilities of nonsentential utterances in dialogue and multilogue in the British National Corpus. In light of these benchmarks, we then consider three possible transformations to dialogue protocols, formulated within an issue-based approach to dialogue management. We show that one such transformation yields protocols for querying and assertion that fulﬁll these benchmarks. 
Clariﬁcation requests (CRs) in conversation ensure and maintain mutual understanding and thus play a crucial role in robust dialogue interaction. In this paper, we describe a corpus study of CRs in task-oriented dialogue and compare our ﬁndings to those reported in two prior studies. We ﬁnd that CR behavior in task-oriented dialogue differs signiﬁcantly from that in everyday conversation in a number of ways. Moreover, the dialogue type, the modality and the channel quality all inﬂuence the decision of when to clarify and at which level of the grounding process. Finally we identify formfunction correlations which can inform the generation of CRs. 
Non-sentential utterances (e.g., shortanswers as in “Who came to the party?”— “Peter.”) are pervasive in dialogue. As with other forms of ellipsis, the elided material is typically present in the context (e.g., the question that a short answer answers). We present a machine learning approach to the novel task of identifying fragments and their antecedents in multiparty dialogue. We compare the performance of several learning algorithms, using a mixture of structural and lexical features, and show that the task of identifying antecedents given a fragment can be learnt successfully (f (0.5) = .76); we discuss why the task of identifying fragments is harder (f (0.5) = .41) and ﬁnally report on a combined task (f (0.5) = .38). 
In this paper we describe a novel data structure for phrase-based statistical machine translation which allows for the retrieval of arbitrarily long phrases while simultaneously using less memory than is required by current decoder implementations. We detail the computational complexity and average retrieval times for looking up phrase translations in our sufﬁx array-based data structure. We show how sampling can be used to reduce the retrieval time by orders of magnitude with no loss in translation quality. 
 English sentence e is modeled as:  We present a statistical phrase-based translation model that uses hierarchical phrases— phrases that contain subphrases. The model is formally a synchronous context-free grammar but is learned from a bitext without any syntactic information. Thus it can be seen as a shift to the formal machinery of syntaxbased translation systems without any linguistic commitment. In our experiments using BLEU as a metric, the hierarchical phrasebased model achieves a relative improvement of 7.5% over Pharaoh, a state-of-the-art phrase-based system. 
We describe a novel approach to statistical machine translation that combines syntactic information in the source language with recent advances in phrasal translation. This method requires a source-language dependency parser, target language word segmentation and an unsupervised word alignment component. We align a parallel corpus, project the source dependency parse onto the target sentence, extract dependency treelet translation pairs, and train a tree-based ordering model. We describe an efficient decoder and show that using these treebased models in combination with conventional SMT models provides a promising approach that incorporates the power of phrasal SMT with the linguistic generality available in a parser. 1. Introduction Over the past decade, we have witnessed a revolution in the field of machine translation (MT) toward statistical or corpus-based methods. Yet despite this success, statistical machine translation (SMT) has many hurdles to overcome. While it excels at translating domain-specific terminology and fixed phrases, grammatical generalizations are poorly captured and often mangled during translation (Thurmair, 04). 1.1. Limitations of string-based phrasal SMT State-of-the-art phrasal SMT systems such as (Koehn et al., 03) and (Vogel et al., 03) model translations of phrases (here, strings of adjacent words, not syntactic constituents) rather than individual words. Arbitrary reordering of words is allowed within memorized phrases, but typically  Colin Cherry University of Alberta Edmonton, Alberta Canada T6G 2E1 colinc@cs.ualberta.ca only a small amount of phrase reordering is allowed, modeled in terms of offset positions at the string level. This reordering model is very limited in terms of linguistic generalizations. For instance, when translating English to Japanese, an ideal system would automatically learn largescale typological differences: English SVO clauses generally become Japanese SOV clauses, English post-modifying prepositional phrases become Japanese pre-modifying postpositional phrases, etc. A phrasal SMT system may learn the internal reordering of specific common phrases, but it cannot generalize to unseen phrases that share the same linguistic structure. In addition, these systems are limited to phrases contiguous in both source and target, and thus cannot learn the generalization that English not may translate as French ne…pas except in the context of specific intervening words. 1.2. Previous work on syntactic SMT1 The hope in the SMT community has been that the incorporation of syntax would address these issues, but that promise has yet to be realized. One simple means of incorporating syntax into SMT is by re-ranking the n-best list of a baseline SMT system using various syntactic models, but Och et al. (04) found very little positive impact with this approach. However, an n-best list of even 16,000 translations captures only a tiny fraction of the ordering possibilities of a 20 word sentence; re-ranking provides the syntactic model no opportunity to boost or prune large sections of that search space. Inversion Transduction Grammars (Wu, 97), or ITGs, treat translation as a process of parallel parsing of the source and target language via a synchronized grammar. To make this process 
This paper presents a probabilistic framework, QARLA, for the evaluation of text summarisation systems. The input of the framework is a set of manual (reference) summaries, a set of baseline (automatic) summaries and a set of similarity metrics between summaries. It provides i) a measure to evaluate the quality of any set of similarity metrics, ii) a measure to evaluate the quality of a summary using an optimal set of similarity metrics, and iii) a measure to evaluate whether the set of baseline summaries is reliable or may produce biased results. Compared to previous approaches, our framework is able to combine different metrics and evaluate the quality of a set of metrics without any a-priori weighting of their relative importance. We provide quantitative evidence about the effectiveness of the approach to improve the automatic evaluation of text summarisation systems by combining several similarity metrics. 
In Statistics-Based Summarization - Step One: Sentence Compression, Knight and Marcu (Knight and Marcu, 2000) (K&M) present a noisy-channel model for sentence compression. The main difﬁculty in using this method is the lack of data; Knight and Marcu use a corpus of 1035 training sentences. More data is not easily available, so in addition to improving the original K&M noisy-channel model, we create unsupervised and semi-supervised models of the task. Finally, we point out problems with modeling the task in this way. They suggest areas for future research. 
This paper describes a summarization system for technical chats and emails on the Linux kernel. To reflect the complexity and sophistication of the discussions, they are clustered according to subtopic structure on the sub-message level, and immediate responding pairs are identified through machine learning methods. A resulting summary consists of one or more mini-summaries, each on a subtopic from the discussion. 
This paper presents the ﬁrst probabilistic parsing results for French, using the recently released French Treebank. We start with an unlexicalized PCFG as a baseline model, which is enriched to the level of Collins’ Model 2 by adding lexicalization and subcategorization. The lexicalized sister-head model and a bigram model are also tested, to deal with the ﬂatness of the French Treebank. The bigram model achieves the best performance: 81% constituency F-score and 84% dependency accuracy. All lexicalized models outperform the unlexicalized baseline, consistent with probabilistic parsing results for English, but contrary to results for German, where lexicalization has only a limited effect on parsing performance. 
In this paper, we present an unlexicalized parser for German which employs smoothing and sufﬁx analysis to achieve a labelled bracket F-score of 76.2, higher than previously reported results on the NEGRA corpus. In addition to the high accuracy of the model, the use of smoothing in an unlexicalized parser allows us to better examine the interplay between smoothing and parsing results. 
Consistency of corpus annotation is an essential property for the many uses of annotated corpora in computational and theoretical linguistics. While some research addresses the detection of inconsistencies in positional annotation (e.g., partof-speech) and continuous structural annotation (e.g., syntactic constituency), no approach has yet been developed for automatically detecting annotation errors in discontinuous structural annotation. This is signiﬁcant since the annotation of potentially discontinuous stretches of material is increasingly relevant, from treebanks for free-word order languages to semantic and discourse annotation. In this paper we discuss how the variation n-gram error detection approach (Dickinson and Meurers, 2003a) can be extended to discontinuous structural annotation. We exemplify the approach by showing how it successfully detects errors in the syntactic annotation of the German TIGER corpus (Brants et al., 2002). 
In this paper we present a quantitative and qualitative analysis of annotation in the Hinoki treebank of Japanese, and investigate a method of speeding annotation by using part-of-speech tags. The Hinoki treebank is a Redwoods-style treebank of Japanese dictionary deﬁnition sentences. 5,000 sentences are annotated by three different annotators and the agreement evaluated. An average agreement of 65.4% was found using strict agreement, and 83.5% using labeled precision. Exploiting POS tags allowed the annotators to choose the best parse with 19.5% fewer decisions. 
Sitting at the intersection between statistics and machine learning, Dynamic Bayesian Networks have been applied with much success in many domains, such as speech recognition, vision, and computational biology. While Natural Language Processing increasingly relies on statistical methods, we think they have yet to use Graphical Models to their full potential. In this paper, we report on experiments in learning edit distance costs using Dynamic Bayesian Networks and present results on a pronunciation classiﬁcation task. By exploiting the ability within the DBN framework to rapidly explore a large model space, we obtain a 40% reduction in error rate compared to a previous transducer-based method of learning edit distance. 
Stochastic Optimality Theory (Boersma, 1997) is a widely-used model in linguistics that did not have a theoretically sound learning method previously. In this paper, a Markov chain Monte-Carlo method is proposed for learning Stochastic OT Grammars. Following a Bayesian framework, the goal is ﬁnding the posterior distribution of the grammar given the relative frequencies of input-output pairs. The Data Augmentation algorithm allows one to simulate a joint posterior distribution by iterating two conditional sampling steps. This Gibbs sampler constructs a Markov chain that converges to the joint distribution, and the target posterior can be derived as its marginal distribution. 
Conditional random ﬁelds (Lafferty et al., 2001) are quite effective at sequence labeling tasks like shallow parsing (Sha and Pereira, 2003) and namedentity extraction (McCallum and Li, 2003). CRFs are log-linear, allowing the incorporation of arbitrary features into the model. To train on unlabeled data, we require unsupervised estimation methods for log-linear models; few exist. We describe a novel approach, contrastive estimation. We show that the new technique can be intuitively understood as exploiting implicit negative evidence and is computationally efﬁcient. Applied to a sequence labeling problem—POS tagging given a tagging dictionary and unlabeled text—contrastive estimation outperforms EM (with the same feature set), is more robust to degradations of the dictionary, and can largely recover by modeling additional features. 
Most current statistical natural language processing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use. We show how to solve this dilemma with Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference. We use this technique to augment an existing CRF-based information extraction system with long-distance dependency models, enforcing label consistency and extraction template consistency constraints. This technique results in an error reduction of up to 9% over state-of-the-art systems on two established information extraction tasks. 
The applicability of many current information extraction techniques is severely limited by the need for supervised training data. We demonstrate that for certain ﬁeld structured extraction tasks, such as classiﬁed advertisements and bibliographic citations, small amounts of prior knowledge can be used to learn effective models in a primarily unsupervised fashion. Although hidden Markov models (HMMs) provide a suitable generative model for ﬁeld structured text, general unsupervised HMM learning fails to learn useful structure in either of our domains. However, one can dramatically improve the quality of the learned structure by exploiting simple prior knowledge of the desired solutions. In both domains, we found that unsupervised methods can attain accuracies with 400 unlabeled examples comparable to those attained by supervised methods on 50 labeled examples, and that semi-supervised methods can make good use of small amounts of labeled data. 
This paper presents a novel algorithm for the acquisition of Information Extraction patterns. The approach makes the assumption that useful patterns will have similar meanings to those already identiﬁed as relevant. Patterns are compared using a variation of the standard vector space model in which information from an ontology is used to capture semantic similarity. Evaluation shows this algorithm performs well when compared with a previously reported document-centric approach. 
We directly investigate a subject of much recent debate: do word sense disambigation models help statistical machine translation quality? We present empirical results casting doubt on this common, but unproved, assumption. Using a state-ofthe-art Chinese word sense disambiguation model to choose translation candidates for a typical IBM statistical MT system, we ﬁnd that word sense disambiguation does not yield signiﬁcantly better translation quality than the statistical machine translation system alone. Error analysis suggests several key factors behind this surprising ﬁnding, including inherent limitations of current statistical MT architectures. 
Shortage of manually sense-tagged data is an obstacle to supervised word sense disambiguation methods. In this paper we investigate a label propagation based semisupervised learning algorithm for WSD, which combines labeled and unlabeled data in learning process to fully realize a global consistency assumption: similar examples should have similar labels. Our experimental results on benchmark corpora indicate that it consistently outperforms SVM when only very few labeled examples are available, and its performance is also better than monolingual bootstrapping, and comparable to bilingual bootstrapping. 
In this paper we present a supervised Word Sense Disambiguation methodology, that exploits kernel methods to model sense distinctions. In particular a combination of kernel functions is adopted to estimate independently both syntagmatic and domain similarity. We deﬁned a kernel function, namely the Domain Kernel, that allowed us to plug “external knowledge” into the supervised learning process. External knowledge is acquired from unlabeled data in a totally unsupervised way, and it is represented by means of Domain Models. We evaluated our methodology on several lexical sample tasks in different languages, outperforming signiﬁcantly the state-of-the-art for each of them, while reducing the amount of labeled training data required for learning. 
Information extraction systems incorporate multiple stages of linguistic analysis. Although errors are typically compounded from stage to stage, it is possible to reduce the errors in one stage by harnessing the results of the other stages. We demonstrate this by using the results of coreference analysis and relation extraction to reduce the errors produced by a Chinese name tagger. We use an N-best approach to generate multiple hypotheses and have them re-ranked by subsequent stages of processing. We obtained thereby a reduction of 24% in spurious and incorrect name tags, and a reduction of 14% in missed tags. 
Entity relation detection is a form of information extraction that finds predefined relations between pairs of entities in text. This paper describes a relation detection approach that combines clues from different levels of syntactic processing using kernel methods. Information from three different levels of processing is considered: tokenization, sentence parsing and deep dependency analysis. Each source of information is represented by kernel functions. Then composite kernels are developed to integrate and extend individual kernels so that processing errors occurring at one level can be overcome by information from other levels. We present an evaluation of these methods on the 2004 ACE relation detection task, using Support Vector Machines, and show that each level of syntactic processing contributes useful information for this task. When evaluated on the official test data, our approach produced very competitive ACE value scores. We also compare the SVM with KNN on different kernels. 
Extracting semantic relationships between entities is challenging. This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM. Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement. This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking. We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance. Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types. 
In this work, we provide an empirical analysis of differences in word use between genders in telephone conversations, which complements the considerable body of work in sociolinguistics concerned with gender linguistic differences. Experiments are performed on a large speech corpus of roughly 12000 conversations. We employ machine learning techniques to automatically categorize the gender of each speaker given only the transcript of his/her speech, achieving 92% accuracy. An analysis of the most characteristic words for each gender is also presented. Experiments reveal that the gender of one conversation side inﬂuences lexical use of the other side. A surprising result is that we were able to classify male-only vs. female-only conversations with almost perfect accuracy. 
The paper presents the Position Speciﬁc Posterior Lattice, a novel representation of automatic speech recognition lattices that naturally lends itself to efﬁcient indexing of position information and subsequent relevance ranking of spoken documents using proximity. In experiments performed on a collection of lecture recordings — MIT iCampus data — the spoken document ranking accuracy was improved by 20% relative over the commonly used baseline of indexing the 1-best output from an automatic speech recognizer. The Mean Average Precision (MAP) increased from 0.53 when using 1-best output to 0.62 when using the new lattice representation. The reference used for evaluation is the output of a standard retrieval engine working on the manual transcription of the speech collection. Albeit lossy, the PSPL lattice is also much more compact than the ASR 3-gram lattice from which it is computed — which translates in reduced inverted index size as well — at virtually no degradation in word-error-rate performance. Since new paths are introduced in the lattice, the ORACLE accuracy increases over the original ASR lattice.  
Sentence boundary detection in speech is important for enriching speech recognition output, making it easier for humans to read and downstream modules to process. In previous work, we have developed hidden Markov model (HMM) and maximum entropy (Maxent) classiﬁers that integrate textual and prosodic knowledge sources for detecting sentence boundaries. In this paper, we evaluate the use of a conditional random ﬁeld (CRF) for this task and relate results with this model to our prior work. We evaluate across two corpora (conversational telephone speech and broadcast news speech) on both human transcriptions and speech recognition output. In general, our CRF model yields a lower error rate than the HMM and Maxent models on the NIST sentence boundary detection task in speech, although it is interesting to note that the best results are achieved by three-way voting among the classiﬁers. This probably occurs because each model has different strengths and weaknesses for modeling the knowledge sources. 
We present a framework for word alignment based on log-linear models. All knowledge sources are treated as feature functions, which depend on the source langauge sentence, the target language sentence and possible additional variables. Log-linear models allow statistical alignment models to be easily extended by incorporating syntactic information. In this paper, we use IBM Model 3 alignment probabilities, POS correspondence, and bilingual dictionary coverage as features. Our experiments show that log-linear models signiﬁcantly outperform IBM translation models. 
This paper proposes an alignment adaptation approach to improve domain-specific (in-domain) word alignment. The basic idea of alignment adaptation is to use out-of-domain corpus to improve in-domain word alignment results. In this paper, we first train two statistical word alignment models with the large-scale out-of-domain corpus and the small-scale in-domain corpus respectively, and then interpolate these two models to improve the domain-specific word alignment. Experimental results show that our approach improves domain-specific word alignment in terms of both precision and recall, achieving a relative error rate reduction of 6.56% as compared with the state-of-the-art technologies. 
We present a version of Inversion Transduction Grammar where rule probabilities are lexicalized throughout the synchronous parse tree, along with pruning techniques for efﬁcient training. Alignment results improve over unlexicalized ITG on short sentences for which full EM is feasible, but pruning seems to have a negative impact on longer sentences. 
In this paper, we examine the task of extracting a set of biographic facts about target individuals from a collection of Web pages. We automatically annotate training text with positive and negative examples of fact extractions and train Rote, Na¨ıve Bayes, and Conditional Random Field extraction models for fact extraction from individual Web pages. We then propose and evaluate methods for fusing the extracted information across documents to return a consensus answer. A novel cross-ﬁeld bootstrapping method leverages data interdependencies to yield improved performance. 
A complex relation is any n-ary relation in which some of the arguments may be be unspeciﬁed. We present here a simple two-stage method for extracting complex relations between named entities in text. The ﬁrst stage creates a graph from pairs of entities that are likely to be related, and the second stage scores maximal cliques in that graph as potential complex relation instances. We evaluate the new method against a standard baseline for extracting genomic variation relations from biomedical text. 
This paper presents an effective approach for resume information extraction to support automatic resume management and routing. A cascaded information extraction (IE) framework is designed. In the first pass, a resume is segmented into a consecutive blocks attached with labels indicating the information types. Then in the second pass, the detailed information, such as Name and Address, are identified in certain blocks (e.g. blocks labelled with Personal Information), instead of searching globally in the entire resume. The most appropriate model is selected through experiments for each IE task in different passes. The experimental results show that this cascaded hybrid model achieves better F-score than flat models that do not apply the hierarchical structure of resumes. It also shows that applying different IE models in different passes according to the contextual structure is effective. 
We describe a method for discriminative training of a language model that makes use of syntactic features. We follow a reranking approach, where a baseline recogniser is used to produce 1000-best output for each acoustic input, and a second “reranking” model is then used to choose an utterance from these 1000-best lists. The reranking model makes use of syntactic features together with a parameter estimation method that is based on the perceptron algorithm. We describe experiments on the Switchboard speech recognition task. The syntactic features provide an additional 0.3% reduction in test–set error rate beyond the model of (Roark et al., 2004a; Roark et al., 2004b) (signiﬁcant at p < 0.001), which makes use of a discriminatively trained n-gram model, giving a total reduction of 1.2% over the baseline Switchboard system.  
We have established a phonotactic language model as the solution to spoken language identification (LID). In this framework, we define a single set of acoustic tokens to represent the acoustic activities in the world’s spoken languages. A voice tokenizer converts a spoken document into a text-like document of acoustic tokens. Thus a spoken document can be represented by a count vector of acoustic tokens and token n-grams in the vector space. We apply latent semantic analysis to the vectors, in the same way that it is applied in information retrieval, in order to capture salient phonotactics present in spoken documents. The vector space modeling of spoken utterances constitutes a paradigm shift in LID technology and has proven to be very successful. It presents a 12.4% error rate reduction over one of the best reported results on the 1996 NIST Language Recognition Evaluation database. 
Reading proﬁciency is a fundamental component of language competency. However, ﬁnding topical texts at an appropriate reading level for foreign and second language learners is a challenge for teachers. This task can be addressed with natural language processing technology to assess reading level. Existing measures of reading level are not well suited to this task, but previous work and our own pilot experiments have shown the beneﬁt of using statistical language models. In this paper, we also use support vector machines to combine features from traditional reading level measures, statistical language models, and other language processing tools to produce a better method of assessing reading level. 
We describe a method for incorporating syntactic information in statistical machine translation systems. The ﬁrst step of the method is to parse the source language string that is being translated. The second step is to apply a series of transformations to the parse tree, effectively reordering the surface string on the source language side of the translation system. The goal of this step is to recover an underlying word order that is closer to the target language word-order than the original string. The reordering approach is applied as a pre-processing step in both the training and decoding phases of a phrase-based statistical MT system. We describe experiments on translation from German to English, showing an improvement from 25.2% Bleu score for a baseline system to 26.8% Bleu score for the system with reordering, a statistically signiﬁcant improvement. 
Syntax-based statistical machine translation (MT) aims at applying statistical models to structured data. In this paper, we present a syntax-based statistical machine translation system based on a probabilistic synchronous dependency insertion grammar. Synchronous dependency insertion grammars are a version of synchronous grammars defined on dependency trees. We first introduce our approach to inducing such a grammar from parallel corpora. Second, we describe the graphical model for the machine translation task, which can also be viewed as a stochastic tree-to-tree transducer. We introduce a polynomial time decoding algorithm for the model. We evaluate the outputs of our MT system using the NIST and Bleu automatic MT evaluation software. The result shows that our system outperforms the baseline system based on the IBM models in both translation speed and quality. 
In this paper, we propose a new contextdependent SMT model that is tightly coupled with a language model. It is designed to decrease the translation ambiguities and efﬁciently search for an optimal hypothesis by reducing the hypothesis search space. It works through reciprocal incorporation between source and target context: a source word is determined by the context of previous and corresponding target words and the next target word is predicted by the pair consisting of the previous target word and its corresponding source word. In order to alleviate the data sparseness in chunk-based translation, we take a stepwise back-off translation strategy. Moreover, in order to obtain more semantically plausible translation results, we use bilingual verb-noun collocations; these are automatically extracted by using chunk alignment and a monolingual dependency parser. As a case study, we experimented on the language pair of Japanese and Korean. As a result, we could not only reduce the search space but also improve the performance. 
In this paper, we present a novel training method for a localized phrase-based prediction model for statistical machine translation (SMT). The model predicts blocks with orientation to handle local phrase re-ordering. We use a maximum likelihood criterion to train a log-linear block bigram model which uses realvalued features (e.g. a language model score) as well as binary features based on the block identities themselves, e.g. block bigram features. Our training algorithm can easily handle millions of features. The best system obtains a ¢¤£¦¥ § % improvement over the baseline on a standard Arabic-English translation task.  
This paper describes a novel instancebased sentence boundary determination method for natural language generation that optimizes a set of criteria based on examples in a corpus. Compared to existing sentence boundary determination approaches, our work offers three signiﬁcant contributions. First, our approach provides a general domain independent framework that effectively addresses sentence boundary determination by balancing a comprehensive set of sentence complexity and quality related constraints. Second, our approach can simulate the characteristics and the style of naturally occurring sentences in an application domain since our solutions are optimized based on their similarities to examples in a corpus. Third, our approach can adapt easily to suit a natural language generation system’s capability by balancing the strengths and weaknesses of its subcomponents (e.g. its aggregation and referring expression generation capability). Our ﬁnal evaluation shows that the proposed method results in signiﬁcantly better sentence generation outcomes than a widely adopted approach. 
We present an approach to using a morphological analyzer for tokenizing and morphologically tagging (including partof-speech tagging) Arabic words in one process. We learn classiﬁers for individual morphological features, as well as ways of using these classiﬁers to choose among entries from the output of the analyzer. We obtain accuracy rates on all tasks in the high nineties. 
Semantic role labeling is the process of annotating the predicate-argument structure in text with semantic labels. In this paper we present a state-of-the-art baseline semantic role labeling system based on Support Vector Machine classiﬁers. We show improvements on this system by: i) adding new features including features extracted from dependency parses, ii) performing feature selection and calibration and iii) combining parses obtained from semantic parsers trained using different syntactic views. Error analysis of the baseline system showed that approximately half of the argument identiﬁcation errors resulted from parse errors in which there was no syntactic constituent that aligned with the correct argument. In order to address this problem, we combined semantic parses from a Minipar syntactic parse and from a chunked syntactic representation with our original baseline system which was based on Charniak parses. All of the reported techniques resulted in performance improvements. 
Despite much recent progress on accurate semantic role labeling, previous work has largely used independent classiﬁers, possibly combined with separate label sequence models via Viterbi decoding. This stands in stark contrast to the linguistic observation that a core argument frame is a joint structure, with strong dependencies between arguments. We show how to build a joint model of argument frames, incorporating novel features that model these interactions into discriminative loglinear models. This system achieves an error reduction of 22% on all arguments and 32% on core arguments over a stateof-the art independent classiﬁer for goldstandard parse trees on PropBank. 
Previous work has used monolingual parallel corpora to extract and generate paraphrases. We show that this task can be done using bilingual parallel corpora, a much more commonly available resource. Using alignment techniques from phrasebased statistical machine translation, we show how paraphrases in one language can be identiﬁed using a phrase in another language as a pivot. We deﬁne a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities, and show how it can be reﬁned to take contextual information into account. We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments, and contrast the quality with paraphrases extracted from automatic alignments. 
 extraction systems have no obvious connection  This paper introduces a new method for identifying candidate phrasal terms (also known as multiword units) which applies a nonparametric, rank-based heuristic measure. Evaluation of this measure, the mutual rank ratio metric, shows that it produces better results than standard statistical measures when applied to this task.  with the criteria that linguists would argue define a phrasal term (noncompositionality, fixed order, nonsubstitutability, etc.). They function, instead, to reduce the number of a priori improbable terms and thus improve precision. The association measure does the actual work of distinguishing between terms and plausible nonterms. A variety of methods have been applied, ranging from simple frequency (Justeson & Katz 1995), modified  
This paper describes a novel system for acquiring adjectival subcategorization frames (SCFs) and associated frequency information from English corpus data. The system incorporates a decision-tree classiﬁer for 30 SCF types which tests for the presence of grammatical relations (GRs) in the output of a robust statistical parser. It uses a powerful patternmatching language to classify GRs into frames hierarchically in a way that mirrors inheritance-based lexica. The experiments show that the system is able to detect SCF types with 70% precision and 66% recall rate. A new tool for linguistic annotation of SCFs in corpus data is also introduced which can considerably alleviate the process of obtaining training and test data for subcategorization acquisition. 
In this paper, we explore the power of randomized algorithm to address the challenge of working with very large amounts of data. We apply these algorithms to generate noun similarity lists from 70 million pages. We reduce the running time from quadratic to practically linear in the number of elements to be computed. 
This paper examines the collocational patterns of Mandarin verbs of conversation and proposes that a finer classification scheme than the flat structure of ‘frames’ [cf. Fillmore and Atkins 1992; Baker et al. 2003] is needed to capture the semantic granularity of verb types. The notion of a ‘subframe’ is introduced and utilized to explain the syntactic-semantic interdependencies among different groups of verbs in the Conversation Frame. The paper aims to provide detailed linguistic motivations for distinguishing subframes within a frame as a semantic anchor for further defining near-synonym sets. Keywords: Mandarin Verbs of Conversation, Semantic Frame, Subframe, Collocational Association 1. Introduction1 As the importance of lexical semantic research has grown with the need to represent human knowledge, various lexically-based information networks have been proposed. This includes the comprehensive work on differentiating word senses and sense relations in WordNet [Miller et al. 1990], the ontological hierarchy in SUMO [Niles and Pease 2003], and the more linguistically-motivated model of FrameNet [Baker et al. 2003]. While all these databases provide valuable information regarding word senses, the first two are constructed in a more ∗ Graduate Institute of Foreign Literatures and linguistics, National Chiao Tung University, Taiwan E-mail: mliu@mail.nctu.edu.tw + Graduate Institute of Linguistics, National Tsing Hua University, Taiwan E-mail: d948704@oz.nthu.edu.tw 
In this paper, we propose clear-cut definitions of distinct temporal adverbs and provide descriptive features for each class of temporal adverbs. By measuring time points in temporal axis, we revise and reclassify the temporal adverbs listed in [Lu and Ma 1999] into four classes of semantic roles, namely, time, frequency, duration, and time manner. The descriptive features enable us to distinguish temporal relations and predict logical compatibility between temporal adverbs and aspects. Keywords: Temporal Adverbs, Aspects, Temporal Schema, Speaking Time, Reference Time, Event Time 1. Introduction There are about 130 temporal adverbs in Mandarin Chinese. Lu and Ma [1999] classified temporal adverbs into two groups, speaking-time related adverbs (abbr: ST-related adverbs, 定時時間副詞) and reference-time related adverbs (abbr: RT-related adverbs, 不定時時間副 詞). The ST-related adverbs consist of 27 temporal adverbs, which are subdivided into three subclasses. In the class of RT-related adverbs, 104 temporal adverbs are listed and subdivided into 18 subclasses. In Lu and Ma’s definition, ST-related adverbs modify events that happen at specific time points; RT-related adverbs modify events that have happened in the past or will happen in the future. However, the subdivisions are vague, and the definition is incomprehensible. For example, cengjing 曾經, ceng 曾, yeyi 業已 and yejing 業經 are grouped into two different subclasses of ST-related adverbs. The former two are grouped into the same subclass, which includes actions or situations that happened or existed before the speaking time. The later two are grouped into the same subclass, which includes actions or situations have been completed or have occurred. In fact, it is difficult to differentiate between actions or situations that have ‘happened’ or have ‘completed’, especially when the situation type is an achievement situation with SHORTLY-PRECEDE(t1,t2) or NEARLY-EQUAL  ∗ CKIP, Institute of Information Science, Academia Sinica, Taipei E-mail: {shihmin, jess}@hp.iis.sinica.edu.tw; kchen@iis.sinica.edu.tw [Received April 12, 2005; Revised June 13, 2005; Accepted August 15, 2005]  446  Shih-Min Li et al.  (t1,t2).1 Moreover, temporal adverbs may not have the same syntactic behaviour even though they are classified into the same subclass. For instance, the ST-related adverbs cong 從, conglai 從來, zhijin 至今, xianglai 向來, sulai 素來, lilai 歷來, su 素, and yixiang 一向 are grouped into the same subclass. When they co-occur with the aspect markers le 了, guo 過, and zhe 著, however, cong, conglai, and zhijin are incompatible with le and zhe; in addition, xianglai, sulai, lilai, su, and yixiang are incompatible with le and guo. The reason for the difference in the compatibility of temporal adverbs with aspects will be discussed in the following. In this paper, based on the application of Smith’s proposal for temporal location, we suggest more clear-cut definitions and provide descriptive features for each subclass of temporal adverbs. The descriptive features help us to define temporal relations and to predict the compatibility between temporal adverbs and aspect markers. 2. Literature Review and Methodology To make clear-cut differentiations, time points in the temporal axis and the notion of temporal location proposed by [Smith 1991] will be used here to redefine the temporal relations of the temporal adverbs treated in [Lu and Ma 1999]. We will use the data in the Academia Sinica Balanced Corpus (Sinica Corpus) to analyze Mandarin Chinese temporal adverbs. Smith [1991] discussed aspectual systems in language. She mentioned that the temporal information in a sentence is located in the time when the situation occurs. Adverbials further specify temporal location. Time is represented as a straight line stretching in both directions from Speech Time. The linear representation is presented in (1):  (1) Time Line [Smith 1991]  -------------------------- Speech Time --------------------------  Past  Present  Future  
The research on word sense disambiguation (WSD) has great theoretical and practical significance in many fields of natural language processing (NLP). This paper presents an unsupervised approach to Chinese word sense disambiguation based on Hownet (an electronic Chinese lexical resource). In our approach, contexts that include ambiguous words are converted into vectors by means of a second-order context method, and these context vectors are then clustered by the k-means clustering algorithm. Lastly, the ambiguous words can be disambiguated after a similarity calculation process is completed. Our experiments involved extraction of terms, and an 82.62% average accuracy rate was achieved. Keywords: Word Sense Disambiguation, Hownet,Second-Order Context, K-Means Clustering 1. Introduction Ambiguous words are widely used in various kinds of natural languages and are distributed widely. Word sense disambiguation (WSD) has long been studied as an important problem in natural language processing (NLP), and the research on WSD has great theoretical and practical significance in many areas of NLP. Much work has been done on WSD. In [Lu et al. 2002], the author presented an unsupervised approach to WSD based on a vector space model. This method defines the 
 Mei-Chih Tsai 摘要 本文以動詞「送」的多義辨識為例，通過語料庫句式分佈統計，研究詞義和句 式間的對應關係，為多義詞詞義辨識提供基礎資源。根據搭配句式的不同，多 義詞「送」可切分為遞送、贈送、送行、斷送四個義項，遣送、推送兩項則分 別歸入送行義、遞送義，作義面處理。結果證明，句式搭配訊息確實能夠在詞 義辨識過程中起一定作用。 關鍵詞：多義詞、詞義辨識、語料庫、義面 Abstract This paper explores the correlation between lexical semantics and syntactic construction, with particular attention paid to delimiting the lexical semantic distinctions between the multiple senses of the polysemous verb song4 ‘to send’. Different types of syntactic constructions are first categorized according to the distribution of arguments, such as direct objects, indirect objects, and locatives. Four senses and two meaning facets are then identified and formulated. Keywords: Polysemy, Word Sense Disambiguation, Corpus, Meaning Facet  
This paper reports on a synchronous corpus-based study of the everyday usage of a set of Chinese judgement terms. An earlier study on Hong Kong data found that these terms were more polysemous than their English counterparts within the legal domain, and were even more fuzzily used in general news reportage. The current study further compares their usage in general texts from other Chinese speech communities (Beijing, Taiwan, and Singapore) to explore the regional differences in lexicalisation and perception of the relevant legal concepts. Corpus data revealed the distinctiveness of the Singapore data, and that the contrasting frequency distributions of the terms and senses could be a result of the varied focus in reportage or the use of alternative expressions for the same concepts in individual communities. The analysis will contribute to the construction and enrichment of Pan-Chinese lexico-semantic resources, which will be useful for many natural language processing applications, such as machine translation. Keywords: Synchronous Corpus-Based Study, Legal Concept and Terminology, Regional Differences, Pan-Chinese Lexico-Semantic Resources 1. Introduction In this paper, we report on a synchronous corpus-based study on a set of semantically related legal terms, and propose a Pan-Chinese lexico-semantic resource for the legal domain, such as one in the form of a thesaurus, to differentiate the usage and perception of closely related legal concepts and terminology across various Chinese speech communities. The situation with language and law is a very interesting one in Hong Kong. As pointed out by Tsou and Kwong [2003], the legal system in Hong Kong has operated through English ∗ Language Information Sciences Research Centre, City University of Hong Kong, Tat Chee Avenue, Kowloon, Hong Kong E-mail: {rlolivia, rlbtsou}@cityu.edu.hk [Received April 14, 2005; Revised June 15, 2005; Accepted August 14, 2005]  520  Oi Yee Kwong and Benjamin K. Tsou  solely for more than 150 years. Following the implementation of legal bilingualism in the 90’s, Hong Kong became the first community that follows the Common Law system which at the same time allows the use of both English and Chinese in court proceedings. In contrast to the many precisely lexicalised legal concepts in English, given its long and established tradition in Common Law, the use of their Chinese equivalents is apparently more fuzzy, as is evident from the lack of one-to-one correspondence of legal terms between English and Chinese. This difference in the cross-lingual lexicalisation of legal concepts between English and Chinese has thus become a substantial linguistic hurdle in the implementation of legal bilingualism, and is directly related to whether both languages could eventually be used in balanced and proper ways to the same effect in the legal domain. The apparent fuzziness with Chinese legal terms is nevertheless peculiar in the Hong Kong context, but not in other places where Chinese is also used as the official language in the legal domain, such as in Mainland China. A possible reason is that preciseness is somehow diluted upon translation. In English, for instance, a “verdict” and a “sentence” are sufficiently distinguished, despite their semantic relatedness (as both are related to the results of a trial). However, when expressed in, or more often translated into, Chinese, the preciseness is somehow weakened. On the one hand, the expression or translation in Chinese might have to take into account the corresponding syntactic constraints and stylistic differences in the two languages in order to sound natural, hence the variation in expressing the same concept in different contexts. On the other hand, the translation could sometimes be affected by usages in other places. For example, although “contract” is mostly expressed as “合約” in Hong Kong, it is sometimes expressed as “合同”, which is the term used in Mainland China for this concept. A set of semantically related legal terms was studied by Tsou and Kwong [2003] with respect to their usage in the legal domain and in general texts. The terms are “裁定” (hold, convicted), “裁決” (determine, verdict), “判決” (judgement, conviction), “裁斷” (find, finding), and “裁判” (Magistracy)1, all of which have one or more senses referring to some aspects of “judgement”. Their uses were studied via a corpus of bilingual court judgments2 and a general corpus of news articles from Hong Kong. From the corpus of bilingual judgments, it was observed that these Chinese terms were considerably polysemous, such that most of them were found as the renditions for multiple English terms, which are distinct though closely related in meaning. Even more varied usages were found for the Chinese terms 
YingJu Xia, Hao Yu and Fumihito Nishino 摘要 在信息檢索、信息抽取等應用中，命名實体的處理十分重要。本文在目前的命 名實体分類体系的基礎上，從信息檢索和抽取的角度對命名實体的細分類進行 了深入的研究。提出了命名實体的多級分類并給出了每一級的詳細分類。為了 檢驗該分類体系的實際效果，我們在人民日報語料上進行了初步的標注。并使 用常用的基于統計模型的命名實体識別算法在人民日報語料上做了一系列的 對比實驗。實驗結果表明：面向機器處理的細分類能有效地提高識別系統的性 能并最終有助于信息檢索和抽取。 關鍵字：命名實体、分類、語料庫、自然語言處理 Abstract Named entity recognition is a very important part of information retrieval and information extraction. Classification is also very important. This paper investigates the sub-classification of named entities from the point of view of information retrieval and information extraction. This paper also presents multi-classification and gives detailed information about each sub-class. We have manually annotated people’s daily corpus (1998) and conducted a serial of experiments using the statistical model of named entity recognition. The ∗ 富士通研究開發中心有限公司，100016 北京市朝陽區霄雲路 26 號鵬潤大廈 B306 室 Internet Application Laboratory, Fujitsu Research & Development Center Co., LTD. Room B306, Eagle Run Plaza No. 26, Xiao Yun Road, Chao Yang District, Beijing, 100016, P. R. China E-mail: {yjxia, yu, nisino}@frdc.fujitsu.com [Received April 12, 2005; Revised June 15, 2005; Accepted August 14, 2005]  534  夏迎炬 等  experimental results show that the sub-classes presented by this paper can enhance the recognition system’s performance and aid information retrieval and information extraction. Keywords: Named Entity, Classification, Corpus, Natural Language Processing 1. 前言 
The use of lexical resources in linguistic analysis has expanded rapidly in recent years. However, most lexical resources, such as WordNet or online dictionaries, at this point do not usually indicate figurative meanings, such as conceptual metaphors, as part of a lexical entry. Studies that attempt to establish the relationships between literal and figurative language by detecting the connectivity between WordNet relations usually do not deal with linguistic data directly. However, the present study demonstrates that SUMO definitions can be used to identify the source domains used in conceptual metaphors. This is achieved by identifying the relationships between metaphorical expressions and their corresponding ontological nodes. Such links are important because they show which lexical items are mapped under which concepts. This, in turn, helps specify which lexical items in electronic resources involve conceptual mappings. Looking specifically at the concept of PERSON, this work also establishes connectivity between lexical items which are related to “Organism.” Therefore, the methodology reported herein not only aids the categorizing of lexical items according to their conceptual domains but also can establish links between these items. Such bottom-up and top-down analyses of lexical items may provide a means of representing metaphorical entries in lexical resources. Keywords: Concept, Ontology, Conceptual Metaphor, Source Domain, WordNet, SUMO  ∗ Graduate Institute of Linguistics, National Taiwan University, No. 1, Section 4, Roosevelt Road, Taipei, Taiwan R.O.C. 106. Phone: +886-2-3366-4104 Fax: +886-2-2363-5358 E-mail: claricefong6376@hotmail.com; kathleenahrens@yahoo.com + Academia Sinica, No.128, Sec. 2, Academia Road, Nangang, Taipei, Taiwan R.O.C. 115 Phone: +886-2-2352-3108 Fax: +886-2-2785-6622 E-mail: churen@gate.sinica.edu.tw [Received April 15, 2005; Revised June 15, 2005; Accepted August 15, 2005]  554  Siaw-Fong Chung et al.  1. Introduction Many studies have found that metaphors are not represented separately in lexical resources [Alonge and Castelli 2002ab, 2003; Peter and Wilks 2003; Lönneker 2003]. When metaphors are found in lexical resources, they are most often represented using meaning entries in addition to non-metaphorical ones. WordNet [http://www.cogsci.princeton.edu/~wn/; Fellbaum 1998] is one of the lexical resources that sometimes lists metaphorical meanings as different senses along with other non-metaphorical ones. However, where conceptual metaphors [Lakoff 1993; Lakoff and Johnson 1980] are concerned, there is no uniform representation of source (concrete) and target (abstract) domains in WordNet. Due to this fact, studies have been carried out which have attempted to represent figurative meanings by indicating the source-target domain pairing in addition to the literal meaning [Lönneker 2003; Alonge and Lönneker 2004; Peters and Wilks 2003]. In order to do so, these studies first established the relationship between the literal and figurative entry in the lexical resource. As a result, the mapping between the source and target domains could then be extracted. Researchers who have attempted to incorporate metaphors into WordNet include Eilts and Lönneker [2002], who created the Hamburg Metaphor Database, a database which provides French and German metaphors by creating links between metaphorical expressions and their related synsets in WordNet. Another possible way to establish the relationships between literal and figurative entries is to determine the semantic relations between lexical entries. For instance, Lönneker [2003] and Peters and Wilks [2003] suggested that the meronymic relations in WordNet can be used to identify the links between lexical items by establishing a connection between the event and its participants or the action carried out by the participants. However, neither of these studies used ontologies to link the concepts in the source or target domain. Alonge and Castelli [2003] were the first to suggest that the EuroWordNet Top Ontology needs to be extended with more concepts in order to deal with figurative language. The advantages of ontologies have been outlined by Navigli and Velardi [2004]; they include the fact that a) an ontology has a wide “coverage” of domain concepts; b) that it is a result of “consensus” reached by a group of people and c) that most importantly, it is easily accessible through electronic resources. Most of the above mentioned works, which tried to generate the underlying connectivity between synsets, have based their mechanisms on distributed models of processing [Rumelhart and McClelland 1986] or the coarse-coding of lexicons [Harris 1994]. Harris stated that the “representational units” in a coarse-coding mechanism “do not match the information represented…in a one-to-one fashion” [Harris 1994]. Rather, they are connected to one another, and when one unit is activated, the others will also be activated. It is through  Source Domains as Concept Domains in Metaphorical Expressions  555  these units that clusters of information (which contain one or more concepts) are built. This activation of concepts governs the generation of underlying relations between WordNet relations. Working within this framework, our aim in this work is to establish links between metaphorical items by identifying the shared concept carried by a cluster of lexical items. For instance, when one active concept (from a lexical item) such as “Growth” is activated, the related concept, such as “Organism” is also activated. This link between lexical items in the same domain is necessary to show which lexical items are mapped under which concepts; i.e., lexical items with the same shared concepts will be sorted into the same source domain of a metaphor. In this paper, the building of a “concept” involves the use of knowledge nodes from an ontology, which is a shared understanding of some domain of interest [Uschold and Gruninger 1996]. Keil [1979] stated that the knowledge representation in an ontology “has unique properties and is highly structured. Moreover, it constrains the nature of semantic and conceptual knowledge.” The particular ontology we use in this paper, SUMO, was developed by the IEEE Standard Upper Ontology Working Group. Huang, Chung and Ahrens (In press) applied SUMO to explore how an ontology can be used to predict metaphorical mappings. Our work herein extends that of Huang et al. (In press) by focusing on how the source domain of a metaphor can be determined via SUMO definitions. All metaphorical instances in this work are identified using the Conceptual Mapping (CM) model [Ahrens 2002], and data in this study come from both English and Chinese corpora. Searching for ontology nodes is facilitated by the Academia Sinica Bilingual Ontological Wordnet [Sinica BOW, Huang, Chang and Lee (2004) http://BOW.sinica.edu.tw], a system that integrates WordNet, the English-Chinese Translation Equivalents Database (ECTED), and SUMO. 2. Conceptual Metaphor, Concept Domain and Ontology Ahrens [2002] suggested that the metaphorical expressions can be analyzed in terms of the entities, qualities and functions that can map between a source and a target domain. When these conventionalized metaphorical expressions have been examined, an underlying reason for these mappings can then be postulated. This particular study collected data from native speaker intuitions to determine the mappings from the source to the target domain. For example, in the three examples from the metaphor LOVE IS PLANT, given below, the Mapping Principle (MP) of “Love is understood as plant because plants involve physical growth and love involves emotional growth” was extracted based on the fact that all the examples in some way had to do with growth.  556  Siaw-Fong Chung et al.  1. (a) 兩 人 的 愛 苗 最近 才 剛 萌芽 liang ren de ai miao zuijin cai gang mengya two people MOD love seedling lately just recently sprout ‘Their love just begins to sprout lately.’  (b) 我 對 他的 愛意 漸漸 滋長 wo dui tade ai-yi jianjian zizhang I for his love gradually grow ‘My love for him has grown gradually.’  (c) 愛情 需要 辛勤  的 灌溉  aiqing xuyao xinqin  de quanqai  love need industriously water  ‘Love needs to be watered industriously.’  [Ahrens 2002]  Ahrens, Chung and Huang [2003] extended this study by proposing a corpus-based approach to establish the systematicity between source and target domain pairings (i.e. Mapping Principles (MPs)). They suggest that each source-target domain pairing will have a prototypical instance of mapping determined by the most frequent mapping, as compared with other mappings. In a later paper, Ahrens 2004, they use Suggested-Upper-Merged-Ontology (SUMO) in combination with WordNet to determine Mapping Principles when there is no highly frequent mapping. SUMO can be used to infer knowledge through automatic reasoning as well as to constrain the falsifiability of the MP. This study extends the previous works of Ahrens, Chung and Huang [2003, 2004] by using SUMO to define the concepts involved in source domains through the use of two major databases -- WordNet (1.6), and SUMO nodes along with their definitions. The integration of WordNet and SUMO by Niles and Pease [2003] enables us to examine the ontological nodes in SUMO that have hyperlinks to WordNet semantic definitions. 3. Metaphor Analysis: CAREER IS A PERSON as a Sample In order to find conceptual metaphors in corpora, a single target word was used for the target domain. Four target domains were chosen, namely, CAREER, CULTURE, STOCK MARKET and ECONOMY, of which the latter two are composed Chinese-English data (see Table 1).  Source Domains as Concept Domains in Metaphorical Expressions  557  The target domains of STOCK MARKET and ECONOMY have been discussed by Ahrens et al. [2003] and Chung, Ahrens and Sung [2003], and in this paper, we further refine the previous findings. In this paper, also, the Chinese only CAREER and CULTURE target domains will be added to strengthen the methodology discussed herein.  Table 1. The sources and frequencies of the corpora instances found  Target domains (Chinese) Shiye 事業 CAREER (Chinese) Wenhua 文化 CULTURE (Chinese-English) Gushi 股市 STOCK MARKET (Chinese-English) Jingji 經濟 ECONOMY  Sources  Number of hits  Sinica Corpus (http://www.sinica.edu.tw/SinicaCorpus/)  1062  Sinica Corpus  2000  1997 Huashishingwen 華視新聞焦點 1997 Gungshangshibao 工商時報 [Chung, Ahrens and Sung, 2003] 1994 Wall Street Journal (Available at the Language Data Consortium http://www.ldc.upenn.edu/ldc/online/index.html) Sinica Corpus  NA 500 2000  1994 Wall Street Journal  500  Number of metaphorical expressions 84 335 135 130 311 215  The four target domains were used to extract instances from the corpora. After all instances were extracted, they were analyzed manually for the instances of metaphors. A metaphor was identified when there was a source-target domain mapping. The following sentence shows a metaphorical instance for the target domain CAREER: 他事業的生命力日 趨旺盛 “the life-force of his career is becoming exuberant day-by-day,” the concrete meaning of “life force” is mapped onto CAREER. In another example,事業創傷 “the wound of career,” the more concrete source “wound” is mapped onto the abstract target CAREER. Through similar analysis, all metaphorical instances were marked and extracted. Once all the metaphorical instances had been identified, the next step was to define the source domains of these instances. In order to do so, the corresponding WordNet and SUMO nodes for each lexical item were searched for in Sinica Bow. Through this system, each metaphorical instance was keyed in at the WordNet page. This was done to extract the WordNet explanations which were linked to the corresponding SUMO nodes in the system. In order to obtain the WordNet explanation, a prior step was performed, i.e., the most appropriate meaning was selected from the list of senses available. This selection was done manually, but most often, the most appropriate meaning was found to be the most concrete meaning in the list. For example, when the Chinese keyword chuangshang 創傷 “wound” from the target domain CAREER was searched for in Sinica Bow, the senses listed in Table 2 were extracted.  558  Siaw-Fong Chung et al.  Table 2. The search result for chuangshang 創傷 “wound” in Sinica Bow  WordNet (1.6) Sense 1:trauma Sense 2:wound  WordNet Explanations an emotional wound or shock often having long-lasting effects any break in the skin or an organ caused by violence or surgical incision  Corresponding SUMO Nodes EmotionalState(情緒狀態) Injuring(傷害)  Among these senses, the more concrete sense (i.e., the more concrete meaning that was mapped from the source domain) was selected. In this case, sense 2 was selected and then the corresponding node (the rightmost column in Table 2) was found. In this case, it is “Injuring.” The SUMO definition for “Injuring” is “The process of creating a traumatic wound or injury. Since injuring is not possible without some biologic function of the organism being injured, it is a subclass of biological process.” The keywords in the SUMO definition (shaded in the previous sentence) are the terms that helped us categorize the metaphorical instances. Through the collection of SUMO definitions for all the metaphorical instances, all the similar metaphorical expressions with the same related nodes are grouped into categories. For instance, all the metaphorical expressions related to “Organism” were grouped together. Then, from these instances, the source domains were decided. The expressions that were grouped under the same source domain formed a cluster of lexical items under a domain. In the following discussion, we will provide a detailed example using the CAREER domain.  For the target domain CAREER, all 84 metaphorical instances are listed in Table 3. Each of the items in Table 3 was looked up using the Sinica Bow system to find their corresponding WordNet senses and, later, the SUMO nodes.  Table 3. Metaphorical expressions related to shiye ‘career’ (tokens are in brackets)  新創(1) 紮實(1) 溶進...之中(1) 軌道(1)  意識(1)  異軍(1) 幕後功臣(1) 前途(2)  創造(5) 起步(1) 收起來(1) 轟轟烈烈(2) 搖身一變(1) 改革(1) 玩掉(1)  投入(1)  開創(1) 走向(2) 投身...中(1) 走上(1)  創傷(1)  兵符(1) 投向(1)  壯大(1)  共創(1) 第一步(1) 基礎(5)  火車頭(1) 打拼(1)  前程(1) 開發(1)  成長(1)  再創(2) 闖(2) 追求(4)  退出(2)  掙(1)  競爭(1) 登上...位子(1) 角色(1)  挑戰(1) 關(5) 風險(1)  躍進(1)  拼(1)  抗爭(1) 包袱(1)  策略(2) 關卡(1) 供輸(1)  放手(1)  大舞台(2) 投(1)  趨勢(3) 過程(1) 賭(1)  生命力(1) 階梯(1)  衝刺(1)  Table 4 shows how the SUMO definitions help us differentiate between source domains. However, due to limited space, only selected instances from Table 3 will be discussed.  Source Domains as Concept Domains in Metaphorical Expressions  559  Table 4. Defining source domains through WordNet and SUMO  Expressions 策略 ‘tactic’ 軌道 ‘track’  WordNet senses  WordNet explanations  SUMO nodes SUMO definitions  the act of concealing  A Contest where one participant  6: ambush  yourself and lying in ViolentContest attempts to physically injure another wait to attack by (暴力性的競爭) participant.  surprise  a pair of parallel rails  A TransportationDevice is a Device  providing a runway Transportation which serves as the instrument in a  2: track for wheels  Device  Transportation Process which carries  (運輸工具) the patient of the Process from one  point to another.  For the expressions listed in Table 4, their SUMO definitions (the rightmost column) provide the keywords referring to the source domain to which these expressions might belong1. For instance, the keyword “Contest” for 策 略 “tactic” might refer to WAR or COMPETITION, whereas “TransportationDevice” and “Transportation” for 軌道 “track” might refer to VEHICLE. When all the lexical items in Table 3 are looked up, the similarities of these items at the upper ontological levels can be established. In this paper, we will use CAREER IS A PERSON as an example. All items in (2) are found to have “Organism” as the shared concept2.  (2) 意識 “consciousness” 生命力 “the force to live” 成長 “grow”  創傷 “wound”  第一步 “first step”  起步 “start a step”  放手 “let go”  走向 “walk towards”  Table 5 below shows the SUMO definitions for the items in (2). The original method produced the list of all instances in Table 3 in the form shown in Table 5; however, due to space limitations, this paper does not include the full list of all 84 items in Table 3. Only the ones related to “Organism” are shown. Among the SUMO nodes listed in Table 5, “Awake” and “Emotional State” are related to the upper node “State of mind” or a psychological process. The other nodes are related to a physical aspect of the organism, such as “Body  
Metaphors in idioms are universal in languages. The progression from conceptual meaning to metaphorical meaning is a cognitive activity of mapping from one thing to another. This paper, based on the analysis of 2,347 metaphorical idioms, their linguistic features, grammatical structures, grammatical functions, and semantic categories, tries to identify categories of metaphorical idioms. Most of the metaphorical idioms in Chinese are found to be composed of projective semantic 
Multiple-choice cloze items constitute a prominent tool for assessing students’ competency in using the vocabulary of a language correctly. Without a proper estimation of students’ competency in using vocabulary, it will be hard for a computer-assisted language learning system to provide course material tailored to each individual student’s needs. Computer-assisted item generation allows the creation of large-scale item pools and further supports Web-based learning and assessment. With the abundant text resources available on the Web, one can create cloze items that cover a wide range of topics, thereby achieving usability, diversity and security of the item pool. One can apply keyword-based techniques like concordancing that extract sentences from the Web, and retain those sentences that contain the desired keyword to produce cloze items. However, such techniques fail to consider the fact that many words in natural languages are polysemous so that the recommended sentences typically include a non-negligible number of irrelevant sentences. In addition, a substantial amount of labor is required to look for those sentences in which the word to be tested really carries the sense of interest. We propose a novel word sense disambiguation-based method for locating sentences in which designated words carry specific senses, and apply generalized collocation-based methods to select distractors that are needed for multiple-choice cloze items. Experimental results indicated that our system was able to produce a usable cloze item for every 1.6 items it returned. Keywords: Computer-assisted language learning, Computer-assisted item generation, Advanced authoring systems, Natural language processing, Word sense disambiguation, Collocations, Selectional preferences  ∗ Department of Computer Science, National Chengchi University, Taipei 11605, Taiwan E-mail: chaolin@nccu.edu.tw （劉昭麟及王俊弘，臺北市文山區，國立政治大學資訊科學系） + Department of Foreign Languages and Literatures, National Taiwan University, Taipei 10617, Taiwan E-mail: zmgao@ntu.edu.tw （高照明，臺北市大安區，國立臺灣大學外國語文學系） [Received February 3, 2005; Revised June 15, 2005; Accepted June 17, 2005]  304  Chao-Lin Liu et al.  1. Introduction  Due to the advent of modern computers and the Web, academic research on intelligent tutoring systems (ITSs) have grown in the last decade. Figure 1 shows a possible functional structure of the main components of an ITS that uses test items to assess students’ competence levels. With the development of mature techniques for intelligent systems and the abundant information now available on the Internet, a computer-assisted Authoring Component that can help course designers construct large databases of high-quality test items and course materials has become possible [Irvine and Kyllonen 2002; Wang et al. 2003]. With Test-Item and Course-Material Databases, the Tutoring Component must find ways to provide materials appropriate for students. In the ideal case, we should be able to determine students’ competence levels effectively and efficiently by means of various forms of assessment and provide course materials that are tailored to each individual student’s particular needs [van der Linden and Hambleton 1997; van der Linden and Glas 2000; Liu 2005]. For this purpose, we need to have appropriate techniques and a Student-Model Database that together enable the Adaptive Tester and Course Sequencer to identify students’ competence levels, predict their needs, and provide useful course materials. When the tutoring component cannot meet students’ needs, the students should be able to feedback their requests or complaints to the course designers to facilitate future improvements.  Course Designers  Internet  U  Adaptive  Tutoring Component  USER  S  Tester  E  Test-Item Database  INTERFACE  S  R  T  U  I  Test Results  Student  Student-Model  Test-Item Generator  D  N  Modeler  Database  E  T  N  E  T  R  Course  S  F  Sequencer  Course-Material Database  Course-Material Collector  A  C  Feedback  E  Collector  Feedback Authoring Database Component  Figure 1. A functional structure of an intelligent tutoring system  Using Lexical Constraints to Enhance the Quality of  305  Computer-Generated Multiple-Choice Cloze Items  As shown in Figure 1, the quality and quantity of test items are crucial to the success of the whole system, as the decisions for adaptive interactions with students depend heavily on students’ responses to test items. Good test items help teachers identify students’ competence levels more efficiently, and a large quantity of test items avoids the overuse of particular test items, thereby increasing the security of the item database [Dean and Sheehan 2003; Oranje 2003]. Although human experts can create test items of very high quality, the costs involved in using human experts exclusively in the authoring task can be formidable. It is thus not surprising that computer-assisted item generation (CAIG) has attracted the attention of educators and learners, who find that it offers several desirable features of generated item pools [Irvine and Kyllonen 2002]. CAIG offers the possibility of creating a large number of diverse items for assessing students’ competence levels at relatively low cost, while alleviating problems related to keeping the test items secure [Dean and Sheehan 2003; Oranje 2003]. In this paper, we concern ourselves with a fundamental challenge for computer assisted language learning (CALL) and propose tools for assembling multiple-choice cloze items that are useful for assessing students’ competency in the use of English vocabulary. If it is unable to determine ability to understand vocabulary, an ITS cannot choose appropriate materials for such CALL tasks as reading comprehension. To demonstrate our main ideas, we tackle the problem of generating cloze items for the college entrance examinations in Taiwan [Taiwan CEEC 2004]. (For the sake of brevity, we will henceforth use cloze items or items instead of multiple-choice cloze items when there is no obvious risk of confusion.) With the growth of the Web, we can search and sift online text sources for candidate sentences and come up with a list of cloze items economically with the help of natural language processing techniques [Gao and Liu 2003; Kornai and Sundheim 2003]. Techniques for natural language processing can be used to generate cloze items in different ways. One can create sentences from scratch by applying template-based methods [Dennis et al. 2002] or more complex methods based on some predetermined principles [Deane and Sheehan 2003]. One can also take existing sentences from a corpus and select those that meet the criteria for test items. Generating sentences from scratch provides a basis of obtaining specific and potentially well-controlled test items at the costs of more complex systems, e.g., [Sheehan et al. 2003]. On the other hand, since the Web puts ample text sources at our disposal, we can also filter texts to obtain candidate test items of higher quality. Administrators can then select really usable items from these candidates at relatively low cost. Some researchers have already applied natural language processing techniques to compose cloze items. Stevens [1991] employed the concepts of concordancing and collocation to generate items using general corpora. Coniam [1997] applied factors such as word frequency in a tagged corpus to create test items of particular types. In previous works, we  306  Chao-Lin Liu et al.  considered both the frequencies and selectional preferences of words when utilizing the Web as the major source of sentences for creating cloze items [Gao and Liu 2003; Wang et al. 2003]. Despite the recent progress, more advanced natural language processing techniques have not yet been applied to generate cloze items [Kornai and Sundheim 2003]. For instance, many words in English carry multiple senses, and test administrators usually want to test a particular usage of a word. In this case, blindly applying a keyword matching method, such as a concordancer, may result in a long list of irrelevant sentences that will require a lot of postprocessing work. In addition, composing a cloze item requires more than just a useful sentence. Figure 2 shows a sample multiple-choice item, where we call the sentence with a gap the stem, the answer to the gap the key, and the other choices the distractors of the item. Given a sentence for a particular key, we still need distractors for a multiple-choice item. The selection of distractors affects the item facility and item discrimination of a cloze item and is a vital task [Poel and Weatherly 1997]. Therefore, the selection of distractors also calls for more deliberate strategies, and simple considerations alone, such as word frequency [Gao and Liu 2003; Coniam 1997], may not result in high-quality multiple-choice cloze items.  Figure 2. A multiple-choice cloze item for English To remedy these shortcomings, we propose a novel integration of dictionary-based and unsupervised techniques for word sense disambiguation for use in choosing sentences in which the keys carry the senses chosen by test administrators. Our method also utilizes the techniques for computing collocations and selectional preferences [Manning and Schütze 1999] for filtering candidate distractors. Although we can find many works on word sense disambiguation in the literature [Edmonds et al. 2002], providing a complete overview on this field is not the main purpose of this paper. Manning and Schütze [1999] categorized different approaches into three categories: supervised, dictionary-based, and unsupervised methods. Supervised methods typically provide better chances of pinpointing the senses of polysemous words, but the cost of preparing training corpora of acceptable quality can be very high. In contrast, unsupervised methods can be more economical but might not produce high-quality cloze items for CALL applications. Our approach differs from previous dictionary-based methods in that we employ sample sentences of different senses in the lexicon as well as the definitions of polysemous words. We compare the definitions of the competing senses of the key based on a generalized notion of selectional preference. We also compare the similarities  Using Lexical Constraints to Enhance the Quality of  307  Computer-Generated Multiple-Choice Cloze Items  between the candidate sentence, which may become a cloze item, and samples sentences which contain the competing senses of the key. Hence, our approach is a hybrid of dictionary-based and unsupervised approaches. Results of empirical evaluation show that our method can identify correct senses of polysemous words with reasonable accuracy and create items of satisfactory quality. In fact, we have actually used the generated cloze items in freshmen-level English classes at National Chengchi University. We analyze the cloze items used in the college entrance examinations in Taiwan, and provide an overview of the software tools used to prepare our text corpus in Section 2. Then, we outline the flow of the item generation process in Section 3. In Section 4, we elaborate on the application of word sense disambiguation to select sentences for cloze items, and in Section 5, we delve into the application of collocations and selectional preferences to generate distractors. Evaluations, discussions and related applications of our approaches to the tasks of word sense disambiguation and item generation are presented in Section 6, which will be followed by the concluding section. 2. Data Analysis and Corpus Preparation 2.1 Cloze items for Taiwan College Entrance Examinations Since our current goal is to generate cloze items for college entrance examinations, we analyzed the effectiveness of considering the linguistic features of cloze items with statistics collected from college entrance examinations administered in Taiwan. We collected and analyzed the properties of the test items used in the 1992-2003 entrance examinations. Among the 220 collected multiple-choice cloze items, the keys to the cloze items that were used in the examinations were only verbs (31.8%), nouns (28.6%), adjectives (23.2%) or adverbs (16.4%). For this reason, we will focus on generating cloze items for these four categories. Moreover, the cloze items contained between 6 and 28 words. Figure 3 depicts the distribution of the number of words in the cloze items. The mean was 15.98, and the standard deviation was 3.84. 35 30 25 20 15 10 5 0 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 Number of words Figure 3. Distribution of the lengths of multiple-choice cloze items  Number of Sentences  308  Chao-Lin Liu et al.  In addition, the Web site of the College Entrance Examination Center provides statistics of testees’ responses to a total of 40 multiple-choice cloze items that were used in college entrance examinations held in the years 2002 and 2003 [Taiwan CEEC 2004]. In each of these administrations, more than 110,000 students took the English test. The Web site contains statistics for the error rates of three different groups: ALL, HIGH, and LOW. The ALL group includes all testees, the HIGH group consists of testees whose overall English scores are among the top third, and the LOW group consists of testees whose scores are among the bottom third. Table 1 shows the correlations between the word frequency and selectional-preference (SP, henceforth) strengths of keys and distractors with the error rates observed in different student groups. We will explain how we calculated the frequencies and SP strengths of words in Sections 0 and 4.1, respectively. From the perspective of correlation, the statistics slightly support our intuition that less frequent words make cloze items more difficult to solve. This claim holds for the ALL and HIGH groups in Table 1. However, the error rates for the LOW group do not correlate with the ranks of word frequency significantly. We suspect that this might be because examinees in the LOW group made more random guesses than average students did. We subtracted the error rates of the HIGH group from the error rates of the LOW group, and computed the correlation between the resulting differences between test items and the ranks of word frequency of the keys in the test items. The results are reported in the DIFF column. The DIFF column shows that using less frequent words in items reduced the items’ ability to discriminate between students in the HIGH and LOW groups. The differences in error rates between these groups decreased when less frequent words were used in the cloze items. Figure 4 shows details of the relationships between the error rates and ranks of word frequency of the 40 items that we used to generate Table 1. Since the correlations are not very high, as shown in Table 1, clear trends are not apparent. The charts are included here to allow readers to make their own judgments as to how the error rates and ranks of word frequency are related. In stark contrast, the correlations shown in the bottom half of Table 1 do not offer a consistent interpretation of the relationship between the error rates of different groups and the SP strengths. The negative numbers in the third row of statistics indicate that, when the SP strengths between the keys and stems increase, the error rates of all groups decrease. This is what one might expect. However, the negative statistics in the last row also suggest that as the SP strengths between the distractors and stems increase, the error rates decrease as well—a phenomenon quite hard to explain. We had expected to see the opposite trend, because distractors should be more misleading when they are more related to the stem. This surprising result might be due to the fact that selectional preference alone is not sufficient to explain students’ performance in English tests. Identifying all the factors that can explain students’ performance in language tests may require expertise in education, psychology, and linguistics,  Using Lexical Constraints to Enhance the Quality of  309  Computer-Generated Multiple-Choice Cloze Items  which is beyond the expertise of the authors and the scope of this paper. Nevertheless, as we will show shortly, selectional preference can be instrumental in selecting sentences with desired word senses for use in the item-generation task.  Table 1. Correlations between linguistic features and (1) error rates of items for all students (ALL), (2) error rates of items for the top 33% of the students in the English tests (HIGH), (3) error rates of items of the bottom 33% of the students (LOW), and (4) the differences in error rates of items for the LOW and HIGH groups (DIFF)  rank of word frequency (rank 1 is most frequent) selectional-preference strength with the stem of the items  key distractors key distractors  ALL 0.07 0.11 -0.17 -0.20  HIGH 0.14 0.15 -0.15 -0.14  LOW -0.07 0.03 -0.07 -0.21  DIFF -0.21 -0.15 0.13 0.00  error rate  35 30 25 20 15 10 5 0 0  ALL students 5000 10000 15000 20000 rank of word frequency  error rate  35 30 25 20 15 10 5 0 0  HIGH group 5000 10000 15000 20000 rank of word frequency  error rate  35 30 25 20 15 10 5 0 0  LOW group 5000 10000 15000 20000 rank of word frequency  difference in error rate (LOW-HIGH)  A comparison between HIGH and LOW 25  15  5  -5  -15 0  5000 10000 15000 20000 rank of word frequency  Figure 4. The relationships between error rates and rank of word frequency  310  Chao-Lin Liu et al.  2.2 Corpus Preparation and Lexicons  As indicated in Figure 1, a major step in our approach is acquiring sentences from the Web before we produce items. In this pilot study, we retrieved materials from Studio Classroom <www.studioclassroom.com>, the China Post <www.chinapost.com.tw>, Taiwan Journal <taiwanjournal.nat.gov.tw> and Taiwan Review <publish.gio.gov.tw> by using a Web crawler. We chose these online journals and news reports partially because they offer up-to-date news at a low spelling error rate and partially because that they can be downloaded at no cost. So far, we have collected in our corpus 163,719 sentences that contain 3,077,474 word tokens and 31,732 word types. Table 2 shows the statistics for verbs, nouns, adjectives, adverbs, and the whole database.  Table 2. Statistics of words in the corpus  Word Tokens Word Types  Verbs 484,673 (16%) 5,047 (16%)  Nouns 768,870 (25%) 14,883 (47%)  Adjectives 284,331 (9%) 7,690 (24%)  Adverbs 121,512 (4%) 1,389 (4%)  Overall 3,077,474 (100%) 31,732 (100%)  As a preprocessing step, we look for useful texts from Web pages that are encoded in the HTML format. We need to extract texts from titles, the main bodies of reports, and multimedia contents, and then segment the extracted paragraphs into individual sentences. We segment the extracted texts with the help of Reynar’s MXTERMINATOR, which achieved 97.5% precision in segmenting sentences in the Brown and Wall Street Journal corpora [Reynar and Ratnaparkhi 1997]. We then tokenize words in the sentences before assigning useful tags to the tokens. Because we do not employ very precise methods for tokenization, strings may be separated into words incorrectly. Hence, although the statistics reported in Table 2 should be close to actual statistics, the numbers are not very precise.  We augment the texts with an array of tags that facilitate cloze item generation. We assign part-of-speech (POS) tags to words using Ratnaparkhi’s MXPOST, which adopts the Penn Treebank tag set [Ratnaparkhi 1996]. Based on the assigned POS tags, we annotate words with their lemmas. For instance, we annotate classified with classify and classified, respectively, when the classified has VBN (i.e., past participle) and JJ (i.e., adjective) as its POS tags. We also mark the occurrences of phrases and idioms in sentences using Lin’s MINIPAR [Lin 1998]. This partial parser also allows us to identify such phrases as arrive at and in order to that appear consecutively in sentences. This is certainly not sufficient for creating items for testing phrases and idioms, and we are currently looking for a better alternative.  MINIPAR mainly provides partial parses of sentences that we can use in our system. With these partial parses, words that are directly related to each other can be identified easily,  Using Lexical Constraints to Enhance the Quality of  311  Computer-Generated Multiple-Choice Cloze Items  and we can apply these relationships between words in word sense disambiguation. For easy reference, we will call words that have a direct syntactic relationship with a word W as W’s signal words or simply signals. After performing these preprocessing steps, we can calculate the word frequencies using the lemmatized texts. As explained in Section 2.1, we consider the most frequent word as the first word in the list, and order the words according to decreasing frequency. Also, as stated in Section 2.1, we focus on creating items for testing verbs, nouns, adjectives, and adverbs, we focus on the signals of words with these POS tags in sentences for disambiguating word senses, and we annotate such information in each sentence. When we need lexical information about English words, we resort to machine readable lexicons. Specifically, we use WordNet <www.cogsci.princeton.edu/~wn/> when we need definitions and sample sentences of words for disambiguating word senses, and we consult HowNet <www.keenage.com> for information about classes of verbs, nouns, adjectives, and adverbs.  3. System Architecture  Tagged Corpus  Target-Dependent Item Requirements  Target Sentence  Item  Sentence  Specification Retriever with WSD  Distractor Generator  Cloze Item  Figure 5. Main components of our cloze-item generator  We create cloze items in two major steps as shown in Figure 5. Constrained by the administrator’s Item Specification and Target-Dependent Item Requirements, the Sentence Retriever selects a sentence for a cloze item from a Tagged Corpus, which we discussed its preparation in Section 0. Using the Item Specification, the test administrator selects the key for the desired cloze item and specifies the part-of-speech and sense of the key that will be used in the item. Figure 6 shows the interface of the Item Specification. Our system then attempts to create the requested items. The Target-Dependent Item Requirements specify general principles that should be followed in creating items for a particular test. For example, the number of words in cloze items in the college entrance examinations administered in Taiwan ranges from 6 to 28, and one may wish this as a guideline for creating drill tests. In addition, our system allows the test administrator to not specify the key and to request that the system provide a particular number of items for a particular part of speech instead.  312  Chao-Lin Liu et al.  Please enter the specifications for the desired items.  Figure 6. Interface for specifying cloze items After retrieving the target sentences, the Distractor Generator considers such constraining factors as word frequency, collocations, and selectional preferences in selecting distractors. In cases where the generator cannot find sufficient distractors that go with the key and the target sentence, our system abandons the target sentence and starts the process all over again. 4. Target Sentence Retriever The sentence retriever shown in Figure 5 extracts qualified sentences from the corpus. A sentence must contain the desired key with the requested POS to be considered as a candidate target sentence. We can easily conduct this filtering step using the MXPOS. Having identified a candidate sentence, the item generator needs to determine whether the sense of the key also meets the requirement. We conduct word sense disambiguation based on a generalized notion of selectional preferences. 4.1 Learning Strength of Generalized Selectional Preferences Selectional preferences refer to the phenomenon that, under normal circumstances, some words constrain the meanings of other words in a sentence. A common illustration of selectional preferences is the case in which the word “chair” in the sentence “Susan interrupted the chair” must denote a person rather than a piece of furniture [Resnik 1997; Manning and Schütze 1999]. We extend this notion to the relationships between a word of interest and its signals, with the help of HowNet. HowNet provides the semantic classes of words; for instance, both instruct and teach belong to the class of teach, and both want and demand may belong to the class of need. Let w be a word of interest, and let π be the word class, defined in HowNet, of a signal of w. We denote the frequency with which both w and π participate in the syntactic relationship, v, as fν (w,π ) , and we denote the frequency with which w participates in the v relationship in all situations as fv (w) . We define the strength of the selectional preference of  Using Lexical Constraints to Enhance the Quality of  313  Computer-Generated Multiple-Choice Cloze Items  w and π under the relationship v as follows:  Aν (w,π ) =  fν (w,π ) . fν (w)  (1)  We consider limited types of syntactic relationships. Specifically, the signals of a verb include its subject(noun), object(noun), and the adverbs that modify the verb. Hence, the syntactic relationships for verbs include verb-object, subject-verb, and verb-adverb. The signals of a noun include the adjectives that modify the noun and the verb that uses the noun as its object or predicate. For instance, in “Jimmy builds a grand building,” both “build” and “grand” are signals of “building.” The signals of adjectives and adverbs include the words that they modify and the words that modify the adjectives and adverbs.  We obtain statistics about the strengths of selectional preferences from the tagged corpus. The definition of fv (w) is very intuitive and is simply the frequency with which the word w participates in a relationship v with any other words. We initialize fv (w) to 0 and add 1 to it every time we observe that w participates in a relationship v with any other words.  In comparison, it is more complex to obtain fν (w,π ) . Assume that s is a signal word that participates in a relationship v with w, and that the POS of s is x in this relationship. When s has only one possible sense under the POS x, and when the main class of this sole sense is π, we increase fν (w,π ) by 1. (When HowNet uses multiple fundamental words to describe a sense, the leading word is considered the main class in our computation.) When s itself is polysemous, the learning step is a bit more involved. Assume that s has y possible senses under the POS x, and that the main classes of these senses belong to classes in Π(s) = {π1,…,π i ,…,π y } . We increase the co-occurrences of each of these classes and w, fν (w,π i ) , i=1,…,y, by 1/y. We distribute the weight for a particular co-occurrence of π i with w evenly, because we do not have a semantically tagged corpus. With MINIPAR, we only know what syntactic relationship holds between s and w. Without further information or disambiguating the signal words, we choose to weight each sense of s equally. Table 3 shows the statistics, collected from our corpus, for three verbs eat, see and find to take two classes of nouns, Human and Food, as their objects.  Table 3. Examples of the strengths of selectional preferences, Averb−object (w,π )  Verb-Object Eat  See  Find  Human 0.047 0.487 0.108  Food  0.441 0.005 0.057  314  Chao-Lin Liu et al.  4.2 Word Sense Disambiguation We employ generalized selectional preferences to determine the sense of a polysemous word in a sentence. Consider the task of determining the sense of spend in the candidate target sentence “They say film makers don’t spend enough time developing a good story.” The word spend has two possible meanings in WordNet. 1. (99) spend, pass – (pass (time) in a specific way; “How are you spending your summer vacation?”) 2. (36) spend, expend, drop – (pay out; “I spend all my money in two days.”) Each definition of a possible sense includes (1) the head words that summarize the intended meaning, (2) a short explanation, and (3) a sample sentence. When we focus on the disambiguation of a word, we do not consider the word itself as a head word. Hence, spend has one head word, i.e., pass, in the first sense and two head words, i.e., extend and drop, in the second sense. An intuitive method of determining the meaning of spend in a target sentence is to replace spend in the target sentence with its head words. The head words of the correct sense should fit into the target sentence better than head words of other competing senses. We judge whether a head word fits well into the position of the key based on the SP strength of the head word along with the word class of the signals of the key. Since a sense of the key may include many head words, we define the score of a sense as the average SP strength of the head words of the sense along with all the signal words of the key. This intuition leads to the first part of the total score for a sense, i.e., Ωt , that we will present shortly. In addition, we can compare the similarity of the contexts of spend in the target sentence and sample sentences, where context refers to the classes of the signals of the key being disambiguated. For the current example, we can compare whether the subject and object of spend in the target sentence belong to the same classes as the subjects and objects of spend in the sample sentences. The sense whose sample sentence offers a more similar context for spend in the target sentence receives a higher score. This intuition leads to the second part of the total score for a sense, i.e., Ωs , that we will present below. 4.2.1 Details of Computing Ωt (θi | w,T ) : Replacing Keys with Head Words Assume that word w has n senses in the lexicon. Let Θ = {θ1,…,θi ,…,θn} be the set of senses of w. Assume that sense θi of word w has mi head words in WordNet. (Note that we do not consider w as its own head word.) We use the set Λ i = {λi,1, λi,2 ,…, λi,mi } to denote the set of head words that WordNet provides for sense θi of word w. When we use the partial parser to parse the target sentence T for a key, we obtain information about the signal words of the key. Moreover, when each of these signals is not  Using Lexical Constraints to Enhance the Quality of  315  Computer-Generated Multiple-Choice Cloze Items  polysemous under their current POS tags, we look up their classes in HowNet and adopt the  first listed class for each of the signals. Assume that there are µ(T) signals for the keyword w in a sentence T. We use the set Ψ(T , w) = {ψ 1,T ,ψ 2,T ,…,ψ µ(T ),T } to denote the set of signals for w in T. Correspondingly, we use ν k,T to denote the syntactic relationship between w and ψ k,T in T, and use Γ(T , w) = {ν1,T ,ν 2,T ,…,ν µ(T ),T } to denote the set of relationships between signals in Ψ(T , w) and w. Finally, we denote the class of ψ k,T as π k,T and the set of classes of the signals in Ψ(T , w) as Π(T , w) = {π1,T ,π 2,T ,…,π µ(T ),T } .  Recall that Equation (1) defines the strength of the selectional preference between a word  and a class of the word’s signal. Therefore, the following formula defines the averaged  strength of the selectional preference of a head word λi, j of sense θi of w with the signal words of w in T:  
In this paper, we propose a new method for extracting bilingual collocations from a parallel corpus to provide phrasal translation memories. The method integrates statistical and linguistic information to achieve effective extraction of bilingual collocations. The linguistic information includes parts of speech, chunks, and clauses. The method involves first obtaining an extended list of English collocations from a very large monolingual corpus, then identifying the collocations in a parallel corpus, and finally extracting translation equivalents of the collocations based on word alignment information. Experimental results indicate that phrasal translation memories can be effectively used for computer assisted language learning (CALL) and computer assisted translation (CAT). Keywords: Bilingual Collocation Extraction, Collocational Translation Memory, Collocational Concordancer 1. Introduction Example-based machine translation (EBMT) has been proposed as an alternative approach to automatic translation. Translations of examples range from two-word to multi-word, translations, with or without syntactic or semantic structures [Nagao 1984; Kitano 1993; Smadja 1993; Lin 1998; Andrimanankasian et al. 1999; Carl 1999; Brown 2000; Pearce 2001; Seretan et al. 2003]. In the approach, text and translations are preprocessed and stored in a translation memory, which serves as an archive of existing translations that the MT system can reuse. A number of proposed applications for machine translation and computer assisted translation systems use translation examples found in bilingual corpora; these methods include ∗ Department of Computer Science and Information Engineering, Vanung University, No. 1, Vanung Road, Jhongli, Taoyuan, Taiwan E-mail: tomchuang@msa.vnu.edu.tw + Department of Computer Science, National Tsing Hua University, 101, Kuangfu Road, Hsinchu, Taiwan [Received January 31, 2005; Revised August 24, 2005; Accepted August 26, 2005]  330  Thomas C. Chuang et al.  [Transit 2005], [Deja–Vu 2005], [TransSearch 2005], and [TOTALrecall 2005]. Statistical methods have been proposed for automatic acquisition of bilingual collocations [Smadja et al. 1996; Gao et al. 2002; Wu and Zhou 2003] from parallel bilingual corpora [Kupiec 1993; Smadja et al. 1996; Echizen-ya et al. 2003] or from two comparable monolingual corpora [Lu and Zhou 2004]. These bilingual collocations, if acquired in quantity, can enable a machine translation system to produce more native-speaker like translations. However, parallel corpora of substantial size are harder than monolingual corpora to come by. Therefore, in small- to mid-size parallel texts, collocations may not have high enough counts for a statistical method to reliably extract them. Consider the example of extracting verb-noun collocations for the noun “influence” from 50,000 bilingual sentences (SMEC-50000) in the Sinorama Mandarin-English Corpus (SMEC)1. Some useful bilingual collocations in SMEC have very low occurrence counts. For instance, the bilingual collocation “use influence; 發揮 影響力” appears only once in SM-50000 (see Example 1). Such collocations may not be extracted by the methods proposed in the literature. (1) These circumstances make it unlikely that APEC will be able to avoid reform. In Lai's analysis, the implosion of the Asian economies last year demonstrates their interconnectedness. Therefore, in order to place its own existence on a more secure foundation, Taiwan should carefully observe changes in APEC and use its influence to make the organization into a vehicle driving regional consolidation. 他分析，亞洲國家近年來經濟危機持續不去，證明瞭彼此的連動性，因 此台灣應該注意觀察APEC的轉變，發揮意見的影響力，以使APEC能 夠成為區域整合的火車頭，為我國創造更大的生存利基。 A good way of extracting such bilingual collocation might be to first extract “use influence” as a collocation in a large, separate, monolingual corpus, and then identify its instances and translations in the given parallel corpus (e.g., the Sinorama Mandarin-English Corpus). At present, it is not difficult to obtain a much larger monolingual corpus (e.g., the British National Corpus) that contains enough instances of “use influence” such that extraction of such a collocation type is mostly effective. Example (2) shows one of the 60 instances of 
The importance of automatically recognizing emotions in human speech has grown with the increasing role of spoken language interfaces in human-computer interaction applications. In this paper, a Mandarin speech based emotion classification method is presented. Five primary human emotions, including anger, boredom, happiness, neutral and sadness, are investigated. Combining different feature streams to obtain a more accurate result is a well-known statistical technique. For speech emotion recognition, we combined 16 LPC coefficients, 12 LPCC components, 16 LFPC components, 16 PLP coefficients, 20 MFCC components and jitter as the basic features to form the feature vector. Two corpora were employed. The recognizer presented in this paper is based on three classification techniques: LDA, K-NN and HMMs. Results show that the selected features are robust and effective for the emotion recognition in the valence and arousal dimensions of the two corpora. Using the HMMs emotion classification method, an average accuracy of 88.7% was achieved. Keywords: Mandarin, emotion recognition, LPC, LFPC, PLP, MFCC 1. Introduction Research on understanding and modeling human emotions, a topic that has been predominantly dealt with in the fields of psychology and linguistics, is attracting increasing attention within the engineering community. A major motivation comes from the need to improve both the naturalness and efficiency of spoken language human-machine interfaces. Researching emotions, however, is extremely challenging for several reasons. One of the main difficulties results from the fact that it is difficult to define what emotion means in a precise way. Various explanations of emotions given by scholars are summarized in [Kleinginna et al. 1981]. Research on the cognitive component focuses on understanding the environmental and attended situations that give rise to emotions; research on the physical components emphasizes the physiological response that co-occurs with an emotion or rapidly follows it. In short, ∗ Department of Computer Science and Engineering, Tatung University, 40 ChungShan N. Rd., 3rd Sec, Taipei 104, Taiwan, R.O.C, Tel: +886-2-2592-5252 Ext. 2212, Fax: +886-2-2592-5252 Ext. 2288 E-mail: tlpao@ttu.edu.tw; {d8906005, d9306002, d8906004}@ms2.ttu.edu.tw [Received January 31, 2005; Revised July 15, 2005; Accepted July 19, 2005]  348  Tsang-Long Pao et al.  emotions can be considered as communication with oneself and others [Kleinginna et al. 1981]. Traditionally, emotions are classified into two main categories: primary (basic) and secondary (derived) emotions [Murray et al. 1993]. Primary or basic emotions generally can be experienced by all social mammals (e.g., humans, monkeys, dogs and whales) and have particular manifestations associated with them (e.g., vocal/facial expressions, behavioral tendencies and physiological patterns). Secondary or derived emotions are combinations of or derivations from primary emotions. Emotional dimensionality is a simplified description of the basic properties of emotional states. According to the theory developed by Osgood, Suci and Tannenbaum [Osgood et al. 1957] and in subsequent psychological research [Mehrabian et al. 1974], the computing of emotions is conceptualized as three major dimensions of connotative meaning: arousal, valence and power. In general, the arousal and valence dimensions can be used to distinguish most basic emotions. The locations of emotions in the arousal-valence space are shown in Figure 1, which provides a representation that is both simple and capable of conforming to a wide range of emotional applications.  anger 
In this paper, a bi-lingual large vocaburary speech recognition experiment based on the idea of modeling pronunciation variations is described. The two languages under study are Mandarin Chinese and Taiwanese (Min-nan). These two languages are basically mutually unintelligible, and they have many words with the same Chinese characters and the same meanings, although they are pronounced differently. Observing the bi-lingual corpus, we found five types of pronunciation variations for Chinese characters. A one-pass, three-layer recognizer was developed that includes a combination of bi-lingual acoustic models, an integrated pronunciation model, and a tree-structure based searching net. The recognizer’s performance was evaluated under three different pronunciation models. The results showed that the character error rate with integrated pronunciation models was better than that with pronunciation models, using either the knowledge-based or the data-driven approach. The relative frequency ratio was also used as a measure to choose the best number of pronunciation variations for each Chinese character. Finally, the best character error rates in Mandarin and Taiwanese testing sets were found to be 16.2% and 15.0%, respectively, when the average number of pronunciations for one Chinese character was 3.9. Keywords: Bi-lingual, One-pass ASR, Pronunciation Modeling 1. Introduction Words can be pronounced in more than one ways according to a lexicon; i.e., they usually have multiple pronunciations. Words are also pronounced differently by different people, a ∗ Chang Gung University, Taiwan E-mail: rylyu@mail.cgu.edu.tw + National Tsing Hua University, Taiwan ∗∗ Academia Sinica, Taiwan E-mail: {daucheng, chunnan}@iis.sinica.edu.tw [Received February 22, 2005; Revised July 22, 2005; Accepted August 15, 2005]  364  Dau-Cheng Lyu et al.  phenomenon called “pronunciation variation.” Pronunciation variation has been studied in the speech recognition field [Chen 1996; Cremelie 1996], and reports show that pronunciation variation can cause the performance of automatic speech recognizers to deteriorate if it is not well accounted for. A common approach to solving the pronunciation variation problem is to use pronunciation modeling; where multiple pronunciations are added to each lexeme in a lexicon in order to fit the acoustic data better. A Chinese character is pronounced differently in different languages which use that Chinese character in their writing systems. The same character may or may not have the same meaning in such languages. For instance, the Chinese character “窗”(window) is pronounced “chuang11” in Mandarin and “tang11” in Taiwanese, and these are considered to be multiple pronunciations in a Mandarin/Taiwanese bi-lingual lexicon. “Chuang11” is often mistakenly pronounced “cuang11” (the un-retroflex of “chuang11”) by native Taiwanese speakers, who do not have un-retroflex consonants in their language. This is a common cause of pronunciation variation. In the case of English, which has a more complex vowel inventory than the Han language family, the words “ear” and “year” are difficult for Mandarin speakers to tell apart. In other words, pronunciation variation is a natural and unavoidable phenomenon in a multi-lingual environment. In this world of people who are well-connected by various types of communication devices, multi-lingual communication is necessary, and multi-lingual speech recognition is a must. This paper focuses on Mandarin-Taiwanese bi-lingual large vocabulary speech recognition, and the framework studied here is applicable to other language combinations as well. 
During the process of Chinese word segmentation, two main problems occur: segmentation ambiguities and unknown word occurrences. This paper describes a method to solve the segmentation problem. First, we use a dictionary-based approach to segment the text. We apply the Maximum Matching algorithm to segment the text forwards (FMM) and backwards (BMM). Based on the difference between FMM and BMM, and the context, we apply a classification method based on Support Vector Machines to re-assign the word boundaries. In so doing, we use the output of a dictionary-based approach, and then apply a machine-learning-based approach to solve the segmentation problem. Experimental results show that our model can achieve an F-measure of 99.0 for overall segmentation, given the condition that there are no unknown words in the text, and an F-measure of 95.1 if unknown words exist. Keywords: Chinese, word segmentation, segmentation ambiguity, unknown word, maximum matching algorithm, support vector machines 1. Introduction The first step in Chinese information processing is word segmentation. This is because in written Chinese, all characters are joined together, and there are no separators to mark word boundaries. A similar problem also occurs with languages like Japanese, but at least with Japanese, there are three types of characters (hiragana, katakana and kanji). This helps provide clues for finding word boundaries. In the case of Chinese, as there is only one type of character (hanzi), more segmentation ambiguities may occur in a text. During the process of segmentation, two main problems are encountered: segmentation ambiguities and unknown word occurrences. This paper focuses on solving the segmentation ambiguity problem and proposes a sub-model to solve the unknown word problem. There are basically two types of segmentation ambiguity: covering ambiguity and overlapping ambiguity. The definitions are ∗ Graduate School of Information Science, Nara Institute of Science and Technology, Japan E-mail: {ling-g, masayu-a, matsu}@is.naist.jp [Received August 31, 2004; Revised February 21, 2005; Accepted February 21, 2005]  382  Chooi-Ling Goh et al.  given below. Let x, y, z be some strings which could consist of one or more Chinese characters. Assuming that W is a given dictionary, the covering ambiguity is defined as follows: For a string w = xy, x ∈ W, y ∈ W, and w ∈ W. As almost any single character in Chinese can be considered as a word, the above definition reflects only those cases where both word boundaries .../xy/... and .../x/y/... can be found in sentences. On the other hand, overlapping ambiguity is defined as follows: For a string w = xyz, both w1 = xy ∈ W and w2 = yz ∈ W hold. Although most of the time, one form of segmentation is preferred over the other, we still need to know about the contexts in which the other form is used. Both types of ambiguity require that the context be considered to decide which is the correct segmentation form given a particular occurrence in the text. (1a) and (1b) show examples of covering ambiguity. The string “一家” is treated as a word in (1a) but as two words in (1b). (1a)胡/世庆/一家/三/口/ Hu/ Shiqing/ whole family/ three/ member (All three members of Hu Shiqing’s family) (1b)在/巴黎/一/家/杂志/上/ in/ Paris/ one/ company/ magazine/ at/ (At one magazine company in Paris) On the other hand, (2a) and (2b) are examples of overlapping ambiguity. The string “不 可以” is segmented as “不/可以” in (2a) and as “不可/以” in (2b), according to the context in each sentence. (2a)不/可以/淡忘/远在/故乡/的/父母/ not/ can/ forget/ far away/ hometown/ DE/ parents/ (Cannot forget parents who are far away at home) (2b)不可/以/营利/为/目的/ cannot/ by/ profit/ be/ intention (Cannot have the intention to make a profit)  Chinese Word Segmentation by Classification of Characters  383  We intend to solve the ambiguity problems by combining a dictionary-based approach with a statistical model. In so doing, we make use of the information in a dictionary in a statistical approach. The Maximum Matching (MM) algorithm, a very early and simple dictionary-based approach, is used to initially segment the text by referring to a dictionary. It tries to match the longest possible words found in the dictionary. We can parse a sentence either forwards or backwards. Normally, the differences between the results of forward and backward parsing will indicate the locations where overlapping ambiguities occur. Then, we use a Support Vector Machine-based (SVM) classifier to decide which output should be the correct answer. As for covering ambiguities, in most cases, forward and backward MM will give the same output. In this case, we just make use of the contexts to decide whether or not to split a word into two or more words. Our experimental results show that the proposed method can solve 92% of overlapping ambiguities and 52% of covering ambiguities. 2. Previous Works Solving the ambiguity problems is a fundamental task in Chinese segmentation process. Although many previous researches have focused on segmentation, only a few have reported on the accuracy achieved in solving ambiguity problems. Li et al. [2003] proposed an unsupervised method for training Naïve Bayes classifiers to resolve overlapping ambiguities. They achieved 94.13% accuracy in 5,759 cases of ambiguity. An alternative form of TF.IDF weighting was proposed for solving the covering ambiguity problem in [Luo et al. 2002]. They focused on 90 ambiguous words and achieved an accuracy of 96.58%. Most of the previous methods reported on the accuracy of overall segmentation. Recently, many researches have adopted multiple models. Furthermore, most researchers have realized that character-based approaches are more effective than word-based approaches to Chinese word segmentation. In [Xue and Converse 2002], two classifiers were combined to perform Chinese word segmentation. First, a Maximum Entropy model was used to segment the text, and then an error driven transformation model was used to correct the word boundaries. Their method also used character-based tagging to assign the positions of characters in words. They achieved an F-measure of 95.17 using the Penn Chinese Treebank. Another recent study was that of Fu and Luke [2003], who proposed hybrid models for integrated segmentation. Modified word juncture models and word-formation patterns were used to find word boundaries and at the same time to identify unknown words. They achieved and F-measure of 96.1 using the Peking University Corpus. As the above studies used different corpora in their experiments, it is difficult to tell which method performed better. Solving the unknown word problem is also an important step in word segmentation. An unknown word is a word not found in a dictionary. Therefore, it cannot be segmented correctly by simply referring to the dictionary. Many approaches for unknown word detection  384  Chooi-Ling Goh et al.  have been proposed [Chen and Bai 1997; Chen and Ma 2002; Fu and Wang 1999; Lai and Wu 1999; Ma and Chen 2003; Nie et al. 1995; Shen et al. 1998; Zhang et al. 2002; Zhou and Lua 1997]. These include rule-based, statistics-based, and hybrid models. We cannot ignore the unknown word problem since there are always some unknown words (such as person names, numbers etc.) in a text even when we use a very large dictionary. The creation of new words in Chinese is a continuous process. For example, names for new diseases, technical terms, and new expressions are always being created. The accuracy is better if one focuses only on certain types of unknown words such as person names, place names, or transliteration names, when accuracy of over 80% can be achieved. However, for general unknown words, such as common nouns, verbs etc., the accuracy ranges from only 50% to 70%. 3. Proposed Method We propose a method that uses only minimum resources, meaning that only a segmented corpus is required. The underlying concept of our proposed method is as follows. We regard the problem as a character classification problem. We believe that each character in Chinese tends to appear in certain positions in words. A character can be used at the beginning of a word, in the middle of a word, at the end of a word, or as a single-character word. It can appear at different positions in different words. By looking at the usage of the characters, we can decide on their position tags using a machine learning based model, which in our case is the Support Vector Machines model [Vapnik 1995]. Our method employs a model to solve the ambiguity problem and, at the same time, embeds a model to detect unknown words. We will next describe the method in more detail in the following section. 3.1 Maximum Matching Algorithm We intend to solve the ambiguity problem by combining a dictionary-based approach with a statistical model. The Maximum Matching (MM) algorithm is regarded as the simplest dictionary-based word segmentation approach. It starts from one end of a sentence and tries to match the first longest word wherever possible. It is a greedy algorithm, but it has been empirically proved to achieve over 90% accuracy if the dictionary used is large. However, the ambiguity problem cannot be solved effectively, and it is impossible to detect unknown words because only those words existing in the dictionary can be segmented correctly. If we look at the outputs produced by segmenting the sentence forwards (FMM), from the beginning of the sentence, and backwards (BMM), from the end of the sentence, we can determine the places where overlapping ambiguities occur. For example, FMM will segment the string “即将来临 时” (when the time comes) into “即将/来临/时/”(immediately/ come/ when), but BMM will segment it into “即/将来/临时/”(that/ future/ temporary). Let Of and Ob be the outputs of FMM and BMM, respectively. According to Huang  Chinese Word Segmentation by Classification of Characters  385  [1997], for overlapping cases, if Of = Ob, then the probability that both the MMs will be the correct answer is 99%. If Of ≠Ob, then the probability that either Of or Ob will be the correct answer is also 99%. However, for covering ambiguity cases, even if Of = Ob, both Of and Ob could be correct or could be wrong. If there exist unknown words, they normally will be segmented as single characters by both FMM and BMM. Based on the differences and contexts created by FMM and BMM, we apply a machine learning based model to re-assign the position tags which indicate character positions in words.  3.2 Support Vector Machines  Support Vector Machines (SVM) [Vapnik 1995] are binary classifiers that search for a hyperplane with the largest possible margin between positive and negative samples (see Figure 1). Suppose we have a set of training data for a binary class problem: (x1, y1),…, (xN, yN), where xi ∈ Rn is the feature vector of the ith sample in the training data and yi ∈{+1, -1} is its label. The goal is to find a decision function which accurately predicts the label y for an unseen x. An SVM classifier gives a decision function f(x) for an input vector x, where  ⎛  ⎞  f (x) = sign ⎜ ∑ αi yi K (x, zi ) + b ⎟ .  ⎝ Zi∈SV  ⎠  f(x)= +1 means that x is a positive member, and f(x) = -1 means that x is a negative member. The vectors zi are called support vectors, and they are assigned a non-zero weight αi. Support vectors and the parameters are determined by solving a quadratic programming problem. K(x, z) is a kernel function which computes an extended inner product of input vectors. We use a polynomial kernel function of degree 2, that is, K(x, z) = (1 + x⋅ z)2.  Figure 1. Maximizing the margin  386  Chooi-Ling Goh et al.  We use YamCha [Kudo and Matsumoto 2001] to train our SVM models. YamCha is an SVM-based multi-purpose chunker. It extends binary classification to n-class classification for natural language processing purposes, where we would normally want to classify the words into several classes, as in the case of POS tagging or base phrase chunking. Two straightforward methods are mainly used for this extension, the “one-vs-rest” method and the “pairwise” method. In the “one-vs-rest” method, n binary classifiers are used to compare one ( ) class with the rest of the classes. In the “pairwise” method, n binary classifiers are used to 2 compare between all pairs of classes. We need to classify the characters into 4 categories (B, I, E or S, as shown in Table 1) in our method. We used the “pairwise” classification method in our experiments because it is more efficient during the training phase. Details of the system can be found in [Kudo and Matsumoto 2001]. Table 1. Position tags in a word (BIES tags) Tag Description S one-character word B first character in a multi-character word I intermediate character in a multi-character word (for words longer than two characters) E last character in a multi-character word 3.3 Classification of Characters We intend to classify the characters using the SVM-based chunker [Kudo and Matsumoto 2001] as described in Section 3.2. [Xue and Converse 2002] proposed to regard the word segmentation problem as a character tagging problem. Instead of segmenting a sentence into word sequences directly, characters are first assigned with position tags. Later, based on these postion tags, the characters are converted into word sequences. The basic features used are the characters. However, the number of examples per feature will be small if there is only character information and no other information is provided. Since there are always more known words than unknown words in a text, it is advantageous if we can segment known words beforehand. Therefore, we supply the outputs from FMM and BMM as some of the features. In this case, the learning by SVM is guided by a dictionary for known word segmentation. The similarities and differences between FMM and BMM are used to train the SVM to solve the segmentation ambiguity problem. First, we convert the output of the MMs into a character-wise form, where each character is assigned a position tag as described in Table 1. The BIES tags are as described in [Uchimoto et al. 2000] and [Sang and Veenstra 1999] for named entity extraction. These tags show possible character positions in words. For example, the character “本” is used as a single character word in “一/本/书/＂(a book), at the end of a word in “剧本’ (script), at the  Chinese Word Segmentation by Classification of Characters  387  beginning of a word in“本来＂ (originally), or in the middle of a word in “基本 上＂(basically).  The solid box in Figure 2 shows the features used to determine the tag of the character “春” at location i. In other words, our feature set consists of the characters, the FMM and BMM outputs, and the previously tagged outputs. The context window is two characters on both the left and right sides of the current character. Based on the output position tags, finally, we get the segmentation “迎/新春/联谊会/上/＂ (welcome/ new year/ get-together party/ at/).  Position Char. FMM  BMM Output  i-2  迎  B  S  S  i-1  新  E  B  B  i  春  B  E  E  i+1  联  E  B  B  i+2  谊  S  E  I  i+3  会  B  B  E  i+4  上  E  E  S  Figure 2. An illustration of classification process applied to “At the New Year gathering party”  4. Experiments and Results  We run our experiments with two datasets, the PKU Corpus and the SIGHAN Bakeoff data. The evaluation was conducted using the tool provided in SIGHAN Bakeoff [Sproat and Emerson 2003].  4.1 Experiment with the PKU Corpus  4.1.1 Accuracy on Solving Ambiguity Problem The corpus used for this experiment was provided by Peking University (PKU)1 and consists of about 1.1 million words. It is a segmented and POS-tagged corpus, but we only used the segmentation information for our experiments. We divided the corpus randomly into two parts consisting of 80% and 20% of the corpus, for training and testing, respectively. Since our purpose in this experiment was only to solve the ambiguity problem, not the unknown word  
This paper presents the design and construction of the PolyU Treebank, a manually annotated Chinese shallow treebank. The PolyU Treebank is based on shallow annotation where only partial syntactical structures within sentences are annotated. Guided by the Phrase-Standard Grammar proposed by Peking University, the PolyU Treebank has been designed and constructed to provide a large amount of annotated data containing shallow syntactical information and limited semantic information for use in natural language processing (NLP) research. This paper describes the relevant design principles, annotation guidelines, and implementation issues, including the achievement of high quality annotation through the use of well-designed annotation workflow and effective post-annotation checking tools. Currently, the PolyU Treebank consists of a one-million-word annotated corpus and has been used in a number of NLP research projects with promising results. Keywords: Shallow Treebank, Shallow Parsing, Corpus Annotation, Natural Language Processing 1. Introduction A treebank can be defined as a syntactically processed corpus. It is a language resource with linguistic information annotated at, variously, the word, phrase, clause, and sentence levels, in order to form a bank of linguistic trees. Many treebanks have been constructed for different languages, including Penn Treebank [Marcus et al. 1993] and the ICE-GB [Wallis et al. 2003] for English, and the Penn Chinese Treebank [Xia et al. 2000; Xue et al. 2002] and the Sinica Treebank [Chen et al. 1999; Chen et al. 2003] for Chinese. Most of the reported Chinese treebanks, including the Penn Chinese Treebank and Sinica Treebank, are based on full parsing, where complete syntactical analysis is performed. This includes determining the syntactic categories of words, locating chunks that can be nested, ∗ Department of Computing, The Hong Kong Polytechnic University, Kowloon, Hong Kong Tel: +852-27667326; +852-27667247; +852-27667325 Fax: +852-27740842 E-mail: {csrfxu, csluqin, csyinli, cswyli}@comp.polyu.edu.hk [Received October 13, 2004; Revised March 18, 2005; Accepted March 21, 2005]  398  Ruifeng Xu et al.  finding relations between phrases, and resolving attachment ambiguities. Thus, the output of full parsing is a set of complete syntactic trees. Due to the complexity of natural languages, automatic full parsing is still quite challenging. An alternative to automatic full parsing is to adopt a divide-and-conquer strategy, i.e., to divide full parsing into several independent sub-tasks which can be applied relatively easily. One of these sub-tasks is shallow (or partial) parsing. The purpose of shallow parsing is to identify local syntactical structures that are relatively simple and easy to identify while ignoring the complicated task of analyzing how these phrases are syntactically used to construct sentences. Thus, shallow parsing only identifies local structures in sentences. These local structures form the sub-trees of a full syntactic tree. Because shallow parsing does not involve complex and ambiguous attachment analysis, it can find some local structures at much lower cost and with a much higher accuracy. For these reasons, shallow parsing has in recent years been the focus of more research, and it has been applied in many NLP applications. However, the lack of a large-scale Chinese shallow treebank has been an impediment to research in this area. This has motivated us to construct a Chinese shallow treebank for Chinese natural language processing applications. This treebank, referred as the PolyU Treebank, is named after the University where it is being developed. One problem with shallow parsing is that, unlike full parsing, it seeks to identify only certain local structures in a sentence. Furthermore, at present, there is no widely-accepted common standard for the determining scope and depth of local structures, and different reported works vary in how they define what local structures are [Dalemans et al. 1999; Sun 2001; Li et al. 2003]. Therefore, in this work, we will first discuss the objectives of shallow parsing based on our needs and those of other NLP researchers and define the scope of shallow parsing. In accordance with this defined scope, we will then show how the PolyU Treebank has been constructed by manually annotating shallow syntactic structures from a selected corpus. Obviously, the scope and the depth of shallow annotation should be determined based on the requirements of the applications using the treebank. Based on the typical requirements of NLP research tasks such as Chinese collocation extraction, terminology extraction, and the acquisition of descriptions of terminologies conducted at the authors’ research institution, we restrict shallow syntactic structures to the maximal phrases that play various roles as subjects, predicates, complement clauses and other syntactic components in sentences. Within the scope of the present work, our aim is to identify base-phrases, that is minimum syntactic unit in a maximal phrase. We also identify those nested phrases between base-phrases and maximal phrases which we call mid-phrases. Maximal phrase, Base-phrase, Mid-phrase will be defined in detail in Section 3. Each identified phrase is given a mandatory syntactic label and an optional semantic label. Its header is also identified. An important feature of our treebank is  The Design and Construction of the PolyU Shallow Treebank  399  that the identified phrases are augmented with semantic information. This kind of information is useful in many areas of NLP research but is difficult to identify automatically and sometimes not annotated in the other existing treebanks. For guidance in syntactic annotation, we choose to use the Phrase-Standard Grammar (PSG) as proposed by Peking University [Yu et al. 1998]. There are two reasons for this choice. First, the PSG grammar framework is widely accepted in mainland China. Second, in order to reduce the cost of annotation and to ensure the maximum sharing of our output, we perform shallow syntactic annotation on the segmented and tagged People’s Daily corpus, developed in Peking University [Yu et al. 2001]. The process of constructing our treebank, which has taken more than 15 months, has included guideline design, the development of annotation specifications, and annotation and quality assurance checking. The one-million-word annotated shallow treebank is more than 98.8% accurate in terms of phrase bracketing and more than 98% accurate in phrase labeling. Such a large-scale treebank can be used to support a variety of NLP research. Currently, it has been used to train and to test a shallow parser [Lu et al. 2003]. Furthermore, other research conducted in authors’ institution, including Chinese collocation extraction, Chinese terminologies extraction, and information retrieval, have also benefited from the PolyU Treebank. We are currently optimizing the treebank and making it available to other researchers as a public resource. This paper presents the major issues involved in the design and construction of the PolyU Treebank and its quality control mechanisms. The rest of this paper is organized as follows. Section 2 introduces the design principles. Section 3 describes the annotation guidelines. Section 4 describes the tasks involved in annotating the PolyU Treebank, including corpus data preparation, word segmentation, POS tagging, phrase bracketing, and phrase labeling specifications. Section 5 discusses the quality assurance mechanisms and the post-annotation checking tools developed for this project. Section 6 gives some examples to illustrate how this shallow treebank can be used in NLP. Section 7 gives conclusions. 2. Design Principles Due to the fact that currently, no large-scale shallow-annotated Chinese treebanks are available, in the course of designing PolyU Treebank, we referenced two important fully-annotated Chinese treebank: the Penn Chinese Treebank and the Sinica Treebank. The Penn Chinese Treebank was annotated based on the Government and Bind framework and contains about 500,000 Chinese words, most of which were mainly manually annotated according to a strict quality assurance process [Xue et al. 2002]. The Sinica Treebank was developed by the Academic Sinica, Taiwan. Phrase bracketing and annotation were carried out using a head-driven chart parser guided by Information-based Case Grammar (ICG), and  400  Ruifeng Xu et al.  followed by manual post-editing. The Sinica Treebank contains 39,000 parsed trees and 329,000 words [Chen et al. 1999; Chen et al. 2003]. A natural way to obtain a shallow treebank is to extract shallow structures from a fully annotated treebank. Unfortunately, the Penn Treebank and Sinica Treebank were annotated using different grammar frameworks as well as different word segmentation/POS tagging strategies, making them unsuitable for our annotation scheme. To ensure that the PolyU Treebank would be high in quality and widely accepted, it was designed and constructed based on four basic principles: Principle 1: High resource-sharing capability The PolyU Treebank was designed to sever as a general purpose treebank for use in as wide a range of applications as possible. This called for the selection of an effective and well-accepted grammatical framework for representing syntactical information as well as for a well-accepted word segmentation/POS tagging scheme. We chose to use the Phrase-Standard Grammar (PSG), proposed by Peking University. PSG is widely accepted by Chinese NLP researchers. In the PSG framework, phrases rather than words are treated as basic Chinese syntactical units. The reason is that while an individual word can be used in different ways and may have different part-of-speech (POS) tags representing its different functions in sentences, a phrase is made up of a number of words normally driven by a headword, and consequently, has a stable internal structure and order. Based on this framework, syntactical analysis should be performed in a cascaded fashion, and a linear character string can finally be syntactically analyzed to form a cascaded tree. In the absence of an orthographic device for delimiting words in Chinese, it is necessary to segment words before performing POS tagging. We used a segmented and tagged corpus consisting sentences from the People’s Daily, annotated by Peking University. This corpus was accurately segmented and tagged in accordance with the PSG framework, and contains articles from the People’s Daily published in 1998. The claimed accuracy of word segmentation and POS tagging is 99.9% and 99.5%, respectively [Yu et al. 2001]. Using this popular and accurate resource significantly reduced the cost of annotation in our research and ensured the maximum sharing of our output. Principle 2: Low structural complexity The second design principle was that the PolyU Treebank should not be structurally very complex; its annotation framework should be clear and simple and its syntactic and functional information should be labeled according to commonly used and widely accepted standards. To ensure that our shallow annotation approach satisfied the requirements typical language applications in terms of syntactical information, we chose to focus on the annotation  The Design and Construction of the PolyU Shallow Treebank  401  of phrases and the identification of headwords while ignoring sentence-level syntax. More specifically, we wanted to identify three types of information: (1) base-phrases, that is, non-nesting phrases with at least one headword; (2) maximal phrases, that is, phrases that marked the boundary of our scope of examination, inclosing the base-phrases and plays the role of subject, predicate, complement clause, embedded clause, or other syntactic components of sentences; and (3) mid-phrases, that is the intermediate nesting phrases between base-phrases and maximal phrases if they existed. Maximal phrases and base-phrases will be defined and discussed in detail in Section 3. As for mid-phrases, a limit was imposed on the level of nesting since we did not intend to provide full parsing information. In order to limit the structural complexity, we limited nesting brackets to only three levels. In other words, mid-phrases were limited to only at most one level. Principle 3: Sufficient and useful syntactic information The third design principle was to provide syntactic information at a low level of complexity that would be useful for and effective in a wide variety of NLP applications. Earlier works in Chinese shallow annotation had annotated only non-nesting base-phrases [Sun 2001]. However, base-phrase annotation alone is not adequate for many applications. Our annotation scheme permits three levels of nesting, and this has a number of advantages. First, maximal phrases indicate the essential syntactic elements of a sentence, such as the subject and predicate, and the availability of this information makes it is possible in many applications to refine the search context window. Secondly, base-phrases are the simplest and most stable structural elements of a sentence. Thus, they are regarded as the smallest syntactic units. Lastly, nested mid-phrases are useful for describing distant modifier relations within maximal phrases, which is helpful in certain applications. The PolyU Treebank provides not only adequate syntactical information but also some semantic information. To achieve this, each phrase is given a syntactic label and sometimes also a label providing semantic information. For example, “国家航空和宇宙航行局”(NASA) is a noun phrase and is assigned the label NP. Furthermore, in terms of semantics, it is a noun phrase that indicates the name of an organization, so it is given the appropriate additional label, NT. The fact that the PolyU Treebank is a “Not-So-Shallow” treebank makes it substantially different from and more useful than other base-phrase only shallow treebanks. The information it provides can be used in language applications to remove ambiguities. Finally, we should point out that in our treebank, the headword of a base-phrase is also annotated. Principle 4: Large quantities of annotated data with great accuracy The sizes of existing Chinese treebanks range from 100,000 to 500,000 words. It is an acceptable size for full parsing [Leech and Garside 1996] but not sufficient for lexical-level analysis. With reference to work on the English language, it is our goal to create a treebank of  402  Ruifeng Xu et al.  one million words. A treebank of this size can support the design and training of a shallow parser and be directly used in the collocation extraction and named entity identification work being conducted by authors’ research group. A well-developed treebank must be very accurately annotated. With the goal of reducing annotation errors, we have designed clear and simple annotation guidelines. To avoid inaccuracies arising from automatic parsing, we have performed annotation manually, and post-annotation error and consistency checking have been performed with tools developed by us. Finally, to avoid human errors, some texts are double- and triple-annotated and then compared. This allows makes it easy to identify and correct errors. 3. Annotation Guideline Design The establishment of annotation guidelines is the first step in treebank development. To ensure high quality output, the guidelines must follow the design principles and must be clear, unambiguous, easy to understand, and easy to follow. The PolyU Treebank guidelines include definitions of (1) syntactical phrase categories, (2) categories of semantic information, and (3) different phrase levels, including maximal phrases, mid-phrases and base-phrases. Because the PolyU Treebank is based on a segmented and POS tagged corpus, the part-of-speech tags in the corpus are used (with only minor modifications for the sake of annotation consistency). Appendix 1 provides a complete list and explanations of the POS tags. These tags will be used in the examples provided in this paper. Brackets, [ and ] are used to indicate the left and right boundaries of phrases. The right bracket is appended with syntactic labels in the form of [Phrase]SS-FF, where SS is a mandatory syntactic label, such as NP(noun phrase) and AP(adjective phrase), and FF is an optional label indicating internal semantic information, such as BL(parallel). For example, a noun phrase with parallel components will be annotated as [荣誉/n 与/c 尊严/n]NP-BL (honor and dignity). 3.1 Defining the syntactical phrase categories The first level of information for describing phrases is that in the syntactical phrase category. With reference to the works of Penn Chinese Treebank and Sinica Treebank, our guidelines define a total of eight syntactical phrase categories: NP — Noun phrase. An NP is headed by a noun and the header is normally the last noun in the phrase, e.g., [市场/n 经济/n#]NP (market economy). TP — Time phrase. A TP consists of continuous time words and is used to indicate a time, e.g., [早上/t８时/t]TP (8:00 in the morning).  The Design and Construction of the PolyU Shallow Treebank  403  FP — Position phrase. A FP is headed by a position word, f, and is used to indicate position information, e.g., [内蒙古/ns东北部/f#]FP (North-east of Inner Mongolia). VP — Verb phrase. A VP is a phrase headed by a predicate and containing no subject, e.g., [顺利/a启动/v#]VP-ZZ (successfully start), and [分析/v# 问题/n]VP-SBI (analyze the problem). AP — Adjective phrase. The header of an AP is an adjective and the whole phrase acts as an adjective in the sentence, e.g.,[公正/a合理/a#]AP (fair and reasonable). DP — Adverb phrase. The header of a DP is an adverb, and the whole phrase plays the role of an adverbial role in a sentence, e.g., [已/d 不再/d#]DP (no longer). PP — Preposition phrase. A PP is the phrase which begins with a preposition, e.g., [在/p贵州 /ns农村/n]PP (In the countryside of Guizhou Province). QP — Quantifier phrase. A QP consists of a number and a quantifier. The quantifier acts as the header. Normally, a QP is used as the modifier of an NP or a VP, e.g., [[数千/m名/q#]QP 士兵/n (several thousand soldiers). 3.2 Defining semantic information categories The PolyU Treebank is unique in that it is annotated with semantic labels. A annotation of the FF labels is not mandatory. Only those phrases with pre-defined semantic phrase categories are labeled. Semantic information is very useful for some language applications. For example, 山东/ns 烟台/ns 市/n (Yantai City, Shan Dong Province) and 烟台/ns 大学/n (Yantai University) are both noun phrases, but the first one is the name of a place and the second that of an organization. Using the semantic information labels NS (Name of a place) and NT (Name of an organization) allows one to distinguish between these two NPs. This is highly useful in named entity extraction and automatic summarization. The additional semantic labels can be considered a natural byproduct of manual annotation since annotators naturally need to go through the mental process of identifying them. We simply making them available so that such used knowledge are not wasted during annotation. In the following, we listed the semantic categories. Semantic information categories for Noun Phrases NT — Name of an organization, e.g., [烟台/ns 大学/n]NP-NT (Yantai University). NS — Name of a place, e.g., [江苏省/ns铜山县/ns]NP-NS (Jiangsu Province, Tongshan Country). NR — Name of a person, e.g., [胡/nr 锦涛/nr]NP-NR (Hu Jintao). NZ — Other proper noun phrase, e.g., [诺贝尔/nr奖/n]NP-NZ (The Nobel Prize).  404  Ruifeng Xu et al.  BL — Juxtaposition structure. A BL label indicates that the phrase is made up of two or more parallel components, e.g., [中国/ns 与/c 南非/ns]NP-BL (China and South Africa). FZ — Appositive. An NP with FZ labels normally has two equivalents, e.g., [[国家/n主席 /n]NP [江/nr 泽民/nr ]NR]NP-FZ (the president of China, Jiang Zemin). PZ — Noun modifier. A PZ is the default semantic structure of an NP, e.g., [美丽/a 的/u 花 /n#]NP-PZ (beautiful flower). FS — Noun plurals. A FS indicates that the last word in a noun phrase is a suffix for noun plurals, e.g., [朋友/j# 们/k]NP-FS (friends). DE — A DE construction is a special kind of an NP structure in Chinese. It ends with “的”(DE) and indicates the absence of the complementation, e.g., 比/v[原先/d预料/v的 /u]NP-DE低/a (lower than originally expected). SU — A SU construction is a special kind of NP structure in Chinese. The typical pattern is 所(SUO)+VP+NP, e.g., [所/u画/v禽鸟/n#]NP-SU (the birds painted by). Semantic information categories for Verb Phrases SBI — Predicate and its object. A VP with the label SBI contains of a predicate and an object, e.g., [打/v# 篮球/n]VP-SBI 是/v 我/r 的/u 爱好/n (playing basketball is my hobby). SBU — Complement. The label SBU indicates that the second part of the VP phrase is the complement modifying the first part of the VP, e.g.[医治/v# 无效/v]VP-SBU (ineffectively treat). ZZ — When a VP has the label ZZ, the verb is the header and other words are its modifiers, e.g., [[有效/ad 打击/v#]VP-ZZ了/u 敌人/n]VP-SBI (effectively strike the enemy). SD — Serial verb constructions. A SD indicates that there are serial actions in a VP phrase, where the last action is the cardinal action, e.g., [[审核/v 发放/v]VP-SD 护照/n]VP-SBI (verify and issue the passport). BA — A BA construction is a special kind of VP structure in Chinese. The typical pattern is把 (BA)+NP1 +VP, e.g., [把/p[扶贫/vn开发/vn工作/vn]NP-PZ 作为/v#]VP-BA (place the work of poverty reduction and social development as). BEI — A BEI-construction is a special kind of a VP structure in Chinese. The typical patterns are被(BEI)+ NP+VP and NP+被+VP, e.g., 商店/n [被/p[责令/v# 停业/vn]VP-SBI]VP-BEI (the shop was ordered to close). Semantic information categories for Time Phrases PO — A point-of-time indicator. The label PO indicates that the TP carries point-of-time information, e.g., [７月/t １日/t]TP-PO (July 1).  The Design and Construction of the PolyU Shallow Treebank  405  DU — A period-of-time indicator. A DU indicates a period of time, e.g., [今后/t ３/m年 /q]TP-DU (following three years). Semantic information categories for Prepositional Phrases YY — Causation information. A YY label is used only to modify a PP to indicate that the PP carries causation information, e.g., [因/p 饿/a]PP-YY 死亡/v (starved to death). DX — Object information. The label DX is used to modify a PP to indicate object information, e.g., [向/p [受灾/vn 地区/n]NP]PP-DX (to the disaster area). DD — Place information. This is the place indicator of a PP, e.g., [在/p 深圳/ns]PP-DD (in Shenzhen). FM — Method information. A PP with an FS label signals the existence of method information, e.g., [通过/p [股票/n 上市/v]S]PP-FM (Through the stock market). MD — Motivation information. A PP with an MD label signals the existence of motivation information, e.g., [为/p 动武/v]PP-MD [找/v 借口/n]VP-SBI (looking for an excuse for war). GJ — Tool information. A GJ label indicates that a PP carries tool information, e.g., [用/p 公车/n]PP-GJ (using a public-bus). SJ — Time information. A SJ label indicates that a PP carries time information, e.g., [到/v 目前/t 为止/v]PP-SJ (up to now). 3.3 Phrase bracketing Phrases in the PolyU Treebank are divided into three levels: maximal phrases, mid-phrases and base-phrases. The syntactical analysis and annotation of the PolyU Treebank begins with the identification of maximal phrases which define the scope of examination for bracketing. A maximal phrase is a predicate that plays the role a distinct syntactic component of a sentence, realized by the maximum span of its non-overlapping length. Maximal phrases form the backbone of a sentence. The identification of maximal phrases is one of the most difficult steps in the whole process in that annotators have to syntactically analyze sentences and understand their syntactic components even though they have not yet been labeled. The objective of identifying maximal phrases is to separate a sentence into several syntactic components for examination. After maximal phrases are identified, the base-phrases can then be identified within the scope of examination, that is, within each maximal phrase. A base-phrase is defined as a minimum non-nesting phrase with a stable internal structure and independent semantic role. Normally, a base-phrase has a lexical word as its headword. Essentially, a base-phrase must consist of continuous words and contain no nesting components. It never overlaps with other phrases and must be contained within a maximal  406  Ruifeng Xu et al.  phrase. Base-phrases normally conform to a number of typical patterns, such as [a+n]->NP, [a+a]->AP. A mid-phrase is a nested phrase within a maximal phrase and has a base-phrase as its header. A mid-phrase may contain more than one base-phrase, but only one will be its header. A mid-phrase may have nested components, but none of them may overlap. The headword of each phrase is also annotated. Further details and examples of phrase bracketing will be provided in Section 4. 4. Implementation of the PolyU Treebank 4.1 Corpus data preparation The People’s Daily corpus, developed by Peking University, consists of more than 13,000 articles and a total of five million words. Since only one million words are required in the PolyU Treebank, we carried out a data selection process. To avoid the duplication of short-lived events and topics, we treated each day’s news as a single unit, and we picked six random days in each month from among the six months of data in the entire collection as the raw treebank data. 4.2 Word Segmentation and Part-of-Speech Tagging In the tasks of the word segmentation and POS tagging of the People’s Daily corpus, we were guided by the PSG grammar and “The Grammatical Knowledge-base of Contemporary Chinese” [Yu et al. 1998]. The specifications include a total of 43 POS tags. Peking University claimed that the accuracy of word segmentation and POS tagging was higher than 99.9% and 99.5%, respectively [Yu et al. 2001]. In this project, we directly used the PKU POS tagging results and made only some notational changes. These changes were made to ensure consistent labeling in our system, where lower cases are used to in word-level tags and upper cases are used in phrase-level labels. 4.3 Phrase Bracketing and Annotation Identification of Maximal-phrases: A maximal phrase contains at least one base-phrase and plays a syntactic role in the sentence. Consider the following example sentence: 中国/ns 旅游年/n 是/v 一/m 次/q 国家级/b 的/u 宣传/vn 促销/vn 活动/vn (Example.1) (China Tourism Year is a national-level promotion and marketing activity)  The Design and Construction of the PolyU Shallow Treebank  407  We find that the above sentence has a S-V-O structure. 中国/ns 旅游年/n is the subject, 是/v is the predicate, and 一/m 次/q 国家级/b 的/u 宣传/vn 促销/vn 活动/vn is the object. Clearly there are three syntactic components in this sentence, thus, two separate maximal-phrases, [中国/ns 旅游年/n]NP (China Tourism Year) and [一/m 次/q 国家级/b 的/u 宣传/vn 促销/vn 活动/vn]NP (a national-level promotion and marketing activity) are annotated. Note that 是/v is also considered a maximal phrase because it acts as a predicate. However, since it has only one lexical word and is structurally unambiguous, by default, it is not bracketed. Admittedly, 是/v and 一/m 次/q 国家级/b 的/u 宣传/vn 促销/vn 活动/vn can be constructed as a VP, but we regard this kind of bracketing is more useful for indicating how phrases may be used to construct a sentence. That is to say, this kind of bracketing would take us into the realm of full parsing, which is not our objective. Thus, we choose to bracket them as separate phrases. As a result, the maximal phrase annotation result is [中国/ns 旅游年/n]NP 是/v [一/m 次/q 国家级/b 的/u 宣传/vn 促销 /vn 活动/vn]NP-PZ. Consider another example, 富裕/v 起来/v 的/u 当地/a 农民/n 自发/d 地/u 组织/v 了/u 多个/a 业余/a 乐团/n (the rich farmers took the initiative to organize several amateur bands) (Example 2) We can separate this sentence into three components, 富裕/v 起来/v 的/u 当地/a 农民 /n is the subject, 自发/d 地/u 组织/v 了/u is the predicate, and 多个/a 业余/a 乐团/n is the object. Thus, this sentence is annotated with three maximal phrases, bracketed and labeled as follows: [富裕/v 起来/v 的/u 当地/a 农民/n#]NP [自发/d 地/u 组织/v# 了 /u]VP-ZZ [多个/a 业余/a 乐团/n]NP-PZ Most syntactical labels can be used in maximal phrases, except for AP (adjective phrase), DP (adverb phrase), and QP (quantifier phrase). Meanwhile, NP-NT, NT-NS, NP-NZ may only be used to label maximal phrases. These types of phrases do not normally contain nesting components or header words.  408  Ruifeng Xu et al.  Base-phrases Identification: Base-phrases are identified only within an already-identified maximal phrase, either nesting inside it or overlapping it. Normally a base-phrase contains two-to-four words with one lexical word as its header. Take the maximal phrase [一/m 次/q 国家级/b 的/u 宣传/vn 促销/vn 活动/vn]NP-PZ in Example 1 as an example, [一/m 次/q]QP (a) and [宣传/vn 促销/vn 活动/vn#]NP-PZ (promotion and marketing activity) are base-phrases in this maximal phrase. Thus, the sentence is annotated as follows: [中国/ns 旅游年/n]NP 是/v [[一/m 次/q]QP 国家级/b 的/u [宣传/vn 促销/vn 活动/vn]NP-PZ]NP-PZ. As it happens, [中国/ns 旅游年/n]NP and 是/v are also base-phrases, but because they overlap with maximal phrases, they are not further bracketed. Our annotation principle here is that if a base-phrase overlaps with a maximal phrase, it will not be bracketed twice. It should be pointed out that the identification of base-phrase is the most fundamental and important goal of treebank annotation. The identification of maximal phrases can be thought as the parsing of a clause using a top-down approach. The identification of base-phrase is however, follows bottom-up approach, the object of which is to identify the most basic units within maximal phrases. Mid-Phrases Identification: Because other syntactic structures may sometimes exist between base-phrases and maximal phrases, it is useful to identify one more level of syntactic structure within a maximal-phrase, the mid-phrase. This step begins with the examination of a base-phrase. Thus, Example 1 is further annotated as follows: [中国/ns 旅游年/n]NP 是/v [[一/m 次/q]QP [国家级/b 的/u [宣传/vn 促销/vn 活动/vn]NP-PZ]NP-PZ]NP-PZ where, the underlined text contains the additional annotations. As we limit nesting to three levels, any further nested phrases are ignored. The following sentence shows the result of annotation with three levels of nesting:  The Design and Construction of the PolyU Shallow Treebank  409  [目前/t [企业/n 发展/vn]NP [值得/v 注意/v 的/u [[几/m 个/q]QP 问题 /n]NP-PZ]NP]NP (several issues which are worthy of consideration in the development of current enterprise). Full annotation would identify four levels of nesting, as shown below, but our system does not include the additional level of bracketing indicated by the underlined annotations as this is beyond our limit of 3 levels. [目前/t [ [企业/n 发展/vn]NP [值得/v 注意/v 的/u [[几/m 个/q]QP 问题 /n]NP-PZ]NP ]NP ]NP. Annotation of Headwords In our system, a ‘#’ tag is appended to a word to indicate that it is a headword. Here, a headword must be a lexical word (sometimes also called a content word) rather than a function word. In most cases, a headword stays in a fixed position in a base-phrase. For example, the headword of a noun phrase is normally the last noun in the phrase. Thus, it is considered to be in the default position and to need no explicit annotation. For example, in the clause [美国/ns 科学家/n]NP [绘制/v 出/v]VP-SBU (the American scientists drafted ), [绘制/v 出/v] (drafted) is a verb phrase, and the headword of the phrase is 绘制/v, which is not in the default position for a verb phrase headword. Thus, this phrase is further annotated as: [美国/ns 科学家/n]NP [绘制/v# 出/v]VP-SBU. Note that 科学家/n is also a headword in [美 国/ns 科学家/n] (the American scientists), but since it is in the default position (for the noun phrase NP, according to the default grammatical structure, the last noun in the phrase is the headword, and the other components are the modifiers taking the PZ label), no explicit annotation is needed. 5. Quality Assurance and Annotation Progress Our research team is made up of four people from the Hong Kong Polytechnic University (HKPU), two linguists from Beijing Language and Culture University (BLCU), and some research collaborators from Peking University. The annotation work has been carried out by four post-graduate students of languages and computational linguistics from BLCU.  410  Ruifeng Xu et al.  
Precise phone/syllable boundary labeling of the utterances in a speech corpus plays an important role in constructing a corpus-based TTS (text-to-speech) system. However, automatic labeling based on Viterbi forced alignment does not always produce satisfactory results. Moreover, a suitable labeling method for one language does not necessarily produce desirable results for another language. Hence in this paper, we propose a new procedure for refining the boundaries of utterances in a Mandarin speech corpus. This procedure employs different sets of acoustic features for four different phonetic categories. In addition, a new scheme is proposed to deal with the “periodic voiced + periodic voiced” case, which produced most of the segmentation errors in our experiment. Several experiments were conducted to demonstrate the feasibility of the proposed approach. Keywords: speech assessment methods phonetic alphabet, speech corpus, sequential forward selection, k-nearest neighbor rule, leave-one-out, speaker-adapted model, context-dependent hidden Markov model (HMM). 1. INTRODUCTION Corpus-based speech synthesis systems are becoming more and more popular due to the high degree of fluency achieved and the natural feel of the generated speech. However, such systems always require a significant amount of human effort in labeling the phonetic boundaries of the corresponding corpus [Van Erp et al. 1988] [Wang et al. 1999] [Cosi et al. 1991]. Therefore, a great deal of research on automatic phonetic labeling methods has been conducted over the past several years [Ljolje et al. 1993, 1994] [Demuynck et al. 2002]. In general, most of these methods involve the following two steps: (1) rough phonetic segmentation by means of Viterbi forced alignment using HMM (hidden Markov models) or other statistical methods; ∗ Multimedia Information Retrieval Laboratory, Dept. of Computer Science, National TsingHua University, Hsing-Chu, Taiwan, Tel: +88635715131-3506 E-Mail: {gavins, jang, marco}@wayne.cs.nthu.edu.tw [Received December 8, 2004; Revised April 6, 2005; Accepted April 6, 2005]  146  Cheng-Yuan Lin et al.  (2) high time-resolution analysis of the phonetic boundaries using boundary checking rules. These HMM-based recognizers can be categorized in various ways. For example, some use context-dependent HMM, while others use context-independent HMM [Makashay et al. 2000]. Also, there are various types of HMM training methods, including speaker-dependent (SD), speaker-independent (SI), and speaker-adapted (SA) models. Although the HMM-based speech recognizer using MFCCs (mel-frequency cepstral coefficients) is well known for its excellent speech recognition, ability, its use of automatic phonetic segmentation and labeling does not always produce precise and satisfactory results necessary for the development of TTS. As a result, other acoustic features and refinement algorithms have been proposed in the literature to improve the phonetic labeling results obtained from HMM-based recognizers. Several works have focused on automatic phonetic labeling, in the last few years. For example, in [Bonafonte et al. 1996], Bonafonte et al. took Gaussian probability density distribution as a similarity measure. In [van Santen et al. 1990], Jan P. H. van Santen et al. adopted broad-band and narrow-band edge detection. In [Torre Toledano et al. 1998], Toledano et al. tried to mimic human labeling using a set of fuzzy rules. In [Sethy et al. 2002], Sethy at al. employed adapted CDHMM (continuous density hidden Markov model) models [Lamel et al. 1993]. The main focus of all of these studies has been English speech, and they have seldom addressed the question of which phonetic class tends to be more error prone. Moreover, the methods proposed in the above papers may not perform equally well when dealing with another language. For example, most approaches for English utterance segmentation can be divided into two categories: rule-based [Torre Toledano et al. 1998] and statistics-based [Sethy et al. 2002] methods. For a rule-based approach, one needs to define a set of rules (crisp or fuzzy) for various phonetic transitions. For a statistics-based approach, one needs to collect a sample data set and label the set accordingly. Conceptually, the rule-based approaches for English corpora can be adapted for application to Chinese corpora. But in fact, it is hard to design such a system without the aid of human experts who have a thorough understanding of the similarities and differences between the phonetic sets of these two languages. It is our belief that the above two approaches should be used in a seamless, integrated manner. As a result, we have developed a hybrid approach, where most of the boundaries are identified via statistical pattern recognition (Sequential Forward Selection, K-Nearest Neighbor Rule and Leave-One-Out) [Whitney 1971] [Duda et al. 2001], while the most difficult cases (periodic voiced + periodic voiced) are handled using a rule-based approach. Mandarin Chinese is a tonal language, and each character is associated with one or several syllables. A Chinese syllable is either composed of a CV (Consonant-Vowel or INITIAL-FINAL [Chou et al. 2002] [Lee 1997]) structure or a single V (Vowel) structure. Therefore, the primary effort in speech labeling focuses on precisely identifying the  Automatic Segmentation and Labeling for Mandarin Chinese  147  Speech Corpora for Concatenation-based TTS  boundaries of each syllable. Then the boundary between a consonant and a vowel within a syllable can be identified according to the type of a consonant. In most cases, the consonant is fricative, affricate, or plosive, and the consonant can easily be distinguished using several acoustic features other than MFCCs, such as zero-crossing rate or pitch, etc. If the consonant is periodic, as in the case of “ ㄌ ” ( “l” in SAMPA (http://www.phon.ucl.ac.uk/home/sampa/home.htm), the acronym of ‘Speech Assessment Methods Phonetic Alphabet’), then the consonant does not need to be segmented, and the whole syllable should be treated as a single unit for TTS, since further operations involving pitch or time scale modification should be performed on both the consonant and the vowel. In [Chou et al. 1998, 2002], Chou et al. proposed an SD-based HMM model plus simple boundary correction rules for Mandarin Chinese. However, to construct this system is time consuming because of the iterative procedure used for forced alignment, the correction rules and, re-training. In addition, it becomes particularly inefficient if the speech corpus is updated incrementally and regularly, such as by adding one hour of speech data per week. Furthermore, the SD-based HMM model may not outperform the SA-based HMM if the size of the training data is moderate; for example, there is one hour of data for the same speaker. In this paper, we propose an SA-based HMM recognizer that performs a forced alignment first and then employ a refinement procedure to modify the identified boundaries. The proposed refinement procedure uses several innovative acoustic features to refine boundaries for various phonetic categories. These approaches and experimental results obtained using them will be described in the following sections. This paper is organized as follows. Section 2 introduces our forced alignment procedure that uses an HMM recognizer to get initial estimations of all boundaries. Section 3 explains the refinement procedure specially designed for four phonetic categories and describes acoustic features are chosen by the SFS (Sequential Forward Selection) [Whitney 1971] algorithm. Section 4 describes the experiments conducted to demonstrate the performance of the proposed refinement procedure, and presents error analysis of irretrievable errors. Section 5 draws conclusions and discusses future work. 2. HMM BASED RECOGNIZER 2.1 From Orthographic Transcription to Phonetic Transcription Forced alignment using the HMM-based recognizer relies on knowledge of the underlying phonetic transcription of a given utterance. In general, once the orthographic transcription and speech data are both available, we can employ forced alignment for automatic phonetic transcription. However, some commonly used Chinese characters have multiple syllables with different pronunciations, depending on the lexical contexts; For instance, the Chinese  148  Cheng-Yuan Lin et al.  character “重” (meaning “heavy”) is pronounced “ㄓㄨㄥˋ”(“TS-U-@N, 4th tone” in SAMPA) in “重要” (meaning “important”) and “ㄔㄨㄥˊ”(“TS_h-U-@N, 2nd tone” in SAMPA) in “重疊” (meaning “overlap”). As a result, word segmentation in the text sentence is necessary for correct phonetic transcription for the purpose of alignment. Commonly used approaches to word segmentation in Chinese NLP (natural language processing) include the forward or backward maximum word matching algorithm [Chen et al. 1992][Yeh et al. 1991], and the dynamic-programming-based statistic probability method [Sproat et al. 1990]. However, no word segmentation algorithm can guarantee perfect results for the following reasons: (1) Word segmentation relies on a collection of Chinese words in the form of a dictionary, which cannot cover all existing words since new words are constantly being created. (2) Even if the word dictionary were complete, some pronunciations could not be determined through dictionary lookup, especially for the case of Chinese poems. For instance, the first character of “朝辭白帝彩雲間” (meaning “leaving Baidi city in colored dawn”) is pronounced “ㄓㄠ” (“TS-au, 1st tone” in SAMPA, meaning “dawn”), not “ㄔㄠˊ” (“TS_h-au, 2nd tone” in SAMPA, meaning “to head for”). This error cannot be corrected through dictionary lookup since “朝” is a single-character word meaning “morning”. (3) Conflicts in word segmentation can lead to different results. For instance “老掌櫃順手把錢 揣在懷裡” (meaning “the old shopkeeper smoothly slipped the money into his pocket”) will be labeled as “老 掌櫃 順手 把 錢 揣 在 懷裡” (meaning “the old + shopkeeper + smoothly + slipped + the money + into + his pocket”) if forward maximum word matching is used. On the other hand, it will be labeled as “老 掌櫃 順 手把 錢 揣 在 懷裡” (meaning “The old + shopkeeper + smoothly + handle bar + the money + into + his pocket”) if the backward approach is adopted. In order to avoid errors resulting from phonetic transcription, we perform the following two steps to achieve a better performance: (1) We perform word segmentation using forward and backward maximum matching based on a word dictionary containing around 90,000 entries. We keep the phonetic transcriptions as candidates for use in the next step. (If the result is the same, then we have only a single phonetic transcription.) (2) We expand the list of obtained phonetic transcription candidates by adding possible syllables for polyphonic characters that are not found in any of the words obtained through the above word segmentation process. We use these different phonetic transcription candidates to perform a forced alignment through Viterbi decoding. We accept the phonetic transcription that has the maximum log likelihood.  Automatic Segmentation and Labeling for Mandarin Chinese  149  Speech Corpora for Concatenation-based TTS  The above steps combine both word segmentation in NLP and forced alignment in speech recognition to achieve better phonetic transcription performance. When the TTS-455 speech corpus with about 6,000 Chinese syllables was used, the syllable error rate was 2.1% and 1.9% for forward and backward maximum matching, respectively. With the addition of step 2, the error rate was reduced to 1.0%, which represents a significant reduction of 50% in the error rate. Some of the error cases are shown in Table 1.  Table 1. Labeling errors when orthographic transcription was transformed to phonetic transcription.  Text sentences of speech corpus.  Human transcription  春風秋月何時『了』 他囊『括』七面金牌  ㄌ一ㄠˇ (“l-I-au, 3rd tone”) ㄎㄨㄛˋ (“k_h-U-o, 4th tone”)  道『行』高深的老僧 掐指一算就知道對方的來意  ㄏㄤˊ (“x-aN, 2nd tone”)  Note: Symbols in parentheses are described in SAMPA.  Machine transcription ㄌㄜ． (“l-@, 5th tone”) ㄍㄨㄚ (“k-U-a,1st tone”) ㄒ一ㄥˊ (“6-I-@N, 2nd tone”)  The last character of the first sentence is a typical single character having multiple pronunciations that cannot be identified through word dictionary lookup. Unfortunately, forced alignment cannot find the correct phonetic transcription, either, because the utterance itself is ambiguous and unclear. The second sentence demonstrates the inadequacy of the word dictionary since “括” in “囊括” (meaning “to obtain”) is labeled “ㄍㄨㄚ” (“k-U-a, 1st tone”in SAMPA) in the dictionary, while it is also pronounced “ㄎㄨㄛˋ”(“k_h-U-o, 4th tone” in SAMPA) colloquially. The error from the third sentence indicates the inadequacy of the word dictionary; the word “道行” (meaning “capability” or “achievement”) should be in the word dictionary, but it is not. 2.2 Speech Corpus Introduction Once a phonetic transcription is obtained, we can perform forced alignment by using a HMM recognizer. In this study, we used two Mandarin Chinese speech corpora: (1) TTS-455 speech corpus: This corpus contains 455 sentences spoken by one speaker and covers about 6,000 syllables. It is mainly for TTS. The details are as follows: I. time duration: 30 minutes (66MB of disk space); II. sampling rate and bit rate: 20,000 Hz, 16bits; III. base syllables: 408; IV. tonal syllables: 1196.  150  Cheng-Yuan Lin et al.  More information on this corpus can be found in (http://speech.cs.nthu.edu.tw/gavins/ Research/SpeechSynthesis/content_hsf455.txt).  (2) TCC-300  speech  corpus  (http://rocling.iis.sinica.edu.tw/ROCLING/MAT/  Tcc_300brief.htm): It contains sentences spoken by 300 subjects from National Taiwan  University, Chiao Tung University, and Cheng Kung University in Taiwan. The recorded  texts were selected from the “Academia Sinica Balanced Corpus”  (http://www.sinica.edu.tw/~tibe/2-words/modern-words).  In order to perform a forced alignment on the TTS-455 speech corpus, we need to train an HMM-based recognizer. This recognizer will be described in Section 4.  3. DESIGN OF THE REFINEMENT PROCEDURE  A post-processing scheme must be used to refine the identified syllable boundaries. Specifically, since a forced alignment is based on MFCCs only, it makes sense to use other acoustic features to enhance precision. As mentioned in Section 1, using either a rule-based or a statistics-based approach alone is inadequate. Therefore, we combine these two methods to deal with a Mandarin Chinese speech corpus. First of all, we divide all Chinese phonemes into four categories. Then, we determine which set is suitable for which method (rule-based or statistics-based) by applying pattern recognition techniques. These steps will be described in detail in the following subsections.  3.1 Four Phonetic Categories There are 37 distinct phonetic alphabets in Mandarin Chinese. This makes it difficult to develop a general method that can be used to refine labeling between all possible phonetic transitions. Hence, we divide all Chinese phonemes into four categories according to their acoustic characteristics. These four categories are fricative and affricate, unaspirated stop, aspirated stop, and periodic voiced [Lu 2002], as listed below in SAMPA format and in the MPA (Mandarin Phonetic Alphabet) format:  z Fricative and affricate: (consonants only) (Fricative) ¾ SAMPA: f x 6 S s ¾ MPA: ㄈ ㄏ ㄒ ㄕ ㄙ (Affricate) ¾ SAMPA: t6 t6_h TS TS_h ts ts_h ¾ MPA: ㄐ ㄑ ㄓ ㄔ ㄗ ㄘ  Automatic Segmentation and Labeling for Mandarin Chinese  151  Speech Corpora for Concatenation-based TTS  z Unaspirated stop: (consonants only) ¾ SAMPA: p t k ¾ MPA: ㄅ ㄉ ㄍ z Aspirated stop: (consonants only) ¾ SAMPA: p_h t_h k_h ¾ MPA: ㄆ ㄊ ㄎ z Periodic voiced: (Consonants) ¾ SAMPA: m n l Z ¾ MPA: ㄇ ㄋ ㄌ ㄖ (Vowels) ¾ SAMPA: a o @ e ai ei au ou an @n aN @N 2 I U y ¾ MPA: ㄚ ㄛ ㄜ ㄝ ㄞ ㄟ ㄠ ㄡ ㄢ ㄣ ㄤ ㄥ ㄦ 一 ㄨ ㄩ  Fricative and affricate are combined in a single category is mainly because of the similarity of the acoustic characteristics. In particular, for any given syllable with an affricate or fricative consonant, according to our observations, the duration ratio between the aperiodic and periodic parts is almost constant; in addition, there usually exists a high zero-crossing rate at the aperiodic part. As for the periodic voiced category, we include both consonants and vowels since they both contain stable harmonic or pitch structures.  3.2 Feature Definition In order to refine the boundaries identified by the HMM-based recognizer, we need to employ several acoustic features other than MFCCs. Some of these acoustic features are commonly used in speech processing; they include the zero-crossing rate, log energy, pitch, and entropy [Shen et al. 1998]. In addition, we also adopt two new acoustic features, the bisector frequency and the burst degree, to help identify boundaries more precisely.  3.2.1 Bisector Frequency  The bisector frequency is defined in equations (1) and (2):  N  freqIndex = arg min 1<k < N  k ∑ Af f =1  −  ∑ Af f =1 2  ,  (1)  152  Cheng-Yuan Lin et al.  bi sectorFreq = freqIndex × sampleRate ,  (2)  N  where Af is the amplitude of the fth frequency component and there are N distinct frequency components in the spectrum. The key characteristic of the bisector frequency is that its value is smaller for a voiced frame but larger for an unvoiced frame. Thus, we can use this feature to distinguish unvoiced from voiced patterns. Although the zero-crossing rate can also be used to detect unvoiced patterns, it is not sufficiently robust, especially when the mean amplitude of an unvoiced frame deviates from zero. For example, in Figure 1, the second unvoiced part of the waveform can be better detected by means of the bisector frequency than the zero-crossing rate.  In our implementation, we normalize the value of this feature to the range [0,1] according to equation (3):  bi  sec  torfreq  =  ⎛ ⎜ ⎝  bi  sec torfreq highfreq −  − lowfreq lowfreq  ⎞ ⎟ ⎠  ,  (3)  where the values of highfreq and lowfreq are empirically set to be sampleRate × 0.8 and 100,  respectively.  2  magnitude  frequency (Hz)  0.1 0 −0.1 −0.2 6000 4000 2000 0 0 60 40 20 0 0  Waveform  0.1  0.2  0.3  0.4  0.5  0.6  time (seconds)  Bisector Frequency  0.1  0.2  0.3  0.4  0.5  0.6  time (seconds)  Zero Crossing Rate  0.1  0.2  0.3  0.4  0.5  0.6  time (seconds)  Counts  Figure 1. A comparison between the bisector frequency and the zero-crossing rate. The second unvoiced part of the waveform is better detected by means of the bisector frequency than the zero-crossing rate. The content of this waveform is “在視為” (“ts-ai, S, U-ei” in SAMPA).  Automatic Segmentation and Labeling for Mandarin Chinese  153  Speech Corpora for Concatenation-based TTS  3.2.2 Burst Degree  It is difficult to recognize a burst pattern in speech using the zero-crossing rate and/or pitch. This is a stable pitch structure does not exist, and the zero-crossing rate is relatively low. To deal with this situation, we adopt a new feature called the burst degree, which is a weighted average between the log energy and the reciprocal average distance between the local maxima, as shown in equation (4):  burst  deg ree  =  ⎛ ⎜W1 ⎝  ×  avg (local  
In this paper, we deal with the linguistic analysis approach adopted in the Formosan Language Corpora, one of the three main information databases included in the Formosan Language Archive, and the language processing programs that have been built upon it. We first discuss problems related to the transcription of different language corpora. We then deal with annotation rules and standards. We go on to explain the linguistic identification of clauses, sentences and paragraphs, and the computer programs used to obtain an alignment of words, glosses and sentences in Chinese and English. We finally show how we try to cope with analytic inconsistencies through programming. This paper is a complement to Zeitoun et al. [2003] in which we provided an overview of the whole architecture of the Formosan Language Archive. Keywords: Formosan languages, Formosan Language Archive, corpora, linguistic analysis, language processing 1. Introduction1 The Formosan Language Archive at Academia Sinica2, Taipei, is part of the Language ∗ Institute of Linguistics, Academia Sinica, Taipei, Taiwan E-Mail: {hsez, harryyu}@gate.sinica.edu.tw 
This paper describes the collection and processing of a pilot speech corpus annotated in dialogue acts. The Mandarin Topic-oriented Conversational Corpus (MTCC) consists of annotated transcripts and sound files of conversations between two familiar persons. Particular features of spoken Mandarin, such as discourse particles and paralinguistic sounds, are taken into account in the orthographical transcription. In addition, the dialogue structure is annotated using an annotation scheme developed for topic-specific conversations. Using the annotated materials, we present the results of a preliminary analysis of dialogue structure and dialogue acts. Related transcription tools and web query applications are also introduced in this paper. Keywords: Taiwan Mandarin, dialogue act, speech corpus 1. Introduction A number of large scale corpora have been collected, processed, and made available for public use over the decades, for instance, the British National Corpus [Leech 1994] and the American National Corpus [Ide and Macleod 2001]. However, most of these corpora contain written language data only. For modern Mandarin, the Sinica Balanced Corpus contains a small percentage of transcripts of spoken data [Chen and Huang 1996]. Duanmu et al. (1998) also published the Taiwanese Putonghua Corpus (TWPTH) via LDC (Linguistic Data Consortium), and it includes five dialogues and thirty monologues. The above two corpora contain materials of Mandarin which is used in Taiwan. In addition, the Chinese Academy of Social Sciences (CASS) has created a national corpus of phonetically and prosodically labelled speech data for the purpose of speech synthesis [Li et al. 2000]. The focus of the CASS corpus is the phonetic variations of spontaneous Mandarin spoken in Mainland China. Nevertheless, because free conversations have no domain specification in the topics, it leads to greatly diverse vocabulary types and sentence varieties. It is sometimes disadvantageous to use free conversations for linguistic analysis or as engineering training data because the individual tokens available in the data are not enough for statistical analysis. ∗ Institute of Linguistics, Academia Sinica, Taipei, Taiwan E-Mail: tsengsc@gate.sinica.edu.tw [Received December 8, 2004; Revised February 4, 2005; Accepted February 5, 2005]  202  Shu-Chuan Tseng  To resolve this problem, several pilot spoken corpora which collected natural dialogues, such as the ATIS Corpus [Kowtko and Price 1989], the TRAINS Corpus [Heeman and Allen 1995], and the Map Task Corpus [Anderson et al. 1991], are all recorded in specific situations or tasks. They have been available to the research community for more than ten years. As stated by Zheng (2004), proper design of a speech corpus before the actual collection process takes place influences the value of the corpora to a great degree. Research results on spoken language processing obtained by applying the above pilot corpora illustrate the importance of spoken corpora. For linguists, corpora are more than merely data. They enable researchers to gain a different understanding of human language use. The enormous, automatic calculation power now available through modern information technology, including software and hardware, facilitates data analysis and summarization for speech engineers. However, well-designed method for the collection and processing of speech corpora will produce more information on research topics, because they will give considerations to the properties and structures of the to-be-collected corpora. This must be done beforehand by humans, and it is not a trivial task. Through the Mandarin speech corpus presented in this paper, we hope to make substantial contributions to linguistic analysis, automatic speech processing, and dialogue structure research. 2. Data Collection and Processing The Mandarin Topic-oriented Conversational Corpus (MTCC) is part of the National Digital Archives Project (2002-2006). Its special focus is on the collection of spoken Mandarin data which reflect synchronic language use and document sociolinguistic properties in Taiwan. Our first aim in data collection is to archive conversations between familiar persons. The topics should be freely chosen by the dialogue participants to a certain degree, but restricted to contemporary events. Therefore, thirty speakers who participated in a previous project that involved collecting dialogues between strangers in 20011 were invited to join the MTCC project [Tseng 2004b]. They were required to come to Academia Sinica with a person with whom they were familiar. Before recordings were made, an instruction sheet was given to the speakers. It indicated that the speakers should choose one piece of news from the year 2001 and carry on a conversation about that topic. The length of the conversation was limited to twenty minutes. When the conversation time had nearly reached twenty minutes, the lab assistants signalled to the speakers to end the conversation naturally. Because the well-known Switchboard corpus is also a topic-specific corpus, we compared it with the MTCC corpus and found three main differences. (1) We collected conversations between familiar persons; the Switchboard Corpus contains conversations between strangers. (2) We recorded conversations 
The MATBN Mandarin Chinese broadcast news corpus contains a total of 198 hours of broadcast news from the Public Television Service Foundation (Taiwan) with corresponding transcripts. The primary purpose of this collection is to provide training and testing data for continuous speech recognition evaluation in the broadcast news domain. In this paper, we briefly introduce the speech corpus and report on some preliminary statistical analysis and speech recognition evaluation results. Keywords: broadcast news, corpus, speech recognition, Mandarin Chinese, transcription, annotation 1. Introduction Starting in 1995, the Defense Advanced Research Projects Agency of the United States (DARPA) directed its research program for continuous speech recognition to focus on automatic transcription of broadcast news [Stern 1997]. Since then, many research groups worldwide have paid attention to this challenging task and put much effort into collecting broadcast news corpora of various languages [Matsuoka et al. 1997, Federico et al. 2000, Graff 2002]. Though some Mandarin Chinese broadcast news corpora are available from LDC (Linguistic Data Consortium, USA)1, they all exhibit the Mainland China accent, and the wording is quite different to that used in the Taiwan area. To support researchers and technology developers who are interested in studying Mandarin Chinese spoken in the Taiwan area, we have collected Mandarin Chinese news broadcast in Taiwan. Due to the success of a previous project which collected Mandarin speech data across Taiwan (MAT) [Wang 1997] and was completed by a group of researchers from several universities and research institutes in Taiwan, the same group of people decided to collaborate again on collecting spontaneous speech data in 2001. The first MAT project spanned the ∗ Institute of Information Science, Academia Sinica, Taipei, Taiwan E-Mail: whm@iis.sinica.edu.tw + Graduate Institute of Computer Science and Information Engineering, National Taiwan Normal University, Taipei, Taiwan 
Keywords: TAICAR, in-car speech, speech database, multi-channel recording, corpus collection and annotation 1. Introduction 1.1 In-car speech corpora review Driver information systems are becoming increasingly complex as more and more functions are integrated into modern cars. Speech-enabled functions will enhance the safety and convenience of operating for future vehicles. To realize such functions, in-car speech processing techniques need to be built and tested first. Thus, it is necessary to collect an in-car speech database. Although many speech corpora [Tapisa et al. 1994], [Roach et al. 1996], [Kudo et al. 1994], [Bernstein et al. 1994] have been created to improve speech-processing effectiveness, few in-car speech databases have been reported. ∗ Department of Information Management, Chang Jung Christian University, 396 Chang Jung Road, Sec.1, Kway Jen, Tainan, Taiwan, R.O.C. Tel: 886-6-2785123 ext. 2071; Fax: 886-6-2785657 E-Mail: wangbb@mail.cju.edu.tw + Department of Electrical Engineering, National Cheng Kung University ∗∗ Department of Computer Science and Information Engineering, National Cheng Kung University [Received November 30, 2004; Revised April 19, 2005; Accepted April 20, 2005] 
This paper describes our initial attempt to design and develop a bilingual reading comprehension corpus (BRCC). RC is a task that conventionally evaluates the reading ability of an individual. An RC system can automatically analyze a passage of natural language text and generate an answer for each question based on information in the passage. The RC task can be used to drive advancements of natural language processing (NLP) technologies imparted in automatic RC systems. Furthermore, an RC system presents a novel paradigm of information search, when compared to the predominant paradigm of text retrieval in search engines on the Web. Previous works on automatic RC typically involved English-only language learning materials (Remedia and CBC4Kids) designed for children/students, which included stories, human-authored questions, and answer keys. These corpora are important for supporting empirical evaluation of RC performance. In the present work, we attempted to utilize RC as a driver for NLP techniques in both English and Chinese. We sought parallel English, and Chinese learning materials and incorporated annotations deemed relevant to the RC task. We measured the comparative levels of difficulty among the three corpora by means of the baseline bag-of-words (BOW) approach. Our results show that the BOW approach achieves better RC performance in BRCC (67%) when compared to Remedia (29%) and CBC4Kids (63%). This reveals that BRCC has the highest degree of word overlap between questions and passages among the three corpora, which artificially simplifies the RC task. This result suggests that additional effort should be devoted to authoring questions with a various grades of difficulty in order for BRCC to better support RC research across the English and Chinese languages. Keywords: bilingual, reading comprehension, corpus.  ∗ Human-Computer Communications Laboratory, Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Hong Kong SAR, China E-Mail: {kxu, hmmeng}@se.cuhk.edu.hk [Received February 2, 2005; Revised May 16, 2005; Accepted May 16, 2005]  252  Kui Xu and Helen Meng  1. Introduction RC is a task that conventionally evaluates the reading ability of an individual, especially during language learning. Typically, the subject is presented with a passage of natural language text and asked to read and comprehend the passage. He or she is then presented with a series of questions about the passage and asked to answer each question based on information understood from the passage. Recently, research efforts have been devoted to the development of automatic RC systems [Anand et al. 2000; Charniak et al. 2000; Hirschman et al. 1999; Ng et al. 2000; Riloff and Thelen 2000]. An RC system can automatically analyze a passage of natural language text. When the system is then presented with a series of (human-generated) questions, it is expected to automatically generate an answer for each question, based on information it extracted or retrieved from the passage. While the conventional RC task can evaluate the reading capability of a human, the task can also be used to drive advancements in natural language processing (NLP) technologies incorporated into automatic RC systems. Furthermore, an RC system presents a novel paradigm of information search, when compared to the predominant paradigm of text retrieval in search engines on the Web. Several comparisons are made below: 1. An RC system has only a limited amount of direct context upon which to draw in order to answer questions, whereas a Web-based search engine has huge amounts of direct context available on the Internet. 2. An RC system aims to generate specific, precise answers to user-posed questions based on the given passage, thus eliminating the need for the user to read the entire passage, whereas a Web-based search engine presents a list of text documents that closely match the user’s query so that the user to further browse for potential answers. 3. A cross-language RC system is analogous to a cross-language text retrieval system, in that a user-posed question/query may be expressed in a different language from that used for passages or archived documents. The first two points suggest that in-depth syntactic and semantic analyses are needed to facilitate the automatic RC task. Hence, the RC task has been a driver of NLP development. Previous works on automatic RC, as cited above, typically involved language learning materials designed for children/students, which included stories, human-authored questions, and answer keys. These materials included the Remedia corpus [Hirschman et al. 1999] and the CBC4Kids corpus [Anand et al. 2000; Dalmas et al. 2003]. These corpora are important because they support empirical evaluation of RC performance. An analytical review of the passages, questions, and answer keys in the Remedia and CBC4Kids training sets reveals that a suite of natural language processing and information extraction technologies are  Design and Development of a Bilingual Reading Comprehension Corpus  253  indispensable for achieving comprehension. Hence, an RC corpus needs to be annotated to support the development of such technologies, as explained below: 1. If questions and answers have equivalent meanings in term of the same frame structures, such as the same predicates, logical subjects, and logical objects, then RC systems should be able to identify the same frame structures. 2. Inference may be based on a common ontology (e.g., the synonymy/antinomy, is-a, part-of, causality, and entailment relations in WordNet), human feelings (e.g., what is strange, happy, sad, etc.) or semantically equivalent descriptions (e.g., “the man is a farmer” means “the man makes a living by farming”). 3. Inference may be based on the context knowledge in a passage. For example, the two sentences “A merry-go-round has wooden animals on it” and “The weather damages the animals” imply that “The weather damages the merry-go-round.” An RC system should be able to find inferences by using context knowledge to identify equivalent meanings. 4. An RC system should be able to perform summarization to answer such questions as “What can we draw from this story?” 5. An RC system should be able to perform calculations in order to answer such questions as “How many boroughs are there in New York City?” 6. An RC system should be able to resolve anaphora in documents in order to identify equivalent meanings. 7. When questions ask for specific persons, times, locations, or numbers, an RC system should be able to identify different named entity types in order to identify equivalent meanings. 8. For definition questions, the answers follow some patterns. An RC system should be able to perform pattern matching to identify equivalent meanings. An example question-answer pair is “Who is Christopher Robin?” and “He is the same person that you read about in the book, Winnie the Pooh.” In addition, previous corpora involved English-only language learning materials. This paper describes our initial attempt to design and develop a bilingual corpus for reading comprehension (RC). In the current work, we attempted to utilize RC as a driver for NLP techniques in both English and Chinese. As an initial step, we sought parallel English and Chinese learning materials for language learning and incorporated annotations deemed relevant to the RC task. We refer to this corpus as the bilingual reading comprehension corpus (BRCC). The rest of this paper is organized as follows. Section 2 reviews related works.  254  Kui Xu and Helen Meng  Section 3 presents considerations for designing a bilingual corpus. Section 4 describes the development of the BRCC. Section 5 discusses BOW matching results on the BRCC, and section 6 draws conclusions.  2. Related Work  Remedia is the first corpus developed for the evaluation of automated RC systems [Hirschman et al. 1999]. This corpus consists of remedial reading materials for grades 3 to 6 and was annotated by the MITRE Corporation [Hirschman et al. 1999]. An example passage with questions and answer keys is shown in Table 1. An answer key is the answer to a given question as provided by the publisher. It may not be an extract from the passage itself. In each story, as exemplified in Table 1, the first line is the title; the second line is the dateline; the others are story sentences. This corpus was used as a test-bed in 2000 in an ANLP-NAACL workshop on “Reading Comprehension Tests as Evaluation for Computer-Based Language Understanding Systems” [Charniak et al. 2000; Hirschman et al. 1999; Ng et al. 2000; Riloff and Thelen 2000].  Table 1. Sample story and questions in the Remedia corpus  Passage Questions Answer keys  Pony Express Makes Final Run (JOPLIN, MISSOURI, October 26, 1861) From now on, mail will be sent a new, faster way. It is called the telegraph. It uses wires to send messages. Now there will be no need for the Pony Express. Since April, 1860, mail has been sent this way. The Last Pony Express rider leaves town today.… Who left Joplin on October 26, 1861? What did the Pony Express riders do? When did the Pony Express start? the last Pony Express rider they carried mail April, 1860  To evaluate RC systems on Remedia, three evaluation metrics were proposed in [Hirschman et al. 1999], namely, P&R, HumSent, and AutSent. P&R directly uses answer keys to compute precision and recall, while both HumSent and AutSent use answer sentences which are obtained according to answer keys. The difference between HumSent and AutSent is that answer sentences in HumSent are marked by humans, while those in AutSent are generated by an automated routine based on which sentence has the highest recall compared with the answer key [Hirschman et al. 1999]. HumSent accuracy is calculated by comparing the system answers with the human-marked answers, scoring one point to an answer from the system that is identical to the human marked answer and zero points otherwise. The average score across all the questions is the HumSent accuracy. Riloff and Thelen [2000] believed that  Design and Development of a Bilingual Reading Comprehension Corpus  255  HumSent was more reliable than P&R and AutSent, since human-marked answer sentences were involved.  In 2000, the CBC4Kids corpus was developed based on the Canadian Broadcasting Corporation web page1 for kids [Anand et al. 2000; Dalmas et al. 2003]. Stories in CBC4Kids are all news articles and cover 12 domains: politics, health, education, science, human interest, disaster, sports, business, crime, war, entertainment, and the environment [Anand et al. 2000]. For each story, Ferro and Bevins from the MITRE Corporation added between eight and twelve questions and answer keys [Anand et al. 2000]. According to the answer keys, the answer sentences were also annotated. In 2000, CBC4Kids was used to develop and evaluate reading comprehension technologies in a summer workshop on reading comprehension held at Johns Hopkins University.2  Details about Remedia and CBC4Kids3 are given in Table 2. The distributions of different types of questions4 in the Remedia training set and CBC4Kids training set are shown in Table 3 and Table 4, respectively. In Table 3 and Table 4, we divide what questions into four sub-types, since this question type asks for a variety of information, ranging from definitions to reasons, numbers, events, time, locations, person names, etc. What–DEF questions ask for definitions; What–VP questions ask about actions that the question subjects performed; What–NP questions ask for noun phrases which are subjects or objects of question predicates; What–OTH questions ask for reasons, numbers, times, locations, person names, organizations, etc.  Table 2. Details of Remedia and CBC4Kids  Publisher Training set Test set Corpus size # questions Annotated information  Remedia Remedia Publications 55 stories 60 stories 20K words 575 named entities, anaphora co-references, and answer sentences  CBC4Kids Canadian Broadcasting Corporation 73 stories 52 stories 35K words 1232 part-of-speech tags, base forms of words, named entity tags, anaphora co-references, parse trees, and answer sentences  
In order to efficiently manage and use knowledge, ontology technologies are widely applied to various kinds of domain knowledge. This paper proposes a Chinese term clustering mechanism for generating semantic concepts of a news ontology. We utilize the parallel fuzzy inference mechanism to infer the conceptual resonance strength of a Chinese term pair. There are four input fuzzy variables, consisting of a Part-of-Speech (POS) fuzzy variable, Term Vocabulary (TV) fuzzy variable, Term Association (TA) fuzzy variable, and Common Term Association (CTA) fuzzy variable, and one output fuzzy variable, the Conceptual Resonance Strength (CRS), in the mechanism. In addition, the CKIP tool is used in Chinese natural language processing tasks, including POS tagging, refining tagging, and stop word filtering. The fuzzy compatibility relation approach to the semantic concept clustering is also proposed. Simulation results show that our approach can effectively cluster Chinese terms to generate the semantic concepts of a news ontology. Keywords: Ontology, Chinese Natural Language Processing, Fuzzy Inference, Feature Selection, Concept Clustering 1. Introduction An ontology is an explicit, machine-readable specification of a shared conceptualization [Studer et al. 1998]. It is an essential element in many applications, including agent systems, knowledge management systems, and e-commerce platforms. It can help generate natural language, integrate intelligent information, provide semantic-based access to the Internet, and extract information from texts [Gomez-Perez et al. 2002] [Fensel 2002] [Schreiber et al. 2001]. Soo et al. [2001] considered an ontology to be a collection of key concepts and their inter-relationships, collectively providing an abstract view of an application domain. With the ∗ Department of Information Management, Chang Jung Christian University, Tainan, Taiwan E-Mail: leecs@mail.cju.edu.tw; leecs@cad.csie.ncku.edu.tw + CREDIT Research Center, National Cheng Kung University, Tainan, Taiwan [Received July 2, 2004; Revised January 25, 2005; Accepted January 26, 2005]  278  Chang-Shing Lee et al.  support of an ontology, a user and a system can communicate with each other through their shared and common understanding of a domain. M. MissiKoff et al. [2002] proposed an integrated approach to web ontology learning and engineering that can build and access a domain ontology for intelligent information integration within a virtual user community. The proposed approach involves automatic concept learning, machine-supported concept validation, and management. Embley et al. [1998] presented a method of extracting information from unstructured documents based on an ontology. Alani et al. [2003] proposed the Artequakt, which automatically extracts knowledge about artists from the Web based on a domain ontology. It can generate biographies that are tailored to a user’s interests and requirements. Navigli et al. [2003] proposed OntoLearn with ontology learning capability to extract relevant domain terms from a corpus of text. OntoSeek [Guarino et al. 1999] is a system designed for content-based information retrieval. It combines an ontology-driven content-matching mechanism with moderately expressive representation formalism. Lee et al. [2004] proposed an ontology-based fuzzy event extraction agent for Chinese news summarization. The summarization agent can generate a sentence set for each piece of Chinese news. In this paper, we propose a Chinese term clustering mechanism for generating the semantic concepts of a news ontology. The parallel fuzzy inference mechanism is adopted to infer the conceptual resonance strength for any two Chinese terms. The CKIP tool [Academia Sinica 1993] is used in Chinese natural language processing, including POS tagging, refining tagging, and stop word filtering. The remainder of this paper is structured as follows. Section 2 introduces the structure of the Chinese term clustering mechanism. Semantic concept analysis for Chinese term clustering is presented in Section 3. Section 4 introduces the parallel fuzzy inference mechanism for semantic concept generation. Section 5 presents experimental results. Finally, some conclusions are drawn in Section 6. 2. The Structure of the Chinese Term Clustering Mechanism An ontology is defined as a set of representational terms called concepts. The inter-relationships among these concepts describe a target world. Here, we will briefly describe the structure of the object-oriented ontology [Lee et al. 2003]. An object-oriented ontology consists of several basic components: (1) Domain: The top layer of the ontology is the name of the domain knowledge. In this study, an ontology was constructed for Chinese news, so its domain name is Chinese news. (2) Category: The second layer contains the categories of the domain ontology. Each category is composed of some concepts with various inter-relationships. There are seven categories for our Chinese news ontology. They are “Political” (政治焦點), “International” (國際要聞), “Finance” (股市財經), “Cross-Strait” (兩 岸風雲), “Societal” (社會地方), “Entertainment” (運動娛樂), and “Life” (生活新知). (3)  A Chinese Term Clustering Mechanism for Generating  279  Semantic Concepts of a News Ontology  Concept Set: The Concept Set is composed of various concepts and relations. We treat each concept in the ontology as a class, so the structure of the Concept Set can be treated as a class diagram. Figure 1 shows an example for our Chinese Political news domain ontology [Lee et al. 2003].  新黨  無黨籍  憲法  財經、財政、經濟  +支出()  政黨、黨團、政團、黨派、派系  實施  復甦、  -黨主席 : String  -副主席 : String -秘書長 : String  
Theoretically, an improvement in a language model occurs as the size of the n-grams increases from 3 to 5 or higher. As the n-gram size increases, the number of parameters and calculations, and the storage requirement increase very rapidly if we attempt to store all possible combinations of n-grams. To avoid these problems, the reduced n-grams’ approach previously developed by O’ Boyle and Smith [1993] can be applied. A reduced n-gram language model, called a reduced model, can efficiently store an entire corpus’s phrase-history length within feasible storage limits. Another advantage of reduced n-grams is that they usually are semantically complete. In our experiments, the reduced n-gram creation method or the O’ Boyle-Smith reduced n-gram algorithm was applied to a large Chinese corpus. The Chinese reduced n-gram Zipf curves are presented here and compared with previously obtained conventional Chinese n-grams. The Chinese reduced model reduced perplexity by 8.74% and the language model size by a factor of 11.49. This paper is the first attempt to model Chinese reduced n-grams, and may provide important insights for Chinese linguistic research. Keywords: Reduced n-grams, reduced n-gram algorithm / identification, reduced model, Chinese reduced n-grams, Chinese reduced model 1. Introduction to the Reduced N-Gram Approach P O’ Boyle and F J Smith [1992, 1993] proposed a statistical method to improve language models based on the removal of overlapping phrases. The distortion of phrase frequencies were first observed in the Vodis Corpus when the bigram “RAIL ENQUIRIES” and its super-phrase “BRITISH RAIL ENQUIRIES” were examined and reported by O’ Boyle. Both occur 73 times, which is a large number for such a small corpus. “ENQUIRIES” follows “RAIL” with a very high probability when it is preceded by “BRITISH.” However, when “RAIL” is preceded by words other than “BRITISH,” “ENQUIRIES” does not occur, but words like “TICKET” or “JOURNEY” may. Thus, the * Computer Science School, Queen's University Belfast, Belfast BT7 1NN, Northern Ireland, UK. Email: {q.le, p.hanna, fj.smith}@qub.ac.uk + Email: rowan@rowan.ws  20  Le Quan Ha et al.  bigram “RAIL ENQUIRIES” gives a misleading probability that “RAIL” is followed by “ENQUIRIES” irrespective of what precedes it. At the time of their research, Smith and O’ Boyle reduced the frequencies of “RAIL ENQUIRIES” by using the frequency of the larger trigram, which gave a probability of zero for “ENQUIRIES” following “RAIL” if it was not preceded by “BRITISH.” This problem happens not only with word-token corpora but also corpora in which all the compounds are tagged as a unit since overlapping n-grams still appear. Therefore, a phrase can occur in a corpus as a reduced n-gram in some places and as part of a larger reduced n-gram in other places. In a reduced model, the occurrence of an n-gram is not counted when it is a part of a larger reduced n-gram. One algorithm to detect/identify/extract reduced n-grams from a corpus is the so-called reduced n-gram algorithm. In 1992, P O’ Boyle and F J Smith were able to store the entire content of the Brown corpus of American English [Francis and Kucera 1964] (of one million word tokens, whose longest phrase-length is 22), which was a considerable improvement at the time. There was no additional way for O’ Boyle to evaluate the reduced n-grams, so his work was incomplete. We have developed and present here our perplexity method, and we discuss its usefulness for reducing n-gram perplexity. 2. Similar Approaches and Capability Recent progress in variable n-gram language modeling has provided an efficient representation of n-gram models and made the training of higher order n-grams possible. Compared to variable n-grams, class-based language models are more often used to reduce the size of a language model, but this typically leads to recognition performance degradation. Classes can alternatively be used to smooth a language model or provide back-off estimates, which have led to small performance gains but also an increase in language model size. For the LOB corpus, the varigram model obtained 11.3% higher perplexity in comparison with the word-trigram model [Niesler and Woodland 1996], but it also obtained a 22-fold complexity decrease. Reinhard Kneser [1996] built up variable-context length language models based on North American Business News (NAB - 240 million words of newspaper data) and the German Verbmobil (300,000 words with a vocabulary of 5,000 types). His results show that the variable-length model outperforms conventional models of the same size, and if a moderate loss in performance is acceptable, that the size of a language model can be reduced drastically by using his pruning algorithm. Kneser’s results improve with longer contexts and the same number of parameters. For example, reducing the size of the standard NAB trigram model by a factor of 3 results in a loss of only 7% in perplexity and 3% in the word error rate.  Reduced N-Grams for Chinese Evaluation  21  The improvement obtained by Kneser‘s method depends on the length of the fixed context and on the amount of available training data. In the case of the NAB corpus, the improvement was 10% in perplexity. M. Siu and M. Ostendorf [2000] developed Kneser‘s basic ideas further and applied the variable 4-gram, thus improving the perplexity and word error rate results compared to a fixed trigram model. The obtained word error reductions of 0.1 and 0.5% (absolute) in development and evaluation test sets, respectively, were not statistically significant. However, the number of parameters was reduced by 60%. By using the variable 4-gram, they were able to model a longer history while reducing the size of the model by more than 50%, compared to a regular trigram model, and at the same time improve both the test-set perplexity and recognition performance. They also reduced the size of the model by an additional 8%. Another related work was that of Hu, Turin, Brown [1997].  2.1 The first algorithm [R Kneser 1996]  Variable-length models are determined by the set S of word sequences. If T is the set of all word sequences in the training data with a maximal length of M, then variable-length models can be created by finding a suitable subset S of the set T of all the M-gram sequences in the training data with a given maximal context length M. The distance measure between model PS and model PT is as follows:  ∑ ∑ M −1 D 2 ( PT || PS ) :=  d1(hk , w) ,  (1)  k =0 (hk ,w)∈T \S  where the terms of the sum are defined by the average Kullback Leiber distance  d1 (hk , w)  :=  PT (hk , w)  log  PT (w | hk ) γ T (hk ) PT (w | hk −1 )  ,  (2)  where hk is a phrase history of word w and γ is the normalisation factor. In the implementation, they store the word sequences of S in a tree structure. Each node of the tree corresponds to a word sequence, and each arc is labeled with a word identity. For each node W = (hk , w) ∈ S, Succ(W) is the set of all longer word sequences starting with the same words as W. If a node W is removed, then all Succ(W) will be removed.  Therefore, the average contribution to the sum d2 is  d1 (w) + ∑ d1 (V )  d 2 (W ) =  V∈Succ(W ) . 1+ Succ(W )  (3)  22  Le Quan Ha et al.  The pruning algorithm is as follows: Start with S = T While (|S| > K) For all nodes in S calculate d2 Remove node with lowest d2 2.2 The second algorithm [T R Niesler and P C Woodland 1996] 1. Initialisation: L = -1 2. L = L +1 3. Grow: Add level #L to level #(L-1) by adding all the (L+1)-Grams occurring in the training set for which the L-Grams already exist in the tree. 4. Prune: For every (newly created) leaf in level #L, apply a quality criterion and discard the leaf if it fails. 5. Termination: If there is a nonzero number of leaves remaining in level #L, goto step 2. The quality criterion checks for improvement in the leaving-one-out cross-validation training set likelihood achieved by the addition of each leaf. 2.3 Combination of variable n-grams and other language model types Using the first algorithm, M Siu and M Ostendorf [2000] combined their variable n-gram method with the skipping distance method and class-based method in a study on the Switchboard corpus, consisting of 2 million words. In 1996, using the second algorithm, T R Niesler and P C Woodland developed the variable n-gram based category in a study on LOB, consisting of 1 million English words. In order to obtain an overview of variable n-grams, we combine all of these authors’ results in Table 1. 3. O’ Boyle and Smith’s Reduced N-Gram Algorithm and Application Scope The main goal of this algorithm is to produce three main files from the training text: • The file that contains all the complete n-grams appearing at least m times is called the PHR file (m ≥ 2). • The file that contains all the n-grams appearing as sub-phrases, following the removal of the first word from any other complete n-gram in the PHR file, is called the SUB file.  Reduced N-Grams for Chinese Evaluation  23  Table 1. Comparison of combinations of variable n-grams and other Language Models  COMBINATION OF LANGUAGE MODEL TYPES  Basic Variable Category Skipping Classes #params Perpl- Size  n-gram n-grams  distance  exity  Trigram √  987k  474  1M  Bigram √  -  603.2  Trigram √  -  544.1  √  √  -  534.1  Trigram √  743k  81.5  2M  Trigram √  379k  78.1  Trigram √  √  363k  78.0  Trigram √  √  √  338k  77.7  4-gram √  580k  108  4-gram √  √  577k  108  4-gram √  √  √  536k  107  5-gram √  383k  77.5  5-gram √  √  381k  77.4  5-gram √  √  √  359k  77.2  Source LOB Switch board Corpus  • The file that contains any overlapping n-grams that occur at least m times in the SUB file is called the LOS file. Therefore, the final result is the FIN file of all reduced n-grams, where  FIN := PHR + LOS – SUB.  (4)  Before O’ Boyle and Smith‘s work, Craig used a loop algorithm that was equivalent to FIN := PHR – SUB. This yields negative frequencies for resulting n-grams with overlapping, hence the need for the LOS file. There are 2 additional files: 1. To create the PHR file, a SOR file is needed that contains all the complete n-grams regardless of m (the SOR file is the PHR file in the special case where m = 1). To create the PHR file, words are removed from the right-hand side of each SOR phrase in the SOR file until the resultant phrase appears at least m times (if the phrase already occurs more than m times, no words will be removed).  24  Le Quan Ha et al.  2. To create the LOS file, O’ Boyle and Smith applied a POS file: for any SUB phrase, if one word can be added back on the right-hand side (previously removed when the PHR file was created from the SOR file), then one POS phrase will exist as the added phrase. Thus, if any POS phrase appears at least m times, its original SUB phrase will be an overlapping n-gram in the LOS file. The application scope of O’ Boyle and Smith ‘s reduced n-gram algorithm is limited to small corpora, such as the Brown corpus (American English) of 1 million words [1992], in which the longest phrase has 22 words. Now their algorithm, re-checked by us, still works for medium size and large corpora with training sizes of 100 million word tokens. 4. Reduced N-Grams and Zipf’s Law By re-applying O’Boyle and Smith’s algorithm, we obtained reduced n-grams from the Chinese TREC corpus of the Linguistic Data Consortium1, catalog no. LDC2000T52. TREC was collected from full articles in the People’s Daily Newspaper from 01/1991 to 12/1993 and from Xinhua News Agency articles from 04/1994 to 09/1995. Originally, TREC had 19,546,872 syllable tokens but only 6,300 syllable types. Ha, Sicilia-Garcia, Ming and Smith [2002] proposed an extension of Zipf ’s law and applied it to the TREC syllable corpus. Then in 2003, they produced a compound word version of TREC with 50,000 types, this version was employed in our study for reduced n-gram creation. We will next present the Zipf curves for Chinese reduced n-grams, starting with syllables. 4.1 Chinese syllables The TREC syllable reduced n-grams were created in 28 hours on a Pentium II with 512 MB of RAM and 2 GB of free hard-drive space. The most common TREC syllable reduced unigrams, bigrams, trigrams, 4-grams and 5-grams are shown in Table 3. It can be seen that much noise existed in the unigram frequency observations when only one syllable “年 YEAR” re-appeared in the top ten syllable unigrams [Ha, Sicilia-Garcia, Ming and Smith 2002], listed in Table 2.  
This paper presents a novel approach to ontology alignment and domain ontology extraction from two existing knowledge bases: WordNet and HowNet. These two knowledge bases are automatically aligned to construct a bilingual ontology based on the co-occurrence of words in a bilingual parallel corpus. The bilingual ontology achieves greater structural and semantic information coverage from these two complementary knowledge bases. For domain-specific applications, a domain-specific ontology is further extracted from the bilingual ontology using the island-driven algorithm and domain-specific corpus. Finally, domain-dependent terminology and axioms between domain terminology defined in a medical encyclopedia are integrated into the domain-specific ontology. In addition, a metric based on a similarity measure for ontology evaluation is also proposed. For evaluation purposes, experiments were conducted comparing an automatically constructed ontology with a benchmark ontology constructed by ontology engineers or experts. The experimental results show that the constructed bilingual domain-specific ontology mostly coincided with the benchmark ontology. As for application of this approach to the medical domain, the experimental results show that the proposed approach outperformed the synonym expansion approach to web search. Keywords: Ontology, island driven algorithm, cross language application, WordNet, HowNet  ∗ Department of Computer Science and Information Engineering, National Cheng Kung University, Tainan, Taiwan, ROC E-mail: {jfyeh, chwu, mjchen, lcyu}@csie.ncku.edu.tw  36  Jui-Feng Yeh et al.  1. INTRODUCTION In the past few decades, a considerable number of studies been invested focused on developing concept bases for building technology that allows knowledge reuse and sharing. As information exchangeability and communication becomes increasingly global, multilingual lexical resources that provide transnational services are becoming increasingly important. On the other hand, multi-lingual ontologies are very important for natural language processing, such as machine translation (MT), web mining [Oyama et al. 2004], and cross-language information retrieval (CLIR). Generally, a multi-lingual ontology maps the keywords of one language to another language, or computes the co-occurrence of the words among languages. A key merit of a multilingual ontology is that it can achieve greater relation and structural information coverage by aligning or merging two or more language-dependent ontologies with different semantic features. In recent years, significant effort has focused on constructing ontologies manually according to domain experts’ knowledge. Manual ontology merging using conventional editing tools without intelligent support is difficult, labor intensive, and error prone. Therefore, several systems and frameworks to help knowledge engineers perform ontology merging have recently been proposed [Noy and Musen 2000]. To avoid reiteration in ontology construction, algorithms for ontology merging [UMLS http://umlsks.nlm.nih.gov] [Langkilde and Knight 1998] and ontology alignment [Vossen and Peters 1997] [Weigard and Hoppenbrouwers 1998] [Asanoma 2001] have been investigated. In these approaches, the final ontology is a merged version of the original ontologies with aligned links between them [Daudé et al. 2003]. Alignment is usually performed when ontologies cover domains that are complementary to each other. In the past, a domain ontology was usually constructed manually based on the knowledge or experience of experts or ontology engineers. Recently, automatic and semi-automatic methods have been developed. OntoExtract [Fensel et al. 2002] [Missikoff et al. 2002] provides an ontology engineering chain for constructing a domain ontology from WordNet and SemCor. Some recent approaches have been discussed in [Euzenat et al. 2004]. In [Euzenat et al. 2004], the alignment approaches were classified as local or global methods. Four main local methods, that is, the terminological, extensional, semantics, and structure methods, were introduced to measure the correspondence between two ontologies at the local level. Nowadays, much work is being invested in ontology construction for domain applications. Performing authoritative evaluation of ontologies is becoming a critical issue. Some evaluation methods are integrated into ontology tools to detect and prevent mistakes, which might be made in the course of developing taxonomies with frames as described in [Gómez-Pérez 2001]. They defined three main types of mistakes: inconsistency, incompleteness, and redundancy mistakes. Although the previous research on ontology alignment has achieved much, some  Automated Alignment and Extraction of a Bilingual Ontology for  37  Cross-Language Domain-Specific Applications  important issues still require further investigation: (1) How can we to construct or extract domain concepts from a corpus? (2) Should the alignment of a cross-language or multilingual ontology be performed automatically or semi-automatically? (3) Authoritative assessment of ontology construction is desirable. In this study, the WordNet and HowNet knowledge bases were aligned to construct a bilingual universal ontology based on the co-occurrence of words in a bilingual parallel corpus. For domain-specific applications, the medical domain ontology was further extracted from the universal ontology using the island-driven algorithm and two corpora, one for the medical domain and another for the contrastive domain. Finally, axioms between medical terminology were derived based on a medical encyclopedia. A benchmark ontology based on the Unified Medical Language System (UMLS) and constructed by ontology engineers and experts was used to evaluate the constructed bilingual ontology. This paper also defines two measures, the taxonomic relation and non-taxonomic relation, as quantitative metrics for evaluating ontologies. The rest of the paper is organized as follows. Section 2 describes the ontology construction process. Section 3 presents experimental results for the evaluation of our approach. Section 4 gives some concluding remarks. 2. Ontology Construction Figure 1 shows a block diagram of the ontology construction process. There are two major stages in the proposed approach: bilingual ontology alignment and domain ontology extraction.  2.1 Bilingual Ontology Alignment In this approach, a bilingual ontology is constructed by aligning Chinese words in HowNet with their corresponding synsets defined in WordNet according to the co-occurrence of the words in a bilingual parallel corpus. The hierarchical structure of the ontology is actually a conversion of HowNet. One of the important parts of HowNet consists of definitions of lexical entries. In HowNet, each lexical entry is defined as a combination of one or more primary features and a sequence of secondary features. The primary features indicate the entry’s category, for example, the relation “is-a” in a hierarchical structure. Based on the entry’s category, the secondary features make the entry’s sense more explicit, but they are non-taxonomic. Totally, 1,521 primary features are divided into 6 upper categories: Event, Entity, Attribute Value, Quantity, and Quantity Value. These primary features are organized into a hierarchical structure.  38  Jui-Feng Yeh et al.  Figure 1. Ontology construction framework  In the alignment process, the Sinorama [Sinorama 2001] database, containing over 6,500 documents with 48,000,000 words from 1976 to 2000 in Chinese and English, is adopted as the bilingual parallel corpus. This corpus is then used to compute the conditional probability of the words in WordNet, given the words in HowNet. Then, a bottom up algorithm is used to perform relation mapping. In WordNet, a word may be associated with many synsets, each corresponding to a different sense of the word. To find a relation between two different words, all the synsets associated with each word are considered [Fellbaum 1998]. In HowNet, each word is composed of primary features and secondary features. The primary features indicate the word’s category. The goal of this approach is to increase the amount of relation and structural information coverage by aligning their semantic features in WordNet and HowNet.  Equation (1) shows the alignment between the words in HowNet and the synsets in  WordNet. Given a Chinese word, CWi , the probability of the word being related to synset,  synset k , can be obtained via its corresponding English synonyms,  EW  k j  ,  j  = 1,…, m,  which  Automated Alignment and Extraction of a Bilingual Ontology for  39  Cross-Language Domain-Specific Applications  are the elements in synset k . The probability is estimated as follows:  Pr(synset k  | CWi ) =  m ∑  Pr(synset  k  ,  EW  k j  | CWi )  j =1  (1)  =  m ∑  (Pr( synset  k  |  EW  k j  ,  CWi  )  ×  (Pr(EW  k j  | CWi )) ,  j =1  where  Pr(synset k  |  EW  k j  ,  CWi  )  =  N  (synset  k j  ,  EW  k j  ,  CWi  )  ∑  N  (synset  l j  ,  EW  k j  ,  CWi  )  .  (2)  l  In the above equation,  N  (synset  k j  ,  EW  k j  ,  CWi  )  represents  the  number  of  co-occurrences  of CWi  ,  EW  k j  ,  and  synset  k j  .  The  probability  Pr(EW  k j  | CWi )  is set to one when at least one  of the primary features, PFil (CWi ) , of the Chinese word CWi defined in HowNet matches  one of the ancestor nodes of synset ,  synset  k j  (EW  j  )  ,  except  for  the  root  nodes  in  the  hierarchical structures of the noun and verb. Otherwise, the probability  Pr(EW  k j  | CWi )  is  set to zero:  ⎧ ⎪1, if ⎪  ⎜⎜⎝⎛  ∪ l  PFil  (CWi  )  −  {entity,  event,  act,  play}⎟⎟⎠⎞  ∩  ⎪  ( ) Pr EW j | CWi  =  ⎪ ⎨  ⎪  ⎜⎜⎝⎛  ∪  ancestor(∪ k  synset  k j  (  EW  j  ))  −  {entity,  event,  act  ,  play}⎟⎟⎠⎞  ≠  ∅,  (3)  ⎪⎪0, Otherwise  ⎪⎩  where {entity, event, act, play} is the concept set in the root nodes of HowNet and WordNet,  and  ⎜⎜⎝⎛  ∪ l  PFil  (CWi  )  −  {entity,  event,  act,  play}⎟⎟⎠⎞  represents  all  the  primary  features  of  the  Chinese word C Wi except for {entity, event, act, play}. Finally, the Chinese concept, CWi ,  is  integrated into the synset ,  synset  k j  ,  in  WordNet  as  long  as the probability,  Pr(synset k | CWi ) , is not zero. Figure 2(a) shows the concept tree generated by aligning WordNet and HowNet.  2.2 Domain ontology extraction Now, we will attempt to extend the ontology to domain applications. In domain-specific information retrieval, more detailed definitions and terminology are required. This paper proposes a two-stage domain ontology extraction method. This approach extracts the ontology from the cross-language ontology by using the island-driven algorithm in the first stage. The  40  Jui-Feng Yeh et al.  terminology and axioms defined in a medical encyclopedia are integrated into the domain ontology in the second stage.  Figure 2(a).Concept tree generated by aligning WordNet and HowNet.The nodes in bold circles represent operative nodes following concept extraction.The nodes on gray backgrounds represent operative nodes following relationn expansion.  2.2.1 Extraction using the island-driven algorithm  Generally, an ontology provides consistent concepts and world representations necessary for clear communication within the knowledge domain. Even in domain-specific applications, the number of words can be expected to be huge. Synonym pruning is an effective way to perform word sense disambiguation. This paper proposes a corpus-based statistical approach to extracting a domain ontology. The steps are listed as follows:  Step 1. Linearization: In this step, the tree structure in the general purpose ontology shown in Figure 2(a) is decomposed into a vertex list that is an ordered node sequence starting at the root node and ending at the leaf nodes.  Step 2. Concept extraction from the corpus: The node is defined as an operative node when the  tf-idf value of word Wi in the domain corpus is higher than that in its corresponding contrastive (out-of-domain) corpus. That is,  ⎧1, operative _ node(Wi ) = ⎩⎨0,  if  tf-idfDomain (Wi )  >  tf  −  idfContrastive  (Wi  ) ,  Otherwiae  (4)  where  Automated Alignment and Extraction of a Bilingual Ontology for  41  Cross-Language Domain-Specific Applications  tf  − idf Domain (Wi ) =  freqi,Domain  × log  ni,Domain + ni,Contrastive ni,Domain  ,  tf  − idf Contrastive (Wi ) =  freqi,Contrastive  × log  ni,Domain + ni,Contrastive ni,Contrastive  .  In the above equations, freqi,Domain and freqi,Contrastive are the frequencies of word Wi in the domain documents and its contrastive (out-of-domain) documents, respectively; ni,Domain and ni,Contrastive are the numbers of documents containing word Wi in the domain documents and its contrastive documents, respectively. The nodes shown in bold circles in Figure 2(a) represent operative nodes. Step 3. Relation expansion using the island-driven algorithm: Some domain concepts are no longer operative after the previous steps have been performed due to the problem of data sparseness. According to the analysis performed during ontology construction, most of the inoperative concept nodes have operative hypernym nodes and hyponym nodes. Therefore, the island-driven algorithm is adopted to activate these inoperative concept nodes if their ancestors and descendants are all operative. The nodes shown on gray background in Figure 2(a) are activated operative nodes. Step 4. Domain ontology extraction: In the final step, the linear vertex list sequence is merged into a hierarchical tree. However, some noisy concepts defined as nodes not belonging to this domain are operative according to Equation (5). For example, the node with the concept “solid” shown in Figure 2(b) is an operative noisy concept. Accordingly, the second goal is to filter out the nodes with operative noisy concepts. In this step, noisy concepts without ancestors or descendants belonging to the domain are removed. Finally, the domain ontology is extracted, and the final result is shown in Figure 2(b). 2.2.2 Axiom and terminology integration In practice, specific domain terminology and axioms should be derived and introduced into an ontology for domain-specific applications. There are two approaches to integrating terminology and axioms into an ontology: the first one is manual editing performed by ontology engineers, and the second is automatic integration from a domain encyclopedia. For medical domain applications, 1,213 axioms were derived here from a medical encyclopedia with terminology related to diseases, syndromes, and the clinic information. Figure 3 shows an example of an axiom. In this example, the disease “diabetes” is tagged as level “A,” which means that this disease occurs frequently. The degrees for the corresponding syndromes indicate the causality between the disease and the syndromes. The axioms also provide two fields, “department of the clinical care” and “the category of the disease,” for  42 medical information retrieval or other medical applications.  Jui-Feng Yeh et al.  Figure 2(b). The domain ontology after isolated concepts are filtered out  Occurrence A: frequent B: medium C: infrequent  Degree 1 (Top-1): cardinal symptoms 2 (Top-2): subordinate symptoms 3(Top-3): lowest correlated symptoms  Category of Disease a: an acute disease b: a medium disease c: a chronic disease  A 糖尿病 1 不發燒 3 體力下降 3 口渴  內科  C  A Diabetes  
Main verb identification is the task of automatically identifying the predicate-verb in a sentence. It is useful for many applications in Chinese Natural Language Processing. Although most studies have focused on the model used to identify the main verb, the definition of the main verb should not be overlooked. In our specification design, we have found many complicated issues that still need to be resolved since they haven’t been well discussed in previous works. Thus, the first novel aspect of our work is that we carefully design a specification for annotating the main verb and investigate various complicated cases. We hope this discussion will help to uncover the difficulties involved in this problem. Secondly, we present an approach to realizing main verb identification based on the use of chunk information, which leads to better results than the approach based on part-of-speech. Finally, based on careful observation of the studied corpus, we propose new local and contextual features for main verb identification. According to our specification, we annotate a corpus and then use a Support Vector Machine (SVM) to integrate all the features we propose. Our model, which was trained on our annotated corpus, achieved a promising F score of 92.8%. Furthermore, we show that main verb identification can improve the performance of the Chinese Sentence Breaker, one of the applications of main verb identification, by 2.4%. Keywords: Chinese Main Verb Identification, Text Analysis, Natural Language Processing, SVM  
We present a new approach to aligning sentences in bilingual parallel corpora based on punctuation, especially for English and Chinese. Although the length-based approach produces high accuracy rates of sentence alignment for clean parallel corpora written in two Western languages, such as French-English or German-English, it does not work as well for parallel corpora that are noisy or written in two disparate languages such as Chinese-English. It is possible to use cognates on top of the length-based approach to increase the alignment accuracy. However, cognates do not exist between two disparate languages, which limit the applicability of the cognate-based approach. In this paper, we examine the feasibility of exploiting the statistically ordered matching of punctuation marks in two languages to achieve high accuracy sentence alignment. We have experimented with an implementation of the proposed method on parallel corpora, the Chinese-English Sinorama Magazine Corpus and Scientific American Magazine articles, with satisfactory results. Compared with the length-based method, the proposed method exhibits better precision rates based on our experimental reuslts. Highly promising improvement was observed when both the punctuation-based and length-based methods were adopted within a common statistical framework. We also demonstrate that the method can be applied to other language pairs, such as English-Japanese, with minimal additional effort. Keywords: Sentence Alignment, Cognate Alignment, Machine Translation  ∗ Department of Computer Science, Vanung University, No. 1 Van-Nung Road, Chung-Li Tao-Yuan, Taiwan, ROC E-mail: tomchuang@cc.vit.edu.tw + Department of Computer Science, National Tsing Hua University, 101, Kuangfu Road, Hsinchu, 300, Taiwan, ROC  96  Thomas C. Chuang and Kevin C. Yeh  1. Introduction Bilingual corpora are very important for building natural language processing systems [Moore 2002; Gey et al. 2002], including data-driven machine translation [Dolan et al. 2002], computer-assisted revision of translations [Jutras 2000], and cross-language information retrieval [Chen and Gey 2001]. In order to develop NLP systems, it is useful to align bilingual corpora at the sentence level with very high precision [Moore 2002; Chuang et al. 2002, Kueng and Su 2002]. With aligned sentences, further analysis such as phrase and word alignment analysis [Ker and Chang 1997; Melamed 1997], bilingual terminology [Déjean et al. 2002] and collocation [Wu 200] extraction analysis can be performed. Yang, C, Li, K. [2003] proposed an alignment method for bilingual title pairs on the Web for automatic generation of bilingual parallel corpora. The hybrid dictionary approach [Collier et al. 1998], text-based alignment [Kay and Röscheisen 1993], part of speech-based alignment [Chen and Chen 1994], and the lexical method [Chen 1993] are other examples of sentence alignment methods. While these methods presume little or no prior knowledge of source and target languages, they are relatively complex and require significant amounts of parallel text and language resources. Much work reported in the computational linguistics literature has focused on aligning English-French and English-German sentences. While the length-based approach [Gale and Church 1993; Brown et al. 1991] to sentence alignment produces surprisingly good results for French and English with success rates well over 96%, it does not work well for the alignment of English and Chinese sentences. Simard, Foster, and Isabelle [1992] proposed using cognates on top of the length-based approach to improve the alignment accuracy. They use an operational definition of cognates, which include digits, alphanumerical symbols, punctuation, and alphabetical words. Several other measures of cognateness have also been suggested [Melamed 1999; Danielsson and Muhlenbock 2000; Ribeiro et al. 2001], but none of them are sufficiently reliable, and all of them are tailored to close Western language pairs. Simard, Foster, and Isabelle [1992] pointed out that cognates in two close languages, such as English and French, can be used to measure the likelihood of mutual translation. Those cognates include alphabetic words, numeric expressions, and punctuation that are almost identical and readily recognizable by computers. However, for disparate language pairs, such as Chinese and English, that lack a shared Roman alphabet, it is not possible to rely on such cognates to achieve high-precision sentence alignment of noisy parallel corpora. Research on sentence alignment of English and Chinese texts [Wu 1994], indicates that the lengths of English and Chinese texts are not as highly correlated as they are with French and English, leading to lower precision rates (86.4-95.2%) for length based aligners. A comparison of the correlation between German-English and Chinese-English bilingual corpora is depicted in Figure 1, where 138 German- English and 151 Chinese-English aligned  Aligning Parallel Bilingual Corpora Statistically with Punctuation Criteria 97  sentences are analyzed. The correlations are 0.99 and 0.95 for the German-English and Chinese-English cases, respectively. The expected ratios and the corresponding standard deviations are (0.92, 0.1124) and (4.614, 0.84) for the German-English and Chinese-English cases, respectively.  Translation Paragraph Length  1800 1600 1400 1200 1000 800 600 400 200 0 0  German Chinese  500  1000  1500  English Paragraph Length  Figure 1. The relationships between German-English [Gale and Church 1993] and Chinese-English bilingual paragraph lengths [Chuang et al. 2002]. The correlations are 0.99 and 0.95 for the German-English and Chinese-English cases, respectively. The expected ratios and the corresponding standard deviations are (0.92, 0.1124) and (4.614, 0.84) for the German-English and Chinese-English cases, respectively.  Furthermore, for English-Chinese alignment tasks, no orthographic, phonetic, or semantic cognates, that are readily recognizable by computer exist. Therefore, the inexpensive cognate-based approach is not applicable to Chinese-English tasks. We are thus motivated to find alternative evidence that two blocks of texts are mutual translations. It turns out that punctuation can be telling evidence, if we do more than hard matching of punctuation and take into consideration intrinsic sequencing and the statistical distribution of punctuation in ordered comparison. What is attractive about this approach is that it easily leads to sub-sentential alignment, which has been shown to be useful for statistical machine translation.  98  Thomas C. Chuang and Kevin C. Yeh  Section 2 of this paper, we provide some information about the similarity in the use of punctuation in Chinese and English literature and also the differences. Our conclusion is that using punctuation as cognates to align disparate parallel texts will fail to provide adequate alignment results. Section 3, we define a punctuation compatibility factor as an indicator of mutual translation. A translation model that employs a punctuation probability function is proposed. In Section 4, we present experiments based on our novel approach of using the statistical properties of punctuation in parallel texts being analyzed to perform bilingual sentence alignment. We demonstrate that one can use punctuation alone to develop a high-precision sentence alignment program for distant parallel texts like those in Chinese-English corpora. Additionally, we examine the performance of sentence alignment by using punctuation in combination with length. In Section 5, we demonstrate that the proposed method is a very cost effective approach that can be effectively applied to other disparate bilingual languages like English-Japanese without a priori language knowledge of them. A brief conclusion is provided in Section 6. 2. Punctuation across languages According to the Longman Dictionary of Applied-Linguistics [Richards et al. 1985], a cognate is “a word in one language which is similar in form and meaning to a word in another language because both languages are related.” Although the ways in which different languages around the world use punctuation vary, symbols such as commas and full stops are used in most languages to demarcate writing, while question and exclamation marks are used to show interrogation and emphasis. However, these forms of punctuation can often look different or be used in different ways. The traditional Chinese writing system does not have punctuation, and it is up to the reader to demarcate the text while reading. With the influx of Western culture in the eighteenth century, punctuation systems similar to the one used with Roman script was adopted in China and Japan. The punctuation includes the period, comma, colon, dash, etc. Although most of those forms of punctuation look similar to Roman ones, they are usually coded as double-bytes and tend to be used differently. The full stop in Chinese and Japanese is a small empty circle, quite different in appearance from the Roman period. Quotes are also very different, shaped like a Greek letter Γ, upright or upside down. There are forms of punctuation that have no counterparts in Roman text. For instance, “、” is the pause symbol, which is used somewhat like the comma but only when separating items in a list. On the other hand, there are several uses of the Roman comma which do not occur in Chinese texts. A few examples are given below:  Aligning Parallel Bilingual Corpora Statistically with Punctuation Criteria 99 (Parenthetical expressions) (1e) Evolution, as far as we know, doesn’t work this way. (1c) 我們所知道的進化論不是如此的。 (Appositives) (2e) His father, Tom, is a well-known scholar. (2c) 他的父親湯姆是一位有名的學者。 Yang [1981] described more punctuation marks in Chinese used in various ways that are similar or dissimilar to English punctuation. In summary, although Simard et al. [1992] considered the various forms of punctuation in English and French to be cognates, in general, punctuation forms are not cognates for many other language pairs. In both Chinese and English texts, the average ratio of the punctuation count to the total number of tokens available is low (less than 15%). But punctuation provides valid additional evidence, which can help one achieve a high degree of alignment precision. Our method can easily be generalized to other language pairs since minimal a priori linguistic knowledge is required. 3. Punctuation and Sentence Alignment 3.1 Punctuation Marks in English and Chinese In this section, we will describe how punctuation in two languages can be used to measure the likelihood of mutual translation in sentence alignment. We will use an example in the following to illustrate the method. A formal description also follows: Example 3 shows a Chinese sentence and its translation counterpart of two English sentences in a parallel corpus. (3c) 逐漸的，打鼓不再能滿足他，「打鼓原是我最喜歡的，後來卻變成邊打邊睡，一個 月六萬元的死工作」，薛岳表示。 (3e) Over time, drums could no longer satisfy him. "Drumming was at first the thing I loved most, but later it became half drumming, half sleeping, just a job for NT$60,000 a month," says Simon.  100  Thomas C. Chuang and Kevin C. Yeh  If we keep punctuations in the above examples in the original order and strip everything else out, we have ten pieces of punctuation from the English part (3e) and eight from the Mandarin part (3c) as follows:  (4c) ， ， 「 ，  ， 」 ， 。  (4e) ,  .  "  ,  ,  ,  ,  "  .  They can be arranged into different match types as shown below.  Match type (4c) (4e)  1-1  ，  ,  1-1  ，  .  1-1  「  "  1-1  ，  ,  0-1  ,  1-1  ，  ,  2-2  」， , "  1-1  。  .  Figure 2. The correspondence between two punctuation strings There are several frequently used punctuation forms in Chinese text that are not available in English text, for example, the punctuation forms "、" and "。". These punctuation forms often correspond to the English punctuation forms "," and ".", respectively. It is not difficult to see that the two punctuation strings above match up quite nicely, indicating that the corresponding texts are mutual translations. Roughly, the first two commas in Chinese correspond to the first two English punctuation marks (comma and period), while the Chinese open quote in the third position corresponds to the English open quote also in the third position. The two Chinese commas inside the quotes correspond to two of the four commas within the quotes in English. The two consecutive marks (」，) correspond to (,"), forming a 2-2 match. These correspondences can be unraveled via a dynamic programming procedure, much like sentence alignment. See Figure 2 for more details.  Aligning Parallel Bilingual Corpora Statistically with Punctuation Criteria 101  It is apparent that the punctuation in the two strings match up very consistently, and that the matching is somewhat continuous with respect to the alignment of regular words surrounding the punctuation (see the double-lined links in Figure 3 for details). Therefore, the example gives a convincing indication that the correspondence between punctuation across two languages can provide telling evidence that two texts are mutual translations.  3.2 Punctuation marks as Good Indicators of Mutual Translation  Based on our initial observation, the portion of the identifiable punctuation matches between two parallel texts in Chinese and English is over 50%. Examining Figure 2, we can identify institutively the matches between the Chinese punctuation and the equivalent English punctuation marks: (「) corresponds to (“), etc. This implies that although direct match information is useful, there is still a large discrepancy in the punctuation mappings between Chinese and English. We, therefore, define here a punctuation compatibility factor that can be used to further analyze the relationship between the punctuation found in parallel texts. The punctuation compatibility factor as an indicator of mutual translation is defined as  γ= c ,  (1)  max(n, m)  where  γ = the punctuation compatibility factor, c = the number of direct punctuation matches, n = the number of Chinese punctuation marks, m = the number of English punctuation marks.  We took aligned English-Chinese sentences that had the same punctuation count (which is the denominator of Equation 1), take ten for example, in order to determine how well punctuation works as an indicator of mutual translation of English and Chinese sentences. We also took the same English sentences and matched them up with randomly selected Chinese sentences to calculate the compatibility of punctuation marks in unrelated texts.  The results obtained indicated that the average compatibility of pairs of sentences, which were mutual translations, was about 0.67 (with a standard deviation of 0.170), while the average compatibility of random pairs of bilingual sentences was 0.34 (with a standard deviation of 0.167).  102  Thomas C. Chuang and Kevin C. Yeh  Figure 3. English punctuation across aligned sentences  Aligning Parallel Bilingual Corpora Statistically with Punctuation Criteria 103  Figures 4 through 6 show the compatibility results based on punctuation counts of eight, ten and twelve respectively. These graphs were constructed by analyzing around 50,000 aligned sentences found in the Sinorama Magazine (1990-2000). 521, 259, and 143 sentences were selected to obtain values of n and m equal to 8, 10, and 12, respectively. The solid lines simply connect data points for easier observation.  Probability  0.45  Random match  0.4  Mutual translation  0.35  0.3  0.25  0.2  0.15  0.1  0.05  0  0  0.2  0.4  0.6  0.8  
Collocation extraction systems based on pure statistical methods suffer from two major problems. The first problem is their relatively low precision and recall rates. The second problem is their difficulty in dealing with sparse collocations. In order to improve performance, both statistical and lexicographic approaches should be considered. This paper presents a new method to extract synonymous collocations using semantic information. The semantic information is obtained by calculating similarities from HowNet. We have successfully extracted synonymous collocations which normally cannot be extracted using lexical statistics. Our evaluation conducted on a 60MB tagged corpus shows that we can extract synonymous collocations that occur with very low frequency and that the improvement in the recall rate is close to 100%. In addition, compared with a collocation extraction system based on the Xtract system for English, our algorithm can improve the precision rate by about 44%. Keywords: Lexical Statistics, Synonymous Collocations, Similarity, Semantic Information 1. Introduction A collocation refers to the conventional use of two or more adjacent or distant words which hold syntactic and semantic relations. For example, the conventional expressions “warm greetings”, “broad daylight”, “思想包袱”, and “托运行李” all are collocations. Collocations bear certain properties that have been used to develop feasible methods to extract them automatically from running text. Since collocations are commonly found, they must be recurrent. Therefore, their appearance in running text should be statistically significant, making it feasible to extract them using the statistical approach.  ∗ Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong Tel: +852-27667326; +852-27667247 Fax:+852-27740842 E-mail: {cswyli, csluqin, csrfxu}@comp.polyu.edu.hk  124  Wanyin Li et al.  A collocation extraction system normally starts with a so-called headword (sometimes also called a keyword) and proceeds to find co-occurring words called the collocated words. For example, given the headword “基本”, such bi-gram collocations as “基本理论”, “基本工 作”, and, “基本原因” can be found using an extraction system where “理论”, “工作”, and “原 因” are called collocated words with respect to the headword “基本.” Many collocation extraction algorithms and systems are based on lexical statistics [Church and Hanks 1990; Smadja 1993; Choueka 1993; Lin 1998]. As the lexical statistical approach was developed based on the recurrence property of collocations, only collocations with reasonably good recurrence can be extracted. Collocations with low occurrence frequency cannot be extracted, thus affecting both the recall rate and precision rate. The precision rate achieved using the lexical statistics approach can reach around 60% if both word bi-gram extraction and n-gram extraction are employed [Smadja 1993; Lin 1997; Lu et al. 2003]. The low precision rate is mainly due to the low precision rate of word bi-gram extraction as only about a 30% - 40% precision rate can be achieved for word bi-grams. The semantic information is largely ignored by statistics- based collocation extraction systems even though there exist multiple resources for lexical semantic knowledge, such as WordNet [Miller 98] and HowNet [Dong and Dong 99]. In many collocations, the headword and its collocated words hold specific semantic relations, hence allowing collocate substitutability. The substitutability property provides the possibility of extracting collocations by finding synonyms of headwords and collocate words. Based on the above properties of collocations, this paper presents a new method that uses synonymous relationships to extract synonym word bi-gram collocations. The objective is to make use of synonym relations to extract synonym collocations, thus increasing the recall rate. Lin [Lin 1997] proposed a distributional hypothesis which says that if two words have similar sets of collocations, then they are probably similar. According to one definition [Miller 1992], two expressions are synonymous in a context C if the substitution of one for the other in C does not change the truth-value of a sentence in which the substitution is made. Similarly, in HowNet, Liu Qun [Liu et al. 2002] defined word similarity as two words that can substitute for each other in a context and keep the sentence consistent in syntax and semantic structure. This means, naturally, that two similar words are very close to each other and they can be used in place of each other in certain contexts. For example, we may either say “买书”or “订书” since “买” and “订” are semantically close to each other when used in the context of buying books. We can apply this lexical phenomena after a lexical statistics-based extractor is applied to find low frequency synonymous collocations, thus increasing the recall rate. The rest of this paper is organized as follows. Section 2 describes related existing collocation extraction techniques that are based on both lexical statistics and synonymous collocation. Section 3 describes our approach to collocation extraction. Section 4 describes the  Similarity Based Chinese Synonym Collocation Extraction  125  data set and evaluation method. Section 5 evaluates the proposed method. Section 6 presents our conclusions and possible future work. 2. Related Works Methods have been proposed to extract collocations based on lexical statistics. Choueka [Choueka 1993] applied quantitative selection criteria based on a frequency threshold to extract adjacent n-grams (including bi-grams). Church and Hanks [Church and Hanks 1990] employed mutual information to extract both adjacent and distant bi-grams that tend to co-occur within a fixed-size window. However, the method can not be extended to extract n-grams. Smadja [Smadja 1993] proposed a statistical model that measures the spread of the distribution of co-occurring pairs of words with higher strength. This method can successfully extract both adjacent and distant bi-grams, and n-grams. However, it can not extract bi-grams with lower frequency. The precision rate of bi-grams collocation is very low, only around 30%. Generally speaking, it is difficult to measure the recall rate in collocation extraction (there are almost no reports on recall estimation) even though it is understood that low occurrence collocations cannot be extracted. Sun [Sun 1997] performed a preliminary Quantitative analysis of the strength, spread and peak of Chinese collocation extraction using different statistical functions. That study suggested that the statistical model is very limited and that syntax structures can perhaps be used to help identify pseudo collocations. Our research group has further applied the Xtract system to Chinese [Lu et al. 2003] by adjusting the parameters so at to optimize the algorithm for Chinese and developed a new weighted algorithm based on mutual information to acquire word bi-grams which are constructed with one higher frequency word and one lower frequency word. This method has achieved an estimated 5% improvement in the recall rate and a 15% improvement in the precision rate compared with the Xtract system. A method proposed by Lin [Lin 1998] applies a dependency parser for information extraction to collocation extraction, where a collocation is defined as a dependency triple which specifies the type of relationship between a word and the modifiee. This method collects dependency statistics over a parsed collocation corpus to cover the syntactic patterns of bi-gram collocations. Since it is statistically based, therefore it still is unable to extract bi-gram collocations with lower frequency. Based on the availability of collocation dictionaries and semantic relations of words combinatorial possibilities, such as those in WordNet and HowNet, some researches have made a wide range of lexical resources, especially synonym information. Pearce [Pearce 2001] presented a collocation extraction technique that relies on a mapping from one word to its synonyms for each of its senses. The underlying intuition is that if the difference between the occurrence counts of a synonym pair with respect to a particular word is at least two, then they  126  Wanyin Li et al.  can be considered a collocation. To apply this approach, knowledge of word (concept) semantics and relations with other words must be available, such as that provided by WordNet. Dagan [Dagan 1997] applied a similarity-based smoothing method to solve the problem of data sparseness in statistical natural language processing. Experiments conducted in his later research showed that this method could achieve much better results than back-off smoothing methods in terms of word sense disambiguation. Similarly, Hua [Wu 2003] applied synonyms relationships between two different languages to automatically acquire English synonymous collocations. This was the first time that the concept of synonymous collocations was proposed. A side intuition raised here is that a natural language is full of synonymous collocations. As many of them have low occurrence rates, they can not be retrieved by using lexical statistical methods. HowNet, developed by Dong et al. [Dong and Dong 1999] is the best publicly available resource for Chinese semantics. Since semantic similarities of words are employed, synonyms can be defined by the closeness of their related concepts and this closeness can be calculated. In Section 3, we will present our method for extracting synonyms from HowNet and using synonym relations to further extract collocations. While a Chinese synonym dictionary, Tong Yi Ci Lin 《( 同义辞林》), is available in electronic form, it lacks structured knowledge, and the synonyms listed in it are too loosely defined and are not applicable to collocation extraction. 3. Our Approach Our method to extract Chinese collocations consists of three steps. Step 1: We first take the output of any lexical statistical algorithm that extracts word bi-gram collocations. This data is then sorted according to each headword, wh, along with its collocated word, wc. Step 2: For each headword, wh, used to extract bi-grams, we acquire its synonyms based on a similarity function using HowNet. Any word in HowNet having a similarity value exceeding a threshold is considered a synonym headword, ws, for additional extractions. Step 3: For each synonym headword, ws, and the collocated word, wc, of wh, if the bi-gram (ws, wc) is not in the output of the lexical statistical algorithm applied in Step 1, then we take this bi-gram (ws, wc) as a collocation if the pair appears in the corpus by applying an additional search on the corpus. 3.1 Bi-gram Collocation Extraction In order to extract Chinese collocations from a corpus and to obtain result in Step 1 of our algorithm, we use an automatic collocation extraction system named CXtract, developed by a research group at Hong Kong Polytechnic University [Lu et al. 2003]. This collocation  Similarity Based Chinese Synonym Collocation Extraction  127  extraction system is based on English Xtract [Smaja 1993] with two improvements. First, the  parameters (K0, K1, U0) used in Xtract are adjusted so as to optimize them for a Chinese  collocation extraction system, resulting in an 8% improvement in the precision rate. Secondly,  a solution is provided to the so-called high-low problem in Xtract, where bi-grams with a high  frequency the head word, wh, but a relatively low frequency collocated word, wi can not be  extracted. We will explain the algorithm briefly here. According to Xtract, a word concurrence  is denoted by a triplet (wh, wi, d), where wh is a given headword and wi is a collocated word  appeared in the corpus with a distance d within the window [-5, 5]. The frequency, fi , of the  collocated word, wi , in the window [-5, 5] is defined as  5  fi = ∑ fi, j  (1)  j = −5  where fi, j is the frequency of the collocated word wi at position j in the corpus within the window.  The average frequency of fi, denoted by f i , is given by  5  fi = ∑ fi, j /10 .  (2)  j = −5  Then, the average frequency, f , and the standard deviation, σ, are defined as  f  =  
Automatic Drills Selection  No  End  Evaluation  Yes  Personalized Error Rules  Speech Recorder  Evaluation Stage  Sentence Verification  Yes Reject No  General Error Rules Pronunciation Network Generation  Error Patterns Update Error Patterns＇ Probability  Similar Error Rules＇ Probability Estimation  Pronunciation proccessor  Course Management  Pronunciation Network Generation  Pronunciation proccessor  Practice Stage  Visual Feedback  NO Yes  End Practice  圖一：系統流程  Update Error Rules＇ Count  3.1 發音錯誤類型評估 這個部份的目的主要是要找出使用者常犯的發音錯誤類型。首先為了避免與標準語音內容差異過 大，先針對輸入的語音做內容的驗證。本論文利用 log-posterior score 來對整句語音訊號做可信度 分析，若分數小於門檻值則拒絕此語音的輸入。在發音錯誤類型偵測的部份，主要是透過語音辨 識的方式，根據人工標記所找出來的發音錯誤規則將所有可能的發音(包含正確發音與錯誤發音) 建立成對應的辨識網路，利用這樣的辨識網路來偵測發音錯誤的類型。由於我們希望能以較少的 測試句來找出使用者個人的發音錯誤類型，利用計算句子的 entropy 與句子中還需納入考慮音素 佔句子的比例來當做句子計分的準則。根據每一次的測試句所辨識出來的結果，我們可以計算出 已測試過發音規則的發生機率，然而當測試語料量較少時，尚未出現在測試語料中之發音規則其 機率則利用其它相關性較高的發音規則的機率來估計。每經過一次句子的測試，就需針對尚未被 念過的句子重新計算其分數，然後挑選分數最大的句子當作下一次的測試句，直到每個音素的機 率分布變化量小於我們所設定的門檻值時，即停止測試產生出個人的發音錯誤類型。 3.2 發音練習與視覺回饋 根據找出來的個人發音錯誤類型，我們挑選包含較多使用者常犯的發音錯誤的句子來讓使用者練 習。在視覺回饋方面，本論文運用影像處理及 3D 動畫的合成，建立一 3D 虛擬人形動畫系統， 且特別針對發音時之唇型及舌位，給予使用者正確的發音動作。運用 3D 虛擬人形動畫系統除了 可增加趣味外，使用者也可以經由不同的角度來觀察唇型與舌位的變化。  4. 發音內容驗證 4.1 聲學模型 4.1.1 語料 針對母語為英語的聲學模型，我們使用 TIMIT 語料來訓練聲學模型。TIMIT 內容共 6300 句，由 來自美國八個主要口音地區中的 438 位男性、192 位女性所錄製，每人錄製 10 句。我們以 TIMIT 建議的 4620 句做為訓練語料(語料總容量為 440 Megabytes、所有語料長度總和約為 3 小時 49 分 10 秒)來訓練母語為英語的聲學模型。由於本系統是針對母語為中文之使用者，因此我們也同時 找了五位英語發音較佳的台灣人，錄製了一套台灣人口音的英語語料，來進行語者的調適。語料 內容共 600 句，由 3 位男性、2 位女性大學生所錄製，每人錄製 120 句。 4.1.2 特徵參數擷取 要訓練聲學模型前必須先將訓練資料經過特徵參數的擷取。因此，對於處理語音這種高度差異的 訊號時，需要找到能夠具有鑑別度特徵，這裡我們使用三十九維的梅爾倒頻譜參數(Mel-Frequency Cepstral Coefficients, MFCC)，包含有十二階的頻譜值加上一階能量值，並取一階微分和二階微 分。圖二為特徵參數的擷取流程圖：  語音訊號  MFCC 參數  音框化  三角通 濾波器  漢明窗  傅利葉 轉換  圖二：特徵參數的擷取  4.1.3 聲學模型的建立 英文的一個音節是由一或多個音標所組成，每個音標均對應一種發音。TIMIT 語料定義了 62 個 聲學模型，然而由於訓練語料的不足以及台灣人發音上準確度較低的情況下，我們不考慮同一個 音標在不同位置下的重音情形。因此，我們定義了 42 個聲學模型，包含 40 個 monophones、1 個 silence model 與 1 個 short pause model。我們使用 HTK[10]來訓練聲學模型，表一為我們所定 義的 40 個發音模型。  表 1：聲學模型與 KK 音標對照表  由於直接拿 TIMIT 訓練的聲學模型來辨識臺灣人口音的英文句，其辨識率便會有所下降，因此  必須針對使用者的母語經過適當的調整。我們先使用 TIMIT 語料訓練初始的聲學模型，之後使  用自行錄製的臺灣人口音的英文句，利用 MLLR (Maximum Likelihood Linear Regression)[11]調整  使用 TIMIT 訓練出來的聲學模型。  4.2 語音內容驗證  在偵測使用者發音錯誤類型之前，我們希望使用者的語音內容與標準語音差異不致於過大，所以  必須先針對使用者的語音做一個驗證的動作。我們的驗證機制主要是利用訓練好的 HMM  Model，在已知使用者發音內容的情況下，做可信度的分析。  4.2.1 驗證機制  我們參考了兩個可信度分析的方法來建立本系統的驗證機制。其一是使用 LLR(log-likelihood  ratio)[12]，然而此方法需要同時訓練 native speaker 與 non-native speaker 的聲學模型，因此需要  有較大量的 non-native 語料。現階段受限於收集的台灣人口音語料的不足，於是我們使用  log-posterior probability score [13]來對整句語音做評分。假設 yt 及 qi 分別代表輸入語句中第 t 個 frame 的語音參數及其所對應的第 i 個音素。則事後機率 P (qi | yt ) 的計算如下式(1)：  ( ) ( ) P (qi | yt ) =  P ( yt | qi ) P (qi ) M  .  ∑ P yt | q j P q j  (1)  j =1  ( ) 假設所有 model 出現的機率均相等即 P (qi ) = P q j ，因此上式(1)可近似為下式(2)：  ∑ ( ) P (qi | yt ) =  P ( yt | qi ) M  ,  P yt | q j  (2)  j =1  第 i 個音素之 log-posterior probability score ρi 便是計算此音素中所有對應 frame 之 log-posterior probability 平均：  ∑ ( ) ρi  =  
Polarity inversion based speech watermarking scheme hide data in speech by modiﬁcation of the speech polarity. This paper build a statistical model of the polarity detection problem, based on this model, the original polarity detection scheme and the optimal detection scheme are analyzed and compared. The theoretical analysis results are validated by Monte Carlo simulation, the optimal polarity detector shows signiﬁcant performance gain compared with the original polarity detection algorithm.  
This paper aims to further probe into the problems of ambiguities in automatic identification of determinative-measure compounds (DMs) in Chinese. It is known that Chinese DMs are identifiable by regular expression rules. However, rule matching only partially solve structural and lexical ambiguities. In this paper, a deep analyses based on corpus data was studied. With the subtle analyses of error identification and disambiguation of DM compounds, we classified three types of ambiguities, i.e. structural, sense, and functional ambiguities. We also proposed resolution principles to eliminate the problems and thus to improve word segmentation and POS (Part-Of-Speech) tagging. 
*Dep. Of CSIE, Chaoyang University of Technology, Taichung County, Taiwan, R.O.C shwu@cyut.edu.tw Abstract Recently, shallow parsing has been applied to various information processing systems, such as information retrieval, information extraction, question answering, and automatic document summarization. A shallow parser is suitable for online applications, because it is much more efficient and less demanding than a full parser. In this research, we formulate shallow parsing as a sequential tagging problem and use a supervised machine learning technique, Maximum Entropy (ME), to build a Chinese shallow parser. The major features of the ME-based shallow parser are POSs and the context words in a sentence. We adopt the shallow parsing results of Sinica Treebank as our standard, and select 30,000 and 10,000 sentences from Sinica Treebank as the training set and test set respectively. We then test the robustness of the shallow parser with noisy data. The experiment results show that the proposed shallow parser is quite robust for sentences with unknown proper nouns. 1. Introduction Parsing is a basic technique in natural language processing; however, a full parser is usually costly and slow. Recently, shallow parsing has been applied to various information processing systems [12]. Compared to the performance of full parsers, a shallow parser is much faster and the parsing result is more useful for various applications, such as information retrieval and extraction, question answering, and automatic document summarization. In this paper, we adopt a machine learning approach to the Chinese shallow parsing problem. Chinese full parsing is very challenging,[18, 22] because it is difficult to achieve high accuracy, and the performance is not suitable for online applications. Shallow parsing of Chinese, on the other hand, is promising and desirable in terms of efficiency. Researchers in Beijing, Harbin, Shenyang, and Hong Kong have also developed related techniques [10, 15, 16, 20, 21]. Most of these works use machine learning approaches, instead of the rule-based approach used in full parsing. Popular machine learning methods such as SVM, CRF, and ME, have been tested. The parsing speed of each approach is fast and the parsing accuracy is acceptable.  Currently, there is no standard for Chinese shallow parsing. Li et. al. [9] developed a Chinese shallow parsed treebank to extract Chinese collocations automatically and built a large collocation bank. There are also some works on a standard for Chinese shallow parsing [9, 19, 20]. Nevertheless, the POS standard and vocabulary in each approach are different; thus, between simplified Chinese and traditional Chinese, we cannot adopt their standard for simplified Chinese to traditional Chinese. Instead, we use the first level of the parsing results of Sinica Treebank as our shallow parsing standard [4]. Originally, Sinica Treebank was designed to provide full parsing results, whereby sentences could be labeled with POS tags and the full parsing structure. There are 54,000 sentences in Sinica Treebank, from which we randomly selected 30,000 and 10,000 sentences as the training set and test set respectively. Since there are many unknown words in Chinese [11], a Chinese shallow parser must be robust against such words [22]. For example, it is not hard to correctly chunk the sentence “高漸離/擊筑/的/ 音調/忽然/急轉成/悲壯” into “高漸離擊筑的音調/NP 忽然/Dd 急轉成/DM 悲壯/VP”, if we know that “高漸離” is a proper noun. However, if the name is unknown, it could be split into three single characters and tagged with the three POS of the single characters, i.e., “高/漸/離 [VH13/Dd/P15]”. It might then be incorrectly chunked as ”高漸/NP 離擊筑的音調/PP 忽然/Dd 急轉成/DM 悲壯/VP”. In this research, we simulate unknown words by adding some noises to the corpus in order to test the robustness of the shallow parser. Since new proper nouns are normally unknown, we design three ways to add noises to the training and testing sets by treating proper nouns as unknown words.  2. Shallow Parsing Standard  Sinica Treebank provides a full parse tree for each sentence. Here, we use the first-layer parsing results of Sinica Treebank as the standard for shallow parsing. Instead of using all the phrase tags in Sinica Treebank, we annotate five of them for chunking; all other phrases (including single words not in any phrase) are tagged as others (X). The five tags, namely, noun phrase (NP), verb phrase (VP), preposition phrase (PP), geographic phrase (GP), and clause (S), are the major tags in Sinica Treebank, and therefore play significant syntactical roles. Thus, the constituents of the root node of a parse tree are NP, VP, PP, GP, S, and X. Table 1 lists examples of the six types of constituent.  Table 1. Chunk Tags  Chunk Tag NP VP PP GP S X  Description Noun Phrase Verb Phrase Preposition Phrase Geographic Phrase Clause Others  Example 前十名 / 的 / 選手 [DM / DE / Nab] 傳遞 / 區運 / 聖火 [VD1 / Nad / Nac] 在 / 旅客 / 口 / 中 [P21 / Nab / Nab / Ncda] 一個 / 星期 / 以後 [DM / Nac / Ng] 窗戶 / 玻璃 / 破掉 [Nab / Nab / VH11] ０ / 到 / ２度 [DM / Caa / DM]  3. A Maximum Entropy-based Shallow Parser  Parsing is a fundamental technique in natural language processing, the results of which can be used to improve various natural language tasks, such as word-sense disambiguation (WSD) [3] and part-of-speech (POS) tagging [12]. Many natural language processing tasks, such as part-of-speech tagging, named-entity recognition, and shallow parsing, can be viewed as sequence analysis tasks. Shallow parsing identifies the non-recursive core of each phrase type in a text as a precursor to full parsing or information extraction [1, 6]. The paradigmatic shallow parsing problem is called NP chunking, which finds the non-recursive cores of noun phrases called base NPs. Ramshaw and Marcus introduced NP chunking as a machine-learning problem [14]. Machine learning techniques, such as maximum entropy (ME) and conditional random fields (CRF), are quite popular for sequential tagging. We adopt ME to build a robust Chinese shallow parser.  3.1 The B-I-O Scheme of Our Shallow Parser  In this work, we regard each word as a token, and consider a test corpus and a set of n phrase categories. Since a phrase can have more than one token, we associate two tags, x: x_begin and x_continue, with each category. In addition, we use the tag others to indicate that a token is not part of a phrase. The shallow parsing problem can then be redefined as a problem of assigning one of 2n + 1 tags to each token. This is called the B-I-O scheme. There are 5 named entity categories and 11 tags: NP_begin, NP_continue, VP_begin, VP_continue, PP_begin, PP_continue, GP_begin, GP_continue, S_begin, S_continue, and X(others).  3.2 Maximum Entropy Formula  ME is a flexible statistical model that assigns an outcome to each token based on its history and  features [2]. The outcome space is comprised of the tags for an ME formulation. ME computes the  probability p(o|h) for any o from the space of all possible outcomes, O, and for every h from the space  of all possible histories, H. A history is composed of all the conditioning data that enables one to assign  probabilities to the space of outcomes. In shallow parsing, history can be viewed as all the information  derived from the test corpus relevant to the current token.  The computation of p(o|h) in ME depends on a set of binary-valued features, which is helpful in  making a prediction about the outcome. For instance, one of our features is as follows: when the  current token is a verb, it is likely to be the leading character of a verb phrase. More formally, we can  represent this feature as  f  (h, o)  =  ⎧1: if Current ⎩⎨0 : else  -  token  -  verb(h)  =  true  and  o  =  VP  _  begin  (1)  Here, Current-token-verb(h) is a binary function that returns the value true if the current token of the  history h is a verb.  Given a set of features and a training corpus, the ME estimation process produces a model in  which every feature fi has a weight αi. This allows us to compute the conditional probability as follows:  ∏ p(o | h) = 1 Z (h)  i  αi fi (h,o) ,  (2)  where Z(h) is a normalization factor. Intuitively, the probability is the multiplication of the weights of active features (i.e., those fi (h,o) = 1). The weight αi is estimated by means of a procedure called Generalized Iterative Scaling (GIS) [8], which improves the estimation of the weights at each iteration. The ME estimation technique guarantees that, for every feature fi, the expected value of αi will be equal to the empirical expectation of αi in the training corpus. ME allows the designer to concentrate on finding the features that characterize the problem, while letting the ME estimation routine deal with  assigning relative weights to the features.  3.3 Decoding After an ME model has been trained and the proper weight αi has been assigned to each feature fi, decoding (i.e., marking up) a new piece of text becomes a simple task. First, the model tokenizes the text and preprocesses the test sentence. Then, for each token, it checks which features are active and combines the αi of the active features according to Equation 2. Finally, a Viterbi search is run to find the highest probability path through the lattice of conditional probabilities that does not produce any invalid tag sequences. Further details of the Viterbi search can be found in [17].  4. Experiment  By comparing models with and without noisy training data, we can determine whether our Chinese shallow parser is noisy-data-tolerant. In this section, we describe how we add noisy data to maximum entropy models and evaluate the tolerance of our system to Chinese chunking.  4.1 Data and Features  Sinica Treebank contains more than 54,000 sentences, from which we randomly extract 30,000 for training and 10,000 for testing. The tokenized results and the corresponding part-of-speech sequences of these sentences are extracted into a feature file, and the top-level chunks of the parsing tree structure can be taken as the standard for training and evaluation. The information in the feature file is translated into machine learning features by ME model in both the training and testing phrases. The features we adopted are: words, adjacent characters, prefixes of words (1 and 2 characters), suffixes of words (1 and 2 characters), word length, POS of words, adjacent POS tags, and the word’s location in the chunk it belongs to. To analyze the performance of our shallow parser under noisy conditions, we build a standard model and various noisy models. Training data consisting of the tokenization and POS information derived from the manually annotated Sinica Treebank is used as the standard model in our experiments. The accuracy of chunking in this model is then compared with that of models containing noise to  observe the difference. 4.2 Noise Model Generation The most important issue in noisy model generation is how to mix noisy features with correct features as smoothly as in a real parsing system. We design three methods for adding noise to generate different types of models with noisy tokenization and POS sequences. The first two approaches are based on unknown word replacement. We find that unknown words are one of the major causes of noisy data in real world system processing, because most unknown words are proper nouns. Theoretically, we can pick a certain number of proper nouns in the selected data and substitute them with noisy data to simulate real world input. In our experiment, “Nb” and “Nc”, which are defined as “proper nouns” and “proper location nouns” respectively in the Sinica Treebank tagging guideline [5], are chosen as replacement targets. Words with these two target POS are regarded as replacement target strings and replaced by noisy data. We adopt two types of noisy data for unknown word replacement. The first is the split character sequence of a replacement target string in a sentence. Initially, we extract the correct tokenization results and POS sequences of all data in the Sinica Treebank with “Nb” and “Nc”. Then, wherever applicable, we split the replacement target string in a sentence into single Chinese characters. The corresponding POS tag of each split character is re-assigned by selecting the most frequent POS tags of these single characters in Sinica Treebank. For example, “馬來西亞” (Malaysia) would be split into “馬”, ”來”, ”西”, and “亞”, and the original POS tag “Nca” would be replaced by the pos tags of four single characters: “Nab”, “Dbab”, “Ncda”, and “Nca”. In this experiment, we control the amount of noisy data in models to observe the relation between the percentage of imprecise data and the chunking performance. The model generated by this approach is called a Type 1 noise model. Another approach, called the Type 2 noise model, tokenizes the replacement target with AUTOTAG, which may produce segmenting boundaries and POS tags that differ from those in Sinica Treebank. The information is then used as noisy features and replaces the target string. For instance, the replacement target string “太白金 星” with POS tag ”Nb” would be tagged by AUTOTAG as “太白/Nb” and “金星/Nb”. The above noise-adding approaches are used to generate training data, as well as various kinds of noisy information in the test sets. In addition, we adopt an automatic tool, CKIP AUTOTAG [7], to obtain the tokenization information and POS features for generating models. This is a Chinese tokenizing tool that can deal with word segmentation in both the training and testing sets. CKIP AUTOTAG provides the POS sequences of the sentences. The tokenized sentences and POS sequences produced by AUTOTAG are used to generate feature files for ME processing. 5. Results and Discussion In our experiment, we adopt the B-I-O scheme to identify the boundaries of Chinese chunks and the position of each element word in the chunks. In addition, we employ the following four standards  when calculating the accuracy of Chinese shallow parsing: evaluation by token, by chunk boundary, by chunk category sequence, and by chunks. Token evaluation is based on the number of Chinese words. All words in the test data can be verified independently to determine if they have the correct boundaries and belong to the right chunks. Evaluation by chunk boundary only checks the boundaries of each chunk, while evaluation by chunk category sequence only checks if all the chunks in a sentence can be identified successfully and disregards the constituents. By contrast, in chunk evaluation, the basic unit is the whole chunk, and only a chunk with the right constituents and tagged with proper categories can be considered correct. We use an example to demonstrate the evaluation process. The input sentence is “小朋友 換成 你 來 試試看”, which consists of five tokens; and the standard parsing result is “小朋 友/NP 換成/VC 你/NP 來-試試看/VP”, which contains four chunks. The parsing result we obtain from the system is “小朋友/NP 換成/VC 你/NP 來/Db 試試看/VE”, which contains five chunks. In this case, the accuracy of the chunk boundary and the chunk category are both 3/4=0.75, because the first three chunks in the sentence have the correct boundaries and phrase tags, and the last VP chunk is separated by two units. The token number in this sentence is 5 and the last two tokens have incorrect phrase category tags. Therefore, the accuracy of the token is 3/5=0.6. In chunk evaluation, three of the four chunks are identified successfully and the chunk accuracy is 3/4=0.75. We adopt these evaluation methods in all the experiment configurations in Tables 2 to 5.  5.1 Performance on Noisy Data  Table 2 shows the accuracy rates using Type 1 noisy models with different scales of noisy data for chunking clean test data. The columns show the percentage of ‘Nb’ and ‘Nc’ replaced by single character noisy data in the training model, and the rows indicate the four evaluation methods. We find that the accuracy in this series decreases slightly, while the percentage of single character noisy data increases.  Table 2. Results of chunking clean test data with the Type 1 noise model  0 (%) 10 (%) 20 (%) 30 (%) 40 (%) 50 (%) 60 (%) 70 (%) 80 (%) 90 (%) 100 (%)  Boundary 84.83 84.74 84.80 84.77 84.70 84.64 84.56 84.42 84.53 84.38 84.26  Category 70.10 69.92 69.94 69.88 69.77 69.65 69.57 69.39 69.67 69.44 69.51  Tokens 69.14 69.04 69.03 69.10 68.97 69.02 68.78 68.76 68.99 68.58 68.57  Chunks 70.47 70.30 70.26 70.20 70.13 70.00 69.82 69.59 69.77 69.72 69.75  Table 3 shows the accuracy rates using the Type 1 model with different scales of noisy data for chunking test data with single character noise (Type 1). It is quite interesting that the curve is not monotonically increasing or decreasing. This indicates that the accuracy in this series decreases until the percentage of noise reaches 60%, and then it increases. Figures 1 to 4 show the differences between the clean test data and the noisy test data in Tables 2 and 3. We can observe the trends in the experiment results more intuitively.  Table 3. Results of chunking test data containing Type 1 noisy data with the Type 1 noise model  0 (%) 10 (%) 20 (%) 30 (%) 40 (%) 50 (%) 60 (%) 70 (%) 80 (%) 90 (%) 100 (%)  Boundary 83.73 83.65 83.72 83.69 83.62 83.58 83.57 83.52 83.63 83.65 83.77  Category 69.09 69.00 69.11 69.20 69.14 69.05 69.07 69.24 69.46 69.49 69.67  Tokens 65.16 65.33 65.34 65.37 65.42 65.52 65.36 65.70 65.83 65.69 65.85  Chunks 66.51 66.36 66.30 66.27 66.25 66.13 66.00 66.07 66.25 69.30 66.42  Accuracy (%)  84.9  84.7  84.5  84.3  84.1  83.9  83.7  83.5  0  10  Clean Test Data  Evaluation on Boundary 20 30 40 50 60 70 80 Noisy Data Percentage in Training Data (%) Test Data With Character Noise  90 100  Figure 1. Evaluation of the boundaries in different experiment configurations  Accuracy (%)  70.2 70 69.8 69.6 69.4 69.2 69 68.8 0 10 Clean Test Data  Evaluation on Category 20 30 40 50 60 70 80 Noisy Data Percentage in Training Data (%) Test Data with Characters Noise  90 100  Figure 2. Evaluation of the chunking category in different experiment configurations  Accuracy (%)  69.5  69  68.5  68  67.5  67  66.5  66  65.5  65  0  10  Clean Test Data  Evaluation on Tokens 20 30 40 50 60 70 80 Noisy Data Percentage in Training Data (%) Test Data with Character Noise  90 100  Figure 3. Evaluation of tokens in different experiment configurations  Accuracy (%)  71.5  70.5  69.5  68.5  67.5  66.5  65.5  0  10  Clean Test Data  Evaluation on Chunks 20 30 40 50 60 70 80 Noisy Data Percentage in Training Data (%) Test Data with Character Noise  90 100  Figure 4. Evaluation of chunks in different experiment configurations  Table 4 shows the accuracy rates using the Type 2 noise model with and without tokenized strings for chunking clean test sentences and test data with tokenized strings. There are four configurations: C-C: Using a clean training model and clean test data. C-N: Using a clean training model and noisy test data in which all ‘Nb’ and ‘Nc’ are replaced by tokenized results. N-C: Using a training model with noisy data in which all ‘Nb’ and ‘Nc’ are replaced by the tokenized results of chunking clean test data. N-N: Both the training model and the test data have noisy data in which all ‘Nb’ and ‘Nc’ are replaced by tokenized results. Table 4 also shows that noisy training data yields better accuracy for both clean and noisy test data, although the difference is quite small.  Table 4. Results of chunking with the Type 2 noise model  Boundary  Category  Tokens  Chunks  C-C  84.83  70.10  69.14  70.47  C-N  84.84  70.09  69.04  70.37  N-C  84.89  70.13  69.15  70.51  N-N  84.90  70.11  69.02  70.38  Table 5 shows the accuracy rates using the model generated by AUTOTAG-parsed data and Sinica Treebank chunking tags. Both the training and the test sets are preprocessed by AUTOTAG. This experiment is designed for open testing; thus, we can use the AUTOTAG program to tokenize any  sentence and give it POS tags. However, compared to the standard model, the chunking accuracy is lower. The parsing results of the AUTOTAG-parsed model and the Type 2 noise models are shown in Figure 5.  Table 5. Accuracy using the model generated by AUTOTAG-parsed data  Fully AUTOTAG  Boundary 81.42  Category 64.81  Tokens 61.80  Chunks 61.30  Accuracy (%)  90 85 80 75 70 65 60 Boundary  Category  Tokens  Chunks  C-C C-N N-C N-N AUTOTAGPars ed  Figure 5. Comparison of various experiment configurations using tokenized string noisy data (the Type 2 noise model) and the AUTOTAG-parsed model  In Tables 6, 7, and 8, we give examples of the correct and incorrect shallow parsing results of four sentences. In each table, the left column contains the original sentences tokenized and tagged with POS tags; the center column shows the standard chunking result from Sinica Treebank; and the right column shows the shallow parsing result of our system. Table 6 shows the parsing examples with Type 1 noise. The shallow parsing results of the first two sentences are correct, while those of the last two sentences are incorrect.  Table 6. Shallow parsing examples with Type 1 noise  Sentence and POS sequences with Type Chunking standard from Chunking results of our  
- 摘要 本文實做 Kudo and Matsumoto (2000, 2001)以向量支撐機（SVM）辨識基底名詞組（base NP）演算法。我們以 中央研究院中文句結構樹資料庫 Sinica Treebank 3.0 的 80％作為訓練語料,20％作為測試語料,並比較以 Sinica Treebank 三種不同的詞性標記集訓練出來的 SVM 的辨識率（簡化標記，精簡標記，及簡化標記的大類）。實驗 的結果顯示具備詳細次分類的簡化標記的辨識率最高，在封閉測試的 F-measure 為 87.43％，初步小規模開放測 試的 F-measure 為 78.79％。詳細次分類的標記集的名詞組辨識率較高的原因是中文某些類別的動詞能夠修飾名 詞，因此沒有詳細次分類的詞類標記集無法區別那些類別的動詞可以修飾名詞。與英文日文高達 94％以上的辨 識率相比較，SVM 在中文基底名詞組辨識的效果並不理想，我們認為中研院句法樹的表示法與中文本身的特性 是造成辨識率不夠高的主要原因。 1. 前言 名詞組的辨識與標示（NP Chunking）是自然語言處理（NLP）的一個重要研究議題 (Ramshaw and Marcus (1995), Kudo and Matsumoto (2000, 2001))，無論是句法處理中的剖析(parsing)語意處理中的語意角色的標示（semantic role labeling）及篇章處理中的回指(co-reference)與連貫性（coherence），其它領域如資訊檢索(information retrieval) 資訊擷取（information extraction）文件探勘（text mining）文件分類，與文件自動摘要都需要名詞組的辨識， 例如在資訊檢索中最常被檢索的大都是名詞組（特別是人名，地名，組織名等所謂的 name entity），因此在文件 或網頁中自動辨識名詞組並建立索引以方便檢索分類及自動摘要是智慧型資訊處理極為重要的一環。 一般名詞組的辨識指的是基底名詞組（base NP），也就是將名詞組下面又包含名詞組的複雜名詞組（如關係子 句及名詞組並列結構(NP conjunction)）排除在外。目前英文名詞組的辨識正確率可以達到 94％以上(Kudo and Matsumoto (2000, 2001))，但中文名詞組的辨識至今只有少數零星的研究。本文採用監督式機器學習（supervised learning）嘗試以向量支撐機(SVM, support vector machine)透過中研院句法樹庫實做 Kudo and Matsumoto (2000, 2001)所提出的演算法。本研究的主要目的在於（一）探討中文基底名詞組辨識的重要特徵（二）評估各種基底 名詞組辨識的 SVM 表示法與其限制（三）從語言學的觀點分析影響中文基底名詞組辨識率的原因。 2. 文獻回顧  在大規模語法樹庫還沒有建立之前，名詞組辨識常將組成名詞組結構的規律透過有限狀態機（finite state machines）去找出符合名詞組的 pattern (Voutilainen (1993)) 或從標記好詞性的語料庫以統計的方式得到 （Church (1988)）, 或結和語言規律和語料庫統計（Chen and Chen (1994)）。自從賓州大學大規模的英文語法樹 庫(Penn Treebank)建構完成後 ( Marcus, Santorini and Marcinkiewicz (1993)),絕大多數的名詞組辨識研究是以機 器學習（machine learning）的方法透過語法樹庫裡面的語法結構及前後語境的特徵得到。運用機器學習辨識名 詞組的方法大致可分為 HMM (hidden Markov model)，transformation-based（Ramshaw and Marcus （1995））, memory-based （Veenstra （1998）, Tjong Kim Sang and Veenstra （1999） Argamon, Dagan and Krymolowski （1998））, maximum entropy （Skut and Brants（1998））, 及 SVM (Kudo and Matsumoto, 2000, 2001)等方法。 上述幾種的方法都是監督式學習。HMM (hidden Markov model)使用統計的方法在 finite state machine 的 transition function 之上加上語料庫的統計結果。transformation-based learning 由現有的語料庫訓練出 transformational rules, 再利用這些規則對測試資料作 parse。HMM，transformation-based learning, memory-based learning 在自然語言處理中已被廣泛應用。 SVM 則是一種較新的 machine learning 技術,近幾年逐漸被應用到自 然語言處理的各項研究議題,。 上述這些演算法針對英文 Wall Street Journal Corpus 訓練得到的結果顯示,精確率(precision)與召回率(recall)大都 超過 90%, 其中以 SVM （Kudo and Matsumoto（2001））的效果最好，精確率(precision)與召回率(recall)都超過 94% （http://staff.science.uva.nl/~erikt/research/np-chunking.html）。 中文名詞組辨識的研究起步較晚，迄今只有零星的研究，還沒有針對同一個語料庫的大規模的測試與比較。例 如中國大陸學者 Zhao and Huang (1998)提出以語料庫統計結合規律，利用 minimum description length principle (MDL)得到 quasi-dependency strength 加上規律來得到 base NP。這種採用非監督式機器學習（unsupervised learning）的方法，在封閉測試(close test)和開放測試(open test)中分別有 91.5% 和 88.7%的精確率。本研究使用 SVM 演算法來辨識中文基底名詞組（base NP），以便瞭解影響中文基底名詞組辨識的最重要特徵究竟有哪些， 以及 SVM 在中文基底名詞組辨識的效果與限制。 3. 中文句法樹庫 由於 SVM 是監督式學習的演算法，我們必須擁有中文句法樹庫(treebank)的資料才能訓練出辨識名詞組的程 式。目前我們擁有的兩個句法樹庫資料,一個是中央研究院中文句結構樹資料庫（Sinica Treebank 3.0） (http://www.aclclp.org.tw/use_stb_c.php)，另一個為美國賓州大學中文句法樹庫（Penn Chinese Treebank 4.0） (http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2004T05)。兩者在語言，語料來源，語料庫大 小，標記集，標記單位，標記訊息，及依據的語言學理論都不相同。下面是兩者的比較。  表（一）Sinica Treebank 3.0 與 Penn Chinese Treebank 4.0 的綜合比較  語言  語料來源 語料大小 標記集  句法樹的  性質  標記的訊 息  所採用的語法理 論  Sinica Chinese Treebank 3.0 Penn Chinese Treebank 4.0  繁體中文 簡體中文  
This paper presents a new speech enhancement approach originated from factor analysis (FA) framework. FA is a data analysis model where the relevant common factors can be extracted from observations. A factor loading matrix is found and a resulting model error is introduced for each observation. Interestingly, FA is a subspace approach properly representing the noisy speech. This approach partitions the space of noisy speech into a principal subspace containing clean speech and a complimentary (minor) subspace containing the residual speech and noise. We show that FA is a generalized data model compared to signal subspace approach. To perform FA speech enhancement, we present a perceptual optimization procedure that minimizes the signal distortion subject to the energies of residual speech and noise under a specified level. Importantly, we present a hypothesis testing approach to optimally perform subspace decomposition. In the experiments, we implement perceptual FA speech enhancement using Aurora2 corpus. We find that proposed approach achieves desirable speech recognition rates especially when signal-to-noise ratio is lower than 5 dB. 1. Introduction Automatic speech recognition (ASR) systems have been employed to many real-world applications. However, ASR systems are always degraded in presence of different noises in practical situations. To provide good speech quality for ASR systems, the speech enhancement is an important preprocessing procedure for noisy speech recognition. In the past decade, the researchers on speech enhancement for robust ASR have been attracting many people working on this issue. Spectral subtraction algorithm [2] is one of the most popular methods for speech enhancement. This algorithm has the drawbacks of producing speechdistortionand“musicalnoise”.The method in [11] was proposed to overcome “musicalnoise”problem byusinghumanauditory models where theperceptualeffectof“musical noise”was reduced under predefined threshold. Below the masking threshold, the residual noise becomes inaudible by human ear. Other researchers presented subspace approaches to balance the trade off between speech distortion and residual noise [5] [8]. The general concept of subspace approaches is originated from that the noisy speech signal can be projected onto two subspaces; one is the signal subspace in which clean speech signal and few noises are included, and the other is the noise subspace that only contains noise information. In [4], Ephraim and Van Trees proposed signal subspace approach to find optimal estimator or filter by 
Recognizing transliteration names is challenging due to their flexible formulation and coverage of a lexicon. This paper employs the Web as a huge-scale corpus. The patterns extracted from the Web are considered as a live dictionary to correct speech recognition errors. In our approach, the plausible character strings recognized by ASR (Automated Speech Recognition) are regarded as query terms and submitted to Google. The top n returned web page summaries are entered into PAT trees. The terms of the highest scores are selected. Total 100 Chinese transliteration names, including 50 person names and 50 location names, are used as test data. In the ideal case, we input the correct syllable sequences, convert them to text strings and test the recovery capability of using Web corpus. The results show that both the recall rate and the MRR (Mean Reciprocal Rank) are 0.94. That is, the correct answers appear in the top 1 position in 94 cases. When a complete transliteration name recognition system is evaluated, the experiments show that ASR model with a recovery mechanism can achieve 3.82% performance increases compared to ASR only model on character level. Besides, the recovery capability improves the average ranks of correct transliteration names from the 18th to the 3rd positions on word level. 1. Introduction Named entities [1], which denote persons, locations, organizations, etc., are common foci of searchers. Capturing named entities is challenging due to their flexible formulation and up-to-date use. The issues behind speech recognition make named entity recognition more challenging on spoken level than on written level. This paper emphasizes on a special kind of named entities, called transliteration names. They denote foreign people, places, etc. Spoken transliteration name recognition is useful for many applications. For example, cross language image retrieval via spoken query aims to employ spoken queries in one language to retrieve images with captions in another language [2]. In the past, Appelt and Martin [3] adapted TextPro system for processing text to processing transcripts generated by a speech recognizer. Miller et al [4] analyzed the effects of out-of-vocabulary errors and loss of punctuation in name finding of automatic speech recognition. Huang and Waibel [5] proposed an adaptive  Figure 1. Flow of transliteration name recognition. method of named entity extraction for meeting understanding. Chen [6] dealt with spoken cross-language access to image collection. The coverage of a lexicon is one of the major issues in spoken transliteration name access. Recently, researchers are interested in exploring the Web, which provides huge-collection of up-to-date data, as a corpus. Keller and Lapata [7] employed the Web to obtain frequencies for bigrams that are unseen in a given corpus. In this paper, we consider the Web as a live dictionary for recognizing spoken transliteration names, and employ fuzzy search capability of Google to retrieve relevant web page summaries. Section 2 sketches the overall flow of our method. Section 3 employs PAT trees to learn patterns from the Web dynamically and correct the recognition errors. Section 4 shows the experiments with/without the uses of the Web. Section 5 concludes the remarks. 2. Flow of transliteration name recognition A spoken transliteration name recognition system shown in Figure 1 accepts a speech signal denoting a foreign named entity, and converts it into a character string. It is composed of the following four major stages. Stages (1) and (2) are fundamental tasks of speech recognition. Stages (3) and (4) try to correct the speech-to-text errors by using the Web. (1) At first, we employ the speech recognition models built by HTK (http://htk.eng.cam.ac.uk/) and SRILM (http://www.speech.sri.com/projects/srilm/) toolkits to get a syllable lattice of a speech signal. (2) Then, the syllable lattice is mapped into a character lattice using a mapping table. Top-n character strings are selected from the character lattice using bi-character model trained from a transliteration name corpus. The character strings are called ASR strings hereafter. (3) Next, each ASR string is regarded as a query, and is submitted to a web search engine like Google. From the top-m search result summaries of a query (i.e., an ASR string), the higher frequent patterns similar to the ASR string are considered as candidates. Because we employ PAT tree [8, 11] to extract patterns, the patterns are called PAT candidates hereafter. For PAT tree example, “湯姆漢克斯湯姆克魯斯喬治克魯尼” with MS950 encode, be shown in Figure2. The circle represents semi-infinite string number. The number located over the circle is length. The length indicates the first different bit of the character strings recorded in the sub-trees. In this example, the highest length patterns are “克魯” and “湯姆” on the nodes (7, 12) and (0, 5)  Figure 2. An example for extracting highest length patterns and its frequency. with length 33 and 34 bits. The second highest length patterns are “克”,”魯”,”姆” and “斯” on nodes (3, 7, 12),(8, 13),(1, 6) and (4, 9) with length 16,17,18 and 18 bits. The pattern extraction task will be discussed in detail in Section 3. (4) Finally, the PAT candidates of all ASR strings will be merged together, and ranked by their number of occurrences and similarity scores. Candidates of the best ranks will be regarded as recognition results of a spoken transliteration name. Consider an example shown in Figure 3. The Chinese speech signal is a transliteration name “湯姆克魯 斯” in Chinese denoting a famous movie star “Tom Cruise”. Syllable lattice illustrates different combinations of syllables. Each syllable corresponds to several Chinese characters. For example, ke is converted to “克”, “柯”, “科”, “可”, “喀”, “刻”, etc. ASR strings “塔莫克魯斯”, “塔門克魯斯”, “塔莫柯魯斯”, etc. are selected from character lattice. Through Google fuzzy search using query “塔莫克魯斯”, some summaries of Chinese web pages are reported in Figure 4. Although common transliteration of “Tom Cruise” in Chinese is “湯姆克 魯斯”, which is different from the query “塔莫克魯斯”, fuzzy matching by Google can still identify the relevant summaries containing the correct transliteration. We call this operation recognition error recovery using the Web hereafter. In the above examples, partial matching part is enclosed in rectangle symbol, e.g., “克魯斯”, and the correct transliteration name is underlined, e.g., “湯姆克魯斯”. Summaries (1), (4) and (5), mention a movie star “湯 姆克魯斯” (Tom Cruise), and summaries (2) and (3) mention a football star “克魯斯” (Cruz). Figure 3 shows that the PAT patterns like “聖塔克魯斯”, “湯姆克魯斯”, “姆克魯斯演”, etc. are proposed. After merging and ranking, the possible recognition results in sequence are “湯姆克魯斯”, “洛普克魯茲”, “聖塔克魯茲”, etc.  Figure 3. An example for recognizing transliteration name “湯姆克魯斯” (“Tom Cruise”). (1) … 至於贏家部份，則還是湯姆漢克斯、湯姆克魯斯、喬治克魯尼這些老面孔，… (2) ... 第 76 分鐘，克魯斯換下梅開二度的維埃裏。 (3) … 國際米蘭(4-4-2)：豐塔納/科爾多瓦，布爾迪索，馬特拉齊，法瓦利/斯坦科維奇，貝隆，扎內蒂， 埃姆雷/克魯斯，馬丁斯。 (4) … 提起妮可即發火湯姆克魯斯“想殺死記者”. (5) … 電影節最具看點的明星當然非妮可基德曼與湯姆克魯斯有望在水城的戲劇性重逢莫屬。 Figure 4. Summaries of fuzzy search for query “塔莫克魯斯” 3. Recognition error recovery using the Web The error recovery module tries to select the higher frequent pattern from the Web search results, and substitute the speech recognition results of Stages 1 and 2 (shown in Section 2) with the pattern. PAT tree [8,11], which was derived from Patricia tree, can be employed to extract word boundary and key phrases automatically. In this paper, the Web search results of an ASR string will be placed in a PAT tree and PAT candidates will be selected from the tree. Two issues are considered. A PAT candidate should occur more times in the PAT tree and should be similar to the ASR string. The frequency Freq of a PAT candidate can be computed easily from PAT tree structure. The similarity of a PAT candidate and an ASR string is modeled by edit distance, which is minimum number of insertions, deletions and substitutions to transform one character string (ASR) into another string (PAT). The less the number is, the more similar they are. The similarity Sim of ASR and PAT strings is the length of string alignment minus the number of edit operations. Finally, the ranking score of a PAT string relative to an ASR string is defined as follows. Score(ASR, PAT) = × Freq(PAT) + × Sim(ASR, PAT) It is computed by weighted merging of the frequency of the PAT string, and the similarity of the ASR string and PAT string. This value determines if the ASR string will be replaced by the PAT string. In the above example, Freq(湯姆克魯斯)=43 and Sim(塔莫克魯斯, 湯姆克魯斯)=3.  4. Experimental results The speech input to the transliteration name recognition system is Chinese utterance. We employed 51,114 transliteration names [9] to train the bi-character model specified in Section 2. In the experiments, the test data include 50 American state names, 29 movie star names from 31st annual people’s choice awards (http://www.pcavote.com), and 21 NBA star names from NBA 2005 all star (http://www.nba.com/allstar2005/). The test set is different from the training set and it is open test. Because there may be more than one transliteration for a foreign named entity, the answer keys are manually prepared and checked with respect to the Web. For example, “Arizona” has four possible transliterations in Chinese – say, “亞利桑納”, “亞歷桑 納”, “亞利桑那”, and “亞歷桑那”. On the average, there are 1.9 Chinese transliterations for a foreign name in our test set. In appendix A lists the name test set and its answer keys. As shown in Section 2, the transliteration name recognition system is composed of four major stages. Stages 1 and 2 perform the fundamental speech recognition task, and Stages 3 and 4 perform the error recovery task. To examine the effects of these two parts, we evaluate them separately and wholly in the following two subsections.  4.1 Performance of error recovery task  Assume correct syllables have been identified in speech recognition task. We simulate the cases by  transforming all the characters in the answer keys back to syllables. Then, Stage 2 maps the syllable lattice  into character lattice. Total 50 ASR strings are extracted from character lattice at stage 2, and submitted to  Google. Finally, the best 10 PAT candidates are selected. We use recall rate and MRR (Mean Reciprocal  Rank) [10] to evaluate the performance. Recall rate means how many transliteration names are correctly  recognized. MRR defined below means the average ranks of the correctly identified transliteration names in  the proposed 10 PAT candidates.  MRR  =  
Recently, we have proposed several effective Web-based term translation extraction methods exploring Web resources to deal with translation of Web query terms. However, many unknown proper names in Web queries are still difficult to be translated by using our previous Web-based term translation extraction methods. Therefore, in this paper we propose a new hybrid translation extraction method, which combines our pervious Web-based term translation extraction method and a new Web-based transliteration method in order to improve translation of unknown proper names. In addition, to efficiently construct a good quality transliteration model, we also present a mixed-syllable-mapping transliteration model and a Web-based semi-supervised learning algorithm to explore search-result pages further for collecting large amounts of English-Chinese transliteration pairs from the Web. 
Previous work shows that the process of parallel text exploitation to extract transfer mappings between language pairs raises the capability of language translation. However, while this process can be fully automated, one thorny problem called “divergence” causes indisposed mapping extraction. Therefore, this paper discuss the issues of parallel text exploitation, in general, with special emphasis on divergence analysis and processing. In the experiments on a Mandarin-English travel conversation corpus of 11,885 sentence pairs, the perplexity with the alignments in IBM translation model is reduced averagely from 13.65 to 5.18 by sieving out inappropriate sentences from the collected corpus. 1. Introduction Over the past decade, research has focused on the automatic acquisition of translation knowledge from parallel text corpora. Statistical-based systems build alignment models from the corpora without linguistic analysis [1,2]. Another class of systems analyzes sentences in parallel texts to obtain transfer structures or rules [6]. Previous work shows that the process of parallel text exploitation to extract transfer mappings (models or rules) between language pairs can raise the capability of language translation. However, previous work is still hampered by the difficulties in transfer mapping extraction of achieving accurate lexical alignment and acquiring reusable structural correspondences. Although automatic extraction methods of lexical alignment and structural correspondences are introduced, they are not capable of handling exceptional cases like “divergence” presented in [4]. In general, divergence arises with variant lexical usage of role, position, and morphology between two languages. Therefore, while mapping extraction can be fully automated from parallel texts, divergence causes indisposed mapping extraction. Furthermore, the existence of translation divergences also makes adaptation from source structures into target structures difficult [5,7,8]. For parallel text exploitation, these divergences make the training process of transfer mapping extraction between languages impractical including parsing and word-level alignment, lexical-semantic lexicography, and syntactic structures. Therefore, study of parallel text exploitation needs a careful study of translation divergence.  The framework of this paper is as follows. A brief overview of parallel text exploitation is discussed in Section 2. In Section 3, translation divergence analysis and processing for Mandarin-English parallel texts is presented. Section 4 shows experimental results with the alignments in IBM translation model. Finally, generalized conclusions are presented in Section 5. 2. Overview of Statistical-based Parallel Text Exploitation The goal of parallel text exploitation is to acquire the knowledge for translation of a text given in some source (“Mandarin”) string of words, m into a target (“English”) string of words, e. For the presented statistical approach [1] to string translation of Pr(e | m) , among all possible target strings, the string will be chosen with the highest probability which is given by Bayes’ decision rule as follows:  eˆ = arg max Pr(e) Pr(m | e)  (1)  e  Pr(e) is the language model of target language and Pr(m| e) is the translation model. In order to estimate the correspondence between the words of the target sentence and the words of the source sentence, a sort of pair-wise dependence by considering all word pairs for a given sentence pair [m, e] is assumed, referred to as alignment models. Figure 1 shows an example for the translation parameters ( ) of a sentence pair. In general, these parameters are lexicon probability, ex. p m j | ei , sentence length probability, ex. p(lm | le ) , and alignment probability, ex. p( j | i,lm ,le ) . Therefore, given more parallel texts, more probability parameters could be estimated for translation.  Mandarin: lm = 4 m = m1m2 Lm j Lmlm :(ɓ)1 (ૉ)2 (εˇ)3 (፺)4 ?  English: le = 4 e = e1e2 Lei Lele :(How much)1 (for)2 (a)3 (night)4 ?  p(m2 | e4 )  : lexicon probability  p(lm = 4 | le = 4) : sentence length probability p( j = 2 | i = 4,lm ,le ) : alignment probability  Fig. 1. An example for the translation parameters of a sentence pair However, it is difficult to achieve straightforward and correct estimation for these probability parameters. In the above example, the English word “for” is one major factor called “divergence” makes the estimation process between sentence pairs impractical. Therefore, in the next section, we present the analysis and processing of the translation divergence for improving the performance on parallel text exploitation.  3. Translation Divergence Analysis and Processing 3.1 Analysis of Divergence Problems Dorr’s work [3] of divergence analysis is based on English-Spanish and English-German translations. Based on these two language pairs, 5 different categories have been identified. In this section, we discuss more multiform examples among the 5 types of divergences in Mandarin-English parallel texts. For each example, three sentences are given: e means an original English sentence in parallel texts, m means a Mandarin sentence, and ẽ means an amended English sentence which is better for translation parameter training with m. 3.1.1 Identification of Thematic Divergence Thematic divergence often involves a “swap” of the subject and object position and obtains unpredictable word-level alignment. For example, e: (Is)1 (credit card)2 (acceptable)3 (to)4 (them)5 ? m: (˼ࡁ)1 (ટա)2 (‫)̔͜ڦ‬3 (෗)4 ? ẽ: (Do)1 (they)2 (accept)3 (credit card)4 ? Here, credit card appears in subject position in e and in object position (“‫ )”̔͜ڦ‬in m; analogously, the object them appears as the subject they (“˼ࡁ”). Therefore, for the thematic divergence, the position alignments of 2↔3 and 5↔1 are obtained in a sentence pair [m, e]. However, if a sentence pair [m, ẽ] can be provided, the position alignments of 1↔2, 2↔3, and 3↔4 are better for ( ) straightforward parameter estimation of p j | i,lm ,le . 3.1.2 Identification of Morphological Divergence Morphological divergence involves the selection of a target-language word that is a morphological variant of the source-language equivalent and it raises the ambiguity of lexical-semantic lexicography. e: (May)1 (I)2 (have)3 (your)4 (signature)5 (here)6 ? m: (ሗ)1 (Ы)2 (ί)3 (வ)4 (ᖦΤ)5 (λ෗)6 ? ẽ: (Could)1 (you)2 (sign)3 (here)4 ? In this example, the predicate is nominal (signature) in e but verbal (“ᖦΤ”) in m. While ( ) inputting two sentence pairs [m, e] and [m, ẽ], the parameter estimation of p m j | ei should be ( ) reformulated with two morphological translation conditions: p m j , m j ∈V | ei , ei ∈ N and ( ) p m j , m j ∈V | ei ,ei ∈V . Therefore, with growing of various morphological translations, more  conditions would raise more complexity of lexicon transfer parameter estimation and cause more ambiguity of lexical-semantic lexicography. 3.1.3 Identification of Structural Divergence In structural divergence, a verbal argument has a different syntactic realization in the target language and the appearance of the divergence causes additional syntactic structural mapping constructions. e: (About)1 (the)2 (center)3 . m: (ɽ฿)1 (ί)2 (ʕග)3 . ẽ: (About)1 (in)2 (the)3 (center)4 . Observe that the place object is realized as a noun phrase (the center) in e and as a prepositional phrase (“ί ʕග”) in m. For this example, the divergence causes the alignment of 0←2, which is a null mapping for Mandarin lexicon “ί”. In addition, the divergence also causes alignments of 2↔3 and 3↔3, which result in non-equal mapping number q-to-n (q>1, n>1, and q≠n). For a raised null ( ) ( ) mapping, the parameter estimation of p j | i,lm ,le and p lm | le become more complicated by further considering translation of lexicon insertion (i=0) and deletion (j=0). More raised non-equal ( ) mapping number in parallel texts, more parameter estimation of p lm | le and more length generation condition for translation. 3.1.4 Identification of Conflational Divergence Conflation is the incorporation of necessary participants (or arguments) of a given action. A conflational divergence arises when there is a difference in incorporation properties between two languages. In addition, there are word compounds in Chinese language by embedding some semantic contiguity. For this divergence, the complexity of training process for transfer mapping extraction is extremely increased. e: (Please)1 (have)2 (him)3 (call)4 (me)5 . m: (ሗ)1 (ᔷѓ)2 (˼)3 (Ϋ)4 (ࡈ)5 (ཥ༑)6 (ഗ)7 (Ң)8 . ẽ: (Please)1 (tell)2 (him)3 (to)4 (give)5 (me)6 (a)7 (call)8 . This example illustrates the conflation of a constitution in e that must be overly realized in m: the effect of the action (give me a call) is indicated by the word “Ϋ ࡈ ཥ༑ ഗ Ң” whereas this information is incorporated into the main verb (call me) in e. Therefore, this divergence causes most ( ) complexity on parameter estimation of translation including p m j | ei , p(lm | le ) , and p( j | i,lm ,le ).  3.1.5 Identification of Lexical Divergence For lexical divergence, the event is lexically realized as the main verb in one language but as a different verb in other language. It typically raises the ambiguity of lexical-semantic lexicography and also can be viewed as a side effect of other divergences. Thus, the formulation thereof is considered to be some combination of those given above, such as a conflational divergence forces the occurrence of a lexical divergence.  e: (Nothing)1 (can)2 (beat)3 (Phantom of the Opera)4 . m: (ӚϞ)1 (什ჿ)2 (ˢ੻ɪ)3 (ဂᄌቾᅂ)4 . ẽ: (Nothing)1 (can)2 (compare)3 (with)4 (Phantom of the Opera)5 . Here the main verb “beat” in e but as a different verb “ˢ੻ɪ” (to compare with) in m. Other examples are like “cash”, “have”, “take”, and etc. in English but “г౬ ϓ ତ金”, “ᔷѓ”, “Ѭ”, and etc. in Mandarin, respectively.  3.2 Processing of Divergence Evaluation According to the above divergence analysis, the divergent mappings between sentence pairs are composed of non-equal mapping number (q-to-n, q>1, n>1, qÛn), different position mapping (i↔j, iÛ j), and null mapping (i→0 or 0→j). Unlike non-equal mapping number and different position mapping, the null mapping cannot provide target language translation information for lexical item selection and position generation. Therefore, we want to use a simple and straightforward measurement method to evaluate the possible null mappings. For example to the Mandarin-English parallel text corpus, given a Mandarin sentence m = m1m2 Lm j Lmlm and an English sentence e = e1e2 Lei Lele : , direct lexical mappings in the mapping space can be extracted using the relevant bilingual dictionary [13]. The mapping function is defined as follows:  ( ) ( ) ⎧1 τ m j , ei = δ m j − σ k = ⎩⎨0  if ∃σ k ∈ Θ pi , ∋ m j = σ k otherwise  (2)  where m j is j-th Mandarin segmented term; ei is the i-th English phrase, and Θ pi is represented as a Mandarin lexicon set of the English phrase ei in the chosen bilingual dictionary. The mapping ( ) function τ m j ,ei has the factorσ k , which represents k-th Mandarin lexicon in Θ pi . Therefore, if  ( ) the translation of ei found in the bilingual dictionary is the same to m j , τ m j ,ei is assigned to 1; ( ) otherwise, τ m j ,ei is assigned to 0. And we can obtain the direct lexical mapping sequence  { } ∆M =  a  i j  |0 ≤ i ≤  I  and  0≤  j≤  J  (3)  ( ) where  a  i j  is a mapping referred to as the alignment  i→ j  if τ m j ,ei = 1 or  i →0  ( ) and 0 → j if τ m j ,ei = 0 .  If the lexical mapping sequence ∆M contains more than a particular number, named ε n , of null mappings ( i → 0 and 0 → j ), then the degree of divergence between the sentence pairs [m, e] becomes significant. Hence, the content of m or e should be updated to improve the accuracy and effectiveness of exploration of mapping order between word sequences and derivation of transfer mappings. In this paper, we choose to sieve out the divergent sentence pairs from the parallel texts.  4. Experimental Results  Table 1 shows the basic characteristics of the collected parallel texts extended by travel conversation [11]. The Mandarin words in the corpora were obtained automatically using a Mandarin morphological analyzer at CKIP [10] and an English morphological analyzer referred to LinkGrammar [12].  Table 1. Basic characteristics of the collected parallel texts  Number of sentences Total number of words Number of word entries Average number of words per sentence  Mandarin 11,885 80,699 6,278 6.79  English 11,885 66915 5,118 5.63  The percentage of the various types of divergences for the collected parallel texts is shown on Fig. 2. For the collected corpus of travel conversation, almost two out of three parallel sentences (65 percent) occur the conflational divergence and less than one out of five parallel sentences (19 percent) occur the lexical divergence. In order to assess the effect of translation divergence in the parallel texts, the system also utilizes an alignment training tool called GIZA, which is a program in an EGYPT toolkit  designed by the Statistical Machine Translation team [9]1. Based on segmented Chinese, we use the original GIZA for testing in this paper. In relation to the IBM models in GIZA, this study uses models 1-4 and ten iterations of each training models for the collected corpus. The parallel sentences with various types of divergences are sieved out from the collected corpus and perplexity in IBM original GIZA training model with comparison of sieving various types of divergences is shown on Table 2. The perplexity with sieving thematic divergence is similar to that with sieving structural divergence and the perplexity with sieving morphological divergence is similar to that with sieving lexical divergence. For sieving conflational divergence, a noticeable perplexity reduction is obtained among other types of divergence but the cost is that almost two out of three parallel sentences (65 percent) are sieved out from the collected corpus. Table 3 lists the perplexity of the original parallel sentences and that of the evaluated parallel sentences from GIZA. The results demonstrate that more null mappings can result in higher perplexity, i.e. more translation choices for a lexical item, thus increasing the translation ambiguity and lowering the accuracy of lexical mapping extraction. Two amended translation probabilities with evaluation of ε n <1 are shown in Table 4. The number of translation choices of “have” and “back” are reduced from 7 to 4 and 7 to 3, respectively. After evaluating the divergence of each sentence pair in parallel texts and retaining those with ε n <1, i.e. no null mappings in a sentence pair, the perplexity in the alignment training model can be reduced from 13.65 to 5.18 on average.  Percentage of Divergences  100% 90% 80% 70% 60% 50% 40% 30% 20% 10% 0%  33% Thematic  65% 46% 26%  Morphological Structural  Conflational  Divergence Type  19% Lexical  Fig. 2. The percentage of the various types of divergences for the collected parallel texts  
This is a truly overwhelming experience. Since I am not a cat, I expect to have one lifetime in which to achieve anything, and one award for it at the most. So this is it, and I am honored and humbled and, of course, delighted. It is wonderful to be among so many of the friends who have enriched my life in so many ways. Thank you for permitting me to feel, for some short moments, that one or two of the things I have done may be allowed to count as accomplishments. There is only one thing that gave me pause for a moment when I heard about the award, and that was its title. But my concern was soon put to rest when I checked up on the previous three recipients and found that, as far as I could tell, they were doing quite well. If you will forgive me, I will tell you a little about this lifetime, such as it has been. After all, as I have said, one has but one chance for such gross indulgence. The ambiguities that are a linguist’s constant companion were with me from the start for, if I say I was born in London, then do I mean that the place where I was born was part of London at the time of my birth, or that it is part of London now? In fact, only the second proposition is true. I was born in 1935 in Edgeware, which was in the county of Middlesex then, but was later absorbed by London for reasons that I believe to be unrelated. The ﬁrst event that lodged in my memory was the declaration of war in 1939. I had no idea what it meant, but everyone took it so seriously that it made some kind of impression on me even at the age of four. London was bombed a lot, and Edgeware a little. My mother was afraid of the bombing, but I found it fun. My father was never afraid of anything. He was an inspector of schools and was charged with evacuating children from London. I was sent to a variety of places but invariably returned to London after a few weeks because my mother could not stand to be away from the excitement of the capital for more than a few weeks, even if it did mean being bombed. We went to Dorchester, and Devizes, in the west country, and I encountered new kinds of vowels and discovered that syllables could end in an “r”. I started mimicking people and learning to associate the way people talked with where they came from. We went to Leeds, in the north country, and I learned how diphthongs could become monophthongs and how to tell whether a person came from Leeds or Bradford, despite the fact that these cities are barely ﬁfteen miles apart. People told me I was destined to become a linguist. I did not know what a linguist was, and it turns out, in retrospect, neither did they. ∗ Linguistics Department, Margaret Jacks Hall, Stanford CA 94305-2150. This paper is the text of the talk given on receipt of the ACL’s Lifetime Achievement Award in 2005. © 2006 Association for Computational Linguistics  Computational Linguistics  Volume 31, Number 4  One of the places I was sent to avoid the bombs was Llambadarn, a small village in southern Wales where only Welsh was spoken. The idea that there could be a place where people did not speak English had not occurred to me before, and it fascinated me. One of the things I still regret about the experience is that my mother could not stand it for more than three weeks. Had she been able to, I might have learned Welsh. Why I was called “Martin,” I do not know. The plan for my life that was beginning to take shape seems to have attracted people called “Henry” in the past. For example, there was Henry Lee Smith, who had a radio show in the United States. After hearing somebody talk for 30 seconds, he told them where they came from, where they went to school, where they had moved to after that, and so forth. He would give a history of the person based on how they spoke. Something like that had already become my “party piece” when I was six or seven. Then there was Henry Sweet, who did similar kinds of things in Britain, and another well-known character called Henry Higgins, who was actually based on Henry Sweet, and so should not, I suppose, really count. In 1947, I was sent to one of those rather exclusive places the British like to call “public schools” to get an education. Soon after, since I wanted to learn languages, I went to Tours to learn French and Baden-Baden to learn German. Then I went into the army to defend Britain. And, against the kinds of things that I am equipped to defend a country against, I defended it magniﬁcently. After that, I went to Cambridge to get more education and, as they say in Britain, to read modern and medieval languages. Reading a subject in Cambridge is approximately like majoring in it in America, except that there is nothing but the major. You spend all your time on what you are reading; there is nothing else. Modern and medieval languages constituted about the closest thing I could ﬁnd to what I was looking for, but somewhere in the back of my mind, I knew that what I wanted to do was linguistics, even though all I knew about it was what adults had said to me when I was a child. It turns out that it did exist. In fact, it existed in Michael Halliday’s department, within walking distance of where I lived in London. Whether it would have been better to go there, or whether the experience of going to Cambridge was worth it in its own right, I do not know. But, instead of doing what Henry Sweet and all those other Henrys’ did, I wound up reading Le Rouge et le Noir, La Divina Comedia, Faust, the Chanson de Roland, Parzifal (with a “z”) and other literary monuments. But I found myself saying all the time, “What about language? Doesn’t all this come from ordinary language somehow? Isn’t the language that ordinary people speak a wonderful thing that deserves to be studied in its own right?” I remember asking my French supervisor this question once, and I have to admit that the answer he gave me, wrong though it was, has remained in my memory ever since because I simply could not ﬁnd any refutation for it at the time. What he said was, “If you want to study French, don’t you want to study it as used by those who use it best?” In other words, would I not want to study the French of the literary giants who created great works of art in that language? Lord Adrian, the master of my college, and also vice chancellor of the university, asked me to act as his interpreter when giving an honorary degree to Signor Gronchi, president of Italy. The president had a cold and insisted on wearing his doctoral gown over his raincoat. But he took me for a beer at the Blue Boar afterwards, and I forgave him everything. When I was at Cambridge, it turned out that the undergraduate engineering society ordered a ﬁlm from London, and the people at the ﬁlm library put the wrong ﬁlm in the envelope. Instead of something on steam turbines, or whatever, they got Kurasawa’s Roshomon. I had for many years wanted to see this ﬁlm. I don’t know what  426  Kay  A Life of Language  it was that excited me about it. Maybe it had something to do with language, again. Maybe the idea of four people telling essentially the same story, and it coming out differently as different words were woven around it—maybe that was what fascinated me. As you probably know, it is the story of a murder, told by four witnesses one after the other. One of the accounts is that of the murdered man himself, told through a medium. In order to see this movie, which had always eluded me up until that point, I had to join the undergraduate engineering society. The literature that I received in the mail after joining told me, among many other things, that it offered a prize of 25 pounds every year for the best paper delivered by an undergraduate. I could not avoid asking myself what a medieval linguist would have to do to secure such a prize. A serious paper would obviously stand no chance. What I had to do was ﬁnd a subject that would cause people to laugh, because there was no intersection in the serious world between anything I knew and what were serious matters to them. I decided I would offer a paper on a translating machine, an idea which, surely, nobody could take seriously. I gave the paper and stimulated some laughter. However, sitting in the audience were Margaret Masterman and Frederick Parker Rhodes. Margaret Masterman was the director of the Cambridge Language Research Unit, and Frederick Parker Rhodes was a senior researcher there. They had American money to work on machine translation. Margaret Masterman’s many outstanding properties did not include a robust sense of humor. She found my paper impertinent and insisted that I visit the language unit to learn the truth about machine translation. To cut a long story short, the following year, I became a very junior member of the Cambridge Language Research Unit, having foresworn my evil ways and shown that I could speak Italian and use a soldering iron. I worked for several years with Margaret Masterman. She was the wife of Richard Brathwaite, the Knightsbridge Professor of Ethics whose major work, however, was on the philosophy of science. Margaret Masterman was a student of Wittgenstein, a fact that she never let you forget, because it gave authority to even the most outrageous things she might choose to say. She did pioneering work on semantic nets with R. H. Richens and had a theory of translation based on thesauruses and whose principal formal tool was lattice theory. She was a member of a shadowy society called the “Epiphany Philosophers,” who took it as their goal to show that Christianity and science were not only compatible but that they supported one another. My father confused the Epiphany Philosophers with the Apostles, a secret society founded in the early nineteenth century and intended to have as its members the 12 brightest undergraduates in the university. My father did not know this part of the history. What he did know was that it was based in my college—Trinity—and that its recent members had included the infamous Cambridge Four—Kim Philby, Donald Maclean, Guy Burgess, Anthony Blunt—all engaged in spying for the Soviet Union. A patriot who was concerned for my welfare, as parents are wont to be, my father occasionally expressed the hope that I was devoting my linguistic skills to worthy pursuits and that MI6 would not be coming to visit. Margaret Masterman was one of the cofounders of Lucy Cavendish College, a most unusual institution that admits as undergraduates women who have been away as mothers and who want to come back into mainstream academic life. It is a remarkable place, and she was a remarkable woman. The Cambridge Language Research Unit was small, but it had a number of illustrious alumni. Michael Halliday had spent some time there, mainly as an expert on Chinese. The late Roger Needham started his career there, as did his wife, Karen  427  Computational Linguistics  Volume 31, Number 4  Spark Jones, the previous recipient of this award. Yorick Wilks, surely well known to this audience, also started his career there. After two years at the Cambridge Language Research Unit, I seized upon an opportunity that came up to visit the Rand Corporation in California for six months. This was a trip to the United States that I have been extending ever since. There, I worked for another remarkable person, David G. Hays, to whom we owe, among other things, the Association for Computational Linguistics, the International Conferences on Computational Linguistics, and the very name “Computational Linguistics.” At the time, the Rand Corporation was working on Russian machine translation and was ahead of its time in that it had constructed a million-word dependency tree bank of Russian already in 1962 when such things were somewhat less fashionable than they are today. While I was at Rand, I took over from Hays the organization of a set of seminars to which people interested in computers and language came every week. Several of those people told me later that the meetings had played an important role in determining the later course of their lives. Also, about the same time, I started teaching computational linguistics at UCLA. At Rand, I had the great privilege of rubbing shoulders with a wonderfully rich variety of people who either worked there or who came on extended visits. Some names that come immediately to mind are those of George Danzig, inventor of the Simplex Method for Linear Programming; Norman Dalkey, one of the originators of the Delphi method; Newell, Simon, and Shaw, who could be said to have invented Artiﬁcial Intelligence; Albert Wohlsetter, who once called the Secretary of State from my house to clarify a question that arose over dinner; Keith Uncapher, founder of the Information Sciences Institute at the University of California; Richard Bellman, who created dynamic programming; Daniel Ellsberg of Pentagon Papers fame; Paul Baran, who ﬁrst proposed packet switching for computer networks; and Herman Kahn, who founded the Institute for the Future and who played war games where the score was kept in megadeaths. I would have it known that I had been gone from Rand for 10 years when Donald Rumsfeld became its chairman in 1981 and 20 years before Condoleezza Rice joined its board of trustees. For some years, the linguistics project at Rand had an internship program that included a number of people who are still prominent in our ﬁeld, and for very good reason. They include my old friends and long-time collaborators, Ron Kaplan and Lauri Karttunen. In 1962, the Association for Machine Translation and Computational Linguistics (AMTCL) came into being. A couple of years later, the ﬁrst International Conference on Computational Linguistics was held in New York. These conferences have now come to be called “Coling,” so named by a Swede, who therefore pronounced it “Cooling,” which is also how a Swede pronounces “Koling,” the name of Albert Engstro¨ m’s much-loved cartoon character, a vagabond who made sage remarks about the world between swigs at a bottle of wine. This name was associated with the conferences organized every other year by the International Committee on Computational Linguistics (ICCL) by the late Hans Karlgren. I became chair of the ICCL in 1984 and have been there ever since. The establishment of the AMTCL and ICCL foreshadowed an extremely momentous event that came about because the people involved also had to do with a document known as the ALPAC Report. This was one of the most well known, if least read, documents ever produced concerning our ﬁeld. Its full title was “Languages and Machines: computers in translation and linguistics.” It was called the ALPAC Report  428  Kay  A Life of Language  (ALPAC) after its authors, the “Automatic Language Processing Advisory Committee,” established by the U.S. National Academy of Sciences. It was a mere quarter of an inch thick, with a black cover altogether appropriate to its contents. The remit of the committee was much narrower than most people usually appreciate. The committee was supposed to tell the government how successful, how useful, how worthwhile the research that was being devoted to machine translation with government money was going to be. They were to concern themselves with the potential beneﬁts for the government, not for anybody else. But, since the money for the work came from the government, a negative report could have dire consequences for everybody, and it did. ALPAC discussed a number of interesting questions, but the essential conclusion consisted of two points. First, the government did not really need machine translation and second, even if it did, the lines of research that were being pursued had little chance of giving it to them. So the work should stop, and stop it did. A second recommendation of the committee almost got lost. It was that there should be a new focus of attention on the scientiﬁc questions that might provide a solid foundation for later engineering enterprises like machine translation. It was with the clear aim of responding to this recommendation that this association and these conferences came into being. Now anybody who competes for research grants knows that while substance and competence play a signiﬁcant role, the most important thing to have is a name, and we did not have one for the exciting new scientiﬁc enterprise we were about to engage upon. To be sure, the association and the committee antedated that report, but we had inside information, and we were ready. I use the word “we” loosely. I was precocious, but very junior, so that my role was no more than that of a ﬂy on the wall. However, I was indeed present at a meeting in the ofﬁce of David Hays at Rand when the name “computational linguistics” was settled upon. I remember who was there, but in the interest of avoiding embarrassment, I will abstain from mentioning their names. As I recall, four proposals were put forward, namely: 1. Computational Linguistics 2. Mechanolinguistics 3. Automatic Language Data Processing 4. Natural Language Processing Strong arguments were put forward against the latter two because it was felt that they did not sufﬁciently stress the scientiﬁc nature of the proposed enterprise. The term “Natural Language Processing” is now very popular, and if you look at the proceedings of this conference, you may well wonder whether the question of what we call ourselves and our association should not be revisited. But I would argue against this. Indeed, let me brieﬂy do so. Computational linguistics is not natural language processing. Computational linguistics is trying to do what linguists do in a computational manner, not trying to process texts, by whatever methods, for practical purposes. Natural language processing, on the other hand, is motivated by engineering concerns. I suspect that nobody would care about building probabilistic models of language unless it was thought that they would serve some practical end. There is nothing unworthy in such an enterprise. But ALPAC’s conclusions are as true today as they were in the 1960s—good 429  Computational Linguistics  Volume 31, Number 4  engineering requires good science. If one’s view of language is that it is a probability distribution over strings of letter or sounds, one turns one’s back on the scientiﬁc achievements of the ages and foreswears the opportunity that computers offer to carry that enterprise forward. Statistical approaches to the processing of unannotated text bring up the thorny philosophical question of whether the necessary properties of language are, in fact, emergent properties of text. Could it be that at least some of the facts that one needs to know about a text are not anywhere in it? There is one sense in which the answer has to be “no” for, if they are not in the text, then they are not facts about the particular text, but about texts in general, or about some class of texts to which the given one belongs. But an extreme case that might dispose one rather to answer “yes” would be one in which the “text” simply consisted of the library call number of another text that contained the real information. Someone who knows enough to be able to ﬁnd what the call number leads to can ﬁnd what the text is really about, but it is not spelled out in the original document. When people say that language is situated, they mean that examples of language use always have some of this latter quality. They depend for their understanding on outside references that their receivers must be in a position to resolve. I will give a concrete example in a moment. Part of the problem we are confronting comes from what the famous Swiss linguist Ferdinand de Saussure called l’arbitraire du signe—the arbitrariness of signs. The relationship between a word, a text, or any linguistic item, and its meaning is entirely arbitrary. It therefore does not matter how long you look at a text; you will never discover what it means unless you have some kind of inside information. It may very well be that the relationship between a sentence and its structure is also arbitrary, though here the situation is less clear. This is hardly surprising. All it amounts to is that you cannot understand a text unless you know the language. You can, however, learn a lot about the translation relation from a text and its translation. If that were not the case, Jean Franois Chapolion would not have been able to decipher the Egyptian hieroglyphs on the basis of a stone that contained a translation of hieroglyphic Egyptian into Greek. It seems to follow, therefore, that meaning has little that is essentially to do with translation. Since you cannot get the meaning from the string, but you can ﬁnd out about translation from two strings, then presumably the meaning is not directly involved. Let me argue against this position by giving a counterexample. There is nothing unusual about this counterexample. Examples of this kind are not unusual. At least one can be found in almost any paragraph-sized example of everyday language. I was sitting in a train in Montpelier, and an old lady got in and said, “Does this train go to Perpignon?” The person she was addressing said, “No, it stops in Be´ziers.” What could be simpler than that? Let’s try and translate it into German. “Fa¨hrt dieser Zug nach Perpignon?” No trouble with that as far as I can see! “No, it stops in Be´ziers”—“Nein, er endet in Be´ziers.” So, you should imagine a railway line that runs from Montpelier to Perpignon by way of Be´ziers. If a train were to end its journey in Be´ziers, it would never reach Perpignon. But suppose the situation on the ground were different. Suppose that, after leaving Montpelier, there were a fork in the line, with one branch going to Perpignon and the other to a place called Findeligne by way of Be´ziers. Suppose, furthermore, that it is well known that all trains end their journey in either Findeligne or Perpignon. The interaction with the lady ﬁts the new situation just as well as the old one. Since the train stops in Be´ziers, it must be a Findeligne train, and Perpignon is on the other branch. The German translation, however, must now be different. We can no longer trans-  430  Kay  A Life of Language  late “No, it stops in Be´ziers” as “Nein, er endet in Be´ziers” because “endet” means “stop” only in the sense of completing the journey. We now need to translate “stop” in the sense of “come to a brief stop to allow passengers to get on and off,” and for this, the appropriate word is ha¨lt. It therefore seems that, in order to be able to translate this passage correctly, one needs intimate knowledge of the geography of Provence and the schedules of the trains that run there. Another problem with attempting to learn language from text is Zipf’s Law. Zipf’s Law says that a small number of phenomena—letters, words, rules, whatever—occur with very great frequency, whereas a very large number occur very rarely. Zipf’s Law provides encouragement to people just starting to work on language because it means that almost anything you try to do with language works wonderfully at ﬁrst. You do something on 100 common words with 20 rules designed for unremarkable situations and it works wonderfully. The trouble is that new phenomena that you had not thought of continue to appear indeﬁnitely, but with steadily decreasing frequency. It is true that a textual example does not have to exemplify a new phenomenon in order to be interesting, because, as well as new phenomena, we are often interested in the frequencies of occurrence of old ones. But as a method of learning about different kinds of phenomena, it is subject to a crippling law of diminishing returns. Fortunately, there is an alternative, which is to talk to people who speak the language and who know what the phenomena are. This is what linguists do. Another question is: Do the models that we build actually respect the fact that language is in accordance with Zipf’s Law? If a probability distribution is established over a set of characters, and random text is generated in accordance with that distribution, the expected lengths of “words” will be determined by the probability of the space character, and the distribution of words will be approximately in accordance with Zipf’s Law. If, instead of characters, we work with covert features of some sort, but still including one that determines when we move to the next word, we presumably get a similar distribution. But the models we actually work with rarely, if ever, predict that language will have this striking and invariable statistical property. Time to return to serious things. For me, the event that most clearly marked the birth of computational linguistics was the invention by John Cocke in 1960 of what I always took to be the ﬁrst context-free parsing algorithm that allowed for ambiguity. If it indeed was the ﬁrst—and that turns out not to be beyond doubt—then this was the ﬁrst algorithm designed expressly to meet needs that arose in our ﬁeld. This happened at Rand just before I got there, and it became a source of great excitement for me. The algorithm works only with binary rules, that is, rules with two symbols on the right-hand side. In its simplest form, the algorithm is based on a triangular matrix, which we can call a chart, with a box for each substring of the string being parsed. The idea is to ﬁll the boxes one by one in such an order that the boxes containing potential constituents of the phrases in the current box will already have been ﬁlled. We assume that the boxes corresponding to single words are ﬁlled in an initial dictionary look-up phase. The remaining boxes are ﬁlled in order of increasing length of the corresponding substring, thus maintaining the required invariant. Several other regimes would have the same effect. The original algorithm was embodied in a Fortran program with ﬁve loops, as follows: 1. for Length from 2 to string.length 2. for Position from 0 to string.length − Length + 1 431  Computational Linguistics  Volume 31, Number 4  3. for FirstLength from 1 to Length − 1 4. for FirstConstituent in Chart[Position, FirstLength − 1] 5. for SecondConstituent in Chart[Position + FirstLength, length − FirstLength − 1] The maximum number of iterations of the ﬁrst three loops is determined by the length of the sentence, and this corresponds nicely with the observation, made later, that the time complexity of chart parsing with context-free grammar is O(n3) where n is the string length. The number of iterations of the last two loops, on the other hand, is controlled by the number of items that there could be in a single box in the chart which, in the original algorithm, could grow exponentially with string length. Ron Kaplan and I recognized that, with only very minor adjustments, this algorithm can be turned into one with the O(n3) time complexity I just mentioned. One way to do this would be to give the chart an additional dimension so that, for each substring of the input, there comes to be a separate box for each grammatical symbol. The boxes contain the various structures that the given substring has, and whose top node is labeled with the corresponding symbol. The parser never needs to rehearse these different structures because, for the purposes of building larger structures, they are all equivalent. This means that loops 4 and 5 are no longer controlled by the length of the string, but only by the number of nonterminal symbols in the grammar. What could be more inspiring than this elegant algorithm to a young person trying to see a little bit of rationality in an otherwise apparently random ﬁeld? Another change that we made to the original algorithm involved the introduction of partial phrases, sometimes known as active edges because they could be thought of as on the lookout for complete edges that they could absorb, thus creating either a new complete edge or a partial edge that was nearer to completion. Since one can obviously construct a phrase of arbitrary size by assimilating words one by one to partial phrases, any algorithm that works only with binary context-free rules can easily be turned into one that operates with arbitrary rules. More importantly, partial phrases enabled us to break loose from the rigid regime of loops embedded in a particular way, effectively replacing the algorithm with what I came to refer to as an algorithm schema (Kay, 1982). The basic idea was this: At any given moment, a phrase, or partial phrase that had been recognized, was stored in just one of two data structures, which we referred to as the agenda and the chart. If it was on the agenda, then the possibility that it might be extended by absorbing other edges had not been explored. But if it was in the chart, all such interactions with other phrases already in the chart had been systematically explored. The parsing algorithm consisted in moving phrases and partial phrases from the agenda to the chart in such a way as to maintain this invariant. In other words, repeat the following cycle until the agenda is empty: 1. Remove an arbitrary partial or complete phrase from the agenda. 2. Locate all current items in the chart that it could either absorb or that could be absorbed by it, putting all resulting new phrases and partial phrases on the agenda. In 1974, Ron Kaplan and I joined Xerox PARC, and before long, I had an Alto computer in my ofﬁce with 64K of memory just for me. This was one of the machines that Steve Jobs saw during a legendary visit. My intellectual history, if I may use such a pompous term, is studded with programming languages, and one that I encountered  432  Kay  A Life of Language  soon after arriving at PARC was Prolog. Yes, I encountered Smalltalk, but Prolog made a more lasting impression, and I still use it today. Prolog is a way of extracting generalizations from algorithmic problems which, if they are just the right problems, cannot be done as effectively in any other way. Here, for example, is a top-down parser. parse([], String, String). parse([Goal | Goals], [Goal | String0], String) :- parse(Goals, String0, String). parse([Goal | Goals], String0, String) :- rule(Goal, Rhs), parse(Rhs, String0, String1), parse(Goals, String1, String). It deﬁnes a single predicate, called parse, which is true of a sequence of goals and a pair of lists of words or phrases if the ﬁrst list of words and phrases has a preﬁx that can be broken into sublists that match the goals in the given order, and the remaining sufﬁx is identical with the second string. Notice that, if there is a single goal, namely, Sentence, and if the third argument is the empty list, then the second argument must be a sentence. Given particular arguments, there are three ways in which we can attempt to show the predicate to be true of them; hence the three clauses that make up the program. Here is what the three cases do: 1. If the list of goals is empty, then they can be met only by the empty string and the second and the third arguments are identical. 2. If the ﬁrst goal is met trivially, by matching the ﬁrst item in the list that is the second argument, then if the remainder of that argument is identical to the third argument, the clause succeeds. 3. If the grammar contains a rule that would replace the ﬁrst goal by a sequence of other goals, and if parsing some preﬁx of the second argument meets these goals, and if, furthermore, it can be shown that the remaining part of the string meets the remainder of the initial goals, then the clause will have succeeded. This is a Prolog implementation of the classical recursive-descent parser, or top-down left-to-right parser, celebrated for its inability to handle grammars with left-recursive rules. The Prolog implementation reveals in a unique way the similarity between this parser and the following bottom-up left-corner parser: parse([], String, String). parse([Goal | Goals], [Goal | String], String) :- parse(Goals, String0, String). parse([Goal | Goals], [First | String0], String) :- rule(Lhs, [First | Rhs]), parse(Rhs, String0, String1), parse([Goal | Goals], [Lhs | String1], String).  433  Computational Linguistics  Volume 31, Number 4  There are only minor changes, and they are all in the third clause. Instead of looking for a grammar rule that expands the next goal, we look for one with a righthand side whose ﬁrst item matches the ﬁrst item in the second argument. If such a rule can be found, we treat the remaining items as goals that we try to ﬁnd in what remains of the second argument. If this can be done, we can replace the matching sequence with the single symbol that constitutes the left-hand side of the rule, and we attempt to meet the original set of goals with a string modiﬁed in this way.1 The parallelism exempliﬁed is surely elegant almost to the point of being beautiful and, in the design of algorithms, as Richard O’Keefe said, elegance is not optional (O’Keefe 1999). Ron Kaplan had spent a lot of time working on augmented transitions networks (ATNs) before coming to PARC and, when we got together again, I became interested in them also. They seemed to me to have the power one needed for syntactic analysis, but they lacked a property that I thought crucial, namely reversibility. An ATN that did a good job in analysis was of no use when it came to generation. An ATN, as you doubtless know, is different from a standard ﬁnite-state automaton in two key ways. First, the condition for making a transition in a given network can be that the symbols starting at the current state in the string are acceptable to some other network named in the transition. In other words, they are recursive. The second difference is that networks produce output by making assignments to variables, or registers. The collection of these registers at the end of the network traversal constitutes the output. Consider a greatly simpliﬁed example. Suppose the Sentence network is applied to the string The book was accepted by the publisher. We can assume that one of the transitions from the initial state calls the Noun Phrase network, which recognizes the ﬁrst two words. The transition in the Sentence network puts the phrase in the subject register and moves to the next state, where one of the transitions allows a part of the verb to be, which is put in the verb register. At the next state, one of the possibilities is the past participle of a transitive verb. The system now abandons the search for an active sentence and proceeds on the assumption that it is passive. However, the work that has been done is not abandoned. The noun phrase in the subject register is simply moved to the object register because, if the sentence is passive, it will presumably ﬁll the role of deep object. The subject register is cleared at this point. The Sentence automaton will presumably now be in a ﬁnal state because the sentence could end here. But, if the word by follows, and then another noun phrase, this latter goes into the subject register. Notice that, given the contents of the registers as they would be at the end of this process, it would not be possible to follow the same path through the network and generate the original string. To start with, the publisher would be in the subject register, and not the book. In retrospect, it seems that the answer to this problem should have been obvious. But it is often so. Ron noticed that the key thing here is that you must never replace substantive contents of a register with a new one. If the register is still empty, you can put something in it, but once it has a value, that value must stay there. In other words, registers must be variables, in the mathematical understanding of that term, and  
Two application-based tasks are used for evaluation: automatic thesaurus generation and pseudo-disambiguation. It is possible to achieve signiﬁcantly better results on both these tasks by varying the parameters within the CR framework rather than using other existing distributional similarity measures; it will also be shown that any single unparameterized measure is unlikely to be able to do better on both tasks. This is due to an inherent asymmetry in lexical substitutability and therefore also in lexical distributional similarity. 1. Introduction Over recent years, approaches to a broad range of natural language processing (NLP) applications have been proposed that require knowledge about the similarity of words. The application areas in which these approaches have been proposed range from speech recognition and parse selection to information retrieval (IR) and natural language ∗ Department of Informatics, University of Sussex, Falmer, Brighton, BN1 9QH, UK. Submission received: 4 May 2004; revised submission received: 16 November 2004; accepted for publication: 16 April 2005. © 2006 Association for Computational Linguistics  Computational Linguistics  Volume 31, Number 4  generation. For example, language models that incorporate substantial lexical knowledge play a key role in many statistical NLP techniques (e.g., in speech recognition and probabilistic parse selection). However, they are difﬁcult to acquire, since many plausible combinations of events are not seen in corpus data. Brown et al. (1992) report that one can expect 14.7% of the word triples in any new English text to be unseen in a training corpus of 366 million English words. In our own experiments with grammatical relation data extracted by a Robust Accurate Statistical Parser (RASP) (Briscoe and Carroll 1995; Carroll and Briscoe 1996) from the British National Corpus (BNC), we found that 14% of noun-verb direct-object co-occurrence tokens and 49% of noun-verb direct-object co-occurrence types in one half of the data set were not seen in the other half. A statistical technique using a language model that assigns a zero probability to these previously unseen events will rule the correct parse or interpretation of the utterance impossible. Similarity-based smoothing (Hindle 1990; Brown et al. 1992; Dagan, Marcus, and Markovitch 1993; Pereira, Tishby, and Lee 1993; Dagan, Lee, and Pereira 1999) provides an intuitively appealing approach to language modeling. In order to estimate the probability of an unseen co-occurrence of events, estimates based on seen occurrences of similar events can be combined. For example, in a speech recognition task, we might predict that cat is a more likely subject of growl than the word cap, even though neither co-occurrence has been seen before, based on the fact that cat is “similar” to words that do occur as the subject of growl (e.g., dog and tiger), whereas cap is not. However, what is meant when we say that cat is “similar” to dog? Are we referring to their semantic similarity, e.g., the components of meaning they share by virtue of both being carnivorous four-legged mammals? Or are we referring to their distributional similarity, e.g., in keeping with the Firthian tradition,1 the fact that these words tend to occur as the arguments of the same verbs (e.g., eat, feed, sleep) and tend to be modiﬁed by the same adjectives (e.g., hungry and playful). In some applications, the knowledge required is clearly semantic. In IR, documents might be usefully retrieved that use synonymous terms or terms subsuming those speciﬁed in a user’s query (Xu and Croft 1996). In natural language generation (including text simpliﬁcation), possible words for a concept should be similar in meaning rather than just in syntactic or distributional behavior. In these application areas, distributional similarity can be taken to be an approximation to semantic similarity. The underlying idea is based largely on the central claim of the distributional hypothesis (Harris 1968), that is: The meaning of entities, and the meaning of grammatical relations among them, is related to the restriction of combinations of these entities relative to other entities. This hypothesized relationship between distributional similarity and semantic similarity has given rise to a large body of work on automatic thesaurus generation (Hindle 1990; Grefenstette 1994; Lin 1998a; Curran and Moens 2002; Kilgarriff 2003). There are inherent problems in evaluating automatic thesaurus extraction techniques, and much research assumes a gold standard that does not exist (see Kilgarriff [2003] and Weeds [2003] for more discussion of this). A further problem for distributional similarity methods for automatic thesaurus generation is that they do not offer any obvious way to distinguish between linguistic relations such as synonymy, antonymy, and hyponymy (see Caraballo [1999] and Lin et al. [2003] for work on this). Thus, one may question  
Daniel Marcu∗ Information Sciences Institute University of Southern California  We present a novel method for discovering parallel sentences in comparable, non-parallel corpora. We train a maximum entropy classiﬁer that, given a pair of sentences, can reliably determine whether or not they are translations of each other. Using this approach, we extract parallel data from large Chinese, Arabic, and English non-parallel newspaper corpora. We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system. We also show that a good-quality MT system can be built from scratch by starting with a very small parallel corpus (100,000 words) and exploiting a large non-parallel corpus. Thus, our method can be applied with great beneﬁt to language pairs for which only scarce resources are available. 1. Introduction Parallel texts—texts that are translations of each other—are an important resource in many NLP applications. They provide indispensable training data for statistical machine translation (Brown et al. 1990; Och and Ney 2002) and have been found useful in research on automatic lexical acquisition (Gale and Church 1991; Melamed 1997), crosslanguage information retrieval (Davis and Dunning 1995; Oard 1997), and annotation projection (Diab and Resnik 2002; Yarowsky and Ngai 2001; Yarowsky, Ngai, and Wicentowski 2001). Unfortunately, parallel texts are also scarce resources: limited in size, language coverage, and language register. There are relatively few language pairs for which parallel corpora of reasonable sizes are available; and even for those pairs, the corpora come mostly from one domain, that of political discourse (proceedings of the Canadian or European Parliament, or of the United Nations). This is especially problematic for the ﬁeld of statistical machine translation (SMT), because translation systems trained on data from a particular domain (e.g., parliamentary proceedings) will perform poorly when translating texts from a different domain (e.g., news articles). One way to alleviate this lack of parallel data is to exploit a much more available and diverse resource: comparable non-parallel corpora. Comparable corpora are texts that, while not parallel in the strict sense, are somewhat related and convey overlapping information. Good examples are the multilingual news feeds produced by news agencies such as Agence France Presse, Xinhua News, Reuters, CNN, BBC, etc. Such texts are widely available on the Web for many language pairs and domains. They often  ∗ 4676 Admiralty Way, Suite 1001, Marina del Rey, CA 90292. E-mail: {dragos,marcu}@isi.edu. Submission received: 5 November 2004; Accepted for publication: 3 March 2005. © 2006 Association for Computational Linguistics  Computational Linguistics  Volume 31, Number 4  contain many sentence pairs that are fairly good translations of each other. The ability to reliably identify these pairs would enable the automatic creation of large and diverse parallel corpora. However, identifying good translations in comparable corpora is hard. Even texts that convey the same information will exhibit great differences at the sentence level. Consider the two newspaper articles in Figure 1. They have been published by the English and French editors of Agence France Presse, and report on the same event, an epidemic of cholera in Pyongyang. The lines in the ﬁgure connect sentence pairs that are approximate translations of each other. Discovering these links automatically is clearly non-trivial. Traditional sentence alignment algorithms (Gale and Church 1991; Wu 1994; Fung and Church 1994; Melamed 1999; Moore 2002) are designed to align sentences in parallel corpora and operate on the assumption that there are no reorderings and only limited insertions and deletions between the two renderings of a parallel document. Thus, they perform poorly on comparable, non-parallel texts. What we need are methods able to judge sentence pairs in isolation, independent of the (potentially misleading) context. This article describes a method for identifying parallel sentences in comparable corpora and builds on our earlier work on parallel sentence extraction (Munteanu, Fraser, and Marcu 2004). We describe how to build a maximum entropy-based classiﬁer that can reliably judge whether two sentences are translations of each other, without making use of any context. Using this classiﬁer, we extract parallel sentences from very large comparable corpora of newspaper articles. We demonstrate the quality of our  Figure 1 A pair of comparable texts. 478  Munteanu and Marcu  Exploiting Non-Parallel Corpora  extracted sentences by showing that adding them to the training data of an SMT system improves the system’s performance. We also show that language pairs for which very little parallel data is available are likely to beneﬁt the most from our method; by running our extraction system on a large comparable corpus in a bootstrapping manner, we can obtain performance improvements of more than 50% over a baseline MT system trained only on existing parallel data. Our main experimental framework is designed to address the commonly encountered situation that exists when the MT training and test data come from different domains. In such a situation, the test data is in-domain, and the training data is out-of-domain. The problem is that in such conditions, translation performance is quite poor; the out-of-domain data doesn’t really help the system to produce good translations. What is needed is additional in-domain training data. Our goal is to get such data from a large in-domain comparable corpus and use it to improve the performance of an out-of-domain MT system. We work in the context of Arabic-English and Chinese-English statistical machine translation systems. Our out-of-domain data comes from translated United Nations proceedings, and our indomain data consists of news articles. In this experimental framework we have access to a variety of resources, all of which are available from the Linguistic Data Consortium:1 r large amounts of out-of-domain parallel data; r smaller amounts of in-domain parallel data; r in-domain MT test corpora with four reference translations; and r in-domain comparable corpora: large collections of Arabic, Chinese, and English news articles from various news agencies. In summary, we call in-domain the domain of the test data that we wish to translate; in this article, that in-domain data consists of news articles. Out-of-domain data is data that belongs to any other domain; in this article, the out-of-domain data is drawn from United Nations (UN) parliamentary proceedings. We are interested in the situation that exists when we need to translate news data but only have UN data available for training. The solution we propose is to get comparable news data, automatically extract parallel sentences from it, and use these sentences as additional training data; we will show that doing this improves translation performance on a news test set. The Arabic-English and Chinese-English resources described in the previous paragraph enable us to simulate our conditions of interest and perform detailed measurements of the impact of our proposed solution. We can train baseline systems on UN parallel data (using the data from the ﬁrst bullet in the previous paragraph), extract additional news data from the large comparable corpora (the fourth bullet), accurately measure translation performance on news data against four reference translations (the third bullet), and compare the impact of the automatically extracted news data with that of similar amounts of human-translated news data (the second bullet). In the next section, we give a high-level overview of our parallel sentence extraction system. In Section 3, we describe in detail the core of the system, the parallel sen-  
Daniel Marcu∗ Information Sciences Institute University of Southern California  Current research in automatic single-document summarization is dominated by two effective, yet na¨ıve approaches: summarization by sentence extraction and headline generation via bagof-words models. While successful in some tasks, neither of these models is able to adequately capture the large set of linguistic devices utilized by humans when they produce summaries. One possible explanation for the widespread use of these models is that good techniques have been developed to extract appropriate training data for them from existing document/abstract and document/ headline corpora. We believe that future progress in automatic summarization will be driven both by the development of more sophisticated, linguistically informed models, as well as a more effective leveraging of document/abstract corpora. In order to open the doors to simultaneously achieving both of these goals, we have developed techniques for automatically producing word-to-word and phrase-to-phrase alignments between documents and their humanwritten abstracts. These alignments make explicit the correspondences that exist in such document/abstract pairs and create a potentially rich data source from which complex summarization algorithms may learn. This paper describes experiments we have carried out to analyze the ability of humans to perform such alignments, and based on these analyses, we describe experiments for creating them automatically. Our model for the alignment task is based on an extension of the standard hidden Markov model and learns to create alignments in a completely unsupervised fashion. We describe our model in detail and present experimental results that show that our model is able to learn to reliably identify word- and phrase-level alignments in a corpus of document, abstract pairs. 1. Introduction and Motivation 1.1 Motivation We believe that future success in automatic document summarization will be made possible by the combination of complex, linguistically motivated models and effective leveraging of data. Current research in summarization makes a choice between these two: one either develops sophisticated, domain-speciﬁc models that are subsequently hand-tuned without the aid of data, or one develops na¨ıve general models that can  ∗ 4676 Admiralty Way, Suite 1001, Marina del Rey, CA 90292. Email: {hdaume,marcu}@isi.edu. Submission received: 12 January 2005; revised submission received: 3 May 2005; accepted for publication: 27 May 2005. © 2006 Association for Computational Linguistics  Computational Linguistics  Volume 31, Number 4  Figure 1 Example alignment of a single abstract sentence with two document sentences. be trained on large amounts of data (in the form of corpora of document/extract or document/headline pairs). One reason for this is that currently available technologies are only able to extract very coarse and superﬁcial information that is inadequate for training complex models. In this article, we propose a method to overcome this problem: automatically generating word-to-word and phrase-to-phrase alignments between documents and their human-written abstracts.1 To facilitate discussion and to motivate the problem, we show in Figure 1 a relatively simple alignment between a document fragment and its corresponding abstract fragment from our corpus.2 In this example, a single abstract sentence (shown along the top of the ﬁgure) corresponds to exactly two document sentences (shown along the bottom of the ﬁgure). If we are able to automatically generate such alignments, one can envision the development of models of summarization that take into account effects of word choice, phrasal and sentence reordering, and content selection. Such models could be simultaneously linguistically motivated and data-driven. Furthermore, such alignments are potentially useful for current-day summarization techniques, including sentence extraction, headline generation, and document compression. A close examination of the alignment shown in Figure 1 leads us to three observations about the nature of the relationship between a document and its abstract, and hence about the alignment itself: r Alignments can occur at the granularity of words and of phrases. r The ordering of phrases in an abstract can be different from the ordering of phrases in the document. r Some abstract words do not have direct correspondents in the document, and many document words are never used in an abstract. In order to develop an alignment model that could recreate such an alignment, we need our model to be able to operate both at the word level and at the phrase level, we need it to be able to allow arbitrary reorderings, and we need it to be able to account for words on both the document and abstract side that have no direct correspondence. In this paper, we develop an alignment model that is capable of learning all these aspects of the alignment problem in a completely unsupervised fashion. 
GrapeCity Inc.  Mu Li∗ Microsoft Research Asia Chang-Ning Huang∗ Microsoft Research Asia  This article presents a pragmatic approach to Chinese word segmentation. It differs from most previous approaches mainly in three respects. First, while theoretical linguists have deﬁned Chinese words using various linguistic criteria, Chinese words in this study are deﬁned pragmatically as segmentation units whose deﬁnition depends on how they are used and processed in realistic computer applications. Second, we propose a pragmatic mathematical framework in which segmenting known words and detecting unknown words of different types (i.e., morphologically derived words, factoids, named entities, and other unlisted words) can be performed simultaneously in a uniﬁed way. These tasks are usually conducted separately in other systems. Finally, we do not assume the existence of a universal word segmentation standard that is application-independent. Instead, we argue for the necessity of multiple segmentation standards due to the pragmatic fact that different natural language processing applications might require different granularities of Chinese words. These pragmatic approaches have been implemented in an adaptive Chinese word segmenter, called MSRSeg, which will be described in detail. It consists of two components: (1) a generic segmenter that is based on the framework of linear mixture models and provides a uniﬁed approach to the ﬁve fundamental features of word-level Chinese language processing: lexicon word processing, morphological analysis, factoid detection, named entity recognition, and new word identiﬁcation; and (2) a set of output adaptors for adapting the output of (1) to different application-speciﬁc standards. Evaluation on ﬁve test sets with different standards shows that the adaptive system achieves state-of-the-art performance on all the test sets. 1. Introduction This article is intended to address, with a uniﬁed and pragmatic approach, two fundamental questions in Chinese natural language processing (NLP): What is a ‘word’ in Chinese?, and How does a computer identify Chinese words automatically? Our approach is distinguished from most previous approaches by the following three unique ∗ Natural Language Computing Group, Microsoft Research Asia, 5F, Sigma Center, No. 49, Zhichun Road, Beijing, 100080, China. E-mail: jfgao@microsoft.com, muli@microsoft.com, cnhuang@msrchina.research. microsoft.com. + The work reported in this article was done while the author was at Microsoft Research. His current e-mail address is andi.wu@grapecity.com. Submission received: 22 November 2004; revised submission received: 20 April 2005; accepted for publication: 17 June 2005. © 2006 Association for Computational Linguistics  Computational Linguistics  Volume 31, Number 4  components that are integrated into a single model: a taxonomy of Chinese words, a uniﬁed approach to word breaking and unknown word detection, and a customizable display of word segmentation.1 We will describe each of these in turn. Chinese word segmentation is challenging because it is often difﬁcult to deﬁne what constitutes a word in Chinese. Theoretical linguists have tried to deﬁne Chinese words using various linguistic criteria (e.g., Packard 2000). While each of those criteria provides valuable insights into “word-hood” in Chinese, they do not consistently lead us to the same conclusions. Fortunately, this may not be a serious issue in computational linguistics, where the deﬁnition of words can vary and can depend to a large degree upon how one uses and processes these words in computer applications (Sproat and Shih 2002). In this article, we deﬁne the concept of Chinese words from the viewpoint of computational linguistics. We develop a taxonomy in which Chinese words can be categorized into one of the following ﬁve types: lexicon words, morphologically derived words, factoids, named entities, and new words.2 These ﬁve types of words have different computational properties and are processed in different ways in our system, as will be described in detail in Section 3. Two of these ﬁve types, factoids and named entities, are not important to theoretical linguists but are signiﬁcant in NLP. Chinese word segmentation involves mainly two research issues: word boundary disambiguation and unknown word identiﬁcation. In most of the current systems, these are considered to be two separate tasks and are dealt with using different components in a cascaded or consecutive manner. However, we believe that these two issues are not separate in nature and are better approached simultaneously. In this article, we present a uniﬁed approach to the ﬁve fundamental features of word-level Chinese NLP (corresponding to the ﬁve types of words described earlier): (1) word breaking, (2) morphological analysis, (3) factoid detection, (4) named entity recognition (NER), and (5) new word identiﬁcation (NWI). This approach is based on a mathematical framework of linear mixture models in which component models are inspired by the source–channel models of Chinese sentence generation. There are basically two types of component models: a source model and a set of channel models. The source model is used to estimate the generative probability of a word sequence in which each word belongs to one word type. For each of the word types, a channel model is used to estimate the likelihood of a character string, given the word type. We shall show that this framework is ﬂexible enough to incorporate a wide variety of linguistic knowledge and statistical models in a uniﬁed way. In computer applications, we are more concerned with segmentation units than words. While words are supposed to be unambiguous and static linguistic entities, segmentation units are expected to vary from application to application. In fact, different Chinese NLP-enabled applications may have different requirements that request different granularities of word segmentation. For example, automatic speech recognition (ASR) systems prefer longer “words” to achieve higher accuracy, whereas in-  
I just returned from the Association for Computational Linguistics’ 43rd Annual Meeting (ACL-2005). The acceptance rate was 18%. Is this a good thing or a bad thing? When the acceptance rate is low, precision tends to be high. The audience can judge precision for itself. If the presentations are good, everyone knows it. And if they aren’t, they know that as well. ACL-2005 had great precision. Recall is more subtle. When there is an issue with recall, it isn’t immediately obvious to everyone. If you listen closely, you’ll hear some grumbling in the halls. And then the rejects start to appear elsewhere. ACL’s low recall has been great for other conferences. The best of the rejects are very good, better than most of the accepted papers, and often strong contenders for the best paper award at EMNLP. I used to be surprised by the quality of these rejects, but after seeing so many great rejects over so many years, I am no longer surprised by anything. The practice of setting EMNLP’s submission date immediately after ACL’s notiﬁcation date is a not-so-subtle hint: Please do something about the low recall. When you read some of the ACL reviews for these top EMNLP papers, you realize what is happening. ACL reviewing is paying too much attention to abstentions (and objections from people outside the area). If a reviewer isn’t qualiﬁed to say anything on a particular topic, that’s okay. An abstention shouldn’t kill a paper. Controversial papers are great; boring unobjectionable incremental papers are not. The only bad paper is a paper without an advocate. A paper with a single advocate should trump a paper with lots of seconds, but no advocates. Don’t average votes. The key votes are the advocates. Negative votes matter only if they convince the advocates to change their votes. Recall is a problem for many conferences, not just ACL; SIGIR, for example, rejected the classic paper on page rank, a hugely successful paper in terms of citations, perhaps more successful than anything SIGIR ever published. 1. A Model Consider the following model. Suppose we generate the gold standard so that some fraction, a = 20%, of the s = 400 submitted papers should be accepted, and 1 − a, should be rejected. There are r = 3 reviews for each paper. A review votes either 1 (accept) or 0 (reject). Papers are scored by summing the votes. Some reviews are good (g = 70%), and some (1 − g) are not. A good review votes the same way as the gold standard. A bad review votes randomly, accepting a of the papers and rejecting the rest. The program committee as a whole is evaluated in terms of precision and recall. That is, if they accept a papers with the best votes, how well do those papers match the gold standard? © 2006 Association for Computational Linguistics  Computational Linguistics  Volume 31, Number 4  Figure 1 Accepting more papers is an easy way to improve recall. Results vary slightly from run to run because of the non-determinism in the jitter function. In R (www.r-project.org) notation, we can express this model as a=0.2 # acceptance rate s=400 # submissions r=3 # reviews per paper g=0.7 # mixture of good to random reviews gold = rbinom(s, 1, a) good = matrix(rbinom(s*r, 1, g), ncol=r) score = good*gold + (1-good)*matrix(rbinom(s*r,1,a),ncol=r) accept = rank(jitter(apply(score,1,sum)))>(s*(1-a)) precision = sum(accept * gold)/sum(accept) recall = sum(accept * gold)/sum(gold) According to this model, recall can be improved in at least three ways: r Plan A: Increase a (acceptance rate) r Plan B: Increase r (reviews per paper) r Plan C: Increase g (mixture of good to random reviews) We ought to do all of the above, as much as possible. Increasing the acceptance rate is easy. There is no excuse not to. Last century, we kept the acceptance rate low so everyone could hear every paper in a single plenary session. Given the ever increasing submission rates, and other obvious practical modern realities, the old debate over plenary sessions has long since been forgotten. Nevertheless, we still hear a somewhat similar argument, that we can’t accept more papers because of some (imagined) constraint involving local arrangements. In fact, ACL-2005 could have accepted more papers than it did; there were empty rooms during much of the meeting. Moreover, local arrangements have obvious ﬁnancial incentives to come up with creative ways to accept as many papers as possible: more papers → more $$ (conference registrations). If we can accept more papers without hurting (real or perceived) precision too much, we ought to do so. In any case, acceptance rates should never be allowed to fall below 20%. Even though I can’t use the model above to justify the magic threshold of 20%, it has been my experience that whenever acceptance rates fall below that magic threshold, it be- 576  Church  Reviewing the Reviewers 
University of Manchester  Mary McGee Wood∗ University of Manchester  Agreement statistics play an important role in the evaluation of coding schemes for discourse and dialogue. Unfortunately there is a lack of understanding regarding appropriate agreement measures and how their results should be interpreted. In this article we describe the role of agreement measures and argue that only chance-corrected measures that assume a common distribution of labels for all coders are suitable for measuring agreement in reliability studies. We then provide recommendations for how reliability should be inferred from the results of agreement statistics. Since Jean Carletta (1996) exposed computational linguists to the desirability of using chance-corrected agreement statistics to infer the reliability of data generated by applying coding schemes, there has been a general acceptance of their use within the ﬁeld. However, there are prevailing misunderstandings concerning agreement statistics and the meaning of reliability. Investigation of new dialogue types and genres has been shown to reveal new phenomena in dialogue that are ill suited to annotation by current methods and also new annotation schemes that are qualitatively different from those commonly used in dialogue analysis. Previously prescribed practices for evaluating coding schemes become less applicable as annotation schemes become more sophisticated. To compensate, we need a greater understanding of reliability statistics and how they should be interpreted. In this article we discuss the purpose of reliability testing, address certain misunderstandings, and make recommendations regarding the way in which coding schemes should be evaluated. 1. Agreement, Reliability, and Coding Schemes After developing schemes for annotating discourse or dialogue, it is necessary to assess their suitability for the purpose for which they are designed. Although no statistical test can determine whether any form of annotation is worthwhile or how applications will beneﬁt from it, we at least need to show that coders are capable of performing the annotation. This often means assessing reliability based on agreement between annotators applying the scheme. Agreement measures are discussed in detail in section 2. Much of the confusion regarding which agreement measures to apply and how their results should be interpreted stems from a lack of understanding of what it means to  ∗ School of Computer Science, University of Manchester, Manchester, M13 9PL, U.K. E-mail: richard craggs@yahoo.co.uk; mary mcgee.wood@manchester.ac.uk. © 2005 Association for Computational Linguistics  Computational Linguistics  Volume 31, Number 3  assess reliability. For example, the coding manual for the Switchboard DAMSL dialogue act annotation scheme (Jurafsky, Shriberg, and Biasca 1997, page 2) states that kappa is used to “assess labelling accuracy,” and Di Eugenio and Glass (2004) relate reliability to “the objectivity of decisions,” whereas Carletta (1996) regards reliability as the degree to which we understand the judgments that annotators are asked to make. Although most researchers recognize that reporting agreement statistics is an important part of evaluating coding schemes, there is frequently a lack of understanding of what the ﬁgures actually mean. The intended meaning of reliability should refer to the degree to which the data generated by coders applying a scheme can be relied upon. If we consider the coding process to involve mapping units of analysis onto categories, data are reliable if coders agree on the category onto which each unit should be mapped. The further from perfect agreement that coders stray, the less we can rely on the resulting annotation. If data produced by applying a scheme are shown to be reliable, then we have established two important properties of those data: 1. The categories onto which the units are mapped are not inordinately dependent on the idiosyncratic judgments of any individual coder. 2. There is a shared understanding of the meaning of the categories and how data are mapped onto them. The ﬁrst of these is important for ensuring the reproducibility of the coding. To be able to trust the analysis of annotated corpora, we need to be conﬁdent that the categorization of the units of data is not dependent on which individual performed the annotation. The second governs the value of data resulting from the coding process. For an annotated corpus or the analysis thereof to be valuable, the phenomenon being annotated must represent some notion in which we can enjoy a shared understanding. 2. Agreement Measures There are many ways in which the level of agreement between coders can be evaluated, and the choice of which to apply in order to assess reliability is the source of much confusion. An appropriate statistic for this purpose must measure agreement as a function of the coding process and not of the coders, data, or categories. Only if the results of a test are solely dependent on the degree to which there is a shared understanding of how the phenomena to be described are mapped to the given categories can we infer the reliability of the resulting data. Some agreement measures do not behave in this manner and are therefore unsuitable for evaluating reliability. A great deal of importance is placed on domain speciﬁcity in discourse and dialogue studies and as such, researchers are often encouraged to evaluate schemes using corpora from more than one domain. Concerning agreement, this encouragement is misplaced. Since an appropriate agreement measure is a function of only the coding process, if the original agreement test is performed in a scientiﬁcally sound manner, little more can be proved by applying it again to different data. Any differences in the results between corpora are a function of the variance between samples and not of the reliability of the coding scheme. Di Eugenio and Glass (2004) identify three general classes of agreement statistics and suggest that all three should be used in conjunction in order to accurately evaluate coding schemes. However, this suggestion is founded on some misunderstandings of 290  Craggs and Wood  Evaluating Discourse and Dialogue Coding Schemes  the role of agreement measure in reliability studies. We shall now rectify these and conclude that only one class of agreement measure is suitable. 2.1 Percentage Agreement The ﬁrst of the recommended agreement tests, percentage agreement, measures the proportion of agreements between coders. This is an unsuitable measure for inferring reliability, and it was the use of this measure that prompted Carletta (1996) to recommend chance-corrected measures. Percentage agreement is inappropriate for inferring reliability because it excludes any notion of the level of agreement that we could expect to achieve by chance. Reliability should be inferred by locating the achieved level of agreement on a scale between the best possible (coders agree perfectly) and the worst possible (coders do not understand or cannot perform the mapping and behave randomly). Without any indication of the agreement that coders would achieve by behaving randomly, any deviation from perfect agreement is uninterpretable (Krippendorff 2004b). The justiﬁcation given for using percentage agreement is that it does not suffer from what Di Eugenio and Glass (2004) referred to as the “prevalence problem.” Prevalence refers to the unequal distribution of label use by coders. For example, Table 1 shows an example taken from Di Eugenio and Glass (2004) showing the classiﬁcation of the utterance Okay as an acceptance or acknowledgment. It represents a confusion matrix describing the number of occasions that coders used pairs of labels for a given turn. This table shows that the two coders favored the use of accept strongly over acknowledge. They correctly state that this skew in the distribution of categories increases the expected chance agreement, thus lowering the overall agreement in chance-corrected tests. The reason for this is that since one category is more popular than others, the likelihood of coders’ agreeing by chance by choosing this category increases. We therefore require a comparable increase in observed agreement to accommodate this. Di Eugenio and Glass (2004) perceive this as an “unpleasant behavior” of chancecorrected tests, one that prevents us from concluding that the example given in Table 1 shows satisfactory levels of agreement. Instead they use percentage agreement to arrive at this conclusion. By examining the data, it is clear that this conclusion would be false. In Table 1, the coders agree 90 out of 100 times, but all agreements occur when both coders choose accept. There is not a single case in which they agree on Okay’s being used as an acknowledgment. The only conclusion one may justiﬁably draw is that the coders cannot distinguish the use of Okay as an acceptance from its use as an acknowledgment. Rather than being an unpleasant behavior, accounting for prevalence in the data is an  Table 1 Prevalence in coding.  Coder 1  Coder 2 Accept Ack  Accept  90  Acknowledge  5  5 95  0  5  95  5 100  291  Computational Linguistics  Volume 31, Number 3  important part of accurately reporting the level of agreement. This helps us to avoid arriving at incorrect conclusions such as believing that the data shown in Table 1 suggest reliable coding. 2.2 Chance-Corrected Agreement: Unequal Coder Category Distribution The second class of agreement measure recommended in Di Eugenio and Glass (2004) is that of chance-corrected tests that do not assume an equal distribution of categories between coders. Chance-corrected tests compute agreement according to the ratio of observed (dis)agreement to that which we could expect by chance, estimated from the data. The measures differ in the way in which this expected (dis)agreement is estimated. Those that do not assume an equal distribution between coders calculate expected (dis)agreement based on the individual distribution of each coder. The concern that in discourse and dialogue coding, coders will differ in the frequency with which they apply labels leads Di Eugenio and Glass to conclude that Cohen’s (1960) kappa is the best chance-corrected test to apply. To clarify, by unequal distribution of categories, we do not refer to the disparity in the frequency with which categories occur (e.g., verbs are more common than pronouns) but rather to the difference in proclivity between coders (e.g., coder A is more likely to label something a noun than coder B). Cohen’s kappa calculates expected chance agreement, based on the individual coders’ distributions, in a manner similar to association measures, such as chi–square. This means that its results are dependent on the preferences of the individual coders taking part in the tests. This violates the condition set out at the beginning of this section whereby agreement must be a function of the coding process, with coders being viewed as interchangeable. The purpose of assessing the reliability of coding schemes is not to judge the performance of the small number of individuals participating in the trial, but rather to predict the performance of the schemes in general. The proposal that in most discourse and dialogue studies, the assumption of equal distribution between coders does not hold is, in fact, an argument against the use of Cohen’s kappa. Assessing the agreement between coders and accounting for their idiosyncratic proclivity toward or against certain labels tells us little about how the coding scheme will perform when applied by others. The solution is not to apply a test that panders to individual differences, but rather to increase the number of coders so that the inﬂuence of any individual on the ﬁnal result becomes less pronounced.1 Another reason provided for using Cohen’s kappa is that its sensitivity to bias (differences in coders’ category distribution) can be exploited to improve coding schemes. However, there is no need to calculate kappa in order to observe bias, since it will be evident in a contingency table of the data in question. Even if it were necessary to compute kappa for this purpose, however, this would not justify its use as a reliability test. 2.3 Chance-Corrected Agreement: Assumed Equal Coder Category Distribution The remaining class of agreement measure assumes an equal distribution of categories for all coders. Once we have accepted that this assumption is necessary in order to  
Kathleen R. McKeown† Columbia University  A system that can produce informative summaries, highlighting common information found in many online documents, will help Web users to pinpoint information that they need without extensive reading. In this article, we introduce sentence fusion, a novel text-to-text generation technique for synthesizing common information across documents. Sentence fusion involves bottom-up local multisequence alignment to identify phrases conveying similar information and statistical generation to combine common phrases into a sentence. Sentence fusion moves the summarization ﬁeld from the use of purely extractive methods to the generation of abstracts that contain sentences not found in any of the input documents and can synthesize information across sources. 1. Introduction Redundancy in large text collections, such as the Web, creates both problems and opportunities for natural language systems. On the one hand, the presence of numerous sources conveying the same information causes difﬁculties for end users of search engines and news providers; they must read the same information over and over again. On the other hand, redundancy can be exploited to identify important and accurate information for applications such as summarization and question answering (Mani and Bloedorn 1997; Radev and McKeown 1998; Radev, Prager, and Samn 2000; Clarke, Cormack, and Lynam 2001; Dumais et al. 2002; Chu-Carroll et al. 2003). Clearly, it would be highly desirable to have a mechanism that could identify common information among multiple related documents and fuse it into a coherent text. In this article, we present a method for sentence fusion that exploits redundancy to achieve this task in the context of multidocument summarization. A straightforward approach for approximating sentence fusion can be found in the use of sentence extraction for multidocument summarization (Carbonell and Goldstein 1998; Radev, Jing, and Budzikowska 2000; Marcu and Gerber 2001; Lin and Hovy 2002). Once a system ﬁnds a set of sentences that convey similar information (e.g., by clustering), one of these sentences is selected to represent the set. This is a robust approach that is always guaranteed to output a grammatical sentence. However, extraction is only a coarse approximation of fusion. An extracted sentence may include not only common information, but additional information speciﬁc to the article from ∗ Computer Science and Artiﬁcial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA 02458. E-mail: regina@csail.mit.edu. † Department of Computer Science, Columbia University, New York, NY 10027. E-mail: kathy@cs.columbia.edu. Submission received: 14 September 2003; revised submission received: 23 February 2005; accepted for publication: 19 March 2005. © 2005 Association for Computational Linguistics  Computational Linguistics  Volume 31, Number 3  which it came, leading to source bias and aggravating ﬂuency problems in the extracted summary. Attempting to solve this problem by including more sentences to restore the original context might lead to a verbose and repetitive summary. Instead, we want a ﬁne-grained approach that can identify only those pieces of sentences that are common. Language generation offers an appealing approach to the problem, but the use of generation in this context raises signiﬁcant research challenges. In particular, generation for sentence fusion must be able to operate in a domainindependent fashion, scalable to handle a large variety of input documents with various degrees of overlap. In the past, generation systems were developed for limited domains and required a rich semantic representation as input. In contrast, for this task we require text-to-text generation, the ability to produce a new text given a set of related texts as input. If language generation can be scaled to take fully formed text as input without semantic interpretation, selecting content and producing well-formed English sentences as output, then generation has a large potential payoff. In this article, we present the concept of sentence fusion, a novel text-to-text generation technique which, given a set of similar sentences, produces a new sentence containing the information common to most sentences in the set. The research challenges in developing such an algorithm lie in two areas: identiﬁcation of the fragments conveying common information and combination of the fragments into a sentence. To identify common information, we have developed a method for aligning syntactic trees of input sentences, incorporating paraphrasing information. Our alignment problem poses unique challenges: We only want to match a subset of the subtrees in each sentence and are given few constraints on permissible alignments (e.g., arising from constituent ordering, start or end points). Our algorithm meets these challenges through bottom-up local multisequence alignment, using words and paraphrases as anchors. Combination of fragments is addressed through construction of a fusion lattice encompassing the resulting alignment and linearization of the lattice into a sentence using a language model. Our approach to sentence fusion thus features the integration of robust statistical techniques, such as local, multisequence alignment and language modeling, with linguistic representations automatically derived from input documents. Sentence fusion is a signiﬁcant ﬁrst step toward the generation of abstracts, as opposed to extracts (Borko and Bernier 1975), for multidocument summarization. Unlike extraction methods (used by the vast majority of summarization researchers), sentence fusion allows for the true synthesis of information from a set of input documents. It has been shown that combining information from several sources is a natural strategy for multidocument summarization. Analysis of human-written summaries reveals that most sentences combine information drawn from multiple documents (Banko and Vanderwende 2004). Sentence fusion achieves this goal automatically. Our evaluation shows that our approach is promising, with sentence fusion outperforming sentence extraction for the task of content selection. This article focuses on the implementation and evaluation of the sentence fusion method within the multidocument summarization system MultiGen, which daily summarizes multiple news articles on the same event as part1 of Columbia’s news browsing system Newsblaster (http://newsblaster.cs.columbia.edu/). In the next section, we provide an overview of MultiGen, focusing on components that produce input or operate over output of sentence fusion. In Section 3, we provide an overview of  
Dublin City University Andy Way∗† Dublin City University  Michael Burke∗† Dublin City University Josef van Genabith∗† Dublin City University  We present a methodology for extracting subcategorization frames based on an automatic lexical-functional grammar (LFG) f-structure annotation algorithm for the Penn-II and Penn-III Treebanks. We extract syntactic-function-based subcategorization frames (LFG semantic forms) and traditional CFG category-based subcategorization frames as well as mixed function/category-based frames, with or without preposition information for obliques and particle information for particle verbs. Our approach associates probabilities with frames conditional on the lemma, distinguishes between active and passive frames, and fully reﬂects the effects of long-distance dependencies in the source data structures. In contrast to many other approaches, ours does not predeﬁne the subcategorization frame types extracted, learning them instead from the source data. Including particles and prepositions, we extract 21,005 lemma frame types for 4,362 verb lemmas, with a total of 577 frame types and an average of 4.8 frame types per verb. We present a large-scale evaluation of the complete set of forms extracted against the full COMLEX resource. To our knowledge, this is the largest and most complete evaluation of subcategorization frames acquired automatically for English. 1. Introduction In modern syntactic theories (e.g., lexical-functional grammar [LFG] [Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001], head-driven phrase structure grammar [HPSG] [Pollard and Sag 1994], tree-adjoining grammar [TAG] [Joshi 1988], and combinatory categorial grammar [CCG] [Ades and Steedman 1982]), the lexicon is the central repository for much morphological, syntactic, and semantic information.  ∗ National Centre for Language Technology, School of Computing, Dublin City University, Glasnevin, Dublin 9, Ireland. E-mail: {rodonovan,mburke,acahill,josef,away}@computing.dcu.ie. † Centre for Advanced Studies, IBM, Dublin, Ireland. Submission received: 19 March 2004; revised submission received: 18 December 2004; accepted for publication: 2 March 2005. © 2005 Association for Computational Linguistics  Computational Linguistics  Volume 31, Number 3  Extensive lexical resources, therefore, are crucial in the construction of wide-coverage computational systems based on such theories. One important type of lexical information is the subcategorization requirements of an entry (i.e., the arguments a predicate must take in order to form a grammatical construction). Lexicons, including subcategorization details, were traditionally produced by hand. However, as the manual construction of lexical resources is time consuming, error prone, expensive, and rarely ever complete, it is often the case that the limitations of NLP systems based on lexicalized approaches are due to bottlenecks in the lexicon component. In addition, subcategorization requirements may vary across linguistic domain or genre (Carroll and Rooth 1998). Manning (1993) argues that, aside from missing domain-speciﬁc complementation trends, dictionaries produced by hand will tend to lag behind real language use because of their static nature. Given these facts, research on automating acquisition of dictionaries for lexically based NLP systems is a particularly important issue. Aside from the extraction of theory-neutral subcategorization lexicons, there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG, CCG, and HPSG (Chen and Vijay-Shanker 2000; Xia 1999; Hockenmaier, Bierner, and Baldridge 2004; Nakanishi, Miyao, and Tsujii 2004). In this article we present an approach to automating the process of lexical acquisition for LFG (i.e., grammatical-function-based systems). However, our approach also generalizes to CFG category-based approaches. In LFG, subcategorization requirements are enforced through semantic forms specifying which grammatical functions are required by a particular predicate. Our approach is based on earlier work on LFG semantic form extraction (van Genabith, Sadler, and Way 1999) and recent progress in automatically annotating the Penn-II and Penn-III Treebanks with LFG f-structures (Cahill et al. 2002; Cahill, McCarthy, et al. 2004). Our technique requires a treebank annotated with LFG functional schemata. In the early approach of van Genabith, Sadler, and Way (1999), this was provided by manually annotating the rules extracted from the publicly available subset of the AP Treebank to automatically produce corresponding f-structures. If the f-structures are of high quality, reliable LFG semantic forms can be generated quite simply by recursively reading off the subcategorizable grammatical functions for each local PRED value at each level of embedding in the f-structures. The work reported in van Genabith, Sadler, and Way (1999) was small scale (100 trees) and proof of concept and required considerable manual annotation work. It did not associate frames with probabilities, discriminate between frames for active and passive constructions, properly reﬂect the effects of long-distance dependencies (LDDs), or include CFG category information. In this article we show how the extraction process can be scaled to the complete Wall Street Journal (WSJ) section of the Penn-II Treebank, with about one million words in 50,000 sentences, based on the automatic LFG f-structure annotation algorithm described in Cahill et al. (2002) and Cahill, McCarthy, et al. (2004). More recently we have extended the extraction approach to the larger, domain-diverse Penn-III Treebank. Aside from the parsed WSJ section, this version of the treebank contains parses for a subsection of the Brown corpus (almost 385,000 words in 24,000 trees) taken from a variety of text genres.1 In addition to extracting grammatical-function-  
Malvina Nissim† University of Edinburgh  We compare two ways of obtaining lexical knowledge for antecedent selection in other-anaphora and deﬁnite noun phrase coreference. Speciﬁcally, we compare an algorithm that relies on links encoded in the manually created lexical hierarchy WordNet and an algorithm that mines corpora by means of shallow lexico-semantic patterns. As corpora we use the British National Corpus (BNC), as well as the Web, which has not been previously used for this task. Our results show that (a) the knowledge encoded in WordNet is often insufﬁcient, especially for anaphor– antecedent relations that exploit subjective or context-dependent knowledge; (b) for otheranaphora, the Web-based method outperforms the WordNet-based method; (c) for deﬁnite NP coreference, the Web-based method yields results comparable to those obtained using WordNet over the whole data set and outperforms the WordNet-based method on subsets of the data set; (d) in both case studies, the BNC-based method is worse than the other methods because of data sparseness. Thus, in our studies, the Web-based method alleviated the lexical knowledge gap often encountered in anaphora resolution and handled examples with context-dependent relations between anaphor and antecedent. Because it is inexpensive and needs no hand-modeling of lexical knowledge, it is a promising knowledge source to integrate into anaphora resolution systems. 1. Introduction Most work on anaphora resolution has focused on pronominal anaphora, often achieving good accuracy. Kennedy and Boguraev (1996), Mitkov (1998), and Strube, Rapp, and Mueller (2002), for example, report accuracies of 75.0%, 89.7%, and an F-measure of 82.8% for personal pronouns, respectively. Less attention has been paid to nominal anaphors with full lexical heads, which cover a variety of phenomena, such as coreference (Example (1)), bridging (Clark 1975; Example (2)), and comparative anaphora (Examples (3–4)).1  ∗ School of Computing, University of Leeds, Woodhouse Lane, LS2 9JT Leeds, UK. E-mail: markert@comp.leeds.ac.uk. † School of Informatics, University of Edinburgh, 2 Buccleuch Place, EH8 9LW Edinburgh, UK. E-mail: mnissim@inf.ed.ac.uk. 
What would you say if your refrigerator told you, “You’re having some friends round for hot chocolate later. Maybe you should order two cartons of milk”? Of course, in Spoken Dialogue Technology, Michael McTear will not give an answer to the question of whether talking to domestic appliances makes sense, but he indicates that even a normal household, for instance, may offer a wide ﬁeld of application for spokenlanguage dialogue systems in the near future. Consequently his book primarily focuses on theory and practice of these systems. Addressing undergraduate students as well as postgraduate researchers and practitioners in human-computer interfaces, the book is subdivided into three parts which meet the readers’ needs: “Background to Spoken Dialogue Technology” (Chapters 1– 5), “Developing Spoken Dialogue Applications” (Chapters 6 –11), and “Advanced Applications” (Chapters 12–14). Chapter 1, “Talking with Computers: Fact or Fiction,” and Chapter 2, “Spoken Dialogue Applications: Research Directions and Commercial Deployment,” present recent products and aspects of dialogue technology as well as historical linguistic and artiﬁcial intelligence approaches to dialogue and simulated conversation. Aspects of present-day commercial use of spoken dialogue technology are also discussed. In Chapter 3, “Understanding Dialogue,” the term dialogue is deﬁned, and four of its key characteristics—dialogue as discourse, dialogue as purposeful activity, dialogue as collaborative activity, and utterances in dialogue—and its structures and processes are described in detail. Chapter 4 gives an overview of the components of a spoken language dialogue system: speech recognition, language understanding, language generation, and text-to-speech synthesis. The central component (i.e., dialogue management) is speciﬁed in Chapter 5. Here, dialogue initiative (system initiative, user initiative, and mixed initiative), dialogue control (ﬁnite-state-based, frame-based, and agent-based control), and grounding (how to process the user’s input) are described. Furthermore, knowledge sources (dialogue history, task record, world knowledge model, domain model, generic model, and user model) and problems that arise when interacting with an external knowledge source are discussed. The second part starts with dialogue engineering, which can be subdivided into analysis and speciﬁcation of requirements, design, implementation, testing, and evaluation of a dialogue system. The use-case analysis includes user proﬁle (type of user, language, user’s experience level, etc.) and usage proﬁle (frequency of use, input/output device type, environment, etc.). The spoken-language requirements can  Computational Linguistics  Volume 31, Number 3  
Stuart M. Shieber’s name is well known to computational linguists for his research and to computer scientists more generally for his debate on the Loebner Turing Test competition, which appeared a decade earlier in Communications of the ACM (Shieber 1994a, 1994b; Loebner 1994).1 With this collection, I expect it to become equally well known to philosophers. The collection begins with historical “precursors” to Turing’s paper: two pieces by Descartes—his Discourse on the Method, Chap. V (1637), and his “Letter to the Marquess of Newcastle”—followed by selections from La Mettrie’s Machine Man (1748). The second part contains the centerpiece: Turing’s 1950 paper from Mind, “Computing Machinery and Intelligence,” accompanied by three “ephemera”: two early (1951) and difﬁcult-to-ﬁnd articles by Turing—“Intelligent Machinery, a Heretical Theory” and “Can Digital Computers Think?”—and a transcript of a 1952 BBC radio interview with Turing, M. H. A. Newman, Sir Geoffrey Jefferson, and R. B. Braithwaite, “Can automatic Calculating Machines Be Said to Think?” Shieber’s presentation of the pie`ce de re´sistance (Turing 1950) devotes great attention to the sanctity of the text and is replete with scholarly paraphernalia comparing his carefully edited reprint with the original (which, by the way, is now available online, courtesy of JSTOR.org). The third, and ﬁnal, part contains the immediate reactions to Turing’s Mind paper as they appeared in that journal, followed by now-classic responses and some more-recent, important papers, some arranged chronologically, others logically. The ﬁrst published response was Leonard Pinsky’s early (1951)—and satirical—“Do Machines Think about Machines Thinking?” for which Shieber offers a brief, wry introduction. Next we have a quartet consisting of Keith Gunderson’s important “The Imitation Game” (1964), Richard Purtill’s response (“Beating the Imitation Game,” 1971), and Geoffrey Sampson’s (“In Defence of Turing”) and P. H. Millar’s (“On the Point of the Imitation Game”) 1973 replies to Purtill. Jumping ahead a couple of decades comes Robert M. French’s 1990 “Subcognition and the Limits of the Turing Test.” Next, in more of a logical than a chronological order, comes a trio consisting of John 
r the requirements imposed on the system (e.g., time-sensitive search and detection of obscure relations) (Chapter 2); r scope (e.g., open or restricted application domains) (Chapter 6); r complexity (e.g., fact-seeking vs. exploratory questions with opinion answers) (Chapter 7); r the granularity of information sources (e.g., databases) (Chapter 9) versus unstructured text (Chapter 17) versus full-ﬂedged knowledge bases (Chapter 19); r generally, the expectations of the users of the system (e.g., regular users versus experts), including intelligence analysts. Even if considered in isolation rather than combined together, the majority of these goals are already very far reaching. Most readers will have an immediate intuition as to how difﬁcult it would be in practice to answer, with reliable consistency, questions of seemingly unbounded complexity such as Has there been any change in the ofﬁcial opinion from China toward the 2001 annual U.S. report on human rights since its release? (page 83), How has pollution in the Black Sea affected the ﬁshing industry, and what are the sources of this pollution? (page 134), or What part did ITT (International Telephone and Telegraph) and Anaconda Copper play in the Chilean 1970 election? (page 210). Even if the question complexity is limited to the factual type, current fact-seeking question-answering technology has only a moderate impact on global-scale information-seeking environments such as Web search. The fact that quite a few chapters of the book are motivated as departures from  Computational Linguistics  Volume 31, Number 3  Table 1 List of chapters in New Directions in Question Answering. 
I met Bill in April 1980 and worked as a research linguist on a succession of projects he directed, or codirected, at the Information Sciences Institute (ISI) at the University of Southern California (USC), until I left the institute to take up a position at the University of Sydney in August 1988. Through that period (which happened to coincide, more or less, with the Reagan years), not only was Bill a very wise and wonderful project leader, but he also became a mentor and guide and a true friend. I followed from afar the further developments in his life: his departure from ISI and move to Kenya with his family to take up a position with the Summer Institute of Linguistics (SIL) as a consultant responsible for work on discourse and computational matters, their return to the United States, and his new phase of research with initiatives in RST and DGT. Phase I: Electrical Engineering When I ﬁrst met Bill, he was already in the third phase of his productive career and intellectual journey of discovery and innovation. The ﬁrst phase was launched with a degree in electrical engineering, and this was, I think, an important aspect of his enthusiasm for modeling and for problem solving and innovation. After receiving this degree, he did research in electrical engineering and learned never to do classiﬁed research again. On starting a project involving classiﬁed work, he did a survey of his ﬁeld of research and learned about another researcher doing interesting work. He tried to identify this person in order to learn from his or her ﬁndings, but because this work was also classiﬁed, he was never able to—until, on completing his own project, he discovered that this other researcher was Bill himself! He realized the high cost of secrecy in research: the great loss ∗ Department of Linguistics, Macquarie University, NSW 2109, Australia. E-mail: cmatthei@ling.mq.edu.au. © 2005 Association for Computational Linguistics  Computational Linguistics  Volume 31, Number 2  of information and also the lack of the beneﬁts of peer review. His own approach came to be just the opposite: He championed the development of reusable, shareable resources. Phase II: Artiﬁcial Intelligence The second phase started when Bill went back to school to get a Ph.D. in artiﬁcial intelligence (AI)/computer science at Carnegie Mellon University. His thesis advisors were two of the founding fathers of AI and contributors to cognitive science, Herbert A. Simon (1916–2001) and Allen Newell (1927–1992). Bill’s post-Ph.D. work at ISI did not pursue this AI research directly, but he made good use of AI techniques and tools, including goal pursuit, planning models, and knowledge representation. It was at some time during this second phase, I think, that Bill came across and read the later work by Ludwig Wittgenstein. It had a profound inﬂuence on him, as it has had on a number of other scholars coming from a logical and philosophical background rather than a rhetorical and anthropological one. Bill came to give language a much more central place than it had been given in the mainstream of AI and cognitive science, characterizing it as the most powerful human resource for knowledge representation and emphasizing the centrality of communication. Inspired by Wittgenstein, he developed his dialogue game theory, concerning exchanges in dialogue, in the late 1970s (Mann 1979). But this already takes us into Phase III. Phase III: Computational Linguistics The third phase started in the mid-1970s with Bill’s position as a computational linguistics researcher at ISI, a nonproﬁt research institute founded by Keith Uncapher (1922–2002) in 1972 (together with a small group of people who’d all been at RAND) in association with the University of Southern California. By the time I joined ISI in 1980, it had developed into a major research center. It was a very stimulating and exciting research environment, with cutting-edge research into VLSI design, network protocols, and so on. It played a central role in the development of ARPANET and its successor, the Internet, with key contributions by Jon Postel (1948–1998), a brilliant, soft-spoken researcher at ISI. 
In this article, the term language model is used to refer to any description that assigns probabilities to strings over a certain alphabet. Language models have important applications in natural language processing, and in particular, in speech recognition systems (Manning and Schu¨ tze 1999). Language models often consist of a symbolic description of a language, such as a ﬁnite automaton (FA) or a context-free grammar (CFG), extended by a probability assignment to, for example, the transitions of the FA or the rules of the CFG, by which we obtain a probabilistic ﬁnite automaton (PFA) or probabilistic context-free grammar (PCFG), respectively. For certain applications, one may ﬁrst determine the symbolic part of the automaton or grammar and in a second phase try to ﬁnd reliable probability estimates for the transitions or rules. The current article is involved with the second problem, that of extending FAs or CFGs to become PFAs or PCFGs. We refer to this process as training. Training is often done on the basis of a corpus of actual language use in a certain domain. If each sentence in this corpus is annotated by a list of transitions of an FA recognizing the sentence or a parse tree for a CFG generating the sentence, then training may consist simply in relative frequency estimation. This means that we estimate probabilities of transitions or rules by counting their frequencies in the corpus, relative to the frequencies of the start states of transitions or to the frequencies of the left-hand side nonterminals of rules, respectively. By this estimation, the likelihood of the corpus is maximized. The technique we introduce in this article is different in that training is done on the basis not of a ﬁnite corpus, but of an input language model. Our goal is to ﬁnd estimations for the probabilities of transitions or rules of the input FA or CFG such that ∗ Faculty of Arts, Humanities Computing, P.O. Box 716, NL-9700 AS Groningen, The Netherlands. E-mail: markjan@let.rug.nl. Submission received: 20th January 2004; Revised submission received: 5th August 2004; Accepted for publication: 19th September 2004 © 2005 Association for Computational Linguistics  Computational Linguistics  Volume 31, Number 2  the resulting PFA or PCFG approximates the input language model as well as possible, or more speciﬁcally, such that the Kullback-Leibler (KL) distance (or relative entropy) between the input model and the trained model is minimized. The input FA or CFG to be trained may be structurally unrelated to the input language model. This technique has several applications. One is an extension with probabilities of existing work on approximation of CFGs by means of FAs (Nederhof 2000). The motivation for this work was that application of FAs is generally less costly than application of CFGs, which is an important beneﬁt when the input is very large, as is often the case in, for example, speech recognition systems. The practical relevance of this work was limited, however, by the fact that in practice one is more interested in the probabilities of sentences than in a purely Boolean distinction between grammatical and ungrammatical sentences. Several approaches were discussed by Mohri and Nederhof (2001) to extend this work to approximation of PCFGs by means of PFAs. A ﬁrst approach is to directly map rules with attached probabilities to transitions with attached probabilities. Although this is computationally the easiest approach, the resulting PFA may be a very inaccurate approximation of the probability distribution described by the input PCFG. In particular, there may be assignments of probabilities to the transitions of the same FA that lead to more accurate approximating language models. A second approach is to train the approximating FA by means of a corpus. If the input PCFG was itself obtained by training on a corpus, then we already possess training material. However, this may not always be the case, and no training material may be available. Furthermore, as a determinized approximating FA may be much larger than the input PCFG, the sparse-data problem may be more severe for the automaton than it was for the grammar.1 Hence, even if sufﬁcient material was available to train the CFG, it may not be sufﬁcient to accurately train the FA. A third approach is to construct a training corpus from the PCFG by means of a (pseudo)random generator of sentences, such that sentences that are more likely according to the PCFG are generated with greater likelihood. This has been proposed by Jurafsky et al. (1994), for the special case of bigrams, extending a nonprobabilistic technique by Zue et al. (1991). It is not clear, however, whether this idea is feasible for training of ﬁnite-state models that are larger than bigrams. The reason is that very large corpora would have to be generated in order to obtain accurate probability estimates for the PFA. Note that the number of parameters of a bigram model is bounded by the square of the size of the lexicon; such a bound does not exist for general PFAs. The current article discusses a fourth approach. In the limit, it is equivalent to the third approach above, as if an inﬁnite corpus were constructed on which the PFA is trained, but we have found a way to avoid considering sentences individually. The key idea that allows us to handle an inﬁnite set of strings generated by the PCFG is that we construct a new grammar that represents the intersection of the languages described by the input PCFG and the FA. Within this new grammar, we can compute the expected frequencies of transitions of the FA, using a fairly standard analysis of PCFGs. These expected frequencies then allow us to determine the assignment of probabilities to transitions of the FA that minimizes the KL distance between the PCFG and the resulting PFA.  
1.1 Lexicalized Tree-Adjoining Grammars Tree-adjoining grammar (TAG) is a tree-rewriting formalism originally deﬁned by Joshi, Levy, and Takahashi (1975). A TAG (see Joshi and Schabes 1997 for an introduction) consists of a ﬁnite set of trees (elementary trees). The nodes of these trees are labeled with nonterminals and terminals (terminals label only leaf nodes). Starting from the elementary trees, larger trees are derived using composition operations of substitution (replacing a leaf with a new tree) and adjunction (replacing an internal node with a new tree). In the case of an adjunction, the tree being adjoined has exactly one leaf node that is marked as the foot node (marked with an asterisk). Such a tree is called an auxiliary tree. When such a tree is adjoined to a node µ, in the resulting tree, the subtree with root node µ from the old tree is put below the foot node of the new auxiliary tree. Elementary trees that are not auxiliary trees are called initial trees. Each derivation starts with an initial tree. In the ﬁnal derived tree, all leaves must have terminal labels.1 ∗ UFR de Linguistique, Case 7003, 2 Place Jussiec, 75005 Paris. E-mail: Laura.Kallmeyer@linguist.jussieu.fr. 1 Additionally, TAG allows for each internal node to specify the set of auxiliary trees that can be adjoined using so-called adjunction constraints and, furthermore, to specify whether adjunction at that node is obligatory. This is an important feature of TAG, since it inﬂuences the generative capacity of the formalism: {anbncndn | n ≥ 0}, for example, is a language that can be generated by a TAG with adjunction constraints but not by a TAG without adjunction constraints (Joshi 1985). For this article, however, adjunction constraints do not play any important role. Submission received: 1st September 2003; Revised submission received: 4th May 2004; Accepted for publication: 17th June 2004 © 2005 Association for Computational Linguistics  Computational Linguistics  Volume 31, Number 2  Figure 1 TAG derivation for John always laughs. Figure 1 shows a sample TAG derivation. Here, the three elementary trees for laughs, John, and always are combined: Starting from the elementary tree for laughs, the tree for John is substituted for the noun phrase (NP) leaf and the tree for always is adjoined at the verb phrase (VP) node. TAG derivations are represented by derivation trees that record the history of how the elementary trees are put together. A derivation tree is the result of carrying out substitutions and adjunctions. Each edge in the derivation tree stands for an adjunction or a substitution. The edges are labeled with Gorn addresses of the nodes where the substitutions and adjunctions have taken place: The root has the address , and the jth child of the node with address p has address pj. In Figure 1, for example, the derivation tree indicates that the elementary tree for John is substituted for the node at address 1 and always is adjoined at node address 2. What we have sketched so far are the mathematical aspects of the TAG formalism. For natural languages, TAGs with speciﬁc properties are used. These properties are not part of the formalism itself, but they are additional linguistic principles that are respected when a TAG is constructed for a natural language. First, a TAG for natural languages is lexicalized (Schabes 1990), which means that each elementary tree has a lexical anchor (usually unique, but in some cases, there is more than one anchor). Second, the elementary trees of a lexicalized TAG (LTAG) represent extended projections of lexical items (the anchors) and encapsulate all syntactic arguments of the lexical anchor; that is, they contain slots (nonterminal leaves) for all arguments. Furthermore, elementary trees are minimal in the sense that only the arguments of the anchor are encapsulated; all recursion is factored away. This amounts to the condition on elementary tree minimality (CETM) from Frank (1992) (see also Frank [2002] for further discussions of the linguistic principles underlying TAG).2 The tree for laughs in Figure 1, for example, contains only a nonterminal leaf for the subject NP (a substitution node), and there is no slot for a VP adjunct. The adverb always is added by adjunction at an internal node. Because of these principles, in linguistic applications, combining two elementary trees by substitution or adjunction corresponds to the application of a predicate to an argument. The derivation tree then reﬂects the predicate-argument structure of the sentence. This is why most approaches to semantics in TAG use the derivation tree as an interface between syntax and semantics (see, e.g., Candito and Kahane 1998; Joshi and Vijay-Shanker 1999; Kallmeyer and Joshi 2003). In this article, we are not particularly concerned with semantics, but one of the goals of the article is to obtain analyses with derivation trees representing the correct predicate-argument dependencies. 2 This minimality is actually the reason that the substitution operation is needed; formally TAGs without substitution and TAGs as introduced above have the same weak and strong generative capacity. 188  Kallmeyer  Multicomponent TAGs with Shared Nodes  An extension of TAG that has been shown to be useful for several linguistic applications is multicomponent TAG (MCTAG) (Joshi 1987; Weir 1988). Instead of single elementary trees, an MCTAG has sets of elementary trees. In each derivation step, one of these sets is chosen, and all trees from the set are added simultaneously. Depending on the nodes to which the different trees from the set attach, different kinds of MCTAGs are distinguished: If all nodes are required to be part of the same elementary tree, the MCTAG is called tree-local; if all nodes are required to be part of the same tree set, the grammar is set-local; and otherwise the grammar is nonlocal.  1.2 Scrambling in TAG  Roughly, scrambling can be described as the permutation of elements (arguments and adjuncts) of a sentence (we use the term scrambling in a purely descriptive sense without implying any theory involving actual movement). A special case of scrambling is socalled long-distance scrambling, in which arguments or adjuncts of an embedded inﬁnitive are “moved” out of the embedded VP. This occurs, for instance, in languages such as German, Hindi, Japanese, and Korean. As an example of long-distance scrambling in German, consider example (1):  (1) . . . dass [es]1 der Mechaniker [t1 zu reparieren] verspricht  . . . that it the mechanic to repair  promises  ‘. . . that the mechanic promises to repair it’  In example (1), the accusative NP es is an argument of the embedded inﬁnitive zu reparieren, but it precedes der Mechaniker, the subject of the main verb verspricht, and it is not part of the embedded VP. It has been argued (see Rambow 1994a) that in German, there is no bound on the number of scrambled elements and no bound on the depth of scrambling (i.e., in terms of movement, the number of VP borders crossed by the moved element). TAGs are not powerful enough to describe scrambling in German in an adequate way (Becker, Joshi, and Rambow 1991). By this we mean that a TAG analysis of scrambling respecting the CETM and therefore giving the correct predicate-argument structure (i.e., an analysis with each argument attaching to the verb it depends on) is not possible. Let us consider the TAG analysis of example (1) in order to see why scrambling poses a problem for TAG. If we leave aside the complementizer dass, standard TAG elementary trees for verspricht and reparieren in the style of the XTAG grammar (XTAG Research Group 1998) might look as shown in Figure 2. In the derivation, the  Figure 2 Standard TAG combination of der Mechaniker, zu reparieren, and verspricht in example (1). 189  Computational Linguistics  Volume 31, Number 2  verspricht-tree adjoins to the root node of the reparieren-tree, and the nominative NP der Mechaniker is substituted for the subject node in the verspricht-tree. This leads to the tree on the right in Figure 2. When es is added, there is a problem: It should be added to reparieren, since it is one of its arguments. But at the same time, it should precede der Mechaniker; that is, it must be adjoined either to the root or to the NPnom node. The root node belongs to verspricht, and the NPnom node belongs to der Mechaniker. Consequently, an adjunction to one of them would not give the desired predicate-argument structure. If one wanted to analyze only example (1), one could add a tree to the grammar for reparieren with a scrambled NP that allows adjunction of verspricht between the NP and the verb. But as soon as there are several scrambled elements that are arguments of different verbs, this no longer works. This example has given an idea of why scrambling is problematic for TAG. However, adopting speciﬁc elementary trees, it is possible to deal with a part of the difﬁcult scrambling data: It has been shown (see Joshi, Becker, and Rambow 2000) that TAG can describe scrambling up to depth two (two crossed VP borders). But this is not sufﬁcient. Even though examples of scrambling of depth greater than two are rare, they can occur. An example is example (2), taken from Kulick (2000):  (2) . . . dass [den Ku¨ hlschrank]1 niemand [[[t1 zu reparieren] zu versuchen]  . . . that the refrigerator nobody to repair  to try  zu versprechen] bereit ist to promise willing is ‘. . . that nobody is willing to promise to try to repair the refrigerator’ Consequently, TAG is not powerful enough to account for scrambling.3 Becker, Rambow, and Niv (1992) argue that even linear context-free rewriting sys- tems (LCFRSs) (Weir 1988) are not powerful enough to describe scrambling. (LCFRSs are weakly equivalent to set-local MCTAGs and therefore more powerful than TAGs.) Although we think that the language Becker, Rambow, and Niv deﬁne as a kind of test language for scrambling is not exactly what one needs (see section 2.3), we still suspect that they are right in claiming that LCFRSs cannot describe scrambling.  1.3 TAG Variants Proposed for Scrambling  The problem with long-distance scrambling and TAG is that the trees representing the syntax of scrambled German subordinate clauses do not have the simple nested structure that ordinary TAG generates. The CETM requires that (positions for) all of the arguments of the lexical anchor of an elementary tree be included in that tree. But in a scrambled tree, the arguments of several verbs are interleaved freely. All TAG extensions that have been proposed to accommodate this interleaving involve factoring the elementary structures into multiple components and inserting these components at multiple positions in the course of the derivation. One of the ﬁrst proposals made was an analysis of German scrambling data using nonlocal MCTAG with additional dominance constraints (Becker, Joshi, and Rambow 1991). However, the formal properties of nonlocal MCTAG are not well understood, and  3 See also Gerdes (2002) for a discussion of the limitation of TAG with respect to scrambling in German.  190  Kallmeyer  Multicomponent TAGs with Shared Nodes  it is assumed that the formalism is not polynomially parsable. Therefore this approach is no longer pursued, but it has inﬂuenced the different subsequent proposals. An alternative formalism for scrambling is V-TAG (Rambow 1994a, 1994b; Rambow and Lee 1994), a formalism that has nicer formal properties than nonlocal MCTAG. V-TAG also uses multicomponent sets (vectors) for scrambled elements; in this it is a variant of MCTAG. Additionally, there are dominance links among the trees of the same vector. In contrast to MCTAG, the trees of a vector in V-TAG are not required to be added simultaneously. The lexicalized V-TAGs that are of interest for natural languages are polynomially parsable. Rambow (1994a) proposes detailed analyses of a large range of different word order phenomena in German using V-TAG and thereby shows the linguistic usefulness of V-TAG. Even though V-TAG does not pose the problems of nonlocal MCTAG in terms of parsing complexity, it is still a nonlocal formalism in the sense that, as long as the dominance links are respected, arbitrary nodes can be chosen to attach the single components of a vector. Therefore, in order to formulate certain locality restrictions (e.g., for wh-movement and also for scrambling), one needs an additional means of putting constraints on what can interleave with the different trees of a vector, or in other words, constraints on how far a dominance link can be stretched. V-TAG allows us to put integrity constraints on certain nodes that disallow the occurrence of these nodes between two trees linked by a dominance link. This has the effect of making these nodes act as barriers. With integrity constraints, constructions involving long-distance movements can be correctly analyzed. But the explicit marking of barriers is somewhat against the original appealing TAG idea that such constraints result from imposition of the CETM, according to which the position of the moved element and the verb it depends on must be in the same elementary structure, and from the further combination possibilities of this structure. In other words, in local formalisms with an extended domain of locality such as TAG or tree-local and set-local MCTAG, such constraints result from the form of the elementary structures and from the locality of the derivation operation. That is, they follow from general properties of the grammar, and they need not be stated explicitly. This is one of the aspects that make TAG so attractive from a linguistic point of view, and it gets lost in nonlocal TAG variants. D-tree substitution grammars (DSGs) (Rambow, Vijay-Shanker, and Weir 2001) are another TAG variant one could use for scrambling. DSGs are a description-based formalism; that is, the objects a DSG deals with are tree descriptions. A problem with DSG is that the expressive power of the formalism is probably too limited to deal with all natural language phenomena: According to Rambow, Vijay-Shanker, and Weir (2001) it “does not appear to be possible for DSG to generate the copy language” (page 101). This means that the formalism is probably not able to describe cross-serial dependencies in Swiss German. Furthermore, DSG is nonlocal and therefore, as in the case of V-TAG, additional constraints (path constraints) have to be placed on material interleaving with the different parts of an elementary structure. Another TAG variant using tree descriptions is local tree description grammar (TDG) (Kallmeyer 2001). Local TDG can be used for scrambling in a way similar to DSG or V-TAG. The languages generated by local TDGs are semilinear. However, the formalism allows one to generate tree descriptions with underspeciﬁed dominance relations, and the process of resolving the remaining dominance links is nonlocal. Therefore one may have the same problem as in the case of DSG and V-TAG. Furthermore, so far it has not been shown that the formalism is polynomially parsable, and it is not clear whether such parsing is possible without any additional constraint or limitation on the underspeciﬁed tree descriptions.  191  Computational Linguistics  Volume 31, Number 2  A further TAG variant proposed in order to deal with scrambling is segmented treeadjoining grammar (SegTAG) (Kulick 2000). SegTAG uses an operation on trees called segmented adjunction that consists partly of a standard TAG adjunction and partly of a kind of tree merging or tree uniﬁcation. In this operation, two different things get mixed up, the more or less resource-sensitive adjoining operation of standard TAG, in which subtrees cannot be identiﬁed,4 and the completely different uniﬁcation operation. Perhaps using tree descriptions instead of trees, a more coherent deﬁnition of SegTAG can be achieved. But we will not pursue this here. The formal properties of SegTAG are not clear. Kulick (2000) suggests that SegTAGs are probably in the class of LCFRSs, but there is no actual proof of this. However, if SegTAG is in LCFRS, the generative power of the formalism is probably too limited to deal with scrambling in a general way. But it seems that the limit imposed by the grammar on the complexity of the scrambling data is ﬁxed but arbitrarily high. (With increasing complexity, the elementary trees, however, get larger and larger.) This means that one can probably deﬁne a SegTAG that can analyze scrambling up to some complexity level n for any n ∈ IN. (A deﬁnition of what a complexity level is, is not given; it is perhaps the depth of scrambling.) In this sense, a general treatment of scrambling might be possible. We follow a similar approach in this article by proposing a mildly context-sensitive formalism that can deal with scrambling up to some ﬁxed complexity limit n that can be chosen arbitrarily high. All these TAG variants are interesting with respect to scrambling, and they give a great deal of insight into what kind of structures are needed for scrambling. But as explained above, none of them is entirely satisfying. The most convincing one is VTAG, since this formalism can deal with scrambling, lexicalized V-TAG is polynomially parsable, and the set of languages V-TAG generates contains the set of all tree-adjoining languages (TALs) (in particular, the copy language). Furthermore, a large range of word order phenomena has been treated with V-TAG, and thereby the usefulness of V-TAG for linguistic applications has been shown. But as already mentioned, V-TAG has the inconvenience of being a nonlocal formalism. For the reasons explained above, it is desireable to ﬁnd a local TAG extension for scrambling (as opposed to the nonlocality of derivations in V-TAG, DSG, and nonlocal MCTAG) such that locality constraints for movements follow only from the form of the elementary structures and from the local character of derivations. This article proposes a local TAG variant that can deal with scrambling (at least with an arbitrarily large set of scrambling phenomena), that is polynomially parsable, and that properly extends TAG in the sense that the set of all TALs is a proper subset of the languages it generates. In section 2, tree-local MCTAG with shared nodes (SN-MCTAG) and in particular restricted SN-MCTAG (RSN-MCTAG) are introduced, formalisms that extend TAG in the sense mentioned above. Section 3 shows linguistic applications of RSN-MCTAG, in particular, an analysis of scrambling. In section 4, a relation between RSN-MCTAG and range concatenation grammar (RCG) (Boullier 1999, 2000) is established. This relation allows us to show that certain subclasses of RSN-MCTAG are mildly context-sensitive and therefore in particular polynomially parsable. These subclasses do not cover all cases of long-distance scrambling but, in contrast to TAG, they cover an arbitrarily large  4 More precisely, only the root of the new elementary tree and eventually (i.e., in the case of an adjunction) the foot node get identiﬁed with the node the new tree attaches to. But there is no uniﬁcation of whole subtrees. Consequently, every edge occurring in the derived tree comes from exactly one edge in an elementary tree, and every edge from the elementary trees used in the derivation occurs exactly once in the derived tree. In this sense the operation is resource-sensitive. 192  Kallmeyer  Multicomponent TAGs with Shared Nodes  set, providing scrambling analyses that respect the CETM. This means that the limit they impose on the complexity of the scrambling data one can analyze is variable. Based on empirical studies, it can be chosen sufﬁciently great such that the grammar covers all scrambling cases that one assumes to occur. 2. The Formalism An informal introduction of (restricted) tree-local MCTAG with shared nodes can also be found in Kallmeyer and Yoon (2004). 2.1 Motivation: The Idea of Shared Nodes Let us consider again example (1) in order to illustrate the general idea of shared nodes. In standard TAG, nodes to which new elementary trees are adjoined or substituted disappear; that is, they are replaced by the new elementary tree. For example, after having performed the derivation steps shown in Figure 2, the root node of the reparieren tree does not exist any longer. It is replaced by the verspricht tree, and its daughters have become daughters of the foot node of the verspricht tree. That is, the root node of the derived tree is considered to belong only to the verspricht tree. Therefore, an adjunction at that node is an adjunction at the verspricht tree. However, this standard TAG view is not completely justiﬁed: In the derived tree, the root node and the lower S node might as well be considered to belong to reparieren, since they are results of identifying the root node of reparieren with the root and the foot node of verspricht.5 Therefore, we propose that the two nodes in question belong to both verspricht and reparieren. In other words, these nodes are shared by the two elementary trees. Consequently, they can be used to add new elementary trees to verspricht and (in contrast to standard TAG) also to reparieren. In the following, we use an MCTAG, and we assume tree-locality; that is, the nodes to which the trees of such a set are added must all belong to the same elementary tree. Standard tree-local MCTAGs are weakly and even strongly equivalent to TAGs, but they allow us to generate a richer set of derivation structures. In combination with shared nodes, tree-local multicomponent derivation extends the weak generative power of the grammar (see Figure 4 for a sample tree-local MCTAG with shared nodes that generates a language that is not a tree-adjoining language).6 Let us go back to example (1). Assume the tree set in Figure 3 for the scrambled NP es. If the idea of shared nodes is adopted, this tree set can be added to reparieren using the root of the derived tree for adjunction of the ﬁrst tree and the NPacc substitution node for substitution of the second tree. The operation is tree-local, since both nodes are part of the reparieren tree.  5 Actually, in a feature-structure based TAG (FTAG) (Vijay-Shanker and Joshi 1988), the top feature structure of the root of the derived tree is the uniﬁcation of the top of the root of verspricht and the top of the root of reparieren. The bottom feature structure of the lower S node is the uniﬁcation of the bottom of the foot of verspricht and the bottom of the root of reparieren. In this sense, the root of the reparieren tree gets split into two parts. The upper part merges with the root node of the verspricht tree, and the lower part merges with the foot node of the verspricht tree. 6 In a way, the idea of node sharing is already present in description-based deﬁnitions of TAG-related formalisms (see Vijay-Shanker 1992; Rogers 1994; Kallmeyer 2001). This is why these formalisms are monotonic with respect to the node properties described in the tree descriptions. However, the possibility of exploiting this in order to obtain multiple adjunctions combined with multicomponent tree descriptions has not been pursued so far. 193  Computational Linguistics  Volume 31, Number 2  Figure 3 Derivation of (1) dass es der Mechaniker zu reparieren verspricht (‘that the mechanic promises to repair it’) using shared nodes. The notion of shared nodes means in particular that a node can be used for more than one adjunction. (E.g., in Figure 3, two trees were adjoined at the root of the reparieren tree.) A similar idea has led to the deﬁnition of extended derivation in Schabes and Shieber (1994). For certain auxiliary trees, Schabes and Shieber allow more than one adjunction at the same node. However, the deﬁnition of the derived tree in Schabes and Shieber (1994) is such that if ﬁrst β1 and then β2 are adjoined at some node µ (i.e., in the derivation tree there are edges from some γ to β1 and β2, both with the position p of the node µ in γ), then ﬁrst the whole tree derived from β1 is added to position p, and afterwards the whole tree derived from β2 is added to position p. In other words, before β2 is adjoined, all the trees to be added by adjunction or substitution to β1 must be added. This is different in the case of shared nodes: After β1 and then β2 have been adjoined, the root node of β2 in the derived tree is shared by β1 and β2 and consequently can be used for adjunctions at β1.7 In other words, trees to be adjoined at the roots of β1 and β2 can be adjoined in any order. This is important for obtaining all the possible permutations of scrambled elements. 2.2 Formal Deﬁnition of Tree-Local MCTAG with Shared Nodes As already mentioned, the idea of tree-local MCTAG with shared nodes is the following: In the case of a substitution of an elementary tree α into an elementary tree γ, in the resulting tree, the root node of the subtree α is considered to be part of α and of γ. Similarly, when an elementary tree β is adjoined at a node that is part of the elementary trees γ1, . . . , γn, then in the resulting tree, the root and foot node of β are both considered to be part of γ1, . . . , γn and β. Consequently, if an elementary tree γ is added to an elementary tree γ, and if there is then a sequence of adjunctions at root 7 In this case, one obtains crossed dotted edges in the SN-derivation structure deﬁned later (see Figure 14 for an example). 194  Kallmeyer  Multicomponent TAGs with Shared Nodes  or foot nodes starting from γ , then each of these adjunctions can be considered an adjunction at γ, since it takes place at a node shared by γ, γ , and all the subsequently adjoined trees. Therefore, one way to deﬁne SN-MCTAG refers to the standard TAG derivation tree in the following way. Deﬁne the grammar as an MCTAG and then allow only derivation trees that satisfy the following tree-locality condition: For each instance {γ1, . . . , γk} of an elementary tree set in the derivation tree, there is a γ such that each of the γi is either a daughter of γ or is linked to one of the daughters of γ by a chain of adjunctions at root or foot nodes. As an example, consider the derivation tree for (1) in Figure 3. It shows that the trees used in the derivation are the reparieren tree, the verspricht tree, the Mechaniker tree, and the two trees es and -es from the tree set in Figure 3. -es is substituted into reparieren at position 21, and verspricht is adjoined to reparieren at position . Then, Mechaniker is substituted into verspricht at position 1, and es is adjoined to verspricht at position . The derivation is tree-local in the node-sharing sense, since for the tree set { -es, es}, -es is a daughter of reparieren in the derivation tree and es is linked to reparieren by a ﬁrst adjunction of verspricht to reparieren and a further adjunction of es to the root of verspricht. In the following, we adopt this way of viewing derivations in SN-MCTAG as speciﬁc multicomponent TAG derivations; that is, we deﬁne SN-MCTAG as a variant of MCTAG. This avoids formalizing a notion of shared nodes, even though this was the starting motivation for the formalism. We assume a deﬁnition of TAG as a tuple G = I, A, N, T with I being the set of initial trees, A the set of auxiliary trees, and N and T the nonterminal and terminal node labels, respectively (see, for example, Vijay-Shanker [1987] for a formal deﬁnition of TAG). Now we formally introduce multicomponent tree-adjoining grammars (Joshi 1987; Weir 1988): Deﬁnition 1 A multicomponent tree-adjoining grammar is a tuple G = I, A, N, T, A such that r GTAG := I, A, N, T is a TAG; r A ⊆ P(I ∪ A) is a set of subsets of I ∪ A, the set of elementary tree sets.8 γ ⇒ γ is a multicomponent derivation step in G iff there is an instance {γ1, . . . , γn} of an elementary tree set in A and there are pairwise different node addresses p1, . . . , pn such that γ = γ[p1, γ1] . . . [pn, γn], where γ[p1, γ1] . . . [pn, γn] is the result of adding the γi (1 ≤ i ≤ n) at node positions pi in γ.9 As in TAG, a derivation starts from an initial tree, and in the end, in the ﬁnal derived tree, there must not be any obligatory adjunction constraint, and all leaves must be labeled by a terminal or by the empty word. In each MCTAG derivation step, an elementary tree set is chosen, and the trees from this set are added to the already derived tree. Since they are added to pairwise different 8 P(X) is the set of subsets of some set X. 9 As usual (see Vijay-Shanker 1987; Weir 1988), γ[p, γ ] is deﬁned as follows: If γ is (derived from) an initial tree and the node at position p in γ is a substitution node, then γ[p, γ ] is the tree one obtains by substitution of γ into γ at node position p. If γ is (derived from) an auxiliary tree and the node at position p in γ is an internal node, then γ[p, γ ] is the tree one obtains by adjunction of γ to γ at node position p. Otherwise γ[p, γ ] is undeﬁned. 195  Computational Linguistics  Volume 31, Number 2  nodes, one can just as well add them one after the other; that is, each multicomponent derivation in an MCTAG G = I, A, N, T, A corresponds to a derivation in the TAG GTAG := I, A, N, T . Let us deﬁne the TAG derivation tree of such a multicomponent derivation as the corresponding derivation tree in GTAG. We can then deﬁne tree-local, set-local, and nonlocal MCTAG and also the different variants of SN-MCTAG this article deals with by putting different constraints on this derivation tree.10 Note that for each operation γ[p, γi], the node address p in the derived tree γ points at a node that is at some address p in some elementary tree γ that was already added (γ and p are unique). In the TAG derivation tree, there will be in this case an edge from γ to γi with position p . A TAG derivation tree can be considered a tuple of nodes and edges. As usual in ﬁnite trees, the edges are directed from the mother node to the daughter. Linear precedence is not needed in a derivation tree, since it does not inﬂuence the result of the derivation. So a derivation tree is a tuple N , E , with N being a ﬁnite set of instances of elementary trees and with E ⊂ N × N × IN∗, where IN∗ is the set of Gorn addresses. We deﬁne the parent relation as the relation between mothers and daughters in a derivation tree, the dominance relation as the reﬂexive transitive closure of the parent relation, and the node-sharing relation as the relation between nodes that either are mother and daughter or are linked ﬁrst by a substitution/adjunction and then a chain of adjunctions at root or foot nodes: Deﬁnition 2 Let D = N , E be a derivation tree in a TAG. r PD := { n1, n2 | n1, n2 ∈ N , and there is a p ∈ IN∗ such that n1, n2, p ∈ E} is the parent relation in D. r DD := { n1, n2 | n1, n2 ∈ N , and either n1 = n2, or there is a n3 such that n1, n3 ∈ PD and n3, n2 ∈ DD} is the dominance relation in D. r SN D := { n1, n2 | either n1, n2 ∈ PD or there are t1, . . . , tk ∈ N , such that n1, t1 ∈ PD, n2 = tk and for all j, 1 ≤ j ≤ k − 1: ti, ti+1, p ∈ E with either p = or ti being an auxiliary tree with foot node address p } is the node-sharing relation in D. A node-sharing relation γ1, γ2 that corresponds to an actual parent relation is called a primary node-sharing relation, and γ2 is called a primary SN-daughter of γ1, whereas any other node-sharing relation γ1, γ2 is called secondary and in this case γ2 is called a secondary SN-daughter of γ1. The TAG derivation trees for MCTAG derivations have certain properties resulting from the requirement that the elements of instances of elementary tree sets must be added simultaneously to the already derived tree: First, if an elementary tree set is used, then all trees from this set must occur in the derivation tree. Secondly, one tree from an elementary tree set cannot be substituted or adjoined into another tree from the same set, and, thirdly, two tree sets cannot be interleaved. For nonlocal MCTAG, these are all constraints the TAG derivation tree needs to satisfy.  10 This TAG derivation tree is not the MCTAG derivation tree deﬁned in Weir (1988). The nodes of Weir’s MCTAG derivation trees are labeled by sequences of elementary trees (i.e., by elementary tree sets), and each edge stands for simultaneous adjunctions/substitutions of all elements of such a set. 196  Kallmeyer  Multicomponent TAGs with Shared Nodes  Lemma 1 Let G = I, A, N, T, A be an MCTAG, GTAG := I, A, N, T . Let D = N , E be a derivation tree in GTAG with the corresponding derived tree t being in L(GTAG). D is a possible TAG derivation tree in G with t ∈ L(G) iff D is such that r (MC1) The root of D is an instance of an initial tree α ∈ I and all other nodes are instances of trees from tree sets in A such that for all instances Γ of elementary tree sets from A and for all γ1, γ2 ∈ Γ, if γ1 ∈ N , then γ2 ∈ N . r (MC2) For all instances Γ of elementary tree sets from A and for all γ1, γ2 ∈ Γ, γ1 = γ2: γ1, γ2 ∈ DD. r (MC3) For all pairwise different instances Γ1, Γ2, . . . , Γn, n ≥ 2, of elementary tree sets from A, there are no γ1(i), γ2(i) ∈ Γi, 1 ≥ i ≥ n, such that γ1(1), γ2(n) ∈ DD and γ1(i), γ2(i−1) ∈ DD for 2 ≥ i ≥ n. The proof of this lemma is given in the appendix. The lemma gives us a way to characterize nonlocal MCTAG via the properties of the TAG derivation trees the grammar licenses. With this characterization we get rid of the original simultaneity requirement: The corresponding properties are now captured in the three constraints (MC1)–(MC3). But since these constraints need to hold only for the TAG derivation trees that correspond to derived trees in the tree language, subderivation trees need not satisfy them. In other words, γ1 and γ2 from the same elementary tree set can be added at different moments of the derivation as long as the ﬁnal complete TAG derivation tree satisﬁes (MC1)–(MC3). We now deﬁne tree-local, set-local, SN-tree-local, and SN-set-local TAG derivation trees by imposing further conditions. Basically, the difference between the ﬁrst two and their SN variants is that in the ﬁrst two, the deﬁnition refers to the parent relation, whereas in the second two, it refers to the node-sharing relation. Deﬁnition 3 Let G = I, A, N, T, A be an MCTAG. Let D = N , E be a TAG derivation tree for some t ∈ L( I, A, N, T ). r D is a multicomponent derivation tree iff it satisﬁes (MC1)–(MC3). r D is tree-local iff for all instances {γ1, . . . , γn} of elementary tree sets with γ1, . . . , γn ∈ N , there is one γ such that γ, γ1 , . . . , γ, γn ∈ PD. r D is set-local iff for all instances {γ1, . . . , γn} of elementary tree sets with γ1, . . . , γn ∈ N , there is an instance Γ of an elementary tree set such that for all 1 ≤ i ≤ n, there is a ti ∈ Γ with ti, γi ∈ PD. r D is SN-tree-local iff for all instances {γ1, . . . , γn} of elementary tree sets with γ1, . . . , γn ∈ N , there is one γ such that γ, γ1 , . . . , γ, γn ∈ SN D. r D is SN-set-local iff for all instances {γ1, . . . , γn} of elementary tree sets with γ1, . . . , γn ∈ N , there is an instance Γ of an elementary tree set such that for all 1 ≤ i ≤ n, there is a ti ∈ Γ with ti, γi ∈ SN D.  197  Computational Linguistics  Volume 31, Number 2  Figure 4 SN-MCTAG for {w3 | w ∈ T∗}. The formalism we are proposing for scrambling is MCTAG with SN-tree-local TAG derivation trees. We call these grammars tree-local MCTAGs with shared nodes: Deﬁnition 4 Let G be an MCTAG. G is a tree-local MCTAG with shared nodes iff the set of trees generated by G, LT(G), is deﬁned as the set of those trees that can be derived with an SN-tree-local multicomponent TAG derivation tree in G. As usual, the string language LS(G) is then deﬁned as the set of strings yielded by the trees in LT(G). All tree-adjoining languages can be generated by SN-MCTAGs, since a TAG corresponds to an MCTAG with unary multicomponent sets. For such an MCTAG, each TAG derivation tree is trivially SN-tree-local. In other words, in this case the tree sets are the same, whether the grammar is considered a TAG, a tree-local MCTAG, or an SN-MCTAG.11 In particular, all TAG analyses proposed so far can be maintained, since each TAG is trivially also an instance of SN-MCTAG. SN-MCTAG is a proper extension of TAG (and of tree-local MCTAG) in the sense that there are languages that can be generated by an SN-MCTAG but not by a TAG. As an example, consider Figure 4, which shows an SN-MCTAG for {www | w ∈ T∗}.12 Similar to the grammar in Figure 4, for all copy languages {wn | w ∈ T∗} for some n ∈ IN, an SN-MCTAG can be found. Other languages that can be generated by SN-MCTAG and that are not TALs are the counting languages {an1 . . . ank | n ≥ 1} for any k > 4 (for k ≤ 4, these languages are tree-adjoining languages). There are two crucial differences between V-TAG and SN-MCTAG: First, in VTAG, the adjunctions of auxiliary trees from the same set need not be simultaneous. In this respect, V-TAG differs not only from SN-MCTAG, but from any of the different 11 However, viewing a TAG as an SN-MCTAG allows us to obtain a richer set of SN-derivation structures, as introduced in the next section. This is exploited in Kallmeyer (2002) for semantics. 12 The subscript NA in the ﬁgure stands for null adjunction; that is, it disallows adjunctions at the node in question. 198  Kallmeyer  Multicomponent TAGs with Shared Nodes  MCTAGs mentioned above. Secondly, V-TAG is nonlocal in the sense of nonlocal MCTAG, whereas SN-MCTAG is local, even though the locality is not based on the parent relation in the derivation tree, as is the case in standard local MCTAG, but on the SNdominance relation in the derivation tree. As a consequence of the locality, we do not need dominance links (i.e., dominance constraints that have to be satisﬁed by the derived tree) in SN-MCTAG, in contrast to other TAG variants for scrambling. The locality condition put on the derivation sufﬁciently constrains the possibilities for attaching the trees from elementary tree sets: Different trees from a tree set attach to different nodes of the same elementary tree. Consequently, the dominance relations among these different nodes determine the dominance relations among the different trees from the tree set. Therefore extra dominance links are not necessary. This is different for nonlocal TAG variants such as V-TAG or DSG, in which one can in principle attach the different components of an elementary structure at arbitrary nodes in the derived tree. 2.3 SN-MCTAG and Scrambling: Formal Considerations Figure 5 shows an SN-MCTAG generating a language that cannot even be generated by linear context-free rewriting systems (see Becker, Rambow, and Niv [1992] for a proof), and therefore not by set-local MCTAG. This example, however, concerns neither weak nor strong generative capacity, but something that Becker, Rambow, and Niv (1992) call derivational capacity: the derivation of nkvk must be such that the π(i)th n and the ith v come from the same elementary tree set in the grammar. The grammar in Figure 5 works in the following way: Each derivation starts with α. Then a ﬁrst instance of the tree set (yielding n1 and v1) is added to the N and V nodes in α. For each further instance of the tree set (yielding ni and vi), βv is adjoined to the root node of the βv tree of vi−1. Therefore all βv adjunctions except the ﬁrst are occurring at root nodes, and consequently all βv are (primary or secondary) SN-daughters of α. The βn tree of ni can be adjoined to any of the root or foot nodes of the βn that have already been added, since in this way all adjunctions of βn except the ﬁrst one occur at root or foot nodes, and therefore all these βn are SN-daughters of α. This allows us to place ni at any position in the string already containing {n1, . . . , ni−1}, and thereby any permutation of the ns can be obtained. Since all nodes in the derivation tree are SNdaughters of α, the derivation is SN-tree-local. Note that in the grammar in Figure 5, there is no NA constraint on the foot node of the ﬁrst auxiliary tree in the tree set. This is crucial for allowing all permutations of the n1, . . . , nk. In this respect, the elementary trees differ from what is usually done in TAG. Becker, Rambow, and Niv (1992) argue that a formalism that cannot generate the language in Figure 5 is not able to analyze scrambling in an adequate way. We think,  Figure 5 SN-MCTAG for {nπ(k) . . . nπ(1)vk . . . v1 | k ≥ 0, ni = n, vi = v, and ni and vi are in the same elementary tree set and they were added in the ith derivation step for all i, 1 ≤ i ≤ k, and π is a permutation of (1, . . . , k)}. 199  Computational Linguistics  Volume 31, Number 2  Figure 6 Predicate argument structure for SCR. however, that this language is not exactly what one needs for scrambling. The assumption underlying the language in Figure 5 is that ni is an argument of vi. But in this case, instead of adding ni and vi at the same time, ni should be added to vi. If one makes the additional assumption that argument NPs are added by substitution, then one can require that the argument NPs have already been substituted (this is what Joshi, Becker, and Rambow [2000] call the weak co-occurrence constraint), that is, that the tree for vi contain ni. In this case, the language in Figure 5 is an appropriate test language for scrambling. But we do not want to make this assumption. Furthermore, there are more predicate-argument dependencies: vi is also an argument of vi−1 for i ≤ 2. This is what Joshi, Becker, and Rambow (2000) call the strong co-occurrence constraint. In other words, the dependency tree should be as in Figure 6. Additionally to the permutation of the n1, . . . , nk, also the vi can be moved leftward, as long as they do not permute among themselves. Consequently, for scrambling data (without extraposition), one rather wants to generate the following language: SCR := {w = π(n1 . . . nkv1 . . . vk) | k ≥ 1, ni = n, vi = v, for all 1 ≤ i ≤ k, and π is a permutation of n1 . . . nkv1 . . . vk such that ni precedes vi in w for all 1 ≤ i ≤ k and vi precedes vi−1 in w for all 1 < i ≤ k} with the derivation structure in Figure 6. An SN-MCTAG generating this language is shown in Figure 7. The SN-MCTAG in Figure 7 yields the following derivations: Either start with α2, in which case an instance of {βn, αn} must be added and nv is obtained with n depending on v, or start with α1 for v1, in which case, for all v except the leftmost one, the set {βv1, αv1 } is added, for the leftmost v, a set {βv2, αv2} is added, and for all the ns, sets {βn, αn} are added. These sets can be added in any order; the auxiliary tree is always adjoined to the root node of the already derived tree that is shared by all auxiliary trees that have been used so far and by the ﬁrst α1. The initial tree is primarily substituted Figure 7 SN-MCTAG for SCR. 200  Kallmeyer  Multicomponent TAGs with Shared Nodes  into the argument slot it ﬁlls. So the only condition for adding such a tree set is that the verb it depends on has already been added, since the tree of this verb provides the substitution node for the initial tree. Therefore, since the lexical material is always left of the foot node, one obtains that vi precedes vi−1 for all 1 < i ≤ k and ni precedes vi for all 1 ≤ i ≤ k. Note that in Figure 7, for a scrambled ni, the substitution node is ﬁlled with an empty node, while the n is adjoined higher at a node that is not yet available in the elementary structure of vi. So the combination of ni and vi cannot be precompiled here. 2.4 Restricted SN-MCTAG When the formal properties of SN-MCTAG are examined, it becomes clear that the formalism is hard to compare to other local TAG-related formalisms, since in the derivation tree, arbitrarily many trees can be secondary SN-daughters of a single elementary tree, such that these secondary links are considered to be adjunctions to that tree. This means that these secondary links are relevant for the SN-tree-locality of the derivation. An example is the grammar in Figure 5, in which in each derivation step, the relevant nodesharing relations are the links between α and the two auxiliary trees of the new set. This means that for a word of length k, there are k SN-daughters of α that are relevant for the SN-tree-locality of the derivation. The grammar in Figure 5 indicates that this property of SN-MCTAG is at least partly responsible for the fact that SN-MCTAG allows us to generate languages that are not even mildly context-sensitive (i.e., that are not in the class of languages that can be generated by LCFRS). However, it would be desirable to stay inside the class of mildly context-sensitive languages. Therefore, in the following, we deﬁne a restricted version, RSN-MCTAG, that limits the number of relevant secondary SN-daughters of an elementary tree. The restriction is obtained as follows: We require that in each derivation step, among the SN-relations between the old γ and the new set Γ, there be at least one primary SN-relation. The number of primary SN-daughters of a speciﬁc elementary tree is limited, since the primary SN-daughters correspond to substitutions/adjunctions at pairwise different nodes and the number of nodes in an elementary tree is limited. Consequently, the number of relevant secondary SN-daughters for a node is limited as well. An example of a derivation satisfying the new constraint is that in Figure 3, in which es is a secondary SN-daughter of reparieren, while the second element of the tree set, -es, is a primary SN-daughter of reparieren. Deﬁnition 5 r Let G = I, A, N, T, A be an MCTAG. Let D = N , E be the TAG derivation tree of a tree t ∈ LT( I, A, N, T ). D is RSN-tree-local iff for all instances {γ1, . . . , γn} of an elementary tree set with γ1, . . . , γn ∈ N , there is one γ such that 1. γ, γ1 , . . . , γ, γn ∈ SN D; 2. there is one i, 1 ≤ i ≤ n, with γ, γi ∈ PD. r An MCTAG G is called a restricted SN-MCTAG iff the set of trees generated by G, LT(G), is deﬁned as the set of those trees that can be derived with an RSN-tree-local multicomponent TAG derivation tree in G.  201  Computational Linguistics  Volume 31, Number 2  The ﬁrst condition of the deﬁnition says that the grammar is SN-tree-local, and the second condition ensures that at least one of the relevant SN-daughters of γ is a primary SN-daughter, that is, an actual daughter of γ. As for SN-MCTAG, all tree-adjoining languages can also be generated by RSNMCTAGs. The sample grammars in Figures 4 and 5 are not RSN-MCTAGs. We suspect that there is no RSN-MCTAG that generates the language in Figure 5. But the grammar in Figure 7 for the language SCR is an RSN-MCTAG. It can be shown that for the TAG derivation trees of an RSN-MCTAG, the following holds: For each instance of an elementary tree set Γ, the γ to which all elements of Γ are linked by node-sharing relations with at least one primary link is unique (which is not necessarily the case for general SN-MCTAG). This is formulated in the following lemma: Lemma 2 Let G = I, A, N, T, A be an RSN-MCTAG. Let D = N , E be a TAG derivation tree in G. Then for all instances {γ1, . . . , γn} of elementary tree sets with γ1, . . . , γn ∈ N , there is exactly one γ such that γ, γ1 , . . . , γ, γn ∈ SN D, and there is one i, 1 ≤ i ≤ n, with γ, γi ∈ PD. For such an elementary tree set {γ1, . . . , γn}, with γ being the unique elementary tree as described in the lemma, all γ, γi ∈ SN D \ PD, 1 ≤ i ≤ n, are called secondary adjunction links in D. The proof of the lemma is given in the appendix. Now we introduce the SN-derivation structure of a TAG derivation tree D in an RSN-MCTAG. It consists of D enriched with additional links for the secondary adjunctions. These links are equipped with the positions of the ﬁrst substitutions/adjunctions on the chain that corresponds to the secondary adjunctions. Deﬁnition 6 Let G = I, A, N, T, A be an RSN-MCTAG. Let D = N , E be a TAG derivation tree in G. The SN-derivation structure of D, DSN, is then DSN := N , E , with E ⊆ E . r For all secondary adjunction links γ1, γ2 in D with γ and p such that γ1, γ , p ∈ E and γ , γ2 ∈ DD: γ1, γ2, p ∈ E . r These are all elements of E . All e ∈ E are called primary edges in DSN, and all e ∈ E \ E are called secondary edges in DSN. With the notion of the SN-derivation structure, we can formulate the limitation on the maximal number of secondary adjunctions to an elementary tree that we mentioned at the beginning of this section: Lemma 3 Let G = I, A, N, T, A be an RSN-MCTAG. Then there is a constant c such that for all TAG derivation trees D in G with SN-derivation structure DSN := N , E , the following holds: 202  Kallmeyer  Multicomponent TAGs with Shared Nodes  There is no n ∈ N such that there exist m ≥ c + 1 pairwise different n1, . . . , nm such that for all i, 1 ≤ i ≤ m, there is a p such that r either n, ni, p is a secondary edge in DSN; r or n, ni, p is a primary edge in DSN, and there are no n and p such that n , ni, p is a secondary edge in DSN. That this lemma holds is nearly immediate: Each secondary adjunction must be associated with a primary adjunction or substitution into the same tree instance. There are at most k primary adjunctions or substitutions into any tree instance if k is the maximal number of nodes per elementary tree. Consequently there are at most k × n secondary adjunctions per node if n + 1 is the maximal number of trees per elementary tree set. In linguistic applications, the SN-derivation structure is intended to reﬂect the predicate-argument dependencies of a sentence in the following way: For each tree in the SN-derivation structure, if this tree is secondarily adjoined to some other tree γ, then it depends on γ. Otherwise it depends on its mother node in the TAG derivation tree. In this way, the grammar for SCR in Figure 7 yields the desired dependency structure.  3. Linguistic Applications  3.1 Scrambling with RSN-MCTAG  In this section, we present a small German grammar that allows us to analyze some cases of scrambling. The aim is not an exhaustive treatment of the phenomenon, but just to show that in principle, an analysis of scrambling in German is possible using RSN-MCTAG. The data to which we restrict ourselves are word order variations of example (3) without extraposition, that is, under the assumption that the order of the verbs is zu reparieren zu versuchen verspricht:  (3) . . . dass er dem Kunden das Fahrrad zu reparieren . . . that henom the customerdat the bikeacc to repair  zu versuchen verspricht  to try  promises  ‘. . . that he promises the customer to try to repair the bike’  The elementary trees and tree sets for example (3) are shown in Figure 8. In contrast to standard TAG practices, which are often guided by technical considerations, we represent all arguments of a verb (including an embedded VP) by substitution nodes. For those parts that might be scrambled, there is a single elementary tree (for the case without scrambling) and a tree set used for scrambling. The tree set contains an auxiliary tree that can be primarily or secondarily adjoined to some root node and a tree with the empty word that is intended to ﬁll the argument position. In order to avoid spurious ambiguities, we assume that whenever a derivation using the single elementary tree is possible, this is chosen. A scrambled element always adjoins to a VP node, and the scrambled element is to the left of the foot node. Therefore it precedes everything that is below or on the  203  Computational Linguistics  Volume 31, Number 2  Figure 8 Elementary trees for scrambling. right of the VP node to which it adjoins. Consequently, given the form of the verbal elementary trees in Figure 8, in which the verb is always below or to the right of all VP nodes allowing adjunction, the order x v for an x being a nominal or a verbal argument of v is always respected. For an element (a lexical item), the tree set for scrambling is used whenever one of the following three cases holds: r The element is scrambled. r Scrambling of depth more than one out of the element takes place. r The element intervenes between some element A (on its right) and some element B (on its left) scrambled out of A, and the element itself does not belong to A. In other words, the fact that the set for scrambling is used for some element does not necessarily mean that this element is scrambled. It just means that one of the three cases above holds, that is, that some scrambling around this element takes place. One could actually do without the single trees and always use the tree sets. In this case, even if no scrambling took place, all argument slots would be ﬁlled by empty words, and all lexical material would be adjoined to the root node of the derived tree. At ﬁrst glance, this seems rather odd. But if one does not consider the substitution nodes argument slots but rather some kind of subcategorization features marking which arguments need to be added, an analysis using only the tree sets makes sense. However, for this article, we keep the single trees. For example (3), a derivation without secondary adjunctions and using only the single trees is possible. Let us consider the following word orders as examples of how secondary adjunction is used for scrambling: 204  Kallmeyer  Multicomponent TAGs with Shared Nodes  (4) . . . dass er1 [[das Fahrrad zu reparieren] zu versuchen]2 t1 dem Kunden t2  . . . that he the bike to repair to try  the customer  verspricht promises  (5) . . . dass er [das Fahrrad zu reparieren]1 dem Kunden [t1 zu versuchen]  . . . that he the bike to repair  the customer to try  verspricht promises  (6) . . . dass [das Fahrrad]1 er2 [[t1 zu reparieren] zu versuchen]3 t2 dem Kunden t3  . . . that the bike he to repair to try  the customer  verspricht promises  In example (4), the versuchen-VP and er are scrambled.13 Consequently, for versuchen and er, the sets with two trees are used, whereas for all the other elements, the single trees can be used. In example (5), the reparieren-VP is scrambled out of the versuchen-VP, with dem Kunden intervening between the two. Therefore, the tree sets are used for reparieren and dem Kunden. For versuchen, the single tree can be used, since the scrambling out of versuchen is of depth one. In example (6), we have the same scrambling as in example (4), and additionally, das Fahrrad is scrambled out of the reparieren-VP and the versuchen-VP (depth two). Consequently, in this case one needs tree sets for Fahrrad, er, versuchen, and reparieren. Let us consider the analysis of example (4): Starting with verspricht, the single tree for dem Kunden and the tree set for versuchen (with adjunction of the auxiliary tree at the root) are added. This leads to the ﬁrst tree in Figure 9. The VP nodes in boldface type in the ﬁgure are shared by versuchen and verspricht; that is, they can be used for further adjunction at the verspricht tree. (Of course, only the root node can be used for adjunction, since the other nodes have NA constraints.) It does not matter in which order er and zu reparieren are added. For er, the tree set is used. The auxiliary tree is secondarily adjoined to the root node, and the initial tree is substituted for the NPnom node in the verspricht tree. This leads to the second tree in Figure 9. For reparieren and das Fahrrad, the single trees are added below the VP substitution node in the versuchen tree. The corresponding SN-derivation structure (see Figure 9) contains the desired predicateargument dependencies. The TAG derivation tree is RSN-tree-local. Next, let us consider example (5). Here, the single trees for er and versuchen are added to verspricht. This leads to the ﬁrst tree in Figure 10. The VP node in boldface type in the ﬁgure belongs to verspricht and versuchen. It is next used for secondary adjunction of dem Kunden to the verspricht tree. The initial tree is substituted at the NPdat slot. This leads to the second tree. Here, the bold VP node belongs to verspricht, versuchen, and Kunde. It is next used for secondary adjunction of the auxiliary tree of reparieren to versuchen, while the initial tree is substituted for the VP leaf in the versuchen tree. This  13 Actually, er here is not really scrambled, but since in our formalism, scrambled elements attach at the left of a VP, any other element even more to the left is treated as if it is scrambled (even if it depends on the matrix verb).  205  Computational Linguistics  Volume 31, Number 2  Figure 9 Analysis of example (4). Figure 10 Analysis of example (5). leads to the third tree. After that, one needs only to add the single tree for das Fahrrad to reparieren. Note that this is a derivation in which the foot node of the elementary tree containing the lexical material does not dominate the tree with the empty word. Now let us consider the derivation of example (6). Here, only for dem Kunden, the single tree is added by substitution. In all other cases, the tree set is used with (primary 206  Kallmeyer  Multicomponent TAGs with Shared Nodes  or secondary) adjunction at the root node of the already derived tree. This root node consequently belongs to all verbs that have already occurred in the derivation and can therefore be used to add arguments to any of them. We leave it to the reader to verify that all word orders can be generated. This kind of analysis also works for more than two embeddings. Since all scrambled elements attach to a VP node in the elementary tree of the verb they depend on, they cannot attach to the VP of a higher ﬁnite verb that embeds the sentence in which the scrambling occurs. In this way, a barrier effect is obtained without establishing any explicit barrier, as is done in V-TAG. Instead, this locality of scrambling is a consequence of the form of the elementary trees and of the locality of the derivations. Concerning adjunct scrambling, each adjunct has a single auxiliary tree as in standard TAG and additionally a set of two auxiliary trees, a lower auxiliary tree with an empty word and a higher auxiliary tree with the adjunct. This is shown in Figure 11. The internal VP node of the higher tree in the tree set serves as an adjunction site for the lower parts of other adjuncts. Similarly, the elementary trees of verbs need an extra VP node in order to adjoin adverbs. For more analyses of scrambling, including scrambling in combination with extraposition and topicalization, and also for an extension of the analysis presented here to Korean data, see Kallmeyer and Yoon (2004). 3.2 Raising Verbs and Subject-Auxiliary Inversion Other phenomena often mentioned in the TAG literature (see, e.g., Rambow, VijayShanker, and Weir 1995; Kulick 2000; Dras, Chiang, and Schuler 2004) as being problematic for TAG and tree-local MCTAG are sentences with raising verbs and subject-auxiliary inversion, as in examples (7) and (8): (7) Does Gabriel seem to be likely to eat gnocchi? (8) What does John seem to be certain to like? The standard TAG analyses of examples (7) and (8) (see Figure 12 for the analysis of example (8)) start with the eat and like tree, respectively, adjoin an auxiliary tree for likely and certain, respectively, and then add the trees for does and seem, respectively. If we assume that these trees are in the same elementary tree set, then this last derivation step is nonlocal, since the does tree adjoins to eat and like, respectively, while the seem tree adjoins to likely and certain, respectively. Though different from scrambling, this problem seems to be of a similar nature, and formalisms that have been proposed for scrambling have also been used to treat these examples (see Kulick 2000). RSN-MCTAG allows us to analyze examples (7) and (8) in a way that puts does and seem into a single elementary tree set: After having adjoined to be likely and to be certain,  Figure 11 Trees for adjuncts. 207  Computational Linguistics  Volume 31, Number 2  Figure 12 Derivation for (8).  respectively, the root nodes of the adjoined trees are considered still to be part of the elementary trees of eat and like, respectively. These elementary trees can then be used to add the elementary tree set for does and seem: Both auxiliary trees are adjoined to these trees. Figure 12 shows the corresponding SN-derivation structure.  4. RSN-MCTAG and Range Concatenation Grammar  In the following, we show that for each RSN-MCTAG of a certain type (i.e., with an additional restriction), a weakly equivalent simple range concatenation grammar (Boullier 1999, 2000) can be constructed. It has been shown that RCGs generate exactly the class of all polynomially parsable languages (Bertsch and Nederhof 2001; appendix A). Furthermore, as shown in Boullier (1998b), simple RCGs in particular are even weakly equivalent to linear context-free rewriting systems (Weir 1988). As a consequence, one obtains that the languages generated by simple RSN-MCTAGs are mildly context-sensitive. This last property was introduced in Joshi (1985). It includes formalisms that are polynomially parsable, are semilinear, and allow only a limited number of crossing dependencies. (We do not give formal deﬁnitions of mild contextsensitivity and of LCFRS, since we do not need these deﬁnitions in this article.) Concerning RSN-MCTAGs in general, that is, without any further restriction, we are almost sure that they are not mildly context-sensitive. Perhaps they can even generate languages that are not in the class of languages generated by RCGs.  4.1 Range Concatenation Grammars  This section deﬁnes range concatenation grammars.14  Deﬁnition 7 A range concatenation grammar is a tuple G = N, T, V, S, P such that  r N is a ﬁnite set of predicates, each with a ﬁxed arity;  r T and V are disjoint ﬁnite sets of terminals and of variables;  r S ∈ N is the start predicate, a predicate of arity 1;  r P is a ﬁnite set clauses of the form A0(x01, . . . , x0a0 ) → ,  or A0(x01, . with n ≥ 1  . . , x0a0 ) → A1(x11 and Ai ∈ N, xij ∈  ,..., (T ∪  Vx1)a∗1  ) . . . An(xn1, . . . , xnan ), and ai being the arity  of  Ai.  14 Since throughout the article, we use only positive RCGs; whenever we say “RCG,” we actually mean “positive RCG.”  208  Kallmeyer  Multicomponent TAGs with Shared Nodes  When applying a clause with respect to a string w = t1 · · · tn, the arguments of the predicates in the clause are instantiated with substrings of w, more precisely, with the corresponding ranges. A range i, j with 0 ≤ i < j ≤ n corresponds to the substring between positions i and j, that is, to the substring ti+1 · · · tj. If i = j, then i, j corresponds to the empty string . If i > j, then i, j is undeﬁned.  Deﬁnition 8 For a given clause, an instantiation with respect to a string w = t1 . . . tn consists of a function f : {t | t is an occurrence of some t ∈ T in the clause} ∪ V → { i, j | i ≤ j, i, j ∈ IN} such that r for all occurrences t of a t ∈ T in the clause: f (t ) := i, i + 1 for some i, 0 ≤ i < n, such that ti = t; r for all v ∈ V: f (v) = j, k for some 0 ≤ j ≤ k ≤ n; r if consecutive variables and occurrences of terminals in an argument in the clause are mapped to i1, j1 , . . . , ik, jk for some k, then jm = im+1 for 1 ≤ m < k. By deﬁnition, we then state that f maps the whole argument to i1, jk . The derivation relation is deﬁned as follows. For a predicate A of arity k, a clause A(. . .) → . . . , and ranges i1, j1 , . . . , ik, jk with respect to a given w: If there is an instantiation of this clause with left-hand side A( i1, j1 , . . . , ik, jk ), then A( i1, j1 , . . . , ii, jk ) can be replaced with the right-hand side of this instantiation. The language of an RCG G is the set of strings that can be reduced to the empty word, that is, {w | S( 0, |w| ) ⇒∗ with respect to w}.15 An RCG with maximal predicate arity n is called an RCG of arity n. For illustration, let us consider a sample RCG: The RCG with N = {S, A, B}, T = {a, b}, V = {X, Y, Z}, start predicate S, and clauses S(X Y Z) → A(X, Z) B(Y), A(a X, a Y) → A(X, Y), B(b X) → B(X), A( , ) → , B( ) → has the string language {anbkan | k, n ∈ IN}. Consider the reduction of w = aabaa: We start from S( 0, 5 ). First we can apply the following clause instantiation:  S(X Y  Z ) → A(X , Z ) B(Y )  0, 2 2, 3 3, 5  aa  b  aa  0, 2 3, 5 2, 3  aa  aa  b  With this instantiation, S( 0, 5 ) ⇒ A( 0, 2 , 3, 5 )B( 2, 3 ). Then  B(b X ) → B(X )  2, 3 3, 3  3, 3  b  15 |w| is the length of the word w; that is, the range 0, |w| with respect to w corresponds to the whole word w. 209  Computational Linguistics  Volume 31, Number 2  and B( ) → lead to A( 0, 2 , 3, 5 )B( 2, 3 ) ⇒ A( 0, 2 , 3, 5 )B( 3, 3 ) ⇒ A( 0, 2 , 3, 5 ). Next,  A(a  X  a  Y ) → A(X , Y )  0, 1 1, 2 3, 4 4, 5  a  a  a  a  1, 2 4, 5  a  a  leads to A( 0, 2 , 3, 5 ) ⇒ A( 1, 2 , 4, 5 ). Then  A(a  X  a  Y ) → A(X , Y )  1, 2 2, 2 4, 5 5, 5  a  a  2, 2 5, 5  and A( , ) → lead to A( 1, 2 , 4, 5 ) ⇒ A( 2, 2 , 5, 5 ) ⇒ . An RCG is said to be noncombinatorial if each of the arguments in the right-hand sides of the clauses are single variables. It is said to be linear if no variable appears more than once in the left-hand sides of the clauses and no variable appears more than once in the right-hand side of the clauses. It is said to be nonerasing if for each clause, each variable occurring in the left-hand side occurs also in the right-hand side and vice versa. It is said to be simple if it is noncombinatorial, linear, and nonerasing. Simple RCGs and LCFRSs are equivalent (Boullier 1998b).  4.2 Relation between RSN-MCTAG and Simple RCG  The goal of this section is to construct an equivalent simple RCG for a given RSNMCTAG. In order to be able to perform this construction, in the following we further constrain the formalism of RSN-MCTAG by deﬁning RSN-MCTAG of a speciﬁc arity n. For this version of RSN-MCTAG, the construction of an equivalent simple RCG is possible. First, let us sketch the general idea of the transformation from TAG to RCG (see Boullier 1998a). The RCG contains predicates α (X) and β (L, R) for initial and auxiliary trees, respectively. X covers the yield of α and all trees added to α, and L and R cover those parts of the yield of β (including all trees added to β) that are to the left and the right of the foot node of β. The clauses in the RCG reduce the argument(s) of these predicates by identifying those parts that come from the elementary tree α/β itself and those parts that come from one of the elementary trees added by substitution or adjunction. A sample TAG with an equivalent RCG is shown in Figure 13. For the construction of an equivalent RCG from a given RSN-MCTAG, we follow the same ideas while considering a secondary adjunction of β at some γ as adjunction at γ and not as adjunction at the elementary tree that is the mother node of β in the TAG derivation tree. There are two main differences between RSN-MCTAG and TAG that inﬂuence the construction of an equivalent RCG. First, more than one tree can be added to a node. Therefore we allow predicates of the form αβ1 . . . βk and β0β1 . . . βk . The ﬁrst means that at the node in question, ﬁrst α was added by substitution, and then β1 . . . βk (in this order) were secondarily adjoined. The second means that at the node in question (an internal node), ﬁrst β0 was primarily adjoined, and then β1 . . . βk were secondarily adjoined. Since the number of secondary adjunctions at a node is limited by some constant depending on the grammar  210  Kallmeyer  Multicomponent TAGs with Shared Nodes  Figure 13 A sample TAG and an equivalent RCG. (see Lemma 3), k is limited as well, and therefore this extension with respect to TAG adds only a ﬁnite number of predicates. Second, the contribution of an elementary tree α/β including the trees added to it can be separated into arbitrarily many parts. Since each of the arguments of the predicates in the RCG has to cover a true substring of the input string, one needs predicates of arbitrary arities, namely, α . . . (Ln, . . . , L1, X, R1, . . . , Rn) and β . . . (Ln, . . . , L1, L0, R0, R1, . . . , Rn), for the case where n auxiliary trees were added at the root of α/β that were actually secondarily adjoined at some higher tree such that these n trees separate the contribution of α/β into 2n + 1 / 2n + 2 parts, respectively. This extension is problematic, since it leads to an RCG with predicates of arbitrary arity: a dynamic RCG (Boullier 2001), a variant of RCG that is not polynomially parsable and that we therefore want to avoid. For this reason, we need an additional constraint on the RSN-MCTAGs we employ. An example in which the contribution of an elementary tree is separated into three different parts is example (9), analyzed with the RSN-MCTAG in section 3.1 (see Figure 14). In the derived tree, the VP das Fahrrad zu reparieren zu versuchen (the broken triangles), which is the contribution of versuchen, is separated into three parts,  Figure 14 Analysis of example (9). 211  Computational Linguistics  Volume 31, Number 2  since reparieren secondarily adjoins at versuchen and das Fahrrad secondarily adjoins at reparieren.  (9) . . . dass [das Fahrrad]1 er [t1 zu reparieren]2 dem Kunden [t2 zu versuchen]  . . . that the bike  he to repair  the customer to try  verspricht promises  The crucial point in example (9) is that in the SN-derivation structure (see Figure 14),  there are two crossings of secondary edges inside one group of secondary links. This  means that the contribution of versuchen is interrupted twice by arguments of verspricht  (by Kunde and er). In order to avoid predicates of arbitrary arity, we therefore limit  the number of crossings of secondary links. We deﬁne the arity of an RSN-MCTAG  depending on the maximal number of crossings that are allowed.  First, we deﬁne special subgraphs of the SN-derivation structure, secondary  groups. These are subgraphs consisting of a chain of one primary substitu-  tion/adjunction and subsequent adjunctions at root or foot nodes such that there are  secondary adjunctions along the whole chain. For example, the nodes verspricht, zu  versuchen, Kunde, zu reparieren, er, and Fahrrad in the SN-derivation structure in Figure 14  form such a group. For an SN-derivation structure of a certain arity, the number of  crossings of secondary edges inside a single secondary group is then limited: For an  SN-derivation structure of arity n, the number of crossings of secondary edges per  secondary  group  is  limited  to  n 2  − 1.  In  other  words,  if  i  is  the  maximal  number  of  crossings, then 2(i + 1) is the arity of the grammar. Of course, the arity is chosen such  that an equivalent RCG of the same arity can be constructed. TAG, for example, is a  grammar with 0 crossings, that is, an arity 2(0 + 1) = 2 if the grammar is viewed as an  SN-MCTAG, and the corresponding RCG is actually of arity 2.  Deﬁnition 9 Let DSN = N , E be a SN-derivation structure.  1.  r  N , E is a secondary group in DSN iff N = {n0, n1, . . . , nk} ⊆ N for some k > 1 such that there are  r  primary edges ni, ni+1, pi for 0 ≤ i < k with pi ∈ IN; E ⊆ E such that for all n, n ∈ N , p ∈ IN with n, n , p ∈ E: if  r  n, n ∈ N , then n, n , p ∈ E ; for all i, 0 < i < k, there are i1, i2 with i1 ≤ i ≤ i2, i1 = i2, such that  ni1 , ni2 , p ∈ E is a secondary edge in D for some p ∈ IN.  2. DSN is of arity n iff for each secondary group N , E in DSN with primary  edges n0, n1, p0 , n1, n2, p1 , . . . , nk−1, nk, pk−1 as above, there are at most  i  ≤  n 2  −  
Ambiguity is ubiquitous in natural language. It is most dramatic when it concerns the parsing of a sentence in examples such as The High Court judges rape and murder suspects. I heard a giant swallow after seeing a horse ﬂy. La petite brise la glace. (‘The girl breaks the mirror.’/‘The little breeze chills her.’) (from Fuchs 1996) However, the most common form of ambiguity concerns the meanings of individual words, as in the following examples: The minister decided to leave the party. (‘church minister’/‘government minister’, ‘drinks party’/‘political party’) He’s a curious individual. (‘odd’/‘nosey’) Je suis un imbe´cile. (‘I’m following an idiot.’/‘I am an idiot.’) ∗ IRIT, Universite´ de Toulouse III, 118 route de Narbonne, 31062 Toulouse, France. E-mail: cooper@irit.fr. Submission received: 12th September 2003; Revised submission received: 29th April 2004; Accepted for publication: 7th December 2004 © 2005 Association for Computational Linguistics  Computational Linguistics  Volume 31, Number 2  The last example involves homographs (different words which happen to be spelled the same). However, it should be noted that only a small percentage of word sense ambiguity is due to homography. (We obtained an estimate of approximately 2% by random sampling of English and French dictionaries [12, 21, 22, 24].) Many words have gained multiple senses by metonymy or by ﬁgurative or metaphorical uses. The resulting senses are sufﬁciently different to be considered by lexicographers as distinct concepts (e.g., political party/drinks party). In information retrieval systems with natural language interfaces (Mandala, Tokunaga, and Tanaka 1999; Stevenson and Wilks 1999) or in models of human language processing via networks of semantic links (Fellbaum 1998; Hayes 1999; Vossen 2001), a fundamental question is what should correspond to a basic semantic concept. Is it a word, a word sense, or a group of word senses? This article presents a stochastic model of the evolution of language which allows us to answer this question. Applying the model to statistics obtained from a large number of monolingual and bilingual dictionaries provides convincing evidence that neither words nor individual word senses (as identiﬁed by lexicographers) correspond to concepts, but rather groups of word senses. Our model demonstrates that each word represents, on average, about 1.3 distinct concepts. This can be compared with the average 2.0 distinct senses per word listed in the dictionaries. This model also allows us to propose a novel and formal deﬁnition of the word concept. There are clear applications in artiﬁcial intelligence (Mandala, Tokunaga, and Tanaka 1999; Stevenson and Wilks 1999), cognitive science (Cruse 1995), lexicography, and historical linguistics (Algeo 1998; Antilla 1989; Schendl 2001; Geeraerts 1997). 2. The Genesis of Word Senses Word origin and the evolution of spelling, pronunciation, and meaning have long been studied by etymologists. Etymology tells us that many words in everyday use have a history that can be traced back thousands of years (Onions 1966; Picoche 1992). This can be contrasted with the hundreds of new entries which lexicographers add to each new edition of a dictionary. These new entries are not only neologisms, but also new senses for existing words. The history of the variations in spelling and pronunciation of particular words are not of direct concern here. We are interested in how word, sense pairs enter (or leave) a language. For each such semantic change, we can try to identify the originator, the reason, and the mechanism by which it occurs. 2.1 Origins of Semantic Change Picoche (1992) states that the majority of words in French have a scholarly origin and were introduced by clerics, jurists, intellectuals, and scientists directly from Latin and Greek. However, it is clear that many words are of popular origin (e.g., bike, trainers, and OK in English or ve´lo, baskets, and OK in French) and have become accepted terms as the result of common use. The principal reason why new word, sense pairs are introduced is to adapt language to new communicative requirements (Schendl 2001). Discoveries and inventions can give rise to neologisms (e.g., kangaroo, quark, Internet) or new senses for existing words (e.g., the ‘armoured vehicle’ sense of tank which coexists with the earlier ‘large container’ sense). Another driving force in historical semantics is the human tendency toward efﬁciency of communication, by, for example, shortening words or expressions (e.g., clipping of omnibus to bus), ignoring unnecessary semantic distinctions, and inventing new words to replace long expressions. Other reasons for semantic change 228  Cooper  A Mathematical Model of Historical Semantics  have more to do with human psychology than with practical necessity. Taboo leads to the introduction of slang words or euphemisms, such as to terminate for ‘to kill’ or senior for ‘old’. Litotes is a special case in which a word is replaced by the negation of its opposite (e.g., not bad for ‘good’). New words may be employed to make an old product sound more modern, exotic, or appetizing (e.g., the old-fashioned British word chips is often replaced by french fries or frites on menus). The human tendency to emphasize or exaggerate leads to the replacement of severe by horriﬁc or very by awfully (Schendl 2001). 2.2 Mechanisms of Semantic Change We can divide the mechanisms for neologism into three categories (Algeo 1998; Antilla 1989; Chalker and Weiner 1994; Gramley 2001; Schendl 2001; Stockwell and Minkova 2001): 1. Word-creation from no previous etymon. This is rare but is the most likely explanation for echoic words such as vroom, cuckoo, oh! (Bloomﬁeld 1933). 2. Borrowing from another language. This includes loan words (e.g., strudel from German, pizza from Italian) and loan translations in which each element of a word is translated (e.g., spring roll from Chinese, dreamtime from the Australian aboriginal alcheringa (Gramley 2001), and chien-chaud, which is the French Quebec version of hot dog). 3. Word formation from existing etyma (words or word components). This includes (a) compounding (e.g., bookcase, bushﬁre), (b) blending (e.g., brunch, motel), (c) afﬁxation (e.g., overcook, international, likeness, privatize), (d) shortening (e.g., petrol(eum), radar, telly, AIDS), (e) eponyms (e.g., kleenex, sandwich, jersey, casanova), (f) internal derivations (Gramley 2001) (e.g., extend/extent or sing/song), (g) reduplication (Stockwell and Minkova 2001) (e.g., ﬁfty-ﬁfty, dum-dum), (h) morphological reanalysis (Schendl 2001) (e.g., the nonexistent verb to edit was formed from the noun editor; the word cheeseburger was derived from hamburger even though this word comes from the proper name Hamburg). Mechanisms 1 and 3(f)–(h) are rare compared to 2 and 3(a)–(e) (Algeo 1998). Clearly, the above mechanisms are not exclusive. Borrowing and word formation are obviously both at play in examples such as blitz, which is a clipping of the German word blitzkreig, and the French word tennisman, which is a compound of two English words. Word creation, borrowing, and word formation generally produce a new word with a single sense, except when by coincidence the word being created, borrowed, or formed already exists with a different sense. In the rest of the article, we consider homographs produced by such coincidences to be different words. In most dictionaries, homographs have distinct entries. For example, the term bug, meaning ‘error in a computer program,’ was borrowed into French as bogue (by assimilation with the already existing word with the unrelated meaning ‘husk’), but these two meanings of bogue are listed in French dictionaries as two distinct words. 229  Computational Linguistics  Volume 31, Number 2  Nevertheless, we should mention three cases in which a neologism is often not recognized as a genuinely new word: ellipsis (Antilla 1989) (e.g., daily (newspaper)), zero derivation (Nevalainen 1999) (also known as conversion [Gramley 2001; Schendl 2001]) (e.g., to cheat > a cheat), and borrowing of an already-existing word with a related sense (e.g., to control was borrowed into French as controˆler, thus giving an extra sense to this French word meaning ‘to verify’). The following is a list of mechanisms which can create a new sense for an alreadyexisting word (adapted from Algeo [1998]): 1. referential shift (e.g., to print now also refers to laser printers). 2. generalization (e.g., chap used to mean ‘a customer’) or abstraction (e.g., zest denoted orange or lemon peel used for ﬂavoring before being used in the abstract sense of ‘gusto’). 3. specialization (e.g., in Old English fowl meant any kind of bird and meat any kind of food [Onions 1966; Schendl 2001]) or concretion. 4. metaphor (e.g., kite, meaning ‘bird of prey,’ applied to a toy). 5. metonymy (literally, ‘name change’), that is, naming something by any of its parts, accompaniments, or indexes (e.g., the crown for the sovereign, the City for the people who work there, tin for the container made of that metal, cognac for the drink originating from that region) (Traugott and Dasher 2002). 6. clang association or folk etymology (e.g., belfry meant ‘a movable tower used in attacking walled positions,’ but the ﬁrst syllable was associated with bell, and now the basic meaning is ‘bell tower’ [Antilla 1989]). 7. embellishment of language by using words which are more acceptable, attractive, or ﬂattering than existing terms (hyperbole, litotes, euphemisms, etc., as discussed above). We use the general term association to cover all these cases. 3. The Near-Exponential Rule In order to study the relative importance of neologism, obsolescence, and the creation of new meanings for existing words, we counted the number of senses listed per word in several different monolingual dictionaries. We observed the following general empirical rule satisﬁed to within a fairly high degree of accuracy by all the dictionaries studied [9, 12, 19, 20, 21, 23, 24, 28, 31, 32]: Near-Exponential Rule: The number of senses per word in a monolingual dictionary has an approximately exponential distribution. One way of testing this rule is by plotting log(Ns) against s, where Ns is the number of words in the dictionary with exactly s senses. If the near-exponential rule is satisﬁed, then the resulting plot should be very close to a straight line with a negative slope. This is indeed the case for the dictionaries tested, with varying values of the slope depending on the dictionary. Figures 1 and 2 show the plot of Ns, on a logarithmic scale, against s 230  Cooper  A Mathematical Model of Historical Semantics  Figure 1 Plot of Ns (number of words with s senses) against s for various monolingual English dictionaries.  Figure 2 Plot of Ns (number of words with s senses) against s for various monolingual French dictionaries. 231  Computational Linguistics  Volume 31, Number 2  Figure 3 Plot of Ns (number of entries with s senses) for four different dictionaries, each showing a nonexponential distribution. for four English [19, 24, 28, 31] and ﬁve French [9, 12, 20, 21, 23] dictionaries. Only those points (s, Ns) for which Ns > 12 are plotted in the ﬁgures. For each dictionary, the values of Ns were obtained by sampling a random set of pages of the dictionary. Sampling was performed independently for each dictionary, meaning that the random sample of words was different in each case. We excluded entries corresponding to proper names, foreign words, spelling variants, derived words (such as past participles), regional words, abbreviations, and expressions. We allowed hyphens within words but not spaces. Thus cat-o’-nine-tails counted as a word, but tower block and phrasal verbs such as give up did not. Only words forming part of British English or French spoken in metropolitan France were considered. All words were treated equally irrespective of their relative frequencies. Thus the words get and ﬂoccinaucinihilipiliﬁcation were given the same importance. The size of each dictionary that was sampled is given in the reference section. The dictionaries sampled vary in size from 20,000 to 80,000 words. To ensure that the near-exponential rule was not simply an artifact of our choice of experimental procedure or of lexicographical practice, we performed the same analysis on a dictionary of abbreviations and acronyms [7], a dictionary of scientiﬁc terms [1], a bilingual dictionary of slang words [17], and a dictionary of French synonyms [10]. The resulting curves, shown in Figure 3, are far from straight lines. We performed similar counts for bilingual dictionaries. Figures 4 and 5 show the number of words NTt with t translations plotted against t for several pairs of languages. The NTt scale is again logarithmic. Although the near-exponential rule could also be said to hold for certain bilingual dictionaries, the curvature of the log NTt against t curve varies considerably depending on the distance between the two languages. For pairs of languages with strong etymological connections (such as French and Spanish), the average curvature is positive (Figure 4), but for pairs of distant languages (such as Japanese and English) the average curvature is negative (Figure 5). A theoretical explanation of this phenomenon is outside the scope of the present article, but it is 232  Cooper  A Mathematical Model of Historical Semantics  Figure 4 Number NTt of words in French with t translations in English, Spanish, Italian, and Portuguese. Figure 5 Number NTt of words in language A with t translations in language B, for distant languages A and B. probably due to the greater differences in the segmentation of semantic space by distant languages (see Resnik and Yarowsky [2000] for some illustrative examples). It will be treated in detail in a follow-up article. 4. Words, Senses, and Concepts In the following section we present a mathematical model which explains the nearexponential distribution of word senses observed in English and French dictionaries. Not only do the curves of Figures 1 and 2 share the property of being close to straight lines (i.e., having curvature close to zero), but in each case, the curvature that they do exhibit is positive rather than negative. Although barely discernible for some of the curves, this positive curvature cannot be ignored. We ﬁtted a straight line to the curves and then used a chi-square test to judge the closeness of ﬁt of this straight line to the data. For each curve the chi-square test demonstrated a signiﬁcant discrepancy 233  Computational Linguistics  Volume 31, Number 2  between the model and the data. For example, the signiﬁcance level was 15 standard deviations for the Longman Dictionary of Contemporary English (LCDE) [24]. In order to ﬁnd a satisfactory model to explain this slight but consistently positive curvature, we study in more detail the process by which words gain new senses. The word panel provides a good example of a word whose number of meanings has grown since its introduction into English from Old French in the 13th century. Its original meaning was a piece of cloth placed under a saddle. Over the centuries it gained many meanings, by extension of this original sense, which can be grouped together in the following concept: (C1) an often rectangular-shaped part of a surface (of a wall, fence, cloth, etc.), possibly decorated or with controls fastened to it. Concept (C1) covers four of the meanings of panel listed in the LDCE. However, during the 14th century panel also gained the following meaning: piece of parchment (attached to a writ) on which names of jurors were written (hence by metonymy) list of jurymen: jury (Onions 1966). Four of the meanings of panel listed in the LDCE can be considered to be covered by the following general concept: (C2) a group of people (or the list of their names) brought together to answer questions, make judgements, etc. If panel were to gain new meanings, such as 1. a side of a tower block 2. a school disciplinary committee then these would be by association with the two concepts listed above, (C1) and (C2), respectively. Note that neither of these potential new meanings would constitute a truly new concept, since they can be considered to be covered by the existing concepts (C1) and (C2). If, on the other hand, panel were to gain the following new meanings 3. a wall which divides a large room into smaller units but which does not reach the ceiling 4. a combined table and bench that can be used, for example, by a panel of experts by association with concepts (C1) and (C2), respectively, then these new meanings could be considered as corresponding to new concepts. These meanings are sufﬁciently different from the existing meanings listed in the LCDE that they themselves could give rise to further new meanings by metonymy, metaphor, etc., which would simply not be possible by direct association with the existing meanings. For example, the following meanings could theoretically be derived from the meanings 3 and 4, respectively, above (but not directly from concepts (C1) and (C2), respectively): 5. any division of something into smaller units 6. a combined desk and bench for a single person 234  Cooper  A Mathematical Model of Historical Semantics  We continue with another example, this time from French. The word toilette has eight meanings listed in Le petit Robert [21], which we can translate and paraphrase as follows: 1. a small piece of cloth (from toile = ‘piece of cloth’) and, in particular, one that was used in the past to wrap up objects 2. a membrane used by butchers to wrap up certain pieces of meat 3. clothes, jewelry, comb, etc. (objects necessary to prepare one’s appearance before going out, which used to be laid out on a small cloth) 4. the action of combing, making up, dressing 5. a woman’s style of dressing 6. the cleaning of one’s body before dressing 7. a washroom, toilet 8. the cleaning, preparation of an object, text, etc. We can group these meanings into three concepts: (D1) a small piece of material (meanings 1, 2) (D2) the objects used for, the action of or the style of dressing, making-up, cleaning of a person or an object (meanings 3, 4, 5, 6, 8) (D3) a washroom, toilet (meaning 7) We have grouped meanings together in this way because we consider it likely that new meanings for toilette which could enter the French language by association with an existing meaning would be very similar for those meanings grouped into the same concept, but very different for those corresponding to different concepts. This discussion leads us naturally to the following technical deﬁnition of concept: Deﬁnition Two meanings of a given word correspond to the same concept if and only if they could inspire the same new meanings by association. We suggest grouping together different senses of a word, not only according to their parts of speech or to their etymology (i.e., the history) of word senses, but also according to their potential future: whether or not they could inspire the same new meanings by association. This can be compared with the biological deﬁnition of species in terms of the ability to breed together to produce viable offspring rather than in terms of history or physical characteristics. 5. A Mathematical Model of Word Sense Genesis This section describes a stochastic model of the creation of word senses. This model not only explains the near-exponential rule but also provides a deeper insight into the process of naming. Let LD be a language as deﬁned by the set of word, sense pairs in a 235  Computational Linguistics  Volume 31, Number 2  dictionary D. We consider the evolution of the language LD over time. We must always bear in mind that LD is, of course, only an approximate representation of the semantics of the corresponding natural language. For example, the compiler of a dictionary may choose to include archaic words as a historical record or to exclude whole categories of words such as slang or technical terms. Consider the evolution of LD as a stochastic process in which each step is either (a) the elimination of a word sense (by obsolescence), (b) the introduction of a new word (by creation, borrowing, word-formation, or any other mechanism), or (c) the addition of a new sense for an existing word (by association with an existing sense). Let t be the probability of a step of type (a), u the probability of a step of type (b), and v the probability of a step of type (c). Note that t + u + v = 1. The parameters of our model t, u, v are unknowns which will be estimated from the observed values of Ns (the number of words with s senses). We make the following simplifying assumptions: 1. New-word single-sense assumption: When a neologism enters the language LD, it has a single sense. 2. Independence of obsolescence and number of senses: The probability that a word, sense pair leaves the language LD by obsolescence is independent of the number of senses this word has in LD. The new-word single-sense assumption is an essential part of our model. To test it we require two editions of the same dictionary. The 1994 edition of the Dictionnaire de l’acade´mie franc¸aise indicates which words are new compared to the 1935 edition. Less than 17% of these words are polysemic. Furthermore, this corresponds, according to our model and to within-sample error, to the proportion of originally monosemic words entering the language that can be expected to acquire new senses during the period between the publication of the two editions. Assumption 2 above is not as important as assumption 1, since later we restrict ourselves to a no-obsolescence model. As discussed in the previous section, the set of s senses of an ambiguous word may correspond to a number c of essentially distinct concepts, where c is some number between one and s. For example, the plumbing and anatomy senses of joint correspond to the same concept, since they could inspire the same new senses by association. The ‘cigarette containing cannabis’ sense of joint clearly corresponds to a different concept, since it could inspire a very different set of new senses by association. Associations inspired by distinct concepts are assumed to occur independently. We assume that a word with s senses in LD represents on average 1 + α(s − 1) concepts. We call α the concept creation factor (since, in a no-obsolescence model, α is simply the probability that a new sense for a word w can be considered a new concept compared to the existing senses for w). We can now state a third assumption: 3. Associations are with concepts: The probability that a concept gives rise to a new sense for a word w by association is proportional to the number of concepts represented by w in LD, which is assumed to be on average 1 + α(s − 1), where s is the number of senses of w and α is a constant. The concept creation factor α is another unknown which will be estimated from the values of Ns. 236  Cooper  A Mathematical Model of Historical Semantics  Table 1 Number Ns of words with s senses in samples from the 1933 and the 1993 edition of the Shorter Oxford English Dictionary. N1 N2 N3 N4 N5 N6 N7 N8 N9 1933 427 186 104 49 24 15 22 6 8 1993 403 176 86 44 32 16 14 7 1  We make a fourth hypothesis in order to render the problem mathematically tractable:  4. Stationary-state hypothesis: LD considered as a stochastic process is in a stationary state, in the sense that the probability P(s) that an arbitrary word of LD has exactly s senses does not change as LD evolves.  To test the validity of the stationary-state hypothesis, we compared the 1933 and 1993 editions of the Shorter Oxford English Dictionary (SOED) [32, 28]. In the space of 60 years, the number of words in the SOED increased by 24%. Nevertheless the values of P(s) (s = 1, 2, . . . , 9) remained almost constant. A chi-square test revealed that the differences in the values of P(s) (s = 1, 2, . . .) could be accounted for by sampling error. The corresponding values of Ns are given in Table 1. The results of further experiments carried out to test the validity of the assumptions on which our model is based are given in a later section, so as not to clutter up the presentation of the model in this section. Let m be the expected number of senses per word in LD. Since  ∞  m = s P(s)  (1)  s=1  and the values of P(s) are constant by the stationary-state hypothesis, m is also a  constant.  The expected net increase in the number of word senses in LD during one step of the process is −t + (1 − t) = 1 − 2t, since the probability that a word sense is lost by  obsolescence is t and the probability that a word sense is gained is 1 − t. If r denotes the  expected net increase in the number of words in LD during one step of the process, then  we  must  have  1−2t r  =  m,  since  m  is  a  constant.  Thus  r = (1 − 2t)/m  (2)  Note that the number of words in LD would be constant if and only if t = 0.5.  Let pout(s) represent the probability that the next change in the language LD is that  a word with s senses loses one of its senses by obsolescence. Let pin(s) represent the  probability Note that  that the next  ∞ s=1  pout  (s)  =  change t and  in∞ s=L1Dpinis(st)ha=t  a word with s senses gains a new v, by the deﬁnitions of t and v.  sense.  By the stationary-state hypothesis, the expected net increase in Ns (the number  of words in LD with exactly s senses) during one step must be proportional to P(s).  Denote the expected net increase in Ns by δs = dP(s), for some constant d. We then have  237  Computational Linguistics  Volume 31, Number 2  ∞ s=1  δs  =  d, since  ∞ s=1  P(s)  =  1. But  ∞ s=1  δs  =  r, since the total expected increase in  the number of words is r. Thus δs = rP(s) = (1 − 2t)P(s)/m (by equation (2)).  We can also express δs, the expected net increase in Ns, in terms of the probabilities  pin(s) and pout(s), which gives the following equation:  (1 − 2t)P(s)/m = −pin(s) − pout(s) + pin(s − 1) + pout(s + 1)  (3)  since Ns is decremented when a word with s senses gains or loses a sense and Ns is incremented when a word with s − 1 senses gains a sense or a word with s + 1 senses  loses a sense.  From the assumption of the independence of obsolescence and number of senses,  it follows directly that constant K. Then, since  pou∞ st=(s1 )poisut  proportional to (s) = t, we have  sP(s). t=  Let ∞ s=1  pout (s ) KsP(s)  = KsP(s), = Km by  for some equation  (1). Thus K = t/m and  pout (s )  =  tsP(s) m  Under the assumption that associations are with concepts, pin(s) is  1+ v,  α(s − 1) and P(s). Suppose that  ∞ s=1  P(s)  =  1,  and  ∞ s=1  sP(s)  pin (s ) = m,  = K P(s)(1 + α(s we have v =  − 1)).  ∞ s=1  K  proportional to both  Since  ∞ s=1  pin  (s)  =  P(s)(1 + α(s − 1)) =  K (1 − α) + K αm. Thus K = v/(1 − α + αm), and hence, for s = 1, 2, . . .  pin (s )  =  v(1 + α(s − 1))P(s) 1 − α + αm  Note that the creation of a new word with a single sense is a special case. By deﬁnition of u as the probability that the next step of the process is the creation of a new word,  pin(0) = u Summing equation (3), for s = 1, 2, . . ., gives  
Edward Gibson∗∗ Massachusetts Institute of Technology  This article aims to present a set of discourse structure relations that are easy to code and to develop criteria for an appropriate data structure for representing these relations. Discourse structure here refers to informational relations that hold between sentences in a discourse. The set of discourse relations introduced here is based on Hobbs (1985). We present a method for annotating discourse coherence structures that we used to manually annotate a database of 135 texts from the Wall Street Journal and the AP Newswire. All texts were independently annotated by two annotators. Kappa values of greater than 0.8 indicated good interannotator agreement. We furthermore present evidence that trees are not a descriptively adequate data structure for representing discourse structure: In coherence structures of naturally occurring texts, we found many different kinds of crossed dependencies, as well as many nodes with multiple parents. The claims are supported by statistical results from our hand-annotated database of 135 texts. 1. Introduction An important component of natural language discourse understanding and production is having a representation of discourse structure. A coherently structured discourse here is assumed to be a collection of sentences that are in some relation to each other. This article aims to present a set of discourse structure relations that are easy to code and to develop criteria for an appropriate data structure for representing these relations. There have been two kinds of approaches to deﬁning and representing discourse structure and coherence relations. These approaches differ with respect to what kinds of discourse structure they are intended to represent. Some accounts aim to represent the intentional-level structure of a discourse; in these accounts, coherence relations reﬂect how the role played by one discourse segment with respect to the interlocutors’ intentions relates to the role played by another segment (e.g., Grosz and Sidner 1986). Other accounts aim to represent the informational structure of a discourse; in these accounts, coherence relations reﬂect how the meaning conveyed by one discourse segment relates to the meaning conveyed by another discourse segment (e.g., Hobbs 1985; Marcu 2000; Webber et al. 1999). Furthermore, accounts of discourse structure vary greatly with respect to how many discourse relations they assume, ranging from 2 (Grosz and Sidner 1986) to over 400 different coherence relations (reported in Hovy and ∗ Computer Laboratory and Genetics Department, Cambridge, CB3 0FD, U.K. E-mail: Florian.Wolf@cl.cam.ac.uk ∗∗ Department of Brain and Cognitive Sciences, Cambridge, MA 02139. E-mail: egibson@mit.edu. Submission received: 15th June 2004; Revised submission received: 5th September 2004; Accepted for publication: 23rd October 2004 © 2005 Association for Computational Linguistics  Computational Linguistics  Volume 31, Number 2  Maier [1995]). However, Hovy and Maier (1995) argue that, at least for informationallevel accounts, taxonomies with more relations represent subtypes of taxonomies with fewer relations. This means that different informational-level-based taxonomies can be compatible with each other; they differ with respect to how detailed or ﬁne-grained a manner they represent informational structures of texts. Going beyond the question of how different informational-level accounts can be compatible with each other, Moser and Moore (1996) discuss the compatibility of rhetorical structure theory (RST) (Mann and Thompson 1988) with the theory of Grosz and Sidner (1986). However, note that Moser and Moore (1996) focus on the question of how compatible the claims are that Mann and Thompson (1988) and Grosz and Sidner (1986) make about intentional-level discourse structure. In this article, we aim to develop an easy-to-code representation of informational relations that hold between sentences or other nonoverlapping segments in a discourse monologue. We describe an account with a small number of relations in order to achieve more generalizable representations of discourse structures; however, the number is not so small that informational structures that we are interested in are obscured. The goal of the research presented is not to encode intentional relations in texts. We consider annotating intentional relations too difﬁcult to implement in practice at this time. Note that we do not claim that intentional-level structure of discourse is not relevant to a full account of discourse coherence; it just is not the focus of this article. The next section describes in detail the set of coherence relations we use, which are mostly based on Hobbs (1985). We try to make as few a priori theoretical assumptions about representational data structures as possible. These assumptions are outlined in the next section. Importantly, however, we do not assume a tree data structure to represent discourse coherence structures. In fact, a major result of this article is that trees do not seem adequate to represent discourse structures. This article is organized as follows. Section 2 describes the procedure we used to collect a database of 135 texts annotated with coherence relations. Section 3 describes in detail the descriptional inadequacy of tree structures for representing discourse coherence, and Section 4 provides statistical evidence from our database that supports this claim. Section 5 offers some concluding remarks. 2. Collecting a Database of Texts Annotated with Coherence Relations This section describes (1) how we deﬁned discourse segments, (2) which coherence relations we used to connect discourse segments, and (3) how the annotation procedure worked. 2.1 Discourse Segments There is agreement that discourse segments should be nonoverlapping spans of text. However, there is disagreement in the literature about how to deﬁne discourse segments (cf. the discussion in Marcu [2000]). Whereas some argue that discourse segments should be prosodic units (Hirschberg and Nakatani 1996), others argue for intentional units (Grosz and Sidner 1986), phrasal units (Lascarides and Asher 1993; Longacre 1983; Webber et al. 1999), or sentences (Hobbs 1985). For our database, we mostly adopted a clause-unit-based deﬁnition of discourse segments. We chose this method of segmenting discourse because it was easy to use. 250  Wolf and Gibson  Representing Discourse Coherence  Table 1 Contentful conjunctions used to illustrate coherence relations.  Cause–effect Violated expectation Condition Similarity Contrast Temporal sequence Attribution Example Elaboration Generalization  because; and so although; but; while if . . . (then); as long as; while and; (and) similarly by contrast; but (and) then; ﬁrst, second, . . . ; before; after; while according to . . . ; . . . said; claim that . . . ; maintain that . . . ; stated that . . . for example; for instance also; furthermore; in addition; note (furthermore) that; (for, in, on, against, with, . . . ) which; who; (for, in, on, against, with, . . . ) whom in general  However, we also assumed that contentful coordinating and subordinating conjunctions (cf. Table 1) can delimit discourse segments. Note that we did not classify and as delimiting discourse segments if it was used to conjoin nouns in a conjoined noun phrase, like dairy plants and dealers in example (1) (from wsj 0306; Wall Street Journal 1989 corpus [Harman and Liberman 1993]) or if it was used to conjoin verbs in a conjoined verb phrase, like snowed and rained in example (2) (constructed): (1) Milk sold to the nation’s dairy plants and dealers averaged $14.50 for each hundred pounds. (2) It snowed and rained all day long. We classiﬁed periods, semicolons, and commas as delimiting discourse segments. However, in cases like example (3) (constructed), in which they conjoin a complex noun phrase, commas were not classiﬁed as delimiting discourse segments. (3) John bought bananas, apples, and strawberries. We furthermore treated attributions (John said that . . .) as discourse segments. This was empirically motivated. The texts used here were taken from news corpora, and there, attributions can be important carriers of coherence structures. For instance, consider a case in which some source A and some source B both comment on some event X. It should be possible to distinguish between a situation in which source A and source B make basically the same statement about event X and a situation in which source A and source B make contrasting comments about event X. Note, however, that we treated cases like example (4) (constructed) as one discourse segment and not as two separate ones ( . . . cited and transaction costs . . .). We separated attributions only if the attributed material was a complementizer phrase, a sentence, or a group of sentences. This is not the case in example (4): The attributed material is a complex NP (transaction costs from its 1988 recapitalization). (4) The restaurant operator cited transaction costs from its 1988 recapitalization. 251  Computational Linguistics  Volume 31, Number 2  2.2 Discourse Segment Groupings Adjacent discourse segments could, in our approach, be grouped together. For example, discourse segments were grouped if they all stated something that could be attributed to the same source (cf. section 2.3 for a deﬁnition of attribution coherence relations). Furthermore, discourse segments were grouped if they were topically related. For example, if a text discussed inventions in information technology, there could be groups of a few discourse segments each talking about inventions by speciﬁc companies. There might also be subgroups, consisting of several discourse segments each, talking about speciﬁc inventions at speciﬁc companies. Thus, marking groups could determine a partially hierarchical structure for the text. Other examples of discourse segment groupings included cases in which several discourse segments described an event or a group of events that all occurred before another event or another group of events described by another (group of) discourse segments. In those cases, what was described by a group of discourse segments was in a temporal sequence relation with what was described by another (group of) discourse segments (cf. section 2.3 for a deﬁnition of temporal-sequence coherence relations). Note furthermore that in cases in which one topic required one grouping and a following topic required a grouping that was different from the ﬁrst grouping, both groupings were annotated. Unlike approaches such as the TextTiling algorithm (Hearst 1997), ours allowed partially overlapping groups of discourse segments. The idea behind this option was to allow groupings of discourse segments in which a transition discourse segment belonged to the previous as well as the following group. However, the option was not used by the annotators (i.e., in our database of 135 hand-annotated texts, there were no instances of partially overlapping discourse segment groups). 2.3 Coherence Relations As pointed out in section 1, we aim to develop a representation of informational relations between discourse segments. Note one difference between schema-based approaches (McKeown 1985) and coherence relations as we used them: Whereas schemas are instantiated from information contained in a knowledge base, coherence relations as we used them do not make (direct) reference to a knowledge base. There are a number of different informational coherence relations, dating back, in their basic deﬁnitions, to Hume, Plato, and Aristotle (cf. Hobbs 1985; Hobbs et al. 1993; Kehler 2002). The coherence relations we used are mostly based on Hobbs (1985); below we describe each coherence relation we used and note any differences between ours and Hobbs’s (1985) set of coherence relations (cf. Table 2 for an overview of how our set of coherence relations relates to the set of coherence relations in Hobbs [1985]). The kinds of coherence relations we used include cause–effect relations, as in example (5) (constructed), in which discourse segment 1 states the cause for the effect that is stated in discourse segment 2: (5) Cause–effect 1. There was bad weather at the airport 2. and so our ﬂight got delayed. Our cause–effect relation subsumed the cause as well as the explanation relation in Hobbs (1985). A cause relation holds if a discourse segment stating a cause occurs 252  Wolf and Gibson  Representing Discourse Coherence  before a discourse segment stating an effect; an explanation relation holds if a discourse segment stating an effect occurs before a discourse segment stating a cause. We encoded this difference by adding a direction to the cause–effect relation. In a graph, this can be represented by a directed arc going from cause to effect. Another kind of causal relation is condition. Hobbs (1985) does not distinguish condition relations from either cause or explanation relations. However, we felt that it might be important to distinguish between a causal relation describing an actual causal event (cause–effect, cf. above), on the one hand, and a causal relation describing a possible causal event (condition, cf. below), on the other hand. In example (6) (constructed), discourse segment 2 states an event that will take place if the event described by discourse segment 1 also takes place: (6) Condition 1. If the new software works, 2. everyone should be happy. In a third type of causal relation, the violated expectation relation (also violated expectation in Hobbs [1985]), a causal relation between two discourse segments that normally would be present is absent. In example (7) (constructed), discourse segment 1 normally would be a cause for everyone’s being happy; this expectation is violated by what is stated by discourse segment 2: (7) Violated expectation 1. The new software worked great, 2. but nobody was happy. Other possible coherence relations include similarity (parallel in Hobbs [1985]) or contrast (also contrast in Hobbs [1985]) relations, in which similarities or contrasts are determined between corresponding sets of entities or events, such as between discourse segments 1 and 2 in example (8) (constructed) and discourse segments 1 and 2 in example (9) (constructed), respectively: (8) Similarity 1. The ﬁrst ﬂight to Frankfurt this morning was delayed. 2. The second ﬂight arrived late as well. (9) Contrast 1. The ﬁrst ﬂight to Frankfurt this morning was delayed. 2. The second ﬂight arrived on time. Discourse segments might also elaborate (also elaboration in Hobbs [1985]) on other sentences, as in example (10) (constructed), in which discourse segment 2 elaborates on discourse segment 1: (10) Elaboration 1. A probe to Mars was launched from the Ukraine this week. 2. The European-built “Mars Express” is scheduled to reach Mars by late December. Discourse segments can provide examples for what is stated by another discourse segment. In example (11) (constructed), discourse segment 2 states an example 253  Computational Linguistics  Volume 31, Number 2  (exempliﬁcation in Hobbs [1985]) for what is stated in discourse segment 1: (11) Example 1. There have been many previous missions to Mars. 2. A famous example is the Pathﬁnder mission. Hobbs (1985) also includes an evaluation relation, as in example (12) (from Hobbs [1985]), in which discourse segment 2 states an evaluation of what is stated in discourse segment 1. We decided to call such relations elaborations, since we found it too difﬁcult in practice to reliably distinguish elaborations from evaluations (according to our annotation scheme, in example (12), what is stated in discourse segment 2 elaborates on what is stated in discourse segment 1): (12) Elaboration (labeled as evaluation in Hobbs [1985]) 1. (A story.) 2. It was funny at the time. Unlike Hobbs (1985), we did not have a separate background relation as in example (13) (modiﬁed from Hobbs [1985]), in which what is stated in discourse segment 1 provides the background for what is stated in discourse segment 2. As with the evaluation relation, we found the background relation too difﬁcult to reliably distinguish from elaboration relations (according to our annotation scheme, in example (13), what is stated in discourse segment 1 elaborates on what is stated in discourse segment 2): (13) Elaboration (labeled as background in Hobbs [1985]) 1. T is the pointer to the root of a binary tree. 2. Initialize T. In a generalization relation, as in example (14) (constructed), one discourse segment (here discourse segment 2) states a generalization for what is stated by another discourse segment (here discourse segment 1): (14) Generalization 1. Two missions to Mars in 1999 failed. 2. There are many missions to Mars that have failed. Furthermore, discourse segments can be in an attribution relation, as in example (15) (constructed), in which discourse segment 1 states the source of what is stated in discourse segment 2 (cf. [Bergler 1991] for a more detailed semantic analysis of attribution relations): (15) Attribution 1. John said that 2. the weather would be nice tomorrow. Hobbs (1985) does not include an attribution relation. However, we decided to include attribution as a relation because, as pointed out in section 2.1, the texts we annotated are taken from news corpora. There, attributions can be important carriers of coherence structures. 254  Wolf and Gibson  Representing Discourse Coherence  In a temporal sequence relation, as in example (16) (constructed), one discourse segment (here discourse segment 1) states an event that takes place before another event stated by the other discourse segment (here discourse segment 2): (16) Temporal Sequence 1. First, John went grocery shopping. 2. Then he disappeared in a liquor store. In contrast to cause–effect relations, there is no causal relation between the events described by the two discourse segments. The temporal sequence relation is equivalent to the occasion relation in Hobbs (1985). The same relation, illustrated by example (17) (constructed), is an epiphenomenon of assuming contiguous distinct elements of text (Hobbs [1985] does not include a same relation). A same relation holds if a subject NP is separated from its predicate by an intervening discourse segment. For instance, in example (17), discourse segment 1 is the subject NP of a predicate in discourse segment 3, and so there is a same relation between discourse segments 1 and 3; discourse segment 1 is the ﬁrst and discourse segment 3 is the second segment of what is actually one single discourse segment, separated by the intervening discourse segment 2, which is in an attribution relation with discourse segment 1 (and therefore also with discourse segment 3, since discourse segments 1 and 3 are actually one single discourse segment): (17) Same 1. The economy, 2. according to some analysts, 3. is expected to improve by early next year. Table 2 provides an overview of how our set of coherence relations relates to the set of coherence relations in Hobbs (1985). We distinguish between asymmetrical or directed relations, on the one hand, and symmetrical or undirected relations, on the other hand (Mann and Thompson 1988; Marcu 2000). Cause–effect, condition, violated expectation, elaboration, example, generalization, attribution, and temporal sequence are asymmetrical or directed relations, whereas similarity, contrast, and same are symmetrical or undirected relations. In asymmetrical or directed relations, the directions of relations are as follows: r Cause–effect: from the discourse segment stating the cause to the discourse segment stating the effect r Condition: from the discourse segment stating the condition to the discourse segment stating the consequence r Violated expectation: from the discourse segment stating the cause to the discourse segment describing the absent effect r Elaboration: from the elaborating discourse segment to the elaborated discourse segment r Example: from the discourse segment stating the example to the discourse segment stating the exempliﬁed r Generalization: from the discourse segment stating the special case to the discourse segment stating the general case 255  Computational Linguistics  Volume 31, Number 2  Table 2 Correspondence between the set of coherence relations in Hobbs (1985) and our set of coherence relations.  Hobbs (1985)  Our annotation scheme  Occasion Cause Explanation — Evaluation Background Exempliﬁcation: example stated ﬁrst, then general case; directionality indicated by directed arcs in a coherence graph Exempliﬁcation: general case stated ﬁrst, then example; directionality indicated by directed arcs in a coherence graph Elaboration Parallel Contrast Violated expectation — —  Temporal sequence Cause–effect: cause stated ﬁrst, then effect; directionality indicated by directed arcs in a coherence graph Cause–effect: effect stated ﬁrst, then cause; directionality indicated by directed arcs in a coherence graph Condition Elaboration Elaboration Example Generalization Elaboration Similarity Contrast Violated expectation Attribution Same  r Attribution: from the discourse segment stating the source to the attributed statement r Temporal sequence: from the discourse segment stating the event that happened ﬁrst to the discourse segment stating the event that happened second This deﬁnition of directionality is related to Mann and Thompson’s (1988) notion of nucleus and satellite nodes (where the nodes can represent [groups of] discourse segments): For asymmetrical or directed relations, the directionality is from satellite to nucleus node; by contrast, symmetrical or undirected relations hold between two nucleus nodes. Note also that in our annotation project, we decided to annotate a coherence relation either if there was a coherence relation between the complete content of two discourse segments, or if there was a relation between parts of the content of two discourse segments. Consider the following example (from ap890104-0003; AP Newswire corpus; [Harman and Liberman 1993]): (18) 1. a[ Difﬁculties have arisen ] b[ in enacting the accord for the independence of Namibia ] 2. for which SWAPO has fought many years, For this example we would annotate an elaboration relation from discourse segment 2 to discourse segment 1 (discourse segment 2 provides additional details about the accord  256  Wolf and Gibson  Representing Discourse Coherence  mentioned in discourse segment 1), although the relation actually only holds between discourse segment 2 and the second part of discourse segment 1, indicated by brackets. Although it is beyond the scope of the current project, future research should investigate annotations with discourse segmentations that allow annotating relations only between parts of discourse segments that are responsible for a coherence relation. For example, consider example (19) (from ap890104-0003; AP Newswire corpus [Harman and Liberman 1993]), in which brackets indicate how more-ﬁnegrained discourse segments might be marked: (19) 1. a[ for which ] b[ SWAPO ] c[ has fought many years, ] 2. referring to the acronym of the South-West African Peoples Organization nationalist movement. In our current project, we annotated an elaboration relation from discourse segment 2 to discourse segment 1 (discourse segment 2 provides additional details, the full name, for SWAPO, which is mentioned in discourse segment 1). A future, more detailed, annotation of coherence relations could then annotate this elaboration relation to hold only between discourse segment 2 and the word SWAPO in discourse segment 1. 2.4 Coding Procedure To code the coherence relations of a text, we used a procedure consisting of three steps. In the ﬁrst step, a text was segmented into discourse segments (cf. section 2.1). In the second step, adjacent discourse segments that were topically related were grouped together. The criteria for this step are described in section 2.2. In the third step, coherence relations (cf. section 2.3) were determined between discourse segments and groups of discourse segments. Each previously unconnected (group of) discourse segment(s) was tested to see whether it connected to any of the (groups of) discourse segments that had already been connected to the already existing representation of discourse structure. In order to help determine the coherence relation between (groups of) discourse segments, the annotators judged which, if any, of the contentful coordinating conjunctions in Table 1 resulted, when used, in the most acceptable passage (cf. Hobbs 1985; Kehler 2002). If using a contentful conjunction to connect (groups of) discourse segments resulted in an acceptable passage, this was used as evidence that the coherence relation corresponding to the mentally inserted contentful conjunction held between the (groups of) discourse segments under consideration. This mental exercise was done only if there was not already a contentful coordinating conjunction that disambiguated the coherence relation. The following list (which was also used by the annotators to guide them in their task) shows in more detail how the annotations were carried out: 1. Segment the text into discourse segments: (a) Insert segment boundaries at every period that marks a sentence boundary (i.e., not at periods such as those in Mrs. or Dr.). (b) Insert segment boundaries at every semicolon and colon that marks a sentence or clause boundary. (c) Insert segment boundaries at every comma that marks a sentence or clause boundary; do not insert segment boundaries at commas that conjoin complex noun or verb phrases. 257  Computational Linguistics  Volume 31, Number 2  (d) Insert segment boundaries at every quotation mark, if there is not already a segment boundary based on steps (a)–(c). (e) Insert segment boundaries at the contentful coordinating conjunctions listed in Table 1, if there is not already a segment boundary based on steps (a)–(d). For and, do not insert a segment boundary if it is used to conjoin verbs or nouns in a conjoined verb or noun phrase.  2. Generate groupings of related discourse segments:  (a) Group contiguous discourse segments that are enclosed by pairs of quotation marks. (b) Group contiguous discourse segments that are attributed to the same source. (c) Group contiguous discourse segments that belong to the same sentence (marked by periods, commas, semicolons, or colons). (d) Group contiguous discourse segments that are topically centered on the same entities or events.  3. Determine coherence relations between discourse segments and groups of discourse segments. For each previously unconnected (group of) discourse segment(s), test whether it connects to any of the (groups of) discourse segments that have already been connected to the already existing representation of discourse structure. Use the following steps for each decision:  (a) (b) (c) i. ii. iii. iv. (d) i.  Use pairs of quotation marks as a signal for attribution. For pairs of (groups of) discourse segments that are already connected with one of the contentful coordinating conjunctions from Table 1, choose the coherence relation that corresponds to the coordinating conjunction. For pairs of (groups of) discourse segments that are not connected with one of the contentful coordinating conjunctions from Table 1: Mentally connect the (groups of) discourse segments with one of the coordinating conjunctions from Table 1 and judge whether the resultant passage sounds acceptable. If the passage sounds acceptable, choose the coherence relation that corresponds to the coordinating conjunction selected in step (c.i). If the passage does not sound acceptable, repeat step (c.i) until an acceptable coordinating conjunction is found. If the passage does not sound acceptable with any of the coordinating conjunctions from Table 1, assume that the (groups of) discourse segments under consideration are not related by a coherence relation. Iterative procedure for steps (a) and (b): Start with any of the unambiguous coordinating conjunctions from Table 1 (because, although, if . . . then, . . . said, for example).  258  Wolf and Gibson  Representing Discourse Coherence  Table 3 Statistics for texts in our database.  Number of words Number of discourse segments  Mean  545  61  Minimum 161  6  Maximum 1,409  143  Median  529  60  ii. (e) i. ii.  If none of the unambiguous coordinating conjunctions results in an acceptable passage, use the more ambiguous coordinating conjunctions (and, but, while, also, etc.). Important distinctions for steps (2) and (3) (this is based on practical issues that came up during the annotation project): Example versus elaboration: An example relation sets up an additional entity or event (the example), whereas an elaboration relation provides more details about an already introduced entity or event (the one on which one elaborates). Cause–effect versus temporal sequence: Both cause–effect and temporal sequence describe a temporal order of events (in cause–effect, the cause has to precede the effect). However, only cause–effect relations have a causal relation between what is stated by the (groups of) discourse segments under consideration. Thus, if there is a causal relation between the (groups of) discourse segments under consideration, assume cause–effect rather than temporal sequence (cf. Lascarides and Asher 1993).  2.5 Annotators  The annotators for the database were MIT undergraduate students who worked in our lab as research students. For training, the annotators received a manual that described the background of the project, discourse segmentation, coherence relations and how to recognize them, and how to use the annotation tools that we developed in our lab (Wolf et al. 2003). The ﬁrst author of this article provided training for the annotators. Training consisted of explaining the background of the project and the annotation method and of annotating example texts (these texts are not included in our database). Training took 8–10 hours in total, distributed over ﬁve days of a week. After completing the training, annotators worked independently.  2.6 Statistics on Annotated Database  In order to evaluate hypotheses about appropriate data structures for representing coherence structures, we have collected a database of 135 texts from the Wall Street Journal 1987–1989 (30 texts) and the AP Newswire 1989 (105 texts) (both from Harman and Liberman [1993]) in which the relations between discourse segments have been labeled with the coherence relations described above. Table 3 shows statistics for this database.  259  Computational Linguistics  Volume 31, Number 2  Steps 2 (discourse segment grouping) and 3 (coherence relation annotation) of the coding procedure described in section 2.4 were performed independently by two annotators. For step 1 (discourse segmentation), a pilot study on 10 texts showed that agreement on this step, as determined by number of common segments/(number of common segments + number of differing segments), was never below 90%. Therefore, all 135 texts were segmented by two annotators together, resulting in segmentations that both annotators could agree on. In order to determine interannotator agreement for step 2 of the coding procedure for the database of annotated texts, we calculated kappa statistics (Carletta 1996). We used the following procedure to construct a confusion matrix: First, all groups marked by either annotator were extracted. Annotator 1 had marked 2,616 groups, and annotator 2 had marked 3,021 groups in the whole database. The groups marked by the annotators consisted of 536 different discourse segment group types (for example, groups that included the ﬁrst two discourse segments of each text were marked 31 times by both annotators; groups that included the ﬁrst three discourse segments of each text were marked 6 times by both annotators). Therefore, the confusion matrix had 536 rows and columns. For all annotations of the 135 texts, the agreement was 0.8449, per chance agreement was 0.0161, and kappa was 0.8424. Annotator agreement did not differ as a function of text length, arc length, or kind of coherence relation (all χ2 values < 1). We also calculated kappa statistics to determine interannotator agreement for step 3 of the coding procedure.1 For all annotations of the 135 texts, the agreement was 0.8761, per chance agreement was 0.2466, and kappa was 0.8355. Annotator agreement did not differ as a function of text length (χ2 = 1.27, p < 0.75), arc length (χ2 < 1), or kind of coherence relation (χ2 < 1). Table 4 shows the confusion matrix for the database of 135 annotated texts that was used to compute the kappa statistics. The table shows, for example, that much of the interannotator disagreement seems to have been driven by disagreement over how to annotate elaboration relations (in the whole database, annotator 1 marked 260 elaboration relations where annotator 2 marked no relation; annotator 2 marked 467 elaboration relations where annotator 1 marked no relation). The only other comparable discourse annotation project that we are currently aware of is that of Carlson, Marcu, and Okurowski (2002).2 Since they use trees and split the annotation process into different substeps than those in our procedure, their annotator agreement ﬁgures are not directly comparable to ours. Furthermore, note that Carlson and her colleagues do not report annotator agreement ﬁgures for their database as a whole, but for different subsets of four to seven documents that were each annotated by different pairs of annotators. For discourse segmentation, they report kappa values ranging from 0.951 to 1.00; for annotation of discourse tree spans, their kappa values ranged from 0.778 to 0.929; for annotation of coherence relation nuclearity (whether a node in a discourse tree is a nucleus or a satellite, cf. section 2.3 for the deﬁnition of these terms), kappa values ranged from 0.695 to 0.882; for assigning types of coherence relations, they reported kappa values ranging from 0.624 to 0.823.  
First, to say how much I appreciate the completely unexpected honour of this award, and to thank the ACL for it. I want to look at one line, or thread, in natural language processing (NLP) research: how it began, what happened to it, what it suggests we need to investigate now. I shall take some papers of my own as pegs to hang the story on, but without making any claims for particular merit in these papers. My first paper, ‘‘The analogy between MT and IR,’’ with Margaret Masterman and Roger Needham, was for a conference in 1958. The analogy it proclaimed was in the need for a thesaurus, that is, a semantic classification. Machine translation (MT) and information retrieval (IR) are different tasks in their granularity and in the role of syntax. But we argued that both need a means of finding common concepts behind surface words, so we at once identify text content and word senses. We took Roget’s Thesaurus as such a tool in our MT experiments (with punched cards), where we exploited the redundancy that text always has to select class labels, that indicate senses, for words. The essential idea is illustrated, in extremely simple form (as with all my examples) in Figure 1. If we have to translate The farmer cultivates the field, where field has a range of senses including LAND and SUBJECT that may well have different target language equivalents, the fact that the general concept AGRICULTURE underlies each of farmer, cultivates, and field selects the sense LAND for field. But we found in our research that existing thesauruses, like Roget’s, were not wholly satisfactory, for example through missing senses; and we wanted to build a better one, ideally automatically. The natural way to do this, the obverse of the way the thesaurus would be applied once it was constructed, was by using text distribution data for words and applying statistical classification methods to these data. There were of course no corpora available then, so in my thesis work I finessed getting the input data for classification by taking dictionary definitions, which often consist of close synonym sets, as showing the most primitive and minimal shared- Ã Computer Laboratory, William Gates Building, JJ Thomson Avenue, Cambridge CB3 0FD, UK. E-mail: ksj@cl.cam.ac.uk. 
Marie¨t Theune- University of Twente  Emiel Krahmer. Tilburg University  This article challenges the received wisdom that template-based approaches to the generation of language are necessarily inferior to other approaches as regards their maintainability, linguistic well-foundedness, and quality of output. Some recent NLG systems that call themselves ‘‘template-based’’ will illustrate our claims. 1. Introduction Natural language generation (NLG) systems are sometimes partitioned into applicationdependent systems which lack a proper theoretical foundation, on the one hand, and theoretically well-founded systems which embody generic linguistic insights, on the other. Template-based systems are often regarded as automatically falling into the first category. We argue against this view. First, we describe the received view of both template-based and ‘‘standard’’ NLG systems (section 2). Then we describe a class of recent template-based systems (section 3) that will serve as a basis for a comparison between template-based and other NLG systems with respect to their potential for performing NLG tasks (section 4). We ask what the real difference between templatebased and other systems is and argue that the distinction between the two is becoming increasingly blurred (section 5). Finally, we discuss the implications of engineering shortcuts (Mellish 2000) and corpus-based methods (section 6). 2. Templates versus Real NLG: The Received View Before we can argue against the distinction between template-based and ‘‘real’’ NLG systems, we should first sketch how these two classes are commonly understood. It is surprisingly difficult to give a precise characterization of the difference between them (and we will later argue against the usefulness of such a characterization), but the idea is the following. Template-based systems are natural-language-generating systems that map their nonlinguistic input directly (i.e., without intermediate representations) to the linguistic surface structure (cf. Reiter and Dale 1997, pages 83–84). Crucially, this linguistic structure may contain gaps; well-formed output results when the gaps are Ã Computing Science Department, King’s College, University of Aberdeen, United Kingdom. E-mail: KvDeemter@csd.abdn.ac.uk. . Communication and Cognition/Computational Linguistics, Faculty of Arts, Tilburg University, Tilburg, The Netherlands. E-mail: E.J.Krahmer@uvt.nl. - Human Media Interaction Group, Computer Science, University of Twente, The Netherlands. E-mail: M.Theune@ewi.utwente.nl. * 2005 Association for Computational Linguistics  Computational Linguistics  Volume 31, Number 1  filled or, more precisely, when all the gaps have been replaced by linguistic structures that do not contain gaps. (Canned text is the borderline case of a template without gaps.) Adapting an example from Reiter and Dale (1997), a simple template-based system might start out from a semantic representation saying that the 306 train leaves Aberdeen at 10:00 AM: Departureðtrain306; locationabdn; time1000Þ and associate it directly with a template such as ½train is leaving ½town now where the gaps represented by [train] and [town] are filled by looking up the relevant information in a table. Note that this template will be used only when the time referred to is close to the intended time of speaking; other templates must be used for generating departure announcements relating to the past or future. ‘‘Real’’ or, as we shall say, standard NLG systems, by contrast, use a less direct mapping between input and surface form (Reiter 1995; Reiter and Dale 1997). Such systems could start from the same input semantic representation, subjecting it to a number of consecutive transformations until a surface structure results. Various NLG submodules would operate on it (determining, for instance, that 10:00 AM is essentially the intended time of speaking), jointly transforming the representation into an intermediate representation like Leavepresent ðtraindemonstrative; Aberdeen; nowÞ where lexical items and style of reference have been determined while linguistic morphology is still absent. This intermediate representation may in turn be transformed into a proper sentence, for example: This train is leaving Aberdeen now. Details vary; in particular, many systems will contain more intermediate representations. Template-based and standard NLG systems are said to be ‘‘Turing equivalent’’ (Reiter and Dale 1997); that is, each of them can generate all recursively enumerable languages. However, template-based systems have been claimed to be inferior with respect to maintainability, output quality and variation, and well-foundedness. Reiter and Dale (1997) state that template-based systems are more difficult to maintain and update (page 61) and that they produce poorer and less varied output (pages 60, 84) than standard NLG systems. Busemann and Horacek (1998) go even further by suggesting that template-based systems do not embody generic linguistic insights (page 238). Consistent with this view, template-based systems are sometimes overlooked. In fact, the only current textbook on NLG (Reiter and Dale 2000) does not pay any attention to template-based generation, except for a passing mention of the ECRAN system (Geldof and van de Velde 1997). Another example is a recent overview of NLG systems in the RAGS project (Cahill et al. 1999). The selection criteria employed by the authors were that the systems had to be fully implemented, complete (i.e., generating text from nontextual input), and accepting non-hand-crafted input; although these criteria appear to favor template based systems, none of the 19 systems investigated were template-based. In what follows, we claim that the two types of systems have more in common than is generally thought and that it is counterproductive to treat them as distant cousins instead of close siblings. In fact, we argue that there is no crisp distinction between the two.  16  van Deemter, Krahmer, and Theune  Real versus Template-Based NLG  3. Template-Based NLG Systems in Practice  In recent years, a number of new template-based systems have seen the light,  including TG/2 (Busemann and Horacek 1998), D2S (van Deemter and Odijk 1997;  Theune et al. 2001), EXEMPLARS (White and Caldwell 1998), YAG (McRoy, Channarukul,  and Ali 2003), and XTRAGEN (Stenzhorn 2002). Each of these systems represents a  substantial research effort, achieving generative capabilities beyond what is usually  expected from template-based systems, yet they call themselves template-based,  and they clearly fall within the characterization of template-based systems offered  above.  In this article we draw on our own experiences with a data-to-speech method  called D2S. D2S has been used as the foundation of a number of language-generating systems, including GOALGETTER, a system that generates soccer reports in Dutch.1 D2S  consists of two modules: (1) a language generation module (LGM) and (2) a speech  generation module (SGM) which turns the generated text into a speech signal. Here  we focus on the LGM and in particular on its use of syntactically structured templates  to convert a typed data structure into a natural language text (annotated with prosodic  information). Data structures in GOALGETTER are simple representations describing  lists of facts, such as      goal -event  TEAM  Ajax  PLAYER  Kluivert  MINUTE  38  GOAL-TYPE  penalty  Besides goal events, there are several other types of events, such as players receiving yellow or red cards. Figure 1 shows a simple template, which the LGM might use to express the above fact as, for instance, Kluivert scored a penalty in the 38th minute.  Figure 1 Sample syntactic template from the GOALGETTER system. 
 Terry KooÃ Massachusetts Institute of Technology  This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75% F-measure, a 13% relative decrease in Fmeasure error over the baseline model’s score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative—in terms of both simplicity and efficiency—to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation. 1. Introduction Machine-learning approaches to natural language parsing have recently shown some success in complex domains such as news wire text. Many of these methods fall into the general category of history-based models, in which a parse tree is represented as a derivation (sequence of decisions) and the probability of the tree is then calculated as a product of decision probabilities. While these approaches have many advantages, it can be awkward to encode some constraints within this framework. In the ideal case, the designer of a statistical parser would be able to easily add features to the model Ã MIT Computer Science and Artificial Intelligence Laboratory (CSAIL), the Stata Center, Building 32, 32 Vassar Street, Cambridge, MA 02139. Email: mcollins@csail.mit.edu, maestro@mit.edu. Submission received: 15th October 2003; Accepted for publication: 29th April 2004 * 2005 Association for Computational Linguistics  Computational Linguistics  Volume 31, Number 1  that are believed to be useful in discriminating among candidate trees for a sentence. In practice, however, adding new features to a generative or history-based model can be awkward: The derivation in the model must be altered to take the new features into account, and this can be an intricate task. This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). The algorithm can be viewed as a feature selection method, optimizing a particular loss function (the exponential loss function) that has been studied in the boosting literature. We applied the boosting method to parsing the Wall Street Journal (WSJ) treebank (Marcus, Santorini, and Marcinkiewicz 1993). The method combines the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The baseline model achieved 88.2% F-measure on this task. The new model achieves 89.75% Fmeasure, a 13% relative decrease in F-measure error. Although the experiments in this article are on natural language parsing, the approach should be applicable to many other natural language processing (NLP) problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation. See Collins (2002a) for an application of the boosting approach to named entity recognition, and Walker, Rambow, and Rogati (2001) for the application of boosting techniques for ranking in the context of natural language generation. The article also introduces a new, more efficient algorithm for the boosting approach which takes advantage of the sparse nature of the feature space in the parsing data. Other NLP tasks are likely to have similar characteristics in terms of sparsity. Experiments show an efficiency gain of a factor of 2,600 for the new algorithm over the obvious implementation of the boosting approach. Efficiency issues are important, because the parsing task is a fairly large problem, involving around one million parse trees and over 500,000 features. The improved algorithm can perform 100,000 rounds of feature selection on our task in a few hours with current processing speeds. The 100,000 rounds of feature selection require computation equivalent to around 40 passes over the entire training set (as opposed to 100,000 passes for the ‘‘naive’’ implementation). The problems with history-based models and the desire to be able to specify features as arbitrary predicates of the entire tree have been noted before. In particular, previous work (Ratnaparkhi, Roukos, and Ward 1994; Abney 1997; Della Pietra, Della Pietra, and Lafferty 1997; Johnson et al. 1999; Riezler et al. 2002) has investigated the use of Markov random fields (MRFs) or log-linear models as probabilistic models with global features for parsing and other NLP tasks. (Log-linear models are often referred to as maximum-entropy models in the NLP literature.) Similar methods have also been proposed for machine translation (Och and Ney 2002) and language understanding in dialogue systems (Papineni, Roukos, and Ward 1997, 1998). Previous work (Friedman, Hastie, and Tibshirani 1998) has drawn connections between log-linear models and  26  Collins and Koo  Discriminative Reranking for NLP  boosting for classification problems. One contribution of our research is to draw similar connections between the two approaches to ranking problems. We argue that the efficient boosting algorithm introduced in this article is an attractive alternative to maximum-entropy models, in particular, feature selection methods that have been proposed in the literature on maximum-entropy models. The earlier methods for maximum-entropy feature selection methods (Ratnaparkhi, Roukos, and Ward 1994; Berger, Della Pietra, and Della Pietra 1996; Della Pietra, Della Pietra, and Lafferty 1997; Papineni, Roukos, and Ward 1997, 1998) require several full passes over the training set for each round of feature selection, suggesting that at least for the parsing data, the improved boosting algorithm is several orders of magnitude more efficient.1 In section 6.4 we discuss our approach in comparison to these earlier methods for feature selection, as well as the more recent work of McCallum (2003); Zhou et al. (2003); and Riezler and Vasserman (2004). The remainder of this article is structured as follows. Section 2 reviews historybased models for NLP and highlights the perceived shortcomings of history-based models which motivate the reranking approaches described in the remainder of the article. Section 3 describes previous work (Friedman, Hastie, and Tibshirani 2000; Duffy and Helmbold 1999; Mason, Bartlett, and Baxter 1999; Lebanon and Lafferty 2001; Collins, Schapire, and Singer 2002) that derives connections between boosting and maximum-entropy models for the simpler case of classification problems; this work forms the basis for the reranking methods. Section 4 describes how these approaches can be generalized to ranking problems. We introduce loss functions for boosting and MRF approaches and discuss optimization methods. We also derive the efficient algorithm for boosting in this section. Section 5 gives experimental results, investigating the performance improvements on parsing, efficiency issues, and the effect of various parameters of the boosting algorithm. Section 6 discusses related work in more detail. Finally, section 7 gives conclusions. The reranking models in this article were originally introduced in Collins (2000). In this article we give considerably more detail in terms of the algorithms involved, their justification, and their performance in experiments on natural language parsing. 2. History-Based Models Before discussing the reranking approaches, we describe history-based models (Black et al. 1992). They are important for a few reasons. First, several of the best-performing parsers on the WSJ treebank (e.g., Ratnaparkhi 1997; Charniak 1997, 2000; Collins 1997, 1999; Henderson 2003) are cases of history-based models. Many systems applied to part-of-speech tagging, speech recognition, and other language or speech tasks also fall into this class of model. Second, a particular history-based model (that of Collins [1999]) is used as the initial model for our approach. Finally, it is important to describe history-based models—and to explain their limitations—to motivate our departure from them. Parsing can be framed as a supervised learning task, to induce a function f : X YY given training examples ðxi, yiÞ, where xi Z X , yi Z Y. We define GENðxÞÎY to be the set of candidates for a given input x. In the parsing problem x is a sentence, and 
University of Pennsylvania  Daniel Gildea. University of Rochester  The Proposition Bank project takes a practical approach to semantic representation, adding a layer of predicate-argument information, or semantic role labels, to the syntactic structures of the Penn Treebank. The resulting resource can be thought of as shallow, in that it does not represent coreference, quantification, and many other higher-order phenomena, but also broad, in that it covers every instance of every verb in the corpus and allows representative statistics to be calculated. We discuss the criteria used to define the sets of semantic roles used in the annotation process and to analyze the frequency of syntactic/semantic alternations in the corpus. We describe an automatic system for semantic role tagging trained on the corpus and discuss the effect on its performance of various types of information, including a comparison of full syntactic parsing with a flat representation and the contribution of the empty ‘‘trace’’ categories of the treebank. 1. Introduction Robust syntactic parsers, made possible by new statistical techniques (Ratnaparkhi 1997; Collins 1999, 2000; Bangalore and Joshi 1999; Charniak 2000) and by the availability of large, hand-annotated training corpora (Marcus, Santorini, and Marcinkiewicz 1993; Abeille´ 2003), have had a major impact on the field of natural language processing in recent years. However, the syntactic analyses produced by these parsers are a long way from representing the full meaning of the sentences that are parsed. As a simple example, in the sentences (1) John broke the window. (2) The window broke. a syntactic analysis will represent the window as the verb’s direct object in the first sentence and its subject in the second but does not indicate that it plays the same underlying semantic role in both cases. Note that both sentences are in the active voice Ã Department of Computer and Information Science, University of Pennsylvania, 3330 Walnut Street, Philadelphia, PA 19104. Email: mpalmer@cis.upenn.edu. . Department of Computer Science, University of Rochester, PO Box 270226, Rochester, NY 14627. Email: gildea@cs.rochester.edu. Submission received: 9th December 2003; Accepted for publication: 11th July 2004 * 2005 Association for Computational Linguistics  Computational Linguistics  Volume 31, Number 1  and that this alternation in subject between transitive and intransitive uses of the verb does not always occur; for example, in the sentences  (3) The sergeant played taps. (4) The sergeant played.  the subject has the same semantic role in both uses. The same verb can also undergo syntactic alternation, as in  (5) Taps played quietly in the background.  and even in transitive uses, the role of the verb’s direct object can differ:  (6) The sergeant played taps. (7) The sergeant played a beat-up old bugle.  Alternation in the syntactic realization of semantic arguments is widespread, affecting most English verbs in some way, and the patterns exhibited by specific verbs vary widely (Levin 1993). The syntactic annotation of the Penn Treebank makes it possible to identify the subjects and objects of verbs in sentences such as the above examples. While the treebank provides semantic function tags such as temporal and locative for certain constituents (generally syntactic adjuncts), it does not distinguish the different roles played by a verb’s grammatical subject or object in the above examples. Because the same verb used with the same syntactic subcategorization can assign different semantic roles, roles cannot be deterministically added to the treebank by an automatic conversion process with 100% accuracy. Our semantic-role annotation process begins with a rule-based automatic tagger, the output of which is then handcorrected (see section 4 for details). The Proposition Bank aims to provide a broad-coverage hand-annotated corpus of such phenomena, enabling the development of better domain-independent language understanding systems and the quantitative study of how and why these syntactic alternations take place. We define a set of underlying semantic roles for each verb and annotate each occurrence in the text of the original Penn Treebank. Each verb’s roles are numbered, as in the following occurrences of the verb offer from our data:  (8) . . . [ the company] to . . . offer [ a 15% to 20% stake] [ to the public]  (wsjA_r0g0345)1  Arg1  Arg2  (9) . . . [ Sotheby’s] . . . offered [ the Dorrance heirs] [ a money-back  Arg0  Arg2  Arg1  guarantee] (wsj_1928)  (10) . . . [ an amendment] offered [ by Rep. Peter DeFazio] . . . (wsj_0107)  Arg1  Arg0  (11) . . . [ Subcontractors] will be offered [ a settlement] . . . (wsj_0187)  Arg2  Arg1  We believe that providing this level of semantic representation is important for applications including information extraction, question answering, and machine  
Centro de Informa´tica e Tecnologias de InformaZa˜ o  Alexandre Agustini. Pontif´ıcia Universidade Cato´ lica do Rio Grande do Sul, Centro de Informa´tica e Tecnologias de InformaZa˜o  This article describes an unsupervised strategy to acquire syntactico-semantic requirements of nouns, verbs, and adjectives from partially parsed text corpora. The linguistic notion of requirement underlying this strategy is based on two specific assumptions. First, it is assumed that two words in a dependency are mutually required. This phenomenon is called here corequirement. Second, it is also claimed that the set of words occurring in similar positions defines extensionally the requirements associated with these positions. The main aim of the learning strategy presented in this article is to identify clusters of similar positions by identifying the words that define their requirements extensionally. This strategy allows us to learn the syntactic and semantic requirements of words in different positions. This information is used to solve attachment ambiguities. Results of this particular task are evaluated at the end of the article. Extensive experimentation was performed on Portuguese text corpora. 1. Introduction Word forms, as atoms, cannot arbitrarily combine with each other. They form new composites by both imposing and satisfying certain requirements. A word uses a linguistic requirement (constraint or preference) in order to restrict the type of words with which it can combine in a particular position. The requirement of a given word is characterized by at least two different objects: the position occupied by the words that can be combined with the given word and the condition that those words must satisfy in order to be in that position. For a word w and a specific description of a location loc, the pair bloc, wÀ represents a position with regard to w. In addition, Ã Departamento de L´ıngua Espanhola, Faculdade de Filologia, Universidade de Santiago de Compostela, Campus Universitario Norte, 15782 Santiago de Compostela, Spain. E-mail: gamallo@fct.unl.pt. . Faculdade de Informa´tica, Pontif´ıcia Universidade Cato´ lica do Rio Grande do Sul, Av. Ipiranga 6681 pre´dio 30 bloco 4, CEP 90619-900 Porto Alegre (RS), Brazil. E-mail: agustini@inf.pucrs.br. - Department of Computer Science, Faculty of Science and Technology, Universidade Nova de Lisboa, Quinta da Torre, 2829-516, Caparica, Portugal. E-mail: gpl@di.fct.unl.pt. Submission received: 13th June 2004; Revised submission received: 4th May 2004; Accepted for publication: 17th June 2004 * 2005 Association for Computational Linguistics  Computational Linguistics  Volume 31, Number 1  condition cond represents the set of linguistic properties that words must satisfy in order to be in position bloc, wÀ. So a linguistic requirement of w can be represented as the pair:  bbloc, wÀ, condÀ  ð1Þ  Consider, for instance, position bof_right, ratificationÀ, where of_right is a location described as being to the right of preposition of. This position represents the argument slot ratification of [_ ]. Considering also that cond stands for the specific property being a nominal phrase (np) whose head denotes a legal document (abbreviated by doc), then the pair bbof_right, ratificationÀ, docÀ means that the particular position ratification of [_ ] selects for nouns denoting legal documents. In other words, ratification requires nominal arguments denoting legal documents to appear after preposition of. Suppose that there exist some words such as law, treaty, and constitution that are nouns denoting legal documents. Then it follows that they fill the condition imposed by ratification in the of_right location. An expression like the ratification of the treaty is then well-formed, because treaty satisfies the required condition. Let us look now more carefully at several linguistic issues we consider to be important to characterize the notion of linguistic requirement: extensionality/ intensionality, soft/hard requirements, the scope of a condition, syntactic/semantic requirements, and corequirements. A condition can be defined either intentionally or extensionally. For example, the two specific properties being the head of an np and being a legal document are used to define intensionally the condition imposed by position bof_right, ratificationÀ. However, it is also possible to define it extensionally by enumerating all those words that actually possess such properties: for example, law, treaty, and constitution. Moreover, the process of satisfying a condition can be defined as a binary action producing a Boolean (yes/no) value. From this point of view, a word either satisfies or does not satisfy the condition imposed by another word in a specific location. This is a hard requirement. By contrast, the satisfaction process can also be viewed as a soft requirement, in which some words are ‘‘preferred’’ without completely excluding other possibilities. In Beale, Niremburg, and Viegas (1998), hard requirements are named constraints, whereas the term preferences is employed for soft requirements. In the following, we use one of these two terms only if it is necessary to distinguish between hard and soft requirements. Otherwise, requirement is taken as the default term. Let’s describe now what we call the scope of a condition. A position imposes a specific condition on the words that can appear in that position. Yet a specific condition is generally imposed not by only one position, but by a large set of them. If a condition were bound only to a particular position, every combination of words would be a noncompositional idiomatic expression. So speakers could not combine words easily, and new composite expressions would be difficult to learn. The scope of a condition embraces the positions that use it to restrict word combination. For instance, the condition imposed by ratification of [_ ] seems to be the same as the one imposed by the verb ratify on the words appearing at its right: bright, ratifyÀ (to ratify [_ ]). In addition, these positions also share the same conditions as to approve [_ ], to sign [_ ], or signatories to [_ ]. Each of these similar positions is within the scope of a specific condition, namely, being an np whose head denotes a legal document. In this article, we assume that every linguistic condition is associated with a set of similar positions. This  108  Gamallo, Agustini, and Lopes  Clustering Syntactic Positions  set represents the scope of the condition. The larger the set of similar positions, the larger the condition scope, and the more general the property used to characterize the condition. We distinguish syntactic and semantic requirements. A syntactic requirement is characterized by both a position and a morpho-syntactic condition. For instance, requirement bbof_right, ratificationÀ, npÀ consists of a position, bof_right, ratificationÀ, which selects for a nominal phrase. Note that the different syntactic requirements of a word can serve to identify the set of subcategorization frames of that word. Note also that, in some cases, a particular position presupposes a particular morpho-syntactic condition. In our example, position bof_right, ratificationÀ requires only a np. So we can use this position as a shorter form of the syntactic requirement bbof_right, ratificationÀ, npÀ. We call a syntactic position a position that presupposes a specific morphosyntactic condition. On the other hand, a semantic requirement (also known as selection restriction) is characterized by both a position and a semantic condition, which presupposes a syntactic one. So bbof_right, ratificationÀ,docÀ means that position bof_right, ratificationÀ selects for the head of a np denoting a legal document. Condition doc presupposes then a np. Identifying a particular semantic requirement entails the identification of the underlying syntactic one. The final linguistic issue to be introduced is the phenomenon referred to as corequirements. It is assumed that each syntactic dependency between two words (which are the heads of two phrases) is composed of two complementary requirements. For instance, it seems that two different requirements underlie the expression ratification of the treaty: bof_right, ratificationÀ (ratification of [_ ]) needs to be filled by words like treaty, while bof_left, treatyÀ ([_ ] of the treaty) needs to appear with words such as ratification. The main objective of this article is to describe an unsupervised method for learning syntactic and semantic requirements from large text corpora. For instance, our method discovers that the word secretary is associated with several syntactic positions (i.e., positions with morpho-syntactic conditions), such as secretary of [_ ], [_ ] of the secretary, [_ ] to the secretary, and [_ ] with the secretary. The set of syntactic positions defined by a word can be used to characterize a set of subcategorization frames. The precise characterization of these frames remains, however, beyond the scope of this article. In addition, for each syntactic position, we assess the specific semantic condition a word needs to fill in order to appear in that position. Another important objective of the article is to use the semantic requirements to capture contextually relevant semantic similarities between words. In particular, we assume that two words filling the same semantic requirement share the same contextual word sense. Consequently, learning semantic requirements also leads us to induce word senses. Suppose that the word organization fills the condition imposed by secretary of [_ ]. In this syntactic context, the word denotes a social institution and not a temporal process or an abstract setup. To achieve our objectives, we follow a particular clustering strategy. Syntactic positions (and not words) are compared according to their word distribution. Similar syntactic positions are put in more clusters following some constraints that are defined later. Each cluster of positions represents a semantic condition. The features of each cluster are the words that can fill the common condition imposed by those positions: They are the fillers. They are used to extensionally define the particular condition they can fill. That is, a condition is defined by identifying those words likely to appear in positions considered similar. Given that a condition is extensionally defined by the words that are able to fill it, our method describes the process of  109  Computational Linguistics  Volume 31, Number 1  satisfying a condition as a Boolean constraint (yes/no) and not as a probabilistic preference. The similar positions defining a cluster are within the scope of a particular semantic condition. The association between each position of the cluster and that condition characterizes the semantic requirement of a word. This learning strategy does not require hand-crafted external resources such as a WordNet-like thesaurus or a machine-readable dictionary. The information captured by this strategy is useful for two different NLP disambiguation tasks: selecting contextual senses of words (word sense disambiguation) and solving structural ambiguity (attachment resolution). This article is focused on the latter application. In sum, the main contribution of our work is the large amount of linguistic information we learn for each lexical word. Given a word, we acquire, at least, three types of information: (1) an unordered set of syntactic positions, which is a first approximation to define the set of subcategorization frames of the given word, (2) the semantic requirements the word imposes on its arguments, and (3) the different contextual senses of the word. By contrast, related work focuses only on one or two aspects of this linguistic information. Another contribution is the use of corequirements to characterize the arguments of a word. To conclude the introduction, let’s outline the organization of the article. In the next section, we situate our approach with regard to related work on acquisition of linguistic requirements. Later, in sections 3 and 4, we describe in detail the main linguistic assumptions underlying our approach. Special attention will be paid to both the relativized view on word sense (i.e., contextual sense) and corequirements. Then, section 5 depicts a general overview of our strategy. Two particular aspects of this strategy are analyzed next. More precisely, section 6 describes both how syntactic positions are extracted and how they are clustered in larger classes (section 7). Finally, in section 8, we evaluate the results by measuring their performance in a particular NLP task: syntactic-attachment resolution.  2. Statistics-Based Methods for Learning Linguistic Requirements During the last years, various stochastic approaches to linguistic requirements acquisition have been proposed (Basili, Pazienza, and Velardi 1992; Hindle and Rooth 1993; Sekine et al. 1992; Grishman and Sterling 1994; Framis 1995; Dagan, Marcus, and Markovitch 1995; Resnik 1997; Dagan, Lee, and Pereira 1998; Marques, Lopes, and Coelho 2000; Ciaramita and Johnson 2000). In general, they follow comparable learning strategies, despite significant differences observed. In this section, we present first the common strategy followed by these approaches, and then we focus on their differences. Special attention is paid to lexical methods. At the end, we situate our strategy with regard to the related work. 2.1 A Common Strategy The main design of the strategy for automatically learning requirements is to compute the association degree between argument positions and their respective linguistic conditions. For this purpose, the first task is to count the frequency with which bbloc, wÀ, condÀ occurs in a large corpus:  Fðbbloc, wÀ, cond ÀÞ  ð2Þ  110  Gamallo, Agustini, and Lopes  Clustering Syntactic Positions  where F counts the frequency of co-occurring bloc, wÀ with cond. Then this frequency is used to compute the conditional probability of cond given position bloc, wÀ:  Pðcond j bloc, wÀÞ  ð3Þ  This probability is then used to measure the strength of statistical association between bloc, wÀ and cond. Association measures such as mutual information or log-likelihood are used for measuring the degree of (in)dependence between these two linguistic objects. Intuitively, a high value of the association measure is evidence of the existence of a true requirement (i.e., a type of linguistic dependence). The stochastic association values obtained by such a strategy turn out to be useful for NLP disambiguation tasks such as attachment resolution in probabilistic parsing and sense disambiguation. 2.2 Specific Aspects of the Common Strategy Despite the apparent methodological unanimity, approaches to learning requirements propose different definitions for the following objects: association measure, position bloc, wÀ, and linguistic condition cond. Many approaches differ only in the way in which the association measure is defined. Yet such differences are not discussed in this article. As regards position bloc, wÀ, we distinguish, at least, among three different definitions. First, it can be considered as a mere word sequence (Dagan, Marcus, and Markovitch 1995): For instance, bright, wÀ, where right means being to the right of. Second, a position can also be defined in terms of co-occurrence within a fixed window (Dagan, Lee, and Pereira 1998; Marques, Lopes, and Coelho 2000). Finally, it can be identified as the head or the dependent role within a binary grammatical relationship such as subject, direct object, or modifier (Sekine et al. 1992; Grishman and Sterling 1994; Framis 1995). In section 4, we pay special attention to the grammatical characterization of syntactic positions. As far as cond is concerned, various types of information are used to define a linguistic condition: syntactic, semantic, and lexical information. The approaches to learning requirements are easily distinguished by how they define cond. Table 1 displays three different ways of encoding the condition imposed by verb approve to the nominal the law in the expression to approve the law. Requirement conditions of the pairs in Table 1 represent three descriptive levels for the linguistic information underlying the nominal expression the law when it appears to the right of the verb approve.1 The properties np, doc, and law are situated at different levels of abstraction. The morpho-syntactic tag np conveys more abstract information than the semantic tag doc (document), which, in turn, is more general than the lemma law. Some conditions can be inferred from other conditions. For instance, doc is used only to tag nouns, which are the heads of nominal phrases. So the semantic tag doc entails the syntactic requirement np. Likewise, the lemma law is associated only with nouns. It entails, then, an np. Some approaches describe linguistic conditions only at the syntactic level (Hindle and Rooth 1993; Marques, Lopes, and Coelho 2000). They count the frequency of pairs  
We present a treebank conversion method by which we construct an RMRS bank for HPSG parser evaluation from the TIGER Dependency Bank. Our method eﬀectively performs automatic RMRS semantics construction from functional dependencies, following the semantic algebra of Copestake et al. (2001). We present the semantics construction mechanism, and focus on some special phenomena. Automatic conversion is followed by manual validation. First evaluation results yield high precision of the automatic semantics construction rules. 
An attempt was made to semi-automatically obtain “lexical units” (LUs) for Japanese from the English LUs deﬁned in the semantic frame database provided by Berkeley FrameNet (BFN) using an English-Japanese bilingual corpus. This task was a prerequisite to building a complete database of semantic frames for Japanese. In the task, a Japanese word is ﬁrst translated into an English word or phrase, E. E is one of the lexical units that evoked a particular semantic frame, F, in the BFN database. When other lexical units of F are translated back into Japanese, this deﬁnes a candidate set of F for the lexical units of F in Japanese. The viability of the proposed method was tested on a Japanese verb (X-ga Y -wo) osou (roughly meaning “X attack(s) Y ,” “X hit(s) Y ,” “X surprise(s) Y ” in English, showing that it is a relatively polysemous word). The resulting translation was compared to semantic descriptions provided by IPAL and Nihongo Goi-Taikei (A Japanese Lexicon), two well-known language resources for Japanese, and also by the Frame Oriented Concept Analysis of Language (FOCAL). The comparison revealed that FOCAL, BFN, Goi Taikei, and IPAL provided ﬁner-grained descriptions in this speciﬁc order. 
The purpose of this paper is to automatically generate Chinese chunk bracketing by a bottom-to-top mapping (BTM) model with a BTM dataset. The BTM model is designed as a supporting model with parsers. We define a word-layer matrix to generate the BTM dataset from Chinese Treebank. Our model matches auto-learned patterns and templates against segmented and POS-tagged Chinese sentences. A sentence that can be matched with some patterns or templates is called a matching sentence. The experimental results have shown that the chunk bracketing of the BTM model on the matching sentences is high and stable. By applying the BTM model to the matching sentences and the Ngram model to the non-matching sentences, the experiment results show the F-measure of an N-gram model can be improved. 
We have constructed a large scale and detailed database of lexical types in Japanese from a treebank that includes detailed linguistic information. The database helps treebank annotators and grammar developers to share precise knowledge about the grammatical status of words that constitute the treebank, allowing for consistent large scale treebanking and grammar development. In this paper, we report on the motivation and methodology of the database construction. 
In this paper, w e introduce our corpora under development, which are recorded in a real environment. These corpora comprise dialogues collected in hospitals with the aim of developing a nursing service support system through a com prehensive understanding of nursing activities. We use the corpora to analyze how nurses perform their nursing duties and how they express the perform ance of their tasks. To understand nursing activities, w e investigated nursing services and the relevant medical charts by using the corpora. In the paper, w e show features and prom ising applications of the corpora. 
The International Corpus of English is a corpus of national and regional varieties of English. The mega-word British component has been constructed, grammatically tagged, and syntactically parsed. This article is a description of work that aims at the automatic induction of a wide-coverage grammar from this corpus as well as an empirical evaluation of the grammar. It first of all describes the corpus and its annotation schemes and then presents empirical statistics for the grammar. I will then evaluate the coverage and the accuracy of such a grammar when applied automatically in a parsing system. Results show that the grammar enabled the parser to achieve 86.1% recall rate and 83.5% precision rate. 
We present a system that automatically identiﬁes Attribution, an intrasentential relation in the RST Treebank. The system uses uses syntactic information from Penn Treebank parse trees. It identiﬁes Attributions as structures in which a verb takes an SBAR complement, and achieves a f-score of .92. This supports our claim that the Attribution relation should be eliminated from a discourse treebank, since it represents information that is already present in the Penn Treebank, in a different form. More generally, we suggest that intra-sentential relations in the RST Treebank might all be eliminable in this way. 
Many NLP tasks that require syntactic analysis necessitate an accurate description of the lexical components, morpho-syntactic constraints and the semantic idiosyncracies of ﬁxed expressions. (Moon, 1998) and (Riehemann, 2001) show that many ﬁxed expressions and idioms allow limited variation and modiﬁcation inside their complementation. This paper discusses to what extent a corpus-based method can help us establish the variation and adjectival modiﬁcation potential of Dutch support verb constructions. We also discuss what problems the data poses when applying an automated data-driven method to solve the problem. 
In this paper, we discuss how error annotation for learner corpora should be done by explaining the state of the art of error tagging schemes in learner corpus research. Several learner corpora, including the NICT JLE (Japanese Learner English) Corpus that we have compiled are annotated with error tagsets designed by categorizing “likely” errors implied from the existing canonical grammar rules or POS (part-of-speech) system in advance. Such error tagging can help to successfully assess to what extent learners can command the basic language system, especially grammar, but is insufficient for describing learners’ communicative competence. To overcome this limitation, we reexamined learner language in the NICT JLE Corpus by focusing on “intelligibility” and “naturalness”, and determined how the current error tagset should be revised. 
This article is devoted to the problem of quantifying noun groups in German. After a thorough description of the phenom ena, the results of corpus-based investigations are described. Moreover, some examples are given that underline the necessity of integrating some kind of information other than grammar sensu stricto into the treebank. We argue that a more sophisticated and ﬁne-grained annotation in the treebank would have very positve effects on stochastic parsers trained on the treebank and on grammars induced from the treebank, and it would make the treebank more valuable as a source of data for theoretical linguistic investigations. The information gained from corpus research and the analyses that are proposed are realized in the framework of SILVA, a parsing and extraction tool for German text corpora. 
This paper proposes an annotating scheme that encodes honorifics (respectful words). Honorifics are used extensively in Japanese, reflecting the social relationship (e.g. social ranks and age) of the referents. This referential information is vital for resolving zero pronouns and improving machine translation outputs. Annotating honorifics is a complex task that involves identifying a predicate with honorifics, assigning ranks to referents of the predicate, calibrating the ranks, and connecting referents with their predicates. 
The lack of readily-available large corpora of aligned monolingual sentence pairs is a major obstacle to the development of Statistical Machine Translation-based paraphrase models. In this paper, we describe the use of annotated datasets and Support Vector Machines to induce larger monolingual paraphrase corpora from a comparable corpus of news clusters found on the World Wide Web. Features include: morphological variants; WordNet synonyms and hypernyms; loglikelihood-based word pairings dynamically obtained from baseline sentence alignments; and formal string features such as word-based edit distance. Use of this technique dramatically reduces the Alignment Error Rate of the extracted corpora over heuristic methods based on position of the sentences in the text. 
An obstacle to research in automatic paraphrase identification and generation is the lack of large-scale, publiclyavailable labeled corpora of sentential paraphrases. This paper describes the creation of the recently-released Microsoft Research Paraphrase Corpus, which contains 5801 sentence pairs, each hand-labeled with a binary judgment as to whether the pair constitutes a paraphrase. The corpus was created using heuristic extraction techniques in conjunction with an SVM-based classifier to select likely sentence-level paraphrases from a large corpus of topicclustered news data. These pairs were then submitted to human judges, who confirmed that 67% were in fact semantically equivalent. In addition to describing the corpus itself, we explore a number of issues that arose in defining guidelines for the human raters. 
The task of machine translation (MT) evaluation is closely related to the task of sentence-level semantic equivalence classiﬁcation. This paper investigates the utility of applying standard MT evaluation methods (BLEU, NIST, WER and PER) to building classiﬁers to predict semantic equivalence and entailment. We also introduce a novel classiﬁcation method based on PER which leverages part of speech information of the words contributing to the word matches and non-matches in the sentence. Our results show that MT evaluation techniques are able to produce useful features for paraphrase classiﬁcation and to a lesser extent entailment. Our technique gives a substantial improvement in paraphrase classiﬁcation accuracy over all of the other models used in the experiments. 
Towards deep analysis of compositional classes of paraphrases, we have examined a class-oriented framework for collecting paraphrase examples, in which sentential paraphrases are collected for each paraphrase class separately by means of automatic candidate generation and manual judgement. Our preliminary experiments on building a paraphrase corpus have so far been producing promising results, which we have evaluated according to cost-efﬁciency, exhaustiveness, and reliability. 
We present a natural language generator that produces a range of medical reports on the clinical histories of cancer patients, and discuss the problem of conceptual restatement in generating various textual views of the same conceptual content. We focus on two features of our system: the demand for “loose paraphrases” between the various reports on a given patient, with a high degree of semantic overlap but some necessary amount of distinctive content; and the requirement for paraphrasing at primarily the discourse level. 
News on electrical bulletin boards consist of high density expressions. Many sentences end with unique expressions that consist of nouns and case particles. This paper focuses on expressions used at the end of sentences and attempts to summarize them by forming noun or case particle endings. We summarize the news sentence through pattern matching approach. Our evaluation illustrates that the summarizer reduces 2.50 characters per sentence on average; the reduction ratio is 6%. We also show that people perceive the correct meanings of the summarized sentences with 95% accuracy. 
Rather than creating and storing thousands of paraphrase examples, paraphrase templates have strong representation capacity and can be used to generate many paraphrase examples. This paper describes a new template representation and generalization method. Combing a semantic dictionary, it uses multiple semantic codes to represent a paraphrase template. Using an existing search engine to extend the word clusters and generalize the examples. We also design three metrics to measure our generalized templates. The experimental results show that the representation method is reasonable and the generalized templates have a higher precision and coverage. 
We propose a method that automatically generates paraphrase sets from seed sentences to be used as reference sets in objective machine translation evaluation measures like BLEU and NIST. We measured the quality of the paraphrases produced in an experiment, i.e., (i) their grammaticality: at least 99% correct sentences; (ii) their equivalence in meaning: at least 96% correct paraphrases either by meaning equivalence or entailment; and, (iii) the amount of internal lexical and syntactical variation in a set of paraphrases: slightly superior to that of hand-produced sets. The paraphrase sets produced by this method thus seem adequate as reference sets to be used for MT evaluation. 
This paper presents an evaluation method employing a latent variable model for paraphrases with their contexts. We assume that the context of a sentence is indicated by a latent variable of the model as a topic and that the likelihood of each variable can be inferred. A paraphrase is evaluated for whether its sentences are used in the same context. Experimental results showed that the proposed method achieves almost 60% accuracy and that there is not a large performance difference between the two models. The results also revealed an upper bound of accuracy of 77% with the method when using only topic information. 
Research on paraphrase has mostly focussed on lexical or syntactic variation within individual sentences. Our concern is with larger-scale paraphrases, from multiple sentences or paragraphs to entire documents. In this paper we address the problem of generating paraphrases of large chunks of texts. We ground our discussion through a worked example of extending an existing NLG system to accept as input a source text, and to generate a range of ﬂuent semantically-equivalent alternatives, varying not only at the lexical and syntactic levels, but also in document structure and layout. 
Automatic paraphrase discovery is an important but challenging task. We propose an unsupervised method to discover paraphrases from a large untagged corpus, without requiring any seed phrase or other cue. We focus on phrases which connect two Named Entities (NEs), and proceed in two stages. The first stage identifies a keyword in each phrase and joins phrases with the same keyword into sets. The second stage links sets which involve the same pairs of individual NEs. A total of 13,976 phrases were grouped. The accuracy of the sets in representing paraphrase ranged from 73% to 99%, depending on the NE categories and set sizes; the accuracy of the links for two evaluated domains was 73% and 86%. 
Summary sentences are often paraphrases of existing sentences. They may be made up of recycled fragments of text taken from important sentences in an input document. We investigate the use of a statistical sentence generation technique that recombines words probabilistically in order to create new sentences. Given a set of event-related sentences, we use an extended version of the Viterbi algorithm which employs dependency relation and bigram probabilities to ﬁnd the most probable summary sentence. Using precision and recall metrics for verb arguments as a measure of grammaticality, we ﬁnd that our system performs better than a bigram baseline, producing fewer spurious verb arguments. 
Domain knowledge is the fundamental resources required by all intelligent information processing systems. With the upsurge of new technology and new products in various domains, the manual construction and updating of domain knowledge base can hardly meet the real needs of application systems, in terms of coverage or effectiveness. Based on natural language text analysis, this paper intends to draw a basic framework for the construction of domain knowledge base. Using encyclopedia resources and text information resources on the Web, we focus on the method of constructing domain knowledge base through technologies in natural language text analysis and machine learning. Moreover, an open network platform will be developed, through which common users can work with domain experts to contribute domain knowledge. The technology can be applied to the construction and updating of domain knowledge base for intelligent  information processing, and it can also provide help for the knowledge updating of encyclopedias. Keywords: Domain knowledge base, Natural language text analysis, machine learning, encyclopedia, open platform 
Parsing is one of the important processes for natural language processing and, in general, a large-scale CFG is used to parse a wide variety of sentences. For many languages, a CFG is derived from a large-scale syntactically annotated corpus, and many parsing algorithms using CFGs have been proposed. However, we could not apply them to Japanese since a Japanese syntactically annotated corpus has not been available as of yet. In order to solve the problem, we have been building a large-scale Japanese syntactically annotated corpus. In this paper, we show the evaluation results of a CFG derived from our corpus and compare it with results of some Japanese dependency analyzers. 
The acquisition of grammar from a corpus is a challenging task in the preparation of a knowledge bank. In this paper, we discuss the extraction of Chinese grammar oriented to a restricted corpus. First, probabilistic context-free grammars (PCFG) are extracted automatically from the Penn Chinese Treebank and are regarded as the baseline rules. Then a corpusoriented grammar is developed by adding specific information including head information from the restricted corpus. Then, we describe the peculiarities and ambiguities, particularly between the phrases “PP” and “VP” in the extracted grammar. Finally, the parsing results of the utterances are used to evaluate the extracted grammar. 
The normalization of corpus metadata plays a key role in building sharable corpora. However, there is no uniform specification for defining and processing metadata in Chinese corpus nowadays. This paper introduces a metadata system we’ve proposed for Chinese corpus. 46 elements are defined in all, which can be divided into 6 classes: information about copyright, information about background of linguistic material creator, information about medium of linguistic material, information about the content of linguistic material, information about collecting linguistic material, and information about management of linguistic material. To distinguish one element from another, or our elements from someone else’s, we provide a potent description method, where 10 subsections are designed to describe the detailed properties for each element. 
Preservation of an endangered language is an important and difficult task. The preservation project should include documentation, archiving and development of shared resources for the endangered language. In addition, the project will consider how to revitalize this endangered language among the younger generation. In this paper, we propose an integrated framework that will connect the three different tasks: language archiving, language processing and creating learning materials. We are using this framework to document one Taiwanese aboriginal language: Yami. The proposed framework should be an effective tool for documenting other endangered languages in Asia.  to this language. It is also important to find an effective approach to teach the endangered language to the ethnic group using the language, particularly to members of the younger generation, who often live in urban areas without any connection to their place of origin. According to a study by Whaley (2003), the factors required to help an endangered language survive include: 1. a well developed preservation and maintenance program for the language; 2. use of information technology in the preserving project; 3. a new world order, especially economic and political shifts; 4. an environment for learning and exploring the language.  
This paper describes the structural annotation of a spoken dialogue corpus. By statistically dealing with the corpus, the automatic acquisition of dialoguestructural rules is achieved. The dialogue structure is expressed as a binary tree and 789 dialogues consisting of 8150 utterances in the CIAIR speech corpus are annotated. To evaluate the scalability of the corpus for creating dialogue-structural rules, a dialogue parsing experiment was conducted.  
Parallel wordnets built upon correspondences between different languages can play a crucial role in multilingual knowledge processing. Since there is no homomorphism between pairs of monolingual wordnets, we must rely on lexical semantic relation (LSR) mappings to ensure conceptual cohesion. In this paper, we propose and implement a model for bootstrapping parallel wordnets based on one monolingual wordnet and a set of cross-lingual lexical semantic relations. In particular, we propose a set of inference rules to predict Chinese wordnet structure based on English wordnet and English-Chinese translation relations. We show that this model of parallel wordnet building is effective and achieves higher precision in LSR prediction. 
Taiwan Child Language Corpus contains scripts transcribed from about 330 hours of recordings of fourteen young children from Southern Min Chinese speaking families in Taiwan. The format of the corpus adopts the Child Language Data Exchange System (CHILDES). The size of the corpus is about 1.6 million words. In this paper, we describe data collection, transcription, word segmentation, and part-of-speech annotation of this corpus. Applications of the corpus are also discussed. 
The Open-domain Question Answering system (QA) has been attached great attention for its capacity of providing compact and precise results for Xsers. The question classification is an essential part in the system, affecting the accuracy of it. The paper studies question classification through machine learning approaches, namely, different classifiers and multiple classifier combination method. By using compositive statistic and rule classifiers, and by introducing dependency structure from Minipar and linguistic knowledge from Wordnet into question representation, the research shows high accuracy in question classification. 
In this paper we present our recent work on harvesting English-Chinese bitexts of the laws of Hong Kong from the Web and aligning them to the subparagraph level via utilizing the numbering system in the legal text hierarchy. Basic methodology and practical techniques are reported in detail. The resultant bilingual corpus, 10.4M English words and 18.3M Chinese characters, is an authoritative and comprehensive text collection covering the speciﬁc and special domain of HK laws. It is particularly valuable to empirical MT research. This piece of work has also laid a foundation for exploring and harvesting English-Chinese bitexts in a larger volume from the Web. 
 This paper proposes a  semi-automatic method to detect  segmentation errors in a manually  annotated Chinese corpus in order  to improve its quality further. A  particular Chinese character string  occurring more than once in a  corpus may be assigned different  segmentations  during  a  segmentation process. Based on  these differences our approach  outputs the segmentation error  candidates found in a segmented  corpus and then on which the  segmentation errors are identified  manually. Segmentation error rate  of a gold standard corpus can be  given using our method. In Peking  University (PK) and Academic  Sinica (AS) test corpora of Special  Interest Group for Chinese  Language Processing (SIGHAN)  Bakeoff1, 1.29% and 2.26%  segmentation error rates are  detected by our method. These  errors decrease the F-measure of  SIGHAN Bakeoff1 baseline test by  1.36% in PK test data and 1.93% in  AS test data respectively.   This work was done while Chengjie Sun was visiting Microsoft Research Asia.  
This paper presents a word-pair (WP) identifier that can be used to resolve homonym/segmentation ambiguities and perform syllable-to-word (STW) conversion effectively for improving Chinese input systems. The experiment results show the following: (1) the WP identifier is able to achieve tonal (syllables with four tones) and toneless (syllables without four tones) STW accuracies of 98.5% and 90.7%, respectively, among the identified word-pairs; (2) while applying the WP identifier, together with the Microsoft input method editor 2003 and an optimized bigram model, the tonal and toneless STW improvements of the two input systems are 27.5%/18.9% and 22.1%/18.8%, respectively. 
We present a method for improving dependency structure analysis of Chinese. Our bottom-up deterministic analyzer adopt Nivre’s algorithm (Nivre and Scholz, 2004). Support Vector Machines (SVMs) are utilized to determine the word dependency relations. We find that there are two problems in our analyzer and propose two methods to solve them. One problem is that some operations cannot be solved only using local feature. We utilize the global features to solve this. The other problem is that this bottom-up analyzer doesn’t use top-down information. We supply the top-down information by constructing SVMs based root node finder to solve this problem. Experimental evaluation on the Penn Chinese Treebank Corpus shows that the proposed extensions improve the parsing accuracy significantly. 
In Chinese, nouns need numeral classiﬁers to express quantity. In this paper, we explore the relationship between classiﬁers and nouns. We extract a set of lexical, syntactic and ontological features and the corresponding noun-classiﬁer pairs from a corpus and then train SVMs to assign classifers to nouns. We analyse which features are most important for this task.  
Part-of-speech tagging, like any supervised statistical NLP task, is more difficult when test sets are very different from training sets, for example when tagging across genres or language varieties. We examined the problem of POS tagging of different varieties of Mandarin Chinese (PRC-Mainland, PRCHong Kong, and Taiwan). An analytic study first showed that unknown words were a major source of difficulty in cross-variety tagging. Unknown words in English tend to be proper nouns. By contrast, we found that Mandarin unknown words were mostly common nouns and verbs. We showed these results are caused by the high frequency of morphological compounding in Mandarin; in this sense Mandarin is more like German than English. Based on this analysis, we propose a variety of new morphological unknown-word features for POS tagging, extending earlier work by others on unknown-word tagging in English and German. Our features were implemented in a maximum entropy Markov model. Our system achieves state-of-the-art performance in Mandarin tagging, including improving unknown-word tagging performance on unseen varieties in Chinese Treebank 5.0 from 61% to 80% correct. 
A hierarchical hidden Markov model (HHMM) based approach of product named entity recognition (NER) from Chinese free text is presented in this paper. Characteristics and challenges in product NER is also investigated and analyzed deliberately compared with general NER. Within a uniﬁed statistical framework, the approach we proposed is able to make probabilistically reasonable decisions to a global optimization by leveraging diverse range of linguistic features and knowledge sources. Experimental results show that our approach performs quite well in two different domains. 
technology for collocation extraction in Chinese. Sketch Engine (Kilgarriff et al., 2004) has proven to be a very effective tool for automatic description of lexical information, including collocation extraction, based on large-scale corpus. The original work of Sketch Engine was based on BNC. We extend Sketch Engine to Chinese based on Gigaword corpus from LDC. We discuss the available functions of the prototype Chinese Sketch Engine (CSE) as well as the robustness of language-independent adaptation of Sketch Engine. We conclude by discussing how Chinese-specific linguistic information can be incorporated to improve the CSE prototype. 
This paper presents a semantic class prediction model of Chinese twocharacter compound words based on a character ontology, which is set to be a feasible conceptual knowledge resource grounded in Chinese characters. The experiment we conduct yields satisfactory results which turn out to be that the task of semantic prediction of two-character words could be greatly facilitated using Chinese characters as a knowledge resource. 
Domain specific words and ontological information among words are important resources for general natural language applications. This paper proposes a statistical model for finding domain specific words (DSW¶s) in particular domains, and thus building the association among them. When applying this model to the hierarchical structure of the web directories node-by-node, the document tree can potentially be converted into a large semantically annotated lexicon tree. Some preliminary results show that the current approach is better than a conventional TF-IDF approach for measuring domain specificity. An average precision of 65.4% and an average recall of 36.3% are observed if the top-10% candidates are extracted as domain-specific words. 
Fluent dialogue requires that speakers successfully negotiate and signal turn-taking. While many cues to turn change have been proposed, especially in multi-modal frameworks, here we focus on the use of prosodic cues to these functions. In particular, we consider the use of prosodic cues in a tone language, Mandarin Chinese, where variations in pitch height and slope additionally serve to determine word meaning. Within a corpus of spontaneous Chinese dialogues, we ﬁnd that turn-unit ﬁnal syllables are signiﬁcantly lower in average pitch and intensity than turnunit initial syllables in both smooth turn changes and segments ended by speaker overlap. Interruptions are characterized by signiﬁcant prosodic differences from smooth turn initiations. Furthermore, we demonstrate that these contrasts correspond to an overall lowering across all tones in ﬁnal position, which largely preserves the relative heights of the lexical tones. In classiﬁcation tasks, we contrast the use of text and prosodic features. Finally, we demonstrate that, on balanced training and test sets, we can distinguish turnunit ﬁnal words from other words at ≈ 93% accuracy and interruptions from smooth turn unit initiations at 62% accuracy.  
The selection of features is critical in providing discriminative information for classifiers in Word Sense Disambiguation (WSD). Uninformative features will degrade the performance of classifiers. Based on the strong evidence that an ambiguous word expresses a unique sense in a given collocation, this paper reports our experiments on automatic WSD using collocation as local features based on the corpus extracted from People’s Daily News (PDN) as well as the standard SENSEVAL-3 data set. Using the Naïve Bayes classifier as our core algorithm, we have implemented a classifier using a feature set combining both local collocation features and topical features. The average precision on the PDN corpus has 3.2% improvement compared to 81.5% of the baseline system where collocation features are not considered. For the SENSEVAL-3 data, we have reached the precision rate of 37.6% by integrating collocation features into contextual features, to achieve 37% improvement over 26.7% of precision in the baseline system. Our experiments have shown that collocation features can be used to reduce the size of human tagged corpus. 
Informal language is actively used in network-mediated communication, e.g. chat room, BBS, email and text message. We refer the anomalous terms used in such context as network informal language (NIL) expressions. For example, “஧(ou3)” is used to replace “ ᚒ (wo3)” in Chinese ICQ. Without unconventional resource, knowledge and techniques, the existing natural language processing approaches exhibit less effectiveness in dealing with NIL text. We propose to study NIL expressions with a NIL corpus and investigate techniques in processing NIL expressions. Two methods for Chinese NIL expression recognition are designed in NILER system. The experimental results show that pattern matching method produces higher precision and support vector machines method higher F-1 measure. These results are encouraging and justify our future research effort in NIL processing. 
Xiang-Bing Li Institute of Information Science, Academia Sinica, Taipei dreamer@hp.iis.sinica.edu.tw  Jia-Fei Hong Institute of Linguistics, Academia Sinica, Taipei jiafei@gate.sinica.edu.tw  Abstract. This paper deals with the robust expansion of Domain LexicoTaxonomy (DLT). DLT is a domain taxonomy enriched with domain lexica. DLT was proposed as an infrastructure for crossing domain barriers (Huang et al. 2004). The DLT proposal is based on the observation that domain lexica contain entries that are also part of a general lexicon. Hence, when entries of a general lexicon are marked with their associated domain attributes, this information can have two important applications. First, the DLT will serve as seeds for domain lexica. Second, the DLT offers the most reliable evidence for deciding the domain of a new text since these lexical clues belong to the general lexicon and do occur reliably in all texts. Hence general lexicon lemmas are extracted to populate domain lexica, which are situated in domain taxonomy. Based on this previous work, we show in this paper that the original DLT can be further expanded when a new language resource is introduced. We applied CiLin, a Chinese thesaurus, and added more than 1000 new entries for DLT and show with evaluation that the DLT approach is robust since the size and number of domain lexica increased effectively. 1. Introduction Domain-based language processing has an inherent research dilemma when the construction of domain lexicons is involved.  The standard approach of building domain lexicon from domain corpora requires a very high threshold of existing domain resources and knowledge. Since only well-documented domains can provide enough quality corpora, it is likely these fields already have good manually constructed domain lexica. Hence this approach is can only deal with domains where only marginal benefit can be achieved, while it cannot deal with domains where it can make most contribution since there is not enough resources to work with. It was observed that the type of domain language processing that has the widest application and best potentials are cross-domain and multi-domain in nature. For instance, a typical web-search is a search for specific domain information from the www as an archive of mixed and heterogeneous domains. The contribution will be immediate and salient to be able to acquire resources and information for a new domain that is not well documented yet. A new approach towards domain language processing by constructing an infrastructure for multi-domain language processing called the Domain Lexico-Taxonomy (DLT) was proposed in Huang et al. (2004). In the DLT approach, domain lexica are semi-automatically acquired to populate domain taxonomy. This lexically populated domain taxonomy serves two purposes: as the basis of stylo-statistical prediction of the domain of a new text, and as the core seed of complete domain lexica. For the first purpose, the DLT approach relies crucially on the ability to effectively identify words that are good indicators of specific domains. For the second purpose, the DLT needs to be robust enough to allow incremental expansion when new content resources are integrated. In this study, we integrate CiLin, a Chinese thesaurus, to show that the DLT architecture is indeed robust.  103  2. Related Work Typical studies on domain lexica focuses on assigning texts to specific classes, hence they use a limited taxonomy augmented with a small set of features (e.g. Avancini et al. 2003, Sebastiani 2002, and Yand and Pederson 1997). However, specialized lemmas cannot be useful in multi-domain processing. To achieve domain versatility in processing, it is necessary to identify lemmas with wider distributions and yet is associated with particular domain(s). We follow the DLT architecture (Huang et al. 2004), which was shown to be effective in predicting the domain of documents extracted from the web. We aim to elaborate that framework by proposing a domain lexica can be incrementally expanded with knowledge from a new resource. 3. Domain Taxonomy A domain taxonomy containing 549 nodes was manually constructed. The main sources of domain classification are from Chinese Library Classification system, Encyclopedia Britannica and the Global View English-Chinese dictionary. Two important criteria were chosen: that the taxonomy is bilingual and that it is maintained locally. First, the bilingual taxonomy is essential for future cross-lingual processing but also allows us to access relevant resources in both languages. Second, since our emphasis was not on the correctness of a dogmatic taxonomy but on the flexibility that allows monotonic extensions, it is essential to be able to monitor any changes in the taxonomy. There are four layers in the constructed domain taxonomy. Fourteen (14) domains are in the upper layer, including Humanities, Social Science, Formal Science, Natural Science, Medical Science, Engineering Science, Agriculture and Industry, Fine Arts, Recreation, Proper Name, Genre/Strata, Etymology, Country Name, Country People. The Second layer has 147 domains. The third layer has 279 domains. Lastly the fourth layer has only 109 domains since not all branches need to be expanded at this level. In sum, there are 549 possible domain tags when the hierarchy is ignored. The domain taxonomy is available online at the Sinica BOW website (http://BOW.sinica.edu.tw/, Huang and Chang 2004).  4. Detection of Domain Lexicon in DLT The challenge in integrating heterogeneous language resources for domain information is that conceptual classification varies from one resource to another and hence cannot be directly harvested. We propose to utilize the inheritance relations of these resources, instead of their hierarchy. In other words, lexical (and hence conceptual) identity is established first, following by expanding this matching with logical inheritance but without branching out on the conceptual hierarchy. DLT establish the correspondences between the taxonomic nodes of domains and the linguistic resources of sub-lexica. Note that a lexical knowledgebase, in a Wordnet fashion, also contains hierarchical relations. The domain taxonomy can be enriched by taking the hierarchical information internal to the lexica. If these resources directly encodes the ϖis-aϗ relation by hyponymy, we assume that both the node (lexicons) and their hyponym node (lexicons) belong to that domain. Using the simple supposition, we can observe the domain knowledge with various resources, and strengthens the domain lexica for domain taxonomy. The process of populating DLT is shown in Fig. 1. Figure. 1. Populating DLT from Linguistic Resource  104  5. Experiment 5.1. The Original Study with Bilingual WordNet The original DLT work was based on bilingual Wordnet (Huang et al. 2004). This is because of the Wordnet lexical knowledgebase is highly enriched with lexical semantic relation information. In addition, the bilingual Wordnet adds an unparallel dimension of knowledge coverage. The bilingual Wordnet used is Sinica BOW (The Academia Sinica Bilingual Ontological WordNet, Huang and Chang (2004)). Sinica BOW is bilingual lexical knowledgebase connecting WordNet and SUMO and mapping both between English and Chinese. The study reported in Huang et al. (2004) also contains a small domain identification experiment to show the application of DLT. 5.1.1 Description of WordNet and Sinica BOW WordNet is inspired by current psycholinguistic and computational theories of human lexical memory (Fellbaum (1998), Miller et al. (1993)). English nouns, verbs, adjectives, and adverbs are organized into synonym sets, each representing one underlying lexicalized concept. Different semantic relations link the synonym sets (synsets). The version of WordNet that Sinica BOW implemented is version 1.6, with nearly 100,000 synsets. In Sinica BOW, ach English synset was given up to 3 most appropriate Chinese translation equivalents. And in cases where the translation pairs are not synonyms, their semantic relations are marked (Huang et al. 2003). The bilingual WordNet is further linked to the SUMO ontology. We use the semantic relations in bilingual resource to expand and predict domain classification when it cannot be judged directly from a lexical lemma. 5.1.2 Experiment and Result with WordNet 463 of the 549 nodes in the domain taxonomy were successfully mapped to a WordNet synset through an identical lemma. 452 or 463 mappings were manually confirmed to be  correct, a precision score of over 97%. These domains were expanded to cover a total of 11,918 synsets corresponding to 15,160 Chinese lemmas. Note that both English and Chinese correspondences are used since our resources (WordNet and domain taxonomy) are both bilingual. Due mostly to hyponymy expansion, each lemma is mapped to 1.38 domains in average. While each lemma is assigned to no more than 8 domains, with the majority (6,464) assigned to only one. These mapped lemmas populate a set of domain lexica. The number of entries in these domain lexica ranges from 1 to 3762. The average size of these domain lexica is 32.8 lemmas. Only 41 domains lexical contain 33 or more lemmas. Since we cannot know the effective of the lexicon of a domain a priori, we take those whose size are above average as the effective domain lexica. These domain lexica and their sizes are shown in Table 1. 5.1.3 Evaluation: precision of domain lexica It is impossible to formally evaluate the recall rate of this domain lexica study since we do not know the total number of entries to be recalled. However, it is possible to evaluate the precision rate of the constructed domain lexica. First, the precision of all recalled lemmas is tested. Among the mapped lemmas, 8696 (out of 15,160) lemmas are assigned to multiple domains, while 6,464 are assigned to single domain. The single domain mappings were spot-checked to be correct. On the other hand, the precision of all 8,696 multi-domain lemmas are carefully evaluated. Among these lemmas, only 4.81% (418) proves to be wrong; and an overwhelming majority of 95.19% turns out to be correct (8278). Second, a more meaningful test is to evaluate how well the domain lexica are defined. Five effective domain lexica with over 100 entries were randomly chosen for evaluation: Insect (515 entries), Natural Science (262 entries), Sports (180 entries), Dance (124 entries) and Religious Music (48 entries). The manually checked precision of these domain lexica is listed below the Table 2:  105  Domain  Domain  Domain  Domain  Vertebrates ૉ෎୏‫ ނ‬3676 Language ᇟ‫ق‬ 699 Country ୯ৎ 250 Sports!ၮ୏ 180  Food १ࠔ 2968 Recreation Ҷ໕ু኷ 548 contest ᝡᖻ 207 commerce ୘཰ 144  Bird ചᜪ 1059 Insect ܲᙝ 515 music ॣ኷ 192 Business ғཀ 144  Fish ങᜪ 729 Natural Science ԾฅࣽᏢ 262 Indian ӑӦӼ 188 Dance ᆸᗂ 124  Heraldic design દക೛ी 120  Medical Science ᙴᕍࣽᏢ 85  Medicine ᙴᏢ 76  Pathological medicine ੰ౛ᙴ Ꮲ 76  Clinical medicine Mathematics  ᖏ‫׉‬ᙴᏢ 76  ኧᏢ 69  Humanities ΓЎᏢࣽ 64  Social Science ‫ࣽ཮ޗ‬Ꮲ 62  physics ‫ނ‬౛Ꮲ 56 Religion ‫ے‬௲ 52  Religious Music ‫ے‬௲ॣ኷ 48  Plastic art ೷‫׎‬᛬ೌ 45  Pure mathematics Anthropology  પኧᏢ 44  ΓᜪᏢ 42  Earth science ӦౚࣽᏢ 39  drawing ై༴ 4:  Norse Mythology  Telecommunication  чኻઓ၉ 39  Philosophy ণᏢ 37 ႝߞ೯ૻ 35  theater ᔍቃ 34  Fine Arts ᛬ೌ 33  Table 1. Domain lexica containing 33 or more lemmas  Domain Label # of entries Precision (%)  Insect  515  99.03  Natural Science  262  69.85  Sports  180  86.11  Dance  124  100.00  Religious Music  48  93.75  Table 2. Size and Precision of selected domain lexica  Table 2 shows an overall precision of over 95%,  while no other lexica has precision lower than  86%, natural science is lowest at just below 70%.  This is because “Natural Science” is a higher  level domain and hence open to more noises in  the detection process. This study clearly showed  that the WordNet helped to effectively build  core domain lexica.  We take the domain “Dance” as an  example to explain the process. First, we map  “Dance” to the Wordnet synset—“dance”, and  we look for the hyponym synsets. Table3 will be  shown the expanding lexica of one of hyponym  synsets. These lexical entries are associated with  domain “Dance” and populate the domain  lexicon.  Level  synset  
In this paper, we study some issues on Chinese domain knowledge dictionary and its application to text classification task. First a domain knowledge hierarchy description framework and our Chinese domain knowledge dictionary named NEUKD are introduced. Second, to alleviate the cost of construction of domain knowledge dictionary by hand, we use a boostrapping-based algorithm to learn new domain associated terms from a large amount of unlabeled data. Third, we propose two models (BOTW and BOF) which use domain knowledge as textual features for text categorization. But due to limitation of size of domain knowledge dictionary, we further study machine learning technique to solve the problem, and propose a BOL model which could be considered as the extended version of BOF model. Naïve Bayes classifier based on BOW model is used as baseline system in the comparison experiments. Experimental results show that domain knowledge is very useful for text categorization, and BOL model performs better than other three models, including BOW, BOTW and BOF models. 
This study addresses pronominal anaphora resolution, including zero pronouns, in Chinese. A syntactic, rule-based pronoun resolution algorithm, the “Hobbs algorithm” was run on “gold standard” hand parses from the Penn Chinese Treebank. While ﬁrst proposed for English, the algorithm counts for its success on two characteristics that Chinese and English have in common. Both languages are SVO, and both are ﬁxed word order languages. No changes were made to adapt the algorithm to Chinese. The accuracy of the algorithm on overt, third-person pronouns at the matrix level was 77.6%, and the accuracy for resolving matrix-level zero pronouns was 73.3%. In contrast, the accuracy of the algorithm on pronouns that appeared in subordinate constructions was only 43.3%, providing support for Miltsakaki’s two-mechanism proposal for resolving inter- vs. intra-sentential anaphors. 
The second international Chinese word segmentation bakeoff was held in the summer of 2005 to evaluate the current state of the art in word segmentation. Twenty three groups submitted 130 result sets over two tracks and four different corpora. We found that the technology has improved over the intervening two years, though the out-of-vocabulary problem is still or paramount importance. 
This article presents our recent work for participation in the Second International Chinese Word Segmentation Bakeoff. Our system performs two procedures: Out-ofvocabulary extraction and word segmentation. We compose three out-of-vocabulary extraction modules: Character-based tagging with different classiﬁers – maximum entropy, support vector machines, and conditional random ﬁelds. We also compose three word segmentation modules – character-based tagging by maximum entropy classiﬁer, maximum entropy markov model, and conditional random ﬁelds. All modules are based on previously proposed methods. We submitted three systems which are different combination of the modules.  word classes and character classes in order to overcome the data sparse problem. The word classes are used as the hidden states in MEMM and CRF-based word segmenters. The character classes are used as the features in character-based tagging, character-based chunking and word segmentation. Model b is our previous method proposed in (Goh et al., 2004b): First, a MaxEnt classiﬁer is used to perform character-based tagging to identify OOV words in the test data. In-vocabulary (IV) word list together with the extracted OOV word candidates is used in Maximum Matching algorithm. Overlapping ambiguity is denoted by the different outputs from Forward and Backward Maximum Matching algorithm. Finally, character-based tagging by MaxEnt classiﬁer resolves the ambiguity. Section 2 describes Models a and c. Section 3 describes Model b. Section 4 discusses the differences among the three models.  
This paper describes a Chinese word segmentation system based on unigram language model for resolving segmentation ambiguities. The system is augmented with a set of pre-processors and post-processors to extract new words in the input texts. 
This paper describes a Chinese word segmentor (CWS) based on backward maximum matching (BMM) technique for the 2nd Chinese Word Segmentation Bakeoff in the Microsoft Research (MSR) closed testing track. Our CWS comprises of a context-based Chinese unknown word identifier (UWI). All the context-based knowledge for the UWI is fully automatically generated by the MSR training corpus. According to the scored results of the MSR closed testing track and our analysis, it shows that our BMM-based CWS with the context-based UWI is a simple and effective system to achieve high Chinese word segmentation performance of more than 95.5% F-measure. 
This paper reports the example-based segmentation system for our participation in the second Chinese Word Segmentation Bakeoff (CWSB-2), presenting its basic ideas, technical details and evaluation. It is a preliminary implementation. CWSB-2 valuation shows that it performs very well in identifying known words. Its unknown word detection module also illustrates great potential. However, proper facilities for identifying time expressions, numbers and other types of unknown words are needed for improvement. 
This paper presents a word segmentation system in France Telecom R&D Beijing, which uses a unified approach to word breaking and OOV identification. The output can be customized to meet different segmentation standards through the application of an ordered list of transformation. The system participated in all the tracks of the segmentation bakeoff -- PK-open, PKclosed, AS-open, AS-closed, HK-open, HK-closed, MSR-open and MSRclosed -- and achieved the state-of-theart performance in MSR-open, MSRclose and PK-open tracks. Analysis of the results shows that each component of the system contributed to the scores. 
We explored a simple, fast and effective learning algorithm, the uneven margins Perceptron, for Chinese word segmentation. We adopted the character-based classiﬁcation framework and transformed the task into several binary classiﬁcation problems. We participated the close and open tests for all the four corpora. For the open test we only used the utf-8 code knowledge for discrimination among Latin characters, Arabic numbers and all other characters. Our system performed well on the as, cityu and msr corpora but was clearly worse than the best result on the pku corpus. 
This paper presents a data-driven language independent word segmentation system that has been trained for Chinese corpus at the second Chinese word segmentation bakeoff. The system consists of a base segmentation algorithm and the refining procedures for the undecided character sequences. It does not use any lexicon and the base segmentation is simply done by character bigram and HMM-model is applied for the remaining character sequences. As a final step, high-frequency character trigram modifies the error-prone parts of the text.1 TT 
We participated in the Second International Chinese Word Segmentation Bakeoff. Speciﬁcally, we evaluated our Chinese word segmenter in the open track, on all four corpora, namely Academia Sinica (AS), City University of Hong Kong (CITYU), Microsoft Research (MSR), and Peking University (PKU). Based on a maximum entropy approach, our word segmenter achieved the highest F measure for AS, CITYU, and PKU, and the second highest for MSR. We found that the use of an external dictionary and additional training corpora of different segmentation standards helped to further improve segmentation accuracy. 
In this paper, we describe in brief our system for the Second International Chinese Word Segmentation Bakeoff sponsored by the ACL-SIGHAN. We participated in all tracks at the bakeoff. The evaluation results show our system can achieve an F measure of 0.9400.967 for different testing corpora. 
We present a Chinese word segmentation system submitted to the closed track of Sighan bakeoff 2005. Our segmenter was built using a conditional random field sequence model that provides a framework to use a large number of linguistic features such as character identity, morphological and character reduplication features. Because our morphological features were extracted from the training corpora automatically, our system was not biased toward any particular variety of Mandarin. Thus, our system does not overfit the variety of Mandarin most familiar to the system's designers. Our final system achieved a F-score of 0.947 (AS), 0.943 (HK), 0.950 (PK) and 0.964 (MSR). 
This paper presents the results of the system IRLAS1 from HIT-IRLab in the Second International Chinese Word Segmentation Bakeoff. IRLAS consists of several basic components and multiple postprocessors. The basic components include basic segmentation, factoid recognition, and named entity recognition. These components maintain a segment graph together. The postprocessors include merging of adjoining words, morphologically derived word recognition, and new word identification. These postprocessors do some modifications on the best word sequence which is selected from the segment graph. Our system participated in the open and closed tracks of PK corpus and ranked #4 and #3 respectively. Our scores were very close to the highest level. It proves that our system has reached the current state of the art. 
We used a production segmentation system, which draws heavily on a large dictionary derived from processing a large amount (over 150 million Chinese characters) of synchronous textual data gathered from various Chinese speech communities, including Beijing, Hong Kong, Taipei, and others. We run this system in two tracks in the Second International Chinese Word Segmentation Bakeoff, with Backward Maximal Matching (right-to-left) as the primary mechanism. We also explored the use of a number of supplementary features offered by the large dictionary in postprocessing, in an attempt to resolve ambiguities and detect unknown words. While the results might not have reached their fullest potential, they nevertheless reinforced the importance and usefulness of a large dictionary as a basis for segmentation, and the implication of following a uniform standard on the segmentation performance on data from various sources. 
This paper presents our recent work for participation in the Second International Chinese Word Segmentation Bakeoff. According to difficulties, we divide word segmentation into several sub-tasks, which are solved by mixed language models, so as to take advantage of each approach in addressing special problems. The experiment indicated that this system achieved 96.7% and 97.2% in F-measure in PKU and MSR open test respectively.  2 ELUS Segmenter All the words are categorized into five types: Lexicon words (LW), Factoid words (FT), Morphologically derived words (MDW), Named entities (NE), and New words (NW). Accordingly, four main modules are included to identify each kind of words, as shown in Figure 1.  Sentence  String  Basic Segmentation  Factoid Detect Lexicon words  
In this paper we present a Two-Phase LMR-RC Tagging scheme to perform Chinese word segmentation. In the Regular Tagging phase, Chinese sentences are processed similar to the original LMR Tagging. Tagged sentences are then passed to the Correctional Tagging phase, in which the sentences are re-tagged using extra information from the ﬁrst round tagging results. Two training methods, Separated Mode and Integrated Mode, are proposed to construct the models. Experimental results show that our scheme in Integrated Mode performs the best in terms of accuracy, where Separated Mode is more suitable under limited computational resources. 
Chinese word segmentation is always much accounted of in ICT-NLP. In this bakeoff, two different systems in ICTNLP participated. The one is SYSTEM_#1 evaluated in three tracks -- PK-close, MSR-close and MSR-open, and SYSTEM_#2 PK-open. Through this bakeoff , the development of Chinese segmentation is learned and the problems are found in our systems.  
This paper describes a hybrid Chinese word segmenter that is being developed as part of a larger Chinese unknown word resolution system. The segmenter consists of two components: a tagging component that uses the transformation-based learning algorithm to tag each character with its position in a word, and a merging component that transforms a tagged character sequence into a word-segmented sentence. In addition to the position-of-character tags assigned to the characters, the merging component makes use of a number of heuristics to handle non-Chinese characters, numeric type compounds, and long words. The segmenter achieved a 92.8% F-score and a 72.8% recall for OOV words in the closed track of the Peking University Corpus in the Second International Chinese Word Segmentation Bakeoff. 
Chinese word segmentation is a fundamental and important issue in Chinese information processing. In order to find a unified approach for Chinese word segmentation, the author develop a Chinese lexical analyzer PCWS using direct maximum entropy model. The paper presents the general description of PCWS, as well as the result and analysis of its performance at the Second International Chinese Word Segmentation Bakeoff. 
In this paper, we present a Chinese word segmentation system which is consisted of four components, i.e. basic segmentation, named entity recognition, error-driven learner and new word detector. The basic segmentation and named entity recognition, implemented based on conditional random fields, are used to generate initial segmentation results. The other two components are used to refine the results. Our system participated in the tests on open and closed tracks of Beijing University (PKU) and Microsoft Research (MSR). The actual evaluation results show that our system performs very well in MSR open track, MSR closed track and PKU open track. 
Ensuring consistency of Part-of-Speech (POS) tagging plays an important role in constructing high-quality Chinese corpora. After analyzing the POS tagging of multi-category words in largescale corpora, we propose a novel consistency check method of POS tagging in this paper. Our method builds a vector model of the context of multicategory words, and uses the -NN algorithm to classify context vectors constructed from POS tagging sequences and judge their consistency. The experimental results indicate that the proposed method is feasible and effective. 
In this paper, the usage and function of Chinese punctuations are studied in syntactic parsing and a new hierarchical approach is proposed for parsing long Chinese sentences. It differentiates from most of the previous approaches mainly in two aspects. Firstly, Chinese punctuations are classified as ‘divide’ punctuations and ‘ordinary’ ones. Long sentences which include ‘divide’ punctuations are broken into suitable units, so the parsing will be carried out in two stages. This ‘divide-and-rule’ strategy greatly reduces the difficulty of acquiring the boundaries of sub-sentences and syntactic structures of sub-sentences or phrases simultaneously in once-level parsing strategy of previous approaches. Secondly, a grammar rules system including all punctuations and probability distribution is built to be used in parsing and disambiguating. Experiments show that our approach can significantly reduce the time consumption and numbers of ambiguous edges of traditional methods, and also improve the accuracy and recall when parsing long Chinese sentences. 
In this paper, we present a hybrid Chinese language model based on a combination of ontology with statistical method. In this study, we determined the structure of such a Chinese language model. This structure is firstly comprised of an ontology description framework for Chinese words and a representation of Chinese lingual ontology knowledge. Subsequently, a Chinese lingual ontology knowledge bank is automatically acquired by determining, for each word, its cooccurrence with semantic, pragmatics, and syntactic information from the training corpus and the usage of Chinese words will be gotten from lingual ontology knowledge bank for a actual document. To evaluate the performance of this language model, we completed two groups of experiments on texts reordering for Chinese information retrieval and texts similarity computing. Compared with previous works, the proposed method improved the precision of nature language processing. 
This paper describes a method for language independent extractive summarization that relies on iterative graph-based ranking algorithms. Through evaluations performed on a single-document summarization task for English and Portuguese, we show that the method performs equally well regardless of the language. Moreover, we show how a metasummarizer relying on a layered application of techniques for single-document summarization can be turned into an effective method for multi-document summarization. 
A problem associated with current P2P (peer-to-peer) systems is that the consistency between copied contents is not guaranteed. Additionally, the limitation of fulltext search capability in most of the popular P2P systems hinders the scalability of P2P-based content sharing systems. We proposed a new P2P content sharing system in which the consistency of contents in the network is maintained after updates or modifications have been made to the contents. Links to the downloaded contents are maintained on a server. As a result, the updates and modifications to the contents can be instantly detected and hence get reflected in future P2P downloads. Natural language processing including morphological analysis is performed distributedly by the P2P clients and the update of the inverted index on the server is conducted concurrently to provide an efficient full-text search. The scheme and a preliminary experimental result have been mentioned 
In this paper, we report a QA system which can answer how type questions based on the conﬁrmed knowledge base which was developed by using mails posted to a mailing list. We ﬁrst discuss a problem of developing a knowledge base by using natural language documents: wrong information in natural language documents. Then, we describe a method of detecting wrong information in mails posted to a mailing list and developing a knowledge base by using these mails. Finally, we show that question and answer mails posted to a mailing list can be used as a knowledge base for a QA system.  words and phrases. To solve this problem, (Kuro 00) and (Kiyota 02) proposed methods of collecting knowledge for answering questions from FAQ documents and technical manuals by using the document structure, such as, a dictionary-like structure and if-then format description. However, these kinds of documents requires the considerable cost of developing and maintenance. As a result, it is important to investigate a method of extracting evidential sentences for answering how type questions from natural language documents at low cost. To solve this problem, (Watanabe 04) proposed a method of developing a knowledge base by using mails posted to a mailing list (ML). We have the following advantages when we develop knowledge base by using mails posted to a mailing list. • it is easy to collect question and answer mails in a speciﬁc domain, and  
We describe a resource-based method of morphological annotation of written Korean text. Korean is an agglutinative language. The output of our system is a graph of morphemes annotated with accurate linguistic information. The language resources used by the system can be easily updated, which allows users to control the evolution of the performances of the system. We show that morphological annotation of Korean text can be performed directly with a lexicon of words and without morphological rules. 
This paper describes a system which solves language tests for second grade students (7 years old). In Japan, there are materials for students to measure understanding of what they studied, just like SAT for high school students in US. We use textbooks for the students as the target material of this study. Questions in the materials are classified into four types: questions about Chinese character (Kanji), about word knowledge, reading comprehension, and composition. This program doesn’t resolve the composition and some other questions which are not easy to be implemented in text forms. We built a subsystem for each finer type of questions. As a result, we achieved 55% 83% accuracy in answering questions in unseen materials. 
We describe our analysis and modeling of the summarization process of Japanese broadcast news. We have studied the entire manual summarization process of the Japan Broadcasting Corporation (NHK). The staff of NHK has been making manual summarizations of news text on a daily basis since December 2000. We interviewed these professional abstractors and obtained a considerable amount of news summaries. We matched the summary with the original text, investigated the news text structure, and thereby analyzed the manual summarization process. We then developed a summarization model on which we intend to build a summarization system. 
This paper describes a mix word-pair mix-WP) identifier to resolve homonym/segmentation ambiguities as well as perform STW conversion effectively for Chinese input. The mix-WP identifier includes a specific word-pair (SWP) identifier and a common wordpair (CWP) identifier. It is designed as a supporting processing with Chinese input systems. Our experiments show that by applying the mix-WP identifier, together with the Microsoft input method editor 2003 (MSIME) and an optimized bigram model (BiGram), the tonal and toneless STW performance of the two input systems can be improved. 
We describe a sentence-level opinion detection system. We first define what an opinion means in our research and introduce an effective method for obtaining opinion-bearing and nonopinion-bearing words. Then we describe recognizing opinion-bearing sentences using these words We test the system on 3 different test sets: MPQA data, an internal corpus, and the TREC2003 Novelty track data. We show that our automatic method for obtaining opinion-bearing words can be used effectively to identify opinion-bearing sentences. 
We present a tool, called ILIMP, which takes as input a raw text in French and produces as output the same text in which every occurrence of the pronoun il is tagged either with tag [ANA] for anaphoric or [IMP] for impersonal or expletive. This tool is therefore designed to distinguish between the anaphoric occurrences of il, for which an anaphora resolution system has to look for an antecedent, and the expletive occurrences of this pronoun, for which it does not make sense to look for an antecedent. The precision rate for ILIMP is 97,5%. The few errors are analyzed in detail. Other tasks using the method developed for ILIMP are described briefly, as well as the use of ILIMP in a modular syntactic analysis system. 
Automatic evaluation metrics for Machine Translation (MT) systems, such as BLEU or NIST, are now well established. Yet, they are scarcely used for the assessment of language pairs like English-Chinese or English-Japanese, because of the word segmentation problem. This study establishes the equivalence between the standard use of BLEU in word n-grams and its application at the character level. The use of BLEU at the character level eliminates the word segmentation problem: it makes it possible to directly compare commercial systems outputting unsegmented texts with, for instance, statistical MT systems which usually segment their outputs. 
We are constricting a Japanese-Chinese parallel corpus, which is a part of the NICT Multilingual Corpora. The corpus is general domain, of large scale of about 40,000 sentence pairs, long sentences, annotated with detailed information and high quality. To the best of our knowledge, this will be the first annotated JapaneseChinese parallel corpus in the world. We created the corpus by selecting Japanese sentences from Mainichi Newspaper and then manually translating them into Chinese. We then annotated the corpus with morphological and syntactic structures and alignments at word and phrase levels. This paper describes the specification in human translation and the scheme of detailed information annotation, and the tools we developed in the corpus construction. The experience we obtained and points we paid special attentions are also introduced for share with other researches in corpora construction. 
This paper presents a prototype dialogue system, K3 , in which a user can instruct agents through speech input to manipulate various objects in a 3-D virtual world. In this paper, we focus on two distinctive features of the K3 system: plan-based anaphora resolution and handling vagueness in spatial expressions. After an overview of the system architecture, each of these features is described. We also look at the future research agenda of this system. 
Honoriﬁc agreement is one of the main properties of languages like Korean or Japanese, playing an important role in appropriate communication. This makes the deep processing of honoriﬁc information crucial in various computational applications such as spoken language translation and generation. We argue that, contrary to the previous literature, an adequate analysis of Korean honoriﬁcation involves a system that has access not only to morphosyntax but to semantics and pragmatics as well. Along these lines, we have developed a typed feature structure grammar of Korean (based on the framework of HPSG), and implemented it in the Linguistic Knowledge Builder System (LKB). The results of parsing our experimental test suites show that our grammar provides us with enriched grammatical information that can lead to the development of a robust dialogue system for the language. 
In this paper, we proposed an approach for detecting the countability of English compound nouns treating the web as a large corpus of words. We classified compound nouns into three classes: countable, uncountable, plural only. Our detecting algorithm is based on simple, viable n-gram models, whose parameters can be obtained using the WWW search engine Google. The detecting thresholds are optimized on the small training set. Finally we experimentally showed that our algorithm based on these simple models could perform the promising results with a precision of 89.2% on the total test set. 
Syntactically annotated corpora (treebanks) play an important role in recent statistical natural language processing. However, building a large treebank is labor intensive and time consuming work. To remedy this problem, there have been many attempts to develop software tools for annotating treebanks. This paper presents an integrated environment for annotating a treebank, called eBonsai. eBonsai helps annotators to choose a correct syntactic structure of a sentence from outputs of a parser, allowing the annotators to retrieve similar sentences in the treebank for referring to their structures. 
This paper studies issues on compiling a bilingual lexicon for technical terms. In the task of estimating bilingual term correspondences of technical terms, it is usually quite difﬁcult to ﬁnd an existing corpus for the domain of such technical terms. In this paper, we take an approach of collecting a corpus for the domain of such technical terms from the Web. As a method of translation estimation for technical terms, we propose a compositional translation estimation technique. Through experimental evaluation, we show that the domain/topic speciﬁc corpus contributes to improving the performance of the compositional translation estimation. 
We present the ﬁrst known empirical test of an increasingly common speculative claim, by evaluating a representative Chinese-toEnglish SMT model directly on word sense disambiguation performance, using standard WSD evaluation methodology and datasets from the Senseval-3 Chinese lexical sample task. Much effort has been put in designing and evaluating dedicated word sense disambiguation (WSD) models, in particular with the Senseval series of workshops. At the same time, the recent improvements in the BLEU scores of statistical machine translation (SMT) suggests that SMT models are good at predicting the right translation of the words in source language sentences. Surprisingly however, the WSD accuracy of SMT models has never been evaluated and compared with that of the dedicated WSD models. We present controlled experiments showing the WSD accuracy of current typical SMT models to be signiﬁcantly lower than that of all the dedicated WSD models considered. This tends to support the view that despite recent speculative claims to the contrary, current SMT models do have limitations in comparison with dedicated WSD models, and that SMT should beneﬁt from the better predictions made by the WSD models. 1The authors would like to thank the Hong Kong Research Grants Council (RGC) for supporting this research in part through grants RGC6083/99E, RGC6256/00E, and DAG03/04.EG09.  
This paper presents an HMM-based chunk tagger for Hindi. Various tagging schemes for marking chunk boundaries are discussed along with their results. Contextual information is incorporated into the chunk tags in the form of partof-speech (POS) information. This information is also added to the tokens themselves to achieve better precision. Error analysis is carried out to reduce the number of common errors. It is found that for certain classes of words, using the POS information is more effective than using a combination of word and POS tag as the token. Finally, chunk labels are also marked on the chunks. 
In this paper, a novel kernel-based method is presented for the problem of relation extraction between named entities from Chinese texts. The kernel is deﬁned over the original Chinese string representations around particular entities. As a kernel function, the Improved-Edit-Distance (IED) is used to calculate the similarity between two Chinese strings. By employing the Voted Perceptron and Support Vector Machine (SVM) kernel machines with the IED kernel as the classiﬁers, we tested the method by extracting person-afﬁliation relation from Chinese texts. By comparing with traditional feature-based learning methods, we conclude that our method needs less manual efforts in feature transformation and achieves a better performance. 
We present a neural-network based selforganizing approach that enables visualization of the information retrieval while at the same time improving its precision. In computer experiments, two-dimensional documentary maps in which queries and documents were mapped in topological order according to their similarities were created. The ranking of the results retrieved using the maps was better than that of the results obtained using a conventional TFIDF method. Furthermore, the precision of the proposed method was much higher than that of the conventional TFIDF method when the process was focused on retrieving highly relevant documents, suggesting that the proposed method might be especially suited to information retrieval tasks in which precision is more critical than recall. 
At present, the population of non-native speakers is twice that of native speakers. It is necessary to explore the text generation strategies for non-native users. However, little has been done in this ﬁeld. This study investigates the features that affect the placement (where to place a cue) of because for non-native speakers. A machine learning program – C4.5 was applied to induce the classiﬁcation models of the placement. 
We present a system to determine content similarity of documents. Our goal is to identify pairs of book chapters that are translations of the same original chapter. Achieving this goal requires identification of not only the different topics in the documents but also of the particular flow of these topics. Our approach to content similarity evaluation employs ngrams of lexical chains and measures similarity using the cosine of vectors of n-grams of lexical chains, vectors of tf*idfweighted keywords, and vectors of unweighted lexical chains (unigrams of lexical chains). Our results show that n-grams of unordered lexical chains of length four or more are particularly useful for the recognition of content similarity. 
In this paper, we present the HybridTrim system which uses a machine learning technique to combine linguistic, statistical and positional information to identify topic labels for headlines in a text. We compare our system with the Topiary system which, in contrast, uses a statistical learning approach to finding topic descriptors for headlines. The Topiary system, developed at the University of Maryland with BBN, was the top performing headline generation system at DUC 2004. Topiary-style headlines consist of a number of general topic labels followed by a compressed version of the lead sentence of a news story. The Topiary system uses a statistical learning approach to finding topic labels. The performance of these systems is evaluated using the ROUGE evaluation suite on the DUC 2004 news stories collection. 
This paper compares two storage models for gazetteers, nameley the standard one based on numbered indexing automata associated with an auxiliary storage device against a pure ﬁnite-state model, the latter being superior in terms of space and time complexity.1 
This paper describes our work to allow players in a virtual world to pose questions without relying on textual input. Our approach is to create enhanced virtual photographs by annotating them with semantic information from the 3D environment’s scene graph. The player can then use these annotated photos to interact with inhabitants of the world through automatically generated queries that are guaranteed to be relevant, grammatical and unambiguous. While the range of queries is more limited than a text input system would permit, in the gaming environment that we are exploring these limitations are offset by the practical concerns that make text input inappropriate. 
This paper addresses the task of extracting opinions from a given document collection. Assuming that an opinion can be represented as a tuple Subject, Attribute, Value , we propose a computational method to extract such tuples from texts. In this method, the main task is decomposed into (a) the process of extracting Attribute-Value pairs from a given text and (b) the process of judging whether an extracted pair expresses an opinion of the author. We apply machine-learning techniques to both subtasks. We also report on the results of our experiments and discuss future directions. 
This paper describes an attempt to recycle parts of the Czech-to-Russian machine translation system (MT) in the new Czech-to-English MT system. The paper describes the overall architecture of the new system and the details of the modules which have been added. A special attention is paid to the problem of named entity recognition and to the method of automatic acquisition of lexico-syntactic information for the bilingual dictionary of the system. The paper concentrates on the problems encountered in the process of reusing existing modules and their solution. 
The Malaysian Language is a formation of subject, predicate and object. The subject is the noun that take the action on the object and the predicate is the verb phrase in the sentence. Without a good corpus that can provide the part of speech, parsing is a complex process. As an option to the parsing, this paper discusses a way to identify the subject and the predicate, known as the pola-grammar technique. A pola or a pattern to be identified in the sentence are the Adjunct, Subject, Conjunction, Predicate and Object. 
In this paper we will brieﬂy survey the key results achieved so far in Hungarian POS tagging and show how classiﬁer combination techniques can aid the POS taggers. Methods are evaluated on a manually annotated corpus containing 1.2 million words. POS tagger tests were performed on single-domain, multiple domain and cross-domain test settings, and, to improve the accuracy of the taggers, various combination rules were implemented. The results indicate that combination schemas (like the Boosting algorithm) are promising tools which can signiﬁcantly degrade the classiﬁcation errors, and produce a more eﬀective tagger application. 
This paper discusses Korean morphological analysis and presents three probabilistic models for morphological analysis. Each model exploits a distinct linguistic unit as a processing unit. The three models can compensate for each other’s weaknesses. Contrary to the previous systems that depend on manually constructed linguistic knowledge, the proposed system can fully automatically acquire the linguistic knowledge from annotated corpora (e.g. part-ofspeech tagged corpora). Besides, without any modiﬁcation of the system, it can be applied to other corpora having different tagsets and annotation guidelines. We describe the models and present evaluation results on three corpora with a wide range of conditions. 
We present a new way to simplify the construction of precise broad-coverage grammars, employing typologicallymotivated, customizable extensions to a language-independent core grammar. Each ‘module’ represents a salient dimension of cross-linguistic variation, and presents the grammar developer with simple choices that result in automatically generated language-speciﬁc software. We illustrate the approach for several phenomena and explore the interdependence of the modules. 
This paper describes a non-statistical approach for semantic annotation of documents by analysing their syntax and by using semantic/syntactic behaviour patterns described in VerbNet. We use a two-stage approach, ﬁrstly identifying the semantic roles in a sentence, and then using these roles to represent some of the relations between the concepts in the sentence and a list of noun behaviour patterns to resolve some of the unknown (generic) relations between concepts. All outlined algorithms were tested on two corpora which differs in size, type, style and genre, and the performance does not vary signiﬁcantly. 
This paper presents a hybrid model for restoring an elided entry word for encyclopedia QA system. In Korean encyclopedia, an entry word is frequently omitted in a sentence. If the QA system uses a sentence without an entry word, it cannot provide a right answer. For resolving this problem, we combine a rule-based approach with Maximum Entropy model to use the merit of each approach. A rule-based approach uses caseframes and sense classes. The result shows that combined approach gives a 20% increase over our baseline. 
In this work we study the inﬂuence of corpus homogeneity on corpus-based NLP system performance. Experiments are performed on both stochastic language models and an EBMT system translating from Japanese to English with a large bicorpus, in order to reassess the assumption that using only homogeneous data tends to make system performance go up. We describe a method to represent corpus homogeneity as a distribution of similarity coefﬁcients based on a crossentropic measure investigated in previous works. We show that beyond minimal sizes of training data the excessive elimination of heterogeneous data proves prejudicial in terms of both perplexity and translation quality : excessively restricting the training data to a particular domain may be prejudicial in terms of In-Domain system performance, and that heterogeneous, Out-ofDomain data may in fact contribute to better sytem performance. 
This paper proposes a unified Transformation Based Learning (TBL, Brill, 1995) framework for Chinese Entity Detection and Tracking (EDT). It consists of two sub models: a mention detection model and an entity tracking/coreference model. The first sub-model is used to adapt existing Chinese word segmentation and Named Entity (NE) recognition results to a specific EDT standard to find all the mentions. The second sub-model is used to find the coreference relation between the mentions. In addition, a feedback technique is proposed to further improve the performance of the system. We evaluated our methods on the Automatic Content Extraction (ACE, NIST, 2003) Chinese EDT corpus. Results show that it outperforms the baseline, and achieves comparable performance with the stateof-the-art methods. 
In this paper, we propose a tree annotation tool using a parser in order to build a treebank. For the purpose of minimizing manual effort without any modiﬁcation of the parser, it performs twophase parsing for the intra-structure of each segment and the inter-structure after segmenting a sentence. Experimental results show that it can reduce manual effort about 24.5% as compared with a tree annotation tool without segmentation because an annotation’s intervention related to cancellation and reconstruction remarkably decrease although it requires the annotator to segment some long sentence. 
This paper proposes a unified evaluation method for multiple reading support systems such as a sentence translation system and a word translation system. In reading a non-native language text, these systems aim to lighten the reading burden. When we evaluate the performance of these systems, we cannot rely solely on these tests, as the output forms are different. Therefore, we must assess the performance of these systems based on the users’ reading comprehension and reading speed. We will further support our findings with experimental results. They show that the reading-speed procedure is able to evaluate the support systems, as well as, the comprehensionbased procedure proposed by Ohguro (1993) and Fuji et al. (2001). 
 Using natural language processing, we carried out a trend survey on Japanese natural language processing studies that have been done over the last ten years. We determined the changes in the number of papers published for each research organization and on each research area as well as the relationship between research organizations and research areas. This paper is useful for both recognizing trends in Japanese NLP and constructing a method of supporting trend surveys using NLP. 
In the Chinese language, a verb may have its dependents on its left, right or on both sides. The ambiguity resolution of right-side dependencies is essential for dependency parsing of sentences with two or more verbs. Previous works on shiftreduce dependency parsers may not guarantee the connectivity of a dependency tree due to their weakness at resolving the right-side dependencies. This paper proposes a two-phase shift-reduce dependency parser based on SVM learning. The leftside dependents and right-side nominal dependents are detected in Phase I, and rightside verbal dependents are decided in Phase II. In experimental evaluation, our proposed method outperforms previous shift-reduce dependency parsers for the Chine language, showing improvement of dependency accuracy by 10.08%. 
This paper presents an unsupervised relation extraction algorithm, which induces relations between entity pairs by grouping them into a “natural” number of clusters based on the similarity of their contexts. Stability-based criterion is used to automatically estimate the number of clusters. For removing noisy feature words in clustering procedure, feature selection is conducted by optimizing a trace based criterion subject to some constraint in an unsupervised manner. After relation clustering procedure, we employ a discriminative category matching (DCM) to ﬁnd typical and discriminative words to represent different relations. Experimental results show the effectiveness of our algorithm. 
Current NER approaches include: dictionary-based, rule-based, or machine learning. Since there is no consolidated nomenclature for most biomedical NEs, most NER systems relying on limited dictionaries or rules do not perform satisfactorily. In this paper, we apply Maximum Entropy (ME) to construct our NER framework. We represent shallow linguistic information as linguistic features in our ME model. On the GENIA 3.02 corpus, our system achieves satisfactory F-scores of 74.3% in protein and 70.0% overall without using any dictionary. Our system performs signiﬁcantly better than dictionary-based systems. Using partial match criteria, our system achieves an F-score of 81.3%. Using appropriate domain knowledge to modify the boundaries, our system has the potential to achieve an F-score of over 80%. 
After lying dormant for over two decades, automated text summarization has experienced a tremendous resurgence of interest in the past few years. Research is being conducted in China, Europe, Japan, and North America, and industry has brought to market more than 30 summarization systems; most recently, a series of large-scale text summarization evaluations, Document Understanding Conference (DUC) and Text Summarization Challenge (TSC) have been held yearly in the United States and Japan. 
One of the most active and promising areas of statistical machine translation (SMT) research are tree-based SMT approaches. Tree-based SMT has the potential to overcome the weaknesses of early SMT architectures which (a) do not handle long-distance dependencies well, and (b) are underconstrained in that they allow too much flexibility in word reordering. 
 Abstract. In this paper, we present an empirical study that utilizes morph-syntactical information to improve translation quality. With three kinds of language pairs matched according to morph-syntactical similarity or diﬀerence, we investigate the eﬀects of various morpho-syntactical information, such as base form, part-of-speech, and the relative positional information of a word in a statistical machine translation framework. We learn not only translation models but also word-based/class-based language models by manipulating morphological and relative positional information. And we integrate the models into a log-linear model. Experiments on multilingual translations showed that such morphological information as part-of-speech and base form are eﬀective for improving performance in morphologically rich language pairs and that the relative positional features in a word group are useful for reordering the local word orders. Moreover, the use of a class-based n-gram language model improves performance by alleviating the data sparseness problem in a word-based language model.  
 Abstract. This paper proposes a lexicon-constrained character model that combines both word and character features to solve complicated issues in Chinese morphological analysis. A Chinese character-based model constrained by a lexicon is built to acquire word building rules. Each character in a Chinese sentence is assigned a tag by the proposed model. The word segmentation and partof-speech tagging results are then generated based on the character tags. The proposed method solves such problems as unknown word identification, data sparseness, and estimation bias in an integrated, unified framework. Preliminary experiments indicate that the proposed method outperforms the best SIGHAN word segmentation systems in the open track on 3 out of the 4 test corpora. Additionally, our method can be conveniently integrated with any other Chinese morphological systems as a post-processing module leading to significant improvement in performance.  
Abstract. This paper shows that our WSD system using rich linguistic features achieved high accuracy in the classification of English SENSEVAL2 verbs for both fine-grained (64.6%) and coarse-grained (73.7%) senses. We describe three specific enhancements to our treatment of rich linguistic features and present their separate and combined contributions to our system’s performance. Further experiments showed that our system had robust performance on test data without high quality rich features. 
 Nick Cramer  M. L. Gregory Elizabeth Hetzler  Pacific Northwest  Pacific Northwest  Pacific Northwest  Pacific Northwest  National Laboratory  National Laboratory  National Laboratory National Laboratory  902 Battelle Blvd.  902 Battelle Blvd.  902 Battelle Blvd. 902 Battelle Blvd.  Richland, WA 99354  Richland, WA 99354  Richland, WA 99354 Richland, WA 99354  {thomas.hoeft;nick.cramer;michelle.gregory;beth.hetzler}@pnl.gov  
This paper describes a novel summarization system, Classummary, for interactive online classroom discussions. This system is originally designed for Open Source Software (OSS) development forums. However, this new application provides valuable feedback on designing summarization systems and applying them to everyday use, in addition to the traditional natural language processing evaluation methods. In our demonstration at HLT, new users will be able to direct this summarizer themselves. 
 We will demonstrate MindNet, a lexical resource built automatically by processing text. We will present two forms of MindNet: as a static lexical resource, and, as a toolkit which allows MindNets to be built from arbitrary text. We will also introduce a web-based interface to MindNet lexicons (MNEX) that is intended to make the data contained within MindNets more accessible for exploration. Both English and Japanese MindNets will be shown and will be made available, through MNEX, for research purposes.  
We describe a method for identifying systematic patterns in translation data using part-ofspeech tag sequences. We incorporate this analysis into a diagnostic tool intended for developers of machine translation systems, and demonstrate how our application can be used by developers to explore patterns in machine translation output. 
There was simply linguistics at the beginning. During the years, linguistics has been accompanied by various attributes. For example corpus one. While a name corpus is relatively young in linguistics, its content related to a language - collection of texts and speeches - is nothing new at all. Speaking about corpus linguistics nowadays, we keep in mind collecting of language resources in an electronic form. There is one more attribute that computers together with mathematics bring into linguistics - computational. The progress from working with corpus towards the computational approach is determined by the fact that electronic data with the ”unlimited” computer potential give opportunities to solve natural language processing issues in a fast way (with regard to the possibilities of human being) on a statistically signiﬁcant amount of data. Listing the attributes, we have to stop for a while by the notion of annotated corpora. Let us build a big corpus including all Czech text data available in an electronic form and look at it as a sequence of characters with the space having dominating status – a separator of words. It is very easy to compare two words (as strings), to calculate how many times these two words appear next to each other in a corpus, how many times they appear separately and so  on. Even more, it is possible to do it for every language (more or less). This kind of calculations is language independent – it is not restricted by the knowledge of language, its morphology, its syntax. However, if we want to solve more complex language tasks such as machine translation we cannot do it without deep knowledge of language. Thus, we have to transform language knowledge into an electronic form as well, i.e. we have to formalize it and then assign it to words (e.g., in case of morphology), or to sentences (e.g., in case of syntax). A corpus with additional information is called an annotated corpus. We are lucky. There is a real annotated corpus of Czech – Prague Dependency Treebank (PDT). PDT belongs to the top of the world corpus linguistics and its second edition is ready to be ofﬁcially published (for the ﬁrst release see (Hajicˇ et al., 2001)). PDT was born in Prague and had arisen from the tradition of the successful Prague School of Linguistics. The dependency approach to a syntactical analysis with the main role of verb has been applied. The annotations go from the morphological level to the tectogrammatical level (level of underlying syntactic structure) through the intermediate syntacticalanalytical level. The data (2 mil. words) have been annotated in the same direction, i.e., from a more simple level to a more  14 Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 14–15, Vancouver, October 2005.  complex one. This fact corresponds to the amount of data annotated on a particular level. The largest number of words have been annotated morphologically (2 mil. words) and the lowest number of words tectogramatically (0.8 mil. words). In other words, 0.8 million words have been annotated on all three levels, 1.5 mil. words on both morphological and syntactical level and 2 mil. words on the lowest morphological level. Besides the veriﬁcation of ’pre-PDT’ theories and formulation of new ones, PDT serves as training data for machine learning methods. Here, we present a system Styx that is designed to be an exercise book of Czech morphology and syntax with exercises directly selected from PDT. The schoolchildren can use a computer to write, to draw, to play games, to page encyclopedia, to compose music - why they could not use it to parse a sentence, to determine gender, number, case, . . . ? While the Styx development, two main phases have been passed: 1. transformation of an academic version of PDT into a school one. 20 thousand sentences were automatically selected out of 80 thousand sentences morphologically and syntactically annotated. The complexity of selected sentences exactly corresponds to the complexity of sentences exercised in the current textbooks of Czech. A syntactically annotated sentence in PDT is represented as a tree with the same number of nodes as is the number of the words in the given sentence. It differs from the schemes used at schools (Grepl and Karl´ık, 1998). On the other side, the linear structure of PDT morphological annotations was taken as it is – only morphological categories relevant to school syllabuses were preserved. 2. proposal and implementation of ex-  ercises. The general computer facilities of basic and secondary schools were taken into account while choosing a potential programming language to use. The Styx is implemented in Java that meets our main requirements – platform-independent system and system stability. At least to our knowledge, there is no such system for any language corpus that makes the schoolchildren familiar with an academic product. At the same time, our system represents a challenge and an opportunity for the academicians to popularize a ﬁeld devoted to the natural language processing with promising future. A number of electronic exercises of Czech morphology and syntax were created. However, they were built manually, i.e. authors selected sentences either from their minds or randomly from books, newspapers. Then they analyzed them manually. In a given manner, there is no chance to build an exercise system that reﬂects a real usage of language in such amount the Styx system fully offers. References Jan Hajicˇ, Eva Hajicˇova´, Barbora Hladka´, Petr Pajas, Jarmila Panevova´, and Petr Sgall. 2001. Prague Dependency Treebank 1.0 (Final Production Label) CD-ROM, CAT: LDC2001T10, ISBN 1-58563-212-0, Linguistic Data Consortium. Miroslav Grepl and Petr Karl´ık 1998. Skladba cˇesˇiny. [Czech Langauge.] Votobia, Praha.  
1. Introduction Machine translation has clearly entered into the marketplace as a helpful technology. Commercial applications are used on the internet for automatic translation of web pages and news articles. In the business environment, companies offer software that performs automatic translations of web sites for localization purposes, and translations of business documents (e.g., memo and e-mails). With regard to education, research using machine translation for language learning tools has been of interest since the early 1990’s (Anderson, 1993, Richmond, 1994, and Yasuda, 2004), though little has been developed. Very recently, Microsoft introduced a product called Writing Wizard that uses machine translation to assist with business writing for native Chinese speakers. To our knowledge, this is currently the only deployed education-based tool that uses machine translation. Currently, all writing-based English language learning (ELL) writing-based products and services at Educational Testing Service rely on e-rater automated essay scoring and the Critique writing analysis tool capabilities (Burstein, Chodorow, and Leacock, 2004). In trying to build on a portfolio of innovative products and services, we have explored using machine translation toward the development of new ELL-based capabilities. We have developed a prototype system for automatically generating translation exercises in Arabic --- the Translation Exercise Assistant. Translation exercises are one kind of task that teachers can offer to give students practice with specific grammatical structures in English. Our hypothesis is that teachers could use such a tool to help them create exercises for the  Daniel Marcu Language Weaver, Inc Marina del Rey, CA 90292 dmarcu@languageweaver.com classroom, homework, or quizzes. The idea behind our prototype is a capability that can be used either by classroom teachers to help them generate sentence-based translation exercises from an infinite number of Arabic language texts of their choice. The capability might be integrated into a larger English language learning application. In this latter application, these translation exercises could be created by classroom teachers for the class or for individuals who may need extra help with particular grammatical structures in English. Another potential use of this system that has been discussed is to use it in ESL classrooms in the United States, to allow teachers to offer exercises in students’ native language, especially for students who are competent in their own language, but only beginners in English. We had two primary goals in mind in developing our prototype. First, we wanted to evaluate how well the machine translation capability itself would work with this application. In other words, how useful were the system outputs that are based on the machine translations? We also wanted to know to what extent this kind of tool facilitated the task of creating translation exercise items. So, how much time is involved for a teacher to manually create these kinds of items versus using the exercise assistant tool to create them? Manually creating such an item involves searching through numerous reference sources (e.g., paper or webbased version of newspapers), finding sentences with the relevant grammatical structure in the source language (Arabic), and then manually producing an English translation that can be used as an answer key. To evaluate these aspects, we implemented a graphical user interface that offered our two users the ability to create sets of translation  16 Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 16–17, Vancouver, October 2005.  exercise items for six pre-selected, grammatical structures. For each structure the system automatically identified and offered a set of 200 system-selected potential sentences per category. For the exercise creation task, we collected timing information that told us how long it took users to create 3 exercises of 10 sentences each, for each category. In addition, users rated a set of up to 200 Arabic sentences with regard to if they were usable as translation exercise items, so that we could gauge the proportion of sentences selected by the application. These were the sentences that remained in the set of 200 because they were not selected for an exercise. Two teachers participated in the evaluation of our prototype. One of the users also did the task manually. 2. Translation Exercise Selection 2.1 Data Sets The source of the data was Arabic English Parallel News Part 1 and the Multiple Translation Arabic Part 1 corpus from the Linguistic Data Consortium.1 Across these data sets we had access to about 45,000 Arabic sentences from Arabic journalistic texts taken from Ummah Press Service, Xinhua News and the AFP News Service available for this research. We used approximately 10,000 of these Arabic sentences for system development, and selected sentences from the remaining Arabic sentences for use with the interface.2 2.2 System Description We used Language Weaver’s3 Arabic-to-English system to translate the Arabic sentences in the data sets. We built a module to find the relevant grammatical structures in the English translations. This module first passes the English 
 Interfaculty Initiative in Information Studies Information Technology Center  University of Tokyo  University of Tokyo  7-3-1 Hongo, Bunkyo, Tokyo,  7-3-1 Hongo, Bunkyo, Tokyo,  113-0033, JAPAN  113-0033, JAPAN  hoshino,nakagawa @dl.itc.u-tokyo.ac.jp  
This work demonstrates the ProMEDPLUS Epidemiological Fact Base. The facts are automatically extracted from plain-text reports about outbreaks of infectious epidemics around the world. The system collects new reports, extracts new facts, and updates the database, in real time. The extracted database is available on-line through a Web server. 
We will demonstrate the MIT Spoken Lecture Processing Server and an accompanying lecture browser that students can use to quickly locate and browse lecture segments that apply to their query. We will show how lecturers can upload recorded lectures and companion text material to our server for automatic processing. The server automatically generates a time-aligned word transcript of the lecture which can be downloaded for use within a browser. We will also demonstrate a browser we have created which allows students to quickly locate and browse audio segments that are relevant to their query. These tools can provide students with easier access to audio (or audio/visual) lectures, hopefully improving their educational experience. 
Consumers have to often wade through a large number of on-line reviews in order to make an informed product choice. We introduce OPINE, an unsupervised, high-precision information extraction system which mines product reviews in order to build a model of product features and their evaluation by reviewers. 
The POSBIOTM/W1 is a workbench for machine-learning oriented biomedical text mining system. The POSTBIOTM/W is intended to assist biologist in mining useful information efﬁciently from biomedical text resources. To do so, it provides a suit of tools for gathering, managing, analyzing and annotating texts. The workbench is implemented in Java, which means that it is platform-independent. 
We propose an approach to summarization exploiting both lexical information and the output of an automatic anaphoric resolver, and using Singular Value Decomposition (SVD) to identify the main terms. We demonstrate that adding anaphoric information results in signiﬁcant performance improvements over a previously developed system, in which only lexical terms are used as the input to SVD. However, we also show that how anaphoric information is used is crucial: whereas using this information to add new terms does result in improved performance, simple substitution makes the performance worse. 
This paper investigates automatic identiﬁcation of Information Structure (IS) in texts. The experiments use the Prague Dependency Treebank which is annotated with IS following the Praguian approach of Topic Focus Articulation. We automatically detect t(opic) and f(ocus), using node attributes from the treebank as basic features and derived features inspired by the annotation guidelines. We present the performance of decision trees (C4.5), maximum entropy, and rule induction (RIPPER) classiﬁers on all tectogrammatical nodes. We compare the results against a baseline system that always assigns f(ocus) and against a rule-based system. The best system achieves an accuracy of 90.69%, which is a 44.73% improvement over the baseline (62.66%). 
We present a novel mechanism for improving reference resolution by using the output of a relation tagger to rescore coreference hypotheses. Experiments show that this new framework can improve performance on two quite different languages -- English and Chinese. 
The paper proposes a Constrained EntityAlignment F-Measure (CEAF) for evaluating coreference resolution. The metric is computed by aligning reference and system entities (or coreference chains) with the constraint that a system (reference) entity is aligned with at most one reference (system) entity. We show that the best alignment is a maximum bipartite matching problem which can be solved by the Kuhn-Munkres algorithm. Comparative experiments are conducted to show that the widelyknown MUC F-measure has serious ﬂaws in evaluating a coreference system. The proposed metric is also compared with the ACE-Value, the ofﬁcial evaluation metric in the Automatic Content Extraction (ACE) task, and we conclude that the proposed metric possesses some properties such as symmetry and better interpretability missing in the ACE-Value. 
In this paper, we use the information redundancy in multilingual input to correct errors in machine translation and thus improve the quality of multilingual summaries. We consider the case of multidocument summarization, where the input documents are in Arabic, and the output summary is in English. Typically, information that makes it to a summary appears in many different lexical-syntactic forms in the input documents. Further, the use of multiple machine translation systems provides yet more redundancy, yielding different ways to realize that information in English. We demonstrate how errors in the machine translations of the input Arabic documents can be corrected by identifying and generating from such redundancy, focusing on noun phrases. 
Recognition errors hinder the proliferation of speech recognition (SR) systems. Based on the observation that recognition errors may result in ungrammatical sentences, especially in dictation application where an acceptable level of accuracy of generated documents is indispensable, we propose to incorporate two kinds of linguistic features into error detection: lexical features of words, and syntactic features from a robust lexicalized parser. Transformation-based learning is chosen to predict recognition errors by integrating word conﬁdence scores with linguistic features. The experimental results on a dictation data corpus show that linguistic features alone are not as useful as word conﬁdence scores in detecting errors. However, linguistic features provide complementary information when combined with word conﬁdence scores, which collectively reduce the classiﬁcation error rate by 12.30% and improve the F measure by 53.62%. 
Browsing through large volumes of spoken audio is known to be a challenging task for end users. One way to alleviate this problem is to allow users to gist a spoken audio document by glancing over a transcript generated through Automatic Speech Recognition. Unfortunately, such transcripts typically contain many recognition errors which are highly distracting and make gisting more difficult. In this paper we present an approach that detects recognition errors by identifying words which are semantic outliers with respect to other words in the transcript. We describe several variants of this approach. We investigate a wide range of evaluation measures and we show that we can significantly reduce the number of errors in content words, with the trade-off of losing some good content words. 
 4. Resolution of co-reference among entities  The accuracy of event extraction is limited by a number of complicating factors, with errors compounded at all sages inside the Information Extraction pipeline. In this paper, we present methods for recovering automatically from errors committed in the pipeline processing. Recovery is achieved via post-processing facts aggregated over a large collection of documents, and suggesting corrections based on evidence external to the document. A further improvement is derived from propagating multiple, locally non-best slot ﬁlls through the pipeline. Evaluation shows that the global analysis is over 10 times more likely to suggest valid corrections to the local-only analysis than it is to suggest erroneous ones. This yields a substantial overall gain, with no supervised training. 
This paper presents a novel approach to combining different word alignments. We view word alignment as a pattern classiﬁcation problem, where alignment combination is treated as a classiﬁer ensemble, and alignment links are adorned with linguistic features. A neural network model is used to learn word alignments from the individual alignment systems. We show that our alignment combination approach yields a signiﬁcant 20-34% relative error reduction over the best-known alignment combination technique on EnglishSpanish and English-Chinese data. 
We present a discriminative, largemargin approach to feature-based matching for word alignment. In this framework, pairs of word tokens receive a matching score, which is based on features of that pair, including measures of association between the words, distortion between their positions, similarity of the orthographic form, and so on. Even with only 100 labeled training examples and simple features which incorporate counts from a large unlabeled corpus, we achieve AER performance close to IBM Model 4, in much less time. Including Model 4 predictions as features, we achieve a relative AER reduction of 22% in over intersected Model 4 alignments. 
Bilingual word alignment forms the foundation of most approaches to statistical machine translation. Current word alignment methods are predominantly based on generative models. In this paper, we demonstrate a discriminative approach to training simple word alignment models that are comparable in accuracy to the more complex generative models normally used. These models have the the advantages that they are easy to add features to and they allow fast optimization of model parameters using small amounts of annotated data. 
This paper presents a maximum entropy word alignment algorithm for ArabicEnglish based on supervised training data. We demonstrate that it is feasible to create training material for problems in machine translation and that a mixture of supervised and unsupervised methods yields superior performance. The probabilistic model used in the alignment directly models the link decisions. Signiﬁcant improvement over traditional word alignment techniques is shown as well as improvement on several machine translation tests. Performance of the algorithm is contrasted with human annotation performance. 
Entity detection and tracking (EDT) is the task of identifying textual mentions of real-world entities in documents, extending the named entity detection and coreference resolution task by considering mentions other than names (pronouns, deﬁnite descriptions, etc.). Like NE tagging and coreference resolution, most solutions to the EDT task separate out the mention detection aspect from the coreference aspect. By doing so, these solutions are limited to using only local features for learning. In contrast, by modeling both aspects of the EDT task simultaneously, we are able to learn using highly complex, non-local features. We develop a new joint EDT model and explore the utility of many features, demonstrating their effectiveness on this task. 
A challenge for search systems is to detect not only when an item is relevant to the user’s information need, but also when it contains something new which the user has not seen before. In the TREC novelty track, the task was to highlight sentences containing relevant and new information in a short, topical document stream. This is analogous to highlighting key parts of a document for another person to read, and this kind of output can be useful as input to a summarization system. Search topics involved both news events and reported opinions on hot-button subjects. When people performed this task, they tended to select small blocks of consecutive sentences, whereas current systems identiﬁed many relevant and novel passages. We also found that opinions are much harder to track than events. 
New Event Detection (NED) involves monitoring chronologically-ordered news streams to automatically detect the stories that report on new events. We compare two stories by ﬁnding three cosine similarities based on names, topics and the full text. These additional comparisons suggest treating the NED problem as a binary classiﬁcation problem with the comparison scores serving as features. The classiﬁer models we learned show statistically signiﬁcant improvement over the baseline vector space model system on all the collections we tested, including the latest TDT5 collection. The presence of automatic speech recognizer (ASR) output of broadcast news in news streams can reduce performance and render our named entity recognition based approaches ineffective. We provide a solution to this problem achieving statistically signiﬁcant improvements. 
We propose a generalized bootstrapping algorithm in which categories are described by relevant seed features. Our method introduces two unsupervised steps that improve the initial categorization step of the bootstrapping scheme: (i) using Latent Semantic space to obtain a generalized similarity measure between instances and features, and (ii) the Gaussian Mixture algorithm, to obtain uniform classiﬁcation probabilities for unlabeled examples. The algorithm was evaluated on two Text Categorization tasks and obtained state-of-theart performance using only the category names as initial seeds. 
We present a method for speeding up the calculation of tree kernels during training. The calculation of tree kernels is still heavy even with efﬁcient dynamic programming (DP) procedures. Our method maps trees into a small feature space where the inner product, which can be calculated much faster, yields the same value as the tree kernel for most tree pairs. The training is sped up by using the DP procedure only for the exceptional pairs. We describe an algorithm that detects such exceptional pairs and converts trees into vectors in a feature space. We propose tree kernels on marked labeled ordered trees and show that the training of SVMs for semantic role labeling using these kernels can be sped up by a factor of several tens.  
In order to promote the study of automatic summarization and translation, we need an accurate automatic evaluation method that is close to human evaluation. In this paper, we present an evaluation method that is based on convolution kernels that measure the similarities between texts considering their substructures. We conducted an experiment using automatic summarization evaluation data developed for Text Summarization Challenge 3 (TSC-3). A comparison with conventional techniques shows that our method correlates more closely with human evaluations and is more robust. 
We approached the problem as learning how to order documents by estimated relevance with respect to a user query. Our support vector machines based classifier learns from the relevance judgments available with the standard test collections and generalizes to new, previously unseen queries. For this, we have designed a representation scheme, which is based on the discrete representation of the local (lw) and global (gw) weighting functions, thus is capable of reproducing and enhancing the properties of such popular ranking functions as tf.idf, BM25 or those based on language models. Our tests with the standard test collections have demonstrated the capability of our approach to achieve the performance of the best known scoring functions solely from the labeled examples and without taking advantage of knowing those functions or their important properties or parameters. 1. Introduction Our work is motivated by the objective to bring closer numerous achievements in the domains of machine learning and classification to the classical task of ad-hoc information retrieval (IR), which is ordering documents by the estimated degree of relevance to a given query. Although used with striking success for text categorization, classification-based approaches (e.g. those based on support vector machines, Joachims, 2001 ) have been relatively abandoned when trying to improve ad hoc retrieval in favor of empirical (e.g. vector space, Salton & McGill, 1983) or generative (e.g. language models; Zhai & Lafferty 2001; Song & Croft; 1999), which produce a ranking function that gives each document a score, rather than trying to learn a classifier that would help to discriminate between relevant and irrelevant documents and order them accordingly. A generative model needs to make assumptions that the query and document words are sampled from the same underlying distributions and that the distributions have certain forms, which entail specific smoothing techniques (e.g. popular Dirichlet-prior). A discriminative (classifier-based) model, on the other side, does not need to make any  assumptions about the forms of the underlying distributions or the criteria for the relevance but instead, learns to predict to which class a certain pattern (document) belongs to based on the labeled training examples. Thus, an important advantage of a discriminative approach for the information retrieval task, is its ability to explicitly utilize the relevance judgments existing with standard test collections in order to train the IR algorithms and possibly enhance retrieval accuracy for the new (unseen) queries. Cohen, Shapire and Singer (1999) noted the differences between ordering and classification and presented a two-stage model to learn ordering. The first stage learns a classifier for preference relations between objects using any suitable learning mechanism (e.g. support vector machines; Vapnik, 1998). The second stage converts preference relations into a rank order. Although the conversion may be NP complete in a general case, they presented efficient approximations. We limited our first study reported here to linear classifiers, in which conversion can be performed by simple ordering according to the score of each document. However, approaching the problem as “learning how to order things” allowed us to design our sampling and training mechanisms in a novel and, we believe, more powerful way. Our classifier learns how to compare every pair of documents with respect to a given query, based on the relevance indicating features that the documents may have. As it is commonly done in information retrieval, the features are derived from the word overlap between the query and documents. According to Nallapati (2004), the earliest formulation of the classic IR problem as a classification (discrimination) problem was suggested by Robertson and Sparck Jones (1976), however performed well only when the relevance judgments were available for the same query but not generalizing well to new queries. Fuhr and Buckley (1991) used polynomial regression to estimate the coefficients in a linear scoring function combining such well-known features as a weighted term frequency, document length and query length. They tested their “description-oriented” approach on the standard small-scale collections (Cranfield, NPL, INSPEC, CISI, CACM) to achieve the relative change in the average precision ranging from -17%  153 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP), pages 153–160, Vancouver, October 2005. c 2005 Association for Computational Linguistics  to + 33% depending on the collection tested and the implementation parameters. Gey (1994) applied logistic regression in a similar setting with the following results: Cranfield +12%, CACM +7.9%, CISI -4.4%, however he did not test them on new (unseen by the algorithm) queries, hypothesizing that splitting documents into training and testing collections would not be possible since “a large number of queries is necessary in order to train for a decent logistic regression approach to document retrieval.” Instead, he applied a regression trained on Cranfield to CISI collection but with a negative effect. Recently, the approaches based on learning have reported several important breakthroughs. Fan et al. (2004) applied genetic programming in order to learn how to combine various terms into the optimal ranking function that outperformed the popular Okapi formula on robust retrieval test collection. Nallapati (2004) made a strong argument in favor of discriminative models and trained an SVM-based classifier to combine 6 different components (terms) from the popular ranking functions (such as tf.idf and language models) to achieve better than the language model performance in 2 out of 16 test cases (figure 3 in Nallapati, 2004), not statistically distinguishable in 8 cases and only 80% of the best performance in 6 cases. There have been studies using past relevance judgements to optimize retrieval. For example, Joachims (2002) applied Support Vector Machines to learn linear ranking function from user click-throughs while interfacing with a search engine. In this study, we have developed a representation scheme, which is based on the discretization of the global (corpus statistics) and local (document statistics) weighting of term overlaps between queries and documents. We have empirically shown that this representation is flexible enough to learn the properties of the popular ranking functions: tf.idf, BM25 and the language models. The major difference of our work from Fan et al. (2004) or Nallapati (2004) or works on fusion (e.g. Vogt & Cottrell, 1999) is that we did not try to combine several known ranking functions (or their separate terms) into one, but rather we learn the weighting functions directly through discretization. Discretization allows representing a continuous function by a set of values at certain points. These values are learned by a machine learning technique to optimize certain criteria, e.g. average precision. Another important motivation behind using discretization was to design a representation with high dimensionality of features in order to combine our representation scheme with Support Vector Machines (SVM) (Vapnik, 1998), which are known to work well with a large number of features. SVM contains a large class of neural nets, radial margin separation (RBF) nets, and polynomial classifiers as special cases. They have been delivering superior performance in classification tasks in general domains, e.g. in face recognition (Hearst, 1998), and in text categorization (Joachims, 2001).  Another important distinction of this work from the prior research is that we train our classifier not to predict the absolute relevance of a document d with respect to a query q, but rather to predict which of the two documents d1, d2 is more relevant to the query q. The motivation for this distinction was that all the popular evaluation metrics in information retrieval (e.g. average precision) are based on document ranking rather than classification accuracy. This affected our specially designed sampling procedure which we empirically discovered to be crucial for successful learning. We have also empirically established that our combination of the representation scheme, learning mechanism and sampling allows learning from the past relevance judgments in order to successfully generalize to the new (unseen) queries. When the representation was created without any knowledge of the top ranking functions and their parameters, our approach reached the known top performance solely through the learning process. When our representation was taking advantage of functions that are known to perform well and their parameters, the resulting combination was able to slightly exceed the top performance on large test collections. The next section formalizes our Discretization Based Learning (DBL) approach to Information Retrieval, followed by empirical results and conclusions. 2. Formalization Of Our Approach 2.1 Query and Document Representation We limit our representation to the so called lw.gw class: R(q, d) = ∑ L ( tf ( t , d ), d ) G ( t ) , t⊂ q where L, local weighting, is the function of the number of occurrences of the term in the document tf, possibly combined with the other document statistics, e.g. word length. G(t), global weighting, can be any collection level statistic of the term. For example, in the classical tf.idf formula L(tf, d) = tf / |d|, where tf is the number of occurrences of the term t in the document, |d| is the length of the document vector and G(t) = log (N / df(t)), where df(t) is the total number of documents in the collection that have term t and N is the total number of documents. Without loss of generality it may also be extended to handle a number of occurrences of the term in the query, but we omit it here in our formalization for simplicity. Lw.gw class includes the BM25 Okapi ranking function which performs well on TREC collections (Robertson et al., 1996). It can be shown that many of the recently introduced language models fall into that category as well, specifically the best performing in TREC ad hoc tests Dirichlet smoothing, Jelinek Mercer smoothing, and Absolute Discounting approaches can be represented that way (see equation 6 and table I in Zhai & Lafferty, 2001). An lw.gw representation of Jelinek Mercer smoothing was used in Nallapati (2004). It has been known for a long time that the shapes of the global and local weighting functions can dramatically affect the precision in standard test collections because it in  154  fact determines the difference between such formulas as tf.idf, bm25 and language models. However, we are not aware of any attempts to learn those shapes directly from the labeled examples, which we performed in this study.  2.2 Intuition behind the discretization-based learning The intuition behind discretization approach is to represent a function by values at the finite number of points. Then, the optimal shape of the function can be learned by using one of the machine learning techniques. Our discretization based learning (DBL) approach to information retrieval learns how important each class of an occurrence of a query term in a document. For example, in some very “primitive” DBL approach, we can define two classes: Class S (“strong”), containing all multiple occurrences of a rare query term (e.g. “discretization”) in a document and Class W (“weak”), containing all single occurrences of a frequent term (e.g. “information”). Then, the machine learning technique should discover that the occurrences of Class S are much stronger indicators of relevance than the occurrences of Class W. In the DBL implementation presented in this paper, each occurrence of a query term is assigned to a class (called bin) based on the term document frequency in the collection (df) and the number of occurrences within the document (tf). The bin determines the weight of the contribution of each occurrence of the query term in the ranking score. Thus, the relevance score is just the weighted sum of the numbers of occurrences within each bin. The other way of looking at it is that the score is produced by a linear classifier, where the total number of occurrences within each bin serves as the feature value. By learning the optimal weights, a linear classifier effectively learns the optimal shapes of the global (gw) and local (lw) weighting functions. By learning the discrimination properties of each bin, rather than separate word terms, DBL method allows generalization to new queries.  2.3 Discretizing global weighting  We discretized the shape of the G(t) function by assigning each term to its global weighting bin g, which is an integer number in the [1, |B|] range, |B| is the total number of global weighting bins. The assignment of the term t to its global weighting bin g(t) is performed on the log linear scale according to the document frequency df of the term:  g(t) = {| B | (1- log (df(t)) )}  (1)  log(N)  where N is the total number of documents, {.} stands for rounding down to the nearest integer. The logarithmic scale allows more even term distribution among bins than simple linear assignment, which is desirable for more efficient learning. It is motivated by a typical histogram of df(t) distribution, which looks much more uniform in a logarithmic scale. It is important to note that it does not have anything to do with the log function in the classical idf weighting  and that the formula for g(t) does not produce any weights but only assigns each term occurrence to a specific bin based on the term document frequency. The weights are later trained and effectively define any shape of global weighting, including such simple functions tried in the prior heuristic explorations as logarithm, square root, reciprocal and others. 2.4 Discretizing local weighting Similarly to the global weighting, we assigned each occurrence of a term to its local weighting bin l, but this time by simply capping tf at the total number of local weighting bins |L|: l (tf(t, d), d) = min( tf (t, d), |L|) ) (1a) Let’s note that this particular representation does not really need rounding since tf is already a positive integer. However, in a more general case, tf can be normalized by document length (as is done in BM25 and language models) and thus local weighting would become a continuous function. It is important to note that our discrete representation does not ignore the occurrences above |L| but simply treats them the same way as tf = |L|. The intuition behind capping is that increasing tf above certain value (|L|) would not typically indicate the higher relevance of the document. Typically, a certain number of occurrences is enough to indicate the presence of the relevant passage. Please note again that this bin assignment does not assign any heuristic weights to the term occurrences. 2.5 Final discretized ranking function The bin assignments based on tf and df specified in sections 2.3 and 2.4 are straightforward and do not involve any significant “feature engineering.” Each occurrence of a query term in a document corresponds to a local/global bin combination (g, l). Each (g,l) combination determines a feature in a vector representing a document-query pair f(d, q) and is denoted below as f( d, q) [g , l] . The dimensionality of the feature space is |L| x |B|. E.g. for 8 local weighting bins and 10 global weighting bins we would deal with the vector size of 80. A feature vector f(d, q) represents each document d with respect to query q. The value of each feature in the vector is just the number of the term occurrences assigned to the pair of bins (g, l):  f ( d, q) [g , l] =  ∑1 (2)  t ⊂q, g (t )= g , l (t ,d )=l  Since our features capture local (tf) and global (df) term occurrence information, in order to represent a ranking function, we can simply use the dot product between the feature vector and the vector of learned optimal weights w: R(q, d) = w * f ( d, q). Ideally, the learning mechanism should assign higher weights to the more important bin combinations (e.g. multiple occurrence of a rare term) and low weights to the less important combinations (e.g. single occurrence of a common term). The exact learned values determine the optimal shape of global and local weighting.  155  We still can make the representation more powerful by considering the learned weights w[g, l] not the replacements but rather the adjustments to some other chosen global G (t) and local L (t, d) weighting functions:  f ( d, q) [g , l] =  ∑ L(t, d )G(t) (2a)  t ⊂q, g (t )= g , l (tf (t ,d ),d )=l  We define the specific choice of global G() and local L() weighting functions as starting ranking function (SRF). When all the bin weights w[g, l] are set to 1, our ranking function is the same as its SRF. The learning process finds the optimal values for w[g, l] for the collection of training queries and their relevance judgments, thus adjusting the important shapes of the global and local weighting to achieve better accuracy. SRF can be chosen from one of the known to perform well ranking functions (e.g. tf.idf or BM25 or based on language models) to take advantage of the fact that those formulas and their optimal parameters on the standard test collections are known for the researchers. Alternatively, we can set SRF to the constant value (e.g. 1 in formula 2), thus not taking advantage of any of the prior empirical investigations and to see if our framework is able to learn reasonable (or even top-notch) performance purely from labeled examples. Below, we describe our experiments with each approach. Since the score is linear with respect to the feature values, we can train the weights w as a linear classifier that predicts the preference relation between pairs of documents with respect to the given query. Document d1 is more likely to be relevant (has a higher score) than document d2 iff f(d1, q) * w > f(d1, q) * w. An important advantage of using a linear classifier is that rank ordering of documents according to the learned pairwise preferences can be simply performed by ordering according to the linear score. Please refer to Cohen et al. (1999) for the ordering algorithms in a more general non linear case. We chose support vector machines (SVM) for training the classifier weights w[g, l] since they are known to work well with large numbers of features, ranging in our experiments from 8 to 512, depending on the number of bins. For our empirical tests, we used the SVMLight package freely available for academic research from Joachims (2001). We preserved the default parameters coming with version V6.01. Although SVMLight package allows learning ranking, we opted for training it as a classifier to retain more control over sampling, which we found crucial for successful learning, as described in the section below.  2.6 Sampling  Since we were training a classifier to predict preference relations, but not the absolute value of relevance, we trained on the differences between feature vectors. Thus, for each selected (sampled) pair of documents (dr, di ), such that dr is a relevant document and di is irrelevant, the classifier was  presented with a positive example created from the vector of differences of features fp = f(q, dr) – f(q, di), and also with the negative example as the inverse of it: fn= f(q, di) – f(q, dr). This approach also balances positive and negative examples. We also informally experimented with training on absolute relevance judgments, similar to the prior work mentioned in the Introduction but obtained much worse results. We explain it by the fact that relative judgments (pairwise comparisons) are more generalizable to new queries than absolute judgments (relevant/irrelevant). This may explain prior difficulties with applying discriminative approaches mentioned in our Introduction. Since presenting all pairs to the training mechanism would be overwhelming, we performed pseudorandom sampling of documents by the following intuitive consideration. Since it is more efficient to present the classifier with the pairs from the documents that are likely to more strongly affect the performance metric (average precision), we first preordered the retrieved documents by any of the reasonably well-performing scoring function (e.g. tf.idf) and limited the sample of documents to the top 1000. Then, for each query, each known relevant document dr from that subset was selected and “paired” with a certain number of randomly selected irrelevant documents. This number was linearly decreasing with the position of the relevant document in the pre-order. Thus, the higher the document was positioned in the pre-order, the more times it was selected for pairing (training). This placed more emphasis at correctly classifying the more important document pairs in the average precision computation. Again, without the correct emphasis during sampling the obtained results were much weaker. However, the choice of the ranking function to perform pre-order was found to be not important: virtually the same results were obtained using tf.idf or bm25 or language models. 3. Empirical Evaluation 3.1 Empirical setup We used the TREC, Disks 1 and 2, collections to test our framework. We used topics 101-150 for training and 151-200 for testing and vice-versa. For indexing, we used the Lemur package (Kraaij et al., 2003), with the default set of parameters, and no stop word removal or stemming. Although those procedures are generally beneficial for accuracy, it is also known that they do not significantly interfere with testing various ranking functions and thus are omitted in many studies to allow easier replication. We used only topic titles for queries, as it is commonly done in experiments, e.g. in Nallapati (2004). We used the most popular average (noninterpolated) precision as our performance metric, computed by the script included with the Lemur toolkit (later verified by trec_eval). The characteristics of the collection after indexing are shown in Table 1. We also reproduced results similar to the reported below on the Disk 3 collection and  156  topics 101-150, but did not include them in this paper due to size limitations.  Collection Number of documents Number of terms Number of unique terms Average doc. length Topics  TREC Disks 1 and 2 741,863 325,059,876 697,610 438 101-200  Table 1. The characteristics of the test collection:  TREC Disks 1,2  3.2 The baseline In this study, we were interested exclusively in the improvements due to learning, thus still staying within the “bag of words” paradigm. Although many enhancements can be easily combined within our framework, we limited our search for the baseline performance to “bag of words” techniques to avoid unfair comparison. We used the results reported in Nallapati (2004) as guidance and verified that the best performing language model on this test collection was the one based on the Dirichlet smoothing with µ = 1900. Our average precision was lower (0.205 vs. 0.256), most likely due to the different indexing parameters, stemming or using a different stopword list. By experimenting with the other ranking functions and their parameters, we noticed that the implementation of BM25, available in Lemur, provided almost identical performance (0.204). Its ranking function is BM25 (tf, df) = tf / (tf + K* (1 – b + b * |d| / |d|a) * log ( N / (df + .5)), where |d| is the document word length and |d|a is its average across all documents. The optimal parameter values were close to the default K = 1.0 and b = .5. We noticed that the query term frequency components could be ignored without any noticeable loss of precision. This may be because the TREC topic titles are short and the words are very rarely repeated in the queries. Since the difference between this ranking function and the optimal from the available language models was negligible we selected the former as both our baseline and also as the starting ranking function (SRF) in our experiments. For simplicity, we call it simply BM25 throughout our paper.  3.3 Discretization accuracy Before testing the learning mechanism, we verified that the loss due to discretization is minimal and thus the approach is capable of capturing global and local weighting. For this, we discretized our baseline BM25 formula replacing each score contribution of the occurrence of a term G(t)L(t,d) = BM25(t, d) with its average across all other occurrences within  the same bin combination [g, l], which is determined by the formulas 1 and 1a. We discovered that for the |B| x |L| = 8 x 8 configuration, the loss in average precision did not exceed 2% (relatively). This demonstrates that the G(t)L(t,d) ranking functions can be discretized (replaced by values at certain points) at this level of granularity without losing much accuracy. We also verified that the weights w[g, l] can affect the performance significantly: when we set them to random numbers in the [0,1] range, the performance dropped by 50% relatively to the baseline. 3.4 Ability to achieve top performance from scratch First, we were curious to see if our framework can learn reasonable performance without taking advantage of our knowledge of the top ranking functions and their parameters. For this, we set our starting ranking function (SRF) to a constant value, thus using only the minimum out of the empirical knowledge and theoretical models developed by information retrieval researchers during several decades: specifically only the fact that relevance can be predicted by tf and df Table 2 shows performance for the 16 x 8 combination of bins. It can be seen that our approach has reached 90-100% of the top performance (baseline) solely through the learning process. The original performance is the one obtained by assigning all the classifier weights to 1. It can be seen that the topics 151-200 are more amenable for the technique that is why they show better recovery when used as a test set even when the training set 101-150 recovers only 90%. In order to evaluate if more training data can help, we also ran tests using 90 topics for training and the remaining 10 for testing. We ran 10 tests each time using 10 different sequential topics for testing and averaged our results. In this case, the averaged performance was completely restored to the baseline level with the mean difference in precision across test queries +0.5% and 1% standard deviation of the mean. Figure 1. Learning local weighting for various  Testing: Training: 101-150 151-200  101-150  151-200  Original  Learned  Baseline  Original  Learned  Baseline  .119  .165  .174  .135  .180  .204  .119  .175  .174  .135  .206  .204  Table 2. Learning without any knowledge of ranking functions. 16 x 8 bin design.  157  Testing: Training: 101-150 151-200  101-150 Learned .180 .179  Baseline .174 .174  151-200  % change +2.3 (+/- 0.9) +1.8 (+/- 1.0)  Learned .208 .210  Table 3. Surpassing the baseline performance. 8 x 8 bin design.  Baseline .204 .204  +2.3 (+/- 1.0) +3.2 (+/- 1.3)  numbers of bins. Learning on 101-150 and testing on 151-200.  Figure 2. Learning global weighting for various numbers of bins. Learning on 101-150 and testing on 151-200. We believe this is a remarkable result considering the difficulties that the prior learning based approaches had with the classical information retrieval task. We attribute our success to both higher flexibility and generalizability of our discrete representation. We also varied the number of bins to evaluate the effect of granularity of representation. Figures 1 and 2 demonstrate that 8 bins suffice for both global and local weighting. Higher numbers did not result in noticeable improvements. When the same set was used for training and testing the result obviously overestimates the learning capability of the framework. However, it also gives the upper bound of performance of a discretized gw.lw combination assuming that the loss due to discretization is negligible which can be easily attained by using sufficiently large number of bins. Thus, the results indicate that gw.lw, which includes practically all the popular “bag of words” ranking formulas such as tf.idf, BM25 or language models, has almost reached its upper limit and other classes of representations and ranking formulas need to be explored to attempt greater improvements. Figure 2. Learning global weighting for various numbers of bins. Learning on 101-150 and testing on 151-200.  3.5 Ability to surpass top performance  In order to test whether our approach can exceed the baseline performance we set BM25 to be our starting ranking function (SRF). Thus, in this case:  G(t) = log ( N / (df + .5))  (6)  L(tf, d) = tf / (tf + K * (1 – b + b * |d| / |d|a) Table 3 shows performance for the 8 by 8 bin design. Although the improvement is relatively small (2-3%) it is still statistically significant at the level of alpha < 0.1, when the paired t-test was performed. The  value in “% change” column shows the mean % improvement across all the queries and its standard deviation. It may differ from the % change of the mean performance since there is wide variability in the performance across queries but smaller variability in the improvement. We believe even such a small improvement is remarkable considering the amount of attention the researches have paid to optimizing the ranking functions for this specific data set which has been available for more than seven years. A number of recent studies reported comparable improvements on the same test collection by using more elaborate modeling or richer representations. Of course the improvement due to the techniques such as those based on n-grams, document structures, natural language processing or query expansion can possibly achieve even better results. However in this study we deliberately limited our focus to the “bags of words.” 3.6 Shape of optimal local weighting Figure 3 shows the optimal shape of the local weighting function L(tf) learned on entire set of 100 topics and plotted against their counterparts of BM25(t, d) = tf / (tf + 1) and tf.idf(t, d) = tf for comparison. For plotting purposes, we assumed that the document length was equal to its average. The values were linearly scaled to meet at the tf = 8 point. It is easy to observe that the behavior of the optimal function is much closer to BM25 than to tf.idf, which explains the good performance of the former on this test set. BM25  Gw(tf)  DBL (Learned)  TF-IDF  
We describe stochastic models of local phrase movement that can be incorporated into a Statistical Machine Translation (SMT) system. These models provide properly formulated, non-deﬁcient, probability distributions over reordered phrase sequences. They are implemented by Weighted Finite State Transducers. We describe EM-style parameter re-estimation procedures based on phrase alignment under the complete translation model incorporating reordering. Our experiments show that the reordering model yields substantial improvements in translation performance on Arabic-to-English and Chinese-to-English MT tasks. We also show that the procedure scales as the bitext size is increased. 
 HMM-based models are developed for the alignment of words and phrases in bitext. The models are formulated so that alignment and parameter estimation can be performed efﬁciently. We ﬁnd that ChineseEnglish word alignment performance is comparable to that of IBM Model-4 even over large training bitexts. Phrase pairs extracted from word alignments generated under the model can also be used for phrase-based translation, and in Chinese to English and Arabic to English translation, performance is comparable to systems based on Model-4 alignments. Direct phrase pair induction under the model is described and shown to improve translation performance. 
Most statistical translation systems are based on phrase translation pairs, or “blocks”, which are obtained mainly from word alignment. We use blocks to infer better word alignment and improved word alignment which, in turn, leads to better inference of blocks. We propose two new probabilistic models based on the innerouter segmentations and use EM algorithms for estimating the models’ parameters. The ﬁrst model recovers IBM Model-1 as a special case. Both models outperform bidirectional IBM Model-4 in terms of word alignment accuracy by 10% absolute on the F-measure. Using blocks obtained from the models in actual translation systems yields statistically signiﬁcant improvements in Chinese-English SMT evaluation. 
We present a new word-alignment approach that learns errors made by existing word alignment systems and corrects them. By adapting transformationbased learning to the problem of word alignment, we project new alignment links from already existing links, using features such as POS tags. We show that our alignment link projection approach yields a signiﬁcantly lower alignment error rate than that of the best performing alignment system (22.6% relative reduction on EnglishSpanish data and 23.2% relative reduction on English-Chinese data). 
We explore the beneﬁt that users in several application areas can experience from a “tab-complete” editing assistance function. We develop an evaluation metric and adapt N -gram language models to the problem of predicting the subsequent words, given an initial text fragment. Using an instance-based method as baseline, we empirically study the predictability of call-center emails, personal emails, weather reports, and cooking recipes. 
During the last years there has been growing interest in using neural networks for language modeling. In contrast to the well known back-off n-gram language models, the neural network approach attempts to overcome the data sparseness problem by performing the estimation in a continuous space. This type of language model was mostly used for tasks for which only a very limited amount of in-domain training data is available. In this paper we present new algorithms to train a neural network language model on very large text corpora. This makes possible the use of the approach in domains where several hundreds of millions words of texts are available. The neural network language model is evaluated in a state-ofthe-art real-time continuous speech recognizer for French Broadcast News. Word error reductions of 0.5% absolute are reported using only a very limited amount of additional processing time. 
This paper proposes a new discriminative training method, called minimum sample risk (MSR), of estimating parameters of language models for text input. While most existing discriminative training methods use a loss function that can be optimized easily but approaches only approximately to the objective of minimum error rate, MSR minimizes the training error directly using a heuristic training procedure. Evaluations on the task of Japanese text input show that MSR can handle a large number of features and training samples; it significantly outperforms a regular trigram model trained using maximum likelihood estimation, and it also outperforms the two widely applied discriminative methods, the boosting and the perceptron algorithms, by a small but statistically significant margin. 
To improve the robustness in multimodal input interpretation, this paper presents a new salience driven approach. This approach is based on the observation that, during multimodal conversation, information from deictic gestures (e.g., point or circle) on a graphical display can signal a part of the physical world (i.e., representation of the domain and task) of the application which is salient during the communication. This salient part of the physical world will prime what users tend to communicate in speech and in turn can be used to constrain hypotheses for spoken language understanding, thus improving overall input interpretation. Our experimental results have indicated the potential of this approach in reducing word error rate and improving concept identification in multimodal conversation. 
We describe the error handling architectture underlying the RavenClaw dialog management framework. The architecture provides a robust basis for current and future research in error detection and recovery. Several objectives were pursued in its development: task-independence, ease-ofuse, adaptability and scalability. We describe the key aspects of architectural design which confer these properties, and discuss the deployment of this architectture in a number of spoken dialog systems spanning several domains and interaction types. Finally, we outline current research projects supported by this architecture. 
We identify a set of prosodic cues for parsing conversational speech and show how such features can be effectively incorporated into a statistical parsing model. On the Switchboard corpus of conversational speech, the system achieves improved parse accuracy over a state-of-the-art system which uses only lexical and syntactic features. Since removal of edit regions is known to improve downstream parse accuracy, we explore alternatives for edit detection and show that PCFGs are not competitive with more specialized techniques. 
Machine summaries can be improved by using knowledge about the cognitive status of news article referents. In this paper, we present an approach to automatically acquiring distinctions in cognitive status using machine learning over the forms of referring expressions appearing in the input. We focus on modeling references to people, both because news often revolve around people and because existing natural language tools for named entity identiﬁcation are reliable. We examine two speciﬁc distinctions—whether a person in the news can be assumed to be known to a target audience (hearer-old vs hearer-new) and whether a person is a major character in the news story. We report on machine learning experiments that show that these distinctions can be learned with high accuracy, and validate our approach using human subjects. 
The paper presents a Bayesian model for text summarization, which explicitly encodes and exploits information on how human judgments are distributed over the text. Comparison is made against non Bayesian summarizers, using test data from Japanese news texts. It is found that the Bayesian approach generally leverages performance of a summarizer, at times giving it a signiﬁcant lead over nonBayesian models. 
In this paper we consider the problem of analysing sentence-level discourse structure. We introduce discourse chunking (i.e., the identiﬁcation of intra-sentential nucleus and satellite spans) as an alternative to full-scale discourse parsing. Our experiments show that the proposed modelling approach yields results comparable to state-of-the-art while exploiting knowledge-lean features and small amounts of discourse annotations. We also demonstrate how discourse chunking can be successfully applied to a sentence compression task. 
This paper presents comparative experimental results on four techniques of language model adaptation, including a maximum a posteriori (MAP) method and three discriminative training methods, the boosting algorithm, the average perceptron and the minimum sample risk method, on the task of Japanese Kana-Kanji conversion. We evaluate these techniques beyond simply using the character error rate (CER): the CER results are interpreted using a metric of domain similarity between background and adaptation domains, and are further evaluated by correlating them with a novel metric for measuring the side effects of adapted models. Using these metrics, we show that the discriminative methods are superior to a MAP-based method not only in terms of achieving larger CER reduction, but also of being more robust against the similarity of background and adaptation domains, and achieve larger CER reduction with fewer side effects. 
Prepositional Phrase-attachment is a common source of ambiguity in natural language. The previous approaches use limited information to solve the ambiguity – four lexical heads – although humans disambiguate much better when the full sentence is available. We propose to solve the PP-attachment ambiguity with a Support Vector Machines learning model that uses complex syntactic and semantic features as well as unsupervised information obtained from the World Wide Web. The system was tested on several datasets obtaining an accuracy of 93.62% on a Penn Treebank-II dataset; 91.79% on a FrameNet dataset when no manuallyannotated semantic information is provided and 92.85% when semantic information is provided. 
Weighted deduction with aggregation is a powerful theoretical formalism that encompasses many NLP algorithms. This paper proposes a declarative speciﬁcation language, Dyna; gives general agenda-based algorithms for computing weights and gradients; brieﬂy discusses Dyna-to-Dyna program transformations; and shows that a ﬁrst implementation of a Dyna-to-C++ compiler produces code that is efﬁcient enough for real NLP research, though still several times slower than hand-crafted code. 
Text generation requires a planning module to select an object of discourse and its properties. This is specially hard in descriptive games, where a computer agent tries to describe some aspects of a game world. We propose to formalize this problem as a Markov Decision Process, in which an optimal message policy can be deﬁned and learned through simulation. Furthermore, we propose back-off policies as a novel and effective technique to ﬁght state dimensionality explosion in this framework. 
The original motivation for using question series in the TREC 2004 question answering track was the desire to model aspects of dialogue processing in an evaluation task that included different question types. The structure introduced by the series also proved to have an important additional beneﬁt: the series is at an appropriate level of granularity for aggregating scores for an effective evaluation. The series is small enough to be meaningful at the task level since it represents a single user interaction, yet it is large enough to avoid the highly skewed score distributions exhibited by single questions. An analysis of the reliability of the per-series evaluation shows the evaluation is stable for differences in scores seen in the track. The development of question answering technology in recent years has been driven by tasks deﬁned in community-wide evaluations such as TREC, NTCIR, and CLEF. The TREC question answering (QA) track started in 1999, with the ﬁrst several editions of the track focused on factoid questions. A factoid question is a fact-based, short answer question such as How many calories are there in a Big Mac?. The track has evolved by increasing the type and difﬁculty of questions that are included in the test set. The task in the TREC 2003 QA track was a combined task that contained list and deﬁnition questions in addition to factoid questions (Voorhees,  2004). A list question asks for different instances of a particular kind of information to be returned, such as List the names of chewing gums. Answering such questions requires a system to assemble an answer from information located in multiple documents. A deﬁnition question asks for interesting information about a particular person or thing such as Who is Vlad the Impaler? or What is a golden parachute?. Deﬁnition questions also require systems to locate information in multiple documents, but in this case the information of interest is much less crisply delineated. Like the NTCIR4 QACIAD challenge (Kato et al., 2004), the TREC 2004 QA track grouped questions into series, using the series as abstractions of information-seeking dialogues. In addition to modeling a real user task, the series are a step toward incorporating context-processing into QA evaluation since earlier questions in a series provide some context for the current question. In the case of the TREC series, each series contained factoid and list questions and had the target of a deﬁnition associated with it. Each question in a series asked for some information about the target. In addition, the ﬁnal question in each series was an explicit “other” question, which was to be interpreted as “Tell me other interesting things about this target I don’t know enough to ask directly”. This last question was roughly equivalent to the deﬁnition questions in the TREC 2003 task. This paper examines the efﬁcacy of series-based QA evaluation, and demonstrates that aggregating scores over individual series provides a more meaningful evaluation than averages of individual ques-  299 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP), pages 299–306, Vancouver, October 2005. c 2005 Association for Computational Linguistics  tion scores. The next section describes the question series that formed the basis of the TREC 2004 evaluation. Since TREC uses different evaluation protocols for different question types, the following section describes the way in which individual question types were evaluated. Section 3 contrasts the scores obtained by aggregating individual question scores by question type or by series, and shows the use of series leads to a reliable evaluation at differences in scores that are observed in practice. 
We explore a hybrid approach for Chinese deﬁnitional question answering by combining deep linguistic analysis with surface pattern learning. We answer four questions in this study: 1) How helpful are linguistic analysis and pattern learning? 2) What kind of questions can be answered by pattern matching? 3) How much annotation is required for a pattern-based system to achieve good performance? 4) What linguistic features are most useful? Extensive experiments are conducted on biographical questions and other deﬁnitional questions. Major ﬁndings include: 1) linguistic analysis and pattern learning are complementary; both are required to make a good deﬁnitional QA system; 2) pattern matching is very effective in answering biographical questions while less effective for other deﬁnitional questions; 3) only a small amount of annotation is required for a pattern learning system to achieve good performance on biographical questions; 4) the most useful linguistic features are copulas and appositives; relations also play an important role; only some propositions convey vital facts. 
Question classiﬁcation is an important step in factual question answering (QA) and other dialog systems. Several attempts have been made to apply statistical machine learning approaches, including Support Vector Machines (SVMs) with sophisticated features and kernels. Curiously, the payoff beyond a simple bag-ofwords representation has been small. We show that most questions reveal their class through a short contiguous token subsequence, which we call its informer span. Perfect knowledge of informer spans can enhance accuracy from 79.4% to 88% using linear SVMs on standard benchmarks. In contrast, standard heuristics based on shallow pattern-matching give only a 3% improvement, showing that the notion of an informer is non-trivial. Using a novel multi-resolution encoding of the question’s parse tree, we induce a Conditional Random Field (CRF) to identify informer spans with about 85% accuracy. Then we build a meta-classiﬁer using a linear SVM on the CRF output, enhancing accuracy to 86.2%, which is better than all published numbers. 
We present a practically unsupervised learning method to produce single-snippet answers to deﬁnition questions in question answering systems that supplement Web search engines. The method exploits on-line encyclopedias and dictionaries to generate automatically an arbitrarily large number of positive and negative deﬁnition examples, which are then used to train an SVM to separate the two classes. We show experimentally that the proposed method is viable, that it outperforms the alternative of training the system on questions and news articles from TREC, and that it helps the search engine handle deﬁnition questions signiﬁcantly better. 
A content selection component determines which information should be conveyed in the output of a natural language generation system. We present an efﬁcient method for automatically learning content selection rules from a corpus and its related database. Our modeling framework treats content selection as a collective classiﬁcation problem, thus allowing us to capture contextual dependencies between input items. Experiments in a sports domain demonstrate that this approach achieves a substantial improvement over context-agnostic methods. 
Consumers are often forced to wade through many on-line reviews in order to make an informed product choice. This paper introduces OPINE, an unsupervised informationextraction system which mines reviews in order to build a model of important product features, their evaluation by reviewers, and their relative quality across products. Compared to previous work, OPINE achieves 22% higher precision (with only 3% lower recall) on the feature extraction task. OPINE’s novel use of relaxation labeling for ﬁnding the semantic orientation of words in context leads to strong performance on the tasks of ﬁnding opinion phrases and their polarity. 
This paper presents a new approach to phrase-level sentiment analysis that ﬁrst determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions. With this approach, the system is able to automatically identify the contextual polarity for a large subset of sentiment expressions, achieving results that are signiﬁcantly better than baseline. 
Recent systems have been developed for sentiment classiﬁcation, opinion recognition, and opinion analysis (e.g., detecting polarity and strength). We pursue another aspect of opinion analysis: identifying the sources of opinions, emotions, and sentiments. We view this problem as an information extraction task and adopt a hybrid approach that combines Conditional Random Fields (Lafferty et al., 2001) and a variation of AutoSlog (Riloff, 1996a). While CRFs model source identiﬁcation as a sequence tagging task, AutoSlog learns extraction patterns. Our results show that the combination of these two methods performs better than either one alone. The resulting system identiﬁes opinion sources with 79.3% precision and 59.5% recall using a head noun matching measure, and 81.2% precision and 60.6% recall using an overlap measure. 
This research is aimed at the problem of disambiguating toponyms (place names) in terms of a classification derived by merging information from two publicly available gazetteers. To establish the difficulty of the problem, we measured the degree of ambiguity, with respect to a gazetteer, for toponyms in news. We found that 67.82% of the toponyms found in a corpus that were ambiguous in a gazetteer lacked a local discriminator in the text. Given the scarcity of humanannotated data, our method used unsupervised machine learning to develop disambiguation rules. Toponyms were automatically tagged with information about them found in a gazetteer. A toponym that was ambiguous in the gazetteer was automatically disambiguated based on preference heuristics. This automatically tagged data was used to train a machine learner, which disambiguated toponyms in a human-annotated news corpus at 78.5% accuracy. 
Exhaustive extraction of semantic information from text is one of the formidable goals of state-of-the-art NLP systems. In this paper, we take a step closer to this objective. We combine the semantic information provided by different resources and extract new semantic knowledge to improve the performance of a recognizing textual entailment system. 
In this paper, we describe an integrated approach to entity mention detection that yields a monolithic, almost language independent system. It is optimal in the sense that all categorical constraints are simultaneously considered. The system is compact and easy to develop and maintain, since only a single set of features and classiﬁers are needed to be designed and optimized. It is implemented using oneversus-all support vector machine (SVM) classiﬁers and a number of feature extractors at several linguistic levels. SVMs are well known for their ability to handle a large set of overlapping features with theoretically sound generalization properties. Data sparsity might be an important issue as a result of a large number of classes and relatively moderate training data size. However, we report results that the integrated system performs as good as a pipelined system that decomposes the problem into a few smaller subtasks. We conduct all our experiments using ACE 2004 data, evaluate the systems using ACE metrics and report competitive performance. 
We present a system for deciding whether a given sentence can be inferred from text. Each sentence is represented as a directed graph (extracted from a dependency parser) in which the nodes represent words or phrases, and the links represent syntactic and semantic relationships. We develop a learned graph matching approach to approximate entailment using the amount of the sentence’s semantic content which is contained in the text. We present results on the Recognizing Textual Entailment dataset (Dagan et al., 2005), and show that our approach outperforms Bag-Of-Words and TF-IDF models. In addition, we explore common sources of errors in our approach and how to remedy them.  imate translations which structurally differ from our reference translation. One sub-task underlying these applications is the ability to recognize semantic entailment; whether one piece of text follows from another. In contrast to recent work which has successfully utilized logicbased abductive approaches to inference (Moldovan et al., 2003; Raina et al., 2005b), we adopt a graphbased representation of sentences, and use graph matching approach to measure the semantic overlap of text. Graph matching techniques have proven to be a useful approach for tractable approximate matching in other domains including computer vision. In the domain of language, graphs provide a natural way to express the dependencies between words and phrases in a sentence. Furthermore, graph matching also has the advantage of providing a framework for structural matching of phrases that would be difﬁcult to resolve at the level of individual words.  
“Bootstrapping” methods for learning require a small amount of supervision to seed the learning process. We show that it is sometimes possible to eliminate this last bit of supervision, by trying many candidate seeds and selecting the one with the most plausible outcome. We discuss such “strapping” methods in general, and exhibit a particular method for strapping wordsense classiﬁers for ambiguous words. Our experiments on the Canadian Hansards show that our unsupervised technique is signiﬁcantly more effective than picking seeds by hand (Yarowsky, 1995), which in turn is known to rival supervised methods. 
Recent studies into Web retrieval have shown that word sense disambiguation can increase retrieval effectiveness. However, it remains unclear as to the minimum disambiguation accuracy required and the granularity with which one must define word sense in order to maximize these benefits. This study answers these questions using a simulation of the effects of ambiguity on information retrieval. It goes beyond previous studies by differentiating between homonymy and polysemy. Results show that retrieval is more sensitive to polysemy than homonymy and that, when resolving polysemy, accuracy as low as 55% can potentially lead to increased performance. 
This paper introduces a graph-based algorithm for sequence data labeling, using random walks on graphs encoding label dependencies. The algorithm is illustrated and tested in the context of an unsupervised word sense disambiguation problem, and shown to signiﬁcantly outperform the accuracy achieved through individual label assignment, as measured on standard senseannotated data sets. 
Distributions of the senses of words are often highly skewed. This fact is exploited by word sense disambiguation (WSD) systems which back off to the predominant sense of a word when contextual clues are not strong enough. The domain of a document has a strong inﬂuence on the sense distribution of words, but it is not feasible to produce large manually annotated corpora for every domain of interest. In this paper we describe the construction of three sense annotated corpora in different domains for a sample of English words. We apply an existing method for acquiring predominant sense information automatically from raw text, and for our sample demonstrate that (1) acquiring such information automatically from a mixeddomain corpus is more accurate than deriving it from SemCor, and (2) acquiring it automatically from text in the same domain as the target domain performs best by a large margin. We also show that for an all words WSD task this automatic method is best focussed on words that are salient to the domain, and on words with a different acquired predominant sense in that domain compared to that acquired from a balanced corpus. 
This paper proposes a hybrid Chinese named entity recognition model based on multiple features. It differentiates from most of the previous approaches mainly as follows. Firstly, the proposed Hybrid Model integrates coarse particle feature (POS Model) with fine particle feature (Word Model), so that it can overcome the disadvantages of each other. Secondly, in order to reduce the searching space and improve the efficiency, we introduce heuristic human knowledge into statistical model, which could increase the performance of NER significantly. Thirdly, we use three sub-models to respectively describe three kinds of transliterated person name, that is, Japanese, Russian and Euramerican person name, which can improve the performance of PN recognition. From the experimental results on People's Daily testing data, we can conclude that our Hybrid Model is better than the models which only use one kind of features. And the experiments on MET-2 testing data also confirm the above conclusion, which show that our algorithm has consistence on different testing data. 
Existing named entity (NE) transliteration approaches often exploit a general model to transliterate NEs, regardless of their origins. As a result, both a Chinese name and a French name (assuming it is already translated into Chinese) will be translated into English using the same model, which often leads to unsatisfactory performance. In this paper we propose a cluster-specific NE transliteration framework. We group name origins into a smaller number of clusters, then train transliteration and language models for each cluster under a statistical machine translation framework. Given a source NE, we first select appropriate models by classifying it into the most likely cluster, then we transliterate this NE with the corresponding models. We also propose a phrasebased name transliteration model, which effectively combines context information for transliteration. Our experiments showed substantial improvement on the transliteration accuracy over a state-of-the-art baseline system, significantly reducing the transliteration character error rate from 50.29% to 12.84%. 
There has been little prior work on Named Entity Recognition for ”informal” documents like email. We present two methods for improving performance of person name recognizers for email: emailspeciﬁc structural features and a recallenhancing method which exploits name repetition across multiple documents. 
Many proper names are spelled inconsistently in speech recognizer output, posing a problem for applications where locating mentions of named entities is critical. We model the distortion in the spelling of a name due to the speech recognizer as the effect of a noisy channel. The models follow the framework of the IBM translation models. The model is trained using a parallel text of closed caption and automatic speech recognition output. We also test a string edit distance based method. The effectiveness of these models is evaluated on a name query retrieval task. Our methods result in a 60% improvement in F1. We also demonstrate why the problem has not been critical in TREC and TDT tasks. 
We present a part-of-speech tagger which introduces two new concepts: virtual evidence in the form of an “observed child” node, and negative training data to learn the conditional probabilities for the observed child. Associated with each word is a ﬂexible feature-set which can include binary ﬂags, neighboring words, etc. The conditional probability of Tag given Word + Features is implemented using a factored language-model with back-off to avoid data sparsity problems. This model remains within the framework of Dynamic Bayesian Networks (DBNs) and is conditionally-structured, but resolves the label bias problem inherent in the conditional Markov model (CMM). 
This paper presents a bidirectional inference algorithm for sequence labeling problems such as part-of-speech tagging, named entity recognition and text chunking. The algorithm can enumerate all possible decomposition structures and ﬁnd the highest probability sequence together with the corresponding decomposition structure in polynomial time. We also present an efﬁcient decoding algorithm based on the easiest-ﬁrst strategy, which gives comparably good performance to full bidirectional inference with signiﬁcantly lower computational cost. Experimental results of part-of-speech tagging and text chunking show that the proposed bidirectional inference methods consistently outperform unidirectional inference methods and bidirectional MEMMs give comparable performance to that achieved by state-of-the-art learning algorithms including kernel support vector machines. 
Finite-state approaches have been highly successful at describing the morphological processes of many languages. Such approaches have largely focused on modeling the phone- or character-level processes that generate candidate lexical types, rather than tokens in context. For the full analysis of words in context, disambiguation is also required (Hakkani-Tu¨r et al., 2000; Hajicˇ et al., 2001). In this paper, we apply a novel source-channel model to the problem of morphological disambiguation (segmentation into morphemes, lemmatization, and POS tagging) for concatenative, templatic, and inﬂectional languages. The channel model exploits an existing morphological dictionary, constraining each word’s analysis to be linguistically valid. The source model is a factored, conditionally-estimated random ﬁeld (Lafferty et al., 2001) that learns to disambiguate the full sentence by modeling local contexts. Compared with baseline state-of-the-art methods, our method achieves statistically signiﬁcant error rate reductions on Korean, Arabic, and Czech, for various training set sizes and accuracy measures.  In this paper, we describe context-based models for morphological disambiguation that take full account of existing morphological dictionaries by estimating conditionally against only dictionary-accepted analyses of a sentence (§2). These models are an instance of conditional random ﬁelds (CRFs; Lafferty et al., 2001) and include overlapping features. Our applications include diverse disambiguation frameworks and we make use of linguistically-inspired features, such as local lemma dependencies and inﬂectional agreement. We apply our model to Korean and Arabic, demonstrating state-of-theart results in both cases (§3). We then describe how our model can be expanded to complex, structured morphological tagging, including an efﬁcient estimation method, demonstrating performance on Czech (§4). 2 Modeling Framework  
Key phrases are usually among the most information-bearing linguistic structures. Translating them correctly will improve many natural language processing applications. We propose a new framework to mine key phrase translations from web corpora. We submit a source phrase to a search engine as a query, then expand queries by adding the translations of topic-relevant hint words from the returned snippets. We retrieve mixedlanguage web pages based on the expanded queries. Finally, we extract the key phrase translation from the secondround returned web page snippets with phonetic, semantic and frequencydistance features. We achieve 46% phrase translation accuracy when using top 10 returned snippets, and 80% accuracy with 165 snippets. Both results are significantly better than several existing methods. 
Traditional approaches to Information Extraction (IE) from speech input simply consist in applying text based methods to the output of an Automatic Speech Recognition (ASR) system. If it gives satisfaction with low Word Error Rate (WER) transcripts, we believe that a tighter integration of the IE and ASR modules can increase the IE performance in more difﬁcult conditions. More speciﬁcally this paper focuses on the robust extraction of Named Entities from speech input where a temporal mismatch between training and test corpora occurs. We describe a Named Entity Recognition (NER) system, developed within the French Rich Broadcast News Transcription program ESTER, which is speciﬁcally optimized to process ASR transcripts and can be integrated into the search process of the ASR modules. Finally we show how some metadata information can be collected in order to adapt NER and ASR models to new conditions and how they can be used in a task of Named Entity indexation of spoken archives. 
 We have studied how context specific web corpus can be automatically created and mined for discovering semantic similarity relationships between terms (words or phrases) from a given collection of documents (target collection). These relationships between terms can be used to adjust the standard vectors space representation so as to improve the accuracy of similarity computation between text documents in the target collection. Our experiments with a standard test collection  (Reuters) have revealed the reduction of similarity errors by up to 50%, twice as much as the improvement by using other  known techniques.  
We describe a new method for the representation of NLP structures within reranking approaches. We make use of a conditional log–linear model, with hidden variables representing the assignment of lexical items to word clusters or word senses. The model learns to automatically make these assignments based on a discriminative training criterion. Training and decoding with the model requires summing over an exponential number of hidden– variable assignments: the required summations can be computed efﬁciently and exactly using dynamic programming. As a case study, we apply the model to parse reranking. The model gives an F – measure improvement of ≈ 1.25% beyond the base parser, and an ≈ 0.25% improvement beyond the Collins (2000) reranker. Although our experiments are focused on parsing, the techniques described generalize naturally to NLP structures other than parse trees. 
German has a productive morphology and allows the creation of complex words which are often highly ambiguous. This paper reports on the development of a head-lexicalized PCFG for the disambiguation of German morphological analyses. The grammar is trained on unlabeled data using the Inside-Outside algorithm. The parser achieves a precision of more than 68% on difﬁcult test data, which is 23% more than the baseline obtained by randomly choosing one of the simplest analyses. Remarkable is the fact that precision drops to 52% without lexicalization.  • Abt (abbot) Ei (egg) Lunge (lung) n (plural inﬂectional ending) • Abt (abbot) ei (abbot → abbey) Lunge (lung) n (plural inﬂectional ending) • Abt (abbot) eil (hurry) ung (nominalization sufﬁx) en (plural inﬂectional ending) • Abtei (abbey) Lunge (lung) n (plural inﬂectional ending) • Abteilung (department) en (plural inﬂectional ending) • ab (separable verb preﬁx) teil (divide) ung (nominalization sufﬁx) en (plural inﬂectional ending)  
 We formalize weighted dependency parsing as searching for maximum spanning trees (MSTs) in directed graphs. Using this representation, the parsing algorithm of Eisner (1996) is sufﬁcient for searching over all projective trees in O(n3) time. More surprisingly, the representation is extended naturally to non-projective parsing using Chu-Liu-Edmonds (Chu and Liu, 1965; Edmonds, 1967) MST algorithm, yielding an O(n2) parsing algorithm. We evaluate these methods on the Prague Dependency Treebank using online large-margin learning techniques (Crammer et al., 2003; McDonald et al., 2005) and show that MST parsing increases efﬁciency and accuracy for languages with non-projective dependencies. 
Humor is one of the most interesting and puzzling aspects of human behavior. Despite the attention it has received in ﬁelds such as philosophy, linguistics, and psychology, there have been only few attempts to create computational models for humor recognition or generation. In this paper, we bring empirical evidence that computational approaches can be successfully applied to the task of humor recognition. Through experiments performed on very large data sets, we show that automatic classiﬁcation techniques can be effectively used to distinguish between humorous and non-humorous texts, with signiﬁcant improvements observed over apriori known baselines. 
While there have been many successful applications of machine learning methods to tasks in NLP, learning algorithms are not typically designed to optimize NLP performance metrics. This paper evaluates an ensemble selection framework designed to optimize arbitrary metrics and automate the process of algorithm selection and parameter tuning. We report the results of experiments that instantiate the framework for three NLP tasks, using six learning algorithms, a wide variety of parameterizations, and 15 performance metrics. Based on our results, we make recommendations for subsequent machine-learning-based research for natural language learning. 
We present a novel almost-unsupervised approach to the task of Word Sense Disambiguation (WSD). We build sense examples automatically, using large quantities of Chinese text, and English-Chinese and Chinese-English bilingual dictionaries, taking advantage of the observation that mappings between words and meanings are often different in typologically distant languages. We train a classiﬁer on the sense examples and test it on a gold standard English WSD dataset. The evaluation gives results that exceed previous state-of-the-art results for comparable systems. We also demonstrate that a little manual effort can improve the quality of sense examples, as measured by WSD accuracy. The performance of the classiﬁer on WSD also improves as the number of training sense examples increases. 
MONA is an automata toolkit providing a compiler for compiling formulae of monadic second order logic on strings or trees into string automata or tree automata. In this paper, we evaluate the option of using MONA as a treebank query tool. Unfortunately, we ﬁnd that MONA is not an option. There are several reasons why the main being unsustainable query answer times. If the treebank contains larger trees with more than 100 nodes, then even the processing of simple queries may take hours. 
Numerous NLP applications rely on search-engine queries, both to extract information from and to compute statistics over the Web corpus. But search engines often limit the number of available queries. As a result, query-intensive NLP applications such as Information Extraction (IE) distribute their query load over several days, making IE a slow, offline process. This paper introduces a novel architecture for IE that obviates queries to commercial search engines. The architecture is embodied in a system called KNOWITNOW that performs high-precision IE in minutes instead of days. We compare KNOWITNOW experimentally with the previouslypublished KNOWITALL system, and quantify the tradeoff between recall and speed. KNOWITNOW’s extraction rate is two to three orders of magnitude higher than KNOWITALL’s. 
In the past decade, several researchers have started reinvestigating the use of sub-phonetic models for lexical representations within automatic speech recognition systems. Lest history repeat itself, it may be instructive to mine the further past for models of lexical representations in the lexical access literature. In this work, we re-evaluate the model of Briscoe (1989), in which a hybrid strategy of lexical representation between phones and manner classes is promoted. While many of Briscoe’s assumptions do not match up with current ASR processing models, we show that his conclusions are essentially correct, and that reconsidering this structure for ASR lexica is an appropriate avenue for future ASR research. 
In addition to information, text contains attitudinal, and more speciﬁcally, emotional content. This paper explores the text-based emotion prediction problem empirically, using supervised machine learning with the SNoW learning architecture. The goal is to classify the emotional afﬁnity of sentences in the narrative domain of children’s fairy tales, for subsequent usage in appropriate expressive rendering of text-to-speech synthesis. Initial experiments on a preliminary data set of 22 fairy tales show encouraging results over a na¨ıve baseline and BOW approach for classiﬁcation of emotional versus non-emotional contents, with some dependency on parameter tuning. We also discuss results for a tripartite model which covers emotional valence, as well as feature set alternations. In addition, we present plans for a more cognitively sound sequential model, taking into consideration a larger set of basic emotions. 
This paper studies how to go beyond relevance and enable a ﬁltering system to learn more interesting and detailed data driven user models from multiple forms of evidence. We carry out a user study using a real time web based personal news ﬁltering system, and collect extensive multiple forms of evidence, including explicit and implicit user feedback. We explore the graphical modeling approach to combine these forms of evidence. To test whether the approach can help us understand the domain better, we use graph structure learning algorithm to derive the causal relationships between different forms of evidence. To test whether the approach can help the system improve the performance, we use the graphical inference algorithms to predict whether a user likes a document based on multiple forms of evidence. The results show that combining multiple forms of evidence using graphical models can help us better understand the ﬁltering problem, improve ﬁltering system performance, and handle various data missing situations naturally. 
Traditional question answering systems adopt the following framework: parsing questions, searching for relevant documents, and identifying/generating answers. However, this framework does not work well for questions with hidden assumptions and implicatures. In this paper, we describe a novel idea, a cascading guidance strategy, which can not only identify potential traps in questions but further guide the answer extraction procedure by recognizing whether there are multiple answers for a question. This is the first attempt to solve implicature problem for complex QA in a cascading fashion using N-gram language models as features. We here investigate questions with implicatures related to biography facts in a web-based QA system, PowerBio. We compare the performances of Decision Tree, Naïve Bayes, SVM (Support Vector Machine), and ME (Maximum Entropy) classification methods. The integration of the cascading guidance strategy can help extract answers for questions with implicatures and produce satisfactory results in our experiments. 
A reading comprehension (RC) system attempts to understand a document and returns an answer sentence when posed with a question. RC resembles the ad hoc question answering (QA) task that aims to extract an answer from a collection of documents when posed with a question. However, since RC focuses only on a single document, the system needs to draw upon external knowledge sources to achieve deep analysis of passage sentences for answer sentence extraction. This paper proposes an approach towards RC that attempts to utilize external knowledge to improve performance beyond the baseline set by the bag-of-words (BOW) approach. Our approach emphasizes matching of metadata (i.e. verbs, named entities and base noun phrases) in passage context utilization and answer sentence extraction. We have also devised an automatic acquisition process for Web-derived answer patterns (AP) which utilizes question-answer pairs from TREC QA, the Google search engine and the Web. This approach gave improved RC performances for both the Remedia and ChungHwa corpora, attaining HumSent accuracies of 42% and 69% respectively. In particular, performance analysis based on Remedia shows that relative performances of 20.7% is due to metadata matching and a further 10.9% is due to the application of Web-derived answer patterns. 1. Introduction A reading comprehension (RC) system attempts to understand a document and returns an answer sentence when posed with a question. The RC  task was first proposed by the MITRE Corporation which developed the Deep Read reading comprehension system (Hirschman et al., 1999). Deep Read was evaluated on the Remedia Corpus that contains a set of stories, each with an average of 20 sentences and five questions (of types who, where, when, what and why). The MITRE group also defined the HumSent scoring metric, i.e. the percentage of test questions for which the system has chosen a correct sentence as the answer. HumSent answers were compiled by a human annotator, who examined the stories and chose the sentence(s) that best answered the questions. It was judged that for 11% of the Remedia test questions, there is no single sentence in the story that is judged to be an appropriate answer sentence. Hence the upper bound for RC on Remedia should by 89% HumSent accuracy. (Hirschman et al. 1999) reported a HumSent accuracy of 36.6% on the Remedia test set. Subsequently, (Ng et al., 2000) used a machine learning approach of decision tree and achieved the accuracy of 39.3%. Then (Riloff and Thelen, 2000) and (Charniak et al., 2000) reported improvements to 39.7% and 41%, respectively. They made use of handcrafted heuristics such as the WHEN rule: if contain(S, TIME), then Score(S)+=4 i.e. WHEN questions reward candidate answer sentences with four extra points if they contain a name entity TIME. RC resembles the ad hoc question answering (QA) task in TREC.1 The QA task finds answers to a set of questions from a collection of documents, while RC focuses on a single 
This paper uses human verb associations as the basis for an investigation of verb properties, focusing on semantic verb relations and prominent nominal features. First, the lexical semantic taxonymy GermaNet is checked on the types of classic semantic relations in our data; verbverb pairs not covered by GermaNet can help to detect missing links in the taxonomy, and provide a useful basis for deﬁning non-classical relations. Second, a statistical grammar is used for determining the conceptual roles of the noun responses. We present prominent syntaxsemantic roles and evidence for the usefulness of co-occurrence information in distributional verb descriptions. 
In this paper, we extend an existing parser to produce richer output annotated with function labels. We obtain state-of-the-art results both in function labelling and in parsing, by automatically relabelling the Penn Treebank trees. In particular, we obtain the best published results on semantic function labels. This suggests that current statistical parsing methods are sufﬁciently general to produce accurate shallow semantic annotation.  S  ¨¨rr  ¨  r  ¨  r  ¨  r  NP-S¨BJ  rVP   the authority  dd      VBD  PP-TMP  NPd-TMP  PP-DIR  dropped IN(T¨M¨Pr)rNP NNP(TMP) TO(D¨¨IR)rrNP  at NN Tuesday midnight  to QP $ 2.8 trillion  Figure 1: A sample syntactic structure with function labels.  
We use logical inference techniques for recognising textual entailment. As the performance of theorem proving turns out to be highly dependent on not readily available background knowledge, we incorporate model building, a technique borrowed from automated reasoning, and show that it is a useful robust method to approximate entailment. Finally, we use machine learning to combine these deep semantic analysis techniques with simple shallow word overlap; the resulting hybrid model achieves high accuracy on the RTE testset, given the state of the art. Our results also show that the different techniques that we employ perform very differently on some of the subsets of the RTE corpus and as a result, it is useful to use the nature of the dataset as a feature. 
Accurate lemmatization of German nouns mandates the use of a lexicon. Comprehensive lexicons, however, are expensive to build and maintain. We present a selflearning lemmatizer capable of automatically creating a full-form lexicon by processing German documents. 
This paper reports a preliminary study addressing two challenges in measuring the effectiveness of information extraction (IE) technology: • Developing a methodology for ex- trinsic evaluation of IE; and, • Estimating the impact of improving IE technology on the ability to perform an application task. The methodology described can be employed for further controlled experiments regarding information extraction. 
In this paper, we study the impact of a group of features extracted automatically from machine-generated parse trees on coreference resolution. One focus is on designing syntactic features using the binding theory as the guideline to improve pronoun resolution, although linguistic phenomenon such as apposition is also modeled. These features are applied to the Arabic, Chinese and English coreference resolution systems and their effectiveness is evaluated on data from the Automatic Content Extraction (ACE) task. The syntactic features improve the Arabic and English systems signiﬁcantly, but play a limited role in the Chinese one. Detailed analyses are done to understand the syntactic features’ impact on the three coreference systems. 
We analyze models for semantic role assignment by deﬁning a meta-model that abstracts over features and learning paradigms. This meta-model is based on the concept of role confusability, is deﬁned in information-theoretic terms, and predicts that roles realized by less speciﬁc grammatical functions are more difﬁcult to assign. We ﬁnd that confusability is strongly correlated with the performance of classiﬁers based on syntactic features, but not for classiﬁers including semantic features. This indicates that syntactic features approximate a description of grammatical functions, and that semantic features provide an independent second view on the data. 
In statistical machine translation, estimating word-to-word alignment probabilities for the translation model can be diﬃcult due to the problem of sparse data: most words in a given corpus occur at most a handful of times. With a highly inﬂected language such as Czech, this problem can be particularly severe. In addition, much of the morphological variation seen in Czech words is not reﬂected in either the morphology or syntax of a language like English. In this work, we show that using morphological analysis to modify the Czech input can improve a Czech-English machine translation system. We investigate several diﬀerent methods of incorporating morphological information, and show that a system that combines these methods yields the best results. Our ﬁnal system achieves a BLEU score of .333, as compared to .270 for the baseline word-to-word system.  
In this work we propose a translation model for monolingual sentence retrieval. We propose four methods for constructing a parallel corpus. Of the four methods proposed, a lexicon learned from a bilingual ArabicEnglish corpus aligned at the sentence level performs best, signiﬁcantly improving results over the query likelihood baseline. Further, we demonstrate that smoothing from the local context of the sentence improves retrieval over the query likelihood baseline. 
We consider the problem of training logistic regression models for binary classiﬁcation in information extraction and information retrieval tasks. Fitting probabilistic models for use with such tasks should take into account the demands of the taskspeciﬁc utility function, in this case the well-known F-measure, which combines recall and precision into a global measure of utility. We develop a training procedure based on empirical risk minimization / utility maximization and evaluate it on a simple extraction task. 
We present Evita, an application for recognizing events in natural language texts. Although developed as part of a suite of tools aimed at providing question answering systems with information about both temporal and intensional relations among events, it can be used independently as an event extraction tool. It is unique in that it is not limited to any pre-established list of relation types (events), nor is it restricted to a speciﬁc domain. Evita performs the identiﬁcation and tagging of event expressions based on fairly simple strategies, informed by both linguisticand statistically-based data. It achieves a performance ratio of 80.12% F-measure.1 
We should not have to look at the entire corpus (e.g., the Web) to know if two words are associated or not.1 A powerful sampling technique called Sketches was originally introduced to remove duplicate Web pages. We generalize sketches to estimate contingency tables and associations, using a maximum likelihood estimator to ﬁnd the most likely contingency table given the sample, the margins (document frequencies) and the size of the collection. Not unsurprisingly, computational work and statistical accuracy (variance or errors) depend on sampling rate, as will be shown both theoretically and empirically. Sampling methods become more and more important with larger and larger collections. At Web scale, sampling rates as low as 10−4 may sufﬁce. 
We demonstrate the value of using context in a new-information detection system that achieved the highest precision scores at the Text Retrieval Conference’s Novelty Track in 2004. In order to determine whether information within a sentence has been seen in material read previously, our system integrates information about the context of the sentence with novel words and named entities within the sentence, and uses a specialized learning algorithm to tune the system parameters. 
We present a novel approach to relation extraction, based on the observation that the information required to assert a relationship between two named entities in the same sentence is typically captured by the shortest path between the two entities in the dependency graph. Experiments on extracting top-level relations from the ACE (Automated Content Extraction) newspaper corpus show that the new shortest path dependency kernel outperforms a recent approach based on dependency tree kernels. 
We address the problem of multi-way relation classiﬁcation, applied to identiﬁcation of the interactions between proteins in bioscience text. A major impediment to such work is the acquisition of appropriately labeled training data; for our experiments we have identiﬁed a database that serves as a proxy for training data. We use two graphical models and a neural net for the classiﬁcation of the interactions, achieving an accuracy of 64% for a 10-way distinction between relation types. We also provide evidence that the exploitation of the sentences surrounding a citation to a paper can yield higher accuracy than other sentences. 
We introduce BLANC, a family of dynamic, trainable evaluation metrics for machine translation. Flexible, parametrized models can be learned from past data and automatically optimized to correlate well with human judgments for different criteria (e.g. adequacy, ﬂuency) using different correlation measures. Towards this end, we discuss ACS (all common skipngrams), a practical algorithm with trainable parameters that estimates referencecandidate translation overlap by computing a weighted sum of all common skipngrams in polynomial time. We show that the BLEU and ROUGE metric families are special cases of BLANC, and we compare correlations with human judgments across these three metric families. We analyze the algorithmic complexity of ACS and argue that it is more powerful in modeling both local meaning and sentence-level structure, while offering the same practicality as the established algorithms it generalizes. 
Many learning tasks have subtasks for which much training data exists. Therefore, we want to transfer learning from the old, generalpurpose subtask to a more speciﬁc new task, for which there is often less data. While work in transfer learning often considers how the old task should affect learning on the new task, in this paper we show that it helps to take into account how the new task affects the old. Speciﬁcally, we perform joint decoding of separately-trained sequence models, preserving uncertainty between the tasks and allowing information from the new task to affect predictions on the old task. On two standard text data sets, we show that joint decoding outperforms cascaded decoding. 
This paper presents a phrase-based statistical machine translation method, based on non-contiguous phrases, i.e. phrases with gaps. A method for producing such phrases from a word-aligned corpora is proposed. A statistical translation model is also presented that deals such phrases, as well as a training method based on the maximization of translation accuracy, as measured with the NIST evaluation metric. Translations are produced by means of a beam-search decoder. Experimental results are presented, that demonstrate how the proposed method allows to better generalize from the training data. 
Conﬁdence measures for machine translation is a method for labeling each word in an automatically generated translation as correct or incorrect. In this paper, we will present a new approach to conﬁdence estimation which has the advantage that it does not rely on system output such as N best lists or word graphs as many other conﬁdence measures do. It is, thus, applicable to any kind of machine translation system. Experimental evaluation has been performed on translation of technical manuals in three different language pairs. Results will be presented for different machine translation systems to show that the new approach is independent of the underlying machine translation system which generated the translations. To the best of our knowledge, the performance of the new conﬁdence measure is better than that of any existing conﬁdence measure. 
In word sense disambiguation, a system attempts to determine the sense of a word from contextual features. Major barriers to building a high-performing word sense disambiguation system include the difﬁculty of labeling data for this task and of predicting ﬁne-grained sense distinctions. These issues stem partly from the fact that the task is being treated in isolation from possible uses of automatically disambiguated data. In this paper, we consider the related task of word translation, where we wish to determine the correct translation of a word from context. We can use parallel language corpora as a large supply of partially labeled data for this task. We present algorithms for solving the word translation problem and demonstrate a significant improvement over a baseline system. We then show that the word-translation system can be used to improve performance on a simpliﬁed machinetranslation task and can effectively and accurately prune the set of candidate translations for a word. 
Hierarchical organization is a well known property of language, and yet the notion of hierarchical structure has been largely absent from the best performing machine translation systems in recent community-wide evaluations. In this paper, we discuss a new hierarchical phrase-based statistical machine translation system (Chiang, 2005), presenting recent extensions to the original proposal, new evaluation results in a community-wide evaluation, and a novel technique for ﬁne-grained comparative analysis of MT systems. 
In this paper, we look at comparing highaccuracy context-free parsers with highaccuracy ﬁnite-state (shallow) parsers on several shallow parsing tasks. We show that previously reported comparisons greatly under-estimated the performance of context-free parsers for these tasks. We also demonstrate that contextfree parsers can train effectively on relatively little training data, and are more robust to domain shift for shallow parsing tasks than has been previously reported. Finally, we establish that combining the output of context-free and ﬁnitestate parsers gives much higher results than the previous-best published results, on several common tasks. While the efﬁciency beneﬁt of ﬁnite-state models is inarguable, the results presented here show that the corresponding cost in accuracy is higher than previously thought. 
We present two methods for incorporating detailed features in a Spanish parser, building on a baseline model that is a lexicalized PCFG. The ﬁrst method exploits Spanish morphology, and achieves an F1 constituency score of 83.6%. This is an improvement over 81.2% accuracy for the baseline, which makes little or no use of morphological information. The second model uses a reranking approach to add arbitrary global features of parse trees to the morphological model. The reranking model reaches 85.1% F1 accuracy on the Spanish parsing task. The resulting model for Spanish parsing combines an approach that speciﬁcally targets morphological information with an approach that makes use of general structural features. 
This paper investigates some computational problems associated with probabilistic translation models that have recently been adopted in the literature on machine translation. These models can be viewed as pairs of probabilistic contextfree grammars working in a ‘synchronous’ way. Two hardness results for the class NP are reported, along with an exponential time lower-bound for certain classes of algorithms that are currently used in the literature. 
We present a very efﬁcient statistical incremental parser for LTAG-spinal, a variant of LTAG. The parser supports the full adjoining operation, dynamic predicate coordination, and non-projective dependencies, with a formalism of provably stronger generative capacity as compared to CFG. Using gold standard POS tags as input, on section 23 of the PTB, the parser achieves an f-score of 89.3% for syntactic dependency deﬁned on LTAG derivation trees, which are deeper than the dependencies extracted from PTB alone with head rules (for example, in Magerman’s style).  
In the REAP system, users are automatically provided with texts to read targeted to their individual reading levels. To find appropriate texts, the user’s vocabulary knowledge must be assessed. We describe an approach to automatically generating questions for vocabulary assessment. Traditionally, these assessments have been hand-written. Using data from WordNet, we generate 6 types of vocabulary questions. They can have several forms, including wordbank and multiple-choice. We present experimental results that suggest that these automatically-generated questions give a measure of vocabulary skill that correlates well with subject performance on independently developed humanwritten questions. In addition, strong correlations with standardized vocabulary tests point to the validity of our approach to automatic assessment of word knowledge. 
Experimental research in psycholinguistics has demonstrated a parallelism effect in coordination: speakers are faster at processing the second conjunct of a coordinate structure if it has the same internal structure as the ﬁrst conjunct. We show that this phenomenon can be explained by the prevalence of parallel structures in corpus data. We demonstrate that parallelism is not limited to coordination, but also applies to arbitrary syntactic conﬁgurations, and even to documents. This indicates that the parallelism effect is an instance of a general syntactic priming mechanism in human language processing.  
Recent work has shown that very large corpora can act as training data for NLP algorithms even without explicit labels. In this paper we show how the use of surface features and paraphrases in queries against search engines can be used to infer labels for structural ambiguity resolution tasks. Using unsupervised algorithms, we achieve 84% precision on PP-attachment and 80% on noun compound coordination. 
We here propose a new method which sets apart domain-speciﬁc terminology from common non-speciﬁc noun phrases. It is based on the observation that terminological multi-word groups reveal a considerably lesser degree of distributional variation than non-speciﬁc noun phrases. We deﬁne a measure for the observable amount of paradigmatic modiﬁability of terms and, subsequently, test it on bigram, trigram and quadgram noun phrases extracted from a 104-million-word biomedical text corpus. Using a community-wide curated biomedical terminology system as an evaluation gold standard, we show that our algorithm signiﬁcantly outperforms a variety of standard term identiﬁcation measures. We also provide empirical evidence that our methodolgy is essentially domain- and corpus-size-independent. 
The lack of annotated data is an obstacle to the development of many natural language processing applications; the problem is especially severe when the data is non-English. Previous studies suggested the possibility of acquiring resources for non-English languages by bootstrapping from high quality English NLP tools and parallel corpora; however, the success of these approaches seems limited for dissimilar language pairs. In this paper, we propose a novel approach of combining a bootstrapped resource with a small amount of manually annotated data. We compare the proposed approach with other bootstrapping methods in the context of training a Chinese Part-of-Speech tagger. Experimental results show that our proposed approach achieves a signiﬁcant improvement over EM and self-training and systems that are only trained on manual annotations. 
 Frame: DEPARTING  This paper considers the problem of automatically inducing role-semantic annotations in the FrameNet paradigm for new languages. We introduce a general framework for semantic projection which exploits parallel texts, is relatively inexpensive and can potentially reduce the amount of effort involved in creating semantic resources. We propose projection models that exploit lexical and syntactic information. Experimental results on an EnglishGerman parallel corpus demonstrate the advantages of this approach. 
We present a lexicon-free post-processing method for optical character recognition (OCR), implemented using weighted ﬁnite state machines. We evaluate the technique in a number of scenarios relevant for natural language processing, including creation of new OCR capabilities for low density languages, improvement of OCR performance for a native commercial system, acquisition of knowledge from a foreign-language dictionary, creation of a parallel text, and machine translation from OCR output. 
Dictionaries and word translation models are used by a variety of systems, especially in machine translation. We build a multilingual dictionary induction system for a family of related resource-poor languages. We assume only the presence of a single medium-length multitext (the Bible). The techniques rely upon lexical and syntactic similarity of languages as well as on the fact that building dictionaries for several pairs of languages provides information about other pairs. 
We develop an unsupervised semantic role labelling system that relies on the direct application of information in a predicate lexicon combined with a simple probability model. We demonstrate the usefulness of predicate lexicons for role labelling, as well as the feasibility of modifying an existing role-labelled corpus for evaluating a different set of semantic roles. We achieve a substantial improvement over an informed baseline. 
This paper addresses the automatic classiﬁcation of the semantic relations expressed by the English genitives. A learning model is introduced based on the statistical analysis of the distribution of genitives’ semantic relations on a large corpus. The semantic and contextual features of the genitive’s noun phrase constituents play a key role in the identiﬁcation of the semantic relation. The algorithm was tested on a corpus of approximately 2,000 sentences and achieved an accuracy of 79% , far better than 44% accuracy obtained with C5.0, or 43% obtained with a Naive Bayes algorithm, or 27% accuracy with a Support Vector Machines learner on the same corpus. 
Measuring the relative compositionality of Multi-word Expressions (MWEs) is crucial to Natural Language Processing. Various collocation based measures have been proposed to compute the relative compositionality of MWEs. In this paper, we deﬁne novel measures (both collocation based and context based measures) to measure the relative compositionality of MWEs of V-N type. We show that the correlation of these features with the human ranking is much superior to the correlation of the traditional features with the human ranking. We then integrate the proposed features and the traditional features using a SVM based ranking function to rank the collocations of V-N type based on their relative compositionality. We then show that the correlation between the ranks computed by the SVM based ranking function and human ranking is significantly better than the correlation between ranking of individual features and human ranking. 
In this paper we investigate an application of feature clustering for word sense disambiguation, and propose a semisupervised feature clustering algorithm. Compared with other feature clustering methods (ex. supervised feature clustering), it can infer the distribution of class labels over (unseen) features unavailable in training data (labeled data) by the use of the distribution of class labels over (seen) features available in training data. Thus, it can deal with both seen and unseen features in feature clustering process. Our experimental results show that feature clustering can aggressively reduce the dimensionality of feature space, while still maintaining state of the art sense disambiguation accuracy. Furthermore, when combined with a semi-supervised WSD algorithm, semi-supervised feature clustering outperforms other dimensionality reduction techniques, which indicates that using unlabeled data in learning process helps to improve the performance of feature clustering and sense disambiguation. 
We consider the problem of questionfocused sentence retrieval from complex news articles describing multi-event stories published over time. Annotators generated a list of questions central to understanding each story in our corpus. Because of the dynamic nature of the stories, many questions are time-sensitive (e.g. “How many victims have been found?”) Judges found sentences providing an answer to each question. To address the sentence retrieval problem, we apply a stochastic, graph-based method for comparing the relative importance of the textual units, which was previously used successfully for generic summarization. Currently, we present a topic-sensitive version of our method and hypothesize that it can outperform a competitive baseline, which compares the similarity of each sentence to the input question via IDF-weighted word overlap. In our experiments, the method achieves a TRDR score that is signiﬁcantly higher than that of the baseline. 
We investigate techniques to support the answering of opinion-based questions. We ﬁrst present the OpQA corpus of opinion questions and answers. Using the corpus, we compare and contrast the properties of fact and opinion questions and answers. Based on the disparate characteristics of opinion vs. fact answers, we argue that traditional fact-based QA approaches may have difﬁculty in an MPQA setting without modiﬁcation. As an initial step towards the development of MPQA systems, we investigate the use of machine learning and rule-based subjectivity and opinion source ﬁlters and show that they can be used to guide MPQA systems. 
Following recent developments in the automatic evaluation of machine translation and document summarization, we present a similar approach, implemented in a measure called POURPRE, for automatically evaluating answers to deﬁnition questions. Until now, the only way to assess the correctness of answers to such questions involves manual determination of whether an information nugget appears in a system’s response. The lack of automatic methods for scoring system output is an impediment to progress in the ﬁeld, which we address with this work. Experiments with the TREC 2003 and TREC 2004 QA tracks indicate that rankings produced by our metric correlate highly with ofﬁcial rankings, and that POURPRE outperforms direct application of existing metrics. 
In this paper we investigate the use of linguistic knowledge in passage retrieval as part of an open-domain question answering system. We use annotation produced by a deep syntactic dependency parser for Dutch, Alpino, to extract various kinds of linguistic features and syntactic units to be included in a multi-layer index. Similar annotation is produced for natural language questions to be answered by the system. From this we extract query terms to be sent to the enriched retrieval index. We use a genetic algorithm to optimize the selection of features and syntactic units to be included in a query. This algorithm is also used to optimize further parameters such as keyword weights. The system is trained on questions from the competition on Dutch question answering within the Cross-Language Evaluation Forum (CLEF). We could show an improvement of about 15% in mean total reciprocal rank compared to traditional information retrieval using plain text keywords (including stemming and stop word removal). 
MIT’s Audio Notebook added great value to the note-taking process by retaining audio recordings, e.g. during lectures or interviews. The key was to provide users ways to quickly and easily access portions of interest in a recording. Several non-speech-recognition based techniques were employed. In this paper we present a system to search directly the audio recordings by key phrases. We have identiﬁed the user requirements as accurate ranking of phrase matches, domain independence, and reasonable response time. We address these requirements by a hybrid word/phoneme search in lattices, and a supporting indexing scheme. We will introduce the ranking criterion, a uniﬁed hybrid posterior-lattice representation, and the indexing algorithm for hybrid lattices. We present results for ﬁve different recording sets, including meetings, telephone conversations, and interviews. Our results show an average search accuracy of 84%, which is dramatically better than a direct search in speech recognition transcripts (less than 40% search accuracy). 
Applying the noisy channel model to search query spelling correction requires an error model and a language model. Typically, the error model relies on a weighted string edit distance measure. The weights can be learned from pairs of misspelled words and their corrections. This paper investigates using the Expectation Maximization algorithm to learn edit distance weights directly from search query logs, without relying on a corpus of paired words. 
Query expansion techniques generally select new query terms from a set of top ranked documents. Although a user’s manual judgment of those documents would much help to select good expansion terms, it is difﬁcult to get enough feedback from users in practical situations. In this paper we propose a query expansion technique which performs well even if a user notiﬁes just a relevant document and a non-relevant document. In order to tackle this speciﬁc condition, we introduce two reﬁnements to a well-known query expansion technique. One is application of a transductive learning technique in order to increase relevant documents. The other is a modiﬁed parameter estimation method which laps the predictions by multiple learning trials and try to differentiate the importance of candidate terms for expansion in relevant documents. Experimental results show that our technique outperforms some traditional query expansion methods in several evaluation measures. 
This paper explores the segmentation of tutorial dialogue into cohesive topics. A latent semantic space was created using conversations from human to human tutoring transcripts, allowing cohesion between utterances to be measured using vector similarity. Previous cohesionbased segmentation methods that focus on expository monologue are reapplied to these dialogues to create benchmarks for performance. A novel moving window technique using orthonormal bases of semantic vectors significantly outperforms these benchmarks on this dialogue segmentation task. 
This work addresses the task of identifying thematic correspondences across subcorpora focused on different topics. We introduce an unsupervised algorithmic framework based on distributional data clustering, which generalizes previous initial works on this task. The empirical results reveal interesting commonalities of different religions. We evaluate the results through measuring the overlap of our clusters with clusters compiled manually by experts. The tested variants of our framework are shown to outperform alternative methods applicable to the task. 
Many language processing tasks can be reduced to breaking the text into segments with prescribed properties. Such tasks include sentence splitting, tokenization, named-entity extraction, and chunking. We present a new model of text segmentation based on ideas from multilabel classiﬁcation. Using this model, we can naturally represent segmentation problems involving overlapping and non-contiguous segments. We evaluate the model on entity extraction and noun-phrase chunking and show that it is more accurate for overlapping and non-contiguous segments, but it still performs well on simpler data sets for which sequential tagging has been the best method. 
We present a novel voice-based humancomputer interface designed to enable individuals with motor impairments to use vocal parameters for continuous control tasks. Since discrete spoken commands are ill-suited to such tasks, our interface exploits a large set of continuous acousticphonetic parameters like pitch, loudness, vowel quality, etc. Their selection is optimized with respect to automatic recognizability, communication bandwidth, learnability, suitability, and ease of use. Parameters are extracted in real time, transformed via adaptation and acceleration, and converted into continuous control signals. This paper describes the basic engine, prototype applications (in particular, voice-based web browsing and a controlled trajectory-following task), and initial user studies conﬁrming the feasibility of this technology. 
This paper addresses a dialogue strategy to clarify and constrain the queries for speech-driven document retrieval systems. In spoken dialogue interfaces, users often make utterances before the query is completely generated in their mind; thus input queries are often vague or fragmental. As a result, usually many items are matched. We propose an efﬁcient dialogue framework, where the system dynamically selects an optimal question based on information gain (IG), which represents reduction of matched items. A set of possible questions is prepared using various knowledge sources. As a bottom-up knowledge source, we extract a list of words that can take a number of objects and potentially causes ambiguity, using a dependency structure analysis of the document texts. This is complemented by top-down knowledge sources of metadata and handcrafted questions. An experimental evaluation showed that the method signiﬁcantly improved the success rate of retrieval, and all categories of the prepared questions contributed to the improvement. 
This paper describes an application of reinforcement learning to determine a dialog policy for a complex collaborative task where policies for both the system and a proxy for a user of the system are learned simultaneously. With this approach a useful dialog policy is learned without the drawbacks of other approaches that require signiﬁcant human interaction. The speciﬁc task that the agents were trained on was chosen for its complexity and requirement that both conversants bring task knowledge to the interaction, thus ensuring its collaborative nature. The results of our experiment show that you can use reinforcement learning to create an effective dialog policy, which employs a mixed initiative strategy, without the drawbacks of large amounts of data or signiﬁcant human input. 
In this paper we argue that Translation Intelligence is the next generation of translation memory technology that supersedes current state-of-the-art translation memory systems, as it is based on real self-learning and intelligent reuse of human translation knowledge, instead of simply attempting to recycle strings of characters, as traditional systems do. We maintain that Translation Intelligence is the only cost-effective method for professional computer-aided translation that is usable by both professional translators and other professionals with frequent translation needs besides their main tasks. It has been shown that due to drawbacks in their techniques, current translation memories have only been able to reach a mere fragment of these wide customer groups. 1. Translation Market Today: Some Figures Estimates on the size of today’s worldwide translation and localization services market vary from USD 4 billion to 15 billion1, with the US and Europe as the main areas of business (approximately 40 % market share each)2. However, the market size of computer-aided translation (CAT) tools, especially translation memories, is noticeably lower. Estimates vary considerably, from USD 22 million3 to 700 million4, but nevertheless, the figures illustrate that the lion’s share of translations is still done without any real computerized means. As the translation need is expected to grow steadily, there is gigantic market potential for the provider of the right kind of translation tool that can be taken into use quickly, with very little tailoring or none at all. Clearly, today’s providers of traditional translation memory systems have not been able to meet this need that professional translators and other professionals translating beside their main work tasks have. These user groups are big, in Finland alone there 
 ep@lingua-et-machina.com www.lingua-et-machina.com  This paper explains how we see the Computer Aided Translation (CAT) new world, introducing Second Generation Translation memories (2gTM).  2gTM use a light linguistic analysis so that the CAT tool is able to deal with the translation of noun phrases and verb phrases rather than complete sentences. This mere feature opens wide the scope of translation memories from 20% up to 80% of the documents that are daily translated in the World.  1. Introduction In the old times, when the Translation Pangea was wild and hot, in the territory of Tradosaurius, Esdiëlorius and some other Wordfastorius, text documents where considered as a sequence of characters. Ibeëmodocus was already extinct. The horde of translators had to struggle hard for their life while they were to find their path between fuzzy sentences and other 101% matches. A new species, Atrilax (maybe you've seen this before?) introduced the notion of “composed translation”, trying to put together patches of previously recorded segments in order to compose a new segment. The attempt was quite “atrevido”, but life was still hard. We propose you a new translation world, where the text is considered as a sequence of words that have a grammatical reality. This new paradigm offers brand new translation automation that we are happy to explain further on. In chapter 2, we show why First Generation Translation Memories (1gTM) loose an important part of the redundancy of the translated documents. Chapter 3 reminds how men and machine see the document text. Chapter 4 explains shows how Second Generation Translation memories (2gTM) can deal with sub-sentence redundancy. Chapter 5 is a presentation of SIMILIS interface and Chapter 6 goes on with the conclusion and perspectives.  2. Step back to first generation translation memories  2.1 A typical translator journey First Generation Translation Memories (1gTM) collects a series of pairs of sentences (the source and its translation - the target) in a database. It proposes to the translator the pair that is the more similar to the sentence he wants to translate. You all know about the story. Yet, let me take a small example. Here is a sentence to be translated from French to English:  Lingua et Machina présente les mémoires de seconde génération Now let us suppose that the only sentence pair you have in the memory is: Les mémoires de seconde génération changent le monde de la traduction. Second generation memories change the translation world. A good 1gTM will propose you this only pair with, say a 56% match for you can find in both source sentences “les mémoires de seconde generationˮ. Remark 1: In a real translation world, the translator would set the fuzzy match threshold to 75% minimum. The only sentence pair of the memory would not even been proposed by the 1gTM tool because it is below the fuzzy match threshold. Now suppose that the translator really wants to use every proposal from the memory. It has set the threshold to 40% and struggles hard! He presses down the correct short-cut that gives him without waiting more precious seconds the following sentence: Second generation memories change the translation world. At this point, please note these two other important points: Remark 2: In this target sentence, the 1gTM tool does not show the translator the part that is common or not with the sentence that he wants to write (in the target sentence), in order to translate the sentences he has to translate. The translator would just loose time right here identifying the part to be changed. Remark 3: In order to use this poor fuzzy match, he will have to erase 45% of the sentence. He would loose time again. 2.2 The consequence in terms of text re-use Here is an extract from a text transcription of a European Parliament session you can find on the web site of the European Parliament. As other sessions, this session has been translated into all European languages. This text is an example of non technical text that is commonly seen as non redundant (0,5% of the sentences are reused). I have underlined some noun groups and verb groups that could be reused further on the session. I have double underlined the groups that are actually reused in the same text. One can see that even in this same small bunch of text, the reusable groups already represent some interesting percentage of the text. Duisenberg, President of the European Central Bank. - It is my great pleasure today to present the third Annual Report of the ECB, two and a half years after the launch of the euro and less than half a year before the single currency will become fully visible for our citizens in the form of banknotes and coins. The year 2000 was a remarkable year for the euro area, as economic growth reached its highest level for over a decade and was accompanied by continued strong job creation. The HICP in the euro area has also, alas, been above 2% since the middle of 2000, mainly owing to oil price increases and the depreciation of the exchange rate of the euro last year. While this increase in the HICP to above the ECB's definition of price stability_is not welcome, the ECB cannot and should not avoid short-term price fluctuations caused by such temporary factors. Nonetheless, it is crucial to prevent any spillover of transitory short-term pressures into medium-term inflation expectations. During 2000 the ECB had to be particularly vigilant in  this regard in a context of strong economic growth and given that monetary developments clearly indicated the existence of upward risks to price stability. It was for this reason that we raised interest rates six times in 2000. By doing so, the ECB contributed to ensuring the sustainability of non-inflationary economic growth in the euro area.  On the other hand, let us now consider a technical text that is supposed to be redundant. I have found this text on the help file of my computer operating system :  To connect a printer directly to your computer  Most new printers support Plug and Play, while many older printers do not. The steps  involved in installing a printer that is attached to your computer differ depending on whether  it supports Plug and Play.  1. Click one of the following links:  • My printer supports Plug and Play.  • My printer does not support Plug and Play.  If you are unsure whether your printer supports Plug and Play, consider the following:  •  Does your printer use infrared technology? If it does, your printer supports Plug and  Play.  •  Consult the owner's manual or packaging of your printer. Most printer manufacturers  advertise the fact that their printer supports Plug and Play. Look for Plus and Play on the  printer's list of features.  •  Check the connector on the end of the printer cable that you plug into your computer.  •  If the connector that attaches the printer cable to the computer is a USB connector,  then the printer supports Plug and Play.  In terms of sentence fuzzy matches, one can easily see that two sentences are similar: • My printer supports Plug and Play. • My printer does not support Plug and Play. We were expecting this fact since this is a technical text. Nevertheless, there are only two sentences that can be fuzzy reused, while 67 words out of 170 (39%) belong to a group that is redundant in this mere text! Just imagine the kind of redundancy you can get with the entire help file, reusing word groups...  The corollary of this small demonstration is two folded: • First, 1gTM loose an important part of the text redundancy and consistency when reusing only sentences • Second, reusing noun and verb phrases expand the type of documents on which Translation Memories can apply.  3. What is new with second generation translation memories  3.1. Texts as perceived by humans and machines In La structure des langues1, Claude Hagège observes that the human perception of a written statement is based on the following three aspects: • morphosyntactic, which separates classifiable forms (morphology) that can be categorised (nouns, verbs etc.) from their function (syntax). These aspects govern relations between larger sections of the statement (predicate, subject and object).  
The Framework for the Evaluation of Machine Translation, FEMTI, brings together the many disparate metrics and methods which have been devised for MT and helps evaluators to design an evaluation plan based on the context of use intended for the system. FEMTI allows therefore the generation of more standardized and reusable evaluation plans. By evaluators we mean not only developers and programmers, but also end users, managers, and anyone else with a stake in the acquisition or deployment of a system. Thus, the use of FEMTI is not limited to experts in the field of MT. In this paper we describe FEMTI and the latest enhancements we are making to it, in particular the interfaces which not only allow evaluators to create their own tailor-made evaluation plans, but also to contribute their experience and expertise in constantly improving the resource for the community at large. 
This paper revises the general perception that localisation is about linguistic and cultural adaptation of digital content to the requirements of foreign markets; that localisation is successful if the origin of the material can no longer be detected. We will show that in a more and more globalised society (not just economy) publishers, and especially publishers of advertisements, play with 'strangeness' and stereotypes. For example, there are advertisements running completely in French on Irish television and radio advertisements in English-speaking countries that are completely in German (or in English with heavy German accents). Rather than adapting to the culture of the target country, rather than avoiding differences, in these cases publishers highlight the differences, focus on ‘strangeness’, introduce (rather than avoid) accents, embrace cultural diversity rather than avoid it - and all that to increase sales. As a complimentary, pleasant and valuable by-product, the entertainment value for the consumer increases significantly. Uneasiness When the localisation industry emerged in the mid 1980s, localisation was technically more complex than it is today. Applications were not properly internationalised; content was not separated from functionality; a full recompilation of the application after translation was almost always the norm, making extensive and labour-intensive testing obligatory. At the same time, however, the question of how to culturally adapt a word processor, a spreadsheet or a similar office-type application - the most common applications to be localised then - was not even asked. The idea was to use localisation as a vehicle to increase return on investment (ROI) in the original application by opening up huge new markets (mainly in rich, developed western European countries) through a relatively cheap and low-tech ‘adaptation’ of the products, which would then make them accessible to non-English speaking consumers. Twenty years later the landscape has changed considerably. Much of the localisation effort has been reduced to simple translation tasks thanks to the use of sophisticated tools that automate much of the engineering and testing effort. Mainstream localisation is now far less technical than it used to be. However, what is being localised has changed so much that the question how localisation should be done has to be re-visited. Web localisation has been and will for the foreseeable future remain the one area in localisation with the highest growth rates. Web localisation deals not just with simple user interfaces but with more general digital content. This digital content can include  material on a wide variety of topics, including history, education, politics, culture, entertainment and gaming. While the technical problems of localising this content have been solved in principle if not strategically, the question of how to adapt this content culturally has not yet been answered. Although localisers have started to discuss the issue of cultural adaptation at conferences and in relevant industry publications, the solutions they are recommending largely follow old principles: design for a global audience, i.e. internationalise your service or your product, keeping the required localisation effort to a minimum; when localising adapt your digital product or service to the expectations of your target audience, i.e. give the Germans a ‘German’ product, the French a ‘French’ product and the Italians an ‘Italian’ product. The general idea is to hide the origin of the original content, strive for the global common cultural denominator and make everyone believe the digital product or service they are dealing with was developed in their own country. To back up this strategy, experts invariably cite the godfather of cultural difference in the workplace, Geert Hofstede, without questioning or critically appraising the findings of his research which has its origins in the 1960s. This contribution aims to clarify an uneasiness with this approach and to open up a better informed discussion about cultural adaptation as part of the overall localisation effort. I18N-L10N-G11N It might be hard to believe but it is true: after twenty years of localisation there is still no consensus of what internationalisation (I18N), localisation (L10N) and globalisation (G11N) mean and how they relate to each other. Definitions given by industry associations and companies on their web sites - although they vary considerably in detail - refer to localisation generally as the ‘linguistic and cultural adaptation of products to the requirements of foreign markets’. Most surprisingly, the fact that all internationalisation and localisation deals exclusively with digital material is always overlooked - or, maybe it is so obvious that it is not even worth mentioning? What makes this oversight so important is that its implications have never been explicitly discussed. The fact that some digital material, be it simple text, a graphic, audio or video, is not being adapted in a traditional medium such as paper or celluloid (what Negroponte described as the world of atoms) but in digital format has important implications on the tools and technology used, the process employed, and the knowledge required by the professionals involved. These implications are important, but can, unfortunately, not be considered in more detail in the context of this contribution. For our purposes, we will use the terms as follows: Internationalisation is the process of designing (or modifying) digital content (in its widest sense) and services so as to isolate the linguistically and culturally dependent parts of an application; and the development of a system that allows linguistic and cultural adaptation supporting users working in different languages and cultures.  Localisation is the linguistic and cultural adaptation of a digital product or service to the requirements of a foreign market and the management of multilinguality across the global, digital information flow. Globalisation, in contrast, is a business strategy (not so much an activity) addressing the issues associated with taking a product to the global market; this includes worldwide marketing, sales and support. The underlying rationale for localisation, the principal driver behind the localisation effort is the interest of the developers of the original product or service to increase their return on investment in that service or product. When software publishers were looking for markets in the mid-eighties where they could sell their products, they realised that there were several potential markets in Europe, ready to absorb their products, with all the right ingredients, i.e. a well-educated population with a sufficiently high income to buy their products. The only problem was that they did not speak English. That moment the localisation industry was born: its mission became to adapt software at a relatively low cost generating a relatively high revenue. While the subject localisers are dealing with has developed and expanded - localisers today deal not just with software but also with more general digital content - the underlying rationale behind this effort has remained the same. There are a number of factors cited by localisation experts when asked what makes a successful localisation project. As in other industries, the successful balance between quality, cost and time required to complete a project are crucial. In relation to the acceptance by users, experts largely agree that localisation has been successful when the localised products and services have been linguistically and culturally adapted to the point that users do not realise that the product or service they are using was developed originally in a different country for a different target group. Therefore, and here we extend the definition of the term, localisation is the linguistic and cultural adaptation with the aim to produce digital products and services for which the country of origin can no longer be traced. In other words, the measure of success is / believe it's mine, you believe it's yours - but, in its essence, it is all the same. Cultural adaptation and localisation Cultural adaptation in the context of localisation can only be understood on the background of its (short) history and rationale. Localisation is a tool used by digital publishers to sell products and services into markets where the original product 'as is' would not sell. The adaptation process aims to ease the use of products and services by removing linguistic and cultural barriers inherent in some digital products. These barriers are present at a shallow level, which is now mostly understood, and at a deep level, which localisers still struggle with. The shallow level includes the use of colours, symbols, sounds, and signals which have different meaning in a different cultural context. The deep level includes less evident but probably even more important aspects of the underlying value system, described, among others, by Geert Hofstede in his five categories of cultural differences which will be examined in more detail later in this paper.  One of the largest, oldest and most global organisations is the Catholic Church. It has adapted not just its translations to reflect changing beliefs and value systems in different cultural spaces, but also the images of the members of the holy family. For example, the image of the Virgin Mary looks distinctly local in Europe. Northern Africa/Middle East and South America.  Egyptian  European  Andean  Modern publishing houses have followed suit. The cover photograph of a guide book to the fun island of Ibiza. originally published in Germany, shows two young women in bathing suits on a beach having fun. This picture was kept for the Dutch version of the same guide book. When the French publishing house Hachette localised the guide book into French, they not only required the author to remove the unsuitable references in the guide book to the large gay community and the widespread use of drugs on the island, they also decided that the cover picture had to be changed. They believed that when French people go on holidays, they are looking for local costumes, folklore and traditions. Therefore, the two women on the beach had to make room for a middle-aged lady dressed in a traditional ibicenco dress. In essence, they adapted the guide to match the expectations of their potential readers; the reality of island life was rather less important.  German  Dutch  French  Although the examples above are taken from the traditional world of paintings and printing presses, the same principles hold in the digital world. In fact, they are probably even more prevalent because changing or replacing images in the digital world is easy, in comparison with the world of traditional publishing. In the following chapters we will explain in more detail the difference between what we have defined as the shallow and the deep levels of cultural adaptation in localisation. Shallow level The shallow level of cultural adaptation has been of relevance in localisation since the introduction of the graphical user interface (GUI). It includes the use of: • Colour • Sensitive pictures and images • Hand signals • Symbols • Sounds • History • Product names and acronyms The following paragraphs provide some example for each of these areas. Use of colour For example, in many Western countries red is an alarming colour, white can indicate a pure or basic state, and black is sombre. This is different in Asian countries like China where red expresses joy. white indicates mourning and black is “the lucky colour”. Green is associated with lush growth and ecology in Western countries, while it is the holy colour of the prophet in the Islamic world.  Sensitive pictures and images For example, the national flag of a country is widely used to identify products aimed at specific markets and is, therefore, often printed on packaged software products. The Saudi Arabian flag contains holy symbols associated with the Koran, which Muslims are forbidden to destroy or bin. Hand signals Hand signals probably represent the most dangerous area of non-verbal communication. For example, a hand held up with the forefinger stretched out and the palm towards the viewer could be used to indicate “Danger!” or “Stop” in many countries — but in Greece it could cause serious offence. The “thumbs-up” sign, and “ok” sign (index finger and thumb forming a circle) used in many Western countries are regarded as sexual gestures in others. Symbols Icons not directly related to system components (disk, printer, monitor etc.) or application-determined elements (drawing, writing, opening files etc.) usually do not cause problems. However, symbols and icons that do not form part of the culture of the target country can cause serious problems for users in that country. Sounds Different cultures use sounds in different ways. For example, while a gong sound alerting a user that he made a mistake is perfectly acceptable in Western cultures, it should not be used in applications aimed at the Japanese market, where it would be seen as embarrassing for the user in front of colleagues. History Historical items frequently dealt with in multimedia encyclopaedias can be especially contentious. For example, which European was first to land on the American continent: was it St. Brendan, was it the Vikings, was it Columbus or was it a representative of the Mormons? Product names and acronyms Acronyms cannot be carried over into different languages and markets, even if they refer to international organisations. NATO is NATO in German, but it is OTAN in Spanish, for instance. Deep level - entertainment, education, information, eContent While at least some aspects of cultural adaptation at the shallow level are well understood, there are no strategies or guidelines helping localisers struggling with the deep level of cultural adaptation, probably best captured by Geert Hofstede and his framework of cultural differences in the workplace. Prof. Geert Hofstede conducted perhaps the most comprehensive study of how values in the workplace are influenced by culture. His study analyzed a large database of employee values scores collected by IBM between 1967 and 1973 covering more than 70 countries. He first used the 40 largest countries only and afterwards extended the analysis to 50 countries and 3 regions. In the editions of his work since 2001, scores are listed for 74  countries and regions, partly based on replications and extensions of the IBM study on different international populations. Subsequent studies validating the earlier results have included commercial airline pilots and students in 23 countries, civil service managers in 14 counties, ‘up-market’ consumers in 15 countries and ‘elites’ in 19 countries. From the initial results, and later additions, Hofstede developed a model that identifies four primary Dimensions to assist in differentiating cultures: Power Distance – PDI, Individualism - IDV, Masculinity - MAS, and Uncertainty Avoidance - UAI. He added a fifth Dimension after conducting an additional international study with a survey instrument developed with Chinese employees and managers. That Dimension, based on Confucian dynamism, is Long-Term Orientation - LTO and was applied to 23 countries. These five Hofstede Dimensions can also be found to correlate with other country and cultural paradigms.1 Power Distance Index (PDI) focuses on the degree of equality, or inequality, between people in the country’s society. A High Power Distance ranking indicates that inequalities of power and wealth are prevalent within the society. A Low Power Distance ranking indicates the society de-emphasizes the differences between a citizen’s power and wealth. Individualism (IDV) focuses on the degree society reinforces individual or collective achievement and interpersonal relationships. A High Individualism ranking indicates that individuality and individual rights are paramount within that society. A Low Individualism ranking typifies societies of a more collectivist nature with close ties between individuals. Masculinity (MAS) focuses on the degree society reinforces, or does not reinforce, the traditional masculine work role model of male achievement, control, and power. A Low Masculinity ranking indicates the country has a low level of differentiation and discrimination between genders. In these cultures, females are treated equally to males in all aspects of society. Uncertainty Avoidance Index (UAI) focuses on the level of tolerance for uncertainty and ambiguity within society, i.e. unstructured situations. A High Uncertainty Avoidance ranking indicates the country has a low tolerance for uncertainty and ambiguity. This creates a rule-oriented society that institutes laws, rules, regulations, and controls in order to reduce the amount of uncertainty. A Low Uncertainty Avoidance ranking indicates the country has less concern about ambiguity and uncertainty and has more tolerance for a variety of opinions. Long-term Orientation (LTO) focuses on the degree society embraces, or does not embrace, long-term devotion to traditional, forward thinking values. High Long-term Orientation ranking indicates the country prescribes to the values of long-term commitments and respect for tradition. A Low Long-term Orientation ranking indicates the country does not reinforce the concept of long-term, traditional 
José Vega (jveqa@mv-xml.com)  Jacques Vergne  Benoît Lamey (blamey@etg.be)  (Jacques.Vergne@info.unicaen.fr)  my-xML (www.my-xml.com)  GREYC, Université de Caen (http://www.qreyc.unicaen.fr/)  Luxembourg  France  Outline This paper presents my-xML's Candidate Term Extractor (MyCatex), a language independent term extractor that works without any language-specific resources. MyCatex is currently developed by my-xML, a Luxembourgish languageengineering company specialized in multilingual content management (http://www.my-xml.com/). The MyCatex term extraction algorithm is based on the current researches of Jacques Vergne from the University of Caen in France. MyCatex can be used for term extraction, semi-automatic and automatic generation of multilingual thesauri and document tagging and classification. The following chapters introduce: √ the "General Architecture" of MyCatex and how the outputs can be exported towards third part applications √ the "Extraction" chapter explains the broad outlines of the algorithms developed for extracting candidate terms from texts in different languages √ the "Validation" process chapter lists the validation functionalities of MyCatex used by the linguists/terminologists/documentalists in order to validate or invalidate the extraction results suggested by the software √ the "Terms Description" process chapter lists the various ways to add and visualise syntax and semantic information to the selected terms √ the "Terms Repository" chapter describes the data format used to store and re-use the selected terms √ the chapter "Corpus" explains how we proceed for functional and regression test automation. Contribution for Translating and the Computer 27 - (24-25 Nov 2005, London) (1)  ________________________________________________________________ 26 oct 05 ___________ First the text is loaded in the software for extraction. The extraction engine doesn't require any language dependant information. However, some languagespecific information can be added in order to improve the extraction results. For instance: stop words, invalid words, validated terms... Then, the extraction engine generates a sorted list of candidate terms. The validation of these terms can be either automatic or semi-automatic. For semiautomatic validation, a set of tools is provided to help the user in the validation process. The selected terms can be stored in a term repository or used to index the source text. When saving terms in the Terms Repository, you can also add syntactical and semantic information. The Terms Description process allows the user to add and visualise this information in a graphical way showing relationships between terms. The Terms Repository uses the XML format (respecting MARTIF standards for the representation of multilingual terminological data ISO-12620 DATA CATEGORIES). This allows the integration of these terms in third party applications or my-xML products, such as MyTerm1 and MyTerm-Glossy2 (further information about these products on www.my-xml.com). 
This paper reports the results of an online survey on terminology management and terminology extraction conducted by the Linguistic Data Processing section of Dept. 4.6 - Applied Linguistics & Translating/Interpreting at Saarland University. The survey was available on the Internet in English, French, [1] German, and Spanish from mid-May until September 2005 . It was promoted in many major CAT mailing lists, and by translator and interpreter associations (BDÜ, ITI). Over 400 professional translators, terminologists, and interpreters all over the world have responded to the questionnaire. With this survey we want to investigate the relationship between research and practice in the area of terminology extraction and evaluate if there is any need to reconcile both. Aimed at translators, terminologists, interpreters and project managers, the main goals of the survey are to investigate the dissemination and application of terminology management tools (with a focus on terminology extraction tools) and to assess the demands on today's terminology extraction tools. 1. Introduction Terminology management has now been taught for several years at universities and in translationoriented education programmes. It continues to be a lively research field involving applied linguists, computational linguists, and computer scientists. One of the most active research areas in terminology management in the last fifteen years has been terminology extraction (TE). A surprising amount of applied research in this field has produced several different approaches (linguistic, statistical, and hybrid) as well as tools to facilitate the time-consuming process of terminology compilation. In recent years, there has been an increasing demand for terminology extraction software (c.f. e.g. Warburton, 2001). However, although a number of term extractors are already available on the market today, it seems that they are still not widely used nor do they meet the real needs of translators, interpreters, and terminologists. Today, there is no doubt about the increasing importance of terminology in our society. Terminology plays a very important role in many different fields such as standardization, translation, technical documentation, and software localization. Accurate and complete terminology improves the productivity of translators and technical writers, and is a prerequisite for successful communication. Consequently, terminology management dealing with the preparation, processing, and documenting of a specialist vocabulary has become an increasingly important activity. In the past 25 years, methods, applications and tools for professional terminology work have changed and developed substantially. Despite these improvements, the creation, organization, and management of terminology are still time consuming and cost-intensive tasks. As has been pointed out by the LISA Terminology Survey Report (2001), term identification and extraction are the tasks that are the most  http://fr46.uni-saarland.de/download/publs/sdv/t-survey_aslib2005_zielinski.htm  18/10/2010  Research meets practice: t-survey 2005  Page 2 of 27  frequently performed manually and that would significantly benefit from automation. Terminology extraction can be defined as the operation of identifying term candidates (TC) in a given text and should be terminologically distinguished from term recognition, which is the operation of comparing the TC lists (the output of TE) with a terminological database (TDB) in order to identify known/unknown terms. According to Lieske (2002), the term extraction process comprises four different tasks, namely a) the compilation of a corpus (machine readable texts, that serve as a basis for the TE), b) terminology extraction (identification and extraction of TCs), c) the evaluation or validation of the results (determining the terminological relevance of a TC), and finally d) the classification of terminology according to classes and categories. TE has three major applications: terminology management, translation, and information retrieval (Thurmair 2003), the first and second being the focus of our investigation. In terminology management and translation, TE is used for the creation, enlargement, and update of terminology. The main goals are to facilitate the task of identifying terms and their translations and to increase the productivity of translators, interpreters, and terminologists. With respect to the number of languages involved in the extraction process, we can basically distinguish between monolingual and bilingual TE. In the translation process, monolingual TE is usually carried out before the translation starts, either on the source texts or on reference texts. The goal is to identify all relevant terminology in a text to be translated or - in the case of terminology - to identify all terms of a certain (sub-) domain. Bilingual TE, on the other hand, is mostly carried out on translated texts (parallel corpora or translation memories). Its main goal is the recognition of potentially equivalent terminological units in two languages (Thurmair, 2003). In both cases the resulting TCs are often compared to existing terminological databases in order to distinguish known terms from unknown terms (cf. Saß 2004). 2. Approaches to terminology extraction Depending upon the type of information used as a basis to identify terms, approaches to TE are usually classified as linguistic, statistical or hybrid. Linguistic and statistical approaches can be further subdivided into term-based (intrinsic) and context-based (extrinsic) methods (cf. Bourigault et al. 2001, Streiter et al. 2003). Terminology extraction tools (TETs) following a linguistic approach try to identify terms by their linguistic structure, e.g. morphological and syntactic structure. For this purpose, texts are annotated with linguistic information with the help of morphological analysers, part-of-speech taggers and parsers. Then TCs with a certain tag structure are filtered from the annotated text by using pattern matching techniques. Intrinsic methods try to filter TCs according to their internal structure, e.g. according to the morphological structure (e.g. German "Zylinderabschaltung (cylinder cutout)" ds=zylinder#ab_$schalten~ung ). Extrinsic methods try to identify TCs by analysing the morphosyntactic structure of a word or phrase, such as looking for part-of-speech sequences like NP= noun + noun (e.g. printer menu). Another technique is to filter TCs by looking for commonly used text structures such as definitions and explanatory contexts like "X is defined as " or "X is composed of " (cf. Pearson 1998, Saß 2004). The assumption underlying the statistical approaches to TE is that specialized documents are characterized by the repeated use of certain lexical units or morpho-syntactic constructions. TETs based on statistics try to filter out words and phrases that appear with a frequency higher than a given threshold by applying different statistical measures (see Manning & Schütze 1999 for an overview). Term-based statistical methods try to compute the structure of TCs e.g. by using n-grams. Often, the structure of existing terms is compared to the structure of words and phrases in a corpus in order to filter out TCs with a similar n-gram structure (example-based approach; cf. Streiter et al. 2003). Another common method is to compare the frequency of words and phrases in a specialized text to their frequency in general language texts assuming that terms tend to appear more often in specialized texts than in general language texts.  http://fr46.uni-saarland.de/download/publs/sdv/t-survey_aslib2005_zielinski.htm  18/10/2010  Research meets practice: t-survey 2005  Page 3 of 27  Different evaluation criteria exist for TETs, involving among others accuracy, supported file formats, languages, etc. (for a detailed list see Lieske 2002, Sauron 2002, Saß 2004). The most frequently used criteria are noise and silence, and recall and precision. While noise refers to the ratio between discarded TCs and the accepted ones, silence refers to the number of terms not detected by a TET. Recall and precision are two measures frequently used in IR, the former being defined as the ratio between the sum of correctly retrieved terms and the sum of existing terms, the latter being defined as the ratio between correctly extracted terms and the sum of proposed TCs (c.f. Zielinski 2002). In order to eliminate noise from the resulting TC lists, linguistic and statistical approaches often make use of stop word lists. Stop word lists contain "empty" words (cf. Carstensen et al. 2001) that are not of interest for the terminologist, because they do not qualify as terminological units, but that are often filtered as TCs because of their morpho-syntactical structure or because of their frequency value (e.g. articles, pronouns, auxiliary verbs, and prepositions). Both approaches, linguistic and statistical, have their advantages and disadvantages. TETs following a purely linguistic approach tend to produce too many irrelevant TCs (noise), whereas those following a purely statistical approach tend to miss TCs that appear with a low frequency value (silence; cf. Clematide 2003). Linguistic-based TETs often provide better delimited TCs than statistical-based ones. Furthermore they usually reduce TCs to their canonical form (base form or lemma) and thus do not provide result lists with repeated TCs as is the case for statistical-based tools. However, the disadvantage of linguistically based TETs is that they are language-dependent and thus only available for major languages. Statistical TETs, on the other hand, can also be used for lesser-used languages that lack computational resources such as minority languages (cf. Streiter et al. 2003). On their own, linguistic and statistical methods still fail to tackle many of the basic inherent problems of TE, such as the reduction of TCs to their canonical form, detection of multi-word terms, term variations, and terms in discontinuous units (i.e. coordinations, juxtapositions). Since the nature of these problems is so varied, it seems that only a combination of approaches will help to provide efficient TETs. In fact, the only method which has been "recovered" by many authors because of its still unexplored possibilities is the hybrid approach. Authors such as Cabré (2001) claim that the only way to make progress in the field of automatic terminology extraction is by combining statistical and linguistic methods. Fortunately, this seems to be the approach current investigations in the area of TE are taking (Thurmair, 2003). Although there are many investigations about the important role semantic information plays in the performance of TETs (e.g. to find out the semantic relations between terms), most TETs currently available on the market do not incorporate a semantic component (Thurmair, 2003). So, from a practical point of view, it can be stated that a term is normally considered by TETs as a word or phrase with a particular structure and belonging to a specific category (noun or nominal phrase) which additionally occurs with relatively high frequency (Thurmair, 2003). 3. Terminology extraction software - a short market overview In this section we will give a short overview of TETs that are commercially available on the market. We will briefly present the major tools and compare their general features and functions, and classify them according to the approaches outlined in the previous section. Tools developed in research, which are in most cases prototypes of limited applicability and dissemination, will not be taken into account. The interested reader is invited to refer to Cabré et al. (2001) for an overview of such systems developed in the 1990s. As has been the case for terminology management systems, in the beginning many different TETs came onto the market. Many companies tried to offer some term extraction functionality, either integrated in their CAT-tools (e.g. Trados TermExtract, now MultiTerm Extract) or as standalone versions (e.g. TerminologyExtractor from Chamblon). Today it seems that only a few major tools have asserted themselves on the market. Among these figure MultiTerm Extract (Trados), SDL PhraseFinder, Xerox Termfinder, Terminology Wizard (Synthema), and TerminologyExtractor (Chamblon).  http://fr46.uni-saarland.de/download/publs/sdv/t-survey_aslib2005_zielinski.htm  18/10/2010  Research meets practice: t-survey 2005  Page 4 of 27  Depending upon the underlying technology (statistical or linguistic), supported languages, file formats, license type (freelance, network) etc., the price for TETs varies from around 100 to several thousand euro.  All TETs provide lists of term candidates that can either be exported for external validation (e.g. in *.txt, *.csv) or validated directly in the TET. During the validation process, the user can usually edit TCs, add manually extracted terms, set status flags (e.g. checked, unchecked), and add TCs to stop word or abbreviation lists. While the first TETs only provided the user with very long TC lists with nearly no additional information that could help to validate the TCs, today's TETs usually link TCs to the context, mostly in form of KWIC concordance windows. Linguistically based tools usually also provide information about POS, and canonical form of the TC. Furthermore, all TETs incorporate sorting functions. TCs can be sorted either according to frequency or alphabetical or - as in the case of SDLPhraseFinder 2005 - according to a "confidence index" in which TCs are ranked according to frequency in combination with the term pattern type. Another feature that is intended to speed up the validation process and that has been added recently to TETs is term recognition. In this step TCs are compared to selected terminological databases in order to distinguish known from unknown words. In order to allow the user the adaptation of the extraction algorithms, most systems provide additional parameters such as the minimum or maximum length of TCs, minimum frequency of TCs, and minimum or maximum of proposed translations in bilingual mode. Trados MultiTerm Extract, for example, gives the user the possibility to adapt the noise/silence ratio by the use of a slider. After the validation process the accepted TCs can either be exported into one of the standard exchange formats or - in the case of integrated TETs - into the respective terminological database. The table below gives a comprehensive overview over some of the major TETs and their properties.  Term extraction Approach Supported languages Term identification (single/multi word terms) Parameters Input File formats Stop lists Term recognition (existing TDBs) Results Additional information Validation of TCs  Trados MultiTerm Extract 7 mono- & bilingual statistical 137 SL & TL SWU, MWU Min_length, max_length, max_num, QAfilter (slider), search for new translations, max_trans, min_trans_freq One or more doc, html, jsp, asp, aspx, xml, sgml, qsc, xtg, ttg, tag, rtf, tmx, tmw, ttx, bif, txt yes yes no lemmatization context, frequency, editable list, different status codes, (check, delete, add translation, generate sample sentences, add manually extracted terms,  SDL Phrase Finder 2005 mono- & bilingual hybrid nl, de, en, fr, it, pt, es SWU, MWU Exclude_uppercase, max_length One or more rtf, html, txt, SDL TM, SDL itd yes no documentation available no documentation available POS, root form, editable list comments,  Multitrans (Corpus builder module) mono- & bilingual statistical all SWU, MWU Min_length, max_length, list possible translations One or more doc, WordPerfect, txt, Unicode text files, html, xml, pdf, TMXcompliant translation memories, etc. yes yes no lemmatization context, frequency, source, editable list  Xerox TermFinder mono- & bilingual hybrid (XELDA Technology) de, fr, it, en, sv, es, ru, pt, no, hu, fi, nl, da (total 15 languages) SWU, MWU no documentation available One or more html, sgml, xml, txt, doc, rtf yes no frequency, context editable list  Synthema Terminology Wizard 3.0 mono- & bilingual hybrid SL= de, en, fr, it, pt, TL=SL+af, da, ca, nl, pl, pt, ru, es, cz, tr, hr SWU, MWU Handle unknown words as nouns, convert all terms to lower case, minimal frequency One or more rtf, html, txt, Trados TM, Transit yes yes lemmatized context editable list, different status codes, add to dictionary, stop list, abbreviation list  Chamblon Terminology Extractor 3.0 monolingual statistical en, fr, + ? SWU, MWU Context width, frequency filter, minimum frequency, text filter, One or more doc, html, rtf, txt yes no documentation available frequency, context no  http://fr46.uni-saarland.de/download/publs/sdv/t-survey_aslib2005_zielinski.htm  18/10/2010  Research meets practice: t-survey 2005  Page 5 of 27  Sorting of TCs Concordance window Export format Exportable info  etc. frequency, alphabetical yes (add as context sentence) Trados MultiTerm Extract 7 MultiTerm, MultiTerm XML, txt, user definable  Export filter  yes  Component or single product Price  part of Trados CAT-Tools, integrates with MultiTerm, Workbench, Workbench Freelance (1055 )  frequency, alphabetical, confidence index yes SDL Phrase Finder 2005 SDL TermBase online, tab delimited user definable  frequency, alphabetical, yes Multitrans (Corpus builder module) no documentation available no documentation available  yes  yes  integrates with SDL family of products, SDLX, TermBase online  integrates with Multitrans  ?  568.00 - 2060.00  no documentation available yes Xerox TermFinder OLIF II no documentation available no documentation available part of Xerox Terminology Suite, integrates with TermOrganizer negotiable  frequency, alphabetical yes Synthema Terminology Wizard 3.0 eif, xls lemmatized term, translation, frequency, status, example sentence or segment yes standalone 500 (mono-), 800 (bilingual)  frequency, alphabetical yes Chamblon Terminology Extractor 3.0 txt terms, example sentences (not both at the same time), with/Without frequency yes standalone ~ 100  Table 1: TETs and their properties  It should be added that several evaluations of TETs have been conducted by researchers and practitioners. Even if it is not our intention to discuss them here, we would like to refer the interested reader to some of these evaluations that may serve as a starting point for further reading: Saß (2004), Lieske (2002), Sauron (2002), and the report of the Schweizer Bundeskanzlei (2001).  4. t-survey 2005 - Description of the survey's design and goals  t-survey was made available on the Internet in English, French, German and Spanish from mid-May 2005 until September 2005. Its target group includes translators, interpreters, terminologists and project managers who work on a freelance basis or as employees. Therefore, in order to reach as many language professionals as possible, the survey was promoted in many major CAT mailing lists, through several terminology portals (TermNet, DTT), and by translator and interpreter associations (BDÜ, ITI). Over 400 professionals all over the world have responded to the questionnaire. Unlike other terminology surveys carried out before (e.g. the LISA terminology survey (2001)), the purpose of our survey was to examine the relationship between research and practice in the area of terminology extraction and to evaluate if there is any need to reconcile the two. In comparison with tsurvey, the LISA terminology survey had completely different purposes, and its research area was differently defined. The Localization Industry Standards Associations (LISA), responsible for providing practical advice, business guidelines and standards information for translation and localization workflow tools, designed a series of surveys in the area of terminology management. These surveys were carried out in 2001 and aimed mainly at the investigation of the methods, tools and practices for managing terminology in the localization industry (Warburton, 2001:1-5). Our t-survey s main goals are to investigate the dissemination and application of terminology management tools with particular focus on terminology extraction tools (TETs), and to assess the demands on today s TETs. Analysis of the users responses should allow us to determine whether there are differences between the requirements of various professional groups working with terminology, i.e. between translators and terminologists, and, if so, what these differences are. Furthermore, it should enable us to summarize the required functionalities and to formulate design criteria for TETs that fit the different user profiles. In summary, the purpose is to obtain useful information which properly used will have an impact both on research as well as on software development. The survey was designed in a way that no previous knowledge about terminology extraction was  http://fr46.uni-saarland.de/download/publs/sdv/t-survey_aslib2005_zielinski.htm  18/10/2010  Research meets practice: t-survey 2005  Page 6 of 27  required by the users. For maximum user-friendliness and minimum frustration that may result from lengthy forms with partly irrelevant questions, the forms were designed to change dynamically according to the answers. The survey s design was based on a task model of translation where, for example, the translation of documents was viewed as a combination of tasks to include analysis, terminology research, translation, term extraction, and term management. The total time estimated to answer the questions was approximately 10 minutes. t-survey is divided into five sections. The first section, called Personal Information, is aimed at identifying the users profiles, such as by age, profession, type of employment, professional experience, and type of education/formation. The second section, Working Environment, focuses on the working conditions and resources that professionals use. This refers, for example, to information such as company size (number of employees), available computer facilities (type of operating systems) and availability of Internet resources (type of Internet connection). Working languages, areas of specialization (such as technology, law, economics or others), text sorts (e.g. operating instructions, patents, software documentation, commercial reports, web pages, business correspondence, laws, certificates, scientific publications, among others), text size (calculated in standard pages) and file formats (*.txt, *.html, *.doc, *.xls, *.pdf, etc.) are some of the types of information requested in the survey s third section called Translation, the main goal of which is to gather information about the translation process itself. In the fourth section, Terminology Management, the central point of interest deals with the process of compilation and storage of terminology. Questions are related to the types of terminology work (e.g. monolingual, bilingual, multilingual) and the frequency in which it is carried out, the method of managing terminology (index cards, Word/Excel tables, relational databases, terminology management systems), the types of information that are stored in the terminological data base (for example, nouns, noun phrases, verbs, collocations, etc.), the criteria for recognizing terms, additional information gathered together with terms (such as administrative information, contexts, definitions, grammatical and semantic information, graphics, etc.) and the estimated time to search terminology during the process of translation. The fifth and last section, Terminology Extraction, is aimed at obtaining concrete information on the use of TETs by the respondents, such as the type of tools used or tested (e.g. MultiTerm Extract (Trados), SDLPhraseFinder (SDL), TermFinder (Xerox), TermFinder (Acrolinx) and Autoterm (IAI)), the purpose of carrying out terminology extraction (extraction of terminology from source texts, from reference texts, from translation memories), the performance of the TETs tested or used, and finally the desired functions for TETs and the amount of money available to be spent on a TET with the desired functions included. Following the design of the survey and viewing translation of documents as a combination of tasks, as previously mentioned, it can be deduced that sections 3-5 of the survey cover three stages of the translation process, namely text analysis, terminology identification and terminology management. Concerning the specific goals of this survey, we primarily want to identify the profiles of those participants who carry out terminology work and those who do not. The idea was to have information from primary sources about the use of TETs in practice and the reception these tools have among language professionals. With regard to those participants who claimed to be involved in terminology activities, we are especially interested in the following points: (1) to what professional groups do they belong, (2) what are their qualifications, (3) what is their knowledge level regarding computer technology and terminology, (4) what are the frequency and type of their terminology work, (5) for what purposes do they perform terminology work, (6) what importance is given to terminology tasks, (7) with what languages and language combinations do these professionals work most of the time,  http://fr46.uni-saarland.de/download/publs/sdv/t-survey_aslib2005_zielinski.htm  18/10/2010  Research meets practice: t-survey 2005  Page 7 of 27  and (8) what TETs are mostly used and preferred. With regard to those participants who do not carry out terminology work, we also want to determine their user profiles and the reasons they give for not taking into consideration terminology tasks in their work. Moreover, we consider it important to learn if these participants have tested some TETs before and if they would be willing to use them, provided that the tools would satisfy their needs. Finally, since the economic aspect was also considered to be important, all participants were asked to indicate the price they would be willing and able to pay for a TET which satisfies their needs. Based on this detailed overview of the users profiles it should be possible to accurately determine the users requirements, which should be the leitmotif for research in the field of TE for the development of TETs.  5. Results and analysis  We will now present the results of the survey and their respective analysis section by section. A total of [2] 451 participants filled in the survey, expending an average time of 15 minutes . The high number of participants is a sign of the great interest translators, interpreters and terminologists have in issues related to translation and terminology management and developments in the area of CAT tools.  5.1 Personal information  Especially translators seemed to be very interested in terminology extraction and terminology extraction tools. This can be concluded from the fact that the majority of the professionals who participated in the survey were translators (371). 81 of the participants were interpreters, 56 project managers and 61 terminologists. Since more than one profession could be selected, 250 participants indicated working exclusively as translators, 5 as interpreters, 12 as project managers and 29 exclusively as terminologists (see figure 1). 86 participants indicated having additional professions, which means that a large number of the participants could be considered highly qualified. To the question regarding the working status, 292 of the total participants indicated working as freelancers, 126 as employees and only 24 both as freelancers and employees (see figure 2).  Professions  00  371  50  00  50  00  50  81  00  61  56  50  0 translators interpreters terminologists project m anagers  Professions  Figure 1  86 others  Answers  Freelancers vs. Employees  350 292 300 250 200 150 100 50 0 Freelancers  126  24 Employees Freelancers and employees  9 No data  Figure 2  Combining information on professions and working status, it turns out that the majority of translators and interpreters work as freelancers whereas the majority of terminologists and project managers work as employees. Of all professional groups, freelance translators were the largest one (see figure 3).  http://fr46.uni-saarland.de/download/publs/sdv/t-survey_aslib2005_zielinski.htm  18/10/2010  Research meets practice: t-survey 2005  Page 8 of 27  Freelancers vs. Employees according to professions  100% 90% 80% 70% 60% 50% 40% 30% 20% 10% 0%  4 23 61 283 Translators  0  3  
A growing number of websites that are only available in one language rely on free online machine translation (MT) services to disseminate their contents in a variety of other languages, in order to make themselves accessible to Internet users with different linguistic backgrounds. This approach to the management and delivery of digital information that bypasses professional localisation and translation raises a number of thorny issues, but clearly shows that free online machine translation services are regarded as valuable tools to overcome language barriers in the online environment. However, the vast majority of websites that adopt this strategy fail to take full advantage of the potential offered by free web-based MT, mainly due to poor consideration of crucial issues in human-computer interaction and web usability that are vital to ensure that Internet users have a positive and successful online experience. This paper presents the key stages and challenges involved in implementing this approach to the multilingual dissemination of online content, whereby free online machine translation is embedded into the architecture of a monolingual website. The main technical and practical issues are illustrated by means of an implementation case study based on the website supporting London’s successful bid to host the 2012 Olympic Games, followed by the discussion of the selected results of extensive user testing and evaluation. 1. Introduction 1.1. Machine Translation (MT) Technology on the Internet The area of Internet-based machine translation (MT) has grown considerably since 1997 when Systran Software Inc. and the popular search engine AltaVista launched Babelfish, the first service offering free-of-charge online machine translation on a large scale (Yang & 
In many companies, especially those relying heavily on technical documentation, the demand for translation heavily outstrips what the translation services can supply. Thus, many translation departments feel the need to make use of machine translation systems (good, bad and indifferent, depending on what is available) and to look for ways to improve the quality of their output. One favourite solution is to use a controlled language. This article reports on a small experiment designed to see to what extent using a controlled language does really improve the output of a machine translation system. INTRODUCTION Many companies now make use of machine translation (MT) in order to respond to the ever increasing demands of the translation market. The primary reason for turning to MT is to increase productivity whilst keeping costs as low as possible. This implies, in turn, keeping post-editing costs as low as possible. However, despite considerable progress in computational linguistics, MT is still a long way from being able to offer results that can be used without any revision. A major concern then, of those companies that use MT, is to improve the quality of the system produced raw translation as much as possible. One promising approach seems to be to influence the input text, especially by constraining the lexical items present and the grammatical constructions used - in other words, restricting input text to a controlled language. In the work reported here, we were interested in quantifying to what extent controlled languages helped to improve MT output. We thus carried out an experiment with the aid of translation students from the University of Geneva translation school and of professional translators from European Centre for Nuclear Research (CERN). The purpose of the experiment was to find out whether the use of a controlled language did in fact result in improved translation, and, if this was the case, the extent of the improvement. We carried out a quantitative and qualitative evaluation of the translation produced for two parallel texts, one in a controlled language, the other not. The MT system used was Reverso. Furthermore, we used two different types of controlled language in order to see whether the nature of the controlled language also influenced the results. -1 -  Here we briefly present the experiment, followed by a description of the corpus used for the experiment. Then we follow through the various stages of the evaluation. In particular, we discuss the classification of errors, as well as the qualitative criteria chosen as part of the evaluation of the translation. Finally, we present the results, followed by some conclusions that could be drawn from them. 1. The Experiment We assembled a corpus made up of parallel examples of controlled language texts and free texts. All the texts came from two companies who use controlled language (CL) as an authoring aid. We translated these texts with a MT system in order to determine whether the CL texts gave better results than the free texts. In the interests of being as objective as possible, we carried out an evaluation of the results in two stages. In the first step we counted the number of errors in each translation, taking into account how serious we considered the error to be. This method provided us with a precise quantitative indication of which version of the text (controlled or free) produced the best results. Secondly, we carried out a qualitative evaluation by asking people with experience of technical translation to evaluate the translations produced by the two versions. Preparing the experiment posed two main problems: the search for CL examples and the choice of the MT system. We were fortunate in being able to obtain Simplified English (SE) texts from Boeing and Caterpillar Technical English (CTE) texts from Caterpillar. These are amongst the most well-known and most frequently used CLs for technical translation. As we hinted earlier, the CL rules for the two are rather different. CTE defines a sub-set of the English lexicon and grammar adapted to the authoring of the maintenance manuals, user manuals and servicing manuals produced by Caterpillar (Hayes et al, 1996: 84). It can be thought of as a pre-editing language, oriented essentially to the needs of the computer system. Simplified English was developed by AECMA during the 70s as a way of improving documentation produced within the aeronautics industry (Gingras 1987, in Shubert et al 1995). Its main purpose is to make texts more easily comprehensible for a human reader (Allen 1999). As a MT system for the experiment, we chose Reverso. The primary reason was that this system is easily accessible to a large number of users and produces relatively acceptable results on the whole. 1.1. The corpus The parallel controlled and free texts used come from Boeing user manuals and Caterpillar maintenance manuals. Since we thought it important that the experiment should reflect real working conditions in the two companies, it was critically important to find examples of sentences which had already been checked by the technical authors at Boeing and Caterpillar. In choosing example phrases for testing, we tried to take phrases which were long and complex enough to be interesting, but still simple enough to allow our test subjects to read and understand them rapidly (Holmback et al 1996). We finally settled on five units of CL and five units of free language for each of our two CLs. A unit is made up of one or several phrases that constitute a single instruction. The length of a unit is between -2-  13 and 47 words. The total number of words in the corpus is 124 for the free version of the Caterpillar text, 138 words for the controlled version of the same text, 158 words for the free version of the Boeing text, and 172 words for the controlled version of this same text. 2. EVALUATION STEPS Before trying to translate the texts, we first created a personal dictionary containing the terms and the technical verbs appearing in the texts but not in Reverso's dictionary. We also provided target language equivalents. One of our reasons for choosing Reverso as an experimental system is that we thought it important to be able to modify the system in such a way as to be able to produce the best results possible. Reverso makes this possible to any user, without having to go through the manufacturer. Next, we needed a reference translation for each of our chosen sentences. This reference translation would be used in assessing what errors were present in the results. We machine translated the example sentences, corrected the translation ourselves and then asked experts in technical authoring and translation to check our corrected version. 2.2. Quantitative evaluation The first step of the evaluation is purely quantitative. We simply counted the errors present in each translation in order to determine whether the translation of controlled sentences contained fewer errors than the translation of free sentences. We did not consider all errors to be equal: we weighted each error using a weighting scheme based on the model proposed by Arnold et al. 1994. 2.2.1. The errors In order to classify the errors appropriately, we made use of proposals made in the context of human translation by Jean Delisle et al (1999) and Hurtado Albir (2001). But since these proposals were made in the context of human translation, we simplified the classification somewhat and also added three categories specific to certain types of errors occurring in MT but not in human translation. We also chose the categories to be informative in the context of our own experiment. The final classification covers eight types of errors (definitions of category 2, 3, and 4 come from the work of Delisle et al. 1999): Punctuation errors: use of punctuation signs that do not correspond to the rules of the target language. Under-translation error: omission of any compensation or explicitation required to obtain an idiomatic translation. Over-translation error: explicitation of elements of the source text that ought to be implicated in the target text. Solecisms: producing a syntactical structure that does not conform to grammatical conventions of the given language. -3-  Faulty syntactic analysis: error in the syntactical category attributed to a word resulting from an incorrect source text analysis. Lexical incorrectness: error in the semantic meaning given to a word. Stylistic clumsiness: when the translation is grammatically correct but is not idiomatic. Constructions having no sense: an error in which a whole part of the sentence is incorrect, making the translation impossible to understand. The cause of this mistake is very difficult to determine, as there is usually more than one cause. 2.2.2. Classifying the errors According to Delisle (1993, in Hurtado Albir 2001: 190), translation errors can be grouped into two categories. The first is "language faults", which are due to an imperfect knowledge of the target language. The second are "translation errors", which are caused by a "poor interpretation of the source text". These two groupings correspond to the analysis and generation modules of a MT system. We therefore grouped our eight error categories into two larger groups as follows: a) Faults occurring during the translation process: over-translation, under-translation and faulty syntactic analysis. b) Language errors: punctuation errors, stylistic clumsiness, lexical incorrectness and solecisms. 2.2.3. Relative severity of errors Intuitively, the errors made by a MT system differ in the repercussions they produce on the quality of the output translation: they cannot all be considered as of equal importance. Each type of error should be multiplied by a precise weighting factor which reflects the seriousness of the error (see Arnold et al 1994: 173-75). Each phrase is given a score which depends on the number of errors it contains, as well as the weighting factor attributed to each error. Taking as a starting point the work of Arnold et al (1994: 173-75), Sager (1989, in Hurtado Albir 2001: 295) and Dancette (1989, ibid: 303), we first classify the errors as minor, or superficial (errors which only have repercussions at the level of the form of the text) and serious (errors which have an incidence on the sense of the phrase). Independently of this distinction, we classified errors due to faulty syntactic analysis of the source text and lexical incorrectness errors into four groups (definitions come from the work of Delisle et al. 1999): False nuance: minor errors of sense which make little or no difference to how the phrase is understood. Incorrect meaning: a sense is attributed to a word or a segment from the source text that is does not have in the context in which it appears. Misinterpretation: a word or segment from the source text is given an entirely erroneous sense from that intended by its author. Nonsense: illogical formulation in the target text due to misinterpretation of the source text or wrong translation methodology. Finally, we fixed the weight of each type of error by drawing inspiration from the five criteria which Hurtado Albir (2001: 304) puts forward as determining the severity of the error: -4-  1. the importance of the error with respect to the original text as a whole 2. its importance with respect to the coherence and cohesion of the target text 3. the degree to which the sense deviates from the original text 4. its importance with respect to the communicative aspects of the target text 5. its impact on the purpose or functionality of the text Table 1 below gives the categories of errors, their degree of severity and the weight attributed to them. -5-  2.3. Qualitative evaluation In the second part of the evaluation we applied qualitative criteria to the evaluation of the translations produced by Reverso 2.3.1. Criteria for the qualitative evaluation We used three different models in defining the criteria for qualitative evaluation, that of Arnold et al (1994: 169-71), that of Hutchins and Somers (1992: 163-64) and that of Holmback et al (1996: 171). Taking these models into account, we picked out four criteria to assess machine translations: the comprehensibility or clarity of the translation, the precision or fidelity in terms of the source text, respect for the form of the source text (style, register, level of language used), and the usefulness of the translation produced. We added this last criterion as a way of measuring the usefulness of the raw translation when considered as a starting point for a human producing a final version. We constructed a questionnaire using these criteria which served as a basis for the qualitative evaluation. The questionnaire itself opens with a brief account of the experiment and of the reasons for carrying it out, as well as instructions on how to complete the questionnaire. Then the four criteria are set out in such a way that the test persons can give a score ranging from one to five four each criterion. As an example, the score 1 for comprehensibility indicates that the test person considers the translation incomprehensible, and 5 that he considers it perfectly clear. 2.3.2. The test persons For our results to have any validity, it was important that the test persons should have previous experience of technical translation. We thus recruited test persons amongst students of the translation school following a course on technical translation. It should be noted that the students were towards the end of their translation training (between two and four years of translation studies already successfully completed), and therefore far from being neophytes in translation work. They had French as their mother tongue, with English amongst their passive languages. We also recruited a second group of professional technical translators from CERN, which meant that the level of experience amongst our test persons was quite high. In all, 12 students and 8 professionals made up our group of twenty subjects. 2.3.3. Carrying out the experiment We divided the translations produced by Reverso into four groups: 1. Translations of the CL version of the Boeing sentences 2. Translations of the free version of the Boeing sentences 3. Translations of the controlled version of the Caterpillar sentences 4. Translations of the free version of the Caterpillar sentences. -6-  Each participant received one group, which contained five sentences. We then distributed the questionnaire. In order to preserve validity of the first criterion, the comprehensibility of the translation, the subjects should not have had access to the source text. Thus, the scoring of this first criterion was done before the source text was distributed. Subsequently, subjects scored precision, respect of the form of the original and the usefulness of the translation produced referring to the source text as they wished. The evaluation using the student group took place during a technical translation class which was part of their normal studies and lasted twenty minutes. The questionnaires were distributed to the technical translators of CERN at their work place. Given a total of 20 subjects, each translation of a given version was scored by three students and two translators. 3. RESULTS 3.4. Results of the quantitative evaluation Once the questionnaires were filled in, we counted the errors produced by the MT system in each sentence and calculated the total number of points for each sentence taking account of the weighting function explained earlier. The sentence with the highest number of points was the sentence with the most errors or with the most serious errors. Thus, according to this metric, the lower the score, the better the translation. Table 2 below shows an example of a sentence annotated with the errors it contains and their type. The free version of the text has been placed in parallel with the controlled version of the text in order to facilitate comparison. Table 3 gives a list of the acronyms used in annotating the errors. Table 4 gives a summary of the structure of table 2. Thus, table 4 tells us that line 1 of table 2 contains the original version of the sentence. Line 2 contains the translation produced by Reverso. Finally, line 3 contains the translation as corrected by the revisors (the reference translation). The parts of the sentence which have been corrected are in bold characters. The lower part of table 2 gives a summary of all the errors found in the sentence. These errors are multiplied by their weighting (See Table 1 for the weightings of each error); then the results are totalled to give an overall score for the sentence. Thus, direct comparison of the final score makes it easy to see whether the CL version has resulted in a better translation. -7-  Table 2  Free version  Controlled version  
We aim at carrying out an empirical study to clarify if texts checked with a controlled language (CL) checker are indeed more translatable than other texts which are not compliant with the CL rule set, evaluating thus the degree of success of the application of such a restricted language with regard to its machine oriented features. For such an evaluation we adapt the FEMTI-Framework (Hovy et al., 2002) to our needs and divide our evaluation in two parts: selection of resources for the evaluation of a CL, and evaluation of the CL. In this article only the findings and results of the first part are presented. In order to simulate a real context of work, we use the system MULTILINT, a sophisticated language checker developed by the Institut der Gesellschaft zur Förderung der Angewandten Informationsforschung e.V. an der Universität des Saarlandes (IAI). Furthermore, we use automatic translations made with a Machine Translation System. 
In both human-human and human-machine conversation, an important task for the participants is to identify the moment the other participant ﬁnishes speaking, giving them the possibility of taking over the turn in talk. In an RT experiment, consistent evidence was found for an intermediate stage in the planning and articulation of elicited minimal responses in the shape of early larynx and glottal movements in laryngograph recordings. Using a simple Response Time model, it is estimated that this intermediate stage occurs at approximately 2/3 of the integration-time needed for the articulation of a response. Impoverished intonation only stimuli were still adequate to elicit minimal responses, but a longer integration-time was required to initiate a response. Keywords Minimal Responses, Response Times, Dialogs, Intonation, Spoken Language Processing, Random Walk 
Do Italian speakers deaccent given information? In this study we examined word tokens repeatedly mentioning the same entity within a task-oriented dialogue. Contrary to what is expected in Germanic languages, results show that the vast majority of the repeated mentions are accented irrespective of their being hearer/discourse given or discourse segment given . Keywords: information structure; prosody. 
This work shows how a dialogue model can be represented as a Partially Observable Markov Decision Process (POMDP) with observations composed of a discrete and continuous component. The continuous component enables the model to directly incorporate a confidence score for automated planning. Using a testbed simulated dialogue management problem, we show how recent optimization techniques are able to find a policy for this continuous POMDP which outperforms a traditional MDP approach. Further, we present a method for automatically improving handcrafted dialogue managers by incorporating POMDP belief state monitoring, including confidence score information. Experiments on the testbed system show significant improvements for several example handcrafted dialogue managers across a range of operating conditions. 
The goal of dialogue management in a spoken dialogue system is to take actions based on observations and inferred beliefs. To ensure that the actions optimize the performance or robustness of the system, researchers have turned to reinforcement learning methods to learn policies for action selection. To derive an optimal policy from data, the dynamics of the system is often represented as a Markov Decision Process (MDP), which assumes that the state of the dialogue depends only on the previous state and action. In this paper, we investigate whether constraining the state space by the Markov assumption, especially when the structure of the state space may be unknown, truly affords the highest reward. In a simulation experiment conducted in the context of a dialogue system for interacting with a speech-enabled web browser, models under the Markov assumption did not perform as well as an alternative model which attempts to classify the total reward with accumulating features. We discuss the implications of the study as well as limitations. 
The lack of suitable training and testing data is currently a major roadblock in applying machine-learning techniques to dialogue management. Stochastic modelling of real users has been suggested as a solution to this problem, but to date few of the proposed models have been quantitatively evaluated on real data. Indeed, there are no established criteria for such an evaluation. This paper presents a systematic approach to testing user simulations and assesses the most prominent domain-independent techniques using a large DARPA Communicator corpus of human-computer dialogues. We show that while recent advances have led to signiﬁcant improvements in simulation quality, simple statistical metrics are still sufﬁcient to discern synthetic from real dialogues. 
 collections, in parallel with iterative reﬁnements on each  dialogue system component. Such an approach tends to  When building a new spoken dialogue appli-  be costly, and more automated methods for obtaining data  cation, large amounts of domain speciﬁc data  are critical for lowering barriers to deployment.  are required. This paper addresses the issue of generating in-domain training data when little or no real user data are available. The twostage approach taken begins with a data induction phase whereby linguistic constructs from out-of-domain sentences are harvested and integrated with artiﬁcially constructed in-domain phrases. After some syntactic and semantic ﬁltering, a large corpus of synthetically assembled user utterances is induced. The second stage involves sampling the synthetic corpus towards the goal of obtaining data that would be representative of the statistics of applicationspeciﬁc real user interactions. The sampling methods proposed employ an example-based generation framework, a simulated user model and information extracted from development data. Evaluation is conducted on recognition performance in a restaurant information domain. We show that word error rate can be reduced when limited amounts of real user training data are augmented with synthetic data derived by our methods.  This paper presents a methodology for synthesizing language model training data tailored to a spoken dialogue query-based application. A training corpus is derived ﬁrst in the absence of any in-domain real-user data, and secondly, in combination with a small development set. The objective is to develop automatic techniques for creating artiﬁcial data that would be well matched to real user data. Created by running user simulations and transforming a large out-of-domain corpus, the artiﬁcial data would ideally be used prior to initial deployment and subsequently as an enhancement to the small initially collected data set. In our approach, we seek to build a training corpus whose frequency distributions would realistically reﬂect those of user interactions with the dialogue system. Thus, the data must be similar in style to conversational speech encompassing repairs and disﬂuencies, while they should also maximize on diversity and coverage in terms of syntactic constructions. Moreover, at the sentence level (e.g., different types of queries), and at the class level (e.g., within-class statistics), distributions should closely approximate those of real user dialogues. To this end, our solution for inducing training data begins with a ﬁrst  stage of taking a restricted set of artiﬁcial domain data,  
The main problem when going from taskoriented dialogue systems to interactive restricted-domain question answering systems is that the lack of task structure prohibits making simplifying assumptions as in task-oriented dialogue systems. In order to address this issue, we propose a solution that combines representations based on keywords extracted from the user utterances with machine learning to learn the dialogue management function. More specifically, we propose to use Support Vector Machines to classify the dialogue state containing the extracted keywords in order to determine the next action to be taken by the dialogue manager. Much of the content selection for clariﬁcation question usually found in dialogue managers is moved to an instance-based generation component. The proposed method has the advantage that it does not rely on an explicit representation of task structure as is necessary for task oriented dialogue systems. 
In this paper we investigate the use of machine learning techniques to classify a wide range of non-sentential utterance types in dialogue, a necessary ﬁrst step in the interpretation of such fragments. We train different learners on a set of contextual features that can be extracted from PoS information. Our results achieve an 87% weighted f-score—a 25% improvement over a simple rule-based algorithm baseline. Keywords Non-sentential utterances, machine learning, corpus analysis 
We use n-gram techniques to identify dependencies between student affective states of certainty and subsequent tutor dialogue acts, in an annotated corpus of human-human spoken tutoring dialogues. We ﬁrst represent our dialogues as bigrams of annotated student and tutor turns. We next use χ2 analysis to identify dependent bigrams. Our results show dependencies between many student states and subsequent tutor dialogue acts. We then analyze the dependent bigrams and suggest ways that our current computer tutor can be enhanced to adapt its dialogue act generation based on these dependencies. 
Current dialogue systems are fairly poor in generating the wide range of clariﬁcation strategies as found in human-human dialogue. The overall aim of this work is to learn when and how to best employ diﬀerent types of clariﬁcation strategies in multimodal dialogue systems. This paper describes a framework for learning multimodal clariﬁcation strategies for an in-car MP3 music player dialogue system. The framework consists of three major parts. First we collect data on multimodal clariﬁcation strategies in a wizard-of-oz study. Second we extract feature in the stateaction space to learn an initial policy from this data. Third we specify a reward function to reﬁne that policy using extensions of existing evaluation schemes. 
This paper describes a multi-modal corpus of hand-annotated meeting dialogues that was designed for studying addressing behavior in face-to-face conversations. The corpus contains annotated dialogue acts, addressees, adjacency pairs and gaze direction. First, we describe the corpus design where we present the annotation schema, annotation tools and annotation process itself. Then, we analyze the reproducibility and stability of the annotation schema. Keywords: multimodal resources, coding schemes, reliability measures 
We present a set of annotations of hierarchical topic segmentations and action item subdialogues collected over 65 meetings from the ICSI and ISL meeting corpora, designed to support automatic meeting understanding and analysis. We describe an architecture for representing, annotating, and analyzing multi-party discourse, including: an ontology of multimodal discourse, a programming interface for that ontology, and an audiovisual toolkit which facilitates browsing and annotating discourse, as well as visualizing and adjusting features for machine learning tasks. 
We present results from an extensive empirical analysis of non-understanding errors and ten non-understanding recovery strategies, based on a corpus of dialogs collected with a spoken dialog system that handles conference room reservations. More specifically, the issues we investigate are: what are the main sources of non-understanding errors? What is the impact of these errors on global performance? How do various strategies for recovery from non-understandings compare to each other? What are the relationships between these strategies and subsequent user response types, and which response types are more likely to lead to successful recovery? Can dialog performance be improved by using a smarter policy for engaging the non-understanding recovery strategies? If so, can we learn such a policy from data? Whenever available, we compare and contrast our results with other studies in the literature. Finally, we summarize the lessons learned and present our plans for future work inspired by this analysis. 
This paper describes our recent work on mechanisms for error recovery in spoken dialogue systems. We focus on the acquisition of city names and dates in the ﬂight reservation domain. We are speciﬁcally interested in addressing the issue of acquiring out-of-vocabulary city names through a speak-and-spell mode subdialogue. In order to explore various dialogue strategies, we developed a user simulation system, which includes a conﬁgurable simulated user and a novel method of utterance generation. The latter utilizes a concatenative speech synthesizer, along with an existing corpus of dialogues, to produce a large variety of simulated inputs. The results from various simulated user conﬁgurations are presented, along with a discussion of how the simulated user facilitates the debugging of dialogue strategies and the discovery of situations unanticipated by the system developer. Keywords: spoken dialogue systems, user simulation, error recovery 
In this paper we discuss features to enhance the usability of a spoken dialogue system (SDS) in an automotive environment. We describe the tests that were performed to evaluate those features, and the methods used to assess the test results. One of these methods is a modiﬁcation of PARADISE, a framework for evaluating the performance of SDSs (Walker et al., 1998). We discuss its drawbacks for the evaluation of SDSs like ours, the modiﬁcations we have carried out, and the test results. 
When humans interact with spoken dialogue systems, parameters can be logged which quantify the flow of the interaction, the behavior of the user and the system, and the performance of individual system modules during the interaction. Although such parameters are not directly linked to the quality perceived by the user, they provide useful information for system development, optimization, and maintenance. This paper presents a collection of such parameters which are now considered to be recommended by the International Telecommunication Union (ITU-T) for evaluating telephone-based spoken dialogue services. As an initial evaluation, a case study is described which shows that the parameters correlate only weakly with subjective judgments, but that they still may be used for predicting quality with PARADISE-style regression models. 
In this paper, a discourse modeller for conversational spoken dialogue systems, called GALATEA, is presented. Apart from handling the resolution of ellipses and anaphora, it tracks the “grounding status” of concepts that are mentioned during the discourse, i.e. information about who said what when. This grounding information also contains concept confidence scores that are derived from the speech recogniser word confidence scores. The discourse model may then be used for concept-level error handling, i.e. grounding of concepts, fragmentary clarification requests, and detection of erroneous concepts in the model at later stages in the dialogue. 
Until recently, rigid and sometimes cumbersome structures, which underly dialog patterns considered manageable for achieving a given task in a controlled manner, proved to be a serious weakness of interactive systems. Through the introduction of the information state as a representation to control the evolving state of a dialog, substantial improvements were obtained, with elaborations made for information-seeking and task-oriented dialogs. For handling tutorial dialogs, more rigid schemas are still in frequent use, due to the diﬀerent requirements for this genre, which include more freedom on behalf of the human conversant due to limited pressure to understand a student’s dialog contributions in full detail. In order to enable more ﬂexible dialogs that also do justice to particularities of tutorial issues, we propose a mixed automaton- and information-state based model of dialogs. Capabilities of this model include elaborations to handle multiple task contributions in one turn, and abstractions from domainand task-speciﬁc reasoning. A consequence of this design is the concentration on issues related to dialog proper, which increases the system’s portability, a burning issue in the area of tutorial systems. 
We present a formal model of agent collaborative problem solving and use it to deﬁne a novel type of dialogue model. The model provides a rich structure for tracking dialogue state and supports a wide range of dialogue, including dialogue which contributes to interleaved planning and execution of domain goals. 
One-stage decoding as an integration of speech recognition and linguistic analysis into one probabilistic process is an interesting trend in speech research. In this paper, we present a simple one-stage decoding scheme that can be realised without the implementation of a specialized decoder, nor the use of complex language models. Instead, we reduce an HMMbased semantic analysis to the problem of deriving annotated versions of the conventional language model, while the acoustic model remains unchanged. We present experiments with the ATIS corpus (Price, 1990) in which the performance of the one-stage method is shown to be comparable with the traditional two-stage approach, while requiring a signiﬁcantly smaller increase in language model size. 
We present a speech-controllable MP3 player for embedded systems. In addition to basic commands such as ”next” or ”repeat” one main feature of the system is the selection of titles, artists, albums, genres, or composers by speech. We will describe the implemented dialog and discuss challenges for a real-world application. The ﬁndings and considerations of the paper easily extend to general audio media. 
Spoken dialogue systems (SDSs) can be used to operate devices, e.g. in the automotive environment. People using these systems usually have different levels of experience. However, most systems do not take this into account. In this paper we present a method to build a dialogue system in an automotive environment that adapts to the user’s experience with the system. We implemented the adaptation in a prototype and carried out exhaustive tests. Our usability tests show that adaptation increases both user performance and user satisfaction. 
As spoken dialogue systems mature, the need for rapid development tools increases. We describe such a tool that is currently being used for commercial design, specification and evaluation, and that is in the process of being developed into a complete case tool. 
This paper presents the second running prototype of a multimodal conversational edutainment system embodying 3D animated fairytale author Hans Christian Andersen. 
The approach presented here enables Japanese users with no knowledge of English or legal English to generate patent claims in English from a Japanese-only interface. It exploits the highly determined structure of patent claims and merges Natural Language Generation (NLG) and Machine Translation (MT) techniques and resources as realized in the AutoPat and PC-Transfer applications. Due to its tuned MT engine, the approach can be seen as a human-aided machine translation (HAMT) system circumventing major obstacles in full-scale Japanese-English MT. The approach is fully implemented on a large scale and will be commercially released in autumn 2005.
In this paper, we present a methodology for the development of interactive domain-tuned patent tools for generating patent claims in English from non-English interfaces. The methodology is based on a merger of an interactive English-to-English patent claim generator, AutoPat1 and any external MT engine that might be appropriate for a certain language. The translation procedure is reduced to translation words and phrases rather than a complex claim sentence. The approach has been successfully used in The J-E patent system 2 , a patent claim generator in English from a Japanese-only interface, and in Dan-Pat3, a similar tool for the Danish-English pair of languages. The two systems use different MT engines but feature similar overall architecture. The methodology is portable to other languages and MT engines.
It is well known that sentences in Japanese patents have long and complicated structures, especially necessary conditions and details. Here, patent sentences are analyzed and classified by pattern of modified relationships. Morphemes were first extracted using the famous morpheme analysis tool Chasen, and then the modified relations were extracted using the software Cabocha. Many modification mistakes were caused by long complicated structures, which required correction by humans. In the process of correction, the modification structure patterns were classified using about 200 sentences. This clarified the characteristics of Japanese patent sentences, and it is useful in machine translation of patent sentences.
A multilingual sense code may chart ``constant-sense connection paths'' across languages. A writer, not versed in any target language, may nonetheless proofread the sense for translation and edit it, to ensure that his meaning is conveyed as he wishes it, to other languages. A translation-ready format may be thus produced, to serve as a printing-press plate, for precise and automatic translation to any language, or to a plurality of languages. The translation-ready format may describe each word and the full document with a comprehensive code, which specifies the multilingual sense code and other relevant information about the word, in a standardized fashion, digitally, forming a unified, language-independent tagging system and a unified, language-independent lexicon.
Large-scale parallel corpus is extremely important for translation memory, example-based machine translation, and the support system to create English sentences. Organized collection or establishment of large-scale corpus is currently ongoing; however it is a difficult project in terms of copyrights as well as economic efficiency. To investigate general tendency of large-scale corpus helps to improve economical efficiency of parallel corpus collection as well as system establishment. In this study, therefore, the relationship between the scale of parallel corpus and the degree of correspondence is clarified, using parallel corpus for patents.
The paper describes some ways to save on knowledge acquisition when developing MT systems for patents by reducing the size of resources to be acquired, and creating intelligent software for knowledge handling and access speed. The approach is illustrated by knowledge acquisition and maintenance in the APTrans system for translating patent claims. Domain tuned resources are based on contrastive studies of multilingual patent documents and are handled by an electronic dictionary with a powerful user-friendly environment for acquisition, editing, browsing, defaulting and coherence proofing.
The domain dependence of translations of nouns in English-to-Japanese patent translation is examined using an automatic method for identifying major translations from a pair of language corpora in the same domain. The method calculates the ratio of the number of associated words of a target word that suggest each translation of the target word to the total number of associated words. This ratio indicates how major a translation is in a domain. Application of the method to a bilingual patent-abstract corpus indicates the necessity and effectiveness of dividing the patent domain into subdomains and adapting a bilingual dictionary to subdomains.
This paper describes a method for retrieving technical terms and finding their translation candidates from patent corpora. The method improves the reliability of bilingual seed words that measure similarity between a target word and its translation candidates. We conducted an experiment with PAJ (Patent Abstracts of Japan), which is a collection of bilingual patent abstracts written in Japanese and English. The experiment result shows that our method achieves a precision of 53.5{\%} and a recall of 75.4{\%}.
This paper addresses the workflow for terminology construction for Korean-English patent MT system. The workflow consists of the stage for setting lexical goals and the semi- automatic terminology construction stage. As there is no comparable system, it is difficult to determine how many terms are needed. To estimate the number of the needed terms, we analyzed 45,000 patent documents. Given the limited time and budget, we resorted to the semi-automatic methods to create the bilingual term dictionary in electronics domain. We will show that parenthesis information in Korean patent documents and bilingual title corpus can be successfully used to build a bilingual term dictionary.
Human translation is based on linguistic and extralinguistic knowledge. Despite promising pioneering advances, knowledge-based machine translation has remained a tempting vision. The bottleneck has been the engineering of sufficiently comprehensive bodies of relevant knowledge The Semantic Web offers opportunities for the gradual evolution of a global heterogeneous knowledge base. The immediate target has been the modelling of certain knowledge domains by practical ontologies. In the talk we will demonstrate the utilization of ontological knowledge indifferent crosslingual applications reaching from crosslingual document retrieval via crosslingual question answering to complex information services involving several crosslingual functionalities, including machine translation. We will then discuss the ramifications of this development and of the evolution of the World Wide Web for future directions in both statistical and rule-based machine translation.
Natural Language is considered the friendliest way of man-machine communication. However the implementation of natural language interfaces faces often the problem of lack of linguistic and world-knowledge, especially when the application domain is not very specific. This is exactly the case of Web-based applications, which aim to serve for retrieval of information in every-day areas of work. The recent Semantic Web activities had as consequence the development of large ontologies for a broad spectrum of domains, as well as of mechanisms for annotating the resources with semantic information. In this paper we present a new architecture aiming to bring together the advantages of natural language querying and the power of semantic W eb. W e will show also how described application can be easily adapted for other domains.
In this paper we give an overview of Semantic Web technologies and the impact of these ones for multilingual Web. We present a possible solution for improving the quality of on-line translation systems, using mechanisms and standards from Semantic Web. We focus on Example based machine translation and the automatization of the translation examples extraction by means of RDF-repositories.
The extraction of lexical sets from a corpus in Digital Signal Processing (DSP) has been detailed before on general sets, with direct ELT applications. In this contribution, a more specialized set is investigated to illustrate the possibility of actually using the results in more ``intelligent'' Text-Processing.
In this paper we present the actions we made to prepare an EBMT system to be integrated into the Semantic Web. We also described briefly the developed EBMT tool for translators.
This paper presents TTPlayer, a trace file analysis tool used to develop TransType, an innovative computer-aided translation system. We first discuss the context of the project and the design of the tracing tool. We show how it was used for discovering interesting patterns of use as well to guide further developments in the TT2 project.
In Thai language, the word boundary is not explicitly clear, therefore, word segmentation is needed to determine word boundary in Thai sentences. Many applications of Thai Language Processing require the word segmentation. Several approaches of Thai word segmentation such as maximal matching, longest matching and n-gram model do not take semantics into consideration. This paper presents a Thai word segmentation system using semantic corpus which is composed of four steps: generating all possible candidates, proper noun consideration, semantic tagging and semantic checking. The first three steps are conducted using a dictionary. Semantic checking is carried out on the basis of corpus-based approach. Finally, we assign the semantic scores to segmented words and select the ones that contain maximum semantic scores. In order to assign semantic scores, we use a Thai proper noun database and the semantic corpus derived from ORCHID corpus. This approach is more reliable than other approaches that do not take the meaning into consideration and performs the level of accuracy at 96-99{\%} depending on the characteristic of input and the dictionary used in the segmentation.
Existing syntactic ordered tree mining methods for extracting characteristic contents from text sets have two problems: 1) subtrees which are semantically the same but are different ordered trees fail to be considered equivalent, and 2) raw extracted subtrees can be difficult to understand. In order to avoid these problems, we have developed a method of transforming all ordered trees so that the ordered trees having the same meaning are considered equivalent. We have also developed a method of constructing Japanese texts from extracted subtrees, and evaluated the effectiveness of our methods as applied to syntactic tree mining.
The issue of translation divergence is an important research topic in the area of machine translation. An exhaustive study of the divergence issues in MT is necessary for their proper classification and resolution. In the literature on MT, scholars have examined the issue and have proposed ways for their classification and resolution (Dorr 1993, 1994). However, the topic still needs further exploration to identify different sources of translation divergence in different pairs of translation languages. In this paper, we discuss translation patterns between Hindi and English of different types of constructions with a view to identifying the potential topics of the translation divergences. We take Dorr{'}s (1993, 1994) classification of translation divergence as the base to examine the different topics of translation divergence in Hindi and English. The primary goal of the paper is to point out different types of translation divergences in Hindi and English MT that have not been discussed in the existing literature.
In the paper we present an outline of our approach to identify languages and encoding schemes in extremely large sets of multi-lingual documents. The large sets we are analyzing in our Language Observatory project [1] are formed by dozens of millions of text documents. In the paper we present an approach which allows us to analyze about 250 documents every second (about 20 million documents/day) on a single Linux machine. Using a multithread processing on a cluster of Linux servers we are able to analyze easily more than 100 million documents/day.
ki is an indeclinable element (particle) in Hindi which is used in multiple roles that have multiple mapping patterns in English. In one of its uses, ki functions as a clause complementizer and is mapped usually by that in declarative clauses and by various wh-words (such as what, why, where, how, etc.) in interrogative clauses. The contexts of these mappings are dependent on syntactic-semantic types of the clause. In its non-complementizer use, ki is used to denote various other functions such as coordinate conjunction, purpose and reason clause conjunction, yes-no question particle, etc. It is a difficult task to identify the different uses of ki and determine its multiple mapping patterns in the context of Hindi-English machine translation. A detailed linguistic analysis is needed to disambiguate the different contexts of ki in Hindi. In this paper, we examine the multiple uses and patterns of ki in Hindi and propose strategies for their identification and disambiguation for Hindi-English MT.
This paper describes a generalized translation memory system, which takes advantage of sentence level matching, sub-sentential matching, and pattern-based machine translation technologies. All of the three techniques generate translation suggestions with the assistance of word alignment information. For the sentence level matching, the system generates the translation suggestion by modifying the translations of the most similar example with word alignment information. For sub-sentential matching, the system locates the translation fragments in several examples with word alignment information, and then generates the translation suggestion by combining these translation fragments. For pattern-based machine translation, the system first extracts translation patterns from examples using word alignment information and then generates translation suggestions with pattern matching. This system is compared with a traditional translation memory system without word alignment information in terms of translation efficiency and quality. Evaluation results indicate that our system improves the translation quality and saves about 20{\%} translation time.
The present work describes a Phrasal Example Based Machine Translation system from English to Bengali that identifies the phrases in the input through a shallow analysis, retrieves the target phrases using a Phrasal Example base and finally combines the target language phrases employing some heuristics based on the phrase ordering rules for Bengali. The paper focuses on the structure of the noun, verb and prepositional phrases in English and how these phrases are realized in Bengali. This study has an effect on the design of the phrasal Example Base and recombination rules for the target language phrases.
This paper describes an attempt to recycle parts of the Czech-to-Russian machine translation system (MT) in the new Czech-to-English MT system. The paper describes the overall architecture of the new system and the details of the modules which have been added. A special attention is paid to the problem of named entity recognition and to the method of automatic acquisition of lexico-syntactic information for the bilingual dictionary of the system.
In this document we will describe a semi-automated process for creating elicitation corpora. An elicitation corpus is translated by a bilingual consultant in order to produce high quality word aligned sentence pairs. The corpus sentences are automatically generated from detailed feature structures using the GenKit generation program. Feature structures themselves are automatically generated from information that is provided by a linguist using our corpus specification software. This helps us to build small, flexible corpora for testing and development of machine translation systems.
This paper presents a strategy for detecting and using multi-word expressions in Statistical Machine Translation. Performance of the proposed strategy is evaluated in terms of alignment quality as well as translation accuracy. Evaluations are performed by using the Verbmobil corpus. Results from translation tasks from English-to-Spanish and from Spanish-to-English are presented and discussed.
This paper presents an online Thai-English MT system, called PARSITTE, which is an extension of PARSIT English-Thai one. We aim to assist foreigners and Thai in exchanging more easily their information. The system is a rule-based and Interlingua approach. To improve the system, we concentrate on pre-processing and rule analysis phases, which are considered necessary because of some specific problems of Thai language.
The use of n-gram metrics to evaluate the output of MT systems is widespread. Typically, they are used in system development, where an increase in the score is taken to represent an improvement in the output of the system. However, purchasers of MT systems or services are more concerned to know how well a score predicts the acceptability of the output to a reader-user. Moreover, they usually want to know if these predictions will hold across a range of target languages and text types. We describe an experiment involving human and automated evaluations of four MT systems across two text types and 23 language directions. It establishes that the correlation between human and automated scores is high, but that the predictive power of these scores depends crucially on target language and text type.
This paper reports the result of our experiment, the aim of which is to examine the efficiency of reading support systems such as a sentence-machine translation system, a word-machine translation system, and so on. Our evaluation method used in the experiment is able to handle the different reading support systems by assessing the usability of the systems, i.e., comprehension, reading speed, and effective speed. The result shows that the reading-speed procedure is able to evaluate the support systems as well as the comprehension-based procedure proposed by Ohguro (1993) and Fuji et al. (2001).
An aligned corpus is an important resource for developing machine translation systems. We consider suitable units for constructing the translation model through observing an aligned parallel corpus. We examine the characteristics of the aligned corpus. Long sentences are especially difficult for word alignment because the sentences can become very complicated. Also, each (source/target) word has a higher possibility to correspond to the (target/source) word. This paper introduces an alignment viewer a developer can use to correct alignment information. We discuss using the viewer on a patent parallel corpus because sentences in patents are often long and complicated.
Uyghur is one of the Turkic languages in the Altaic language family. We are developing a machine translation system to translate from English into Uyghur. As there are no previous researches devoted to machine translation between English and Uyghur and being short of related works that we could use as a base for our research, we noted that by making clear the morphological and syntactic similarities and differences between Japanese and Uyghur we can make use of the approaches and methods of English-Japanese machine translation to make faster progress in our research. In order to attain this goal, we have performed a comparative study on the Japanese and Uyghur grammars. In this paper, we describe the similarities as well as differences between Japanese and Uyghur in both levels of morphology and syntax and we give a brief description of our English-Uyghur transfer method to which we are aiming at applying our comparative study on Japanese and Uyghur grammars.
This paper investigates optimal ways to get maximal coverage from minimal input training corpus. In effect, it seems antagonistic to think of minimal input training with a statistical machine translation system. Since statistics work well with repetition and thus capture well highly occurring words, one challenge has been to figure out the optimal number of {``}new{''} words that the system needs to be appropriately trained. Additionally, the goal is to minimize the human translation time for training a new language. In order to account for rapid ramp-up translation, we ran several experiments to figure out the minimal amount of data to obtain optimal translation results.
This paper describes an approach to preprocess SMS text for Machine Translation. As SMS text behaves differently from normal written text and to reduce the tremendous effort required to customize or adapt the language model of the traditional translation system to handle SMS text style, normalization is performed to moderate the irregularities in English SMS text using a noisy channel model. A mapping model is used to model the three major problems in SMS text. They are (1) substitution of word using non-standard acronym, (2) insertion of flavour word, and (3) omission of auxiliary verb and subject pronoun. Experiment results show that with normalization before translation, the rejection rate of our English-to-Chinese SMS translation for broadcasting purpose is reduced by 15.5{\%}. We believe that the performance of normalization can be further improved with deeper linguistic processing.
This paper presents a look inside the ITC-irst large-vocabulary SMT system developed for the NIST 2005 Chinese-to-English evaluation campaign. Experiments on official NIST test sets provide a thorough overview of the performance of the system, supplying information on how single components contribute to the global performance. The presented system exhibits performance comparable to that of the best systems participating in the NIST 2002-2004 MT evaluation campaigns: on the three test sets, achieved BLEU scores are 26.35{\%}, 26.92{\%} and 28.13{\%}, respectively.
The paper describes the architecture and functionality of LTC Communicator, a software product from the Language Technology Centre Ltd, which offers an innovative and cost-effective response to the growing need for multilingual web based communication in various user contexts. LTC Communicator was originally developed to support software vendors operating in international markets facing the need to offer web based multilingual support to diverse customers in a variety of countries, where end users may not speak the same language as the helpdesk. This is followed by a short description of several additional application areas of this software for which LTC has received EU funding: The AMBIENT project carries out a market validation for multilingual and multimodal eLearning for business and innovation management, the EUCAM project tests multilingual eLearning in the automotive industry, including a major car manufacturer and the German and European Metal Workers Associations, and the ALADDIN project provides a mobile multilingual environment for tour guides, interacting between tour operators and tourists, with the objective of optimising their travel experience. Finally, a case study of multilingual email exchange in conjunction with web based product sales is described.
A survey of the machine translation systems that have been developed in India for translation from English to Indian languages and among Indian languages reveals that the MT softwares are used in field testing or are available as web translation service. These systems are also used for teaching machine translation to the students and researchers. Most of these systems are in the English-Hindi or Indian language-Indian language domain. The translation domains are mostly government documents/reports and news stories. There are a number of other MT systems that are at their various phases of development and have been demonstrated at various forums. Many of these systems cover other Indian languages beside Hindi.
This paper describes a cellular-telephone-based text-to-text translation system developed at Transclick, Inc. The application translates messages bi-directionally in English, French, German, Italian, Spanish and Portuguese. This paper describes design features uniquely suited to hand-held-device based translation systems. In particular, we discuss some of the usability conditions unique to this type of application and present strategies for overcoming usability obstacles encountered in the design phase of the product.
We propose a method to alleviate the problem of referential granularity for Japanese zero pronoun resolution. We use dictionary definition sentences to extract {`}representative{'} arguments of predicative definition words; e.g. {`}arrest{'} is likely to take police as the subject and criminal as its object. These representative arguments are far more informative than {`}person{'} that is provided by other valency dictionaries. They are auto-extracted using both Shallow parsing and Deep parsing for greater quality and quantity. Initial results are highly promising, obtaining more specific information about selectional preferences. An architecture of zero pronoun resolution using these representative arguments is described.
This paper claims that constructing a dictionary using bilingual pairs obtained from parallel corpora needs not only correct alignment of two noun phrases but also judgment of its appropriateness as an entry. It specifically addresses the latter task, which has been paid little attention. It demonstrates a method of selecting a suitable entry using Support Vector Machines, and proposes to regard as the features the common and the different parts between a current translation and a new translation. Using experiment results, this paper examines how selection performances are affected by the four ways of representing the common and the different parts: morphemes, parts of speech, semantic markers, and upper-level semantic markers. Moreover, we used n-grams of the common and the different parts of above four kinds of features. Experimental result found that representation by morphemes marked the best performance, F-measure of 0.803.
We introduce a light-weight interlingua for a cross-language document retrieval system in the medical domain. It is composed of equivalence classes of semantically primitive, language-specific subwords which are clustered by interlingual and intralingual synonymy. Each subword cluster represents a basic conceptual entity of the language-independent interlingua. Documents, as well as queries, are mapped to this interlingua level on which retrieval operations are performed. Evaluation experiments reveal that this interlingua-based retrieval model outperforms a direct translation approach.
This paper proposes a novel Example-Based Machine Translation (EBMT) method based on Tree String Correspondence (TSC) and statistical generation. In this method, the translation examples are represented as TSC, which consists of three parts: a parse tree in the source language, a string in the target language, and the correspondences between the leaf nodes of the source language tree and the substrings of the target language string. During the translation, the input sentence is first parsed into a tree. Then the TSC forest is searched out if it is best matched with the parse tree. The translation is generated by using a statistical generation model to combine the target language strings in the TSCs. The generation model consists of three parts: the semantic similarity between words, the word translation probability, and the target language model. Based on the above method, we build an English-to-Chinese Machine Translation (ECMT) system. Experimental results indicate that the performance of our system is comparable with that of the state-of-the-art commercial ECMT systems.
This paper proposes a method for a machine translation (MT) system to automatically select and learn translation words, which suit the user{'}s tastes or document fields by using a monolingual corpus manually compiled by the user, in order to achieve high-quality translation. We have constructed a system based on this method and carried out experiments to prove the validity of the proposed method.
Rule-Based Machine Translation (RBMT) [1] approach is a major approach in MT research. It needs linguistic knowledge to create appropriate rules of translation. However, we cannot completely add all linguistic rules to the system because adding new rules may cause a conflict with the old ones. So, we propose a memory based approach to improve the translation quality without modifying the existing linguistic rules. This paper analyses the translation problems and shows how this approach works.
The main objective of our project is to extract clinical information from thoracic radiology reports in Portuguese using Machine Translation (MT) and cross language information retrieval techniques. To accomplish this task we need to evaluate the involved machine translation system. Since human MT evaluation is costly and time consuming we opted to use automated methods. We propose an evaluation methodology using NIST/BLEU and METEOR algorithms and a controlled medical vocabulary, the Unified Medical Language System (UMLS). A set of documents are generated and they are either machine translated or used as evaluation references. This methodology is used to evaluate the performance of our specialized Portuguese-English translation dictionary. A significant improvement on evaluation scores after the dictionary incorporation into a commercial MT system is demonstrated. The use of UMLS and automated MT evaluation techniques can help the development of applications on the medical domain. Our methodology can also be used on general MT research for evaluating and testing purposes.
When conducting market research on machine translation, we research the volume of sales continuously in order to determine the scale of the machine translation market in Japan. We have officially announced these figures every year. Furthermore, since 2003, we administered questionnaires regarding the Web translation.
This paper describes one approach to document authoring and natural language generation being pursued by the Summer Institute of Linguistics in cooperation with the University of Maryland, Baltimore County. We will describe the tools provided for document authoring, including a glimpse at the underlying controlled language and the semantic representation of the textual meaning. We will also introduce The Bible Translator{'}s Assistant{\copyright} (TBTA), which is used to elicit and enter target language data as well as perform the actual text generation process. We conclude with a discussion of the usefulness of this paradigm from a Bible translation perspective and suggest several ways in which this work will benefit the field of computational linguistics.
We are constricting a Japanese-Chinese parallel corpus, which is a part of the NICT Multilingual Corpora. The corpus is general domain, of large scale of about 40,000 sentence pairs, long sentences, annotated with detailed information and high quality. To the best of our knowledge, this will be the first annotated Japanese-Chinese parallel corpus in the world. We created the corpus by selecting Japanese sentences from Mainichi Newspaper and then manually translating them into Chinese. We then annotated the corpus with morphological and syntactic structures and alignments at word and phrase levels. This paper describes the specification in human translation and detailed information annotation, and the tools we developed in the project. The experience we obtained and points we paid special attentions are also introduced for share with other researches in corpora construction.
We collected a corpus of parallel text in 11 languages from the proceedings of the European Parliament, which are published on the web. This corpus has found widespread use in the NLP community. Here, we focus on its acquisition and its application as training data for statistical machine translation (SMT). We trained SMT systems for 110 language pairs, which reveal interesting clues into the challenges ahead.
We describe a method of constructing Thai WordNet, a lexical database in which Thai words are organized by their meanings. Our methodology takes WordNet and LEXiTRON machine-readable dictionaries into account. The semantic relations between English words in WordNet and the translation relations between English and Thai words in LEXiTRON are considered. Our methodology is operated via WordNet Builder system. This paper provides an overview of the WordNet Builder architecture and reports on some of our experience with the prototype implementation.
Motivated by the fact that automatic analysis of language crucially depends on semantic constituent detection and attachment resolution, we present our work on the problem of generating and linking semantically relatable sets (SRS). These sets are of the form <entity1 entity2> or <entity1 function-word entity2> or <function-word entity>, where the entities can be single words or more complex sentence parts (such as embedded clauses). The challenge lies in finding the components of these sets, which involves solving prepositional phrase (PP) and clause attachment problems, and empty pronominal (PRO) determination. Use is made of (i) the parse tree of the sentence, (ii) the subcategorization frames of lexical items, (iii) the lexical properties of the words and (iv) lexical resources like the WordNet and the Oxford Advanced Learners’ Dictionary (OALD). The components within the sets and the sets themselves are linked using the semantic relations of an interlingua for machine translation called the Universal Networking Language (UNL). The work forms part of a UNL based MT system, where the source language is analysed into semantic graphs and target language is generated from these graphs. The system has been tested on the Penn Treebank, and the results indicate the promise and effectiveness of our approach. Keywords: Semantically Relatable Sets, Syntactic and Semantic Constituents, Interlingua Based MT, Parse Trees, Lexical Properties, Subcategorization Frames, Penn Treebank. 
In this paper we describe and evaluate different statistical models for the task of realization ranking, i.e. the problem of discriminating between competing surface realizations generated for a given input semantics. Three models are trained and tested; an n-gram language model, a discriminative maximum entropy model using structural features, and a combination of these two. Our realization component forms part of a larger, hybrid MT system.
In this paper, we report on the results of a full-size evaluation campaign of various MT systems. This campaign is novel compared to the classical DARPA/NIST MT evaluation campaigns in the sense that French is the target language, and that it includes an experiment of meta-evaluation of various metrics claiming to better predict different attributes of translation quality. We first describe the campaign, its context, its protocol and the data we used. Then we summarise the results obtained by the participating systems and discuss the meta-evaluation of the metrics used.
The PLATO machine translation (MT) evaluation (MTE) research program has as a goal the systematic development of a predictive relationship between discrete, welldefined MTE metrics and the specific information processing tasks that can be reliably performed with output. Traditional measures of quality, informed by the International Standards for Language Engineering (ISLE), namely, clarity, coherence, morphology, syntax, general and domain-specific lexical robustness, and named-entity translation, as well as a DARPAinspired measure of adequacy are its core. For robust validation, indispensable for refinement of tests and guidelines, we measure inter-rater reliability on the assessments. Here we report on our results, focusing on the PLATO Clarity and Coherence assessments, and we discuss our method for iteratively refining both the linguistic metrics and the guidelines for applying them within the PLATO evaluation paradigm. Finally, we discuss reasons why kappa might not be the best measure of interrater agreement for our purposes, and suggest directions for future investigation. 
Automatic word alignment is an important technology for extracting translation knowledge from parallel corpora. However, automatic techniques cannot resolve this problem completely because of variances in translations. We therefore need to investigate the performance potential of automatic word alignment and then decide how to suitably apply it. In this paper we first propose a lexical knowledge-based approach to word alignment on a Japanese-Chinese corpus. Then we evaluate the performance of the proposed approach on the corpus. At the same time we also apply a statistics-based approach, the well-known toolkit GIZA++, to the same test data. Through comparison of the performances of the two approaches, we propose a multi-aligner, exploiting the lexical knowledge-based aligner and the statistics-based aligner at the same time. Quantitative results confirmed the effectiveness of the multi-aligner.
In this paper, we present the Thot toolkit, a set of tools to train phrase-based models for statistical machine translation, which is publicly available as open source software. The toolkit obtains phrase-based models from word-based alignment models; to our knowledge, this functionality has not been offered by any publicly available toolkit. The Thot toolkit also implements a new way for estimating phrase models, this allows to obtain more complete phrase models than the methods described in the literature, including a segmentation length submodel. The toolkit output can be given in different formats in order to be used by other statistical machine translation tools like Pharaoh, which is a beam search decoder for phrase-based alignment models which was used in order to perform translation experiments with the generated models. Additionally, the Thot toolkit can be used to obtain the best alignment between a sentence pair at phrase level.
In the present communication-based society, no natural language seems to have been left untouched by the trends of code-mixing. For different communicative purposes, a language uses linguistic codes from other languages. This gives rise to a mixed language which is neither totally the host language nor the foreign language. The mixed language poses a new challenge to the problem of machine translation. It is necessary to identify the {``}foreign{''} elements in the source language and process them accordingly. The foreign elements may not appear in their original form and may get morphologically transformed as per the host language. Further, in a complex sentence, a clause/utterance may be in the host language while another clause/utterance may be in the foreign language. Code-mixing of Hindi and English where Hindi is the host language, is a common phenomenon in day-to-day language usage in Indian metropolis. The scenario is so common that people have started considering this a different variety altogether and calling it by the name Hinglish. In this paper, we present a mechanism for machine translation of Hinglish to pure (standard) Hindi and pure English forms.
The South Asian languages are well-known for their replicative words. In these languages, words of almost all the grammatical categories can occur in their reduplicative form. Hindi is one such language which is quite rich in having various types of replicative words in its lexicon. The traditional grammars and some of the research works have discussed the topic to some extent, particularly from the point of view of their descriptions and classifications. However, a detailed study of the topic becomes significant in view of the complexity involved in handling of such replicative words in the area of natural language processing, particularly for machine translation. In this paper, we discuss different types of replicative words in Hindi and their syntactic and semantic characteristics to formulate rules and strategies to identify their multiple functions and mapping patterns in English for machine translation from Hindi to English.
In the LOGON machine translation system where semantic transfer using Minimal Recursion Semantics is being developed in conjunction with two existing broad-coverage grammars of Norwegian and English, we motivate the use of a grammar-specific semantic interface (SEM-I) to facilitate the construction and maintenance of a scalable translation engine. The SEM-I is a theoretically grounded component of each grammar, capturing several classes of lexical regularities while also serving the crucial engineering function of supplying a reliable and complete specification of the elementary predications the grammar can realize. We make extensive use of underspecification and type hierarchies to maximize generality and precision.
This paper addresses a customization process of a Korean-English MT system for patent translation. The major customization steps include terminology construction, linguistic study, and the modification of the existing analysis and generation-module. T o our knowledge, this is the first worth-mentioning large-scale customization effort of an MT system for Korean and English. This research was performed under the auspices of the MIC (Ministry of Information and Communication) of Korean government. A prototype patent MT system for electronics domain was installed and is being tested in the Korean Intellectual Property Office.
In this paper, we present evidence that providing users of a speech to speech translation system for emergency diagnosis (MedSLT) with a tool that helps them to learn the coverage greatly improves their success in using the system. In MedSLT, the system uses a grammar-based recogniser that provides more predictable results to the translation component. The help module aims at addressing the lack of robustness inherent in this type of approach. It takes as input the result of a robust statistical recogniser that performs better for out-of-coverage data and produces a list of in-coverage example sentences. These examples are selected from a defined list using a heuristic that prioritises sentences maximising the number of N-grams shared with those extracted from the recognition result.
This paper describes the evaluation of the FAME interlingua-based speech-to-speech translation system for Catalan, English and Spanish. This system is an extension of the already existing NESPOLE! that translates between English, French, German and Italian. This article begins with a brief introduction followed by a description of the system architecture and the components of the translation module including the Speech Recognizer, the analysis chain, the generation chain and the Speech Synthesizer. Then we explain the interlingua formalism used, called Interchange Format (IF). We show the results obtained from the evaluation of the system and we describe the three types of evaluation done. We also compare the results of our system with those obtained by a stochastic translator which has been independently developed over the course of the FAME project. Finally, we conclude with future work.
Example-based machine translation (EBMT) systems, so far, rely on heuristic measures in retrieving translation examples. Such a heuristic measure costs time to adjust, and might make its algorithm unclear. This paper presents a probabilistic model for EBMT. Under the proposed model, the system searches the translation example combination which has the highest probability. The proposed model clearly formalizes EBMT process. In addition, the model can naturally incorporate the context similarity of translation examples. The experimental results demonstrate that the proposed model has a slightly better translation quality than state-of-the-art EBMT systems.
Statistical machine translation relies heavily on the available training data. However, in some cases, it is necessary to limit the amount of training data that can be created for or actually used by the systems. To solve that problem, we introduce a weighting scheme that tries to select more informative sentences first. This selection is based on the previously unseen n-grams the sentences contain, and it allows us to sort the sentences according to their estimated importance. After sorting, we can construct smaller training corpora, and we are able to demonstrate that systems trained on much less training data show a very competitive performance compared to baseline systems using all available training data.
We describe a method for automatically rating the machine translatability of a sentence for various machine translation (MT) systems. The method requires that the MT system can bidirectionally translate sentences in both source and target languages. However, it does not require reference translations, as is usual for automatic MT evaluation. By applying this method to every component of a sentence in a given source language, we can automatically identify the machine-translatable and non-machinetranslatable parts of a sentence for a particular MT system. We show that the parts of a sentence that are automatically identified as nonmachine-translatable provide useful information for paraphrasing or revising the sentence in the source language, thus improving the quality of the final translation.
We propose a simplified Level Of Detail (LOD) algorithm to learn phrase translation for statistical machine translation. In particular, LOD learns unknown phrase translations from parallel texts without linguistic knowledge. LOD uses an agglomerative method to attack the combinatorial explosion that results when generating candidate phrase translations. Although LOD was previously proposed by (Setiawan et al., 2005), we improve the original algorithm in two ways: simplifying the algorithm and using a simpler translation model. Experimental results show that our algorithm provides comparable performance while demonstrating a significant reduction in computation time.
Most statistical machine translation systems use phrase-to-phrase translations to capture local context information, leading to better lexical choice and more reliable local reordering. The quality of the phrase alignment is crucial to the quality of the resulting translations. Here, we propose a new phrase alignment method, not based on the Viterbi path of word alignment models. Phrase alignment is viewed as a sentence splitting task. For a given spitting of the source sentence (source phrase, left segment, right segment) find a splitting for the target sentence, which optimizes the overall sentence alignment probability. Experiments on different translation tasks show that this phrase alignment method leads to highly competitive translation results.
In this paper we present the ongoing work at RWTH Aachen University for building a speech-to-speech translation system within the TC-Star project. The corpus we work on consists of parliamentary speeches held in the European Plenary Sessions. To our knowledge, this is the first project that focuses on speech-to-speech translation applied to a real-life task. We describe the statistical approach used in the development of our system and analyze its performance under different conditions: dealing with syntactically correct input, dealing with the exact transcription of speech and dealing with the (noisy) output of an automatic speech recognition system. Experimental results show that our system is able to perform adequately in each of these conditions.
This paper presents a practical approach to statistical machine translation (SMT) based on syntactic transfer. Conventionally, phrase-based SMT generates an output sentence by combining phrase (multiword sequence) translation and phrase reordering without syntax. On the other hand, SMT based on tree-to-tree mapping, which involves syntactic information, is theoretical, so its features remain unclear from the viewpoint of a practical system. The SMT proposed in this paper translates phrases with hierarchical reordering based on the bilingual parse tree. In our experiments, the best translation was obtained when both phrases and syntactic information were used for the translation process.
This paper describes a statistical machine translation system that uses a translation model which is based on bilingual n-grams. When this translation model is log-linearly combined with four specific feature functions, state of the art translations are achieved for Spanish-to-English and English-to-Spanish translation tasks. Some specific results obtained for the EPPS (European Parliament Plenary Sessions) data are presented and discussed. Finally, future research issues are depicted.
In Statistical Machine Translation, the use of reordering for certain language pairs can produce a significant improvement on translation accuracy. However, the search problem is shown to be NP-hard when arbitrary reorderings are allowed. This paper addresses the question of reordering for an Ngram-based SMT approach following two complementary strategies, namely reordered search and tuple unfolding. These strategies interact to improve translation quality in a Chinese to English task. On the one hand, we allow for an Ngram-based decoder (MARIE) to perform a reordered search over the source sentence, while combining a translation tuples Ngram model, a target language model, a word penalty and a word distance model. Interestingly, even though the translation units are learnt sequentially, its reordered search produces an improved translation. On the other hand, we allow for a modification of the translation units that unfolds the tuples, so that shorter units are learnt from a new parallel corpus, where the source sentences are reordered according to the target language. This tuple unfolding technique reduces data sparseness and, when combined with the reordered search, further boosts translation performance. Translation accuracy and efficency results are reported for the IWSLT 2004 Chinese to English task.
In (Mellebeek et al., 2005), we proposed the design, implementation and evaluation of a novel and modular approach to boost the translation performance of existing, wide-coverage, freely available machine translation systems, based on reliable and fast automatic decomposition of the translation input and corresponding composition of translation output. Despite showing some initial promise, our method did not improve on the baseline Logomedia1 and Systran2 MT systems. In this paper, we improve on the algorithm presented in (Mellebeek et al., 2005), and on the same test data, show increased scores for a range of automatic evaluation metrics. Our algorithm now outperforms Logomedia, obtains similar results to SDL3 and falls tantalisingly short of the performance achieved by Systran.
This paper investigates the relationship between the amount of the rules and the performance of the rule-based machine translation system. We keep adding more rules into the system and observe successive changes of the translation quality. Evaluations on translation quality reveal that the more the rules, the better the translation quality. A linear regression analysis shows that a positive linear relationship exists between the translation quality and the amount of the rules. We use this linear model to make prediction and test the prediction with newly developed rules. Experimental results indicate that the linear model effectively predicts the possible performance that the rule-based machine translation system may achieve with more rules added.
We evaluate several orthographic word similarity measures in the context of bitext word alignment. We investigate the relationship between the length of the words and the length of their longest common subsequence. We present an alternative to the longest common subsequence ratio (LCSR), a widely-used orthographic word similarity measure. Experiments involving identification of cognates in bitexts suggest that the alternative method outperforms LCSR. Our results also indicate that alignment links can be used as a substitute for cognates for the purpose of evaluating word similarity measures.
This paper proposes an approach to improve statistical word alignment with the boosting method. Applying boosting to word alignment must solve two problems. The first is how to build the reference set for the training data. We propose an approach to automatically build a pseudo reference set, which can avoid manual annotation of the training set. The second is how to calculate the error rate of each individual word aligner. We solve this by calculating the error rate of a manually annotated held-out data set instead of the entire training set. In addition, the final ensemble takes into account the weights of the alignment links produced by the individual word aligners. Experimental results indicate that the boosting method proposed in this paper performs much better than the original word aligner, achieving a large error rate reduction.
The Open A.I. Kit implements the major components of Statistical Machine Translation as an accessible, extendable Software Development Kit with broad applicability beyond the field of Machine Translation. The high-level system design policies of the kit embrace the Open Source development model to provide a modular architecture and interface, which may serve as a basis for collaborative research and development for endeavors in Artificial Intelligence.
We present the current status of development of an open architecture for the translation from Spanish into Basque. The machine translation architecture uses an open source analyser for Spanish and new modules mainly based on finite-state transducers. The project is integrated in the OpenTrad initiative, a larger government funded project shared among different universities and small companies, which will also include MT engines for translation among the main languages in Spain. The main objective is the construction of an open, reusable and interoperable framework. This paper describes the design of the engine, the formats it uses for the communication among the modules, the modules reused from other project named Matxin and the new modules we are building.
By the time Machine Translation Summit X is held in September 2005, our group will have released an open-source machine translation toolbox as part of a large government-funded project involving four universities and three linguistic technology companies from Spain. The machine translation toolbox, which will most likely be released under a GPL-like license includes (a) the open-source engine itself, a modular shallow-transfer machine translation engine suitable for related languages and largely based upon that of systems we have already developed, such as interNOSTRUM for Spanish{---}Catalan and Traductor Universia for Spanish{---}Portuguese, (b) extensive documentation (including document type declarations) specifying the XML format of all linguistic (dictionaries, rules) and document format management files, (c) compilers converting these data into the high-speed (tens of thousands of words a second) format used by the engine, and (d) pilot linguistic data for Spanish{---}Catalan and Spanish{---}Galician and format management specifications for the HTML, RTF and plain text formats. After describing very briefly this toolbox, this paper aims at exploring possible consequences of the availability of this architecture, including the community-driven development of machine translation systems for languages lacking this kind of linguistic technology.
This paper introduces the Multilingual Information Service System being implemented for the Beijing Olympics. Multilingual machine translation is an important component in this system. This real world application asks for advanced as well as mature and proven technologies, where MT is challenged. However, by appropriately choosing domain and scenario, current MT technologies are successfully integrated in the pilot system. Future applications ask MT to have better performance in readability, lexicon coverage and more efficient. Multilinguality support and fast language adaptation is highly desired by such real world systems.
Accurate and timely information on global public health issues is key to being able to quickly assess and respond to emerging health risks around the world. The Public Health Agency of Canada has developed the Global Public Health Intelligence Network (GPHIN). Information from GPHIN is provided to the WHO, international governments and non-governmental organizations who can then quickly react to public health incidents. GPHIN is a secure Internet-based {``}early warning{''} system that gathers preliminary reports of public health significance on a {``}real-time{''} basis, 24 hours a day, 7 days a week. This unique multilingual system gathers and disseminates relevant information on disease outbreaks and other public health events by monitoring global media sources such as news wires and web sites. This monitoring is done in eight languages with machine translation being used to translate non-English articles into English and English articles into the other languages. The information is filtered for relevancy by an automated process which is then complemented by human analysis. The output is categorized and made accessible to users. Notifications about public health events that may have serious public health consequences are immediately forwarded to users. GPHIN employs a {``}best-of-breed{''} approach when it comes to the selection of the machine translation {`}engines{'}. This philosophy ensures that the quality of the machine translation is the best available for whatever language pair selected. It also imposes some unique integration and operational problems. GPHIN has a broad scope. It tracks events such as disease outbreaks, infectious diseases, contaminated food and water, bio-terrorism and exposure to chemicals, natural disasters, and issues related to the safety of products, drugs and medical devices. GPHIN is managed by Health Canada{'}s Centre for Emergency Preparedness and Response (CEPR), which was created in July 2000 to serve as Canada{'}s central coordinating point for public health security. It is considered a centre of expertise in the area of civic emergencies including natural disasters and malicious acts with health repercussions. CEPR offers a number of practical supports to municipalities, provinces and territories, and other partners involved in first response and public health security. This is achieved through its network of public health, emergency health services, and emergency social services contacts.
In the last decade, the statistical approach has found widespread use in machine translation both for written and spoken language and has had a major impact on the translation accuracy. This paper will cover the principles of statistical machine translation and summarize the progress made so far.
Since 1994, China{'}s HTRDP machine translation evaluation has been conducted for five times. Systems of various translation directions between Chinese, English, Japanese and French have been tested. Both human evaluation and automatic evaluation are conducted in HTRDP evaluation. In recent years, the evaluation was organized jointly with NICT of Japan. This paper introduces some details of this evaluation.
MANOS (Multilingual Application Network for Olympic Services) project. aims to provide intelligent multilingual information services in 2008 Olympic Games. By narrowing down the general language technology, this paper gives an overview of our new work on Phrase-Based Statistical Machine Translation (PBT) under the framework of the MANOS. Starting with the construction of large scale Chinese-English corpus (sentence aligned) and introduction four methods to extract phrases, The promising results from PBT systems lead us to confidences for constructing a high-quality translation system and harmoniously integrate it into MANOS platform.
This paper reports on measures to improve the quality of MT systems, by using a hybrid system architecture which adds corpus-based and statistical components to an existing rule-based system backbone. The focus is on improving the accuracy of the dictionary resources.
Example-Based Machine Translation (EBMT) systems have typically operated on individual sentences without taking into account prior context. By adding a simple reweighting of retrieved fragments of training examples on the basis of whether the previous translation retrieved any fragments from examples within a small window of the current instance, translation performance is improved. A further improvement is seen by performing a similar reweighting when another fragment of the current input sentence was retrieved from the same training example. Together, a simple, straightforward implementation of these two factors results in an improvement on the order of 1.0{--}1.6{\%} in the BLEU metric across multiple data sets in multiple languages.
Corpus-based MT systems that analyse and generalise texts beyond the surface forms of words require generation tools to re-generate the various internal representations into valid target language (TL) sentences. While the generation of word-forms from lemmas is probably the last step in every text generation process at its very bottom end, token-generation cannot be accomplished without structural and morpho-syntactic knowledge of the sentence to be generated. As in many other MT models, this knowledge is composed of a target language model and a bag of information transferred from the source language. In this paper we establish an abstracted, linguistically informed, target language model. We use a tagger, a lemmatiser and a parser to infer a template grammar from the TL corpus. Given a linguistically informed TL model, the aim is to see what need be provided from the transfer module for generation. During computation of the template grammar, we simultaneously build up for each TL sentence the content of the bag such that the sentence can be deterministically reproduced. In this way we control the completeness of the approach and will have an idea of what pieces of information we need to code in the TL bag.
This paper presents a generalization technique that induces translation templates from given translation examples by replacing differing parts in these examples with typed variables. Since the type of each variable is also inferred during the learning process, each induced template is associated with a set of type constraints. The type constraints that are associated with a translation template restrict the usage of that translation template in certain contexts in order to avoid some of wrong translations. The types of variables are induced using the type lattices designed for both source language and target language. The proposed generalization technique has been implemented as a part of an EBMT system.
The METIS-II project is an example-based machine translation system, making use of minimal resources and tools for both source and target language, making use of a target-language (TL) corpus, but not of any parallel corpora. In the current paper, we discuss the view of our team on the general philosophy and outline of the METIS-II system.
We describe our use of RSS news feeds to quickly assemble a parallel English-Japanese corpus. Our method is simpler than other web mining approaches, and it produces a parallel corpus whose quality, quantity, and rate of growth are stable and predictable.
The example-based approach to MT is becoming increasingly popular. However, such is the variety of techniques and methods used that it is difficult to discern the overall conception of what example-based machine translation (EBMT) is and/or what its practitioners conceive it to be. Although definitions of MT systems are notoriously complex, an attempt is made to define EBMT in contrast to other MT architectures (RBMT and SMT).
We present a study we conducted to build a repository storing associations between simple dependency treelets in a source language and their corresponding phrases in a target language. To assess the impact of this resource in EBMT, we used the repository to compute coverage statistics on a test bitext and on a n-best list of translation candidates produced by a standard phrase-based decoder.
We designed, implemented and assessed an EBMT system that can be dubbed the {``}purest ever built{''}: it strictly does not make any use of variables, templates or training, does not have any explicit transfer component, and does not require any preprocessing of the aligned examples. It uses a specific operation, namely proportional analogy, that implicitly neutralises divergences between languages and captures lexical and syntactical variations along the paradigmatic and syntagmatic axes without explicitly decomposing sentences into fragments. In an experiment with a test set of 510 input sentences and an unprocessed corpus of almost 160,000 aligned sentences in Japanese and English, we obtained BLEU, NIST and mWER scores of 0.53, 8.53 and 0.39 respectively, well above a baseline simulating a translation memory.
In the present article, a hybrid approach is proposed for implementing a machine translation system using a large monolingual corpus coupled with a bilingual lexicon and basic NLP tools. In the first phase of the METIS system, a source language (SL) sentence, after being tagged, lemmatised and translated by a flat lemma-to-lemma lexicon, was matched against a tagged and lemmatised target language (TL) corpus using a pattern matching algorithm. In the second phase, translations are generated by combining sub-sentential structures. In this paper, the main features of the second phase are discussed while the system architecture and the corresponding translation approach are presented. The proposed methodology is illustrated with examples of the translation process.
We describe a novel approach to machine translation that combines the strengths of the two leading corpus-based approaches: Phrasal SMT and EBMT. We use a syntactically informed decoder and reordering model based on the source dependency tree, in combination with conventional SMT models to incorporate the power of phrasal SMT with the linguistic generality available in a parser. We show that this approach significantly outperforms a leading string-based Phrasal SMT decoder and an EBMT system. We present results from two radically different language pairs, and investigate the sensitivity of this approach to parse quality by using two distinct parsers and oracle experiments. We also validate our automated BLEU scores with a small human evaluation.
Users of sign languages are often forced to use a language in which they have reduced competence simply because documentation in their preferred format is not available. While some research exists on translating between natural and sign languages, we present here what we believe to be the first attempt to tackle this problem using an example-based (EBMT) approach. Having obtained a set of English{--}Dutch Sign Language examples, we employ an approach to EBMT using the {`}Marker Hypothesis{'} (Green, 1979), analogous to the successful system of (Way {\&} Gough, 2003), (Gough {\&} Way, 2004a) and (Gough {\&} Way, 2004b). In a set of experiments, we show that encouragingly good translation quality may be obtained using such an approach.
This paper proposes a method for integrating example-based and rule-based machine translation systems with statistical methods. It extends a greedy decoder for statistical machine translation (SMT), which searches for an optimal translation by using SMT models starting from a decoder seed, i.e., the source language input paired with an initial translation hypothesis. In order to reduce local optima problems inherent in the search, the outputs generated by multiple translation engines, such as rule-based (RBMT) and example-based (EBMT) systems, are utilized as the initial translation hypotheses. This method outperforms conventional greedy decoding approaches using initial translation hypotheses based on translation examples retrieved from a parallel text corpus. However, the decoding of multiple initial translation hypotheses is computationally expensive. This paper proposes a method to select a single initial translation hypothesis before decoding based on a machine learning approach that judges the appropriateness of multiple initial translation hypotheses and selects the most confident one for decoding. Our approach is evaluated for the translation of dialogues in the travel domain, and the results show that it drastically reduces computational costs without a loss in translation quality.
The paper reports an Example based Machine Translation System for translating News Headlines from English to Bengali. The input headline is initially searched in the Direct Example Base. If it cannot be found, the input headline is tagged and the tagged headline is searched in the Generalized Tagged Example Base. If a match is obtained, the tagged headline in Bengali is retrieved from the example base, the output Bengali headline is generated after retrieving the Bengali equivalents of the English words from appropriate dictionaries and then applying relevant synthesis rules for generating the Bengali surface level words. If some named entities and acronyms are not present in the dictionary, transliteration scheme is applied for obtaining the Bengali equivalent. If a match is not found, the tagged input headline is analysed to identify the constituent phrase(s). The target translation is generated using English-Bengali phrasal example base, appropriate dictionaries and a set of heuristics for Bengali phrase reordering. If the headline still cannot be translated using example base strategy, a heuristic translation strategy will be applied. Any new input tagged headline along with its translation by the user will be inserted in the tagged Example base after generalization.
For the METIS-II project (IST, start: 10-2004 {--} end: 09-2007) we are working on an example-based machine translation system, making use of minimal resources and tools for both source and target language, i.e. making use of a target language corpus, but not of any parallel corpora. In the current paper, we present the results of the first experiments with our approach (CCL) within the METIS consortium : the translation of noun phrases from Dutch to English, using the British National Corpus as a target language corpus. Future research is planned along similar lines for the sentence as is presented here for the noun phrase.
The Web has triggered many adjustments in many fields. It also has had a strong impact on the genre repertoire. Novel genres have already emerged, e.g. blog and FAQs. Presumably, other new genres are still in formation, because the Web is still fluid and in constant change. In this paper we present an experiment that explores the possibility of automatically detecting the emerging textual patterns that are slowly taking shape on the Web. Emerging textual patterns can develop into novel Web genres or novel text types in the near future. The experimental set up includes a collection of unclassified web pages, two sets of features and the use of cluster analysis. Results are encouraging and deserve further investigation.
In the past decade, Latent Semantic Analysis (LSA) was used in many NLP approaches with sometimes remarkable success. However, its abilities to express semantic relatedness were not yet systematically investigated. This is the aim of our work, where LSA is applied to a general text corpus (German newspaper), and for a test vocabulary, the lexical relations between a test word and its closest neighbours are analysed. These results are compared to the results from a collocation analysis.
This paper describes the development of Finnish linguistic resources for use in MedSLT, an Open Source medical domain speech-to-speech translation system. The paper describes the collection of medical Finnish corpora, the creation of a Finnish grammar by adapting the original English grammar, the composition of a domain specific Finnish lexicon and the definition of interlingua to Finnish mapping rules for multilingual translation. It is shown that Finnish can be effectively introduced into the existing MedSLT framework and that despite the differences between English and Finnish, the Finnish grammar can be created by manual adaptation from the original English grammar. Regarding further development, the initial evaluation results of English-Finnish speech-to-speech translation are encouraging.
We present an overview of MedSLT, a medium-vocabulary medical speech translation system, focussing on the representational issues that arise when translating temporal and causal concepts. Although flat key/value structures are strongly preferred as semantic representations in speech understanding systems, we argue that it is infeasible to handle the necessary range of concepts using only flat structures. By exploiting the specific nature of the task, we show that it is possible to implement a solution which only slightly extends the representational complexity of the semantic representation language, by permitting an optional single nested level representing a subordinate clause construct. We sketch our solutions to the key problems of producing minimally nested representations using phrase-spotting methods, and writing cleanly structured rule-sets that map temporal and phrasal representations into a canonical interlingual form.
Sentiment analysis dealing with the identification and evaluation of opinions towards a topic, a company, or a product is an essential task within media analysis. It is used to study trends, determine the level of customer satisfaction, or warn immediately when unfavourable trends risk damaging the image of a company. In this paper we present an issues monitoring system which, besides text categorization, also performs an extensive sentiment analysis of online news and newsgroup postings. Input texts undergo a morpho-syntactic analysis, are indexed using a thesaurus and are categorized into user-specific classes. During sentiment analysis, sentiment expressions are identified and subsequently associated with the established topics. After presenting the various components of the system and the linguistic resources used, we describe in detail SentA, its sentiment analysis component, and evaluate its performance.
Multicomponent Tree Adjoining Grammars (MCTAG) is a formalism that has been shown to be useful for many natural language applications. The definition of MCTAG however is problematic since it refers to the process of the derivation itself: a simultaneity constraint must be respected concerning the way the members of the elementary tree sets are added. Looking only at the result of a derivation (i.e., the derived tree and the derivation tree), this simultaneity is no longer visible and therefore cannot be checked. I.e., this way of characterizing MCTAG does not allow to abstract away from the concrete order of derivation. Therefore, in this paper, we propose an alternative definition of MCTAG that characterizes the trees in the tree language of an MCTAG via the properties of the derivation trees the MCTAG licences.
In this paper we assess pause effects corresponding to comma, semicolon, colon and the ones that are not related to any punctuation marks, all of them within sentences. Thus, through the analysis of a corpus of approximately 17 hours of recording, carried out by a female professional speaker (native) of the Brazilian Portuguese language, we observe a large proportion of pauses without punctuation (61.3{\%}). Besides, our data reinforce the presence of topic-comment structures in reading. The results here presented with respect to pause and punctuation are consistent with several studies about this theme.
This paper reports an overview of the evaluation campaign results of the IWSLT 2005 workshop1. The BTEC corpus, which consists of typical travel domain phrases, was used. Data for the five language pairs Arabic/Chinese/Japanese/Korean to English and English to Chinese was prepared. To study how much the amount of the training data and how much different training and decoding approaches contribute to the performance, a supplied data and an unrestricted data track were introduced. In addition, translation results were evaluated not only for text input but also speech recognition output. 19 systems from 17 organizations participated in the evaluation. All machine translation results were evaluated using automatic evaluation metrics. The most popular track, translating text form Chinese to English, was graded by 3 humans in terms of Fluency, Adequacy and Meaning Maintenance. The correlation between automatic evaluation metrics and human judgment was examined. 1. Introduction The Consortium for Speech Translation Advanced Research (C-STAR) had been formed in the 1990s to study and develop techniques for speech-to-speech translation. To further this research C-STAR members have been jointly constructing a multilingual spoken language corpus, the basic travel expression corpus (BTEC, [1]). In 2004 the International Workshop on Spoken Language Translation (IWSLT) was started in order to enable the exchange of knowledge among researchers working on speech-to-speech translations and to create an opportunity to enhance the machine translation (MT) systems by comparing technologies on the same test bed [2]. IWSLT 2005 extended over the 2004 evaluation campaign by translating the output of automatic speech recognition (ASR) systems as well. Speech-to-speech translation systems are designed as systems combining an MT system with automatic speech recognition technology. This introduces additional difficulties into the translation process, 
We propose a novel statistical machine translation decoding algorithm for speech translation to improve speech translation quality. The algorithm can translate the speech recognition word lattice, where more hypotheses are utilized to bypass the misrecognized single-best hypotheses. We also show that a speech recognition conﬁdence measure, implemented by posterior probability, is effective to improve speech translation. The proposed techniques were tested in a Japanese-to-English speech translation task. The experimental results demonstrate the improved speech translation performance by the proposed techniques. 1. Introduction Most current speech translation systems have a cascaded structure: a speech recognition component followed by a machine translation component. Usually only the singlebest outcome of speech recognition is used in the machine translation component. Due to the inevitable errors of speech recognition, speech translation cannot achieve the same level of translation performance as that achieved by perfect text input. To overcome the weakness in speech translation, several architectures have been proposed so far. [1] proposed a coupling structure to combine automatic speech recognition and statistical machine translation. [2] used a uniﬁed structure where the maximum entropy approach is proposed to build entire speech translation system models. [3] and [4] implemented the integrated structure by means of ﬁnite-state network, and a comparison with the cascaded structure was made. Strictly speaking, this approach does not use the statistical speech translation structure [1]. In this work we used the speech recognition word lattice as the output of speech recognition and the input of machine translation. While in this structure the speech recognition component and the machine translation component are sequentially connected, more hypotheses are stored in the word lattice than the single-best structure; complementary information, such as the acoustic model  and language model scores instantiated by posterior probability, was used in the machine translation component to enhance translation performance. In the ﬁeld of statistical machine translation, the famous, early models are the IBM models [5], using Bayes rule to convert P (e|f ) into P (e)P (f |e). The IBM Models 1 through 5 introduced various models for P (f |e) with increasing complexity. Another popular model is called an “HMM” model [6], which adds alignment probability in the basis of IBM Model 1. Recently a direct modeling of P (e|f ) in the maximum entropy framework, the log-linear model, has been proved effective [7]. This model can integrate a number of features log-linearly. Hence, we use the statistical log-linear model as the translation model in this work. We implemented a new lattice translation decoding algorithm specialized for speech translation. In the decoding we used a two-pass search strategy, graph-based plus A*. For the ﬁrst graph search, we integrated features from IBM Model 1 into the log-linear model while IBM Model 4’s features were integrated in the second A* search. We invented a new method to minimize the size of the raw lattice generated by the speech recognizer to reduce the decoding complexity. We found these techniques effective for improving speech translation quality. We also found that sentence posterior probability of speech recognition was very useful for further improving speech translation. A signiﬁcant translation improvement was achieved by ﬁltering low-conﬁdence hypotheses based on the posterior probability in the word lattice. The remaining sections are organized as follows. Section 2 introduces our speech translation models and structure, then Section 3 provides detailed descriptions of the decoding algorithm of the word lattice translation. In Section 4 we describe a lattice reduction method to reduce the computations. We describe the sentence posterior probability approach for choosing hypotheses in Section 5. Section 6 presents our experimental results and a detailed analysis, and Section 7 gives our discussions and conclusions.  source utterance X ASR  word lattice  target translation E WLT  Although the WLT translation model is derived in the form of Eq. 3, we actually used a more advanced model, namely a feature based log-linear models, formalized as:  Figure 1: Speech Translation Framework  2. Proposed Speech Translation Structure The proposed speech translation system is illustrated in Figure 1. It consists of two major components: an automatic speech recognition (ASR) module and a word lattice translation (WLT) module. The interface between the two components is a recognition word lattice. The task of speech translation, in the case of Japaneseto-English translation, can be modeled as to ﬁnd the target English sentence, E, given a source Japanese spoken utterance, X, such that the probability, P (E|X), is maximized. If the intermediate output of ASR is deﬁned as J, we can get the following formula according to [4]:  E = arg max P (E|X) E  = arg max  P (J, E)P (X|J)  (1)  E  J  where P (X|J) is the ASR acoustic model; P (J, E), the joint model of source and target languages. For searching the best E, two methods are used in [4]: serial structure and integrated stochastic ﬁnite-state transducer(SFST). The ﬁrst used the single-best results of ASR, and the second used a ﬁnite-state transducer. We use the word lattice in our work, by which the implementation indicated by the model 1 is approximated in two steps:  • First, use the ASR component to generate a word lattice G. Only the top ranked hypotheses with an ASR scores higher than a threshold, T H, are kept in the word lattice:  G = {J|P (J)P (X|J) > T H}  (2)  • Second, use the WLT component to ﬁnd output which maximizes: < E, J >= arg max {P (E)P (X|J)P (J|E)} E,J J ∈ G (3)  The combination of Eq. 2 and Eq. 3 is an approximation of Eq. 1, where we assume E and X is independent to derive Eq. 2; and summation is replaced by maximization to derive Eq. 3. Meanwhile, a J, producing the best E, is also obtained in WLT despite that the goal of speech translation is only for E.  E = arg max{λ0 log Ppp(J |X) + λ1 log Plm(E) E  + λ2 log Plm(POS(E)) + λ3 log N (Φ|E)  + λ4 log P (Φ0|E) + λ5 log T (J|E)  + λ6 log D(E, J)}  (4)  where we deﬁned seven features represented by the following. (a)ASR hypothesis posterior probability, Ppp(J|X). We used the posterior probability instead of the acoustic model score since the acoustic model score has a large dynamic range and difﬁcult to normalize. The posterior probability is calculated as:  P (J|X) =  P (X|J)P (J) Ji P (X|Ji)P (Ji)  (5)  where the summation is made over all hypotheses in the word lattice, G. Ji is a hypothesis in the word lattice. (b)Word sequence target language model, Plm(E). (c)Partof-speech sequence target language models, Plm(POS(E)). (d)Fertility model, N (Φ|E) represents the probability of the English word, e, generating φ words. (e)NULL translation model, P (Φ0|E) is the probability of inserting a NULL word. (f)Lexicon Model, T (J|E) is the probability of the word, j, in the Japanese source sentence being translated into the corresponding word, e, in the English target sentence. (g)Distortion model, D(E, J) in- dicates the alignment probability of the source and target sentence,(J, E). Eq. 4 is a logarithmic extension of Eq. 1 except that the translation model P (J|E) is extended by IBM model 4 [5] and the acoustic feature is replaced by the posterior probability.  3. Word lattice translation – WLT Word lattice translation is much more complicated than text translation. In contrast to text translation where a single source sentence is known, there is no single source sentence for word lattice translation but a lattice containing multiple hypotheses. Which hypothesis is the best one to be translated is unknown before the decoding is completed. We use the graph+A* decoding approach for the word lattice translation. This approach has been used for text translation by [8]. We extend the approach to speech translation in this work. We adopted this approach because it can keep more hypotheses in a compact structure. The graph+A* decoding is a two-pass decoding. The ﬁrst pass uses a simple model to generate a word graph to save the most likely hypotheses. It amounts to  converting a source language word lattice (SWL) into a target language word graph (TWG). Edges in the SWL are aligned to some edges in the TWG. The second pass uses a complicated model to output the best hypothesis by traversing the target word graph. We describe the two-pass WLT algorithm in the following two sections. 3.1. First pass — from SWL to TWG The bottom in Fig. 2 shows an example of a translation word graph, which corresponds to the recognition word lattice in the top. Each edge in the TWG is a target language word being a translation of a source word in the SWL, either from word translations provided by the lexical models or fertility expansion by the fertility models. Some edges that have the same structure are merged into a node. The node has one element indicating the source word coverage up to the current node. The coverage is a binary vector with size equal to the number of edges in the SWL, indicating the number of translated source edges. If the j-th source word was translated, the j-th element is 1, otherwise it equals to 0. If the node covers all the edges of a full path in the SWL, this node connects to the last node, the end, in the TWG. There are two main operations in expanding a node into edges: DIRECT and ALIGN. DIRECT extends the hypothesis with a target word by translating an uncovered source word. The target word is determined based on current target N -gram context and possible translations of the uncovered source word. ALIGN extends the hypothesis by aligning one more uncovered source word to the current node, where the target word is a translation of multiple source words, increasing fertilities of the target word. The edge is not extended if the resulted hypothesis does not correspond to any hypothesis in the SWL. If the node has covered a full path in the SWL, this node is connected to the end node. When there are no nodes available for possible extension, the conversion is completed. A simple illustration of conversion algorithm is shown in Algorithm 1. The whole process equals the growing of a graph. The graph can be indexed in time slices because the new nodes are created based on the old nodes of the last nearest time slice. New nodes are created by DIRECT or ALIGN to cover the uncovered source edge and connect to the old nodes. The new generated nodes are sorted and merged in the graph buffer if they share the same structure: the same coverage, the same translations and the same N -gram sequence. If the node covers a full hypothesis in the SWL, the node connects to the end. If no nodes need to be expanded, the conversion ﬁnishes. In the ﬁrst pass, we incorporate a simpler translation model into the log-linear model, only the lexical model, IBM model 1. The ASR posterior probability Ppp are calculated by partial hypothesis from the start to the current.  Algorithm 1 Conversion Algorithm from SWL to TWG  1: Initialize graph buffer G[0]=0; t=0  2: DO  3: FOR EACH node n=0,1,..., #(G[t]) DO  4: IF (n cover A FULL PATH) NEXT  5:  FOR EACH edge l=0,1,...,#(EDGES) DO  6:  IF (n cover l) NEXT  7:  IF (n not cover ANY SWL PATH) NEXT  8:  generate new node and push to G[t+1]  9:  merge and prune nodes in G[t+1]  10: t= t+1  11:WHILE (G[t] is empty)  Ppp uses the highest value among all the ASR hypotheses under the current context. The ﬁrst pass serves to keep the most likely hypotheses in the translation word graph, and the second pass as a reﬁned search that uses advanced models. 3.2. Second pass — by an A* search to ﬁnd the best output from the TWG An A* search traverses the TWG generated in last section, and this is the best ﬁrst approach. All partial hypotheses generated are pushed into a priority queue with the top hypothesis popping ﬁrst out of the queue for the next extension. To execute the A* search, the hypothesis score, D(h, n), of a node n is evaluated in two parts: the forward score, F (h, n), and the heuristic estimation, H(h, n), D(h, n) = F (h, n) + H(h, n). The calculation of F (h, n) begins from the start node and accumulates all nodes’ scores belonging to the hypothesis until the current node, n. The H(h, n) is deﬁned as the accumulated maximum probability of the models from the end node to the current node n. In the second pass we incorporated IBM Model 4’s features into the log-linear model. However, we cannot use IBM Model 4 directly because the calculations of the two models, P (Φ0|E) and D(E, J), require the source sentence, but unknown in fact. Hence, the probability of P (Φ0|E) and D(E, J) cannot be calculated precisely in decoding. Our method to ﬁx this problem is to use the maximum over all possible hypotheses. For the above two models, we calculated the scores for all the possible ASR hypotheses under the current context. The maximum value was used as the model’s probability. 4. Minimizing the SWL Because we use HMM-based ASR to generate the raw SWL, the same word identity can be recognized repeatedly in slightly different frames. As a result, the same  j1 j0 j2  j3 j4 j5  j6 j7  e2 10100000 e5 10100100 e7 10100101  s  e0 10000000 e6  e6 00000010 e0  e2 10000010  10100010 e4  10101010  /s  e1 11000010 e3 11010010  Figure 2: Source language word lattice (top) and target language word graph (bottom)  word identity may appear in more than one edge. Direct conversion from SWL to TWG causes duplicated computation and explosion of the TWG space. On the other hand, while the raw SWL contains hundreds of hypotheses, the top N -best hypotheses are among the most signiﬁcant, which are only a small portion of all hypotheses. We can reduce the size of the raw SWL by cutting off all other hypotheses except the top N -best without information loss. In reducing the size of SWL, we follow one rule: the TWG is the translation counterpart of the SWL if and only if any full path in the TWG is a translation of a full path in the SWL. We use the following steps to downsize the raw SWL. From the raw SWL we generate N -best hypotheses in a sequence of edge numbers. We list the word IDs of all the edges in the hypotheses, remove the duplicate words, and index the remainders with new edge IDs. The number of new edges is less than that in the raw SWL. Next, we replace the edge sequence in each hypothesis with a new edge ID. If more than one edge shares the same word ID in one hypothesis, we add a new edge ID for the word again and replace the edge with the new ID. Finally, we generate a new word lattice with a new word list as its edges, consisting of the N -best hypotheses only. The raw SWL becomes the downsized SWL. The downsized SWL is much smaller than the raw SWL. In fact, in our experiments the word lattice is reduced by 50% on average. Fig. 3 shows an example of lattice downsizing. The word IDs are shown in the parentheses. After downsizing, one hypothesis is removed. In fact, the downsized SWL is just the N -best ASR hypotheses with new assigned edge IDs. In this paper we use lattice-hypothesis to indicate the quantities of hypotheses in the lattice, deﬁned as the number of hypotheses used to construct the TWG in the downsized SWL. It is a more suitable metric than lattice density for the downsized SWL because after lattice minimization, the downsized SWL saves only the signiﬁcant N -best ASR hypotheses regardless of the  density of the raw SWL. 5. Selection of hypotheses by conﬁdence measure (CM) ﬁltering As described above, the downsized SWL stores N ASR hypotheses. All the hypotheses can ﬁnd a counterpart in the TWG after the conversion if they are not removed by histogram and threshold pruning. The posterior probability of a hypothesis can determine whether this hypothesis is used in the TWG. In general, the hypotheses with the lowest posterior probability are the least likely to be used in the WLT module. They are most likely pruned in the earlier stage of the decoding process. In the experiments we found removing the hypotheses with extremely low posterior probability in the TWG by hard decisions can improve speech translation. We found that using posterior probability as a conﬁdence measure to ﬁlter low conﬁdence hypotheses achieved better results. For all the hypotheses in the SWL, we used Eq. 5 to compute each hypothesis posterior probability. We then compared each one’s posterior probability with that of the single-best hypothesis, P0, the highest posterior probability. If it exceeds a threshold, P0/T , where T is a conﬁdence factor, the hypothesis is used in WLT, otherwise removed. By applying a conﬁdence measure, WLT automatically selects the hypotheses that are conversed into the TWG. Hence, for a given SWL, the number of hypotheses for translation in WLT is determined by the conﬁdence measure (CM) ﬁltering. 6. Experiments 6.1. BTEC Database & Model Training The Japanese/English bilingual data used in this study were from the Basic Travel Expression Corpus (BTEC) [9], consisting of commonly used sentences published in travel guidebooks and tour conversations. In our experiments we used the BTEC training data to  Raw SWL j1(Wb) j0(Wa) j2(Wb)  j3(Wc) j6(Wd) j4(We) j7(Wd) j5(Wc)  Downsized SWL  l2(Wc)  l0(Wa) l1(Wb)  l4(Wd)  l3(We)  Hypotheses j0 j1 j3 j6 j0 j2 j4 j6 j0 j2 j5 j7 l0 l1 l2 l4 l0 l1 l3 l4  Transfer rules j0 -> l0(Wa) j1,j2 -> l1(Wb) j3,j5 -> l2(Wc) j4 -> l3(We) j6,j7 -> l4(Wd)  Figure 3: An example of word lattice reduction  train the models, the BTEC1 test data #1 as the development data for the parameter optimization of the log-linear model, and the BTEC1 test data #2 for evaluation. The training data contains 468,595 sentences. The develop and test data have 510 and 508 sentences respectively. The adopted ASR is HMM-based, implemented by triphone models with 2,100 states and 25 dimensional MFCC features. A multiclass word bigram and a word trigram language models were used in the ASR with a lexicon of 47,000 words. All the LMs and the feature models of the translation models were trained by the BTEC training data. The automatic evaluation metric, BLEU, was used for evaluating our translation quality. It was calculated by the downloadable tool (version v11a) 1. We use 16 reference sentences for each utterance, created by human translators In the development phase, we optimized the λs of the log-linear model by the approach in [7], for maximizing the BLEU score of the translation results of the development data given the 16 references for each utterance. 6.2. The effect of CM ﬁltering In the experiments the ASR system output the raw lattice. Its performance for one of our test data (BTEC test #2) is shown in Fig. 4, where the word and sentence accuracy for the single-best (lattice-hypothesis=1) recognition are around 93% and 79%, respectively. They increase to 96% and 87% at lattice-hypothesis=20, showing the potential improvement for speech translation. The required lattice for WLT was generated by the lattice reduction approach described in section 4. We set the number of ASR hypotheses to 100 when we created the downsized SWL. The downsized SWL was then 
This paper describes our recent work on integrating speech recognition and machine translation for improving speech translation performance. Two approaches are applied and their performance are evaluated in the workshop of IWSLT 2005. The ﬁrst is direct N-best hypothesis translation, and the second, a pseudo-lattice decoding algorithm for translating word lattice, can dramatically reduce computation cost incurred by the ﬁrst approach. We found in the experiments that both of these approaches could improve speech translation signiﬁcantly. 1. Introduction At least two components are involved in speech to speech translation: automatic speech recognizer and machine translation. Unlike plain text translation, the performance of speech translation may be degraded due to the speech recognition errors. Several approaches have been proposed to compensate for the loss of recognition accuracy. [1] proposed N -best recognition hypothesis translation, which translates all the top N hypotheses and then outputs the highest scored translations by ways of weighing all the translations using a log-linear model. [2] used word lattices to improve translations. [3] used ﬁnite state transducers (FST) to convey the features from acoustic analysis and source target translation models. All these approaches realized an integration between speech recognition modules and machine translation modules so that information from speech recognition, such as acoustic model score and language model score, can be exploited in the translation module to achieve the maximum performance over the single-best translation. In the ﬁeld of machine translation, the phrase-based statistical machine translation approach is widely accepted at present. The related literature can be found in [4] [5]. But previously, word-based statistical machine translation, pioneered by IBM Models 1 to 5 [6], were used widely. In the evaluation, we used both the word-based and phrase-based systems. However, the purpose of this work is not to compare performance of word-based with phrase-based translation. We used two system for different translations. The phrase-based SMT is used in  recognized target  text  best translation  utterance  translation  X  J  N 1  E  NxK 1  E  ASR  SMT  Rescore  Figure 1: N-best hypothesis translation  Chinese-English translation while the word-based SMT is used in Japanese-English translation. In this paper we describe two speech translation structures. The ﬁrst is a direct N-best hypothesis translation system that uses a text-based machine translation engine to translate each of the hypotheses, and the results are rescored by a log-linear model. The second is a pseudolattice translation system, merging the N -best hypotheses into a compact pseudo-lattice which serves as an input to our proposed decoding algorithm for lattice translation. This algorithm runs much faster than the ﬁrst approach. In the following, Section 2 describes the direct N-best hypothesis translation. Section 3 describes the pseudolattice translation. Section 4 introduces the experimental process and translation results in the evaluation of IWSLT2005. Section 5 presents our conclusions concerning the techniques, and some ﬁnal remarks are given. 2. Direct N-best hypothesis translation The structure of the direct N-best hypothesis translation is illustrated in Fig. 1, where there are three modules, an automatic speech recognizer(ASR), a statistical machine translation(SMT), and a log-linear model rescore(Rescore). This structure is used in Chinese to English translation in the evaluation. 2.1. ASR: automatic speech recognition ASR functions as a decoder to retrieve the source transcript from input speech. The input is a speech signal, X. The output is a source sentence, J. The mechanism of ASR is based on HMM pattern recognition. The acoustic models and language models of the source language are required in the decoding. Because speech recognition errors are unavoidable, ASR outputs multiple hypotheses, the top N-best, to increase the accuracy.  2.2. SMT: statistical machine translation The SMT module is to translate the source language, J, into target language, E. A phrase-based statistical machine translation decoder was used in the evaluation. The decoding process is carried out in three steps: First, a word graph is created by beam-search where phrase translation models and trigram models are used to extend beams. Second, A* search is used to ﬁnd the top N-best paths in the word graph. Finally, long-range(> 3) language models are used to rescore the N-best candidates and output the best one. In order to collect source-target translation pairs, we used GIZA++ to do bi-directional alignment, similar to [5]. In one direction alignment, one source word is aligned to multiple target words; In the other direction, one target word is aligned to multiple source words. Finally, the bidirectional alignment are merged and the phrase pairs are extracted from the overlapping alignments. The translation probability of translation pairs were computed by relative frequency, counting the co-occurrences of the pairs in the training data.  2.3. Rescoring: log-linear model rescoring  Loglinear models are applied to rescore the translations which are produced by SMT. The model integrates features from both ASR and SMT. We used three features from ASR and 10 features from SMT. The log-linear model used in our speech translation process, P (E|X), is  PΛ(E|X) =  exp(  M i=1  λifi(X,  E))  E exp(  M i=1  λifi(X, E  ))  Λ = {λM 1 }  (1)  Features from ASR include acoustic model score, source  language model score, and posterior probability calcu-  lated as below.  P (X|Jk)P (Jk) Ji P (X|Ji)P (Ji)  (2)  Features from SMT include target word language model  score, class language model score, target phrase language  model, phrase translation model, distortion model, length  model (deﬁned as the number of words in the target),  deletion model (deﬁned as the NULL word alignment),  lexicon model (obtained from GIZA++), and size model  (representing the size of jump between two phrases.)  For the optimal value of λ, our goal is to minimize  the translation “distortion” between the reference transla-  tions, R, and the translated sentences, E.  λM 1 = optimize D(E, R)  (3)  where E = {E1, · · · , EL} is a set of translations of all utterances. The translation El of the l-th utterance is produced by Eq. 1.  Let R = {R1, · · · , RL} be the set of translation references for all utterances. Human translators paraphrased 16 reference sentences for each utterance, i.e., Rl contains 16 reference candidates for the l-th utterance. D(E, R) is a translation “distortion”, that is, an objective translation assessment. A basket of automatic evaluation metrics can be used, such as BLEU, NIST, mWER, mPER and GTM. Because the distortion function, D(E, R), is not a smoothed function, we used Powell’s search method to ﬁnd a solution [7]. The experimental results in [1] have shown that minimizing the translation distortion in development data is an effective method to improve translation qualities of test data. 3. Pseudo-lattice translation The N -best hypothesis translation improved speech translation signiﬁcantly, as found in [1]. However, the approach is inefﬁcient, computationally expensive and time consuming. We proposed a new decoding algorithm, pseudo-lattice decoding, to improve on the direct N-best translation. This approach can also translate the N-best hypotheses, and the processing time is shorten dramastically because the same word IDs appearing in the N-best hypotheses are translated fewer times than the direct N-best translation. We start from the word lattice minimization produced by ASR to describe the approach. 3.1. Minimizing the source word lattice(SWL) Because we use HMM-based ASR to generate the raw source word lattice(SWL), the same word ID can be recognized repeatedly in slightly different frames. As a result, the same word ID may appear in multiple edges in the SWL. Hence, when N -best hypotheses are generated from the word lattice, the same word ids may appear in multiple hypotheses. Fig. 2 shows an example of lattice downsizing. The word IDs are shown in the parentheses. We use the following steps to minimize the raw SWL by removing the repeated edges. First, from the raw SWL we generate N best hypotheses as a sequence of edge numbers. We list the word IDs of all the edges in the hypotheses, remove the duplicate words, and index the remainders with new edge IDs. The number of new edges is fewer than that in the raw SWL. Next, we replace the edge sequence in each hypothesis with a new edge ID. If more than one edge shares the same word ID in one hypothesis, we add a new edge ID for the word again and replace the edge with the new ID. Finally, we generate a new word lattice with a new word list as its edges, consisting of the N -best hypotheses only. The raw SWL becomes the downsized SWL, which is much smaller than the raw SWL. On av-  erage, the word lattice is reduced by 50% in our experiments. As shown in Fig. 2, one hypothesis is removed after minimization. Sometimes the downsized SWL cannot form a lattice, but the N -best ASR hypotheses with newly assigned edge IDs. So we denote the downsized SWL as a pseudolattice. 3.2. Pseudo-lattice decoding algorithm We use beam search followed by a A* search in pseudolattice translation. This approach has been used in text translation by [8]. We extend the approach to speech translation in this work. It is a two-pass decoding process. The ﬁrst pass uses a simple model to generate a word graph to save the most likely hypotheses. It amounts to converting the pseudo word lattice into a target language word graph (TWG). Edges in the SWL are aligned to the edges in the TWG. Although the SWL is a faked lattice, the generated TWG has a true graph structure. The second pass uses a complicated model to output the best hypothesis by traversing the target word graph. We describe the two-pass WLT algorithm in the following two sections. 3.2.1. First pass — from SWL to TWG The bottom of Fig. 3 shows an example of a translation word graph, which corresponds to the recognition word lattice in the top. Each edge in the TWG is a target language word which is a translation of a source word in the SWL. The edges that have the same structure(including alignment and target context) are merged into a node. The node has one element indicating the source word coverage up to the current node. The coverage is a binary vector with size equal to the number of edges in the SWL, indicating the number of translated source edges. If the j-th source word was translated, the j-th element is set to 1; otherwise it equals 0. If a node covers all the edges of a full path in the SWL, it connects to the last node, the terminal node, in the TWG. There are two main operations in expanding a node into edges: DIRECT and ALIGN. DIRECT extends the hypothesis with a target word by translating an uncovered source word. The target word is chosen based on current target N -gram context and possible translations of the uncovered source word. ALIGN extends the hypothesis by aligning one more uncovered source word to the current node to increase fertilities of target word, where the target word is a translation of multiple source words. The edge is not extended if the resulted hypothesis does not align to any hypothesis in the SWL. If the node has covered a full path in the SWL, this node is connected to the end node. When there is no nodes available for  Algorithm 1 Conversion Algorithm from SWL to TWG  1: Initialize graph buffer G[0]=0; t=0  2: DO  3: FOR EACH node n=0,1,..., #(G[t]) DO  4:  IF (n cover A FULL PATH) NEXT  5:  FOR EACH edge l=0,1,...,#(EDGES) DO  6:  IF (n cover l) NEXT  7:  IF (n not cover ANY SWL PATH) NEXT  8:  generate new node and push to G[t+1]  9:  merge and prune nodes in G[t+1]  10: t= t+1  11:WHILE (G[t] is empty)  possible extension, the conversion is completed. A simple example of conversion algorithm is shown in Algorithm 1. The whole process equals to growing a graph. The graph can be indexed in time slices because the new nodes are created based on the old nodes of the last nearest time slice. New nodes are created by DIRECT or ALIGN to cover the uncovered source edge and connect to the old nodes. The new generated nodes are sorted in the graph buffer and merged if they share the same structure: the same coverage, the same translations, and the same N -gram sequence. If the node covers a full hypothesis in the SWL, the node connects to the terminal node. If no nodes need to be expanded, the conversion terminates. In the ﬁrst pass, we incorporate a simpler translation model into the log-linear model: only the lexical model, IBM model 1. The ASR posterior probabilities Ppp are calculated by partial hypothesis from the start to the current node. Ppp uses the highest value among all the ASR hypotheses under the current context. The ﬁrst pass serves to keep the most likely hypotheses in the translation word graph, and leave the job of ﬁnding the optimal translation to the second pass. 3.3. Second pass — by an A* search to ﬁnd the best output from the TWG An A* search traverses the TWG generated in the last section – the best ﬁrst approach. All partial hypotheses generated are pushed into a priority queue with the top hypothesis popping ﬁrst out of the queue for the next extension. To execute the A* search, the hypothesis score, D(h, n), of a node n is evaluated in two parts: the forward score, F (h, n), and the heuristic estimation, H(h, n), D(h, n) = F (h, n) + H(h, n). The calculation of F (h, n) begins from the start node and accumulates all nodes’ scores belonging to the hypothesis until the current node, n. The H(h, n) is deﬁned as the accumulated maximum probability of the models from the end node to the current node  Raw SWL j1(Wb) j0(Wa) j2(Wb)  j3(Wc) j6(Wd) j4(We) j7(Wd) j5(Wc)  Downsized SWL  l2(Wc)  l0(Wa) l1(Wb)  l4(Wd)  l3(We)  Hypotheses j0 j1 j3 j6 j0 j2 j4 j6 j0 j2 j5 j7 l0 l1 l2 l4 l0 l1 l3 l4  Transfer rules j0 -> l0(Wa) j1,j2 -> l1(Wb) j3,j5 -> l2(Wc) j4 -> l3(We) j6,j7 -> l4(Wd)  Figure 2: An example of word lattice reduction  j1 j0 j2  j3 j4 j5  j6 j7  e2 10100000 e5 10100100 e7 10100101  s 
We designed, implemented and assessed ALEPH, a pure example-based machine translation system. It strictly does not make any use of variables, templates or training, does not have any explicit transfer component, and does not require any preprocessing of the aligned examples. It relies on a speciﬁc operation: the resolution of analogical equations, that neutralizes translation divergences in an elegant way. Starting only from theoretical results, a system that is state-of-the-art with the top IWSLT 2004 results could be built in six month time. Evaluated on the Unrestricted Data track of IWSLT 2004, our system achieved second place in CE, and third place in JE (with best BLEU for this latter track). For this year’s evaluation campaign, the features of the system allowed its immediate application to all possible language pairs in the C-STAR tracks. 1. Introduction We present a novel example-based machine translation system, ALEPH, which relies on an operation speciﬁc to language (proportional analogy). We evaluate this system on the tasks proposed during the previous IWSLT 2004 evaluation campaign in the Japanese-English and Chinese-English Unrestricted Data track. This evaluation shows that our system would have positioned itself in the top systems for this track (best BLEU in JE track, second position in CE track). This study demonstrates that it is possible to implement an EBMT system from scratch in a matter of months (0.5 man/year) and achieve reasonable results. Starting only from the theoretical principles of analogy on strings of characters, building a system that is stateof-the-art with the top IWSLT 2004 results could be achieved in as little as six month time. In the IWSLT 2005 evaluation campaign, this system competed in all C-STAR tracks. Indeed, an appealing feature of this system is that it requires not training whatsoever: the data are just loaded into memory at  startup. As a consequence, it can be applied directly to any language pair for which there is sufﬁcient available data. Following this introduction, the second section of this paper gives a rationale for choosing one and only one basic operation, proportional analogy, to process any sentence of any language. The third section explains in detail the algorithm used to solve proportional analogies, while the fourth section shows its use as a blackbox function to perform translation. The ﬁfth section recalls the experimental conditions of the Unrestricted Data track of IWSLT 2004, gives the conﬁguration details for the ALEPH system and ranks it among the participants of IWSLT 2004. The sixth section gives the results of this year’s campaign. The ﬁnal section discusses the results obtained and future research.  2. Divergences across languages [1] quotes a study on a sample of 19, 000 sentences between English and Spanish showing that one translation pair in three presents divergences. A typical example is the translation of a Spanish verb into an English preposition.  1: Atraveso´ V 2: el r´ıo N 3: ﬂotando particip.  0: It ↔ 3: ﬂoated V 1: across prep. 2: the river N  Approaches that rely on the word as the unit of processing forget the fact that corresponding pieces of information in different languages are indeed distributed over the entire strings and do not necessarily correspond to complete words. For this reason, the correspondence between words given in the example above is in fact not detailed enough. Actually, the ending -o´ of the ﬁrst Spanish word accounts for 3rd person singular past tense. So, not only does atraveso´ correspond to the English preposition across in its meaning, but, in addition, it also corresponds to another complete word in English  (the pronoun it), plus a portion of yet a third English word (the ﬁnal ending -ed of ﬂoated). From the monolingual point of view, trivially, any natural language constitutes a “system” in the Saussurian sense of the term. This systematicity appears at best in commutations exhibited by proportional analogies like in the following examples. Obviously, any sentence of any language can be cast into a wide number of such proportional analogies, like the following ones:  They swam in the sea.  :  They swam across the river.  ::  It ﬂoated in the sea.  :  It ﬂoated across the river.  It walks It walked It ﬂoats It ﬂoated across the : across the :: across the : across the  street.  street.  river.  river.  It swam It ﬂoated He swam. : He ﬂoated. :: across the : across the  river.  river.  In [2] we have shown how to automatically build tables (or matrices) to visualize the many proportional analogies that can be found in the same resource around a given sentence: in such tables, each cell contains a sentence, and rectangles of four cells constitute proportional analogies. Such proportional analogies reveal the paradigmatic and syntagmatic variations around a given sentence. From a bilingual point of view, proportional analogies neutralize translation divergences across languages. They leave the choice for a correct translation to an implicit use of the structure of the target language. The correspondences between the source and the target languages in the proportional analogies are solely and entirely responsible not only for the selection of the correct lemmas, but also for the correct word order. For instance, in the example below:  They swam in the sea.  :  They swam across the river.  ::  It ﬂoated in the sea.  :  It ﬂoated across the river.  Nadaron en el mar.  :  Atravesaron el rio nadando.  :: Floto´ en el : mar.  x  the sole resolution of the analogical equation with the character as the only unit of processing is sufﬁcient to produce the exact translation of It ﬂoated across the river, provided that the three sentence pairs on the left are valid translation pairs. The correct Spanish sentence is therefore: x = Atraveso´ el rio ﬂotando.  This demonstrates that no explicit transfer component is needed in this framework: such proportional analogies, as the two above, do not need to tell which word corresponds to which word, or which syntactic structure corresponds to which syntactic structure. Moreover, there is no requirement at all for a particular word to correspond to any other word. To summarize, the basic element of the proposed framework is the correspondence between two proportional analogies, the sentences of which are valid translation pairs. It can be visualized by the parallelopiped of Figure 1. We shall now explain in detail the vertical planes (Section 3, Proportional analogies) and the horizontal direction (Section 4, Homomorphism between languages of analogical strings) of such a parallelopiped. 3. Proportional analogies 3.1. Scientiﬁc background Our notion of analogies between sentences, or to be more precise between strings of characters, reaches back as far as Euclid and Aristotle: “A is to B as C is to D”, postulating identity of types for A, B, C, and D. The notion has been put forward in morphology by Apollonius Dyscolus and Varro in the Antiquity. In modern linguistics, Saussure [3, part. III, CHAP. IV] considers analogical equations as a typically synchronic operation by which, given two forms of a given word, and only one form of a second word, the fourth missing form is coined: “honor is to hono¯rem as o¯ra¯tor is to o¯ ra¯ to¯ rem1 ”: o¯ra¯to¯rem : o¯ra¯tor :: hono¯rem : x ⇒ x = honor That analogy applies also to syntax, which is the foundation of our framework, has been advocated by Hermann Paul [4, p. 110] and Bloomﬁeld [5, p. 275]. More recently, Itkonen and Haukioja [6] showed how to deliver grammatical sentences by application of proportional analogies to structural representations. 3.2. Theoretical aspects While analogy has been largely mentioned and used in linguistics, algorithmic ways to solve proportional analogies between strings of characters have never been proposed,2 maybe because the operation seems so misleadingly “intuitive”. To our knowledge, we were the ﬁrst to give an algorithm for the resolution of analogical equations in [8]. It is based on the following formalisation of proportional analogies in terms of edit dis- 1Latin: o¯ra¯tor (orator, speaker) and honor (honour) nominative singular, o¯ra¯to¯rem and hono¯rem accusative singular. In the II century BC, honor competed with the etymologically correct honos. 2 Except for Copycat [7, p. 205–265] which adopts an artiﬁcial intelligence point of view, of little use for linguistic applications.  It ﬂoated across the river. It ﬂoated in the sea.  ↔ Atraveso´ el rio ﬂotando.  ↔  Floto´ en el mar.  They swam across the river.  ↔ Atravesaron el rio nadando  They swam in the sea.  ↔  Nadaron en el mar.  Figure 1: The parallelopiped: four sentences in each language forming a proportional analogy. Four horizontal translation relations exist between the sentences.  tances, or equivalently, in terms of similarity (refer to [9, Chap. 3] for these notions). We denote σ(A, B, . . . , N ) as the length of the longest common subsequence in the strings A, B, . . . N , i.e., their similarity. The following formula consistently puts the unknown D on the left of all equal signs, so as to better suit to the resolution of analogical equations. A : B :: C : D ⇒     σ(B, D) = σ(C, D) =    σ(A, B, C, D) |D|  = =  − |A| + |B| + σ(A, C) − |A| + |C| + σ(A, B) − |A| + σ(A, B) + σ(A, C) − |A| + |B| + |C|  The step-by-step mechanism we then adopt during resolution is inspired by [6, p. 149], where they take sentence A as the axis against which sentences B and C are compared, and by opposition to which output sentence D is built. 3.3. An example Rather than explaining once again the algorithm given in [8], we illustrate its application in an actualized way to a particular analogical equation: like : unlike :: known : x. We use words rather than sentences for reasons of space; the same algorithm applies to analogical equations between sentences considered as strings of characters; and it also applies to languages like Japanese, Chinese or Korean where a character is encoded on two bytes instead of just one byte for English. The similarities between strings A and B, and A and C, are computed in an efﬁcient way using a fast algorithm [10]. Based on a result by [11], only minimal diagonal bands are considered in the matrices. In the following matrices, the algorithm follows the paths noted by values in circles in a way similar to that taken in [12] for the output of an edit distance trace.  ek i l nu  kn own  . . . 1 0 0l0 0 . . . . . 210 .i. 00 . . . 3 21 . .k. . 11 . 432 . . .e. . . 11  The succession of moves triggers the copies of characters into the solution D, according to “rules” that tell which character to choose from which string, B or C, according to the moves in both matrices, so that ﬁnally, the solution x = unknown is output.  dirAB diagonal diagonal diagonal diagonal horizontal horizontal horizontal  dirAC diagonal diagonal diagonal diagonal horizontal diagonal diagonal  copy onto D n w o n k n u  from string C C C C C B B  The example above is simplistic, as it reduces to adding a preﬁx to known. Our algorithm is more powerful as it handles parallel inﬁxing, which is inescapable in the morphology of Semitic languages3. aslama : muslimun :: arsala : x ⇒ x = mursilun 4  It is also necessary in our framework because solving analogical equations between sentences involves parallel inﬁxing in the general case. It should be noted that there may be zero, one or several solutions to an analogical equation. Analogical equations are thus a ternary 3 In particular, if we want to handle the morphology of Arabic in the Arabic-English C-STAR track of IWSLT 2005. 4Arabic: arsala (he sent) and aslama (he converted [to Islam]) are verbs 3rd person singular past; mursilun (a sender) and muslimun (a convert, i.e., a muslim) are agent nouns.  operation, i.e., a mapping α : L × L × L → ℘(L) (with L the set of strings considered and ℘(S) its power set). The set of the solutions of an analogical equation is: α(A, B, C) = {D ∈ L | A : B :: C : D }  A : B :: C : D ⇔ A : B :: C : D Using the α operation that structures the source and target languages of analogical strings, an equivalent form of this formula is:  4. Homomorphisms between languages of analogical strings 4.1. Theoretical aspects Based on proportional analogies, we have shown [13] how to deﬁne a family of formal languages, called languages of analogical strings. It is important to note that their construction, as is the case with simple contextual5 grammars [14], does not make any use of nonterminals. Such languages are built by transitive closure starting from a corpus of given sentences (strings of characters) Λ0. We denote α(Λ, Λ, Λ) as the set of sentences produced by solving all possible analogical equations formed with three sentences in Λ. α(Λ, Λ, Λ) = {D | ∃(A, B, C) ∈ Λ3, A : B :: C : D } Then, the language L(Λ0) of analogical strings built from a corpus Λ0 is deﬁned in the following way6:  D = α(A, B, C) = α(A, B, C) This shows that this translation principle “distributes” translation on the arguments of the structuring internal operation α. Thus, it is a homomorphism between two languages of analogical strings that preserves the structuring operation, proportional analogy7. For this reason, the translation system described here has been called ALEPH. It is an acronym for Analogy in Languages & Processing by Homomorphism. 4.2. An example Building on what has been said above, suppose we have a bicorpus at our disposal, i.e., a corpus of aligned sentences in two languages, say, Japanese and English. Suppose that we want to translate the following Japanese input sentence: ೱ͍ίʔώʔ͕ҿΈ͍ͨɻ8  +∞ L(Λ0) = Λn where Λn+1 = α(Λn, Λn, Λn) n=0  Among all possible pairs of sentences from the bicorpus, we may ﬁnd the following two Japanese sentences:  As for the position of such languages in the Chomsky-Schu¨tzenberger hierarchy, it is easy to show that the classical regular language {an|n ≥ 1}, the context-free language {anbn|n ≥ 1}, and the context-sensitive language {anbncn|n ≥ 1} are all languages of analogical  ߚ஡Λ͍ͩ͘͞ɻ ίʔώʔΛ͍ͩ͘͞ɻ  ↔  May I have some tea, please?  ↔  May I have a cup of coffee?  strings. Moreover, we have shown [13] that the famous context-sensitive language {anbmcndm | m, n ≥ 1} used in [15] to refute the context-freeness hypothesis  that will allow us to form the following analogical equation:  of natural language, is a language of analogical strings. More importantly, every language of analogical strings  ߚ஡Λͩ͘ ͍͞ɻ  :  ίʔώʔΛ͘ ͍ͩ͞ɻ  ::  
This paper describes ATR’s hybrid approach to spoken language translation and it’s application to the IWSLT 2005 translation task. Multiple corpus-based translation engines are used to translate the same input, whereby the best translation among the element MT outputs is selected according to statistical models. The evaluation results of the Japanese-to-English and Chinese-to-English translation tasks for different training data conditions showed the potential of the proposed hybrid approach and revealed new directions in how to improve the current system performance. 1. Introduction Corpus-based approaches to machine translation (MT) have achieved much progress over the last decades. There are two main strategies used in corpus-based translation: 1. Example-Based Machine Translation (EBMT) [1]: EBMT uses the corpus directly. EBMT retrieves the translation examples that are best matched to an input expression and then adjusts the examples to obtain the translation. 2. Statistical Machine Translation (SMT) [2]: SMT learns statistical models for translation from corpora and dictionaries and then searches for the best translation at run-time according to the statistical models for language and translation. Despite a high performance on average, these approaches can often produce translations with severe errors. However, different MT engines not always do the same error. Due to the particular output characteristics of each MT engine, quite different translation hypotheses are produced. Thus, combining multiple MT systems can boost the system performance by exploiting the strengths of each MT engine. We propose a corpus-based approach that uses multiple translation engines in parallel. All engines translate the same input, whereby the best translation among the multiple MT outputs is selected according to multiple statistical language and translation models. The outline of our hybrid approach is given in Section 2.  The proposed system was applied to two translation directions (Japanese-to-English, Chinese-to-English) and three data tracks (Supplied Data Track, Supplied+Tools Data Track, C-STAR Track). The evaluation of the obtained results is given in Section 3. 2. System Description We use an architecture in which multiple EBMT and SMT engines work in parallel and their outputs are passed to a post-process that selects the best candidate according to SMT models (cf. Figure 1). This section is structured as follows: (1) eight corpusbased MT engines are introduced in Section 2.1; (2) the SMT-based approach to select the best translation out of multiple hypotheses is explained in Section 2.2; (3) the resources utilized for the IWSLT 2005 translation task are described in Section 2.3; and (4) a summary of the data tracks we participated in and an overview on which MT engines were utilized for the respective tracks is given in Section 2.4. 2.1. MT Engines We employed the following four SMT and four EBMT systems: An SMT engine that uses an example-based decoding method [SAT] (cf. Section 2.1.1), an SMT engine that uses a phrase-based HMM translation model [PBHMTM] (cf. Section 2.1.2), a morpho-syntactically enriched phrase-based SMT engine [MSEP] (cf. Section 2.1.3), an SMT engine based on syntactic transfer [HPATR2] (cf. Section 2.1.4), an EBMT engine that incorporates word-level SMT methods [HPATR] (cf. Section 2.1.5), an EBMT engine based on hierarchical phrase alignments [HPAT] (cf. Section 2.1.6), an DP-match-driven EBMT engine [D3] (cf. Section 2.1.7), and a translation memory system [EM] (cf. Section 2.1.8). The translation knowledge of the eight MT systems is automatically acquired from a parallel corpus. The characteristics of the element MTs are summarized in Table 1. 2.1.1. SAT SAT is an SMT system [3]. The decoder searches for an optimal translation by using SMT models starting from a decoder seed, i.e., the source language input paired with an initial translation hypothesis. In SAT, the search is initiated from  Input  Preprocessing Tagger  Chunker  Parser  Resources Thesaurus  Translation MT1 MTm  Hypothesis 1 Hypothesis m  Statistical Models (LM, TM)  Corpus (monolingual)  Corpus (bilingual)  Selection  SELECTOR  TM LM1  TM LMn  Dictionary  Output  Figure 1: System outline  Table 1: Features of element MT engines  U nit C ov erag e Quality Speed Resources  SAT sentence&word wide excellent modest corpus  SMT PBHMTM phrase wide good slow corpus  MSEP phrase wide good slow corpus, chunker  HPATR2 phrase wide good modest corpus, parser  HPATR phrase wide good modest corpus, parser  EBMT  HPAT  D3  phrase wide  sentence narrow  good fast  excellent fast  corpus, parser, thesaurus  corpus, thesaurus, bilingual dictionary  EM sentence narrow excellent fast corpus  similar translation examples retrieved from a parallel corpus. The similarity measure used here is a combination of an editdistance and tf/idf criteria as seen in the information retrieval framework [4]. The retrieved translations are modiﬁed by using a greedy search approach to ﬁnd better translations [5]. 2.1.2. PBHMTM PBHMTM is a statistical MT system that is based on a phrase-based HMM translation model [6]. The model directly structures the phrase-based SMT approach in a Hidden Markov structure. The probability P (f |e) of translating a foreign source sentence f into a target language sentence e using noisy channel modeling is approximated by introducing two new hidden variables, ¯f and e¯, to explicitly capture the phrase translation relationship: P (f |e) = P(f |¯f , ¯e, e)P(¯f |¯e, e)P(¯e|e) (1) ¯f ,¯e The ﬁrst term represents the probability that a phrasesegmented source language sentence ¯f can be reordered and generated as the source text of f (Phrase Segmentation Model). The second term indicates the translation probability of the two phrase sequences of e¯ and ¯f (Phrase Translation Model). The last term is the likelihood that the phrasesegmented target language sentence e¯ is generated from e (Phrase Ngram Model). If the phrase segmented sentences e¯ and ¯f are expanded into corresponding lattice structures E¯ and F¯ , then the approximation of the proposed models can be regarded as a Hidden Markov Model in which each source phrase in the  lattice F¯ is treated as an observation emitted from a state, a target phrase, in the lattice E¯ . The decoder is a word-graph-based decoder [7], which allows the multi-pass decoding strategies to incorporate complicated submodel structures. The ﬁrst pass of the decoding procedure generates the word-graph, or the lattice, of translations for an input sentence by using a beam search. On the ﬁrst pass, the submodels of all phrase-based HMM translation models were integrated with the word-based trigram language model and the class 5-gram model. The second pass uses the A* strategy to search for the best path for translation on the generated word-graph.  2.1.3. MSEP MSEP is a phrase-based SMT system that utilizes morphosyntactic information such as part-of-speech and chunk information [8]. It exploits a phrase translation lexicon that is created using word-alignment results and chunk boundary information in a target language sentence. Reliable bilingual phrase pairs are identiﬁed using the statistical χ2-test at a signiﬁcance level α=0.05 over a given frequency threshold. For selecting the most probable translation of a given source sentece, a log-linear model is used:  P rΛ(eI1|f1J ) =  exp( m λmhm(eI1, f1J , aJ1 )) eI1,aJ1 exp( m λmhm(eI1, f1J , aJ1 ))  (2)  where hm(eI1, f1J , aJ1 ) is the m-th feature and λm is the weight of the feature.  In addition to the IBM model 4 features (word-based ngram language model P r(eI1), lexicon model t(f |e), fertility model n(φ|e), distortion probability d, and NULL translation model p1), we incorporate the following features into the log- linear translation model:  • Class-based n-gram model: P r(eI1) = i Pr(ei|ci)P r(ci|ci1−1) • Length model: P r(l|eI1, f1J ), whereby l is the length (number of words) of a translated target sentence.  • Phrase matching score: The translated target sentence is matched with phrase translation examples that are extracted from a parallel corpus based on bidirectional word alignment of phrase translation pairs. A score is derived based on the number of matches.  2.1.4. HPATR2  HPATR2 is a statistical MT system based on syntactic transfer [9]. The translation model of HPATR2 is deﬁned as an inside probability of two parse trees, which is used to create probabilistic context-free grammar rules. The system searches for the best translation that maximizes the product of the following probabilities, where F, E are a source and a target parse trees, and θ, π are context-free grammar rules of the source and the target language, respectively.  • Probability of Source Tree Model  P (F) =  P (θ)  (3)  θ:θ∈F  • Probability of Target Tree Model  P (E) =  P (π)  (4)  π:π∈E  • Probability of Tree-mapping Model  P (F|E)P (E|F) =  P (θ|π)P (π|θ) (5)  θ:θ∈F , π:π∈E  A characteristic of HPATR2 is that not only word translations but also the translation of multi-word sequences is carried out by the syntactic transfer. Parsing hypotheses, which are multi-word sequences connected by context-free grammar rules, are created. The best hypothesis (parse tree and translation) is selected based on the above models. Therefore, HPATR2 is an MT system that contains features of phrase-based SMT as well as syntax-based SMT.  2.1.5. HPATR  HPATR is an extension of the example-based HPAT system (cf. Section 2.1.6) that incorporates a word-based statistical MT system [10]. Similar to HPAT, an EBMT module based on syntactic transfer is used to generate translation candidates that have minimum semantic distances. However, word  selection is not performed during transfer, but all possible word translation candidates are generated. In a second step, an SMT module using a lexicon model and an n-gram language model is exploited to search for the best translation that maximizes the product of the probabilities. Therefore, HPATR selects the best translation among the output of example-based MT using models of statistical MT from the viewpoints of adequacy of word translation and ﬂuency of the target sentence. 2.1.6. HPAT HPAT is an example-based MT system based on syntactic transfer [11]. The most important knowledge in HPAT are transfer rules, which deﬁne the correspondences between source and target patterns. The transfer rules can be regarded as synchronized context-free grammar rules. When the system translates an input sentence, the sentence is ﬁrst parsed by using the source side of the transfer rules. Next, a tree structure of the target language is generated by mapping the source grammar rules to the corresponding target rules. When non-terminal symbols remain in the target tree, target words are inserted by referring to a translation dictionary. Ambiguities, which occur during parsing or mapping, are resolved by selecting the rules that minimize the semantic distance between the input words and source examples of the transfer rules. In general, the automatic acquisition process generates many redundant rules. To avoid this problem, HPAT optimizes the transfer rules by removing redundant rules (feedback cleaning, [12]) in order to increase an automatic evaluation score. 2.1.7. D3 D3 (DP-match Driven transDucer) is an EBMT system that exploits DP-matching between word sequences [13]. In the translation process, D3 retrieves the most similar source sentence from a parallel corpus for an input sentence. The similarity is calculated based on the counts of insertion, deletion, and substitution operations, where the total is normalized by the sum of the lengths of the word sequences. Substitution considers the semantic distance between two substituted words and is deﬁned as the division of K, the level of the least common abstraction in the thesaurus of two words, by N, the height of the thesaurus [14]. According to the difference between the input sentence and the retrieved source sentence, the translation of the retrieved source sentence is modiﬁed by using dictionaries. 2.1.8. EM EM is a translation memory system that matches a given source sentence against the source language parts of translation examples extracted from a parallel corpus. In case an exact match can be achieved, the corresponding target language sentence will be used. Otherwise, the system fails to output a translation.  2.2. Selection of the Best MT Engine Output  We use an SMT-based method of automatically selecting the best translation among outputs generated by multiple MT systems [15]. This approach scores MT outputs by using multiple language (LM) and translation model (TM) pairs trained on different subsets of the training data. It uses a statistical test to check whether the average TM·LM score of one MT output is signiﬁcantly higher than those of another MT output. The SELECTOR algorithm is summarized in Figure 2.  (1) proc SELECTOR( Input, Corpus, n, M T 1, . . . , M T m ) ;  (2) begin  (3) (∗ initalize statistical models ∗)  (4) for each i in {1, . . . , n} do  (5)  Corpusi ← subset(Corpus) ;  (6)  T Mi ← translation-model(Corpusi) ;  (7)  LMi ← language-model(Corpusi) ;  (8) od ;  (9) (∗ score MT outputs using multiple TM·LM pairs ∗)  (10) HypScores ← {} ;  (11) for each M T in {M T 1, . . . , M T m} do 
In this paper we describe the CMU statistical machine translation system used in the IWSLT 2005 evaluation campaign. This system is based on phrase-to-phrase translations extracted from a bilingual corpus. We experimented with two different phrase extraction methods; PESA on-the-ﬂy phrase extraction and alignment free extraction method. The translation model, language model and other features were combined in a log-linear model during decoding. We present our experiments on model adaptation for new data in a different domain, as well as combining different translation hypotheses to obtain better translations. We participated in the supplied data track for manual transcriptions in the translation directions: ArabicEnglish, Chinese-English, Japanese-English and KoreanEnglish. For Chinese-English direction we also worked on ASR output of the supplied data, and with additional data in unrestricted and C-STAR tracks. 1. Introduction Large vocabulary text translation has been the primary focus in machine translation research during the past. Much improvements have been achieved with projects such as TIDES, which focused on large vocabulary text translation. With the availability of reliable speech recognition systems and spoken language corpora, now the focus is shifting towards speech translation; and further towards speech-to-speech translation. With the IBM system [1] in early 90’s, statistical machine translation (SMT) has been the most promising approach for machine translation. Many approaches for SMT have been proposed since then [2], [3], [4]. Whereas the original IBM system was based on purely word translation models, current SMT systems incorporate more sophisticated models. The CMU statistical machine translation system uses phrase-to-phrase translations as the primary building blocks to capture local context information, leading to better lexical choice and more reliable local reordering. In section 2, we describe the phrase alignment approaches used by our system.  The main obstacle in using additional data for a translation task is that the new data may belong to a different domain. We explored methods of adapting both the translation model and the language model to overcome this problem, which are described in section 3. Section 4 outlines the architecture of the decoder that combines the translation model, language model, and other models to generate the complete translation. When translating speech recognition output, we integrate multiple translation hypotheses into a single structure and then derive the best hypothesis. This approach is described in section 5. Finally, in section 6 we give an overview of the data and tasks and present the results of the experiments we carried out for different data conditions.  2. Phrase Alignment  In this evaluation, we applied a variation of the alignment-free approach, which is an extension to the previous work in [5] and [6] to extract bilingual phrase pairs for the supplied data tracks. In this extension, we used eleven feature functions including phrase level fertilities and phrase level IBM Model-1 probabilities aiming to locate the phrase pairs from the parallel sentences. The feature functions are then combined in a log-linear model as follows:  P (X|e, f )=  exp( M m=1λmφm(X, e, f ))  {X } exp(  M m=1  λmφm  (X  ,e,f ))  where X→(fjj+l, eii+k) corresponds to a phrase-pair candidate extracted from a given sentence-pair (e, f ); φm is a feature function designed to be informative for phrase extraction. Feature function weights {λm}, are the same as in our previous experiments [7]. This log-linear model serves as a performance measure function in a local search. The search starts from fetching a test-set speciﬁc source phrase (e.g. Chinese ngram); it localizes the candidate ngram’s center in the English sentence; and then around the projected center, it ﬁnds out all the candidate phrase pairs ranked with the log-linear model scores. In the local search, down-hill moves are allowed  so that functional words can be attached to the left or right boundaries of the candidate phrase-pairs. The eleven (M =11) feature functions that compute different aspects of phrase pair (fjj+l, eii+k) are as follows: • Four of them compute the phrase-level length relevance: P (l+1|eii+k) and P (J −l−1|ei ∈/[i,i+k]), where ei ∈/[i,i+k] is denoted as the remaining English words in e: ei ∈/[i,i+k]={ei |i ∈/ [i, i+k]}, and J is the length of f . The probability is computed via dynamic programming using English word-fertility table P (φ|ei). P (k+1|fjj+l) and P (I−k−1|fj ∈/[j,j+l]) are computed in a similar way. • Another four compute the IBM Model-1 scores for the phrase-pairs P (fjj+l|eii+k) and P (eii+k|fjj+l); the remaining parts of (e, f ) excluding the phrasepair is modeled by P (fj ∈/[j,j+l]|ei ∈/[i,i+k]) and P (ei ∈/[i,i+k]|fj ∈/[j,j+l]) using the translation lexicons of P (f |e) and P (e|f ). • Another two of the scores aim to bracket the sentence pair with the phrase-pair as detailed in [7]. • The last function computes the average word alignment links per source word in the candidate phrasepair. We assume each phrase-pair should contain at least one word alignment link. We train the IBM Model-4 with GIZA++ [8] in both directions and grow the intersection with word pairs in the union to collect the word alignment. Because of the last feature-function, our approach is no longer truly “alignment-free”. More details of the log-linear model and experimental analysis of the feature-functions are given in [7]. To use the extracted phrase-pairs in the decoder, a set of eight scores for each phrase-pair are computed: relative frequency of both directions, phraselevel fertility scores for both directions computed via dynamic programming, the standard IBM Model-1 scores for both directions (i.e. P (fjj+l|eii+k) = j ∈[j,j+l] i ∈[i,i+k] P (fj |ei )/(k+1)), and the unnormalized IBM Model-1 scores for both direction (i.e. P (fjj+l|eii+k) = j ∈[j,j+l] i ∈[i,i+k] P (fj |ei )). The standard IBM Model-1 scores prefer short translations; the un-normalized scores prefer longer translations. The scores are combined via the optimization component of the decoder (e.g. Max-BLUE optimization) as described in section 4 in the hope of balancing the sentence length penalty. 3. Model Adaptation The Unrestricted Data track allows the use of additional publicly available data for both translation and language  models. This mainly includes data from LDC and data that is available on the Web. The main problem with additional data is that it usually is from a different domain compared to the original data. Using this data as is, along with the supplied data hurts the performance on the development sets. Therefore, we used a translation model adaptation approach to handle this problem. 3.1. Translation Model Adaptation We adapt the translation model to the test set by selecting a part of the additional out-of-domain data using information retrieval techniques as explained in [9]. For every source language sentence from the test set or the development set, the most similar sentences from the out-of-domain bilingual data are selected using cosine distance with TF-IDF term weights as the similarity measure. The retrieval is done on the source language side with each test sentence as a query, then the information is used to extract respective sentence pair from the bilingual corpus. The selected sentences from the out-of-domain data together with the supplied in-domain data are used to train the translation model for the whole test set. 3.2. Language Model Perplexity for Measuring Selection Quality An important question when selecting additional sentences is how much out-of-domain data should be added to the training corpus. Here, we used a perplexity based re-ranking method [9]. The top 1000 retrieved sentences in the source language (which is much more than the optimal number) are split into small batches of 3-10 sentences which are sequentially added to the selection. To determine how well the selection of training data ﬁts the test sentence, we measure the perplexity of a language model trained from each selection against the respective test sentence. Each batch is classiﬁed according to whether it decreases (good batch) or increases (bad batch) the perplexity. The batches are re-ranked using this information by putting bad batches at the end of the sorted order of sentences. After re-ranking, those sentences that are in the range of twice the lowest perplexity value are included in the ﬁnal training corpus. Still, the main selection criterion is TF-IDF information retrieval, as we look only at the e.g. top 1000 sentences returned by the retrieval and the original TFIDF ranking is kept among the good as well as the bad batches. This method allows to determine the size of the selection without using a development set and shows improvements over the standard method of just choosing the same number for each test sentence.  3.3. Data Weights  To balance the different sizes of the in-domain and outof-domain training corpora we assigned a stronger weight to the in-domain data. We experimented with different weight combinations. A rule of thumb for the weight w for the in-domain data is as in (1) :  w=  #lines out − of − domain #lines in − domain  (1)  3.4. Language Model Adaptation We also applied a basic form of language model (domain) adaptation using additional data crawled from the Web. Based on the English in-domain supplied training data the 5000 most common 3-grams and 4-grams were used as queries for the Google Web search engine. After ﬁltering and basic cleaning of the retrieved web pages this data can be added to the Language Model training data.  4. Decoder The decoder combines all knowledge sources, i.e. translation model, language model, etc. to ﬁnd the best translation. In the CMU SMT decoder the decoding process is organized into two states: • Find all available word and phrase translations. These are inserted into a lattice structure, called translation lattice. • Find the best combinations of these partial translations, such that every word in the source sentence is covered exactly once. This amounts to doing a best path search through the translation lattice, which is extended to allow for word reordering. In addition, the system needs to be optimized. For each model used in the decoder a scaling factor can be used to modify the contribution of this model to the overall score. Varying this scaling factors can change the performance of the system considerable. Minimum error training is used to ﬁnd a good set of scaling factors. In the following sub-sections, these different steps will be described in some more detail.  4.1. Building Translation Lattice The CMU SMT decoder can use phrase tables, generated at training time, but can also do just-in-time phrase alignment. This means that the entire bilingual corpus is loaded and the source side indexed using a sufﬁx array [10]. For all ngrams in the test sentence, occurrences in the corpus are located using the sufﬁx array. For a number of occurrences, where the number can be given as a parameter to the decoder, phrase alignment as described in section 2 is performed and the found target phrase added to the translation lattice.  If phrase translations have already been collected during training time, then this phrase table is loaded into the decoder and a preﬁx tree constructed over the source phrases. This allows for an efﬁcient search to ﬁnd all source phrases in the phrase table which match a sequence of words in the test sentence. If a source phrase is found in the phrase translation table then a new edge is added to the translation lattice for each translation associated with the source phrase. Each edge carries not only the target phrase, but also a number of model scores. There can be several phrase translation model scores, calculated from relative frequency, word lexicon and word fertility. In addition, the sentence stretch model score and the phrase length model score are applied at this stage. 4.2. Searching for Best Path The second stage in the decoding is ﬁnding a best path through the translation lattice, now also applying the language model. To allow for word reordering, the search algorithm is extended. Hypotheses describe partial translations, i.e. a sequence of target language words, which are translations of some of the source words, and a score. As we use a trigram language model, we need to store only the last two words. A hypothesis can be expanded to cover additional source words. To restrict the search space only limited word reordering is done. Essentially, decoding runs from left to right over the source sentence, but words can be skipped within a restricted reordering window and translated later. In other words, the difference between the highest index of already translated words and the index of still untranslated words is smaller than a speciﬁed constant, which typically is 4. When a hypothesis is expanded, the language model is applied to all target words attached to the edge over which the hypothesis is expanded. In addition, the distortion model is applied, adding a cost depending on the distance of the jump made in the source sentence. Hypotheses are recombined whenever the models can not change the ranking of alternative hypotheses in the future. For example, when using a trigram language model, two hypotheses having the same two words at the end of the word sequences generated so far, will get the same increment in language model scores when expanded with an additional word. Therefore, only the better hypothesis needs to be expanded. The translation model and distortion model require that only the hypotheses which cover the same source words are compared. As typically too many hypotheses are generated, pruning is necessary. This means that coarser equivalence classes are used to compare hypotheses, but also to keep not only one hypothesis in one equivalence class, as done in recombination, but to keep all hypotheses, which are close to the best one. Pruning can be done with more  equivalence classes and smaller beam, or coarser equivalence classes and wider beams. For example, comparing all hypotheses, which have translated the same number of source words, no matter what the ﬁnal two words are, would be working with a small number of equivalence classes in pruning. The CMU SMT decoder allows two different recombination and pruning settings. 4.3. Optimizing the System Each model contributes to the total score of the translation hypotheses. As these models are only approximations to the real phenomena they are supposed to describe, and as they are trained on varying, but always limited data, their reliability is restricted. However, the reliability of one model might be higher than the reliability of another model. So, we should put more weight on this model in the overall decision. This can be done by doing a log-linear combination of the models. In other words, each model score is weighted and we have to ﬁnd an optimal set of these weights or scaling factors. When dealing with two or three models, grid search is still feasible. When adding more and more features (models) this no longer is the case and automatic optimization needs to be done. We use the Minimum Error Training as described in [11], which uses rescoring of the n-best list to ﬁnd the scaling factors with maximize BLEU or NIST score. Starting with some reasonably chosen model weights a ﬁrst decoding for some development test set is done. An n-best list is generated, typically a 1000-best list. Then a multi-linear search is performed, for each model weight in turn. The weight, for which the change gives the best improvement in the MT evaluation metric, is then ﬁxed to the new value, and the search repeated, till no further improvement is possible. The optimization is therefore based on an n-best list, which resulted from sub-optimal model weights, and contained only a limited number of alternative translations. To eliminate any restricting effect, a new full translation is done with the new model weights. The resulting new n-best list is then merged to the old n-best list, and the entire optimization process repeated. Typically, after three iterations of doing translation plus optimization, translation quality, as measured by the MT evaluation metric, converges. 5. ROVER on SMT n-best Hypotheses To improve the translation accuracy of the ASR output, we integrate multiple translation hypotheses and select the best translation. Multiple translations can be obtained either by translating each of the n-best hypotheses produced by a speech recognizer, or selecting the n-best translations by a machine translation system.  5.1. ROVER The ROVER approach is useful for integrating multiple word sequences [12]. The word sequences can be integrated based on the edit distance between the sequences, and then represented as a word transition network (WTN) which has the same structure as a confusion network (CN). A WTN differs from CN in that the score of each arc is determined based on the occurrences of words aligned to the same position in the WTN unlike posterior probabilities in CN obtained by speech recognition. Figure 1 shows an example of a word transition network. The integrated multiple word sequence begins with <s> and ends with </s>. In each column, words aligned to the same position are included. The symbol “@” is a special word indicating the possibility of deletion.  Figure 1: Word Transition Network.  To select the best translation from a WTN, we consider two methods. Given a WTN, one method is to simply choose the best scored word sequence Wˆ such that:  |W |  Wˆ = arg max PW T N (wn)  (2)  W ∈W T N n=1  where PW T N (wn) is a score of wn in the WTN that can be calculated as the proportion of the number of occurrences of wn to the sum of occurrences of words in the same column; |W | is the length of the word sequence W.  5.2. ROVER combined with Language model When ROVER system is combined with a language model, it helps to increase the recognition performances considerably for multiple ASR system outputs [13]. We search for the best sequence using both the score of each arc and probabilities given by a language model of the target language such that: |W | Wˆ = arg max PW T N (wn)PLM (wn|wn−2wn−1)λ W ∈W T N n=1 (3) where PLM (wn|wn−2wn−1) is the language model score given by a trigram language model; λ is the scaling factor for the language model. By using a language model, the selected word sequence is expected to be ﬂuent and grammatically correct. The best word sequence can easily be found by using a dynamic programming technique.  Training C-STAR’03 IWSLT’04 IWSLT’05  Table 1: Corpus statistics for the supplied data.  Sentences Words Vocabulary Sentences Words Vocabulary Unknown Words Sentences Words Vocabulary Unknown Words Sentences Words Vocabulary Unknown Words  Arabic 131,711 26,116 2,579 1,322 441 2,712 1,399 484 2,607 1,387 468  Supplied Data Track  Chinese  Japanese Korean  Manual ASR  20,000  176,199  198,453 208,763  8,687  9,277 9,132  3,511 913 117  506  2,835 4,130  1,024  920  245  70  4,084 976 95  500  3,590 2,896 4,131  -  975 1,068  945  -  116 223  61  -  3,743 963 155  506  3,003 4,226  1,091  975  249  169  4,563 969 84  English 183,452 6,956 - - -  5.3. Consolidation on ROVER  
Statistical machine translation relies heavily on the available training data. In some cases it is necessary to limit the amount of training data that can be created for or actually used by the systems. We introduce weighting schemes which allow us to sort sentences based on the frequency of unseen n-grams. A second approach uses TF-IDF to rank the sentences. After sorting we can select smaller training corpora and we are able to show that systems trained on much less training data achieve a very competitive performance compared to baseline systems using all available training data.  1. Introduction The goal of this research was to decrease the amount of training data that is necessary to train a competitive statistical translation system regardless of the actual test data or its domain. “Competitive” here means that the system should not produce significantly worse translations compared to a system trained on a significantly larger amount of data. It is important to note that this is not an adaptation approach as we assume that the test data (and its domain) is not known at the time we select the actual training data.  Statistical machine translation can be described in a formal way as follows:  t* = arg max P(t | s) = arg max P(s | t) ⋅ P(t)  t  t  Here t is the target sentence, and s is the source sentence. P(t)  is the target language model and P(s|t) is the translation model  used in the decoder.  Statistical machine translation searches for the best target  sentence from the space defined by the target language model  and the translation model.  Statistical translation models are usually either phrase- or  word-based and include most notably IBM1 to IBM4 and  HMM ([1], [2], [3]). Some recent developments focused on  online phrase extraction ([4], [5]).  All models use available bilingual training data in the source  and target languages to estimate their parameters and  approximate the translation probabilities.  One of the main problems of Statistical Machine Translation  (SMT) is the necessity to have large parallel corpora  available. This might not be a big issue for major languages,  but it certainly is a problem for languages with fewer  resources ([6], [7]). To improve the data situation for these  languages it is necessary to hire human translators at  enormous costs who translate corpora that can later be used to train SMT systems. Our idea focuses on sorting the available source sentences that should be translated by a human translator according to their approximate importance. The importance is estimated using a frequency based and an information retrieval approach. 2. Motivation There are three inherently different motivations for the goal of limiting the amount of necessary training data for a competitive translation system. We described those motivations and their applications already in the paper [8]. Application 1: Reducing Human Translation Cost The main problem of portability of SMT systems to new languages is the involved cost to generate parallel bilingual training data as it is necessary to have sentences translated by human translators. An assumption could be that a 1 million word corpus needs to be translated to a new language in order to build a decent SMT system. A human translator could charge in the range of approximately 0.10-0.25 USD per word depending on the involved languages and the difficulty of the text. The translation of a 1 million word corpus would then cost between 100,000 and 250,000 USD. The concept here is to select the most important sentences from the original 1 million word corpus and have only those translated by the human translators. If it would still be possible to get a similar translation performance with a significantly lower translation effort, a considerable amount of money could be saved. This could especially be applied to low density languages with limited resources ([6], [7]). Application 2: Translation on Small Devices Another possible application is the usage of statistical machine translation on portable small devices like PDAs or cell phones. Those devices tend to have a limited amount of memory available which limits the size of the models the device can actually hold and a larger training corpus will usually result in a larger model. The more recent approaches to online phrase extraction for SMT make it necessary to have the corpus available (and in memory) at the time of translation ([4], [5]). Given the upper example, a small device might not be able to hold a 1 million word bilingual corpus but e.g. only a corpus  with 200,000 words. The question is now which part of the corpus (especially which sentences) should be selected and put on the device to get the best possible translation system. Application 3: Standard Translation System Even on larger devices that do not have rigid limitations of memory, the approach could be helpful. The complexity of online phrase extraction and standard training algorithms depends mainly on the size of the bilingual training data. Limiting the size of the training data with the same translation performance on these devices would speed up the translations. Another problem is that the still widely used 32 bit machines like the Intel Pentium 4 and AMD Athlon XP series can only address up to 4 gigabytes of memory. There are already bilingual corpora in excess of 4 gigabytes available and therefore it is necessary to select the most important sentences from these corpora to be able to hold them in memory. (The last issue will certainly be resolved by the widespread introduction of 64 bit machines which can theoretically address 17 million terabytes of memory.) 3. Previous Work This research can generally be regarded as an example of active learning. This means the machine learning algorithm does not just passively train on the available training data but plays an active role in selecting the best training data. Active learning, as a standard method in machine learning, has been applied to a variety of problems in natural language processing, for example to parsing ([9]) and to automatic speech recognition ([10]). It is important to note the difference between this approach and approaches to Translation Model Adaptation ([11]) or simple subsampling techniques that are based on the actual test data. Here we assume that the test data is not known at selection time so the intention is to get the best possible translation system for every possible test data. Our previous work in this area focused on improving the ngram (type-) coverage by selecting the sentences based on the number of previously unseen n-grams they contain [8]. Section 4.2 will give a short overview over our previous best method. 4. Description of sentence sorting 4.1. Algorithm The sentences are sorted according to the following very simple algorithm. For all sentences that are not in the sorted list Calculate weight of sentences Find sentence with highest weight Add sentence with highest weight to sorted list The interesting part is the calculation of the weight of each sentence. The weight of a sentence will generally depend on the previously selected sentences. We present three different schemes to calculate the importance of a sentence. Section 4.2 presents our previous best selection  approach and section 4.3 an approach that weights sentences based on the frequency of the unseen n-grams. The method in section 4.4 uses TF-IDF to find sentences that are different from the already seen sentences. 4.2. Previous Best Weighting Scheme As stated earlier our previous work in this area focused on optimizing the sorting of the sentences based on the n-gram coverage. The best results were achieved using the following weighting term: ∑2 # (unseen n − grams) previous_best_weight(sentence) = n=1 sentence  This means for each sentence, which had not been sorted yet, the number of unseen uni- and bigrams was calculated and divided by the length of the sentence (in words). This gave significantly better results than the baseline systems where the sentences were not weighted.  4.3. Weighting of Sentences Based on N-gram Frequency  The problem with the previous best system is that every  unseen unigram gets the same weight. Words that only occur  once in the whole training data will be given the same value as  higher frequent and probably more important words. The same  is certainly true for low- and high-frequency bigrams.  This is why we wanted to make sure that our new weighting  schemes focus on high-frequency n-grams and put less weight  on lower frequency n-grams. This means the goal here is not  necessarily to optimize the coverage of the types but of the  tokens.  We use the frequency of the n-grams in the training data to  estimate their importance. The first term just sums over the  frequencies for every unseen n-gram to get the sentence  weight.  ∑ ∑ j     weight j(sentence)  =  n=1   unseen  frequency( n n−grams  −  gram)   The parameter j here determines the n-grams that are considered and was set to values of 1, 2 and 3 in the experiments. This means an unseen sentence like “Where is the hotel?” will have a high weight, especially for data in the tourism domain because we can assume that every n-gram in this sentence is rather frequent. These simple weighting schemes already show improvements over the baseline systems as shown in the later parts of the paper but they have various shortcomings. They do not take the actual translation cost of the sentence into account. (Translators generally charge per word and not per sentence). This leads to the fact that longer sentences tend to get higher weights than shorter sentences, because they will contain more, and possibly higher frequent, unseen n-grams. The focus on token-coverage is certainly very helpful but longer sentences are more difficult for the training of statistical translation models. (When training the translation model IBM1 for example every possible word alignment between sentences is considered.)  To fix these shortcomings we changed the weighting terms to  incorporate the actual length of a sentence by dividing the sum  of the frequencies of the unseen n-grams by the length of the  sentence:  ∑ ∑ j       frequency(n − gram)  weight j(sentence) =  n=1 unseen  n−grams sentence    This changes the weight to – informally speaking – “newly  covered tokens in the training data per word to translate”.  As noted earlier the algorithms for training translation models  in statistical machine translation usually work better (and  faster) on shorter sentences. For this reason we also tried to  divide by the square of the length of a sentence which prefers  even shorter sentences.  Overall the weighting terms can be written as:  ∑ ∑ j       frequency(n − gram)  weighti, j(sentence) =  n=1 unseen  n−grams sentence i    We introduce the second parameter i here to indicate the exponent of the sentence length (values used in the experiments were 0, 1 and 2). It is certainly possible to use higher values for i and j but the results indicated that higher values would not produce better results.  4.4. Weighting of sentences based on TF-IDF The second approach for the weighting of sentences is based on a different idea and uses an information retrieval method (TF-IDF) to attach a weight to sentences.  TF-IDF similarity measure  TF-IDF is a similarity measure widely used in information retrieval. The main idea of TF-IDF is to represent each document by a vector in the size of the overall vocabulary. Each document D (this will be a sentence or a set of  sentences in our case) is then represented as a vector (w1, w2,...,wm ) if m is the size of the vocabulary. The entry wk is calculated as: wk = tf k * log(idfk ) • tfk is the term frequency (TF) of the k-th word in the vocabulary in the document D i.e. the number  of occurrences.  • idfk is the inverse document (IDF) frequency of the  k-th term, given as  idf k  =  # documents # documents containing k - th term  The similarity between two documents is then defined as the  cosine of the angle between the two vectors.  Sentence weighting with TF-IDF The idea now is to use TF-IDF to find the most different sentence compared to the already selected sentences and give  this one the highest importance - this means we just select the sentence with the lowest TF-IDF score (compared to the already selected sentences) next. The first sentence here has to be randomly selected because there is nothing to compare the available sentences against in the first step. The randomly selected sentence could be: 1. Where is the hotel? In the next step the TF-IDF score for every still available sentence compared to this sentence is calculated. Sentences that do not have a single common word with this sentence will get the lowest possible TF-IDF score of 0 and one of those will again be selected, for example: 1. Where is the hotel? 2. I had soup for dinner. At some point there will be no more sentences left that only contain unseen words so every sentence will get a positive TFIDF score. The lowest TF-IDF score will then be for sentences that have the fewest number of already seen words and the highest document frequency for these words. A selected sentence in this example could be: 1. Where is the hotel? 2. I had soup for dinner. 3. This is fine. This sentence only shares the word “is” with the already sorted sentences. The word “is” most likely has a very high document frequency, thus a low IDF score which leads to an overall low score for this particular sentence. A sentence like “We ate dinner at a restaurant.” will get a higher score because the shared word “dinner” is certainly less frequent than “is” and will get a higher IDF score. The TF score in this example would be the same so it can be ignored. In the next iteration the TF score for “is” in the sorted sentences will be higher, which in turn lowers the chances to select another sentence with “is”. This means overall that this weighting scheme will make sure that at the beginning new and unseen words are covered and it will give more weight to higher frequent words later, which is the same behavior as the weighting schemes presented in section 4.3. A more information-retrieval centered motivation for the TFIDF method could be: We always select the sentence with the topic that is “furthest away” from the topic(s) of the sentences we already sorted. This will make sure that we cover all possible topics that are in our training data and might come up in the test data. Generalizing TF-IDF for N-grams TF-IDF can easily be generalized to n-grams by using every ngram as an entry in the document vectors (instead of only using words). We tried this for n-grams up to bigrams and plan on doing experiments with higher n-grams. The following section 5 will give an overview over the experiments that were done using the three presented approaches to sort sentences according to their estimated importance.  5. Experiments English-Spanish 5.1. Test and Training Data The full training data for the translation experiments consisted of 123,416 English sentences with 903,525 English words (tokens). This data is part of the BTEC corpus ([12]) with relatively simple sentences from the travel domain. The whole training data was also available in Spanish (852,362 words). The testing data which was used to measure the machine translation performance consisted of 500 lines of data from the medical domain. All translations in this task were done translating English to Spanish. 5.2. Machine Translation System The applied statistical machine translation system uses an online phrase extraction algorithm based on IBM1 lexicon probabilities ([3], [13]). The language model is a trigram language model with Kneser-Ney-discounting built with the SRI-Toolkit ([14]) using only the Spanish part of the training data. We applied the standard metrics introduced for machine translation, NIST ([15]) and BLEU ([16]). 5.3. Baseline and Previous Best Systems The baseline system that uses all available training data achieved a NIST score of 4.19 [4.03; 4.35]1 and a BLEU score of 0.141 [0.129; 0.154]1. For the baseline systems that do not use all available training data we selected sentences based on the original order of the training corpus and trained the smaller systems from this data. The second “baseline” systems were trained using the previous best approach presented in section 4.2. Translation systems trained on these (smaller) data sets give the scores shown in diagrams 1 and 2. The diagrams clearly illustrate that after a rather steep increase of the scores until the translation of approximately 400,000 words the scores of the baseline increase only slightly until they reach the final score for the system using all available training data. The previous best selection especially benefits at the beginning for a lower number of translated words and hits a NIST score of 4.0 at 170,000 translated words, which is very close to the confidence interval and only about 5% worse than the best overall score. A NIST score of 4.1 is already achieved at 220,000 translated words and 2% worse than the final baseline of 4.19. At 10,000 translated words the previous best system achieves a NIST score of 2.56, compared to a baseline of 2.04. 
Our participation in the IWSLT 2005 speech translation task is our ﬁrst effort to work on limited domain speech data. We adapted our statistical machine translation system that performed successfully in previous DARPA competitions on open domain text translations. We participated in the supplied corpora transcription track. We achieved the highest BLEU score in 2 out of 5 language pairs and had competitive results for the other language pairs. 
The Microsoft Research translation system is a syntactically informed p hrasal S MT system that u ses a p hrase translation mod el based on d ep end ency treelets and a g lobal reord ering mod el based on the sou rce d ep end ency tree. These mod els are combined w ith sev eral other k now led g e sou rces in a log linear manner. The w eig hts of the ind iv id u al comp onents in the log -linear mod el are set by an au tomatic p arameter-tu ning method . W e g iv e a brief ov erv iew of the comp onents of the system and d iscu ss its p erformance at IW S L T in tw o track s: J ap anese to E ng lish ( su p p lied d ata and tools), and E ng lish to C hinese ( su p p lied d ata and tools).  
The MIT-LL/AFRL MT system is a statistical phrase-based translation system that implements many modern SMT training and decoding techniques. Our system was designed with the long term goal of dealing with corrupted ASR input for Speech-to-Speech MT applications. This paper will discuss the architecture of the MIT-LL/AFRL MT system, and experiments with manual and ASR transcription data that were run as part of the IWSLT-2005 Chinese-to-English evaluation campaign.1 1. Introduction In recent years, the development of statistical methods for machine translation has resulted in high quality translations that can be used in real applications with increasing confidence. Specific advancements include: • Extracting word alignments from parallel corpora [1] [2] • Learning and modeling the translation of phrases [4] [5] • Combining and optimizing model parameters [6] [7] [8] • Decoding and rescoring techniques [9] [10] Our system draws from these advances and implements a number of these techniques including log-linear model combination and minimum error rate training to translate foreign language sentences. We developed our system during preparation for IWSLT-2005 to serve as a platform for future research. Most of the components of our system have been developed in-house in order to facilitate future experimentation. In subsequent sections, we will discuss the details translation system including our alignment and language models and methods we’ve implemented for optimization and decoding. The basic translation training and decoding processes are shown in Figure 1. We start with a word alignment extracted from a training set using GIZA++. These alignments are expanded and phrases are counted to form the phrase translation model. Language models are then trained from the English side of training set (and possibly with other English texts, if available). Using development bitexts separated from the training set, we then employ a minimum error rate training process to 
In this paper we propose a phrase-based translation system. In the system, we use phrase translation model instead of word-based model. An improved method of computing phrase translation probability is studied. We translate numeral phrases first by using a standard templates depository. We develop a phrase-based decoder that employs a beam search algorithm. To make the result more reasonable, we apply those words with fertility probability of zero. We improve the previously proposed tracing back algorithm to get the best path. Some experiments concerned are presented. 
In this paper, we present a novel distortion model for phrase-based statistical machine translation. Unlike the previous phrase distortion models whose role is to simply penalize nonmonotonic alignments[1, 2], the new model assigns the probability of relative position between two source language phrases aligned to the two adjacent target language phrases. The phrase translation probabilities and phrase distortion probabilities are calculated from the N-best phrase alignment of the training bilingual sentences. To obtain Nbest phrase alignment, we devised a novel phrase alignment algorithm based on word translation probabilities and N-best search. Experiments show that the phrase distortion model and phrase translation model improve the BLEU and NIST scores over the baseline method. 1. Introduction In recent years, phrase-based translation models have become the mainstream of statistical machine translation, because they can represent context-based word selection and local word reordering better than word-based translation models. Previous phrased-based translation models[1, 2], however, are not effective for global phrase reordering, because their distortion model is too simplistic. As it was designed simply to penalize nonmonotonic phrase alignment, it is difﬁcult to handle translations that require complex word reordering, such as between Japanese and English. In this paper, we present a novel distortion model for phrase-based statistical machine translation. It models the probability of relative position between two source language phrases aligned to the two adjacent target language phrases. To obtain the distortion model, we ﬁrst make a phrase alignment of each sentence pair in the training corpus. We then calculate the phrase distortion probability from the relative frequency of respective events in the phrase aligned training corpus. In order to cope with the sparse data problem, word reordering is classiﬁed into four states: monotone, monotone-gap, reverse, and reverse-gap. Phrases are also classiﬁed based on the part of speech of the ﬁrst and last word. We need phrase translation probabilities to get phrase  alignment from the training corpus, but we need phrase alignment to get phrase translation probabilities. To solve this chicken and egg problem, we devised a novel phrase alignment algorithm using word translation probabilities and forward beam search. Phrase distortion probabilities mentioned above are calculated from the result of this phrase alignment. The phrase alignment algorithm can easily be extended to obtain N-best phrase alignment using backward A* search, such as [3]. We found that phrase translation probabilities calculated from the result of this N-best phrase alignment improve the translation accuracy signiﬁcantly. In the following sections, we ﬁrst explain our translation model including the phrase distortion model and phrase alignment algorithm. We then report the experiments’ results and show the effectiveness of our phrase distortion model. 2. Baseline Translation Model Ispneraotrbhcaehbifnloiotryisthyoefctthhaaregntenatreg(lEeatnpsgpelnirsothea)ncchseent£otegnmivcaeecn£h¢ tithnheeasttormuanarcxseliam(tfiioozrnee,sigtwhnee) ¥§spseer¦¨nno£ttb© ee¤annb cciceelait¥§n¤y¦.b¥¤ e¦B© ££dye.cauonsmdinpsgoousBeracdyeienssteonruttehlene,cpetrhopedroupbcoatsboteiflrititaoyrrggpeivtroseebnnatbtaeirnlgicteyet  £¢  "!$#%('& ¥¦£© ¤ )'0!$#%(1& ¥§¦ ¤ © £  ¥¦£   Here, the models for computing ¥¦£  and ¥¦ ¤ © £  are called  the language model and translation model, respectively.  In phrase-based statistical machine translation, source  sentence ¤ is segmented into a sequence of 2 phrases ¤53 64 ,  a£ 3 n7 .dTeaargchet  source phrase ¤'3 7 phrases may be  is translated reordered.  into  a  target  phrase  The translation l@ a¦¨tAio7nBDpCEro7¨F b6ab.ility 8  m¦ ¤13 o7 d© £ 3e79l  used in [1] is the product of transand relative distortion probability  ¥¦ ¤ 3 64 © £ 3 46    G4 7IH  6  8  ¦  ¤3 7  ©£37   @  ¦¨A  7PBQCE7¨F  6    (1)  Figure 1: Example of relative distortion  ¢¡ iwesnhtdreaprneossA lia7titodednenoionfttetohseththseoeus-rttchaerttpaphrgoreasitsteipothnrraaonsfsel,tahateenddsoinC u7¨trFoc6 ethdpeeh¦nroaB steest ht-htahet target phrase. Translation probability is calculated from the relative frequency of the respective source phrase given the target phrase.  8 ¦ ¤ 3 © £3   £¥ ¤§£¦©¤§¨¦ ¨¦ ¤3 ¦ ¤£33  £3   (2)  £¥¤§¦©¨  where ¤ 3 aligned  to  ¦¤3 the  £t3 a rggeivtepshtrhaesefr£3eqinuetnhceypoafratlhleel  source phrase corpus. Note  that, due to Bayes rule, the translation direction is inverted  from a modeling standpoint.  The distortion model used in [1] is empirically deﬁned as  follows, with an appropriate value for parameter .  @ ¦¨A 7 BDC 7¨F 6   "! #%$ F& $('0) F 6 !  (3)  Figure 1 illustrates the idea of relative distortion, using  Japanese to English translation as an example. The target  English sentence is generated from left to right by translating  the source Japanese phrases in arbitrary order. Suppose we  are generating target phrase “help” by translating the source  13254 phrase “  ”. The source phrase translated into the pre-  687@9BA vious target phrase “disposed to” is “ start position of the source phrase for  this  ”. target  pShinracseethA e7  C ED C is 4, and the target phrase  eC n7¨F d6  position is 8, the  of the source phrase relative distortion is  for B  previB ous.  The purpose of the distortion model in Equation 3 is sim-  ply to penalize nonmonotonic phrase alignment. It cannot  represent the general tendency of global phrase reordering, in  terms of the distance and direction of the movement, as well  as their dependency on phrase type. For example, for English  to Japanese translation, the verb phrase generally moves to-  ward the end of the sentence. In the next section, we present  a novel phrase distortion model that considers these aspects.  3. Phrase Distortion Model We deﬁne our phrase distortion model as the probability of relative distance between two source language phrases that are aligned to the two adjacent target language phrases,  ¥§¦ @ © £3 7¨F 6  £3 7  ¤ 3 7 F 6  ¤ 3 7   (4)  dwsoihsuterarcneec£3ep7¨hFbre6 ataswenesdea£n3 l7 i¤ ga3 7 rnF ee6 dadatnjoadc£3e¤ 7¨3 n7F .t6  target and  £  3p7 h,raanseds,@  ¤  3  7¨F is  6 and ¤ 3 7 are the relative  Since the above distortion model involves too many pa-  rameters to estimate, we approximat@ e it in several steps. First, we classify the relative distance into four states:  F monotone: The two source phrases are adjacent, and are in the same order as the two target phrases. F monotone-gap: The two source phrases are not ad-  jacent, but are in the same order as the two target  phrases. F reverse: The two source phrases are adjacent, but are in reverse order of the two target phrases. F reverse-gap: The two source phrases are not adjacent, and are in reverse order as the two target phrases.  We then classify each phrase by the part of speech of its  head word. We deﬁne (arguably) the ﬁrst word of each phrase  as head word for English and Chinese, and the last word of  each phrase as head word for Japanese.  Finally, we consider a series of distortion models that  have increasingly complex dependencies.  ¥§¦ ¥§¦ ¥§¦ ¥§¦ ¥§¦  @ @ @ @ @  © © © ©  ££££GGGG  AIAIAIAIH§H§H§H§HHHH  ¦ ¦ ¦ ¦  £££¤333 3  7 7 7 7  " F F F  6 6 6  %%%  £¥£¥£¥GGG  AIAIAIH§H§H§HHH  ¦ ¦ ¦  ¤ £¤3  3 3  7 77¨¥F   6  £¥%G AI£¥HPG HAI¦H§¤ H3  ¦ 7F  ¤ 367  ¥   £¥G  AQH§H  ¦  ¤  3  7      £G IH§H SR where A ¦  represents the classiﬁcation of each phrase.  When we classify each phrase by the part of speech of its  head word, we identify the above ﬁve distortion models as  type 1, 2s, 3s, 4s and 5s, respectively.  Figure 2 and Figure 3 show examples of phrase distor-  tion models type 2s and type 3s, respectively, for Japanese  to English translation. Here, monotone, monotone-gap, re-  £G IH§H £¥G IHPH verse, tivA ely.¦  £3  reverse-gap are represented by 1, 2, -1@,  In 7F  F6 i,gureresp3e,ctthiveelﬁyr.stTthherefeoeulretmh eanntds  are ﬁfth  ,-2,A res¦p¤1e3 7c -, element are  the distortion probability and frequency of this event in the  training corpus.  Since we are not sure whether it is appropriate to deﬁne  £¥G QH§H TR the head word of also tried “dual”  deaiscthorptihornasme ofodrelesa,cwh hlaenreguagA e  a¦  priori, we  of each  phrase represented by both the ﬁrst and the last word of each  £¥G QH§H £¥G IH§H phrase. of 3d is  We call them type shown in Figure 4,  2d, 3d, where  4A d,  a¦ n¤ 3 d7   5d. and  AnA  ex¦a£3m7¨F p6le  are represented by two POS tags.  .------.111111.¢¢¢¢¢¢¡¡¡¡¡¡  ------£¥£¥£¥¨¢¨¢¨¢¡¡¡¤§¤§¤§¦¦¦©©©  ¢¢¡¡--¡ £¥¨¢----¡"#¤§¦!!!©¡  WRB WRB|0.34|17 PRP PRP|0.75|3 DT NNS|1|2 NNP NNP|0.0526315789473684|1 NNP TO|0.333333333333333|1 . NN|1|1  ...  Figure 4: Example of phrase distortion model type 3d  ...  ¢¡ £¥¤§¦ -1 -  |0.456879958687386|9732  ¢¡ ¨¢¡© -1 -  |0.380326288979142|5525  ¡ ¡ -1  -  |0.0594823032223983|563  $&%('0) $1%2'0) -2  -  |0.578082191780822|422  354¡ 354¡ -2  -  |0.159919507575758|1351   ! -2 - |0.00304719568373694|1020  ...  Figure 2: Example of phrase distortion model in type 2s  ... ¢¡ £¥¤§¦ -1 ¢¡ £¥¤§¦ -1 ¢¡ £¥¤§¦ -1 ¢¡ ¨¢¡© -1 ¢¡ ¨¢¡© -1 ¢¡ ¨¢¡© -1 -  WDT|0.676470588235294|69 WP|0.360189573459716|152 WRB|0.309219858156028|218 ,|0.175824175824176|16 .|0.216|27 CC|0.130434782608696|3  ...  Figure 3: Example of phrase distortion model type 3s  4. Phrase Alignment  The phrase distortion model in the previous section is com-  puted from the Viterbi phrase alignment of the training cor-  pus. In order to obtain this phrase alignment, we search for  the segmentation mizes the product  of of  source and target lexical translation  psreonbtaebnicleitsietsh¥at¦  ¤  m3 7 ©a£3 x7 i -,  ¦ ¤ 3¢ 64  £3¢ 46    '0!$#%7 )96 18 ( & 6)  G 7  4 H  6  ¥§¦  ¤13 7  © £3  7    (5)  Here, lexical translation probability [4] is an approximation of phrase translation probability based on the word translation probabilities estimated by using GIZA++[5],  ¥§¦ ¤ 3 © £3    G @  A 7  ¥¦ ¤ @ © £ 7   (6)  @ where ¤ and £ 7 are words in the phrases.  The phrase alignment is obtained by following these  steps:  1. All pairs of one word from the source sentence and one word from the target sentence are considered as the phrase translation candidates. 2. If the lexical translation probability of a phrase translation candidate is less than the threshold, it is deleted. 3. Each phrase translation candidate is expanded toward its neighbors as described in [1]. 4. If the lexical translation probability of the expanded phrase translation candidate is less than the threshold, it is deleted. 5. This expansion and deletion is repeated until no further expansion is possible. 6. Search for consistent phrase alignment among all combinations of the above phrase translation candidates. We can obtain the Viterbi phrase alignment by using beam search from the beginning of the sentence to the end. We also can obtain the N-best phrase alignment by using A* search as described in [3]. Here, we must consider three parameters: phrase translation candidate threshold, beam width, and the number of N-best alignments. Preliminary tests have shown that the appropriate parameter is 1e-15 for phrase candidate threshold, 1000 for beam width, and 20 for the number of N-best. Nbest phrase alignment is used for computing the phrase translation model, and Viterbi alignment is used for computing the phrase distortion model. Figure 5 shows an example of the best 3 phrase alignments for a Japanese-English bilingual sentence. Each line represents a phrase translation candidate, where the ﬁrst item is source phrase, second and third items are start and end positions of the phrase in the source sentence, fourth and ﬁfth items are the parts of speech of the ﬁrst and last words in the source phrase. After that, the same information for the target phrase is listed. 5. Corpus and Tools We participated in Supplied Data + Tools Track in JapaneseEnglish and Chinese-English translation because we need a part of speech tagger to obtain part of speech information  © ¡ ¢ £¥¤ ¦¨§ - | 6|6|  - ¢!  9 ||1 |5-|¢! ¡  - |14¡ |.|5|5|.|.  -154¡  |the light was red|1|4|DT|JJ  2.71232e-06  ¢© £¡¤ ¦¨§ ¦¨§  -  | 6|6||19 ||23|-|¢5¡|! -|¡-  ¢¡ ¡ 1 1 | -  |the light|1|2|DT|NN  54¡ 54¢¡ 1 1 |  -  |was red|3|4|VBD|JJ  ¢!- |.|5|5|.|.  2.4524e-06  ¢£¥© ¤ ¡ ¦§ ¦¨§ ¦§  -  || 36||9 36||||014||¡ 25||--1¢¡4! -||¡ ¢-¡15 |--41¢¢¡ ¡!  ¡1-  |the light|1|2|DT|NN  54¡ 54¡ 1 1 |  -  |was|3|3|VBD|VBD  |red|4|4|JJ|JJ  |.|5|5|.|.  2.38498e-06  Figure 5: Example of N-best phrase alignment for Japanese-English bilingual sentence  for our phrase distortion model. We did not use the word segmentation information of Japanese and Chinese provided in the supplied data because of the constraints of the POS tagger we used. Word segmentation and POS tagging for Japanese was done by ChaSen[6]. As ChaSen’s part of speech has a hierarchy, we used the ﬁrst two layers. Word segmentation and POS tagging for Chinese was done by our own tool[7]. English is tokenized by a tool provided by LDC (tokenizer.sed)[8], and POS tagged by MXPOST[9]. Word translation probabilities are obtained by using GIZA++[5]. English are lowercased for training. We used a back-off word trigram model as the language model. It is trained from the lowercased English side of the parallel training corpus using Palmkit[10]. For Japanese-English translation, we used a minimum error rate training tool provided by CMU[11]. The features used were the following: F Phrase translation probability (both directions)[1] F Lexical translation probability (both directions)[4] F Word penalty[12] F Phrase distortion probability We didn’t apply minimum error rate training to ChineseEnglish translation because we found no signiﬁcant improvements for some reasons. 6. Experiments and Discussions First, we compared our phrase extraction method with the conventional method described in [1]. Table 1 shows the NIST and BLEU scores for development set 2 in JapaneseEnglish translation. We found that our phrase extraction method using N-best phrase alignment signiﬁcantly improved the translation accuracy.  We then compared our phrase distortion model to the conventional distortion model[1]. Figure 6 shows the BLEU scores of the Japanese-English and Chinese-English translations created with various distortion models. Here, distortion model type 0 represents the conventional model [1]. Table 2 and Table 3 are NIST and BLEU scores for development set 2 of Japanese-English translation with various distortion models, before and after minimum error rate training. We found that, in general, distortion models type 2s and 3s yield a slight improvement in accuracy.  Table 1: Translation accuracy for development set 2 of Japanese-English with different phrase extraction methods phrase extraction NIST score BLEU score  conventional our method  7.6162 8.8159  0.3375 0.4471  Table 2: Translation accuracy for development set 2 of Japanese-English with different distortion models (before MER training) distortion type NIST score BLEU score  0  8.7706  0.4050  
A Chinese sentence is represented as a sequence of characters, and words are not separated from each other. In statistical machine translation, the conventional approach is to segment the Chinese character sequence into words during the pre-processing. The training and translation are performed afterwards. However, this method is not optimal for two reasons: 1. The segmentations may be erroneous. 2. For a given character sequence, the best segmentation depends on its context and translation. In order to minimize the translation errors, we take different segmentation alternatives instead of a single segmentation into account and integrate the segmentation process with the search for the best translation. The segmentation decision is only taken during the generation of the translation. With this method we are able to translate Chinese text at the character level. The experiments on the IWSLT 2005 task showed improvements in the translation performance using two translation systems: a phrase-based system and a ﬁnite state transducer based system. For the phrase-based system, the improvement of the BLEU score is 1.5% absolute. 1. Introduction In Chinese texts, words composed of single or multiple characters are not separated by white space, which is different from most of the European languages. In statistical machine translation, the conventional way is to segment the Chinese character sequence into Chinese words before the training and translation. We compared different segmentation methods in [1]. The training and test texts can be segmented into words or used at the character level. In the experiments in [1], the translation results with the previous method outperformed the results with the latter one. Here we continued the investigation on the translation of the text at the character level and developed a new method yielding better translation results than when translation is at the word level. This method handles all the segmentation alternatives instead of only the single-best segmentation. The single-best  one may contain errors or may be not optimal with respect to the training corpus. Instead of reading a single best segmented sentence, our system handles all the segmentation alternatives by reading a segmentation lattice. Similar approaches were applied in the speech translation, e.g. [2], where the speech recognition and text translation are combined by using the recognition lattices. We also weight the different segmentations with a language model trained on the Chinese corpus at the word level. Weighting the word segmentation by language model cost was introduced in [3]. To verify the improvements with the integrated segmentation method, we experimented on two translation systems: translation with the weighted ﬁnite state transducers and translation with the phrase based approach. On the IWSLT 2005 task [4], using a phrase-based translation system, the improvement of the BLEU score reached 1.5% absolute. This paper is structured as follows: ﬁrst we will brieﬂy review the baseline statistical machine translation system in Section 2. In Section 3 we will discuss the idea, the theory, as well as the generation process of the integrated segmentation approach compared to the conventional approach. The experimental results for the IWSLT 2005 task [4] will be presented in Section 4.  2. Statistical machine translation system  2.1. Bayes decision rule  In statistical machine translation, we are given a source language sentence f1J = f1 . . . fj . . . fJ , which is to be translated into a target language sentence eI1 = e1 . . . ei . . . eI . Among all possible target language sentences, we will choose the sentence with the highest probability:  eˆI1ˆ = argmax P r(eI1|f1J )  (1)  eI1 ,I  = argmax P r(eI1) · P r(f1J |eI1)  (2)  eI1 ,I  The decomposition into two knowledge sources in Equation 2 is known as the source-channel approach to statistical machine translation [5]. It allows an independent model-  Single-best segmentation:  T ext  / Segmentation: Equation 3  / Decision: Equation 5  / T ranslation  Segmentation lattice:  T ext  / Global decision: Equation 6  / T ranslation  Figure 1: Segmentation methods  ing of the target language model P r(eI1) and the translation model P r(f1J |eI1)1. In our system, the translation model is trained on a bilingual corpus using GIZA++ [6], and the language model is trained with the SRILM toolkit [7]. 2.2. Weighted ﬁnite-state transducer-based translation We use the weighted ﬁnite-state tool by [8]. A weighted ﬁnite-state transducer (Q, Σ ∪ { }, Ω ∪ { }, K, E, i, F, λ, ρ) is a structure with a set of states Q, an alphabet of input symbols Σ, an alphabet of output symbols Ω, a weight semiring K, a set of arcs E, a single initial state i with weight λ and a set of ﬁnal states F weighted by the function ρ : F → K. A weighted ﬁnite-state acceptor is a weighted ﬁnite-state transducer without the output alphabet. A composition algorithm is deﬁned as: Let T1 : Σ∗ × Ω∗ → K and T2 : Ω∗ × Γ∗ → K be two transducers deﬁned over the same semiring K. Their composition T1◦T2 realizes the function T : Σ∗ × Γ∗ → K. By using the structure of the weighted ﬁnite-state transducers, the translation model is simply estimated as the language model on a bilanguage of source phrase/target phrase tuples, see [9]. 2.3. Phrase-based translation The phrase-based translation model is described in [10]. A phrase is a contiguous sequence of words. The pairs of source and target phrases are extracted from the training corpus and used in the translation. The phrase translation probability P r(eI1|f1J ) is modeled directly using a weighted log-linear combination of a trigram language model and various translation models: a phrase translation model and a word-based lexicon model. These translation models are used for both directions: p(f |e) and p(e|f ). Additionally, we use a word penalty and a phrase penalty. The model scaling factors are optimized with respect to some evaluation criterion [11]. 1The notational convention will be as follows: we use the symbol P r(·) to denote general probability distributions with (nearly) no speciﬁc assumptions. In contrast, for model-based probability distributions, we use the generic symbol p(·).  3. Segmentation methods 3.1. Conventional segmentation methods In this section, we give a short overview of the current Chinese word segmentation methods in statistical machine translation, most of these methods can be classiﬁed into three categories: • The training and test texts are segmented with an automatic segmentation tool. Many segmentation tools use the dynamic programming algorithm and ﬁnd the word boundaries which maximize the product of the word frequencies. But the segmentation may contain some errors, and we also found that a much more accurate word segmentation does not always lead to a large improvement in the translation performance. • The training and test texts are segmented manually. Manual segmentation avoids segmentation errors but requires a human effort. Moreover, the correct segmentation will not result in the best translation result, if the segmentations in the test and training sets are inconsistent. • Each Chinese character is treated as a word Training and translation at the Chinese character level do not require additional tool or human effort. But [1] showed that the translation results are not so good as the results obtained when translation is at the word level. To minimize the number of lexicon entries and to ensure the consistency of the segmentations in the training and in the translation, we developed a new segmentation method, which uses the training text at the word level and translate the test text at the character level. 3.2. Idea Figure 1 shows the translation procedures. With the conventional method, only a single-best word segmentation is transferred to the search for the best translation. This approach is not ideal because the segmentation may not be optimal for the translation. Taking hard decisions in word segmentation may lead to loss of the correct Chinese words.  Table 1: Example of a sentence and its translations.  Source sentence in characters: Manually segmented source sentence: Translation by single-best segmentation: Translation by segmentation lattice: One reference:  zai na li ban li deng ji shou xu ? zai nali banli dengji shouxu ? where to go through boarding formalities ? where do i make my boarding arrangements ? where do i complete boarding procedures ?  With the integrated segmentation method in Figure 1, for one input sentence, we take different segmentation alternatives into account and represent them as a lattice. The input to the translation system is then a set of lattices instead of the segmented text. The search decision of the word segmentation is therefore combined with the translation decision, and the best segmentation of a sentence is only selected while the translation is generated.  3.3. Theory In this section, we will explain the methods in Figure 1 in detail. First, we will describe a general word segmentation model and then how it is used as a single-best segmentation or as a segmentation lattice. A Chinese input sentence is denoted here as cK1 at the character level and f1J at the word level, where c1 . . . ck . . . cK are the succeeding characters and f1 . . . fj . . . fJ are the succeeding words.  Word segmentation model The best segmented Chinese sentence fˆ1Jˆ with Jˆ words can be represented as: fˆ1Jˆ = argmax P r(f1J |cK1 ) f1J ,J = argmax P r(cK1 |f1J ) · P r(f1J ) , (3) f1J ,J which suggests a decomposition into two sub-models:  1. Correspondence of the word sequence f1J and the character sequence cK1 For one Chinese word sequence, its character sequence is unique. Hence, we can deﬁne the probability as one, if the character sequence of a word sequence is the same as the input, and as zero otherwise:  P r(cK1 |f1J ) =  0 : C(f1J ) = cK1 1 : C(f1J ) = cK1  Here, C denotes the separation of a word sequence into characters.  2. The source language model at the word level:  J  P r(f1J ) =  P r(fj |f1j−1)  j=1  J  ∼=  p(fj |fjj−−n1+1)  (4)  j=1  In practice, we use an n-gram language model as shown in the Equation 4.  Single-best segmentation  In the conventional approach, only the best segmentation fˆ1Jˆ is translated into the target sentence:  eˆI1ˆ = argmax P r(eI1|fˆ1Jˆ)  (5)  eI1 ,I  Segmentation lattice  In the transfer of the single-best segmentation from Equation 3 to Equation 5, some segmentations which are potentially optimal for the translation may be lost. Therefore, we combine the two steps. The search is then rewritten as:  eˆI1ˆ = argmax P r(eI1|cK1 )  (6)  I,eI1         =  argmax I ,eI1     f1J  P r(f1J , eI1|cK1 )        =  argmax I ,eI1    f1J  P r(f1J |cK1 ) · P r(eI1|f1J , cK1 )  ∼=  argmax I ,eI1  max f1J  P r(f1J |cK1 ) · P r(eI1|f1J )  Because our translation model in Equation 1 is based on the words, here we make the approximation that the target sentence eI1 depends only on the word based source sentence f1J , but not on the character based one cK1 . We also use the maximum instead of the sum over the segmentations. In this way, the segmentation model and the translation model are combined into a model for the global decision.  0 zai 1 na 2 li 3 ban 4 li 5 deng 6 ji 7 shou 8 xu 9 ? 10  Figure 2: Segmentation lattice: input sentence at the character level as a linear automaton.  banli  na 2 li 14 ban  0 zai 1  nali banli 3 ban 4  5  deng dengji  6  deng li 13 dengji  ji 7  12 shouxu shouxu shou shou  9 8  ? xu 11 ?  10  Figure 3: Segmentation lattice without weights.  shouxu /0.13 14  ? /0.48 15  0  zai /1.8  
This paper presents a novel automatic sentence segmentation method for evaluating machine translation output with possibly erroneous sentence boundaries. The algorithm can process translation hypotheses with segment boundaries which do not correspond to the reference segment boundaries, or a completely unsegmented text stream. Thus, the method is especially useful for evaluating translations of spoken language. The evaluation procedure takes advantage of the edit distance algorithm and is able to handle multiple reference translations. It efﬁciently produces an optimal automatic segmentation of the hypotheses and thus allows application of existing well-established evaluation measures. Experiments show that the evaluation measures based on the automatically produced segmentation correlate with the human judgement at least as well as the evaluation measures which are based on manual sentence boundaries. 1. Introduction Evaluation of the produced results is crucial for natural language processing (NLP) research in general and, in particular for machine translation (MT). Human evaluation of MT system output is a time consuming and expensive task. This is why automatic evaluation is preferred to human evaluation in the research community. A variety of automatic evaluation measures have been proposed and studied over the last years. All of the wide-spread evaluation measures like BLEU [1], NIST [2], and word error rate compare translation hypotheses with human reference translations. Since a human translator usually translates one sentence of a source language text at a time, all of these measures include the concept of sentences, or more generally, segments1. Each evaluation algorithm expects that a machine translation system will produce exactly one target language segment for each source language segment. Thus, the total number of segments in the automatically translated document must be equal to the number of reference segments in the manually translated document. In case of speech translation, the concept of sentences is in general not well-deﬁned. A speaker may leave a sentence 1Throughout the paper, we will use the term “segment”, by which we mean a sequence of words that may or may not have proper punctuation.  incomplete, make long pauses, or speak for a long time without making a pause. A human transcriber of speech is usually able to subjectively segment the raw transcriptions into sentence-like units. In addition, if he or she was instructed to produce meaningful units, each of which has clear semantics, then these sentence-like units can be properly translated into sentence-like units in the target language. However, an automatic speech translation system is expected to translate automatically recognized utterances. In the few speech translation evaluations in the past, an automatic speech recognition (ASR) system was forced to generate segment boundaries in the timeframes which had been deﬁned by a human transcriber. This restriction implied that a manual transcription and segmentation of the test speech utterances had to be performed in advance. We argue that this type of evaluation does not reﬂect real-life conditions. In an on-line speech translation system, the correct utterance transcription is unknown to the ASR component, and segmentation is done automatically based on prosodic or language model features. This automatic segmentation should deﬁne the initial sentence-like units for translation. In addition, some of these units may then be split or merged by the translation system to meet the constraints or modelling assumptions of the translation algorithm. Under these more realistic conditions the automatic segmentation of the input for MT and thus the segment boundaries in the produced translations do not correspond to the segment boundaries in the manual reference translations. Therefore, most of the existing MT error measures will not be applicable for evaluation. In this paper, we propose an algorithm that is able to ﬁnd an optimal re-segmentation of the MT output based on the segmentation of the human reference translations. The algorithm is based on the Levenshtein edit distance algorithm [3], but is extended to take into account multiple human reference translations for each segment. As a result of this segmentation we obtain a novel evaluation measure – automatic segmentation word error rate (AS-WER). The paper is organized as follows. In Section 2, we review the most popular MT evaluation measures and discuss if and how they can be modiﬁed to cope with automatic segmentation of MT output. Section 3 presents the algorithm for automatic segmentation. In Section 4, we compare the error measures based on automatic segmentation with the error measures based on human segmentation and show that the  new evaluation measures give accurate estimates of translation quality for different tasks and systems. We conclude the paper with Section 5, where we discuss the applications of the new evaluation strategy and future work.  2. Current MT Evaluation Measures Here, we analyze the most popular MT evaluation measures and their suitability for evaluation of translation output with possibly incorrect segment boundaries. The measures that are widely used in research and evaluation campaigns are WER, PER, BLEU, and NIST. Let a test document consist of k = 1, . . . , K candidate segments Ek generated by an MT system. We also assume that we have R reference translation documents. Each reference document has the same number of segments, where each segment is a translation of the “correct” segmentation of the manually transcribed speech input2. If the segmentation of the MT output corresponds to the segmentation of the manual reference translations, then for each candidate segment Ek, we have R reference sentences Erk. Let Ik denote the length of a candidate segment Ek, and Nrk the reference lengths of each reference segment Erk. From the reference lengths, an optimal reference segment length Nk∗ is selected as the length of the reference with the lowest segment-level error rate or best score [4]. With this, we write the total candidate length over the document as I := k Ik, and the total reference length as N ∗ := k Nk∗.  2.1. WER  The segment-level word error rate is deﬁned as the Levenshtein distance dL(Ek, Erk) between a candidate segment Ek and a reference segment Erk, divided by the reference length Nk∗ for normalization. For a whole candidate corpus with multiple references, the segment-level scores are combined, and the WER is deﬁned to be:  WER  :=  
We give an overview of the RWTH phrase-based statistical machine translation system that was used in the evaluation campaign of the International Workshop on Spoken Language Translation 2005. We use a two pass approach. In the ﬁrst pass, we generate a list of the N best translation candidates. The second pass consists of rescoring and reranking this N -best list. We will give a description of the search algorithm as well as the models that are used in each pass. We participated in the supplied data tracks for manual transcriptions for the following translation directions: Arabic-English, Chinese-English, English-Chinese and Japanese-English. For Japanese-English, we also participated in the C-Star track. In addition, we performed translations of automatic speech recognition output for ChineseEnglish and Japanese-English. For both language pairs, we translated the single-best ASR hypotheses. Additionally, we translated Chinese ASR lattices. 1. Introduction We give an overview of the RWTH phrase-based statistical machine translation system that was used in the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2005. We use a two pass approach. First, we generate a word graph and extract a list of the N best translation candidates. Then, we apply additional models in a rescoring/reranking approach. This work is structured as follows: ﬁrst, we will review the statistical approach to machine translation and introduce the notation that we will use in the later sections. Then, we will describe the models and algorithms that are used for generating the N -best lists, i.e., the ﬁrst pass. In Section 4, we will describe the models that are used to rescore and rerank this N -best list, i.e., the second pass. Afterward, we will give an overview of the tasks and discuss the experimental results.  1.1. Source-channel approach to SMT  In statistical machine translation, we are given a source language sentence f1J = f1 . . . fj . . . fJ , which is to be translated into a target language sentence eI1 = e1 . . . ei . . . eI . Among all possible target language sentences, we will choose the sentence with the highest probability:  eˆI1ˆ = argmax P r(eI1|f1J )  (1)  I ,eI1  = argmax P r(eI1) · P r(f1J |eI1)  (2)  I ,eI1  This decomposition into two knowledge sources is known as the source-channel approach to statistical machine transla- tion [1]. It allows an independent modeling of the target language model P r(eI1) and the translation model P r(f1J |eI1)1. The target language model describes the well-formedness of the target language sentence. The translation model links the source language sentence to the target language sentence. The argmax operation denotes the search problem, i.e., the generation of the output sentence in the target language.  1.2. Log-linear model  An alternative to the classical source-channel approach is the direct modeling of the posterior probability P r(eI1|f1J ). Us- ing a log-linear model [2], we obtain:  P r(eI1|f1J ) =  exp exp  M m=1  λmhm(eI1  ,  f1J  )  M m=1  λmhm(e  I 1  ,  f1J )  (3)  e  I 1  The denominator represents a normalization factor that depends only on the source sentence f1J . Therefore, we can omit it during the search process. As a decision rule, we ob- tain:  M  eˆI1ˆ = argmax  λmhm(eI1, f1J )  (4)  I ,eI1  m=1  1The notational convention will be as follows: we use the symbol P r(·) to denote general probability distributions with (nearly) no speciﬁc assumptions. In contrast, for model-based probability distributions, we use the generic symbol p(·).  This approach is a generalization of the source-channel approach. It has the advantage that additional models h(·) can be easily integrated into the overall system. The model scaling factors λM 1 are trained according to the maximum entropy principle, e.g., using the GIS algorithm. Alternatively, one can train them with respect to the ﬁnal translation quality measured by an error criterion [3]. For the IWSLT evaluation campaign, we optimized the scaling factors with respect to a linear interpolation of WER, PER, BLEU and NIST using the Downhill Simplex algorithm from [4].  1.3. Phrase-based approach  The basic idea of phrase-based translation is to segment the given source sentence into phrases, then translate each phrase and ﬁnally compose the target sentence from these phrase translations. This idea is illustrated in Figure 1. Formally, we deﬁne a segmentation of a given sentence pair (f1J , eI1) into K blocks:  k → sk := (ik; bk, jk), for k = 1 . . . K. (5)  Here, ik denotes the last position of the kth target phrase; we set i0 := 0. The pair (bk, jk) denotes the start and end positions of the source phrase that is aligned to the kth target phrase; we set j0 := 0. Phrases are deﬁned as nonempty con- tiguous sequences of words. We constrain the segmentations so that all words in the source and the target sentence are covered by exactly one phrase. Thus, there are no gaps and there is no overlap. For a given sentence pair (f1J , eI1) and a given segmentation sK1 , we deﬁne the bilingual phrases as:  e˜k := eik−1+1 . . . eik  (6)  f˜k := fbk . . . fjk  (7)  I = i4  i3  target positions  i2  i1  0 = i0  0 = j0 j2 j1  j3  b2 b1 b3  b4  source positions  j4 = J  Figure 1: Illustration of the phrase segmentation. Note that the segmentation sK1 contains the information on the phrase-level reordering. The segmentation sK1 is introduced as a hidden variable in the translation model. Therefore, it would be theoretically correct to sum over all possible segmentations. In practice, we use the maximum approximation for this sum. As a result, the models h(·) depend not  only on the sentence pair (f1J , eI1), but also on the segmentation sK1 , i.e., we have models h(f1J , eI1, sK1 ). 2. Search algorithms The RWTH phrase-based system supports two alternative search strategies that will be described in this section. Translating a source language word graph. The ﬁrst search strategy that our system supports takes a source language word graph as input and translates this graph in a monotone way [5]. The input graph can represent different reorderings of the input sentence so that the overall search can generate nonmonotone translations. Using this approach, it is very simple to experiment with various reordering constraints, e.g., the constraints proposed in [6]. Alternatively, we can use ASR lattices as input and translate them without changing the search algorithm, cf. [7]. A disadvantage when translating lattices with this method is that the search is monotone. To overcome this problem, we extended the monotone search algorithm from [5, 7] so that it is possible to reorder the target phrases. We implemented the following idea: while traversing the input graph, a phrase can be skipped and processed later. Source cardinality synchronous search. For singleword based models, this search strategy is described in [8]. The idea is that the search proceeds synchronously with the cardinality of the already translated source positions. Here, we use a phrase-based version of this idea. To make the search problem feasible, the reorderings are constrained as in [9]. Word graphs and N -best lists. The two described search algorithms generate a word graph containing the most likely translation hypotheses. Out of this word graph we extract N -best lists. For more details on word graphs and N best list extraction, see [10, 11]. 3. Models used during search We use a log-linear combination of several models (also called feature functions). In this section, we will describe the models that are used in the ﬁrst pass, i.e., during search. This is an improved version of the system described in [12]. More speciﬁcally the models are: a phrase translation model, a word-based translation model, a deletion model, word and phrase penalty, a target language model and a reordering model. 3.1. Phrase-based model The phrase-based translation model is the main component of our translation system. The hypotheses are generated by concatenating target language phrases. The pairs of source and corresponding target phrases are extracted from the wordaligned bilingual training corpus. The phrase extraction algorithm is described in detail in [5]. The main idea is to extract phrase pairs that are consistent with the word alignment. Thus, the words of the source phrase are aligned only  to words in the target phrase and vice versa. This criterion is identical to the alignment template criterion described in [13]. We use relative frequencies to estimate the phrase translation probabilities:  p(f˜|e˜)  =  N (f˜, e˜) N (e˜)  (8)  Here, the number of co-occurrences of a phrase pair (f˜, e˜) that are consistent with the word alignment is denoted as N (f˜, e˜). If one occurrence of a target phrase e˜ has N > 1 possible translations, each of them contributes to N (f˜, e˜) with 1/N . The marginal count N (e˜) is the number of occurrences of the target phrase e˜ in the training corpus. The resulting feature function is:  K  hPhr(f1J , eI1, sK1 ) = log p(f˜k|e˜k)  (9)  k=1  To obtain a more symmetric model, we use the phrase-based model in both directions p(f˜|e˜) and p(e˜|f˜).  3.2. Word-based lexicon model  We are using relative frequencies to estimate the phrase translation probabilities. Most of the longer phrases occur only once in the training corpus. Therefore, pure relative frequencies overestimate the probability of those phrases. To overcome this problem, we use a word-based lexicon model to smooth the phrase translation probabilities. The score of a phrase pair is computed similar to the IBM model 1, but here, we are summing only within a phrase pair and not over the whole target language sentence:  K jk hLex(f1J , eI1, sK1 ) = log  ik p(fj |ei )  k=1 j=bk i=ik−1+1  (10)  The word translation probabilities p(f |e) are estimated as relative frequencies from the word-aligned training corpus. The word-based lexicon model is also used in both directions p(f |e) and p(e|f ).  3.3. Deletion model  The deletion model [14] is designed to penalize hypotheses that miss the translation of a word. For each source word, we check if a target word with a probability higher than a given threshold τ exists. If not, this word is considered a deletion. The feature simply counts the number of deletions. Last year [15], we used this model during rescoring only, whereas this year, we integrated a within-phrase variant of the deletion model into the search:  K jk hDel(f1J , eI1, sK1 ) =  ik [ p(fj|ei) < τ ] (11)  k=1 j=bk i=ik−1+1  The word translation probabilities p(f |e) are the same as for the word-based lexicon model. We use [·] to denote a true or false statement [16], i.e., the result is 1 if the statement is true, and 0 otherwise. In general, we use the following convention:  [C] =  1, if condition C is true 0, if condition C is false  (12)  3.4. Word and phrase penalty model  In addition, we use two simple heuristics, namely word penalty and phrase penalty:  hWP(f1J , eI1, sK1 ) = I  (13)  hPP(f1J , eI1, sK1 ) = K  (14)  These two models affect the average sentence and phrase lengths. The model scaling factors can be adjusted to prefer longer sentences and longer phrases.  3.5. Target language model  We use the SRI language modeling toolkit [17] to train a standard n-gram language model. The smoothing technique we apply is the modiﬁed Kneser-Ney discounting with interpolation. The order of the language model depends on the translation direction. For most tasks, we use a trigram model, except for Chinese-English, where we use a ﬁvegram language model. The resulting feature function is:  I  hLM(f1J , eI1, sK1 ) = log p(ei|eii−−1n+1)  (15)  i=1  3.6. Reordering model We use a very simple reordering model that is also used in, for instance, [13, 15]. It assigns costs based on the jump width: K hRM(f1J , eI1, sK1 ) = |bk − jk−1 − 1| + J − jk (16) k=1  4. Rescoring models The usage of N -best lists in machine translation has several advantages. It alleviates the effects of the huge search space which is represented in word graphs by using a compact excerpt of the N best hypotheses generated by the system. Especially for small tasks, such as the IWSLT supplied data track, rather small N -best lists are already sufﬁcient to obtain good oracle error rates, i.e., the error rate of the best hypothesis with respect to an error measure (such as WER or BLEU). N -best lists are suitable for easily applying several rescoring techniques because the hypotheses are already fully generated. In comparison, word graph rescoring techniques need specialized tools which traverse the graph appropriately. Additionally, because a node within a word graph  allows for many histories, one can only apply local rescoring techniques, whereas for N -best lists, techniques can be used that consider properties of the whole target sentence. In the next sections, we will present several rescoring techniques.  4.1. Clustered language models  One of the ﬁrst ideas in rescoring is to use additional language models that were not used in the generation procedure. In our system, we use clustered language models based on regular expressions [18]. Each hypothesis is classiﬁed by matching it to regular expressions that identify the type of the sentence. Then, a cluster-speciﬁc (or sentence-type-speciﬁc) language model is interpolated into a global language model to compute the score of the sentence:  hCLM(f1J , eI1) =  log  Rc(eI1)  c  (17) αcpc(eI1) + (1 − αc)pg(eI1) ,  where pg(eI1) is the global language model, pc(eI1) the cluster-speciﬁc language model, and Rc(eI1) denotes the true-or-false statement (cf. Equation 12) which is 1 if the cth regular expression Rc(·) matches the target sentence eI1 and 0 otherwise.2  4.2. IBM model 1  IBM model 1 rescoring rates the quality of a sentence by  using the probabilities of one of the easiest single-word based  translation models:      hIBM1(f1J ,  eI1 )  =  log   (I  
This paper describes Sehda’s S2MT (Syntactic Statistical Machine Translation) system submitted to the Korean-English track in the evaluation campaign of the IWSLT-05 workshop. The S2MT is a phrase-based statistical system trained on linguistically processed parallel data. 1. Introduction Sehda’s S2MT (Syntactic Statistical Machine Translation) system is a hybrid system which incorporates linguistic knowledge into statistical learning. The system learns phraseto-phrase mapping and syntactic ordering separately. A feasibility test of the system is performed on the translation task presented by International Workshop on Spoken Language Translation (IWSLT) for Korean-to-English “Supplied+Tools” data track. We show that syntactic phrases are useful units to handle the structural problems of statistical Machine Translation and reduce the need for huge parallel text corpora. 2. Overview of S2MT Our system capitalizes on the intuition that language is broadly divided into two levels: structure and vocabulary. Structure is the syntax or relation among phrases that govern the formation of complex structures in a language. Vocabulary is the word-level representation of individual concepts in a language. In traditional approaches to Statistical Machine Translation (SMT), the system learns both types of information simultaneously. By separating the acquisition of structural information from the acquisition of vocabulary, however, an SMT system can learn both levels more easily and more efficiently. By modifying the existing corpus to isolate structure and vocabulary, we are able to take full advantage of all of the information content of the bilingual corpus, ultimately producing higher quality machine translation with less training data. We separate the two levels of translation information by “chunking” [1] [2] the sentences in the bilingual corpus. Chunking is the process of separating the sentences into contiguous, structurally significant groups, such as noun phrases, verbal clusters, and prepositional phrases.1 In contrast to full syntactic parsing employed in [5] [6], chunking is flexible enough to handle the ungrammaticalities of 
In this paper we investigate the rapid deployment of a twoway Afrikaans to English Speech-to-Speech Translation system. We discuss the approaches and amount of work involved to port a system to a new language pair, i.e. the steps required to rapidly adapt ASR, MT and TTS component to Afrikaans under limited time and data constraints. The resulting system represents the ﬁrst prototype built for Afrikaans to English speech translation. 1. Introduction In this paper we describe the rapid deployment of a two-way Afrikaans to English Speech-to-Speech Translation system. This research was performed as part of a collaboration between the University of Stellenbosch and Carnegie Mellon University. Using speech and text data supplied by the University of Stellenbosch, a native Afrikaans speaker developed the Afrikaans automatic speech recognition (ASR), machine translation (MT) and text-to-speech synthesis (TTS) components over a period of 2.5 months. The components were built using existing software tools created by the Interactive Systems Laboratories (ISL). The prototype is designed to run on a laptop or desktop computer using a close-talking headset microphone. Afrikaans is a Dutch derivative that is one the 11 ofﬁcial languages in the Republic of South Africa. The 11 languages consists of 2 Germanic languages: English and Afrikaans, and 9 Ntu (or Bantu) languages: isiNdebele, Sepedi, SeSotho, Swazi, Xitsonga, Setswana, Tshivenda, isiXhosa, isiZulu. The majority of the population speaks two of the 11 languages: their native mother-tongue and English most often chosen as the second language. Therefore English can be regarded as the pivot language in South African culture and is the most natural choice to translate to and from. Afrikaans was chosen because of the following three reasons: (i) Of the remaining 10 ofﬁcial languages, Afrikaans has the longest written history and therefore the most available text data. (ii) Unlike the Ntu languages, Afrikaans has the same language root as English and therefore the similarities should help in developing Afrikaans-English translation. (iii) The developer is ﬂuent in both Afrikaans and English, but does not speak any of the Ntu languages.  The paper is organised into four parts. In the ﬁrst part we will discuss some of the characteristics of Afrikaans. In the second part we will present the system architecture of the prototype and discuss the different development strategies that were chosen for each component of the system. The third part will discuss the Afrikaans data resources that were available and the last part will discuss the implementation details and performance of the prototype system. 2. Language Characteristics of Afrikaans The following discussion of the characteristics of Afrikaans has been obtained from [1]. 2.1. History Afrikaans is linguistically closely related to 17th century Dutch, and to modern Dutch by extension. Dutch and Afrikaans are mutually understandable. Other less closely related languages include the Low Saxon spoken in northern Germany and the Netherlands, German, and English. Cape Dutch vocabulary diverged from the Dutch vocabulary spoken in the Netherlands over time as Cape Dutch was inﬂuenced by European languages (Portuguese, French and English), East Indian languages (Indonesian languages and Malay), and native African languages (isiXhosa and Khoi and San dialects). The ﬁrst Afrikaans grammars and dictionaries were published in 1875. Besides vocabulary, the most striking difference from Dutch is the much more regular grammar of Afrikaans, which is likely the result of mutual interference with one or more Creole languages based on the Dutch language spoken by the relatively large number of non-Dutch speakers (Khoisan, Khoikhoi, German, French, Malay, and speakers of different African languages) during the formation period of the language in the second half of the 17th century. 2.2. Grammar Grammatically, Afrikaans is very analytic. Compared to most other Indo-European languages, verb paradigms in Afrikaans are relatively simple. With a few exceptions, there is no distinction for example between the inﬁnitive and present forms of verbs. Unlike most other Indo-European  Consonants Short vowels Long vowels Diphthongs  p b t tS d dZ k g P m n ñ N r ö f v w T sSzZHjl iyueøEœOa@æ i: y: u: e: ø: o: E: œ: 3: O: a: æ: iu ia ui eu oi Oi ai aU a:i @i @u æy  Table 1: Afrikaans phone set (IPA).  languages, verbs do not conjugate differently depending on the subject e.g. “ek is, jy is, hy is, ons is” = Eng. “I am, you are, he is, we are”. Unlike in Dutch, Afrikaans nouns do not have grammatical gender, but there is a distinction between the singular and plural forms of nouns. The most common plural marker is the sufﬁx -e, but several common nouns form their plural instead by adding a ﬁnal -s. No grammatical case distinction exists for nouns, adjectives and articles, with the universal deﬁnite article being “die” = Eng. “the” and the universal indeﬁnite article being “ ’n ” = Eng. “a/an”. Vestiges of case distinction remain for certain personal pronouns. No case distinction is made though for the plural forms of personal pronouns, i.e “ons” means both “we” and “us”; “julle” means “you”, and “hulle” means both “they” and “them”. There is often no distinction either between objective pronouns and possessive pronouns when used before nouns. In terms of syntax, word order in Afrikaans follows broadly the same rules as in Dutch. A particular feature of Afrikaans is its use of the double negative, something that is absent from the other West Germanic standard languages, e.g: “Hy kan nie Afrikaans praat nie” = Eng. “He cannot Afrikaans speak not” (literally). It is assumed that either French or San are the origins for double negation in Afrikaans. The double negative construction has been fully grammaticalized in standard Afrikaans and its proper use follows a set of fairly complex rules. 2.3. Orthography As English, Afrikaans is written using the Roman alphabet and words are separated by spaces. Written Afrikaans differs from Dutch in that the spelling reﬂects a phonetically simpliﬁed language, and so many consonants are dropped. The spelling is also considerably more phonetical than Dutch. Notable features include the use of ‘s’ instead of ‘z’, hence South Africa in Afrikaans is written as “SuidAfrika”, whereas in Dutch it is “Zuid-Afrika”. The Dutch letter combination ‘ij’ is written as ’y’, except where it replaces the Dutch sufﬁx -lijk, as in “waarskynlik” = Dutch “waarschijnlijk”. The letters ‘c’, ‘q’ and ‘x’ are rarely seen in Afrikaans, and words containing them are almost exclusively borrowings from English, Greek or Latin. This is usually because words with ‘c’ or ‘ch’ in Dutch are transliterated as ‘k’ or ‘g’ in Afrikaans. The following special letters are used in Afrikaans: e`, e´, eˆ, e¨, ˆı, ¨ı, oˆ uˆ.  2.4. Phone Set The Afrikaans phoneme set (shown in Table 1) consists of 27 consonants, 23 vowels and 12 diphthongs for a total of 62 phones. Vowels are further subdivided into 11 short vowels and 12 long vowels. 3. System Architecture The target platform of the Afrikaans-English speech translation prototype is a desktop or laptop. Speech input is obtained using a standard PC sound card and a close-talking PC headset microphone. The demonstration prototype consists of 3 main components: ASR, MT and TTS. Each component was developed separately and then integrated into the prototype. The breakdown of the prototype system is shown in Fig. 1. The working of the speech translation prototype is broken into three actions: 1. Conversion of source language speech into source language text (ASR). 2. Translation of source language text into target language text (MT). 3. Conversion of target language text into target language speech (TTS). The choices of the recognition, translation and synthesis strategies were heavily inﬂuenced by the amount of labor-intensive work and time that is required to implement each strategy. Data-driven techniques were preferred over knowledge-based techniques as it would enable the prototype to be developed more rapidly. The following strategies were therefore chosen: • For the speech recognition a statistical n-gram language model based recognition strategy was chosen as this does not involve the labor-intensive task of writing recognition grammars. • For the translation strategy a statistical machine translation (SMT) approach was chosen instead of an Interlingua based approach. An Interlingua based approach would require the development of a part-ofspeech tagger, an analysis grammar and a generation grammar. The SMT approach only requires the development of a translation model (TM) and a statistical language model (SLM), both which can be learned directly from text data. • For the synthesis strategy a concatenative speech synthesis approach was chosen as a ﬁrst implementation. Concatenative speech synthesis requires the construction of databases of natural speech for the target domain. A new utterance in the target domain is synthesized by selection and concatenation of appropriate subword units. The disadvantage of unit-selection concatenative speech synthesis is that it requires large amounts of memory.  ASR  SMT  TTS  Source language input speech  Source language text  Target language text  Target language output speech  Figure 1: The system architecture of the Afrikaans-English speech translation prototype.  For each of the main components it was necessary to develop the following subcomponents: • ASR: Acoustic Models, Language Models and Pronunciation Dictionary. • SMT: Translation Models and Language Models. • TTS: Pronunciation Dictionary and Letter-To-Sound Rules. The main components were ﬁnally integrated by simply using the output of each preceding component as the input of the next component. The best ASR output was used as input for the SMT component and the best SMT translation output was used as input for the TTS component. Only the ﬁrst best ASR output was used as input for the SMT component. No effort was made to compensate for recognition errors (by using word lattices as input) or for speech disﬂuencies. that are sometimes used in an attempt to reduce the impact of using recognised speech as input instead of text, on SMT performance. 4. Language Data Resources The biggest challenge to developing the system was the limited amount of available Afrikaans speech and text data. Over the past 100 years Afrikaans has developed a rich literature which resulted in the accumulation of large text data. In contrast, very little efforts have been undertaken so far to record and transcribe spoken speech (suitable for speech recognition). In the rest of this section we will describe the data resources in more detail. 4.1. Text Data The text data consists of multilingual parliament sessions that were translated into both Afrikaans and English. The data consists of 39 parliamentary sessions from the year 20002001 for a total of 43k parallel sentences. The sentences were aligned using Koehn’s Europarl sentence alignment tool based the Church and Gale algorithm [2]. The sentence lengths are distributed from sentences that are single words to sentences that are more than 100 words long. The average sentence length is 17.13 words with a standard deviation of  14.36. The translated parliamentary sessions are commonly referred to as Hansards. In the rest of the paper we will refer to the parliamentary domain as the Hansard domain. 4.2. Speech Data 4.2.1. AST data The Afrikaans speech data was collected during a period of 3 years ending in March 2004 by a consortium known as African Speech Technology (AST) [3, 4]. The AST speech corpus consists of 5 languages for a total of 11 dialects. The data was collected over the telephone and cellphone networks and each participant had to read a datasheet containing 40 utterances. This included a phonetically balanced sentence consisting of 40 words for each dialect. The AST data are orthographically and phonetically transcribed. Speech and non-speech utterances have also been marked and the phonetic transcriptions have been corrected by hand. Only the mother-tongue Afrikaans speech data was used in this research (referred to as the AA data). The AA speech data consists of a total of 265 speakers, 113 male and 152 female, for a total of 10768 utterances. 191 of the recordings were made using landlines and 74 of the recordings were made using the cell phone network for a total of about 6 hours of transcribed Afrikaans speech data. 4.2.2. Hansard data As the prototype was designed to be used with a close-talking PC headset microphone, a channel mismatch would have occurred if only the available Afrikaans speech was used for training the acoustic models. In order to reduce the channel mismatch it was decided to collect a limited amount of Afrikaans speech under the same acoustic conditions as the target application. This would also enable the evaluation of the complete demonstration prototype (excluding the synthesis). As there was only two native Afrikaans speakers, it was decided to record 1,000 utterances (500 utterances per speaker). The utterances were recorded at a sampling frequency of 16kHz using a laptop and a close-talking PC headset microphone (Andrea Anti-noise NC-61). The utterances were recorded in a medium-sized room with low to medium noise levels. The 1,000 sentences were chosen from the par-  allel text data so that the distribution of sentence lengths in the evaluation data would be representative of the distribution found in the parallel text corpus (up to a sentence length of 40 words per utterance). The utterances are classiﬁed as read speech, as the utterances were recorded by prompting the speaker. The utterances were only orthographically transcribed and no manual time-alignment of the speech signal and transcription were performed. 4.2.3. Pronunciation Dictionaries As the AST speech data had been orthographically and phonetically aligned, a pronunciation dictionary containing 5,361 words can be extracted from the transcriptions. The AST pronunciation dictionary has a vocabulary size of 3,795 words and a total of 1.41 pronunciation variants (rounded to the second decimal). Another syllable annotated pronunciation dictionary, developed by the University of Stellenbosch, was also available. The Stellenbosch dictionary has a vocabulary size of 36,783 words and does not contain any pronunciation variants. By combining the AST dictionary and the Stellenbosch dictionary a new dictionary was formed that has a vocabulary size of 38,960 words and a total of 1.08 pronunciation variants (which roughly means that each entry has only one pronunciation). 5. Development of System Components 5.1. Partitioning of data sets In order to be able to evaluate the complete prototype as well as each component separately, it was decided to use the same evaluation set for all evaluations. As previously mentioned 1,000 utterances were selected from the parallel text data and recorded using a close-talking microphone. The 16kHz Hansard utterances are downsampled to 8kHz in order to match the acoustic models. The 200 longest utterances were used for adaptation of the recogniser and the remaining 800 utterances were used for evaluation purposes (which will be referred to as the Hansard evaluation set). The rest of the 41k sentences were used for the development of the translation models. In Table 3 information regarding the Afrikaans and English parallel text data is shown. Although the Afrikaans text data only has a vocubulary size of 25k words and the pronunciation dictionary consists of 39k words, not all the words in the Afrikaans text data were covered by the pronunciation dictionary. The following three constraints were used when selecting the 1,000 sentences to be recorded: 1. Every word in a recorded sentence had to be covered by the pronunciation dictionary. 2. The distribution of words per sentence had to be representative of the distribution in the training data. 3. No sentence containing more than 40 words were recorded.  The Hansard evaluation set has an average sentence length of 24.39 words with a standard deviation of 14.34. The AST speech data was divided into training, development and evaluation sets which each respectively consists of 70%, 15% and 15% of the AST data. The AST training data contains 187 speakers and 7696 utterances. 5.2. Automatic Speech Recognition The Afrikaans acoustic models were bootstrapped from the GlobalPhone [5, 6] MM7 multilingual acoustic models using a web-based tool called SPICE [7]. The MM7 phones did not cover all the Afrikaans phones and it was decided to reduce the 62 phone set to 39 phones which was done by splitting the diphthongs into two separate phones and by not distinguishing between long and short vowels. It is unknown what the impact of the large reduction in the phone set has on the ASR performance. Another possibility would have been to bootstrap unknown Afrikaans phones with neighboring phones, but unfortunately time did not permit the development of a Afrikaans system with a larger phone set. CMU’s Janus JrTk [8, 9] was used to train the acoustic models on 4.2 hours of the AST speech data. As the recogniser will be used with a close-talking headset microphone a channel mismatch exists between the evaluation conditions and the training conditions. There also exists a domain mismatch as the AST data covers various tasks (as described in section 4.2.1) while the Hansard data covers parliamentary debates. In an attempt to adapt to the acoustic environment and the domain, the acoustic models are further trained on 200 utterances of Hansard speech data. The acoustic models were adapted by simply training on the Hansard speech data and not by using MLLR or MAP adaptation. However, as the Hansard speech data consists of only two speakers, this further training probably adapted to the test speakers rather than the evaluation conditions. The Afrikaans recogniser is a fully-continuous 3-state HMM recogniser with 500 triphone models (tied using decision trees). Each state consists of a mixture of 128 Gaussians. The frontend uses 13 MFCCs, power, and the ﬁrst and second time derivatives of the features. These are reduced to 32 dimensional feature vectors using LDA. Both vocal tract length normalisation (VTLN) and constrained MLLR speaker adaptive training (SAT) was employed when training. The Afrikaans and English language models were trained using SRI’s statistical language toolkit SRILM [10]. The indomain Afrikaans SLM is a trigram language model with a perplexity of 103.71 and a OOV rate of 0.0% on the Hansard evaluation set. It was trained on 694,455 words and a vocabulary of 25,623 words. Both the Hansard adapted acoustic models and the unadapted acoustic models were evaluated on the Hansard evaluation set which consists of 15,259 words and has a vocabulary size of 2.45k words. The results are shown in Table 2. It can be seen that the unadapted acoustic models has a fairly poor performance of 46.5% WER. Fortunately the acoustic  models that were adapted to the Hansard evaluation conditions has a WER of only 20.0% which is a relative improvement of 54.3%. Thus the channel and domain mismatch that exists between the training conditions and the evaluation conditions are partially solved by adapting on the Hansard data. The speaker-independency of the Afrikaans recogniser could not be determined (as a result of the limited number of available Afrikaans speakers), but because the Hansard adaptation data only contains two native Afrikaans speakers the Afrikaans recogniser is quite possibly very speakerdependent. It can also be seen that the ASR performs signiﬁcantly better for the male speaker than for the female speaker.  Number of words Vocabulary size Pronunciation variants Trigram LM PP WER (male) WER (female) WER (total)  Unadapted AMs 15,259 2,450 1.08 103.71 39.1% 54.0% 46.5%  Adapted AMs 15,259 2,450 1.08 103.71 17.6% 22.3% 20.0%  Table 2: ASR evaluation results on the Hansard set.  The total development time for the ASR component is estimated to be 8 weeks and was the most difﬁcult and timeconsuming component to develop.  5.3. Statistical Machine Translation  According to [11] statistical machine translation deﬁnes the task of translating a source language sentence (f = f1 . . . fJ ) into a translation sentence (e = e1 . . . eI ) of the target language. The SMT approach is based on Bayes’ decision rule and the noisy channel approach in that the best translation sentence is given by:  eˆ = arg max [P (e|f )] = arg max [P (f |e)P (e)] (1)  e  e  where P (e) is the language model of the target language and P (f |e) is the translation model. The arg max denotes the search algorithm, which ﬁnds the best target sentence given the language and translation models. For a detailed discussion of CMU’s statistical machine translation system refer to [12]. The system contains an IBM1 lexical transducer, a phrase transducer and a class based transducer. Only the IBM1 lexical transducer, which is a one-to-one lexicon mapper, is used in this research. The language model is n-gram based and up to trigrams are used. The decoder is a beam search based on dynamic programming combined with pruning. As words are separated by space in written Afrikaans, it is not necessary to use a segmentor to determine word boundaries in sentences (as is required for languages such as Chinese). As the intention was to develop a two-way speech translation demonstration prototype, both Afrikaans and English  translation systems were developed. The translation models were trained on the 42k Hansard parallel data and was evaluated using the same 800 Hansard sentences that were used to evaluate the ASR component. The same Afrikaans SLM was used as was trained for the ASR component. The English SLM is also a trigram language model with a perplexity of 86.62 and a OOV rate of 0.0% on the Hansard evaluation set. It was trained on 687,154 words and a vocabulary of 17,898 words. The inﬂuence of punctuation on SMT performance was investigated. In the ﬁrst case all punctuation was removed from the parallel text before training and in the second case the punctuation was left in the data. Separate SLMs were also trained for the systems with and without punctuation and the SLM perplexities were measured on the evaluation set. Care was taken to ensure that the SLM without punctuation had to predict the evaluation material where the punctuation was ﬁrst removed. The SLM with punctuation had to predict the evaluation material with punctuation. Table 3 summarizes the information regarding the Afrikaans and English text data. It is interesting to note that the Afrikaans vocabulary size is 43% larger than English vocabulary size. Although Afrikaans is much less inﬂected than English, Afrikaans has less rigid spelling rules regarding the formation of compound words. Afrikaans compound words can be written in three different ways: (i) as a single word, (ii) as separate words or (iii) as separate words connected with dashes. When preparing the text data, no effort was made to force the Afrikaans text to conform to a single method of forming compound words. It has also been noticed that Hansard domain contains a large number of compound words which results in the large vocabulary size for Afrikaans.  Text Data Language Number of Sentences Number of Words Vocabulary Size LM Perplexity w/o punct. LM Perplexity with punct.  English Afrikaans 41,239 687,154 694,455 17,898 25,623 87.21 103.71 62.28 72.28  Table 3: Parallel Corpus Statistics.  In Table 4 the results of the SMT experiments are shown for both Afrikaans-English and English-Afrikaans translation. It can be seen that Afrikaans-English translation does beneﬁt from the use of punctuation as both the NIST and the BLEU metric increase slightly. For English-Afrikaans translation the NIST metric is degraded slightly by the use of punctuation although the BLEU metric is increased. This would seem to indicate that the ﬂuency of the translation beneﬁts from punctuation although the accuracy is not signiﬁcantly affected. It was decided to compare two-way Afrikaans-andEnglish translation results with the Europarl two-way Dutchand-English results, as the domain and language pairs are  similar (ideally, the comparison should be made when also using a similar size parallel corpus). When using a DutchEnglish parallel corpus of 743,880 sentence pairs, Koen reports a BLEU score of 26.35 for Dutch-English translation and a BLEU score of 22.85 for English-Dutch translation [13]. Both the Afrikaans-English (BLEU 36.11, NIST 7.66)  Results IBM1 w/o punct IBM1 with punct  Afrikaans-English BLEU NIST 34.13 7.65 36.11 7.66  English-Afrikaans BLEU NIST 34.68 7.93 34.81 7.73  Table 4: SMT evaluation results on the Hansard test.  and English-Afrikaans (BLEU 34.81, NIST 7.73) translation results are very encouraging when compared to the results obtained by Koehn, as the Afrikaans-English results were obtained using a smaller corpus. Furthermore, there is still much scope for improvement as only the most simple of translation models were applied. The total development time for the SMT component is estimated to be 1 week and was relatively easy when compared to the ASR development.  5.4. Speech Synthesis A limited domain Afrikaans voice is built using the Festival Speech Synthesis System [14]. A male Afrikaans unitselection voice was built following the techniques for building synthetic voices in new languages developed by CMU [15]. The same phone set is used for synthesis as was used for the recogniser. The 500 Hansard utterances that was used for adaptation and evaluation of the recogniser were used for building the unit-selection voice. We were also fortunate to obtain a syllable annotated pronunciation lexicon of 36,783 Afrikaans words. It was therefore not necessary to build a pronunciation lexicon for Afrikaans. A statistical letter-to-sound rule model was trained on 90% of the pronunciation dictionary and evaluated on the remaining 10% [16]. The evaluation pronunciations were chosen by selecting every 10th word in the alphabetically sorted pronunciation dictionary. The results of the letter-to-sound rules are shown in Table 5. The letter-to-sound rules managed to correctly predict 85.24% of the words which is to be expected as Afrikaans spelling reﬂects a phonetically simpliﬁed language. These results are comparable to the results of German (89.38% word correct) [16].  Trainset pronunciations Testset pronunciations Phones correct Words correct  33,121 3,680 97.92% 85.24%  Table 5: Evaluation of Letter-To-Sound rules.  As only two Afrikaans speakers were available it was not  possible to formally evaluate the performance and quality of the Afrikaans speech synthesis. In all cases the Afrikaans pronunciations were understandable, but the following informal observation can be made regarding the quality of the synthesis: • The Afrikaans phone set made no distinction between long and short versions of the same vowel. Consequently some pronunciation errors were made when words contained long vowels. • The lack of diphthongs in the phone set resulted in some incorrect pronunciation of words containing diphthongs. Both of these problems can be corrected by simply using a larger phone set which includes the diphthongs and models both long and short vowels. The total development time of the synthesis component is estimated to have been one week. The availability of a 37k Afrikaans pronunciation dictionary shortened the development of the synthesis component considerably. 5.5. Estimate of total development time Table 6 summaries the estimate of the total system development time.  Task Familiarise with ASR software Preparation of data for AMs Phone set adaptation of dictionary and transcriptions Bootstrap & Training of AMs Tuning of ASR and adaptation of AMs Preparation of data for LMs Generation of LMs Preparation of data for TMs Generation of TMs Preparation of data for TTS Generation of Afrikaans voice Generation of LTS rules Familiarise with ’one4all’ framework Integration of components Evaluation Total development time  Time Days Weeks  7  1.4  5  
This work summarizes a comparison between two approaches to Statistical Machine Translation (SMT), namely Ngram-based and Phrase-based SMT. In both approaches, the translation process is based on bilingual units related by word-to-word alignments (pairs of source and target words), while the main differences are based on the extraction process of these units and the statistical modeling of the translation context. The study has been carried out on two different translation tasks (in terms of translation difﬁculty and amount of available training data), and allowing for distortion (reordering) in the decoding process. Thus it extends a previous work were both approaches were compared under monotone conditions. We ﬁnally report comparative results in terms of translation accuracy, computation time and memory size. Results show how the ngram-based approach outperforms the phrase-based approach by achieving similar accuracy scores in less computational time and with less memory needs. 1. Introduction From the initial word-based translation models [1], research on statistical machine translation has been strongly boosted. At the end of the last decade the use of context in the translation model (phrase-based approach) lead to a clear improvement in translation quality ( [2], [3], [4]). Nowadays the introduction of some reordering abilities is of crucial importance for some language pairs and is an important focus of research in the area of SMT. In parallel to the phrase-based approach, the ngrambased approach [5] also introduces the word context in the translation model, what allows to obtain comparable results under monotone conditions (as shown in [6]). The addition of reordering abilities in the phrase-based approach is achieved by enabling a certain level of reordering in the source sentence. Though, the translation process consists of a composition of phrases, where the sequential composition of the phrases source words corresponds to the source sentence reordered. This procedure poses additional difﬁculties when applied to the ngram-based approach, because the characteristics of the ngram-based translation model. Despite of this, recent works ( [7], [8]) have shown how applying a reordering schema in the training process the ngram-based approach  can also take advantage of the distortion capabilities. In this paper we study the differences and similarities of both approaches (ngram-based and phrase-based), focusing on the translation model, where the translation context is differently taken into account. We also investigate the differences in the translation (bilingual) units (tuples and phrases) and show efﬁciency results in terms of computation time and memory size for both systems. We have extended the comparison in [6] to a Chinese to English task (where the use of distortion capabilities implies a clear improvement in translation quality), and using a much larger Spanish to English task corpus. In section 2 we introduce the modeling underlying both SMT systems, the additional models taken into account in the log-linear combination of features (see equation 1), and the bilingual units extraction methods (namely tuples and phrases). In section 3 is discussed the decoder used in both systems (MARIE) [9], giving details of pruning and reordering techniques. The comparison framework, experiments and results are shown in section 4, while conclusions are detailed in section 5.  2. Modeling Alternatively to the classical source channel approach, statistical machine translation models directly the posterior probability p(eI1|f1J ) as a log-linear combination of feature models [10], based on the maximum entropy framework, as shown in [11]. This simpliﬁes the introduction of several additional models explaining the translation process, as the search becomes:  arg max{exp( λihi(e, f ))}  (1)  eI1  i  where the feature functions hi are the system models (translation model, language model, reordering model, ...), and the λi weights are typically optimized to maximize a scoring function on a development set. The Translation Model is based on bilingual units (here called tuples and phrases). A bilingual unit consists of two monolingual fragments, where each one is supposed to be the translation of its counterpart. During training, the system learns a dictionary of these bilingual fragments, the actual core of the translation systems.  2.1. Ngram-based Translation Model The Translation Model can be thought of a Language Model of bilingual units (here called tuples). These tuples deﬁne a monotonous segmentation of the training sentence pairs (f1J , eI1), into K units (t1, ..., tK ). The Translation Model is implemented using an Ngram language model, (for N = 3):  K  p(e, f ) = P r(tK1 ) = p(tk | tk−2, tk−1)  (2)  k=1  Figure 1 shows an example of tuples extraction from a word-to-word aligned sentence pair. Bilingual units (tuples) are extracted from any word-toword alignment according to the following constraints [6]:  • a monotonous segmentation of each bilingual sentence pairs is produced,  • no word inside the tuple is aligned to words outside the tuple, and  • no smaller tuples can be extracted without violating the previous constraints.  As a consequence of these constraints, only one segmentation is possible for a given sentence pair. Resulting from this procedure, some tuples consist of a monolingual fragment linked to the NULL word (words#NULL and NULL#words). Those tuples with a NULL word in its source side are not kept as bilingual units. To use these tuples in decoding it should appear a NULL word in the input sentence (test to translate). Though, we assign the target words of these tuples to the next tuple in the tuples sequence of the sentence (training). In the example of ﬁgure1, if the NULL word would be contained in the source side, its counterpart (does) would be assigned to the next tuple (does the ﬂight last#dura el vuelo). A complementary approach to translation with reordering can be followed if we allow for a certain reordering in the training data. This means that the translation units are modiﬁed so that they are not forced to sequentially produce the source and target sentences anymore. The reordering procedure in training tends to monotonize the word-to-word alignment through changing the word order of the source sentences. The rationale of this approach is double, on the one hand, it makes sense when applied into a decoder with reordering capabilities as the one presented in the following section, and on the other hand, the unfolding technique generates shorter tuples, alleviating the problem of embedded units (tuples only appearing within long distance alignments, not having any translation in isolation). A very relevant problem in a Chinese to English task. The unfolding technique is here outlined: It uses the word-to-word alignments obtained by any alignment procedure. It is decomposed in two steps:  • First an iterative procedure, where words in one side are grouped when linked to the same word (or group) in the other side. The procedure loops grouping words in both sides until no new groups are obtained. • The second step consists of outputting the resulting groups (unfolded tuples), keeping the word order of target sentece words. Though, the tuples sequence modiﬁes the source sentence word order. Figure 1: Different bilingual units (tuples) are extracted using the extract-tuples and extract-unfold-tuples methods. As can be seen,to produce the source sentence, the extracted unfolded tuples must be reordered. It is not the case of the target sentence, as it can be produced in order using both sequence of units. Figure 1 shows the bilingual units extracted using the extract-tuples and extract-unfold-tuples methods, for a given word-to-word aligned sentence pair. 2.2. Phrase-based Translation Model The basic idea of phrase-based translation is to segment the given source sentence into phrases, then translate each phrase and ﬁnally compose the target sentence from these phrase translations [12]. Given a sentence pair and a corresponding word alignment, phrases are extracted following the criterion in [13] and the modiﬁcation in phrase length in [14]. A phrase (or bilingual phrase) is any pair of m source words and n target words that satisﬁes two basic constraints: 1. Words are consecutive along both sides of the bilingual phrase, 2. No word on either side of the phrase is aligned to a word out of the phrase.  It is infesible to build a dictionary with all the phrases (recent papers show related work to tackle this problem, see [15]). That is why we limit the maximum size of any given phrase. Also, the huge increase in computational and storage cost of including longer phrases does not provide a signiﬁcant improve in quality [16] as the probability of reappearence of larger phrases decreases. In our system we considered two length limits. We ﬁrst extract all the phrases of length X or less (usually X equal to 3 or 4). Then, we also add phrases up to length Y (Y greater than X) if they cannot be generated by smaller phrases. Basically, we select additional phrases with source words that otherwise would be missed because of cross or long alignments [14]. Given the collected phrase pairs, we estimate the phrase translation probability distribution by relative frecuency.  P (f |e)  =  N (f, e) N (e)  (3)  where N(f,e) means the number of times the phrase f is translated by e. If a phrase e has N > 1 possible translations, then each one contributes as 1/N [12].  2.3. Additional features Both systems share the additional features which follows. • Firstly, we consider the target language model. It actually consists of an n-gram model, in which the probability of a translation hypothesis is approximated by the product of word 3-gram probabilities:  k  p(Tk) ≈ p(wn|wn−2, wn−1)  (4)  n=1  where Tk refers to the partial translation hypothesis and wn to the nth word in it.  As default language model feature, we use a standard  word-based trigram language model generated with  smoothing Kneser-Ney and interpolation of higher and  lower order ngrams (by using SRILM [17]).  • The following two feature functions correspond to a forward and backwards lexicon models. These models provides lexicon translation probabilities for each tuple based on the word-to-word IBM model 1 probabilities [18]. These lexicon models are computed according to the following equation:  p((t, s)n)  =  (I  
Nowadays, most of the statistical translation systems are based on phrases (i.e. groups of words). We describe a phrase-based system using a modiﬁed method for the phrase extraction which deals with larger phrases while keeping a reasonable number of phrases. Also, different alignments to extract phrases are allowed and additional features are used which lead to a clear improvement in the performance of translation. Finally, the system manages to do reordering. We report results in terms of translation accuracy by using the BTEC corpus in the tasks of Chinese to English and Arabic to English, in the framework of IWSLT’05 evaluation.  1. Introduction From the initial word-based translation models [3], research on statistical machine translation has been strongly improved. At the end of the last decade the use of context in the translation model (phrase-based approach) supposed a clear improvement in translation quality ( [17], [16], [8]). Statistical Machine Translation (SMT) is based on the assumption that every sentence e in the target language is a possible translation of a given sentence f in the source language. The main difference between two possible translations of a given sentence is a probability assigned to each, which has to be learned from a bilingual text corpus. Thus, the translation of a source sentence f can be formulated as the search of the target sentence e that maximizes the translation probability P (e|f ),  e˜ = argmax P (e|f )  (1)  e  If we use Bayes rule to reformulate the translation probability, we obtain,  e˜ = argmax P (f |e)P (e)  (2)  e  This translation model is known as the source-channel approach [2] and it consists on a language model P (e) and a separate translation model P (f |e) [6].  In the last few years, new systems tend to use sequences of words, commonly called phrases [7], aiming at introducing word context in the translation model. As alternative to the source-channel approach the decision rule can be modeled through a log-linear maximum entropy framework.  M  e˜ = argmax  λmhm(e, f )  (3)  e  m=1  The features functions, hm, are the system models (translation model, language model and others) and weights, λi, are typically optimized to maximize a scoring function [12]. It is derived from the Maximum Entropy approach as shown in [1] and has the advantage that additional features functions can be easily integrated in the overall system. This paper addresses a modiﬁcation of the phraseextraction algorithm in [13] and results in Chinese to English and Arabic to English tasks are reported. It also combines several alignments before extracting phrases and interesting features. It is organized as follows. Section 2 explains the SMT system: the phrase extraction, its modiﬁcation and shows the different features which have been taken into account and, brieﬂy, the decoding; section 3 presents the evaluation framework and the results in Chinese to English and Arabic to English tasks are reported; and the ﬁnal section shows some conclusions on the experiments and in the evaluation of IWSLT’05.  2. SMT system As explained in the introduction, the SMT system which is presented is modeled through a log-linear maximum entropy framework. In this section, we explain the models, the feature functions and the decoding that build this system. The Translation Model is based on bilingual phrase (or phrases). A bilingual unit consists of two monolingual fragments, where each one is supposed to be the translation of its counterpart. During training, the system learns a dictionary of these bilingual fragments, the actual core of the translation systems.  2.1. Phrase-based Translation Model The basic idea of phrase-based translation is to segment the given source sentence into phrases, then translate each phrase and ﬁnally compose the target sentence from these phrase translations [18].  2.1.1. Word alignment Given a sentence pair, we use GIZA++ [10] to align each of them word-to-word. We can train in both translation directions and we obtain: (1) the alignment in the source to target direction (s2t); and (2) the alignment in the target to source direction. If we compose the union of both alignments (sU t), we get a higher recall and a lower precision of the combined alignment.  2.1.2. Phrase-extraction Phrases are extracted from sentence pairs and theirs corespondents word alignments following the criterion in [13] and the modiﬁcation in phrase length in [4]. A phrase is any pair of m source words and n target words that satisﬁes two basic constraints:  1. Words are consecutive along both sides of the bilingual phrase,  2. No word on either side of the phrase is aligned to a word out of the phrase.  It is unfeasible to build a dictionary with all the phrases. That is why we limit the maximum size of any given phrase. Also, the huge increase in computational and storage cost of including longer phrases does not provide a signiﬁcant improve in quality [7] as the probability of reappearance of larger phrases decreases. In our system we considered two length limits.The length of a monolingual phrase is deﬁned as its number of words. The length of a phrase is the greatest of the lengths of its monolingual phrases. We ﬁrst extract all the phrases of length X or less. Then, we also add phrases up to length Y (Y greater than X) if they cannot be generated by smaller phrases. Basically, we select additional phrases with source words that otherwise would be missed because of cross or long alignments [4]. Given the collected phrase pairs, we estimate the phrase translation probability distribution by relative frequency.  P (f |e)  =  N (f, e) N (e)  (4)  where N(f,e) means the number of times the phrase f is translated by e. If a phrase e has N > 1 possible translations, then each one contributes as 1/N [18].  2.2. Additional features • Firstly, we consider the target language model. It actually consists of an n-gram model, in which the probability of a translation hypothesis is approximated by the product of word n-gram probabilities:  k p(Tk) ≈ p(wn|...wn−3, wn−2, wn−1) (5) n=1  where Tk refers to the partial translation hypothesis and wn to the nth word in it.  • As translation model we use the conditional probability. Note that no smoothing is performed, which may cause an overestimation of the probability of rare phrases. This is specially harmful given a bilingual phrase where the source part has a big frequency of appearance but the target part appears rarely. That is why we use the posterior phrase probability, we compute again the relative frequency but replacing the count of the target phrase by the count of the source phrase [11].  P (e|f )  =  N (f, e) N (f )  (6)  where N’(f,e) means the number of times the phrase e is translated by f. If a phrase f has N > 1 possible translations, then each one contributes as 1/N. Adding this feature function we reduce the number of cases in which the overall probability is overestimated. • The following two feature functions correspond to a forward and backward lexicon models. These models provides lexicon translation probabilities for each tuple based on the word-to-word IBM model 1 probabilities [11]. These lexicon models are computed according to the following equation:  p((t, s)n)  =  (I  
This paper provides a description of TALP-Ngram, the tuple-based statistical machine translation system developed at the TALP Research Center of the UPC (Universitat Polite`cnica de Catalunya). Brieﬂy, the system performs a log-linear combination of a translation model and additional feature functions. The translation model is estimated as an N-gram of bilingual units called tuples, and the feature functions include a target language model, a word penalty, and lexical features, depending on the language pair and task. The paper describes the participation of the system in the second international workshop on spoken language translation (IWSLT) held in Pittsburgh, October 2005. Results on Chinese-to-English and Arabic-toEnglish tracks using supplied data are reported. 1. Introduction and overview of the system During the last several years, statistical machine translation (SMT) has gained much attention within the research community. This is mainly due to its relatively easy development in terms of human effort, its robustness in face of non-grammatical input data (such as recognised speech), and its good results against rule-based and transfer-based approaches. The statistical approach to machine translation is based on the assumption that every sentence t in the target language is a possible translation of a given sentence s in the source language, and the main difference between two translation hypotheses is a probability assigned to each, which is to be learned from a bilingual corpus. The ﬁrst SMT systems were based on the noisy channel approach on a word-based basis, modeling the translation of a target language sentence t given a source language sentence t as a translation model probability p(s|t) times a target language model probability p(t) [1]. Recently, word-based translation models have been replaced by phrase-based translation models [2, 3], which are estimated from aligned bilingual corpora by using relative frequencies. On the other hand, according to the maximum entropy framework [4], we can deﬁne the translation hypothesis t given a source sentence s, as the target sen-  tence maximizing a log-linear combination of feature functions, as described in the following equation:  M  tˆI1  =  arg  max tI1  λmhm(sJ1 , tI1) m=1  (1)  where λm correspond to the weighting coefﬁcients of the log-linear combination, and the feature functions hm(s, t) to a logarithmic scaling of the probabilities of each model. Following this approach, the translation system described in this paper implements a log-linear combination of one translation model and four additional feature models. In contrast with standard phrase-based approaches, our translation model is expressed in tuples as bilingual units. Given a word alignment, tuples deﬁne a unique and monotonic segmentation of each bilingual sentence, building up a much smaller set of units than with phrases and allowing N-gram estimation to account for the history of the translation process [5, 6]. This approach has its origins in SMT by using ﬁnite state transducers [7, 8, 9]. The organization of the paper is as follows. Section 2 describes in detail the tuple n-gram translation model, while section 3 introduces the additional features used in the system. Section 4 provides a brief overview of the decoding tool and search strategy used. Next, sections 5 and 6 report and discuss results on IWSLT’05 Chineseto-English and Arabic-to-English tracks, respectively. Finally, Section 7 concludes and outlines future research lines.  2. The Tuple N-gram translation model The tuple N-gram translation model is a language model of a particular language composed by bilingual units which are referred to as tuples. This model approximates the joint probability between source and target languages by using N-grams as described by the following equation:  p(sJ1 , tI1) = · · · =  (2)  K  p((s, t)i|(s, t)i−N+1, ..., (s, t)i−1)  (3)  i=1  where (s, t)i refers to the ith tuple of a given bilingual sentence pair, which is segmented into K tuples. It is important to notice that, since both languages are linked up in tuples, the context information provided by this translation model is bilingual. Tuples are extracted from a word-to-word aligned corpus according to the following constraints [10]: • a monotonic segmentation of each bilingual sentence pair is produced • no word inside the tuple is aligned to words outside the tuple • no smaller tuples can be extracted without violating the previous constraints As a consequence of these constraints, only one segmentation is possible for a given parallel sentence pair and a word alignment. Usually, automatic word-to-word alignments are generated in both source-to-target and target-to-source directions by using GIZA++ [11], and tuples are usually extracted from the union set of alignments. However, in section 5 results are also reported when extracting tuples with the alignment from sourceto-target direction. Figure 1 presents a simple example illustrating the tuple extraction process. I would like NULL to have a huge ice−cream  NULL quisiera ir a comer un helado gigante  t  t  t  t  t  t  
In this work we present the fundamentals of the IQMT framework for MT evaluation. IQMT offers a common workbench on which existing evaluation metrics can be utilized. We suggest the IQ measure and test it on the Chinese-toEnglish data from the IWSLT 2004 Evaluation Campaign. We show how the correlation with human assessments at the system level improves substantially for most individual metrics. Moreover, IQMT allows to robustly combine several metrics avoiding scaling problems and metric weightings. Several metric combinations were tried, but correlations did not further improve signiﬁcantly. 1. Introduction At the current level of improvement in a couple of years there will probably exist Machine Translation (MT) systems that perform better than humans according to existing MT evaluation metrics. By then, these metrics, as they are currently applied, will become useless and more sophisticated metrics will be needed (Franz Och, talk at the ACL 2005 Workshop on “Building and Using Parallel Texts: Data-Driven Machine Translation and Beyond”). We refer to this problem as the ‘2008 MT Evaluation Challenge’. In this work we present the fundamentals of IQMT1 (Inside QARLA MT evaluation), a framework for MT evaluation which intends to overcome the MT evaluation challenge by offering a common workbench on which existing evaluation metrics can be used and combined. Inside QARLA [1], automatic evaluation of translations is interpreted as the application of similarity metrics between a set of candidate translations and a set of reference translations. In this context, one of the main issues is to determine how similar a machine-produced translation must be to a set of human references to certify that it is a good translation. 1The IQMT package is publically available, released under the GNU Lesser General Public License (LGPL) of the Free Software Foundation, and may be freely downloaded at http://www.lsi.upc.edu/˜nlp/IQMT.  That is, how the scale properties of the similarity metrics must be interpreted. Another important issue is how to combine information from different metrics into a single measure of quality. In the last years, it has been repeatedly argued that current MT evaluation metrics do not capture well possible improvements attained by means of incorporating linguistic knowledge [2]. One of the possible reasons for that is that most of the current metrics are based on rewarding lexical similarity, thus not taking into account any additional syntactic or semantic information. We believe that new metrics should be investigated and combined with current ones. The question then would be how to ponderate new similarity metrics with respect to existing ones. The QARLA framework has been successfully applied to the automatic evaluation of summaries [3]. Its probabilistic model is not affected by the scale properties of individual metrics. It allows also to combine evaluation metrics in a single measure, QUEEN, such that it is non-dependent on individual metric scales. Our goal is to adapt the QUEEN measure to MT evaluation. For that purpose, we have deﬁned the IQ (Innovated QUEEN) measure. We have applied IQMT to the task of evaluating the IWSLT 2004 results [4]. We show how existing metrics can be used inside QARLA exhibiting higher levels of correlation with human assessments at the system level. We also worked on combinations of metrics but did not achieve any further improvement. The rest of the paper is organized as follows. In Section 2 the QARLA framework is described. In Section 3 we discuss current trends in MT evaluation. Our approach to MT evaluation inside QARLA is described in Section 4. Experimental work is deployed in Section 5. In Section 6 we present a preliminary evaluation of the results of the IWSLT 2005. Finally, some conclusions and further work are drawn in Section 7.  2. The QARLA Framework  The QARLA framework was originally deﬁned for automatic evaluation of summaries. QARLA uses similarity to models (human references) as a building block for the evaluation of automatic summarisation systems. The input for QARLA, in ¡ a summarisation task, is a set of test cases, a set of similarity metrics , and sets of models for each test case. With such a testbed, QARLA provides a measure QUEEN which combines assorted similarity metrics to estimate the quality of automatic summarisers. QUEEN operates under the assumption that a good summary must be similar to all model summaries according ¡£¢¤¡£¦ ¢¥¡ to all metrics. QUEEN is deﬁned as the probability, over , that for every metric in the automatic summary is closer to a model than two other models to each other:  QUEEN§©¨¦¡!#"%$#¨'&)(10243%(5¨¦76298@(A¨B62C76DC EF  G ¦ where is the automatic summary being evaluated,  6HF6 C 76 C EI ¡ (A¨P¦)F61 are three models in , and  stands for  6 ¦ the similarity of to . QUEEN is stated as a probability,  Q RSUTUV and therefore its range of values is .  We can think of the QUEEN measure as using a set of  tests (every similarity metric in ) to test the hypothesis that  ¦ G ¦)F6HF6 C F6 C I a given summary is a model. Given  , we test  (5¨P¦)762W8(5¨B6 C F6 C  ( ¦ for each metric . is accepted as a  ¨P¦X model only if it passes the test for every metric. QUEEN  ¦ is, then, the probability of acceptance for in the sample  ¡Y¢1¡`¢1¡ space  .  This measure has some interesting properties:  (i) it is able to combine different similarity metrics into a single evaluation measure;  (ii) it is not affected by the scale properties of individual metrics, i.e. it does not require metric normalisation and it is not affected by metric weighting.  (iii) Peers (automatic summaries) which are very far from the set of models all receive QUEEN=0. In other words, QUEEN does not distinguish between very poor summarisation strategies.  (iv) The value of QUEEN is maximised for peers that ( “merge” with the models under all metrics in . (v) The universal quantiﬁer on the metric parameter implies that adding redundant metrics does not bias the result of QUEEN.  3. The ‘2008 MT Evaluation Challenge’ In the last years, many efforts have been devoted to including linguistic information further than lexical units in the parameter estimation of translation models in Statistical Machine Translation.  However, to our knowledge, no signiﬁcant improvement has been reported so far. An exception is the case of [5] who presented a syntax-based language model based upon that described by [6], which combined with the syntax based translation model described by [7], achieved a notable improvement in grammaticality. However, they measured this improvement by means of human evaluation. At this point, one may argue that evaluation metrics are not well suited to capture improvements attained. Most of the existing metrics work only at the lexical level. This is the case of metrics such as BLEU [8], NIST [9], WER and PER [10], and GTM [11]. We may ﬁnd some notable exceptions such as METEOR [12], ROUGE [13], and WNM [14], which consider additional information. For instance, ROUGE and METEOR consider stemming, and allow for WordNet [15] lookup . METEOR performs a synonym search in WordNet. As to WNM, this metric is a variant of BLEU which weights n-grams according to their statistical salience estimated out from a monolingual corpus. Further than that, we may ﬁnd the approach by [16] who introduce a series of syntactic features such as constituent/dependency precision and recall and head-word chain matching. They show how adding syntactic information to the evaluation metric improves both sentence-level and system-level correlation with human judgements. In a recent work, [17] tried to combine some aspects of different metrics. They applied machine learning techniques to build a classiﬁer that distinguished between humangenerated (good) and machine-generated (bad) translations. They used features inspired in metrics like BLEU, NIST, WER and PER, obtaining higher levels of correlation with human judgements. Similarly, the IQMT framework permits metric combinations, with the singularity that there is no need to perform any training or adjustment of parameters. 4. QARLA for MT Evaluation QUEEN operates under the assumption that there exists a set of similarity metrics which are capable of grouping models as opposite to low quality elements. That is, QUEEN assumes that a good element must be close to all models. The question is whether it is possible to ﬁnd such a set of metrics in the context of translations. In a ﬁrst experiment we tried to apply the original QUEEN to MT, but we did not obtain signiﬁcant improvements in correlation with human judgements for most of the metrics. See details in Subsection 5.4. A possible reason is that translations are shorter than summaries. While a summary may contain around 100 words, tipically, sentences are much shorter. For instance, we are working on translations with an average length of 8 words. Less information implies more difﬁculties to ﬁnd metrics which characterise the properties of models. That is, models are not grouped separate from incorrect translations. This means that current MT evaluation metrics do not satisfy the QUEEN conjectures. Therefore, we deﬁned a new metric IQ (Innovated  QUEEN) derived from QUEEN. The ﬁrst change is to assume that a good translation should be similar to just one of the models, and not necessarily to all models. Formally, if ¦ a¢n¡ automatic translation is equal to one of the models, then § ¨P¦)¡ is maximum. For this, we consider the distance ¦ ¡ from to the nearest model in : ¢¡ § ¨¦¡ 62¦ ( § ¤£¦¥¨§© § ¨¦762 © §§ ¨P¦)762 !#"%$#¨E& ( 0 43 (A¨P¦)F61 8 (5¨B6 C F6 C 7  In order to estimate the similarity to one model IQ con- 6 C 6 C siders the distribution of distances between pairs of models ( and in the formula). However, we work under the assumption that the metrics are not capable to group all models. Moreover, we do not know which model pairs should be chosen. Therefore, we deﬁne the following criterion: “a good translation must be at least as similar to one of the models as the rest of model pairs are to each other”. In order to intro- 6 C 6 C duce this idea into the IQ deﬁnition we universally quantify the variables and :  §©  § ¨P¦)F61  T R!  i"#f"%&$¤(1& #0'((5©0¨¦)1&7623 & 96 8@C F(A6 ¨B6C 0C 76¡ C 3  This IQ deﬁnition satisﬁes the QUEEN properties described in Section 2. The main disadvantage of IQ with respect to QUEEN is that IQ considers only the similarity to the nearest model. Furthermore, it does not consider the distribution of distances between models. Therefore, IQ becomes a binary value (zero or one). That is, IQ assumes that there exist just ‘correct’ or ‘incorrect’ translations.  5. Experimental Work 5.1. Data In order to test our approach we utilized the data and results from the IWSLT04 evaluation campaign. We focused on the evaluation of the Chinese-to-English (CE) translation task, in which a set of 500 short sentences from the Basic Travel Expressions Corpus (BTEC) [4] were translated. For purposes of automatic evaluation, 16 reference translations and outputs by 20 different MT systems were available for each sentence. Moreover, each of these outputs was evaluated by three judges on the basis of adequacy and ﬂuency [18].  5.2. Metric Set We considered a set of 26 different metrics from 7 metric families:  T  X S BLEU 2 accumulated BLEU scores for several 2 -gram levels (23 54 76 %8 ). T    S NIST 3 accumulated NIST scores for several 2 -gram levels (29 @4 76 78 5A ). T   GTM 4 for several values of the & parameter (&¦ @4 76 ). mWER (default). mPER (default). METEOR 5 We used 4 variants. METEOR.exact running “exact” module only. METEOR.porter (default) running “exact” and “porter stem” modules, in that order. METEOR.wn1 running “exact”, “porter stem” and “wn stem” modules, in that order. METEOR.wn2 running “exact”, “porter stem”, “wn stem” and “wn synonymy” modules, in that order. T  X S ROUGE 6 for several 2 -grams (2B 54 76 %8 ), and 4 other variants at the 4-gram level, always with stemming: ROUGE-L longest common subsequence (LCS). ROUGE-S* skip bigrams with no max-gap-length. ROUGE-SU* skip bigrams with no max-gap-length, including unigrams. T ROUGE-W weighted longest common subsequence (WLCS) with weighting factor 'C EDF4 . 5.3. Automatic Evaluation Metrics outside QARLA First, we studied the performance of individual metrics outside the QARLA framework. System-level scores for 5 different metrics (i.e. BLEU, NIST, mWER, mPER, and GTM) were available. Additionaly, we computed the rest of metrics described in Subsection 5.2. Table 1 shows Pearson Correlation between individual metrics and human assessments. The ﬁrst two columns, ‘Adequacy’ and ’Fluency’, respectively refer to the correlation with adequacy and ﬂuency outside the QARLA framework. ROUGE variants outperform the rest of metrics both in adequacy and ﬂuency. The highest correlation in adequacy is obtained by ROUGE-S*, whereas for ﬂuency ROUGE.n3 obtains the highest correlation. BLEU and METEOR variants achieve also high levels of correlation. 2We used mteval-kit-v10/mteval-v11b.pl for BLEU calculation. 3We used mteval-kit-v10/mteval-v11b.pl for NIST calculation. 4We used GTM version 1.2. 5We used METEOR version 0.4.3. 6We used ROUGE version 1.5.5. Options are "-z SPL -2 -1 -U -m -r 1000 -n 4 -w 1.2 -c 95 -d".  Metric BLEU.n1 BLEU.n2 BLEU.n3 BLEU.n4 GTM.e1 GTM.e2 GTM.e3 METEOR.exact METEOR.porter METEOR.wn1 METEOR.wn2 NIST.n1 NIST.n2 NIST.n3 NIST.n4 NIST.n5 ROUGE.n1 ROUGE.n2 ROUGE.n3 ROUGE.n4 ROUGE-L ROUGE-S* ROUGE-SU* ROUGE-W mPER/1-PER mWER/1-WER  Adequacy 0.7623 0.8442 0.8449 0.7407 0.5136 0.6784 0.7022 0.8899 0.8837 0.8784 0.8725 0.4077 0.5245 0.5745 0.5965 0.6820 0.8582 0.9287 0.9190 0.9010 0.9153 0.9376 0.9328 0.9219 -0.5779 -0.6427  Fluency 0.6380 0.8002 0.8326 0.8600 0.5214 0.6566 0.6906 0.7463 0.7265 0.7147 0.6923 0.2323 0.3629 0.4222 0.4497 0.5950 0.6590 0.8435 0.8646 0.8527 0.7644 0.8164 0.8114 0.7737 -0.6010 -0.7214  Adequacy 0.6781 0.8770 0.8499 0.8569 0.6204 0.6687 0.6590 0.7836 0.7800 0.7886 0.7784 0.7837 0.8385 0.8421 0.8438 0.8440 0.9028 0.9238 0.9076 0.8756 0.9325 0.9357 0.9317 0.8918 0.4212 0.4507  Fluency 0.5933 0.8215 0.8212 0.8063 0.5452 0.6140 0.6094 0.6888 0.6706 0.6709 0.6513 0.6150 0.6934 0.7000 0.7030 0.7036 0.7303 0.8421 0.8630 0.8156 0.8112 0.8119 0.8096 0.7899 0.3662 0.4209  Adequacy¡¢ 0.0529 0.2567 0.3923 0.3156 0.8293 0.8015 0.7775 0.9358 0.9494 0.9420 0.8942 0.5124 0.7945 0.7277 0.8466 0.8768 0.9695 0.9673 0.9588 0.9492 0.9713 0.9663 0.9656 0.9234 0.0242 0.0880  Fluency¡¢ 0.0802 0.2788 0.4064 0.3434 0.7715 0.8126 0.8213 0.8593 0.8599 0.8554 0.8094 0.4845 0.7063 0.6952 0.8136 0.8650 0.8876 0.9142 0.9180 0.9008 0.8979 0.9062 0.9064 0.8503 0.0421 0.0770  Table 1: Adequacy and Fluency correlation coefﬁcients for individual automatic evaluation metrics for the IWSLT’04 CE Supplied Data track. ’Adequacy’ and ’Fluency’ refer to correlation outside QARLA. ’Adequacy ’ and ’Fluency ’ refer to correlation inside QARLA, using the QUEEN measure. ’Adequacy¡£ ’ and ’Fluency¡¢ ’ refer to correlation inside QARLA, using the IQ measure.  5.4. Automatic Evaluation Metrics inside QARLA First, we computed the QUEEN measure based on each metric individually. See correlation results in Table 1, columns 3 and 4, ‘Adequacy ’ and, ‘Fluency ’, respectively. For most of the metrics there is no signiﬁcant improvement. Only in the case of the NIST family of metrics , there is a consistent and very substantial improvement with respect both to adequacy and ﬂuency. The highest levels of correlation are again achieved by ROUGE.n3 and ROUGE-S* metrics, but at the same degree than outside the QARLA framework. The combination of these two metrics, ¤ ROUGE.n3, ROUGE-S*¥ , does not report any signiﬁcant improvement. Next, we computed IQ measure based on each metric individually. See correlation results in Table 1, columns 5 and 6, ‘Adequacy¡¢ ’ and, ‘Fluency¡¢ ’, respectively. All metrics but BLEU-based, WER and PER, obtain higher levels of correlation both with respect to adequacy and ﬂuency when applied inside QARLA. Again, ROUGE variants attain the highest levels of correlation in adequacy and ﬂuency. METEOR variants obtain also high levels of correlation. The highest correlation in adequacy is obtained by ROUGE-L,  whereas for ﬂuency ROUGE.n3 achieves the highest correlation. The combination of these two metrics, ¤ ROUGE.n3, ROUGE-L¥ , does not report any signiﬁcant improvement. The extremely low levels of correlation attained by BLEU, WER and PER deserve further analysis. By inspecting results, we observe that these metrics generate very low IQ values. A possible explanation is that while most of the current metrics are able to exploit multiple references simultaneously, QARLA works with similarities on a singlereference basis. Each translation is contrasted with each reference independently, so there is a decrease in the reliability of automatic metric scores. The QUEEN measure is not affected because it considers the similarity to all references whereas the IQ measure considers only the similarity to the closest reference. BLEU, WER and PER seem to be specially sensitive to this problem. BLEU looks for high precision over any of the models. We conjecture that BLEU is specially useful when it works over a set of models (multiple references), which is not the case in QARLA. Regarding WER and PER, we think that these metrics are possibly capturing non-relevant differ-  ences between translations. Thus, they are placing models too close to each other. Recall the IQ deﬁnition in Section 4. Good translations must be at least as similar to one of the models as the rest of model pairs are to each other. WER and PER are therefore obliging candidate translations to be extremely similar to one of the references in order to be considered correct.  5.5. Metric Combinations  One of the main features of QARLA is that it allows to ro-  bustly combine several evaluation metrics. We study several  combinations. Due to the computational complexity of exhaustively trying all metric combinations7 we performed a  clustering as described in [3] so as to detect metrics that be-  ¦)F6HF6 C F6 C have similarly. This clustering process is based on the be-  haviour of metrics over samples ¤  ¥ . We con-  ¦ 6 6 C 6 C sider that two sets of metrics behave similarly if the auto- matic translation is as close to the model as ,  are to each other for both sets of metrics. We applied the  k-means algorithm [19].  Clustering results are shown in Table 2. Very interest-  ingly, clusters 1 to 4 group some metric variants at the same  level of granularity (from 1-gram to 4-gram). WER and PER  remain together in cluster 5. Clusters 6 to 9 put together  several variants of METEOR, NIST, GTM, and ROUGE, re-  spectively.  From each cluster we selected a representative based on  the level of correlation between the IQ measure and human  assessments, as reported in Table 1 (columns 5 and 6). Ac-  tually, a representative for adequacy and a representative for  ﬂuency were chosen. We did not use cluster 5. Therefore,  we limited our exploration to 510 metric combinations, 255  for ﬂuency and 255 for adequacy.  Table 3 and Table 4 show correlation with adequacy and  ﬂuency, respectively, for some combinations of metrics. In  the case of adequacy we did not ﬁnd a combination exhibit-  ing a higher correlation than ROUGE-L alone. In the case of  ﬂuency 4 combinations outperformed ROUGE.n3, although  not very signiﬁcantly. The best combination is ¤ ROUGE.n3,  ROUGE-SU*¥ .  We suspect that the beneﬁts of combining metrics are hid-  den by the very high levels of correlation already achieved by  single metrics. We further discuss this problem in Section 7.  6. IQMT for IWSLT 2005  We present preliminary results on the evaluation of the Chinese-to-English Supplied Data track of the IWSLT 2005 Evaluation Campaign [20]. The test set consists of 506 very short sentences (average length of 6 words). 16 reference translations and 11 system outputs were available for each sentence. Human assessments, based on adequacy, ﬂuency and meaning maintenance at the system level were available.  ¡£¢¥¤§¦  7There are  possible combinations if we take into account all  metrics.  We studied the behaviour of individual metrics outside QARLA. Very high levels of correlation (over 0.95) are achieved. METEOR variants and ROUGE.n1 are the metrics that obtain the highest levels of correlation with respect to adequacy (0.98) and meaning maintenance (0.99). For ﬂuency, BLEU.n4 and GTM.e3 obtain the highest correlation (0.95). In spite of the very high levels of correlation already achieved outside QARLA, we tested the behaviour of these metrics inside QARLA. Levels of correlation attained for adequacy and meaning maintenance are also very high inside QARLA. NIST.n1 is the highest scoring metric for adequacy (0.98) and meaning maintenance (0.97). All metrics exhibit very high levels of correlation for adequacy (over 0.82), and meaning maintenance (over 0.85). As in the case of the IWSLT 2004, ROUGE variants obtain very competitive results. However, for ﬂuency, a signiﬁcant drop is observed. The levels of correlation range from 0.56 to 0.85, being the highest correlation value achieved by BLEU.n4. Although most metrics correlate better with ﬂuency inside QARLA, metrics such as BLEU.n4, GTM.e2, GTM.3 or ROUGE.n4, which reward longer matches, exhibit a substantial decrease. We suspect that our framework is not well suited to measure the ﬂuency over translations that are so short (6 words). In fact, we argue whether it makes sense to do so. By working on very short translations we are practically forcing candidate translations to match exactly one of the references. Finally, we tried some metric combinations. Again, due to time constraints, we performed a clustering, obtaining similar clusters to those derived from the IWSLT 2004 data. We arbitrarily explored some combinations by selecting the six most promising metrics. For adequacy and meaning maintenance we explored 63 combinations determined by the set ¤ BLEU.n1, GTM.e1, METEOR.wn2, NIST.n1, ROUGE.n1, 1-PER¥ . For ﬂuency we explored the 63 combinations in the set ¤ BLEU.n4, GTM.e2, METEOR.exact, NIST.n5, ROUGE.n4, 1-WER¥ . Table 5 shows Pearson correlation values with respect to adequacy, ﬂuency and meaning maintenance, for the best combinations. Consistently to the results on the IWSLT 2004 data, no signiﬁcant improvements are reported when combining different metrics. 7. Conclusions The most important conclusion in this work is that most individual metrics improve when they are applied inside the QARLA framework. The reason for that improvement is that IQ takes as reference similarities between models, normalising the scale of the metric regarding to the models set distribution. We observed that improvements obtained in the case of the IWSLT 2004 are more signiﬁcant than in the case of the IWSLT 2005. We believe that the sentence average length is a key factor to explain this fact. Moreover, one of the motivations for our work was to  Cluster id 
1. Introduction I am very pleased to have been invited to give this opening talk at EAMT 2005, although I regret that Harry Somers cannot be with us in Budapest. Australia is a long way from Hungary and he is still enjoying his sabbatical there, but it was when I was talking to Harry a few months ago about some aspects of my work on MT at DSTO that he thought it would be interesting for the EAMT audience. I want to tell you about how we have been able to build a translation system without doing MT and about the way we dealt with the difficulties of getting access to users in our specific environment. Before I tell you what I do at DSTO, I need to say a few words about what it is. DSTO stands for Defence Science and Technology Organisation and is it the R&D organisation for the Australian Defence Organisation. Our customers and end users are primarily the ADF (Australian Defence Force, the military side of Defence) and the ADO (Australian Defence Organisation, which also includes the civilian side of Defence), but also the Australian Government more generally. As an R&D organisation, DSTO may be more “small r and big D” than in earlier times, but it is committed to exploring and utilising technical innovations. In fact, our main role is to give advice on new technologies and to build prototypes to show what advantages these technologies can bring to the end users. I am sure everyone knows where Australia is, but it is always interesting to see on a map  what the world looks like from our perspective. Australia is geographically isolated, certainly far away from Europe and North America, and further from Japan than people realise. We are part of the Pacific-Asia region, with very different linguistic neighbours than our traditional allies, the UK and the US. Australia is a "small" country in spite of its size, with a population of just over 20 million. Our resources are not huge, especially in terms of personnel. The environment, which is very harsh on most of the continent, also means that our technological requirements – and traditions – are quite different from those of European or North American countries. This is one of the reasons why Australia has a Defence R&D organisation, because technologies that may be appropriate for other countries need to be evaluated for our environment and sometimes new solutions need to be developed to meet Australian requirements. And we can argue that this is in fact the case when we look at MT and our linguistic environment. The languages spoken in our part of the world are not those that have traditionally been worked on for MT and, for many of them, NLP tools or resources are not even available. In that context, at DSTO, I am now leading a research programme in language technologies for a variety of purposes. These include spoken dialogue systems (Estival et al., 2003), multimodal interaction in a virtual environment (Estival et al., 2004), document classification (Carr & Estival, 2003), semantic clustering and language translation tools, which is the one I will  EAMT 2005 Conference Proceedings  
Abstract. This paper proposes that quality criteria are set up for machine translation systems that attempt to capture what can be considered the upper limit for the performance of MT systems. It argues that the best performance MT currently should aim for is close translation, and that attempts should be made to codify what that entails for any given language pair. Some guidelines for the selection of close translations are proposed and their application on concrete examples drawn from an English-Swedish parallel corpus is discussed. The definition of different quality levels in relation to a high-quality standard is also touched upon.  1. Introduction Machine translation research and development have long since abandoned the goals of "Fully automatic high quality" translation. While this is obviously a realistic decision, one may wonder if anything has come to replace it. Practical utility is perhaps the most obvious answer and experience tells us that MT systems of very different quality and coverage may be used to good advantage. Evaluation is another aspect that has come to the fore as a prerogative for advances in machine translation. Coupled with practical utility evaluation tends to focus on factors that are relevant for the purpose at hand and translation quality may not be the most prominent factor then. However, disregarding the price factor, there can be no doubt that systems are generally more useful the larger their coverage is and the higher their accuracy. As regards translation quality, evaluation refers to criteria such as fidelity, intelligibility and fluency (White, 2003). These aspects are usually measured by comparisons with reference translations produced by human translators, whether by human judges or automatically (Papineni et al., 2001). While the reference translations are usually good translations, or even "expert translations", their qualities, or the requirements given to the translators, are sel- EAMT 2005 Conference Proceedings  dom discussed in any detail, at least not publicly. Similarly, parallel corpora, such as the Hansards, that are used for training statistical MT systems, are produced by human translators aiming at high quality by human standards. But this quality level may actually be beyond reach for any known system and translation approach developed to date. The question is, then, what output quality we can expect and demand from a MT system, today and in the future. This is an important question both for researchers, potential customers, and the society at large. Researchers would benefit from having challenging though realistic goals to reach. Customers and users would benefit from having standards to compare with. In particular, it would be useful for customers to have access to quality labels that would give rough but reliable information on the linguistic scope and quality of the output from the system (cf. Hutchins, 2000). Finally, language communities would be better off if the production of mistranslations and gibberish could be kept to a minimum. In this paper I make a proposal for how the question of goals and standards in terms of translation quality can be approached. Simply put, the proposal is that we should use available resources, such as contrastive grammars and, in particular the parallel corpora that are now being created and annotated in large numbers, in 13  Ahrenberg combination with our knowledge of what makes machine translation hard, to specify the highest quality requirements that we can expect a system to meet. I will argue below that these requirements would fall within the bounds of what is often called close translation. And since detailed specification is needed, I call it a proposal for codifying close translation. The proposal also entails that we can define quality levels by reducing the requirements on the system and use these to rate and communicate the properties of a given system. The IAMT Certification Group has defined three levels in terms of qualities such as dictionary size, sentence types the system can translate, user facilities and intended use (Hutchins, 2000). An assumption of this proposal is that distinctions relating to grammar coverage would also be helpful. It might be said that all (high-quality) MT systems developed to date have been developed to meet specified requirements. If so, however, these requirements are not known, and it is also not known how well the end product meets them. It is an important part of this proposal that a code for close translation is made public and preferably developed as a community effort. A code for close translation primarily deals with syntactic constructions and the grammatical words and morphemes that go with them. Of course it is of the utmost importance that a system has a large lexicon and can handle the ambiguities of content words including multi-word predicates. This is a separate quality aspect, however. Another complementary quality aspect is accuracy. Obviously, a close translation need not be free of errors. Table 1 displays a cross-tabulation of three quality criteria that are of primary importance in characterising the properties of a MT system: coverage of the source language, coverage of translation possibilities and accuracy. In this paper the focus is on the second aspect as it applies to grammar, though several comments will be made on its relation to the other qualities, since they cannot be treated independently. SL Cov- Translation Accuracy  Grammar Lexis  erage  Coverage X  Table 1: Translation quality criteria and the scope of this paper  Issues of quality cannot be discussed very deeply in the abstract. Thus, it is necessary to look into the relations between specific languages. Moreover, it is to be expected that translation quality can be higher the more closely related the languages under consideration are, and this would apply to all linguistic levels from orthographic conventions to pragmatics. In the paper I will draw examples from a current project of mine in making a proposal for a codified close translation manual for the language pair English-Swedish.  2. Modes of translation The field of translation studies has identified a number of modes of translation that differ in the degree to which there is a correspondence between translation and original. The number of modes may differ between writers but the general idea is to identify a few landmarks as we go from one extreme to another. At one end we have translations that give a counterpart in the target language to every word, or sometimes even every morpheme, of the source sentence, and perform no structural changes whatsoever. This is known as word-by-word translation, or in the most extreme case, as morphematic translation, and is of little use outside the fields of linguistics and language teaching. At the other end we find translations that represent the content of the source text in a manner appropriate for a particular target audience. At this end, often called adaptation, adherence to structure is not a primary concern and there may even be many changes in content and lexis due to cultural, economic or judicial factors. 2.1. Human modes In between the extremes we find the dichotomies of formal vs. dynamic equivalence (Nida & Taber, 1969), or semantic vs. communicative translation (Newmark, 1988), where the one  14  EAMT 2005 Conference Proceedings  member emphasizes the requirement that a translation should represent the content and style of the original, and the other member emphasizes the requirement that a text should communicate a message to a reader in the most transparent way. Finer distinctions are possible to make, however, and Newmark (ibid.) provides a hierarchy of eight categories to cover the whole space. Here I will discuss three categories that Newmark calls literal, faithful, and semantic, respectively, though I use the term ‘close’ rather than ‘faithful’. A literal translation differs from a word-forword translation by adhering to target language grammar. However, words are translated out of context and all kinds of figurative or metaphorical uses of language are missed, as the word is regarded as the only semantic unit of interest. Thus, literal translation produces texts that are non-idiomatic and often with a strange or even funny character. A close translation is one which renders the source text as completely as possible using target language grammar and paying regard to the textual function of words, not only their concrete meaning. A close translation pays attention to structure and does not paraphrase. Correspondences should be found at the lowest level possible; at phrase level if not at word level, at clause level if not at phrase level, and so on. A semantic (or flexible) translation is like a close translation in that it should give a complete rendering of the contents of the source text. However, it pays more attention to the reader's ability to receive the content, and the fluency and naturalness of the target text. In a semantic translation the structure of the source is less important than the style and aesthetic value and so the language of a semantic translation is more varied than that of a close translation. 2.2. Machine translation modes MT has been described as a special mode of translation (Sager, 1994). One aspect of this is that automatic translations tend to be structurally close to the source structure, and in any case, much closer in terms of structure than human translations (Ahrenberg & Merkel, 2000). Another aspect is that the number of al-  Codified close translation as a standard for MT ternatives offered for given source language words and constructions is generally smaller than what a translator can produce without much effort. While some approaches such as example-based MT or interlingual MT can cope better with structural differences and functional variants than other approaches, it is generally true that the more variation, the harder it is for the system to pick one which is appropriate in a given context. This is true also for statistical systems that generally perform better if source and target can be made more structurally similar before training starts (cf. e.g. Nießen and Ney, 2004). 2.3. Close translation as a goal Given this state of affairs it seems reasonable that MT should aim for reaching as far as possible towards the semantic end of the literalsemantic continuum. As a first step, I propose that the goal be set to close translation. This would be a desirable goal for a high-quality gisting system or a core system on which to develop domain-restricted systems. If this can be agreed, it should have some important effects on both training and evaluation, since the data one should use for training and evaluation should have been translated according to the requirements of close translation. This, in turn, means that those requirements need to be specified. Such a specification is what I'd like to call codified close translation, since it needs to be detailed, not just a list of general descriptions with a few illustrative examples. At the same time, it cannot be fully formalized either, since this would make the task too hard and the community too small. A close translation can be described as one whose parts can be aligned exhaustively with corresponding parts of the source language. We may call this The Alignment Criterion which in more precise terms can be spelled out as follows: (1) Every clause of the source will have a counter-part in the target; conversely, every clause of the target will have a counter-part in the source; (2) Syntactic phrases and word tokens of the source, with only few and systematic exceptions, have at least one exponent token in the target; conversely every syntactic phrase and word token of the target must, with only few and systematic exceptions, be an ex-  EAMT 2005 Conference Proceedings  15  Ahrenberg ponent of at least one source phrase or token; (3) Text level alignments belong to types, that can be instantiated in a variety of contexts. 3. Codifying close translation It is a fact that a translation is underdetermined by the source language text. For this reason some authors have argued for interactive architectures where the system asks the user for help whenever the information in the SL text is insufficient to determine a safe translation (e.g., Johnson & Whitelock, 1989; Somers et al., 1990), and others have argued that relevant world knowledge should be modelled to support decisions (e.g., Nirenburg et al. 1991). It may well be that the goals for MT, when developed for general (unknown) text, will have to be set at a low level, lower than this paper assumes. However, it is still of interest to know where that level is, and how it can be characterised in the terms of translation studies, given that we have access to more and more translation data to inform system development. 3.1. General selection criteria The Alignment Criterion in itself allows a wide range of possible translations. For this reason it would be valuable to have guidelines available as we approach the task of selecting translation solutions. An overall goal is that the chosen solutions together should cover the space of possibilities as far as possible. At the same time, their conditions of application should be clearly identifiable so that translation errors are avoided. However, this goal is hard to achieve, and the error rate of a set of options is hard to estimate without empirical testing. The following criteria for selection of useful correspondences seem to be strong candidates, however: Semantic equivalence. A target language item that consistently has the same or a similar meaning potential as a given source item, is likely to be more useful than one that requires contextual support to convey a similar content. Structural similarity. An item of the translation that has the same structure as the corresponding item of the source is generally easier to produce for any MT system; Absolute frequency. The conditions of occurrence for correspondences that occur in high  numbers are usually easier to identify and describe, and, can be estimated statistically with greater confidence; Relative frequency. A choice made more often spontaneously by human translators is likely to be more natural and expected than one which is used relatively rarely; A problem is, of course, that these criteria often point in different directions. A typical case is the English preposition 'of' when used as a genitive. There are three common ways of rendering that meaning in Swedish, as illustrated below: E: the roof of the house S1: taket på huset "the roof on the house" S2: husets tak "the house's roof" S3: hustaket "the house roof" If a preposition is used in the Swedish translation it will be one with a more specific meaning than ‘of’. Thus the choice of preposition in a Swedish phrase of the form 'NP1 P NP2' corresponding to English 'NP1 of NP2' will depend on knowledge about likely actual relations between referents of NP1 and NP2, which makes the task difficult. The s-suffix underspecifies the relation in quite the same way as the preposition 'of'. On the other hand it induces a structural change and puts more demands on the system for this reason. The third possibility, compounding, is generally more underspecified than the s-suffix and may lead to misinterpretations, although it is sometimes the preferred choice: E: the turn of the century S: sekelskiftet (not 'seklets skifte') E: the flat of his hand S: handflatan (not 'handens flata') E: platitudes of the media S: mediaplattityder (here 'mediernas plattityder' would be ok)  16  EAMT 2005 Conference Proceedings  One may regard the first two examples as lexicalized, but since the option is there one would like to have it under control. It is also not so easy always to distinguish 'of' with a genitive reading from other readings. A few examples of this kind are E: the study of literature S: litteraturstudiet E: the quality of life S: livskvalitet The fact that general criteria do not suffice for the task of selection makes it necessary to study different items in detail and see whether it is actually possible to determine the conditions under which the different translations can be used. Ideally, one would like to arrive at one of the following situations: (1) Under a specified set of conditions, only one option is possible and is thus obligatory; (2) Several options exist, but one of them can be used as a default and the use of the others can be attributed to special conditions, or be found only for lexicalized forms; (3) Several options exist and can be used interchangeably. Often, however, we will find that several non-equivalent options exist, although it is quite difficult to specify the conditions that favour the use of one over the others. This seems to be the case with the of-genitive, although my tentative decision is to regard the Swedish s-genitive as the default translation. Generally speaking, this is the kind of situation where we have to make practical decisions on the basis of frequency of use, or observed error rates (and call for more research). 3.2. Towards a descriptive framework While the primary aim of the code is to determine the translation options for an MT system, we may start out from a more comprehensive description that covers the variation we can find in human translations. From this comprehensive grammar we can then select those options that best meet our requirements (as defined in the previous section). The selection process can proceed as follows:  Codified close translation as a standard for MT • What constructions of the source language should be included? • Which of the different translation options for a given construction constitute an optimal set? It should be noted that the two languages have different roles. We are not putting them on an equal footing but rather try to describe what happens when one language is the source and the other the target. Thus, entries are taken from the source language. An entry covers a set of SL strings with associated translations. In referring to the SL and TL parts we use labels in the form of more or less elaborated grammatical descriptions. The pair of an entry and one of its translation options forms a relation that in principle may have an infinite number of instances in parallel texts. Thus, the code provides a partition of possible alignments at the string level, at the same time providing each partition with a reference in grammatical terms. When stating the conditions for this relation, we often need to refer to linguistic material in the surrounding context, which may be small or large. In the descriptions I use the attribute ‘parameters’ to refer to the relevant contextual material, and the attribute ‘scope’ to refer to the size of the context. Salkoff (1999) codifies the relation between French and English constructions using schemata, where a schema is, basically, a string of words and category symbols, where the symbols in turn may represents a cluster of syntactic properties, functional role and, possibly, a semantic category. Similar schemata are also used in the proposed framework, but they complemented with other types of information. In particular, we need attributes that relate entries to each other. Different entries may apply to the same strings and in these cases we should give information as to which entry (if any) takes precedence, or whether they are in competition. Also, the status of an option as obligatory or optional is important information. Another difference between Salkoff’s framework and mine is that he seems to look at his rules as recipes for translation. The translation relations codified in this framework may be looked at that way also, but they need not. The  EAMT 2005 Conference Proceedings  17  Ahrenberg  idea is not to prescribe the internal workings of an MT system, but to define the possible end results. Table 2 gives an overview of the most important attributes of the framework.  Attributes Reference Level Scope Forms Schema Superior entries Competing entries TL option Status Proportions Example  Explanation A label for the SL entry in grammatical terms Word, phrase, clause, … Word, phrase, clause, … A list of instances. The list is marked as exhaustive, finite or open-ended. A structure referring to relevant linguistic parameters and constraints. Entries that take precedence over the current one Entries that may apply to the same forms and source schema A label in grammatical terms for a class of translations The use of a TL option as obligatory, default, optional or as an exception. A percentage indicating how common the TL option is in a given corpus. Source forms with associated translations for a given option  Table 2. A list of attributes for coding source language items and their translation options.  3.3. Examples This section gives some concrete examples of how translation relations can be coded. To save space, not all of the attributes are used, nor are the descriptions always complete. Moreover, for each case we discuss how the selection criteria should be applied. 3.3.1. A function word: The definite article The English definite article is the most common word token of the English language. Swedish also has definite articles, but unlike French or German, also use a definite noun suffix with approximately the same function. In some contexts these two exponents for definiteness are  used in combination, the so called double definite, in other contexts only one of them is used, and, not infrequently, none of them is. In those cases the noun phrase usually contains a determiner other than the definite article. Altogether this amounts to four different translation options that are shown in Table 3. In addition to the options displayed in Table 3 we find examples where the translator has used a different determiner. It is even possible to use an indefinite Swedish NP to translate an English definite NP. E: The rapid and efficient processing of petitions is therefore an excellent means of increasing people’s confidence. S: En snabb och effektiv behandling av petitioner är därför ett utmärkt sätt att öka människornas förtroende. In general, Swedish often accepts both indefinite and definite articles for generic references. While a definite NP in the translation (den snabba och effektiva behand-lingen) would suggest a specific reference, a translation without any article would work equally well. However, this would still require the adjectives to be in indefinite form. Unless the conditions for the choice of the indefinite article can be specified clearly, this may be a case where we would prefer an MT system to perform suboptimally. Another case of explicitation occurs with references using family names. A noun phrase such as the Weasleys can be translated simply as Weasleys, but in many cases this would sound insufficient, and the translator would use an elaborated phrase such as familjen Weasley (the Weasley family) or bröderna Weasley (the Weasley brothers) instead. Again, to perform such a feat, the system would need an ability to understand references that may go beyond what is currently achievable.  SL reference Forms Schema Related  The definite article (exhaustive) The, the [D X* N Y*], where D is the entry, N is the head of D, X and Y. Comparative determiner  18  EAMT 2005 Conference Proceedings  entries Option 1 Forms Schema Example Option 2 Forms Schema Example Example Example Option 3 Forms Schema Status Example Option 4 Schema Status Example Schema Status Example  Double definite (finite) (den+-en), (det+-et) ... [D’ Z+ N’ W* ] where Z translates X or Y.1 the big box : den stora lådan definite noun suffix (finite) –en, -n, -et, -t, -na [N’ Z*] the box : lådan the letter to Mary : brevet till Mary the fire alarm : brandlarmet definite/determinate article den, det, de [ D’ Z* N’ W ] where W is a relative clause optional the mistake that he made : det misstag han gjorde Null translation2 [ G Z* N’ W* ] where G is a genitive noun translating an ofgenitive Y. Obligatory the capital of Sweden : Sveriges huvudstad [Z N’ W* ] where Z translates X or Y with an adjectival determiner such as samma, nästa, följande, ... Default the following day : följande dag  Table 3: Swedish translation options for the English definite article. 3.3.2. Parts-of-speech: Adjectives For all of the common parts-of-speech in translating from English to Swedish, the default case would be a translation of the same part-ofspeech. So, a noun would be translated by a noun, an adjective by an adjective, and so on. This is not always the case, however, and so the  
Catalan is co-official with Spanish within the autonomous regions of Catalonia, Valencia and the Balearic Islands. This means that, quite often, all kind of written texts – Public Administration documents and websites, newspapers and magazines, books, etc. – are published in both languages. Moreover, there is a real need for translation of documentation, user-guides, brochures, etc. both by non-Catalan companies that establish themselves in Catalonia, or by Catalan companies that want to expand their business activities outside the Catalan-speaking territory. This is the perfect environment for the productive use of machine translation systems, especially when the close linguistic relationship enables a very good translation quality, at least in our MT system. 2. Some Facts about the Catalan Language Catalan is a Romance language, linguistically closely related to other neighbor languages such as Occitan, Spanish and French. Catalan is widely spoken in the autonomous regions of Catalonia, Valencia (where the language is  called “Valencià”) and Balearic Islands, in Spain, where it is co-official with Spanish. It is also spoken in the department of Pyrénées Orientales, in France, in the small country of Andorra, where it is the only official language, and in the city of Alghero, on the island of Sardinia. All in all, some 7 million speakers speak it. Catalan is widely present today in the public media, especially in Catalonia: radio and TV stations, daily and weekly press, books of all kinds, cinema, theater and Internet. It is also widely used in public announcements, official documents and websites, business, banks, schools and universities – where teaching usually takes place in Catalan – and, in general, in all kinds of everyday-life activities. The current linguistic situation is thus one of bilingualism: virtually all Catalan native speakers can also speak and understand Spanish. The opposite (Spanish native speakers living in Catalonia who are fluent in Catalan) is also often the case, but not always. 3. The Uses of MT Systems One of the factors that determines the appropriateness of a machine translation system is the specific use that it is given. Basically, there are  EAMT 2005 Conference Proceedings  23  Alonso two kind of uses that can be given to MT systems:  “Informative use”: The MT system is used to extract (at least some) information out of a text written in a language that the user does not know. The aim is thus to use it to obtain information that otherwise would not be available to the user. A typical example of this case is the use of MT systems to translate web pages in foreign languages. In this case, the quality is not a key factor, but the number of language pairs available and the translation speed are.  “Productive use”: The MT system is used to reduce the time and effort needed to produce professional translations (i.e. meant to be published). The aim is thus to use it as an efficient tool within a productive chain. In this case, translation quality is of course the key factor, together with translation speed and customization of the system to the specific needs of the user. 4. MT for Catalan↔Spanish 4.1. The Outstanding Quality of Catalan↔Spanish MT Catalan↔Spanish is, by far, the language pair that yields a better translation quality out of the 23 translation directions currently offered by Comprendium. According to Translation Quality Evaluations recently made in our company, it yields 93% of good and understandable sentences1, compared to a 84% for Spanish→ French or to a 67% for English→Spanish. This excellent translation quality is largely due, but not only, to the close linguistic relationship between both languages. Figure 1 illustrates these results. 
 1. Introduction A text corpus plays a crucial role in many spoken-language applications, such as speech translation and statistical natural language processing. The system’s accuracy often depends on whether we can accumulate a large amount and wide variety of text data containing frequent or domain-specific linguistic expressions. However, there are fewer existing spoken-language corpora than there are written-language corpora. To make matters much more difficult, spokenlanguage corpora specific to the systems’ domain is often unlikely to even exist. For these reasons, we must make an effort to build a spoken-language corpus in the system’s domain. Conventionally, a spoken-language corpus has been built using the following four methods: (a) Collecting text from existing documents : The text related to the system’s domain is copied from existing documents. Electronic data can be also used in some cases. (b) Transcribing recorded dialogs (Hirschman, 1992; Heeman & Allen, 1995; Takezawa, 1999; Allwood et al., 2000): A scripted, situational dialog is recorded. Then, the recorded dialogs are transcribed. EAMT 2005 Conference Proceedings  (c) Storing keyboard chats (Kikui et al., 2003): Two participants chat through their keyboard terminals according to preferences or interests. The chat logs are stored as text data. (d) Writing imaginatively (Hirasawa et al., 2004): Given specific conversational scenarios, wri- ters imagine the following scenes and then create sentences that are likely to be uttered. If we can find a lot of text related to the system's domain, (a) is the most suitable method. However, most of the time very little text exists. Additionally, copyright problems can arise. To avoid these problems, method (b) or (c) is usually used. Method (b) approximates the scenes to which the system will be actually applied and produces good quality text. For example, the CALLHOME corpus from the Linguistic Data Consortium was constructed using this approach (CALLHOME, 1996). However, using method (b) the quantity is apt to be small because it requires at least two people, and it takes a large amount of labour to build a large corpus. Method (c) has the same problem. In contrast to these methods, method (d) reduces the cost of construction. We can create bigger volumes of text using method (d) or a compromise between (c) and (d), in which just 27  Asanoma, et al. one person imaginatively writes chat texts. However, it is difficult to persistently create the variety of expressions available in either method because only one person has a limited imagination. Although combining paraphrases of fragmentary linguistic expressions can create a lot of example sentences in a single sitting, such texts do not accurately reflect the statistics of linguistic phenomena. Moreover, in natural conversation we can not prepare all scenes in advance. To overcome the problems of conventional methods, we propose a method for easily proliferating conversation texts that can reduce costs by providing writers with “germ dialogs”. The germ dialogs are short scripted dialogs that enable the writers to easily image a follow-up dialog. This method is an improvement over the creative writing method (d). The remaining part of the paper is organized as follows. Section 2 explains the method of deriving text from germ dialogs. Section 3 describes the corpus built by the proposed method. Section 4 presents evaluations of the proposed method, based on language models made from prepared corpora. Section 5 will describe our conclusions. 2. Deriving Text from Germ Dialogs In our approach, new dialog texts are derived from the germ dialogs, which can be prepared by creative writing or spoken dialog transcription in conventional methods and by taking excerpts from published conversation books. Because the size of derived texts is assumed to be much larger than that of germ dialogs, we can exclude germ dialogs from the corpus, thus avoiding the copyright issue, even if the excerpt is taken from copyrighted materials. The germ dialogs set the conversation scenes and make it easier to create sentences than to imagine them. Moreover, they control the consistency of the topics and style set by each germ dialog.  We derive text from germ dialogs by applying such techniques as “retrace”, “fill-in”, “replacement”, and “follow-on”. Figure 1 illustrates the text derivation from a germ dialog between two speakers, A and B. “A1” means the first utterance of speaker A, and the number denotes the utterance order. Retrace: Writers create all the possible dialogs arising from the germ dialog. Fill-in: Some utterances in a germ dialog are left blank intentionally. The writer fills in the blanks with as many reasonable expressions as they can imagine in a given context. Replacement: Some utterances in a germ dialog are re- placed with new utterances produced by writers. The new utterances may have the same meaning as the originals, or they may be paraphrased or even have a different meaning than the original. Follow-on: This technique is an easier way to expand the dialog corpora without losing naturalness, as compared with above three techniques. Additionally, it can be positioned as the main part of the proposed text derivation method. Writers are asked to create two or more possible utterances that might reasonably follow the germ dialogs. Then, two or more responses are also created for each possible utterance. For instance, one utterance may yield two utterances in response: these two draw two responses each, or four sentences in all, and so on. In this way the dialog grows exponentially. In addition to the possible utterances, the number of dialog turns is a direct factor in determining how much the corpus size can be expanded. However, we consider that the followon dialogs should be limited to three or four turns, because more turns might dilute the germ dialog.  28  EAMT 2005 Conference Proceedings  Building a conversation corpus  Retrace A-2: A new airport has been built to cope with the ever increasing number of tourists visiting Disneyland. B-2: Is the new airport far from downtown? A-1: The airport is relatively far away, so you should make sure you leave early on the day of your flight. B-1: Is it convenient to take any public transportation to the airport?  Germ dialog A1: Yes, there are buses and taxis available, but I would suggest taking a shuttle bus from a hotel. B1: A2: Almost all hotels in Anaheim have shuttle buses. B2: A3: An airport bus can cost from 10 to 20 dollars depending on which airport. Yes. The hotel bus is usually free. Follow-on  Fill-in B1: Do all hotels have the shuttle bus service? B1: How many of the hotels in Anaheim operate a hotel shuttle bus? B2: How much is the airport bus? Do I have to pay for the hotel shuttle bus as well? : Replacement A3: Not all of them, only the high class hotels. A3: I’m not sure. Let’s ask the people at the reception desk.  B3: I’ll take the hotel bus then.  B3: Actually, I think I’ll take a taxi.  A4: Good choice, you can’t beat the price.  …  …  A4: You won’t regret taking the shuttle bus.  B4: I wonder if I should make use of any other free services.  …  …  …  …  B4: Do you think the hotel bus will be punctual though?  …  …  Figure 1: Text Derivation from a Germ Dialog  EAMT 2005 Conference Proceedings  29  Asanoma, et al. 3. Building a Conversation Corpus We used the proposed method to build a conversation corpus in English and Japanese. We selected fifty germ dialogs from each language from published books, including daily conversations about various topics, such as weather, health, travel, movies, and so on. The length of the germ dialogs was two or three dialog turns, which was enough to imagine the conversational situation clearly. Ten writers were then asked to create utterances from each of the fifty germ dialogs in their native language through the “retrace”, “fill-in”, “replacement”, and “follow-on” techniques as follows. The numbers below are the same in English and Japanese. Retrace: One dialog consisting of two turns (four utterances) was created for each germ dialog by each writer. The total number of utterances was 2,000; (50 germs ∗ 1 dialog ∗ 4 utterances ∗ 10 people). Fill-in: Two utterance blanks for each germ dialog were filled in with two possible utterances, selected by each writer. The total number of utterances was 2,000; (50 germs ∗ 2 blanks ∗ 2 utterances ∗ 10 people). Replacement: One utterance was replaced with three possible utterances for each germ dialog by each writer. The total number of utterances was 1,500; (50 germs * 1 utterance * 3 replacements * 10 people). Follow-on: Each writer followed through two turns, using binary branching for each germ dialog. The total number of utterances was 15,000; (50 germs * (2 + 4 + 8 + 16) utterances * 10 people). Thus, 20,500 utterances (2,000 + 2,000 + 1,500 + 15,000) were created from 50 germ dialogs, and this was about one hundred times the total utterances of the 50 germ dialogs, each of which consisted of about four utterances. Some utterances created consisted of two or more sentences. For example, 15,000 Japanese utterances created using “follow-on” resulted in 18,059 Japanese sentences. 4. Evaluation We applied the corpus built using our method to provide a speech translation system using the domain’s statistical language model, where the possibility of word transition was statistically  calculated for speech recognition, parsing and so on. To verify the cost effectiveness of our method, we evaluated a Japanese language model generated from the corpus by comparing the proposed method with a conventional method (c). We also compared the quality of the corpus built by the proposed method with that of an existing corpus, CALLHOME, created using method (b). During our evaluation, we compared the costs according to the work load differences required to build the same amount of text, using the same number of skilled staff. We measured the adequateness to the domain by comparing the coverage and perplexity of the language model. These were based on the idea that the wider coverage is and the lower perplexity is, the better the quality of the corpus becomes. Our statistical language models were generated from word trigrams calculated from the corpora. Coverage and perplexity are often used as indices of statistical language models in research areas, such as speech recognition. 4.1. Comparison with Keyboard Chats As a conventional method, we built the corpus from keyboard chats; method (c) in Section 1. Two people were asked to have a keyboard chat about a specific topic. They were also asked to type texts as if they were actually speaking. Using this method, we collected 1,464 Japanese utterances or 2,753 sentences. To evaluate the proposed and conventional methods under the same conditions, we randomly extracted 2,753 sentences from the corpus built using our new “follow-on” method. As an evaluation corpus for actual spoken dialogs, we chose CALLHOME Japanese transcripts (LDC96T18) that are available for general use; that is, the comparisons were between the proposed method & CALLHOME and keyboard chat storage & CALLHOME. We extracted 25,822 sentences from CALLHOME corpus, which were then ‘pruned’ by removing their tagging information. The coverage and perplexity of the two language models was calculated for this evaluation corpus. The perplexity values of the two methods were calculated for a 5,893-word vocabulary set, which was constructed by merging the text vocabularies built from the two methods.  30  EAMT 2005 Conference Proceedings  Building a conversation corpus  Table 1: Comparison between the Proposed Method and the Keyboard Chat Storage  Proposed Method Keyboard Chat Storage  Adequateness to CALLHOME  Coverage  Perplexity  84.0%  513.9  82.8%  667.0  Relative Cost # of Sentences  
 Abstract. User acceptance and system integration in existing IT environments are the key factors in machine translation (MT) becoming a universally implemented tool. Although output quality obviously plays a major role, it is not the sole decisive factor. User perception of MT, however, is an important issue, and MT needs to be seen as one of several tools in the user’s work environment. Trends, such as globalisation and the high volume of multilingual information available on the internet and within large companies, have made fast and consistent translation, and thus machine translation, indispensable in accelerating decision making. However, the logistics and workflows necessary for the successful integration of MT are complex and varied. Comprendium employs a highly flexible architecture to deal with these complex customer-specific demands. It thus advances the practical use of MT by allowing a wide range of MT workflow solutions to be implemented both in the translation "cottage industry" and in large corporate translation portals and professional translation environments. This paper gives an overview of two applications at Volkswagen in which the machine translation system has been integrated successfully with very good user acceptance. The applications are the “Volkswagen Language Portal” and the “Process Schedule” (Arbeitsplan).  1. Introduction As is the case with other large international companies, Volkswagen needs to manage a range of issues arising from an increasing volume of information. The sheer volume of different types of information at Volkswagen Customer Service® clearly underlines the urgent need for a concerted and uniform approach. In addition to owner's manuals, approx. 40,000 pages of highly diverse publications are produced annually in the Workshop Information Department of Volkswagen’s Headquarters in Wolfsburg. Workshop manuals, current flow diagrams, maintenance tables, data sheets and software for diagnostic systems also contribute to this total. EAMT 2005 Conference Proceedings  As a global player, Volkswagen sells its vehicles in more than 150 countries, which means that information management tasks and associated potential problems can become very large. Owner’s manuals, for example, are translated into twenty-seven languages, including English, French, Spanish, Chinese, Hebrew and Estonian. German is the source language of approximately 95% of all texts of this type. Just one small mistake in terminology or linguistic ambiguity in the German text can have repercussions in all the languages the text is translated into. A sound and unified approach is needed to master challenges such as 41  Bernardi et al. • significant increase in technical documentation, • greater vehicle complexity and associated technologies (e.g. electronics), • drastically reduced innovation cycles, • stricter EU legislation and other legal re- quirements, • shorter time-to-market and, • growing translation requirements. Thus, approximately 2 years ago, Volkswagen initiated a terminology management project in its Service Division. The project was launched by Volkswagen’s Language Services Department in Wolfsburg and involved translators, terminologists and technical authors from both Volkswagen and Audi. The overall objective of the project was the assurance of high quality information and texts for internal and external use. Key elements are defined, standardised and consistent terminology. This verified terminology is to be used not only by translators and interpreters, but will also form the basis of a new editing tool to be implemented by the technical authors.  Selected production and assembly locations  VW de Mexico Puebla VW do Brasil Resende Taubate Anchieta Curitiba VW Argentina Pacheco VW of SOUTH AFRICA Uitenhage  FAW-VW Changchun SVW Shanghai Crewe Emden Brüssel Dresden Mosel Molsheim Pamplona Palmela Martorell  Wolfsburg Braunschweig Hannover Salzgitter Kassel Poznan Neckarsulm Ingolstadt Kvasiny Vrchlabi Mlada Boleslav Bratislava Györ Sarajewo Sant Agata Bolognese  Figure 1. Selected VW production and assembly locations 2. Volkswagen Language Portal 2.1. Scenario Some decisions can be based on the gist of a document, such as an e-mail, without having to know the exact contents of the document. Providing an exact understanding of a text is not crucial, MT can provide a gist translation in circumstances where there is no time to wait for the services of a human translator. 42  For company brochures, workshop information, and press and marketing information, topquality translation is indispensable – and human translators will always be involved in translating texts of this nature. But even in a translator’s environment, translation tools and automated processes assist in meeting deadlines, help reduce costs and relieve the load on internal and external translators. Advanced, powerful and easy-to-use tools for internal communication within the Volkswagen Group were needed for gaining information quickly for business research, reporting and monitoring to overcome the language barriers between the company’s 340,000+ employees in production, design, research and assembly locations all over the world. Machine translation is a tool which can help accelerate everyday processes by providing the user with “raw” or “gist translations” in a split-second for sources such as e-mails, reports, websites etc. thus helping the user gain a competitive edge when it comes to further action to be taken. For standardisation and real-time availability of specific Volkswagen terminology, integration of terminological tools and large-scale terminology imports into the machine translation system proved to be indispensable. Thus, the basis for the Language Portal was formed, and Volkswagen terminology was integrated into MT engines to ensure high terminology consistency throughout the Group. Providing terminology and machine translation services on a companywide, indeed a worldwide scale, is also a question of target group oriented supply of language-relevant applications. In an ideal world each target group would need and have its own linguists and terminologists. However, this is impractical, if not unmanageable. Nevertheless, Volkswagen strives to provide all internal customers (such as technical writers, translators, interpreters etc.) with the best services the Language Portal can offer in order to guarantee customer satisfaction inside as well as outside the company gates. Volkswagen’s terminology management project has three main components: 
(3) NICT, Kyoto, Japan {isahara, kanzaki}@nict.go.jp, yukie-n@khn.nict.go.jp  Abstract. We present an overview of MedSLT, an Open Source platform for developing limited-domain medical speech translation systems. We focus in particular on the speech understanding architecture, which uses grammar-based language models derived using corpus-based specialisation methods from a single linguistically motivated grammar, and summarise the results of two evaluations which investigate the appropriateness of these design choices. Other sections describe the interlingua and its relationship with the recognition architecture, and the current demo system.  1. Introduction Medical domains are an attractive area for spoken language translation systems. They offer not only potential for interesting applications that can be of real use, but can also be sufficiently constrained to permit reasonable performance. A recent high-profile example is the hand-held Phraselator translator1, which is currently being used by the US military in Iraq. Discussions with physicians suggest that doctor/patient examination dialogues are both useful and manageable as a task, and have several advantageous properties from the point of view of building a spoken language translation system. As interactions are highly constrained, the input to be recognised is limited. Examinations can also be divided into smaller subdomains based on symptom types, for example, headaches, chest pains, gastric complaints, and so on. This gives the possibility of further constraining the range of utterances that needs to be 
In this paper we describe searchable translation memories, which allow translators to search their archives for possible translations of phrases. We describe how statistical machine translation can be used to align subsentential units in a translation memory, and rank them by their probability. We detail a data structure that allows for memory-efﬁcient storage of the index. We evaluate the accuracy of translations retrieved from a searchable translation memory built from 50,000 sentence pairs, and ﬁnd a precision of 86.6% for the top ranked translations. 
The approach developed in this paper is a contribution to METIS-II. In the EU-project METIS-II, a follow-up to METIS-I1 (Dologlou et al., 2003), the aim is to investigate the possibilities to develop a data-driven MT system using a huge monolingual target language (TL) corpus and a bilingual dictionary. While the dictionary is used to map SL items onto the TL, the corpus serves as a model to generate the TL sentences. This parallels with shake & bake (S&B) (Whitelock, 1992). In S&B the bilin- 1http://www.ilsp.gr/metis2/  gual knowledge is exhausted by the equivalence of basic expressions and TL generation is under direct control of the TL grammar. This makes large scale structural reorganisation of the TL possible and overcomes the inherent problems in conventional thirdgeneration transfer-based MT. In these latter systems, TL generation is merely a matter of traversing and printing out intermediate representations which are deﬁned by the structure of the SL text (Whitelock, 1991). The S&B approach also entails that the transfer component and the generation component can be developed and tested independently. In this paper we focus on the generation component of the system. Coherent (possibly discontinuous) sequences of words are coded in one template. We (re) generate the sentences by using template grammars and paraphrase grammars as well as a weighing procedure to rank the produced paraphrases. We compare the paraphrases that are produced with the different grammars. In the following section we outline our approach to TL generation. Then, in section 3., we show how a template-based generation grammar is induced from a large English corpus and how a paraphrase grammar is generated by using lexical and grammatical variation patterns. In section 4. we describe the paraphrase generation algorithm and how weights are assigned and trained. In section 5. we give an evaluation of the approach and show that even a small paraphrase grammar outperforms a template grammar that has a 100 times more rules. Last we outline future work on this approach.  66  EAMT 2005 Conference Proceedings  Using Template-Grammars for Shake & Bake Paraphrasing  2. Approach Shake & bake generation starts from a bag of TL items, where the order of the items in the bag is irrelevant (Whitelock, 1992). Generation freely combines the items to produce all sentences that are compatible with the constraints in the bag and in the TL grammar. According to Brew(Brew, 1992), all items in the bag are to be used in the generated target sentence. While this paper investigates whether the S&B approach can be combined with template grammars, our approach deviates from standard S&B in that: 1. a paraphrase may contain additional items which are not contained in the bag. 2. a paraphrase may be generated that does not contain all the items in the bag. 3. SL word order is marked and therefore accessible in the bag of items. This information can be considered as a preference mechanism and help score solutions, if there are more than one. 4. Grouping information may be carried over from the SL to express grouping preferences in the TL. 5. Additional constraints may be used to guide the generation process. This paper would not consider items 3, 4 and 5. See section 6. for a discussion on future investigation. Since one of the aims of METIS-II is to exploit the knowledge inferable from corpora to a maximum degree, we automatically induce a TL templategrammar from a huge monolingual corpus. The templates contain different degrees of generalisations as outlined in section 3.1.. In this way we induce many more generation rules than could ever be produced by hand. As a second step of generalisation, we generate a paraphrase grammar from the template grammar by enriching the latter with information concerning lexical and grammatical variation as described in section 3.2.. On the basis of the induced template grammar and paraphrase grammar, TL sentences are produced from the initial bag of words and scored according to the length and appropriateness of the rules and templates involved in their generation. We use learning methods to train weights of the rules in order to assign higher scores to better paraphrases. In contrast to S&B, where the free combination of items in the bag is restricted by constraints of a hand-made TL grammar, in our approach most of the constraints, in particular word-order, are left implicit in the context of the templates.  3. Generation of Shake & Bake Grammar The METIS-II project uses the British National Corpus (BNC) as a target language corpus. The BNC2 is a collection of 4,054 annotated spoken and written English texts tagged with the CLAWS tagger3. We have extracted from the BNC two sets of sequences4 : • set1 with 1,000 sequences • set2 with 100,000 sequences All sequences contain at least one ﬁnite verb and are between 5 and 15 words long. The sets have 8,555 and 863,245 words respectively. We have partially parsed the sequences and extracted a CFG-like template grammar. The template grammars for set1 and set2 are stored in grammars G1 and G2 respectively. In addition, we have generated the paraphrase grammars G1P and G2P from G1 and G2. In the remainder of this section we outline how we have parsed the sets, extracted the four grammars and generated the paraphrase grammars. 3.1. Inducing a Template Grammar Partial parsing yields a bracketed structure, as shown in the following example. The pronoun “i”, the adverb “never” and the NP “the embarrassment” are bracketed. (i)pron ’ll (never)adv forget (the (embarrassment)noun)np  We do not allow overlapping and/or ambiguous  segmentation but enable recursive bracketing. Thus,  a “noun” can be bracketed within a larger “np”  which can be part of a “pp” etc. For instance, the  bracketed noun “embarrassment” is contained in the  larger “np” (the (embarrassment)noun )np .  
Since Czech is a language with a relatively high degree of word-order freedom, and its sentences contain certain syntactic phenomena, such as discontinuous constituents (non-projective constructions), which cannot be straightforwardly handled using the annotation scheme of Penn Treebank (Marcus et al., 1993; Linguistic Data Consortium, 1999), based on phrasestructure trees, we decided to adopt for the PCEDT the dependency-based annotation scheme of the Prague Dependency Treebank – PDT (Linguistic Data Consortium, 2001; Sgall et al., 1986), which is described in Section 3. In Section 2., we describe the process of translating the Penn Treebank into Czech and reference retranslations. Section 4. presents manual tectogrammatical annotation for both languages. The automatic process of transformation of Penn Treebank annotation of English into both representations, analytical and tectogrammatical, is described in Section 5., the automatic anno-  tation of Czech is described in Section 6. Section 7. gives an overview of additional resources included in the PCEDT corpus. Section 8. mentions two experiments that have been carried out on the data collection. 2. English to Czech Translation of Penn Treebank Since the PCEDT is aimed as a resource for the purpose of MT, the translators were asked to translate each English sentence as a single Czech sentence and to avoid unnecessary stylistic changes of translated sentences. About half of the Penn Treebank has been translated so far (currently 21,628 sentences), the project aims at translating the whole Wall Street Journal part of the Penn Treebank. 2.1. English Retranslation For the purpose of quantitative evaluation methods, such as NIST or BLEU, for measuring performance of translation systems, we selected a test set of 515 sentences and had them retranslated from Czech into English by 4 different translator ofﬁces, two of them from the Czech Republic and two of them from the U.S.A. This set might be also useful for a linguistic study of the variation between multiple translations. See Figure 1 for an example of reference translations of the sentence “Kaufman & Broad, a home building company, declined to identify the institutional investors.” 3. The Prague Dependency Treebank Annotation The Prague Dependency Treebank is a manually annotated corpus of Czech. The corpus size is approx. 1.5 million words (tokens). In this section we brieﬂy summarize the annotation scheme of PDT adopted by the PCEDT.  EAMT 2005 Conference Proceedings  73  Prague Czech-English Dependency Treebank Original from PTB: Kaufman & Broad, a home building company, declined to identify the institutional investors. Czech translation: Kaufman & Broad, ﬁrma specializuj´ıc´ı se na bytovou vy´stavbu, odm´ıtla instituciona´ln´ı investory jmenovat. Reference 1: Kaufman & Broad, a company specializing in housing development, refused to give the names of their corporate investors. Reference 2: Kaufman & Broad, a ﬁrm specializing in apartment building, refused to list institutional investors. Reference 3: Kaufman & Broad, a ﬁrm specializing in housing construction, refused to name the institutional investors. Reference 4: Residential construction company Kaufman & Broad refused to name the institutional investors. Figure 1: A sample English sentence from WSJ, its Czech translation, and four reference retranslations.  Three main groups (“layers”) of annotation are used: • the morphological layer, where lemmas and tags are being annotated based on their context, • the analytical layer, which roughly corresponds to the surface syntax of the sentence, • the tectogrammatical layer, or linguistic meaning of the sentence in its context. 3.1. The Morphological Layer The annotation of Czech at the morphological layer is an unstructured classiﬁcation of the individual tokens (words and punctuation) of the utterance into morphological classes (morphological tags) and lemmas. Since Czech is a highly inﬂective language, the tagset size used is 4257, with about 1100 different tags actually appearing in the PDT. There are 13 categories used for morphological annotation of Czech: Part of speech, Detailed part of speech, Gender, Number, Case, Possessor’s Gender and Number, Person, Tense, Voice, Degree of Comparison, Negation and Variant. For English we adopted the Penn Treebank POS annotation. 3.2. The Analytical Layer At the analytical layer, two attributes are being an- notated: • (surface) sentence structure, • analytical function. A rooted dependency tree is being built for every sentence as a result of the annotation. Every item (token) from the morphological layer becomes (exactly) one node in the tree, and no nodes (except for the single “technical” root of the tree) are added. Analytical functions, despite being kept at nodes, are in fact names of the dependency relations between a dependent (child) node and its governor (parent) node. Coordination and apposition is handled using “technical” dependencies: the conjunction is the head and the members are its “dependent” nodes. Common modiﬁers of the coordinated structure are also dependents of the coordinating conjunction, but they are not marked as coordinated structure members. This additional “coordinated structure member” markup ( Co,  Ap) gives an added ﬂexibility for handling such constructions. Ellipsis is not annotated at this level (no traces, no empty nodes etc.), but a special analytical function (ExD) is used at nodes that are lacking their governor, even though they (technically) do have a governor node in the annotation. There are 24 analytical functions used, such as Sb (Subject), Obj (Object, regardless of whether the direct, indirect, etc.), Adv (Adverbial, regardless of type), Pred,Pnom (Predicate / Nominal part of a predicate for the (verbal) root of a sentence), Atr (Attribute in noun phrases), Atv, AtvV (Verbal attribute / Complement), AuxV (auxiliary verb – similarly for many other auxiliary-type words, such as prepositions (AuxP), subordinate conjunctions (AuxC), etc.), Coord, Apos (coordination/apposition “head”), Par (Parenthesis head), etc. 3.3. The Tectogrammatical Layer The tectogrammatical layer is the most elaborated, complicated, but also the most theoretically grounded layer of syntactico-semantic (or “deep syntactic”) representation. For the purposes of the annotation of PCEDT, we will sketch only the core components of the tectogrammatical annotation. The tectogrammatical layer goes beyond the surface structure of the sentence, replacing notions such as “subject” and “object” by notions like “actor” (ACT), “patient” (PAT), “addressee” (ADDR) etc., but the representation still relies upon the language structure itself rather than on world knowledge. The nodes in the tectogrammatical tree are autosemantic (content) words only. Dependencies between nodes represent the relations between the (autosemantic) words in a sentence, the dependencies are labeled by functors, which describe the dependency relations. Every sentence is thus represented as a dependency tree, the nodes of which are autosemantic words, and the (labeled) edges name the dependencies between a dependent and its governor. Coordination and apposition is handled in the same way as on the analytical level. Many nodes found at the morphological and analytical layers disappear (such as function words, prepositions, subordinate conjunctions, etc.). The information carried by the deleted nodes is not lost, of course: the relevant attributes of the autosemantic nodes they belong to now contain enough information (at least theoretically) to reconstruct them.  74  EAMT 2005 Conference Proceedings  Cmejrek et al.  Ellipsis is being resolved at this layer. Insertion of (surface-)deleted nodes is driven by the notion of valency and completeness: if a word is deemed to be used in a context in which some of its valency frames applies, then all the frame’s obligatory slots are “ﬁlled” (using regular dependency relations between nodes) by either existing nodes or by newly created nodes, and these nodes are annotated accordingly. 
1. Introduction This paper presents the current status of development and the main motivations of an opensource shallow-transfer machine translation (MT) engine for the Romance languages of Spain (the main ones being Spanish (es), Catalan (ca) and Galician1 (gl)) as part of a larger governmentfunded project which will also include MT engines for non-Romance languages such as 1Most scholars consider Galician and Portuguese (pt) the same language; however, the official orthography of Galician is very different from the ones used for European and Brazilian Portuguese. Therefore, while grammatical resources will be rather reusable, lexical resources will not easily be. EAMT 2005 Conference Proceedings  Basque (eu) and involving four universities and three linguistic technology enterprises.2 The shallow-transfer architecture will also be suitable for other pairs of closely related languages which are not Romance, for example, Czech— Slovak, Danish—Swedish, etc. The multilingual nature of Spain is recognized, to a varying extent, in laws and regulations corresponding to the various levels of go- 2TALP (Universitat Politècnica de Catalunya), SLI (Universidade de Vigo), Transducens (Universitat d’Alacant), IXA (Euskal Herriko Unibertsitatea), imaxin|software (Santiago de Compostela), Elhuyar Fundazioa (Usurbil), and Eleka Ingeniaritza Linguistikoa (Usurbil, coordinator). 79  Corbí-Bellot et al. vernment (the Constitution of Spain and the Statutes of Autonomy granted to Aragon, the Balearic Islands, Catalonia and Valencia (ca), Galicia (gl), and Navarre and the Basque Country (eu)). On the one hand, demand by many citizens in these territories make private companies increasingly interested in generating information (documentation for products and services, customer support, etc.) in languages different from Spanish. On the other hand, the various levels of government (national, autonomic, provincial, municipal) must respect, in the mentioned territories, the linguistic rights recognized to their citizens and promote the use of such languages. Machine translation is a key technology to meet these goals and demands. Existing MT programs for the es—ca and the es—gl pairs (there are no programs for the es—eu pair) are mostly commercial or use proprietary technologies, which makes them very hard to adapt to new usages, and use different technologies across language pairs, which makes it very difficult to integrate them in a single multilingual content management system. The MT architecture proposed here uses finite-state transducers for lexical processing, hidden Markov models for part-of-speech tagging, and finite-state based chunking for structural transfer, and is largely based upon that of systems already developed by the Transducens group such as interNOSTRUM3 (Spanish—Catalan, Canals-Marote et al. 2001) and Traductor Universia4 (Spanish—Portuguese, Garrido-Alenda et al. 2003); these systems are publicly accessible through the net and used on a daily basis by thousands of users. One of the main novelties of this architecture is that it will be released under an opensource license5 (together with pilot linguistic data derived from other open-source projects such as Freeling (Carreras et al. 2004) or created specially for this purpose) and will be distributed free of charge. This means that anyone having the necessary computational and linguis- 3 http://www.internostrum.com/ 4 http://traductor.universia.net/ 5 The license has still to be determined. Most likely, there will be two different licenses: one for the machine translation engine and tools, and another one for the linguistic data. 80  tic skills will be able to adapt or enhance it to produce a new MT system, even for other pairs of related languages. The whole system will be released at the beginning of 2006.6 We expect that the introduction of a unified open-source MT architecture will ease some of the mentioned problems (having different technologies for different pairs, closed-source architectures being hard to adapt to new uses, etc.). It will also help shift the current business model from a licence-centred one to a services-centred one, and favour the interchange of existing linguistic data through the use of the XML-based formats defined in this project. It has to be mentioned that this is the first time that the government of Spain funds a large project of this kind, although the adoption of open-source software by administrations in Spain is not new.7 The following sections give an overview of the architecture (sec. 2), the formats defined for the encoding of linguistic data (sec. 3), and the compilers used to convert these data into an executable form (sec. 4); finally, we give some concluding remarks (sec. 5). 2. The MT architecture The MT strategy used in the system has already been described in detail (Canals-Marote et al. 2001; Garrido-Alenda et al. 2003); a sketch will be given here. The engine is a classical shallowtransfer or transformer system consisting of an 8-module assembly line; we have found that this strategy is sufficient to achieve a reasonable translation quality between related languages such as es, ca or gl. While, for these languages, a rudimentary word-for-word MT model may give an adequate translation for 75% of the text, the addition of homograph dis- 6 Other attempts at open-source implementations of MT systems have been initiated, such as GPLTrans (http://www.translator.cx) and Traduki (http://traduki.sourceforge.net), but the level of activity in these projects is low and far from reaching the usability levels of the existing interNOSTRUM—Traductor Universia engine inspiring the one in this project. 7 The most remarkable case being the success of Linex (http://www.linex.org) , the Linux distribution promoted by the autonomous government of Extremadura. EAMT 2005 Conference Proceedings  An open-source shallow-transfer machine translation engine for the Romance languages of Spain  ambiguation, management of contiguous multiword units, and local reordering and agreement rules may raise the fraction of adequately translated text above 90%. This is the approach used in the engine presented here. To ease diagnosis and independent testing, modules communicate between them using text streams8 (examples below give an idea of the communication format used). This allows for some of the modules to be used in isolation, independently from the rest of the MT system, for other natural-language processing tasks. As in interNOSTRUM or Traductor Universia, implementations of the systems for Linux and Windows architectures will be made available. The modules are shown in figure 1. Most of the modules are capable of processing tens of thousands of words per second on current desktop workstations; only the structural transfer module lags behind at several thousands of words per second. As has been mentioned in the introduction, in addition to this shallow-transfer MT architecture, the project is also designing a deepertransfer architecture for the es—eu pair (Díaz de Ilarraza et al. 2000) . Even though the current prototype is being programmed in an object-oriented framework using code and data from Freeling (Carreras et al. 2004) for es and  conventional syntactical and lexical generation for eu, its architecture could easily be rewritten into one which would share modules and format specifications with the shallow-transfer architecture described here (for instance, morphological analysis and generation, part-of-speech tagging, or lexical transfer). The following sections describe each module of the shallow-transfer architecture in detail. 2.1. The de-formatter The de-formatter separates the text to be translated from the format information (RTF, HTML, etc.). Format information is encapsulated so that the rest of the modules treat it as blanks between words. For example, the HTML text in Spanish: vi <em>una señal</em> (“I saw a signal”) would be processed by the de-formatter so that it would encapsulate the HTML tags between brackets and deliver vi[ <em>]una señal[</em>] The character sequences in brackets are treated as simple blanks between words by the rest of the modules.  Source text →  De-formatter  ↓  Morphological Analyser  
Although Machine Translation (MT) has advanced recently for language pairs with large amounts of parallel data, translation quality has not yet reached satisfactory levels, especially not for resource-poor languages with little if any parallel text to train statistical or examplebased MT systems. Examples of resource poor languages are Quechua and Mapudungun, which contrast with languages that have more economic, and therefore also electronic, resources, such as Spanish and English. Rule-based transfer MT systems are the only feasible solution for resource-poor scenarios. Developing and expanding such systems manually can, however, prove very costly and time consuming. On the other hand, finding trained computational linguists with knowledge of resource-poor languages is a real challenge. Moreover, if the translation rules are written manually, no matter how many rules there already are, coverage and accuracy can always be increased. If they are automatically learned, they might be either too general or too specific. In both cases, the translation rules can be refined to account for new data. The goal of our re-  search is to generalize post-editing efforts in an effective way, by identifying and correcting rules semi-automatically in order to improve coverage and overall translation quality. In this paper, we introduce a novel approach that proposes an MT module for automatically refining translation rules based on the feedback provided by bilingual speakers. There are two main challenges in this approach. First, the elicitation of accurate correction information from non-expert bilingual speakers. Second, the automatic refinement of existing translation rules, given a corrected and word-aligned translation pair, and information about the MT errors. The approach described in this paper automatically determines the appropriate rule refinement operations that need to be applied to a grammar and a lexicon in order for the system to output the correct translation, as given by the native speaker. The resulting refinements and extensions can therefore apply not only to the translation instance corrected by the user, but also to other similar cases where the same error would be encountered.  EAMT 2005 Conference Proceedings  87  Font Llitjós, et al. 2. Related Work 2.1. On Post-editing to Improve MT Post-editing has often been defined as the correction of MT output by human linguists or editors. In the case of native and minority languages on which we are working, the editors are actually bilingual speakers with no expertise in linguistics or translation, and their goal is to evaluate and minimally correct MT output, in a way that is similar to what has been referred to as minimal post-editing in the literature (Allen, 2003). The minimal correction method we are proposing for the task of rule refinement involves grammar correctness and fluency, in addition to meaning preservation. Stylistic changes are not considered minimal post-editing. Some researchers have looked at ways of including user feedback in the MT loop. Su et al. (1995) have explored the possibility of using feedback for a corpus-based MT system to adjust the system parameters so that the user style could be respected in the translation output. They proposed that the distance between the translation output of the system and the translation preferred by the user should be proportional to the amount of adjustment to the parameters involved in the score evaluation function, and should be minimized over time. We could not find, however, any papers reporting testing of these ideas. In the case of languages with limited data, such a system is not feasible, though, since there is not enough data to estimate and train system parameters. Moreover, we are interested in improving the translation rules themselves, which in the case of automatically learned grammars typically lack some of the feature constraints required for the correct application of the rule, rather than just tweaking the evaluation parameters, which in their system are conditional probabilities and their weights. Menezes and Richardson (2001) and Imamura et al. (2003) have proposed the use of reference translations to “clean” incorrect or redundant rules after automatic acquisition. The method of Imamura et al. consists of selecting or removing translation rules to increase the BLEU score of an evaluation corpus. In contrast  to filtering out incorrect or redundant rules, we propose to actually refine the translation rules themselves, by editing valid but inaccurate rules that might be lacking a constraint, for example. 2.2. On Rule Refinement The idea of rule adaptation to correct or expand an initial set of rules is an appealing one and researchers have indeed looked at rule adaptation for several natural language processing applications. Lin et al. (1994) report research on automatically refining models to decrease the error rate of part-of-speech tagging. Brill (1993) introduced a new technique for parsing free text: a transformational grammar is automatically learned that is capable of accurately parsing text into binary-branching syntactic trees with non-terminals unlabeled. The system learns a set of simple structural transformations that can be applied to reduce error. Brill's method can be used to obtain high parsing accuracy with a very small training set. Although small, the learning algorithm does need the training corpus to be partially bracketed and annotated with part-of-speech information, which is a scarce resource for minority languages. Even if we had such a small initial annotated corpus, transforming translation rules is nontrivial and cannot be done with simple patterns like the ones proposed in Brill's method. The rule refinement algorithm proposed here needs to deal with the lexicon, the syntax and the feature constraints in the rules. Corston-Oliver and Gamon (2003) learned linguistic representations for the target language with transformation-based learning (Brill style) and used decision trees to correct binary features describing a node in the logical form to reduce noise. Yamada et al. (1995) use structural comparison (parse tree) between machine translations and manual translations in a bilingual corpus to adapt a rule-based MT system to different domains. In order for this method to work, though, a parser for the target language (TL) needs to be readily available, which is typically not the case for resource-poor languages. Moreover, such a parser must have coverage for the manually corrected output as well as the incorrect MT output to compute the differences. The actual  88  EAMT 2005 Conference Proceedings  A framework for interactive and automatic refinement of transfer-based machine translation  adaptation technique is not described in the paper. In sum, even though adaptation has been researched for MT and other natural language processing applications before, to this day, work so far has not attempted to refine the translation rules themselves, and thus the framework described in this paper constitutes an interesting and novel approach to automatically refine and expand MT systems. 3. Automating the Post-editing process Current solutions to improve MT output are limited to manually correcting the output and, in the best-case scenario, to some post-processing to alleviate the tedious task of manual postediting by correcting the most frequent errors beforehand (Allen & Hogan, 2000). Currently, there exists no solution to fully automating the post-editing process. There are at least two different approaches one could take in order to do that. First, one could try to learn post-editing rules automatically from concrete corrections, which has the advantage of being system independent. With this approach, however, one cannot generalize over specific corrections to correct the same structural error with a different word, say; furthermore, several thousands of sentences would need to be corrected for the same error. Alternatively, we could go to the root of the problem, and try correcting the source of the error by refining existing translation rules automatically. This way, by just fixing one or two translation rules, we can avoid the generation of a structural error that would otherwise creep in thousands of sentences. This approach naturally requires access to translation rules that can be refined. Therefore, the approach proposed by this framework is to attack the core of the problem and refine the incorrect translation rules themselves guided by user corrections. In other words, we propose to automate post-editing efforts by recycling these corrections back into the MT system.  4. Elicitation of Translation Correction Information Even in resource-poor contexts, there is usually at least one resource available, namely, bilingual speakers. Our approach exploits this fact and relies on non-expert bilingual users to extract as much accurate information as possible to determine error location and cause, which can then be used by the Rule Refinement (RR) module. In order to elicit MT error information from naïve speakers reliably, we designed and implemented a graphic user interface, called the Translation Correction Tool (TCTool), that is intuitive and easy to use and that does not assume any knowledge about translation or linguistics. For details on how the TCTool works to elicit translation error information, please refer to Font-Llitjós and Carbonell (2004). A set of English-Spanish user studies showed that bilingual speakers with no linguistics or translation skills, are able to use the TCTool to evaluate and correct MT output with 90% and 72% accuracy, respectively (Font-Llitjós and Carbonell 2004). Given the evidence that non-expert bilingual speaker judgments and corrections of MT output are reliable, automating a rule refinement mechanism based on this information becomes an option.1 5. MT Error typology As part of the initial research mostly based on English to Spanish translation, a preliminary MT error typology was defined, and it is shown in a simplified form in Figure 1. Figure 1: Initial MT Error Typology 
The paper is organized as follows. First, section 2 describes pre-processing and natural align-  EAMT 2005 Conference Proceedings  97  Gamallo Otero ment. Then, section 3 introduce the measure used to compute translation correlations between expression types in the two languages. Section 4 will define the notion of sense-sensitive context. In section 5, we will present the extraction algorithm. And finally, in section 6, a evaluation protocol will be outlined. 2. Pre-processing the Corpus First, the texts of both languages are tokenized, lemmatized and tagged using TreeTagger (Schmid 2002). No manual correction was made on the tagged texts. So, the bilingual lexicon extractor will inherit errors caused by the tagger. Then, the texts are superficially parsed by simple pattern matching, where the objective is to extract sense-sensitive contexts of words. In section 4, we will explain the notion of sense-sensitive context. The following pre-processing step is to align the source and target texts by detecting natural boundaries such as chapters, specific documents, articles, etc. Our experiments were made on a corpus constituted by the English and French versions of the European Legislation in Force. The natural boundaries used to divide this corpus are the beginning and the end of legal documents such as agreements, directives, and regulations. We detected 1,050 English legal documents and their corresponding French translations. Each pair of legal document constituted by the English source and its translation is considered as an aligned segment. The main drawback of this type of alignment is that it only allows to select very large segments. Two advantages, however, deserve to be mentioned. First, deletions and additions found in some parts of the source texts do not prevent the correct alignment of the whole corpus. And second, this alignment does not need manual correction. 3. A Particular Version of the Dice Coefficient We estimate the probability that a candidate target expression is a translation by counting both occurrences of expressions in the corpus as a whole and co-occurrences of the expressions within pairs of aligned segments. Following (Smadja and McKeown, 1996), we selected the Dice coefficient to measure translation correlations. Given a source expression type e1 and a candidate translation e2, our par- 98  ticular version of the Dice coefficient is defined as follows:  Dice(e1,  e2  )  =  2*F F (e1 )  (e1 , +F  e2 ) (e2 )  where  ∑ F (e1, e2 ) = min( f (e1, si ), f (e2 , si )) i and  ∑ F (en ) = f (en , si ) i Note that f (en , si ) , represents the frequency of the expression type e1 occurring in segment si. Unlike most approaches to bilingual lexicon extraction, we consider that the frequency of an expression in a particular segment carries a very significant information. As the segments we use to align the corpus are longer as those used for sentence alignment, then, the same word can occur several times in the same segment. So, an expression type of the target language, e2, is likely to be a translation of a source expression, e1, if both expression tend to have a similar frequency in each segment si. This is an important difference with regard to standard approaches. In most approaches, two expressions are linked if they tend to appear in the same aligned segments. However, as in our approach many different expressions can appear in all segments, we need a more informative feature, namely the number of times an expression appears in each segment.  4. Sense-sensitive Contexts The main contribution of this paper is to use sensesensitive contexts to extract word translations. Consider the following two expressions: 1. vehicle registration 2. registration of the notification Expression (1) is translated into French as “immatriculation du véhicule”, whereas (2) is translated as “enregistrement of the notification”. In (1), the modifier “vehicle” behaves as a sense-sensitive context that selects a specific sense of “registration”: a part of a vehicle. Note that this sense is slightly different from that selected in the context introduced by “of the notification'' in expression (2), where “registration” refers to a specific action. The two sense-sensitive contexts of “registration'” in expressions (1) and (2) are:  EAMT 2005 Conference Proceedings  Extraction of translation equivalents from parallel corpora using sense-sensitive contexts  <vehicle [NOUN]> <[NOUN] of the notification> These contexts seem to be useful to distinguish particular word senses. Note that a sense-sensitive context can be one of the two positions underlying any Head-Modifier dependency. Given a binary dependency, for instance: of (registration, notification) 
• system-internal evaluations in the practical deployment of MT systems There is a rapidly-growing body of literature on automatic metrics for the first scenario, starting with the BLEU metric (Papineni et al. 2002) and the NIST metric (Doddington 2002). Crucially, the BLEU metric and its relatives (e.g. Lin and Och 2004b, Babych and Hartley 2004, Soricut and Brill 2004) rely on one or more human reference translations for each machine-translated sentence to be evaluated. These metrics are all based on the idea that the more shared substrings the machine-translated sentence has with the human reference translation(s), the better the translation is. It has been shown that BLEU scores, despite their shortcomings, correlate surprisingly well with human judgment (Coughlin 2003). The BLEU metric and its relatives are typically computed for fixed multi-sentence test sets in order to track the performance of MT systems over time and to compare different MT systems with respect to these test sets.  Moving from multi-sentence evaluation to single-sentence and even word-level evaluation, Blatz et al. (2004) survey a number of approaches to the estimation of confidence. The training data for their experiments consist of machine translations and human reference translations. This work uses naive Bayes and multilayer perceptrons for classification. For the system-internal evaluation of deployed MT systems, Quirk (2004) uses a small (350 sentence) corpus of machine translations that have been annotated for translation quality by human annotators. He represents translated sentences as feature vectors, training a classifier to emulate the human scoring. The features he uses include sentence perplexity according to a trigram language model (LM) of a training corpus, source sentence features such as length, translation features such as number and size of mappings, whether a translation comes from a learned mapping or from a dictionary, and tilings of source and translated sentences with respect to the training corpus. Quirk demonstrates that this approach can produce usable results for sentence-level confidence. There are also several attempts at using machine-learned classifiers for the purpose of MT quality assessment both at the sentence level and for larger test sets. Corston-Oliver et al (2001) demonstrated that classifiers can distinguish quite  EAMT 2005 Conference Proceedings  103  Gamon et al. reliably at the sentence level between machine translations and human translations. Classification accuracy increases if linguistic features are added to purely perplexity-based features. Kulesza and Shieber (2004), in a similar approach, train a Support Vector Machine (SVM) classifier that is capable of reliably distinguishing machine translation output from human translations. They use a combination of features derived from n-gram precision, length, and word error rate with respect to human reference translations. Additionally, a confidence score produced by the classifier shows high correlation with human judgment. To summarize, there are a number of automatic translation quality metrics both at the multi-sentence and single-sentence level. Most of these metrics (except Quirk 2004) require one or more human reference translations for each sentence to be evaluated. This is a reasonable requirement when the task is system comparison or tracking of system performance over time. The use of MT systems in a production environment, however, requires the evaluation of massive amounts of machine-translated text. In practice, it is necessary to be able to assess the quality of all MT output in order to identify the particularly badly-translated sentences. In cases where MT is employed for the dissemination of large amounts of text (see e.g. Richardson 2004), two of the roles of the human translator are to identify systematic translation errors and to perform post-editing of low-quality MT output. In this scenario, a tool for detecting badly-translated sentences automatically and reliably is essential. It saves time and allows the translator to concentrate on the problematic sentences. By definition, in MT for dissemination there is no human reference translation. The challenge, then, is to find a reasonable automatic evaluation metric for sentence-level translation quality that does not require human reference translations. Translation quality involves both content and form. An ideal translation needs to capture the meaning of the source sentence and express it in a fluent target language sentence. In practice, automatic assessment of semantic adequacy is a much harder problem than evaluation of the fluency of a sentence. As we will discuss below, fluency assessment can serve as a proxy for over-  all translation quality as long as it correlates well enough with overall translation quality. One readily-available solution for the evaluation of MT output fluency at the sentence level that does not require reference translations is the ngram language model (LM). LMs can be trained on a domain-specific corpus in the target language. A perplexity score can be calculated for each machine-translated sentence reflecting the degree to which the observed word sequence is “expected” compared to what has been observed in the training corpus. This approach has been used successfully to score output from different MT engines in multi-engine MT systems (Callison-Burch et al 2001, Akiba et al 2002, Nomoto 2003). In this paper we attempt to improve on a sentence-level language model perplexity score by adding other sources of information about the fluency of the translation. The resources that our approach requires are: i. a set of machine-translated sentences ii. a corpus of target-language text from the same domain (but crucially not translations of the same source sentences that were used in (i)) iii. an automatic linguistic analysis system (parser) By combining perplexity scores with scores provided by a classifier that is trained to distinguish machine-translated sentences from human translations, we are able to improve on the correlation between human judgments and perplexity scores alone. This classifier uses linguistic analysis features to complement the ngram-based language model. We also show that this system performs well when tasked with identifying the worst (i.e. most dysfluent) translations. All experiments were performed using an examplebased machine translation system to translate technical documentation from English into French (Smets et al. 2003). 2. Experimental Setup 2.1. Data We used a corpus of 1,566,265 French sentences from Microsoft technical documentation to train our language models.  104  EAMT 2005 Conference Proceedings  Sentence-level MT evaluation without reference translations: Beyond language modeling  Our training data for the SVM classifiers consists of 198,771 machine-translated sentences (English to French) from the Microsoft Product Support Services Knowledge Base (Richardson 2004), and 260,601 human-translated sentences from the same domain. Language models and SVMs were then tested on a set of 500 held-out sentences which had been annotated by human annotators for both MT quality and fluency. The annotation consisted of separate scores on a scale of 1 to 4, where 1 means completely dysfluent or incomprehensible, and 4 means perfectly fluent or humanquality translation. For fluency annotation, the raters only took the target sentence into account. There were 6 raters for MT quality and 1 rater for fluency. The fluency rating was done independently of the MT quality evaluation to ensure that knowledge of the source sentence was not influencing the fluency evaluation. The distribution of fluency and MT quality scores as assigned by the human evaluators to the test set are shown in Figure 1.  Distribution of scores in the test set  number of sentences  250  200 150  100 50  0  <=1  <=2  <=3  <=4  s cor e  Fluency scores MT quality scores  Figure 1: distribution of scores on the test set  2.2. Language Model The French target language model was trained on all available data from the target domain. The data was preprocessed by converting all tokens to lower case, removing contractions (i.e. converting “d’” and “l’” to de and le, respectively), and removing punctuation. Then a 4-gram language model was built using interpolated Kne-  ser-Ney smoothing. Kneser-Ney smoothing has been shown to outperform all other techniques for smoothing n-gram language models. (Kneser & Ney 1995, Goodman 2000). Per-sentence scores were computed by preprocessing the test data in the same manner as the training data, then computing cross-perplexity with the language model in the usual way:  k  ∑ −  
112  The paper is organized as follows: In the second section, we will look into the heterogeneous nature of prepositions and discuss some of its implications on the translation process. In the third section, we will briefly review some previous experiments on related tasks; we will specifically consider whether they have involved the use of aligned bilingual data or not. The fourth section will outline and motivate the main features of the current approach. In the fifth section, transformation-based learning will be introduced. The sixth section presents the actual experiment: the data and tools, the parameter settings and the choice of templates. Section seven is devoted to a presentation of the results. In the final section, some concluding remarks will be given. 2. How Prepositions Translate Linguists often distinguish two types of prepositional uses; their functional use and their lexical use.1 In its functional use, a preposition is governed by some other word, most often by a verb as in example 1, but sometimes by an adjective (afraid of), or a noun (belief in). 1. I believe in magic. 
en qué tipo de trabajo estás interesado ? what kind of job are you interested in ? en qué tipo de cosas estás interesado ? what kind of things are you interested in ? en qué tipo de excursiones estás interesado ? what kind of tour are you interested in ? Figure 1: Similar patterns in sentences These three sentences differ only in one word in both Spanish sentences as well as their English translations. For a given test sentence, we often 126  find in the training corpus, a very similar sentence with few mismatching words; sometimes even an exact matching sentence. Translation memory (TM) systems typically work well in these situations. In its pure form, a TM system is simply a database of past translations, stored as sentence pairs in source and target languages. Whenever an exact match is found for a new sentence to be translated, the desired translation is extracted from the translation memory. TM systems have been successfully used in Computer Aided Translations (CAT) as a tool for human translators. There have been attempts to combine translation memory with other machine translation approaches. In (Marcu, 2001) an automatically derived TM is used along with a statistical model to obtain translations of higher probability than those found using only a statistical model. Sumita (2001) describes an example-based technique which extracts similar translations and modifies them using a bilingual dictionary. Watanabe and Sumita (2003) proposed an example-based decoder that start with close matching example translations, and then modify them using a greedy search algorithm. Instead of extracting complete sentences from the TM, Langlais and Simard, (2002) work on sub sentential le- EAMT 2005 Conference Proceedings  Augmenting a statistical translation system with a translation memory  vel. Translations for word sequences are extracted from a TM and then fed into a statistical engine to generate the desired translation. In this paper, we present an experiment where we attempted to augment a statistical translation system with a translation memory. For a sentence which has a close match in the training corpus, the idea is to start with the available translation and apply specific modifications to produce the desired translation. By a close match, we mean a very similar sentence with only a few mismatching words. Given a test sentence, we extract sentence pairs from the bilingual training corpus, whose source side is similar to the test sentence. If a close matching sentence is found, we use our TM system to translate it. For each mismatching word in the source side of the close matching pair, we identify its translation in the target side. Then a sequence of substitution, deletion and insertion operations is applied to the target side to produce the correct translation. If a close match is not found in the training corpus, we use a statistical translation system to generate the translation. The system was evaluated using a subset of the Chinese-English BTEC corpus. For those close matching sentences, the translations produced by the TM system were compared with the translations produced by the statistical decoder. In our current experiments TM system did not show an improvement in terms of automatic evaluation metrics. However, a subjective human evaluation found that, in several instances, the TM system produced better translations than the statistical decoder. In the following section we explain the TM system in detail. We also describe the phrase extraction method we used to identify alignments between source words and target words, which is a modified version of the IBM1 alignment model (Brown et al. 1993). In Section 3, we present the experimental setting and the results of the evaluation. It is followed by a discussion in section 4, and conclusions in section 5. We have identified a number of improvements to the current system, some of which are already in progress.  2. Translation Memory System 2.1. Extracting Similar Sentences For each new test sentence F, we find a set of similar source sentences {F1, F2, …} from the training corpus. The similarity is measured in terms of the standard edit distance criterion with equal penalties for insertion, deletion and substitution operations. The corresponding set of translations {E1, E2,…} is also extracted from the bilingual training corpus. Following are some close matching sentences we extracted for the Spanish sentence estoy nerviosa. i. estoy resfriado (i have a cold) ii. estoy cansada (i am tired) iii. estoy resfriado (i feel chilled) If we select the first match as input to the TM system, it will generate the translation, i have nervous. If instead we select the second match, we get, I am nervous, which is the correct translation. Selecting the first best does not always produce better results. Therefore, for each test sentence, we select the 10 best matching sentence pairs as candidates for the next step. If we found an exact match among the extracted sentences, we terminate the search and output the translation of it as the desired translation of the test sentence. In the case of multiple exact matches (which might have different meanings in the target side), we score each sentence pair (Fk, Ek) using a translation model and a language model and select the best one. 2.2. Modifying Translations of Close Matching Sentences If an exact match is not found, but a close matching sentence pair (Fk, Ek) is found, then the translation Ek is slightly altered using a statistical translation model to produce the correct result. We start by identifying the words in Fk that have to be changed and the sequence of substitution, deletion, or insertion operations1 required to make it the same as F. For each of these words, we then identify its alignment in the target side Ek. Finally, the aligned words are modi-  
The goal of this research is to improve the translation performance for a Statistical Machine Translation system. The basic approach is to adapt the translation models. Statistical machine translation can be described in a more formal way as follows:  t* = argmaxP(t | s) = argmaxP(s | t) ⋅ P(t)  t  t  Here t is the target sentence, and s is the source  sentence. P(t) is the target language model and  P(s|t) is the translation model used in the de-  coder. Statistical machine translation searches  for the best target sentence from the space de-  fined by the target language model (LM) and  the translation model (TM).  Statistical translation models are usually ei-  ther phrase- or word-based and include most  
and rarely have the coverage of corpus-based systems. With such a wide range of approaches to machine translation, it would be beneficial to have an effective framework for combining these systems into an MT system that carries many of the advantages of the individual systems and suffers from few of their disadvantages. Attempts at combining outputs from different systems have proved useful in other areas of language technologies, such as the ROVER approach for speech recognition (Fiscus 1997). Several different multi-engine machine translation (MEMT) systems have also been explored in the past ten years, starting with the Pangloss system in 1994 (Frederking and Nirenburg). Several of these systems require significant coupling between the systems in the form of shared lattice structures (Frederking et al 1997; Tidhar & Küssner 2000; Lavie, Probst et al. 2004). Beyond the difficulty  EAMT 2005 Conference Proceedings  143  Jayaraman and Lavie of obtaining compatible lattice representations of the various input systems, these approaches require standardizing confidence scores that come from the individual engines. Another proposed MEMT approach uses string alignment between the different translations and trains a finite state machine to produce a consensus translation (Bangalore et al. 2001). The alignment algorithm described in that work is the standard Levenshtein edit distance which only allows insertions, deletions and substitutions. This model does not accurately capture phrase movement like: In the street, the children cried. The children cried in the street. The standard Levenshtein distance would create an alignment by first deleting the words “The children cried” in the second sentence and inserting them at the end of the sentence. By creating an alignment this way, the fact that the phrase “the children cried” occured in both sentences is lost. In this paper, we propose a new way of combining the translations of multiple MT systems based on a more versatile word alignment algorithm. A “decoding” algorithm then uses these alignments, in conjunction with confidence estimates for the various engines and a trigram language model, in order to score and rank a collection of sentence hypotheses that are synthetic combinations of words from the various original engines. The highest scoring sentence hypothesis is selected as the final output of our system. We experimentally tested the new approach by combining translations obtained from three Chinese-to-English online translation systems. Translation quality is scored using the METEOR MT evaluation metric (Lavie, Sagae et al 2004; Lavie and Banerjee, 2005). Our experiments demonstrate that our new multi-engine combination system achieves an improvement of about 6% over the best original system, and is about equal in translation quality to an “oracle” capable of selecting the best of the original systems on a sentence-by-sentence basis. A second oracle experiment shows that our new approach produces synthetic combination sentence hypotheses that are even far superior to the hypotheses currently selected by the system, but our current scoring is not yet capable of adequately identifying the best combination.  The remainder of this paper is organized as follows. In section 2 we describe the algorithm for generating hypothesis sentence translations. Section 3 describes the experimental setup used to evaluate our new approach, and section 4 presents the results of our evaluation. Our conclusions and future work are presented in section 5. 2. The MEMT Algorithm Our Multi-Engine Machine Translation (MEMT) system operates on the single “top-best” translation output produced by each of several MT systems operating on a common input sentence. MEMT first aligns the words of the output strings produced by different translation systems using a word matching sub-module. Then, using the alignments provided by the matcher, the system generates a set of synthetic sentence hypothesis translations. Each hypothesis translation is assigned a score based on the alignment information, the confidence of the individual systems, and a language model trained on a large target language corpus. The hypothesis translation with the best score is selected as the final output of the MEMT combination. 2.1. The Word Alignment Matcher The task of the matcher is to produce a word-toword alignment matching between the words of two given input strings. Identical words, ignoring case, that appear in both input sentences are potential matches. Since the same word may appear multiple times in the sentence, there are multiple ways to produce an alignment between the two input strings. This phenomenon is especially common with function words. The goal is to find the alignment that represents the best correspondence between the strings. This alignment is defined as the alignment that has the smallest number of “crossing edges”, when a line is drawn between the two matched words in the two sentences. For example, let us consider the alignment between the following two sentences: The boy walked the dog A child walked the dog Since “walked” and “dog” occur only once in each sentence, their alignments are fixed. The word “the” appears twice in the first sentence  144  EAMT 2005 Conference Proceedings 
Incorrect alignments result in missing or extraneous words in the translation hypothesized for a particular fragment of the input. This can 
 working exclusively from French to English, the native language of most of our students, it places more emphasis on precise interpretation of the source text. The students have typically had several years of French in secondary school, so that by third year in our program their French is quite advanced. Our courses immerse them in French, whether the content is literature, linguistics, culture/civilization, or language-based. Yet, the course where the experiment was carried out is the first time most are exposed extensively to journalistic texts and in spite of their extensive study of literature, they find journalistic translation quite a challenge, thanks largely to the abundant cryptic references, the cultural subtext, and the often opaque figurative language. Of course, this is not to downplay target text  
Some of these systems are based on 1st generation ‘word-for-word’ direct translation technology, and share a number of characteristics: (i) they are designed to translate wide-coverage, general language material; (ii) they are robust; and (iii) they perform comparatively limited linguistic analysis. These points are interrelated and bear further elaboration. Detailed automatic linguistic analysis of translation input is potentially costly (both in terms of processing time and required lingware such as computational grammars etc.), and in the past has often been inversely related to coverage and robustness. In other words, the more detailed the linguistic analysis, the smaller the coverage of the system, and conversely, the wider the coverage, the less detailed the linguistic analysis. This has led commercial, wide-coverage MT systems to concentrate on ‘linguistics-lite’, robust design principles. In order to analyse translation input, they often consider only a limited linguistic context.  A consequence of this is that existing commercial systems are much stronger when translating shorter sentences than they are on longer, more complex input. The reason behind this is simple: the longer the input sentence to be translated, the more likely that the automatic translation system will be led astray by the complexities in the source and target languages. We contend that better performance in terms of output quality can be achieved than these systems can obtain by processing the texts that they are required to translate at any one time into smaller chunks. Consider the example in (1): (1) The chairman, a long-time rival of Bill Gates, likes fast and confidential deals A reasonable translation into German (established by a human translator) is: (2) Der Vorsitzende, ein langfristiger Rivale von Bill Gates, mag schnelle und vertrauliche Abkommen. However, the translation produced by the BabelFish MT system is (3): (3) Der Vorsitzende, ein langfristiger Rivale von Bill Gates, Gleiche fasten und vertrauliche Abkommen.  EAMT 2005 Conference Proceedings  189  Mellebeek, et al. This involves a significant distortion of the input proposition, almost to the point of rendering it unrecognisable. The problem is that the English verb likes is mistranslated as a noun (Gleiche) and the adjective fast is completely misrecognised as a verb fasten (‘to fast’) . Contrast what happens if you feed BabelFish the shorter sentences in (4): (4) a. The chairman likes deals Æ Der Vorsitzende mag Abkommen b. The chairman likes fast deals Æ Der Vorsitzende mag schnelle Abkommen Both German strings in (4) are perfectly acceptable translations of the English input and constitute no errors. This small set of translation examples is indicative of a general trend: that commercially available, wide-coverage MT systems tend to be much better at translating short and simple input. They perform much worse on longer strings, as the extra context provided gives ample opportunity for mistakes to be made. In this paper, we present our method which takes long input strings from the Penn-II Treebank and breaks them down recursively into smaller and simpler constituents, and translates those shorter parts individually. At the same time, we keep track of where those individual parts fit into the overall translation in order to stitch together the translation result for the entire input string. It is important to note that throughout the process, the MT engine itself does all the translation: we are essentially helping the system work to the best of its ability so as to generate better translations than would otherwise have been produced to the benefit of the end user. We use SYSTRAN1 because of its widespread use in the industry and Logomedia2 since it was deemed the better of the three onine MT systems tested in (Way & Gough, 2003). Accordingly, we have set ourselves a rather challenging task: we anticipate that the poorer the MT engine, the larger the increase in translation quality to be seen from incorporating the method described here. The remainder of this paper is organised as follows: in section 2, we provide details of re- 
We review the techniques and tools used for regression testing, the primary quality assurance measure, in a multi-site research project working towards a high-quality Norwegian – English MT demonstrator. A combination of hand-constructed test suites, domain-speciﬁc corpora, specialized software tools, and somewhat rigid release procedures is used for semi-automated diagnostic and regression evaluation. Based on project-internal experience so far, we comment on a range of methodological aspects and desiderata for systematic evaluation in MT development and show analogies to evaluation work in other NLP tasks.  
 1. Introduction Ambiguous words pose a very serious problem to existing machine translation systems because in many situations the translation engines do not know how to handle these words. This problem can be particularly serious for organisations which heavily rely on machine translation for their everyday operation. The work presented in this paper is part of a larger project to develop technologies which will enable people and organisations to improve communications by removing language barriers.1 The technology is based on automatically redirecting e-mails, web pages and electronic documents to a centrally based translation facility termed Translution Central. Users simply write emails in their own language in the normal way, press the Send button, and the recipients will receive the email automatically translated into their own language. Similarly, incoming emails will be translated into the user’s own language. The initial release of the product will support five European languages: English, French, 
The meaning of a complex linguistic structure is wholly determined by its sub-structures and the meanings of them. In the Rosetta machine translation system (Landsbergen 1985) we can meet a rather direct application of the compositionality principle: „The meaning of an expression is a function of the meaning of its parts and the way in which they are syntactically combined. This principle was adopted from Montague Grammar (Thomason 1974). Obviously, this principle will lead to an organization of the syntax that is strongly influenced by semantic considerations. But as it is an important criterion of a correct translation that it is meaningpreserving, this seems to be a useful guideline in machine translation.” (Appelo et al 1987) Semantic compositionality was formalized by the rule-to-rule hypothesis of Bach (1976): it says that a tight correspondence is imposed between syntax and semantics such that every rule of syntax is also a rule of semantics. 
1. Introduction Statistical machine translation, like other natural language process tasks, has developed a set of unique evaluation metrics that go beyond simply evaluating the number of sentence errors that a system makes on a test set. While debates continue regarding the relative value of each competing metric, the BLEU [Papeneni, 2001] and NIST [Doddington, 2002] scores (which consider system performance at the corpus level) have shown their effectiveness in driving the development of statistical machine translation systems. These metrics have highlighted the need for more expressive models of translation and a framework to introduce additional knowledge sources within the translation process. The direction translation [Och, 2002] approach [Brown et al, 1993] delivers this framework, and provides a necessary formalism to the process of combining and optimizing additional knowledge sources. Discriminative training considers competing candidate translations from an N-Best list is used to find appropriate scaling factors for each additional knowledge source. The goal is to find scaling factors that improve the metric performance of the candidate translation chosen by the decoder [Vogel, 2003]. In this paper, we EAMT 2005 Conference Proceedings  consider these scaling factors within the decoding process rather than as a post processing reranking step, thereby creating additional considerations regarding the stability of the scaling factors. The choice of evaluation metric, the nature of the additional knowledge sources within the decoding process and the implementation decisions taken in each component, determine the effectiveness of each discriminative method. This paper will focus on comparing the formalism and practical considerations involved with deploying Maximum Mutual Information [Bassat,1982] and Minimum Classification Error [Huang, Katagiri, 1992] training within a statistical machine translation context. We begin by framing the discriminative training task for statistical machine translation and survey directions of active research in the field. We discuss the impact that the corpus level BLEU score has on the discriminative training criteria and the implementation requirements for optimization methods that accommodate for such metrics. We describe the process of generating N-Best lists from a Viterbi decoding using partial and full translation based knowledge sources and then merging these lists across iterations along with experimental results on widely distributed training and test data sets. We conclude with a dis- 271  Venugopal and Vogel  cussion of future work and potentially promising directions in discriminative training.  2. Direct Statistical Machine Translation  Statistical machine translation presents the task of finding a target language (“English”) sequence of word tokens e = e1…eS that is the translation for a source language (“French”) sequence f = f1…fT. A zero-one loss function would suggest that a decision rule that selects the English sentence that has the highest conditional probability, choosing from the set of all possible target language sequences E as shown below.  e* = argmaxe P(e | f )  (1)  where P(e | f ) refers to the true conditional dis-  tribution of e given f. Using this decision rule to select from within the search space of all possi- ble candidate translations minimizes the num- ber of decision errors made under a zero-one loss function (which implies there is one correct translation, and several incorrect translations). The decoder [Vogel, 2003] performs this search using a parameterized estimate Pθ (e | f ) of P(e | f ) . The search is kept tractable by aggres-  sive pruning based on the estimated model. It is clear that this decision rule does not explicitly model performance on an evaluation metric, but rather leverages the effectiveness of estimate Pθ (e | f ) to rank competing candidate sequences. [Kumar, Byrnes, 2004] propose a Minimum Bayes Risk (MBR) decoding process that explicitly minimizes the expected value of the loss according to an evaluation metric for a training set. As stated in [Kumar,Byrnes, 2004], performing the search process and computing the expectation of the loss over the true distributions is computationally prohibitive and they limit the use of their MBR decoder to re-ranking an N-Best list. [Shen, 2004] also proposes discriminative reranking on N-Best lists that focus on separating “good” and “bad” translations according to an evaluation metric. Our discussion will not focus on N-Best list re-ranking, but instead, will investigate methods that use the N-Best list as an approximation of the search environment within the decoder. We limit our scope to MAP decoders  and  determining  model  scaling  factors  θ  =  θ 
1. Introduction In recent years, various phrase-to-phrase translation models (Och 1999; Marcu & Wong 2002; Koehn 2003; Zhang 2003) have shown great advantages over the word-based systems (Brown 1990). We believe that longer phrases encapsulate more contexts of the words and the translation qualities are expected to be higher than that of short phrases. Unfortunately, given the increasing volume of the parallel bilingual data for some major languages such as Arabic and Chinese, storing and loading all possible phrase translations from the training corpus becomes more and more expensive by means of space and time in computation. To keep the phrasal translation model of a reasonable size, some models (Koehn 2003) and (Zhang 2003) limit the length of the phrases to be no more than 3 words while others (Vogel 2003) sub-samples the training corpus based on the testing data to down-scale the problem. In this paper, we introduce a new strategy to cope with this problem. Instead of aligning the phrases offline, we extract the phrase translations on the fly for each testing sentences. We use suffix array (Manber 1990) to index the training corpus and a novel fast algorithm to search all the substrings (phrases) of the testing sentences in the training  data. For each sentence pairs that contain the phrases in the testing sentence, a new phrase alignment model, Alignment via Sentence Partition (ASP) is used to extract the translations for the phrase. Thus, we do not need to store any phrase translations and we can use arbitrarily long phrases. In the following sections, we first show the empirical evidence that long phrases do improve the translation qualities. Then we will introduce our phrase alignment model ASP which finds the alignment for a source phrase of any length. The suffix array and the fast search algorithm, the key components that enables this approach to be feasible are discussed in details in section 4. In the end, we will introduce a mixture online/offline alignment strategy which allows for arbitrarily long phrases and works with arbitrarily large bilingual corpora efficiently. 2. Phrase Length vs. Translation Quality Throughout this paper, TIDES Chinese-English bilingual corpora are used as the training data and all experiments are tested on three years’ TIDES/NIST MT evaluation set. Yet, the approach described in this paper is language independent and can be applied to other language  EAMT 2005 Conference Proceedings  294  An efficient phrase-to-phrase alignment model for arbitrarily long phrase and large corpora  pairs. Table 1 lists the statistics of the training and the testing corpora, including the number of words (N), total number of sentences (Sent.) and the averaged length of each sentence (Avg. m)  Corpus Train- FBIS.gb ing UN.gb TIDES02 Testing TIDES03 TIDES04  N 4.6M 60.0M 24.3K 26.2K 52.2K  Sent. 128K 1.9M 878 919 1788  Table 1. Corpus statistics  Avg. m 36.3 31.3 27.7 28.5 29.2  First, we analyzed the n-gram coverage of the testing data given the training corpus. Table 2 shows the n-gram coverage of the TIDES04 data given two training corpora. On the word level (unigram), both training corpora cover the testing data well. More than 99% of words in the testing data can be found in either training set. On the other hand, the coverage for long phrases decreases rapidly, less than 5% of 5-grams in the testing data occur in the training data. Still, it is worth noticing that there is a significant number of long phrases that are covered in the training data and even one 68-gram occurred in the FBIS training data.  TIDES04  n  FBIS  UN  
