This talk provides an overview of current work in my research group on the syntactic annotation of the Tubingen corpus of spoken German and of the German Reference Corpus (Deutsches Referenzkorpus: DEREKO) of written texts. 
Automatic identification of Chinese personal names in unrestricted texts is a key task in Chinese word segmentation, and can affect other NLP tasks such as word segmentation and information retrieval, if it is not properly addressed. This paper (1) demonstrates the problems of Chinese personal name identification in some IT applications, (2) analyzes the structure of Chinese personal names, and (3) further presents the relevant processing strategies. The geographical differences of Chinese personal names between Beijing and Hong Kong are highlighted at the end. It shows that variation in names across different Chinese communities constitutes a critical factor in designing Chinese personal name identification algorithm. Keywords: Chinese personal name identification, Chinese word segmentation, Chinese IT applications, Chinese linguistic differences 
One main complexity of the copula constructions concerns a mismatch between morphology and syntactic constituency: the copula seems to form a morphological unit with the immediately preceding element, whereas in terms of syntax the copula appears to take this as its syntactic complement. In capturing such mismatches, we show that the copula is treated as an independent verb at the level of tectogrammatical structure (or syntax tree), whereas as a bound morpheme at the level of phenogrammatical structure (or domain tree), in terms of Dowty 1992 (or Reape 1994). This paper, adopting the notion of DOMAIN in HPSG, shows that copula constructions are a subtype of compacting-constructions. These constructions compact the domain value of the copula and that of its preceding element together into one domain unit, eventually making it inert to syntactic phenomena such as scrambling, deletion and pro-form substitution. This construction-based approach provides a clean analysis for the formation of the copula construction and related phenomena. 
The information extraction is to delimit in advance, as part of the specification of the task, the semantic range of the output and to filter information from large volumes of texts. The most representative word of the document is composed of named entities and pronouns. Therefore, it is important to resolve coreference in order to extract the meaningful information in information extraction. Coreference resolution is to find name entities co-referencing real-world entities in the documents. Results of coreference resolution are used for name entity detection and template generation. This paper presents the heuristic-based approach for coreference resolution in Korean. We constructed the heuristics expanded gradually by using the corpus and derived the salience factors of antecedents as the importance measure in Korean. Our approach consists of antecedents selection and antecedents weighting. We used three kinds of salience factors that are used to weight each antecedent of the anaphor. The experiment result shows 80% precision. 
In this paper, we address two questions concerning negative imperatives in Korean: . (i) what is the morpho-syntactic nature of mal in negative imperatives?; and (ii) why is it impossible to form negative imperatives with short negation an? We will argue that the clause structure of imperatives include a projection of deontic modality and a projection of imperative operator encoding illocutionary force, and that mal is a lexicalization of long negation and deontic modality. We then propose that a negative imperative with short negation is ruled out because such construction maps onto incoherent interpretation which can be spelled out as I direct you to bring about a negative state or a negative event.  
 Heejong Yi Dept. of Lingistics University of Delaware 46E. Delaware Ave. Newark, DE 19716, USA hyi©udel.edu  This paper discusses issues in building a 54-thousand-word Korean Treebank using a phrase structure annotation, along with developing annotation guidelines based on the morpho-syntactic phenomena represented in the corpus. Various methods that were employed for quality control and the evaluation on the Treebank are also presented.  
Structural analysis of compound words is necessary and an important process in natural language processing. Proposed here is a corpus- and statistics- based method for the structural analysis of compound words in Japanese. We determine the structure of a compound word by using Internet corpus and calculating the strength of word association among its constituent words. Experiments with 5, 6, 7, and 8 kanji compound words show that our method works well and its performance is better than those of other comparable studies. 
Email: ikeya@yksim.or.jp Hisako Ikawa Department of English, Tsurumi University 2-1-3 Tsurumi-ku, Yokohama-shi, Kanagawa, 230-8501, Japan Email: ikawa@gc4.so-net.ne.jp  The issue of adjuncts has long been a neglected field of linguistic study  whether it be syntactic or semantic. It is only in Pustejovsky  (1995) that we find a brief mention of adjuncts. In addition to what the author calls  true  arguments,  default  arguments,  and  shadow  arguments,  he sets up Mary drove  a class of true down to New  adjuncts citing the following sentence ,  York on Tuesday.  We will  take up a small lexical item sugiru in Japanese, and we will argue that we should  posit the notion of implicit adjuncts in describing the properties with the small Japanese lexical item  sugiru. Throughout the discussions that follow we will demonstrate how  the notion is independently motivated irrespective of what linguistic  theory we are going to adopt.  1. Three Issues  We will start with the discussion by looking at the following sentences.  (1) a. Kono kohii wa atu-sugiru. this coffee Top hot excessively  92  `This coffee is too hot.'  (2) a. Taroo wa kinoo  sake -o nomi sugi  ta  Taroo Top yesterday sake Acc. drink excessively past  `Taro drank too much sake yestereday.'  Taroo wa kinoo  sake -o tskusan nomi- sugi  ta  Taroo Top yesterday sake Acc much drink excessively past  `Taro drank too much sake yestereday.'  (3) Taroo wa eki m hayaku tsuki -sugi  ta  Taroo Top station to early arrive excessively Past  `Taro arrived at the station too early.'  (4) Kono hon wa omottayorimo  muzukasi-sugiru  this book Top than I thought it to be tough excessively  "This book is tougher to read than I thought'.  The sentence in (1) shows that the modifier sugiru 'excessively' is placed not before but after the modifiees atui tot' . The sentence in (2a) demonstrates that sugiru modifies an implicit adjunct much in (2a). This becomes clearer if we compare the Japanese example with the English counterpart, which has much as is shown in sentence (5) below. The sentence in (3) shows that sugiru modifies the non-adjacent modifiee hayaku `early' , which is placed not after the modifier sugiru but before it. One of the issues to be discussed is a view point or a criterion by which an action or a property of something or someone is judged to be in an excessive degree, beyond what is right , desired or needed (s.v. too Pocket Oxford Dictionary 5th edition). The sentence in (4) exemplifies this. In view of these data, there are three main issues to be discussed. The intrinsic nature of these data will become clear if we compare these with the following English data'.  
 Chungmin Lee Dept. of Linguistics, Seoul National University Seoul, Korea clee@snu.ac.kr  Abstract This paper aims to give an explanation of the combination of certain nouns and the verb ha- 'do'. Although the verb ha- 'do' normally takes an event type argument, it takes some substantival nouns such as paiolin 'violin', umsikcem 'restaurant', and so on. A substantival noun undergoes type shifting, because the governing verb ha- 'do' coerces an entity type noun to an event reading, taking missing information from the qualia of the entity type noun. In addition, some nouns like ppallay 'launch-3e are dot objects. The verb taking a dot object selects a proper type between multiple subtypes of the dot object. Type pumping operation makes that selection possible. 1. Introduction Much of the current research on sure 'do' in Japanese and ha- 'do' in Korean has been focused on its light verb function when it combines with verbal nouns since Grimshaw & Mester (1988). Although many papers have tried to uncover the nature of the Korean verb ha- 'do', they still have difficulty in defining the lexical semantic property of this verb. Recent studies of the verb ha- including Jun (2001), Im & Lee (2001) touch on the issues of substantival nouns with ha- 'do'. In this paper, we account for how the verb ha- ' do' is directly combined with substnatival nouns or complex type nouns. Specifically, we first list the nouns used together with the verb ha- 'do' in chapter 2. The nouns are classified into verbal nouns, substantival nouns, and dot objects. In chapter 4, we accounted for the combination of the verb ha-`do' with substantival nouns or complex type nouns. A substantival noun undergoes a type shifting operation by type coercion of the governing verb ha- 'do'. Type pumping operation makes possible that ha- selects a proper type out of multiple types of a dot object. Our explanation is based on the Generative Lexicon (GL) Theory (Pustejovsky, 1995, 2001). 2. Data In Chapter 2, we classify the kinds of nouns that combine with the verb ha- 'do'. Verbal nouns constitute the light verb construction with ha- 'do'. A verbal noun, basically, assigns thematic roles to its arguments. The words in (1) are typical verbal nouns.  103  (1) kensel`construction', phagoy 'destruction', ceycak 'production' ,  Secondly, a substantival noun such as paiolin 'violin' can be a complement of the verb ha- 'do' as in (2):  (2) John-un paiolin-ul ha-n-ta J-TOP violin-ACC do-PRES `John plays the violin'  Although ha- 'do' normally takes an event type noun as its argument, some kinds of substantival nouns combine with ha- 'do' naturally. We present a part of group of substantival nouns combining with the verb ha- do' in (3): (3) a. yencwu 'play': cheyllo 'cello', phiano 'piano', tulem `drum',... b. chakyong 'wearing': mokkeli 'necklace', phalcci 'bracelet', meylppang suspenders',... c. wunyeng 'management': pyengwon 'hospital', kakey`store', kongcang`factory', d. wuncen driving' : thayksi ,... e. hupyen smoking': tampay cigarette', aphyen ' opium' ,... f. umyong drinking' : khephi coffee', swul alcoholic drink', kholla `coke',... In the above, we listed two kinds of nouns combining with the verb ha- 'do'. But there are nouns belonging to neither verbal nouns nor substantival ones. It is not easy to explain the properties of those nouns, because of their ambiguities'. The sentences in (4) show the polysemy of the noun ppallay 'laundry' with different predicates.  (4) a. onul-un ppallay-ka nemwu manh-ta  today-TOP laundry-NOM too much-DEC  `Today, I have too much laundry'  b. na-nun ppallay-lul  imi  kkutnay-ss-ta  I-TOP washing-ACC already finish-PAST-DEC  `I have already finished washing'  In (4a), ppallay 'laundry' is an entity type noun denoting the clothes that need washing. But ppallay `washing clothes' is an event type noun meaning a washing activity. We give an explanation of how those nouns combine with ha- 'do' and what the meaning of those is. The words in (5) have ambiguities because those belong to multiple types on ontology.  
Following Herburger (2000), I will develop an event-based semantics for Japanese emphatic particles which can address the issue of the mechanism of association with focus involving the emphatic particles. The proposed semantics makes use of Herburger's three key ideas: events as basic entities, decomposition of predicates into subatomic formulas, and separation of backgrounded and foregrounded information. 
The so-called overapplication of Coda Neutralization in Korean, the occurrence of a neutralized consonant in a non-neutralizing environment, is often considered as evidence for serial derivation. In this paper I propose that the neutralization effect at surface is not a result of a phonological process at an intermediate level in serial derivation, but due to a constraint requiring the integrity of the morphological constituent: EDGE-INTEGRITY. It is argued that this is not reducible to an alignment constraint, but a genuine faithfulness constraint on the edge of a morphological constituent. The putative opacity related with the coda neutralization is shown to be an epiphenomenon arising from the ambisyllabic representation of a consonant at a morphological juncture, satisfying both EDGE-INTEGRITY and syllabic conditions. Consonant Copy in the Jeju dialect provides further evidence for EDGE-INTEGRITY, the only difference being that the conflict between syllabic conditions and EDGE-INTEGRITY is resolved by insertion of a copied consonant.  
Aligned parallel corpora have proved very useful in many natural language processing tasks, including statistical machine translation and word sense disambiguation. In this paper, we describe an alignment technique for extracting transfer mapping from the parallel corpus. During building our system and data collection, we observe that there are three types of translation approaches can be used. We especially focuses on Traditional Chinese and Simplified Chinese text lexical translation and a method for extracting transfer mappings for machine translation. 
This paper describes our ongoing Korean-Chinese machine translation system, which is based on verb patterns. A verb pattern consists of a source language pattern part for analysis and a target language pattern part for generation. Knowledge description on lexical level makes it easy to achieve accurate analyses and natural, correct generation. These features are very important and effective in machine translation between languages with quite different linguistic structures including Korean and Chinese. We performed a preliminary evaluation of our current system and reported the result in the paper.  
A homonym could be disambiguated by another words in the context as nouns, predicates used with the homonym. This paper using semantic information (co-occurrence data) obtained from definitions of part of speech (POS) tagged UMRD-S 1 ). In this research, we have analyzed the result of an experiment on a homonym disambiguation system based on statistical model, to which Bayes' theorem is applied, and suggested a model established of the weight of sense rate and the weight of distance to the adjacent words to improve the accuracy. The result of applying the homonym disambiguation system using semantic information to disambiguating homonyms appearing on the dictionary definition sentences showed average accuracy of 98.32% with regard to the most frequent 200 homonyms. We selected 49 (31 substantives and 18 predicates) out of the 200 homonyms that were used in the experiment, and performed an experiment on 50,703 sentences extracted from Sejong Project tagged corpus (i.e. a corpus of morphologically analyzed words) of 3.5 million words that includes one of the 49 homonyms. The result of experimenting by assigning the weight of sense rate(prior probability) and the weight of distance concerning the 5 words at the front/behind the homonym to be disambiguated showed better accuracy than disambiguation systems based on existing statistical models by 2.93%. 
Korean predicative verb forms obligatorily denote the three categories speech level, mood and sentence type which are not handled by most of the automatic word form recognition systems for this language. These categories are marked by special endings. This paper examines predicative verb forms concentrating on the lexical description of these endings' in the framework of Left-Associative Grammar (LAG). Additionally this paper suggests a system to analyse verb forms in these aspects. The results of this study have been implemented using Malaga2 and integrated into an automatic word form recognition system for Korean called KMM (Korean Malaga Morphology). 
Abstract The purpose of this paper is to analyze the lexical-semantic structure of morphologically derived passive verbs in Korean based on Pustejovsky (1995)'s Generative Lexicon Theory (GL) and to explain the change of the root verb's lexical-semantic structure by means of passivization. Passivization in this paper is defined as the unaccusaztivization. In Argument Structure of derived passive verbs, the agent argument is deleted and the theme argument is realized as a syntactic subject. As for Event Structure, derived passives express left-headed event (achievement), whereas their roots denote right-headed event (accomplishment). In Qualia Structure, passive verbs and root ones have the same Fomal Role, but in Agentive Role of passive verbs, an act weakens to a process. Both Formal and Agentive Roles have the same theme argument. 
ECM across a CP in Korean poses difficulties from the standpoint of the locality of A-movement/agreement. A phase-based analysis is developed which requires two steps: (i) in the embedded CP, VP/vP containing its VP-internal subject first moves to Spec-CP, which renders the subject accessible to the matrix v, in accordance with Chomsky's Phase Impenetrability Condition; (ii) ECM takes place in a local relation between the matrix v and the embedded subject. It is shown that the otherwise puzzling fact that ECM across a CP, but not Passivization across a CP, is affected by the type of the embedded verb in Korean is accounted for in a principled way, based on the assumption that CP and vP, but not TP and VP, are phases.  
In this paper, we propose a method of generating a proper categorization of morphemes by giving a hierarchical part-of-speech system and a corpus tagged using this part-ofspeech system. Our method use hierarchical information in the part-of-speech system and statistical information in the corpus to generate a category set. The statistical information is based on the context of occurrence of categories. First, we specify the format of given information. Then, we describe an algorithm to generate a proper categorization. Finally, we present the results of our experiments in applying this method. We obtaind a moderately proper categorization and found several candidates for improvement. 
1. Introduction The particle TO is often used between noun phrases to represent coordinate relation in Japanese language such as: JOHN TO MARY GA KENKASURU. John and Mary SUBJ fight John and Mary fight. It is interesting that TO is also used as the case postposition of cooperation, opposition or equality: JOHN GA MARY TO KENKASURU. John SUBJ Mary against fight John fights against Mary. This first example has in most cases the deep case structure same as the second, which shows the structure directly. But the example JOHN TO MARY GA WAKAI. John and Mary SUBJ be young John and Mary are young. does not have such deep case structure but has a parallel pair of propositions that John is young and that Mary is young. Therefore it is necessary to distinguish the first from the third, to produce different semantic structures which correspond to deep case structures. We investigate the particle TO here, and deep case structure for each classification of it. 2. Analysis of TO Japanese postpositions, which include conjunctions in European grammars, are classified by usage as follows. Table 1. classification of postpositions case postposition, conjunctive postposition, terminal postposition, relational postposition, noun modifier, nominalizer, coordinate postposition The particle TO is used as three of them. As a case postposition, two meanings can be distinguished. The four ways of TO usage are in the Table 2. Here we discuss the classes 1 and 2 rather than 3 and 4, since the latter can be dealt with 227  ordinary polysemy disambiguation process.The TO in the class 1 connects a pair of noun phrases, and the combination is also a noun phrase. The grammatical rule is:  noun_phrase ---->  noun_phrase TO noun_phrase  where TO agrees with AND of English except that TO cannot be used between other categories. So it cannot be extended to any metarule. The TO in the class 2 makes a noun phrase a postposition phrase which modifies a verb phrase. This is described as the rules:  verb_modifying_phrase -->  noun_phrase case_postposition  and case_postposition --> TO .  Table 2. classification of the particle TO  class 1 category  ( English )  example sentences I meaning  
 1. Introduction  Japanese has small clauses as shown in (1a). 1 In this example, the small clause subject has its Case feature checked against the matrix verb, resulting in an accusative Case-marking. This is indeed the only way for the small clause subject to check its Case feature. Since there is no finite T in the small clause, as is indicated by the non-inflected form of the predicate, the small clause subject cannot appear in the nominative as in (lb):  (1) a. John-ga [sc Mary-0 [PRED kasikoku]] omotta (koto)  nom  -acc intelligent considered (fact)  "John considered Mary intelligent."  b. *John-ga [Sc Mary-ga [PRED kasikoku]] omotta (koto)  nom  -nom intelligent considered (fact)  However, it is often said that Japanese allows another type of small clause, in which a small clause predicate shows up in a finite form as in (2a). I will refer to this type of small clause as (mite small clause (FSC). That there exists a finite T in the FSC can be made clear by the availability of the nominative Case marking on the embedded subject as in (2b):  (2) a. John-ga Mary-o kasikoi to omotta (koto) nom -acc intelligent C considered (fact) b. John-ga Mary-ga kasikoi to omotta (koto) -nom -nom intelligent C considered (fact)  
We propose an algorithm for the automatic acquisition of a bilingual lexicon in the legal domain. We make use of a parallel corpus of bilingual court judgments, aligned to the sentence level, and analyse the bilingual context profiles to extract corresponding legal terms in both languages. Our method is different from those in past studies as it does not require any prior knowledge source, and naturally extends to multi-word terms in either language. A pilot test was done with a sample of ten legal terms, each with ten or more occurrences in the data. Encouraging results of about 75% average accuracy were obtained. This figure does not only reflect the effectiveness of the method for bilingual lexicon acquisition, but also its potential for bilingual alignment at the word or expression level. 
I examine various controversial aspects of Chinese prosody–tone structure, syllable structure, stress, and intonation–and stress the need to view all of these as interacting systems, aspects of a hierarchical prosodic structure. I examine various proposals at these various levels of the hierarchy and suggest which are most appropriate. Specifically, I suggest the adoption of Bao's version of syllable and tone, and Chen's account of stress. As for intonation, it is still not possible to make any definitive claims regarding an optimal model, but I examine work done by Kratochvil, Shih, and Garding et al, and suggest promising directions for future work. 
A system to assist call routing task for telephone operators at the Directorate General of Telecommunications (DGT) in Taiwan is reported in this paper. The system was developed based on DGT organization profile with description of its six divisions instead of a corpus of recorded and transcribed call-routing dialogs. An acoustic module and an information retrieval module were built specifically for this task. The construction of IR module was based on term extraction and thesaurus discovery processes. By integrating acoustic and IR module, the system achieves satisfactory performance and provides a promising approach to call routing. Simulation results indicated that the proposed algorithm outperforms standard classification methods. A working system based on the proposed approach has been implemented and experimental results are presented. 
This paper aims to account for the backward anaphora that seem to be against the c-command requirements in the anaphor-antecedent relations. It was claimed that the binding conditions should apply at LF for the backward binding cases involving phych-verbs and causatives. Under the recent development of minimalism where the concept of levels disappears to adopt a cyclic derivation, the data that show the backward binding phenomena have not been discussed in the area of the binding theory. In this paper, I argue that the backward binding cases can be incorporated into the core binding phenomena with the general assumptions on the thematic prominency. It is discussed how the dependency between NPs involving backward anaphora is determined by the thematic prominency. The Agree operation takes place between the probe T and the goal with the uninterpretable u[a] and [prominent] feature, by which an anaphor is valued, producing a proper interpretation. 
Since thesaurus is used as a knowledge resource in many natural language processing systems, it is very useful and necessary for the high quality systems, especially for dealing with semantics. In this paper, we introduce a semi-automatic method for the construction of Korean noun semantic hierarchy by utilizing a monolingual MRD and an existing thesaurus. 
This paper provides computational algorithms for a Korean reflexive caki, for which both sentence-bound and long-distance readings are possible. Its analyses are based on Chierichia's theory in Categorial Grammar, and a CCG-like system is introduced for the implementation. In this system, we can get both readings of caki with the same resolution mechanisms, while the difference is where the reflexive is resolved. These algorithms enable us to account for the distributions and characteristics of a long-distance reflexive caki with a more unified way.  
The issues in polysemy with respect to the verbs in WordNet will be discussed in this paper. The hypernymy/hyponymy structure of the multiple senses is observed when we try to build a bilingual network for Chinese and English. There are several types of polysemic patterns and a co-hypernym may have the same word form as its subordinates. Fellbaum (2000) dubbed autotroponymy that the verbs linked by manner relation share the same verb form. However, her syntactic criteria seem not compatible to the hierarchies in WN. Either the criteria or the network should be reconducted. For most verbs in WN 1.7, polysemous relations are unlikely to extend over 3 levels of IS-A relation. Highly polysemous verbs are more complicated and may be involved in certain semantic structures. Semi-automatic sense grouping may be helpful for multimlinguital information retrieveal. 1. Introduction WordNet (WN), which is a large scale, domain-dependent semantic network of English words, provides a broad-coverage of lexical information. It represents a system of semantic relations among words, between words and synsets' , and between synsets themselves (Miller, 1990, 1995). Two features of the system are concept definitions and an inheritance hierarchy of concept types. Rather than using lexical entries only, the design-is based on linguistic theories about cognitive organization of natural languages. English nouns, verbs, adjectives, and adverbs are arranged to synsets that are in turn linked through semantic relations such as antonymy, hypernymy, etc. Like other conventional dictionaries and thesauri, WordNet also provides different meanings for one word. The lexicographic database represents a complex linguistic structure in which a word form may carry multiple senses. These word senses that are related in systematic ways build different synsets for each sense of a word. A word meaning then, is the pairing of a word form with a synset. However, WordNet's sense distinctions are more fine-grained than other machine-readable dictionaries, resulting in abundant polysemy and difficulty of computation (Kilgarriff 1997). In this paper, we will pay particular attention to the issues in polysemy with respect to the verbs in WordNet 1.7 and attempt to find a typical hypernymy/hyponymy structure for the multiple senses of a word form. In the following sections we will briefly overview the verb hierarchies in WN and illustrate the patterns of sense clusters. The types of autotroponymy will be discussed as well. 
JAPAN 980-8576 kei@intcul.tohoku.ac.jp  1. Introduction In this paper we would like to deal with interaction of presupposition triggers and show that the binding theory of presupposition comes up with it better than the satisfaction theory. In section 2, the newly developed presupposition conception is introduced. Section 3 explains the Binding Theory in contrast with the Satisfaction Theory. In sections 4 and 5, Kamp's new idea about the treatment of complex interaction of presuppositions is introduced, and it is suggested that his theory can be extended to two other phenomena; linguistic contexts in which presupposition triggers only have narrow scope reading and specificity interpretation of some Japanese NPs. Based on the recent development of DRT approaches on presuppositions, we will deal with floating quantifiers as a special case of specificity phenomena in sections 6 to 8. In section 6, three different configurations of a quantifier and its restriction NP are introduced: the genitive construction, the compound construction and the floating construction. As a preliminary research, we concentrate in section 7 and 8 on the compound construction of floating quantifiers to the effect that comparison with Semantic Incorporation is made to understand non-specificity of the construction more in detail. This will be done first in GQT terms (section 7) and finally as an application of de Swart & Farkas' theory on weak NPs (section 8).  
This paper discusses constraints on grammaticalization, a primarily diachronic process through which lexical elements take on grammatical functions. In particular, it will argue that two constraints on this process, namely Persistence and Layering, explain the different distributional patterns of time-relationship adverbs in Japanese, Korean, English and German. Furthermore, it will suggest that the distributional difference between Japanese and Korean time-relationship adverbs is not an isolated phenomenon but is a reflection of the overall semantic typological differences between the two languages in the sense of Hawkins (1986). 
This paper presents a unified account of three kinds of constructions in which more than one NP can show up with the same case in simple sentences in Japanese and Korean: double subject, double nominative object and double accusative constructions. Noting that the second NPs in these constructions are functional or relational, this paper proposes to assign them the category and type different from the first NPs. We show the derivations of these three constructions in a parallel manner, and explain the asymmetries in extractability between possessor and possessed NPs in relativization.  
This paper puts forward an analysis of scope interactions between Japanese adverbial quantifiers like mainichi 'everyday' and tokidoki 'sometimes' and a negative morpheme nai 'not' on the basis of Aocus)-structures. In this analysis, three f-structures are assigned to a sentence with an adverbial quantifier and a negative morpheme. One of them represents a negation-wide reading, and the other two represent quantifier-wide readings. Some f-structures, however, are unacceptable due to semantic or pragmatic factors. Different scope behaviors of the two quantifiers mentioned above can then be ascribed to acceptability of f-strucures. 
Shi (2000) claims that topics must be related to a syntactic position in the comment, thus denying the existence of dangling topics in Chinese. Under Shi's analysis, the dangling topic sentences in Chinese are not topic-comment but subject-predicate sentences. However, Shi's arguments are not without problems. In this paper we argue that topics in Chinese can be licensed not only by a syntactic gap but also by a semantic gap/variable without syntactic realization. Under our analysis, all the dangling topics discussed in Shi (2000) are, in fact, not subjects but topics licensed by a semantic gap/variable that can turn the relevant comment into an open predicate, thus licensing- dangling topics and deriving well-formed topic-comment constructions. Our analysis fares better than Shi's in not only unifying the licensing mechanism of a topic to an open predicate without considering how the open predicate is derived, but also unifying the treatment of normal and dangling topics in Chinese. 
I argue that the so-called psychological predicates like komapta 'thankful,' mwusepta 'fearful,' silhta `loathsome,' or kulipta 'missing' require a nominative subject and a locative or dative complement, challenging the claim, a conventional wisdom originated from Kuno(1973), that they are two-place "transitive adjectives" requiring a nominative direct object. I also show that those adjectives are subject to having the locative-dative complement extracted, which is ultimately realized as a focused subject or a topic. Thus, in this type of double nominative constructions, the first nominative is a focused subject, and the second nominative forms an embedded clause with the psychological predicate, which functions as the predicate of the whole sentence. 
Introduction Ever since the traditional grammar (cf. Curme 1931), it has been observed that certain types of English noun phrases can behave like an adverb or an adverbial prepositional phrase on their own, i.e. without any support by a preposition. Such noun phrases have been dubbed adverbial accusatives. They can play virtually the same range of adverbial functions as the "real" adverbials: (1) Time adverbial I stayed there all the summer. I'd like to start Wednesday, the first jury day. Spatial adverbial Let's go some place. Come this way, please. Measure adverbial I should not mind a bit. She used to laugh a good deal Manner adverbial Don't look at me that way. They cook (the) French style. Of these adverbial accusatives above, this paper focuses o adverbial noun phrases which are preceded by a demonstrative article this Limit on Scope Like other adverbial accusatives, this adverbial accusatives show the four types of adverbial behaviors above: (2) Time adverbial John met Mary this summer. Spatial adverbial Come this way, please. Measure adverbial I need a chain about this long. Manner adverbial The young ladies would not like to be hearing you talk this way. 
The left-associative grammar model (LAG) has been applied successfully to the morphologic and syntactic analysis of various european and asian languages. The algebraic definition of the LAG is very well suited for the application to natural language processing as it inherently obeys de Saussure's second law (de Saussure, 1913, p. 103) on the linear nature of language, which phrase-structure grammar (PSG) and categorial grammar (CG) do not. This paper describes the so-called Loom-LAGs (LLAG) —a specialisation of LAGs for the analysis of natural language. Whereas the only means of language-independent abstraction in ordinary LAG is the principle of possible continuations, LLAGs introduce a set of more detailed language-independent generalisations that form the so-called loom of a Loom-LAG. Every LLAG uses the very same loom and adds the language-specific information in the form of a declarative description of the language —much like an ancient mechanised Jacquard-loom would take a program-card providing the specific pattern for the cloth to be woven. The linguistic information is formulated declaratively in so-called syntax plans that describe the sequential structure of clauses and phrases. This approach introduces the explicit notion of phrases and sentence structure to LAG without violating de Saussure's second law and without leaving the ground of the original algebraic definition of LAG. LLAGs can in fact be shown to be just a notational variant of LAG —but one that is much better suited for the manual development of syntax grammars for the robust analysis of free texts. 
It is widely recognized that Japanese manner of motion verbs do not tolerate a 
This paper proposes that the argument structures be stated in a way that uses probabilities derived from a corpus to replace a Boolean-value system of subcategorization. To do this, we make a cognitive model from a situation to an utterance to explain the phenomena of arguments' ellipsis, though the traditional term ellipsis is not suitable under our new concepts. We claim that the binary distinction is neither rational nor suitable for a real syntactic analysis. To solve this problem, we propose two new concepts argumentness and probabilistic Case structures by adapting the prototype theory. We believe that these concepts are effective in the syntactic analysis of NLP. 
Jay Rajasekera Graduate School of International Management International University of Japan jrr@iuj .ac .jp tel: (81) 257791531  1. Introduction Data mining techniques are well accepted for discovering potentially useful knowledge from the large datasets. Our past research work on studying aspects of data mining includes improving the performance of the rule generation [SS95, SWC96], extending the scope of association rule mining [RSC99] and data generation [SLL96]. Prior to data mining process, data cleaning is essential in that the quality of rules derived from the mining process is subject to the quality of data. Recently, significant attention is paid to record deduplication----an important branch of data cleaning. Various reasons are behind different representations of identical record: typographical errors, purposeful entry of false names, inconsistent data formats, incomplete information and registrant moving from one place to another. [LL+99] is a milestone paper in the area of record de-duplication. Experiments on real-world data demonstrate that the methods of de-duplicate records presented in [LL+99] are efficient. Further study indicates that there is still room for improvement in the core part of its whole technology---- the algorithm of the calculation of the Field Similarity. Our paper is to introduce a new algorithm to calculate Field Similarity. Theoretical analysis, concrete examples and experimental result shows that our algorithm can significantly improve the accuracy of the calculation of Field Similarity. The rest of the paper is organized as follows. Section 2 gives a background description of the algorithm of calculating Field Similarity presented in [LL+99]. Section 3 proposes our algorithm of calculating Field Similarity and exhaustively compares the new algorithm with the previous one. Section 4 provides an experiment to prove the performance improvement with the introduction of the new algorithm.  2. Preliminary Background  This section gives a brief description of the algorithm to calculate Field Similarity presented in [LL+99]. Let a field in record X have words Oxl, Ox2,....., Oxn and the corresponding field in record Y have words Oyl, 0y2, , Oym. Each word Oxi ,1 i  n is compared with words Oyj, 1 j  n. let DoSxl, DoSx2,....., DoSxn, DoSyl, DoSy2,....., DoSym be the maximum of the degree of similarities for words Oxl, 0x2,....., Oxn, Oyl, 0y2, , Oym respectively. Then the Field Similarity for record X and Y  
Korean government has adopted the French TGV as a high-speed transportation system and the first service is scheduled at the end of 2003. TGV-relevant documents are consisted of huge volumes, of which over than 76% has been translated in English. A large part of the English version is, however, incomprehensible without referring to the original French version. The goal of this paper is to demonstrate how DiET 2.5, a lexicon builder, makes it possible to build with ease domain-specific terminology lexicon that may contain multimedia and multilingual data with multi-layered logical information. We believe our wok shows an important step in enlarging the language scope and the development of electronic lexica, and in providing the flexibility of defining any type of the DTD and the interconnectivity among collaborators. As an application of DiET 2.5, we would like to build a TGV-relevant lexicon in the near future. 
This paper reports on the design of a lexical database for English which is currently under construction ("FrameNet-2" ), and describes the kinds of linguistic facts that the database is intended to make available, for both human and computer consumers. Building on a recently completed pilot study ("FrameNet-1" ), it is centered on the nature of the relation between lexical meanings and the conceptual structures which underlie them (semantic frames). The database will show the semantic and syntactic combinatorial possibilities (based on frame membership) of the lexical items it includes, as these are documented through grammatical and semantic annotations of sentences extracted from a large corpus of contemporary written English. The notions of profiling within a frame, frame inheritance including multiple inheritance, frame blending, and frame composition will be explained and illustrated, and the manner of storing information about them in the database will be outlined. The building of the database, with its necessary labor-intensive manual component, will be explained. 
Kathleen Ahrens Graduate Institute of Linguistics National Taiwan University Taipei, Taiwn ahrens@ms.cc.ntu.edu.tw  Chu-Ren Huang Institute of Linguistics Academia Sinica Nankang, Taipei, Taiwan 115 churen@gate.sinica.edu.tw  
Current lexical semantic theories provide representations at a coarse grained level. In this paper, I will provide motivations for a fine grained representation for verbs and. nouns. An initial case study is done to serve as evidence that a more detailed representation is needed for tasks that require high accuracy rates, such as machine translation. An automatic approach to gather fine grained information via corpus extraction is described. Lastly, issues of lexical representation and cross-lingual translation are discussed. 
In this paper we propose a method to compositionally interpret the meanings of floated quantifiers on the basis of Categorial Grammar (Carpenter 1997) and Davidsonian semantics (Davidson 1980). In this approach, floated quantifiers are accounted for semantically as a kind of adverbial phrase so that we do not need any special syntactic operations, such as quantifier raising, in order to interpret their meanings. The approach also explains 'pragmatic implication' introduced by floated quantifiers on the basis of the whole-part relationship between sets. 
 This paper concentrates on peculiar behaviors of an English word, else, among various lexical items which have to do with the exceptive constructions. Reviewing its intriguing properties, it is argued that a simple way of reducing an "exception" from the domain of quantification and a more enriched domain of quantification, containing at least pairs of individuals, are preferable.  
We present a theory of grammar based on asymmetrical relations, the Strict Asymmetry Theory, and we provide evidence form Indo-European and non Indo-European languages to show that argument structure restrictions on morphological composition follow in a principled way from the theory. We describe and implement a bottom-up parser for morphological selection in the Strict Asymmetry framework. Core lexical properties including argument structure and derivational affix selection are encoded in a uniform mechanism. We consider the computational implications of three different implementations. In particular, we examine the effect on bottom-up parsing from varying the Specifier-Head-Complement order. We provide computational motivation for the logical separation of overt and covert affixation. 
Japanese emphatic particles such as mo, wa, and sae are known to present exceedingly recalcitrant problems for grammarians. Since they are clearly concerned with connecting discourse presuppositions with the assertive content of the current utterance, their nature has to be pragmatic as well as semantic. Their syntactic, or rather morphological, behaviour also seems highly unmanageable since they interact not only with themselves but also with other types of particles, especially case particles. In this paper, we try to present a basic scheme for treating emphatic particles based on four features: type, self, edge and polarity. We also try to place emphatic particles in their proper place within overall grammar of the Japanese language. 
Knowledge acquisition is an essential and intractable task for almost every natural language processing study. To date, corpus approach is a primary means for acquiring lexical-level semantic knowledge, but it has the problem of knowledge insufficiency when training on various kinds of corpora. This paper is concerned with the issue of how to acquire and represent conceptual knowledge explicitly from lexical definitions and their semantic network within a machine-readable dictionary. Some information retrieval techniques are applied to link between lexical senses in WordNet and conceptual ones in Roget's categories. Our experimental results report an overall accuracy of 85.25% (87, 78, 89, and 87% for nouns, verbs, adjectives and adverbs, respectively) when evaluating on polysemous words discussed in previous literature. 
This paper reports a full-scale linkage of noun senses between two existing lexical resources, namely WordNet and Roget's Thesaurus, to form an Integrated Lexical Resource (ILR) for use in natural language processing (NLP). The linkage was founded on a structurally-based sense-mapping algorithm. About 18,000 nouns with over 30,000 senses were mapped. Although exhaustive verification is impractical, we show that it is reasonable to expect some 70-80% accuracy of the resultant mappings. More importantly, the ILR, which contains enriched lexical information, is readily usable in many NLP tasks. We shall explore some practical use of the ILR in word sense disambiguation (WSD), as WSD notably requires a wide range of lexical information. 
Here is presented and discussed some principles for extracting the semantic or informational content of texts formulated in natural language. More precisely, as a study of computational semantics and information science we describe a method of logical translation that seems to constitute a kind of semantic analysis, but it may also be considered a kind of information extraction. We discuss the translation from Dataflow Structures partly to parser programs and partly to informational content. The methods are independent of any particular semantic theory and seem to fit nicely with a variety of available semantic theories. 
Trie, a well-known searching algorithm, is a basic and important part of various computer applications such as information retrieval, natural language processing, database system, compiler, and computer network. Although a merit of trie structure is its searching speed, the naïve version of trie structure size may not be acceptable when the key set is very large. To reduce its size, several methods were proposed. This paper proposes an alternative approach using a new tie structure called, structure-shared trie (SS-trie). The main idea is to reduce unused space using shared common structure and bit compression. Three techniques are used: (1) path compression, (2) structure sharing, and (3) node compression. A number of experiments are conducted to investigate trie size, sharing rate, and average depth of the proposed method in comparison with binary search, naïve trie, PAT, and LC-trie. The result shows that our method outperforms other methods. 
In this paper we will investigate the centering theory proposed by Grosz, Joshi, and Weinstein (1995) (henceforth GJW) and revised in Walker, Joshi, and Prince (1998) (henceforth WJP), and argue that their theory needs to be further extended and revised. We show that the centering theory proposed by GJW and WJP would make wrong predictions about the preferred transition states in discourse processing since their backward-looking center (Cb) is not primitive and thus cannot be used to adequately model the local coherence of discourse. We propose that Cb should be distinguished from the discourse segment topic (DST), and provide a more restrictive definition of Cb, which restricts Cb(U) to be the element in Cf(U 11 ) that is realized by the subject pronoun or the pronoun contained in the subject in U. 
The temporal structure of events on the discourse level has long been of great interest in both theoretical and computational linguistics. In this paper, we offer a unified approach to the temporal relationships related to a hierarchical discourse structure. We apply the method of pronoun resolution to the interpretation of tense. It is based on an analysis within the framework of the controlled information packaging theory. A unique aspect of our account is that temporal interpretation across discourse segments in global discourse is subject to the same principles as the interpretation of global anaphora, and that there is thus no need to postulate independent principles to account for the discourse behaviour of tense. In this way, we can neatly explain the general view that tense parallels the anaphoric nature of pronouns. 
This paper presents results of an empirical analysis on the structuring of spoken discourse focusing upon how some particular utterance components in Chinese spoken dialogues are highlighted. A restricted number of words frequently and regularly found within utterances structure the dialogues by marking certain significant locations. Furthermore, a variety of signals of monitoring and repairing in conversation are also analysed and discussed. This includes discourse particles, speech disfluency as well as their prosodic representation. In this paper, they are considered a kind of "highlighting-means" in spoken language, because their function is to strengthen the structuring of discourse as well as to emphasise important functions and positions within utterances in order to support the coordination and the communication between interlocutors in conversation.  
The structure of objects employed in the study of Natural Language Semantics has been increasingly being complicated to represent the items of information conveyed by utterances. The complexity becomes a source of troubles when we employ those theories in building linguistic applications such as speech translation system. To understand better how programs operating on semantic representations work, we adopt a logical approach and present a monadic and multiplicative linear logic. In designing the fragment, we refine on the multiplicative conjunction to employ both the commutative and non-commutative connectives. The commutative connective is used to glue up a set of formulae representing a semantic object conjointly. The non-commutative connective is used to glue up a list of formulae representing the argument structure of an object, where the order matters. We also introduce to our fragment a Lambek slash, the directional implication, to concatenate the formula representing the predicative part of the object and the list of formulae representing the argument part. The monadic formulae encode each element of the argument part by representing its sort with the predicate and the element as the place-holder. The fragment enjoys the nice property of being decidable. To encode contextual information involved in utterances, however, we extend the fragment with the exponential operator. The context is regarded as a resource available as many as required, but not infinitely many. We encode the items of context with the exponential operator, but ensure that the operator should appear only in the antecedent. The extention keeps the fragment decidable because the proof search will not fall into an endless search caused by the coupling of unlimited supply and consumption. We show that the fragment is rich enough to encode and transform semantic objects employed in the contemporary linguistic theories. The result guarantees that the theories on natural language semantics can be implemented reasonably and safely on computers. Keyward: multiplicative linear logic, natural language semantics, monadic logic, Lam- bek calculus 175  
This paper investigates an effective method of building domain-independent text generation system. In our English text generation system, Semantic Network is used as the internal Knowledge Representation. Nodes and links of the semantic network are classified according to word class and grammatical relations respectively. Generation results prove that the system works well and can generate coherent text flexibly. 
This paper investigates what words map from the source domain of PLANT in Mandarin Chinese. In particular, we examine how different aspects of the source domain of PLANT are mapped onto the different target domains of LOVE, MARRIAGE, HAPPINESS, and BELIEFS. We found that mapping principles can account for the different mappings (Ahrens 2000).  
Floating quantifiers (FQs) in English exhibit both universal and language-specific properties, and this paper shows that such syntactic and semantic characteristics can be explained in terms of a constraint-based, lexical approach to the construction within the framework of Head-Driven Phrase Structure Grammar (HPSG). Based on the assumption that FQs are base-generated VP modifiers, this paper proposes an account in which the semantic contribution of FQs consists of a "lexically retrieved" universal quantifier taking scope over the VP meaning. 
Expressions in Japanese with indeterminates such as "nani (what)" and "dare (who)" get interpretations very much like universal quantification or existential quantification when followed by conjunctive particle "mo" or disjunctive particle "ka". Focusing our attention on sequences of the form <indeterminate + ka> such as "nani-ka (something)", they generally get interpretations of "something exists which" in affirmative sentences, whereas they generally get interpretations of "something exists which does not" in negative sentences. Interestingly, the latter with negation changes its figure in conditional sentences and in concessive conditional sentences. In both of these, negative sentences with sequences of the form <indeterminate + ka> get not only interpretations of "something exists which does not" but also interpretations of "nothing exists which", depending on the situation in which utterances take place. Similar phenomena can be seen with numerals, where negation can take either a narrower or a wider scope with respect to numerical quantification in conditionals and concessives. In this paper, we would argue that those phenomena could be accounted for by postulating pragmatic inferences with conditionals and concessives, and by assuming that Japanese focus particles "wa" and "mo" in these cases function as a conditional operator and a concessive operator respectively.  
In this paper, two Japanese particles wa and mo are taken up. Wa is known as a topic marker and mo is a correspondent of English particles too and also. After reviewing several studies that claim topics and particles like too and also evoke alternatives, a formal semantic system with structured meanings is constructed for wa and mo, incorporating the insights of the previous studies. The proposed semantics of wa and mo, then, is applied to adverbial quantifier constructions and an attempt is made to derive the special implicatures added by the particles by means of their semantic properties and pragmatic inferences.  
The purpose of this paper is to explore the relationship between case and free word order in Korean by providing proper LP constraints which solve the problems of the previous analyses such as Kuno's (1980) Crossing-Over Constraint (COC). Based on Cho & Chai (2000), we introduce a new type marker which includes case, postpositions, and delimiters. We also add the Adjunct LP Constraint and the Argument LP Constraint adapted from the two LP constraints of Cho & Chai (2000). Our LP constraints present a solution to the alleged counterexamples of Cho & Chai (2000) as well as to the problems of Kuno (1980). The newly postulated type marker enables us to account for the scrambling possibilities of the NPs containing postpositions and delimiters.  
This paper aims to posit a functional category for Thai classifiers and demonstrate the analysis of Thai complex nominals adopting the antisymmetry framework (Kayne 1994). It proposes that Thai classifiers have an independently functional status and project the Classifier Phrase (ClassP) basically because they work in the same way as agreement. Evidence supporting their functional status includes properties of classifiers in forming their own word class distinct from the category of nouns, their non-modificational property by adjectives, and multiple occurrences. The underlying structure of Thai nominals is constructed in terms of the DP analysis. To derive a Thai nominal word order, it is argued that classifiers features are strong and there exist a combination of raising operations regulated by asymmetrical c-command relation (Kayne 1994) as well as feature checking (Chomsky 1995). The analysis suggests that Thai nominals possess a commonly underlying head-initial structure in which movement plays a key role in deriving the surface word order. 1. Introduction A comparison of the word orders of Thai and English simple and complex nominals reveals that Thai is a mirror image of English as shown in 1). 1) nOk tua lek scam tua nan bird clf little three clf that/those "those three little birds" In 1) above, abstracting away classifiers (elf), the Thai word order noun-adjective-numeraldemonstrative is the reverse order of the English demonstrative-numeral-adjective-noun. The Thai linear order of complex nominals, which would otherwise be viewed as completely different from that of English, strongly resembles English with respect to relative adjacency of elements. The linear word order of Thai nominals will be analyzed in a way that classifiers are consequential to the course of the derivation. Working towards this direction, I will organize the remaining part of this paper as follows. In section 2, I will show distributional facts about Thai classifiers ranging from simple to complex nominals. In section 3, I will propose a functional status of classifiers by giving conceptual and syntactic accounts. In section 4, I will briefly discuss Tang's (1990) analysis on Chinese DP in which classifiers are treated as one of the dual heads cooccuring with number in the Klassifier Phrase. I will point out that an analysis without giving an independent status for classifiers as such is incomplete, and cannot account for multiple occurrences of classifiers. Then, I will show my analysis of Thai 259  complex nominals. In my analysis, the structure of complex Thai nominals is constructed in terms of the DP analysis in which classifiers are argued to project ClassP. I incorporate the antisymmetry framework (Kayne 1994) and feature checking (Chomsky 1995) to account for the derivation of the surface word order. Finally in section 5, I provide a conclusion of this study. 2. Distribution of Classifiers in Thai I will briefly describe the positions which classifiers appear in simple and complex Thai nominals. It is noteworthy that the nominals being investigated here are restricted to non-abstract nouns (animate and inanimate alike). Nominals of this type were selected since they are commonly extended with numerals, adjectives, and demonstratives.' 2.1 Simple Nominals The use of classifiers is obligatory when a noun is accompanied by a numeral as shown in 2a) and its ungrammatical counterpart in 2b). 2 2a) n6k slam tua bird three clf "three birds" 2b) *nizik scam bird three The use of classifiers is optional when a noun is accompanied by an adjective or a demonstrative. However, if the noun refers to a specific object, a classifier must be present. By specificity, I refer to singularity and contrast in the presence of adjectives and emphasis in the presence of demonstratives.3 Thus when 'bird' is extended by 'little', [Ws] is obligatory if specificity is involved as in 3b), and omissible if it is not involved, as in 3a). 3a) nOlc lek bird little "little birds" 3b) n6k tua bird clf little "a little bird" 3a) refers to unspecified little birds while 3b) implies singularity and conveys a contrastive value of the adjective 'little', meaning a 'typical little bird' (distinct from a 'typical big bird').4 
It is well-known that the English auxiliaries are sensitive to the so called NICE (Negation, Inversion, Contraction, and Ellipsis) phenomena. Based on these empirical properties of auxiliaries, this paper argues for the existence of the construction aux-headph(rase) whose subtypes include negation-ph, inversion-ph, ellipsis-ph, vp-filler-ph, and the like. Each of this subtype has its own construction-specific constraints as well as those inherited from its supertypes. The present analysis uses grammatical constructions with declarative constraints and posits a rich network of inheritance relations among them. This enables us to provide a clean analysis for some of the puzzling phenomena in English such as negation, VP ellipsis, and VP fronting, while capturing new levels of generalizations among these seemlingly unrelated phenomena. 
This paper deals with cross-linguistic variation in WH-pied-piping. In this paper, I claim that pied-piping is the result of the intermediate stages of WH-movement (i.e., the indirect feature checking movement of Chomsky (1998)) to the Spec of XP and the subsequent percolation of the WH-feature to the XP and that cross-linguistic variation in pied-piping can be explained in terms of various universal and language-specific syntactic constraints and properties. Crucially, I claim that the overt/covert nature of indirect feature checking movement could vary independently from that of direct feature checking movement. This means that in principle, there can be four different types of languages in terms of WH-pied-piping. Specifically, I show that Basque and Imbabura Quechua are languages where both the direct and indirect WH-feature checking movement are overt while WH-in-situ languages like Sinhala and Korean are those where both the direct and indirect WH-feature checking movement are covert. As for the languages where the indirect WH-feature checking movement is covert while the direct WH-feature checking movement, is overt, I claim that English is such a language and show how various facts of pied-piping in English can be explained in a principled manner if we assume this. Finally, concerning the languages where the indirect WH-feature checking movement is overt while the direct WH-feature checking movement is covert, I speculate that a potential candidate for this type of language is Slave, where it was claimed that partial WH-movement is possible without an overt scope marker. 1. Introduction: Cross-Linguistic Variation in Pied-Piping 
The most perspicuous phenomenon that demonstrates the head-final property of Japanese is the sentence-final clusters of auxiliary verbs such as (s)ase and conjunctive particles nagara. Although they are related to the hierarchical clause structure that has been discussed in the literature of Japanese linguistics, the hierarchical complexities have also been one of the major causes of the failure of long sentence parsing in the field of natural language processing. To overcome this parsing problem, we develop Japanese Phrase Structure Grammar, NAIST JPSG, which is a partial implementation of ideas from not only recent developments in Headdriven Phrase Structure Grammar but also Minami's four-level syntactic/semantic hierarchy. We devote our discussion to the analysis of hierarchical clause formation, pseudo-lexical treatment of causatives and regulation of co-occurrence of subordinate clauses, with main focus on their specific lexical information of the sentence-final clusters. Causative constructions exhibit properties of simple/complex thematic relation in spite of their morphologically simple status. A clause can be embedded as subordinate clause if and only if the clause is a member of the same or lower level in Minami's sense than the main clause belongs to. These hierarchical clause phenomena can be explained by introducing lexical description and general mechanism assumed in JPSG.  
 This paper proposes a new analysis of Internally-Headed Relative Clauses (IHRC) in Japanese in Head-driven Phrase Structure Grammar (HPSG) which defines IHRC in terms of the feature specification of the main predicate. IHRC is characterized as a special type of headcomplement structure, in which the predicate has highly marked ARG-ST and VAL features corresponding to IHRC. Motivated by diachronic and synchronic data, the proposed analysis also suggests a way to accommodate the constructional elusiveness of IHRC.  
(1) Nominal GP a. Brown's deft painting of his wife is impressive. b. The tapping on the floor was very irritating. c. John's continuous tapping on the floor annoyed everyone. d. There is no difference any more, no checking of passports, Verbal GP e. Brown's deftly painting his wife is very impressive. f. Mary is well known for singing songs (gracefully). g. John dislikes Linda trying to tell him a lie. h. John was angry at Linda's trying to lie to his brother. i. His not having left yet could be a sign of his reluctance. Many researchers concentrate on verbal type gerunds that are exemplified by ( 1 e-i) and tend to exclude the nominal type listed in (la) through (1d). This paper will attempt to provide a 325  comprehensive analysis to the both types of gerunds observing endocnetricity, and will account for the distribution problem without creating a separate syntactic category'. As is pointed out by Pullum (1991) and many others, the two types of gerunds have some of the characteristics as shown in (2)  (2) Properties unique to verbal GPs  • can take an ordinary NP objects as complements - (le), (10, (1g)  • can contain adverbial modifiers  -- (le), (10, (1i)  • can contain negating particle not  -- (li) (cf. (1d))  • allow some auxiliaries like 'have'  -- (1i)  • The logical subject can be a genitive or an accusative NP  Properties unique to nominal GPs • can take PP objects as complements • can contain adjectival modifiers • contain determiners including no • cannot contains any auxiliaries2 • the logical subject can be a genitive NP  -- (la), (lb), (lc) — (1c) -- (lb), (1d)  2. Distributional Properties and Endocentricity As stated in the first section, there is difficulty in classifying gerund phrases respecting the pre-established system of grammar. For instance, (1g) and (lh) could be critical cases if the GPs in these sentences are categorized as a kind of VP since it is usually assumed that verbal categories do not follow a preposition distributionally but nominal categories do. The dilemma lies in the internal syntactic facts of these GPs that seem to indicate that these are verbal categories. In this section, I will challenge the assumption that verbal categories do not follow a preposition. In doing so, I will try to preserve endocentricity intact. This approach is different from any previous treatments in that endocentricity is strictly obeyed and no new syntactic categories are created for this construction only. 2.1. Endocentricity and Internal Syntax One basic principle in determining the categorical status of GPs is to let the internal properties of a phrase decide the whole syntactic character of the phrase. First consider the examples in (3). ). (3) a. Brown's deft painting of his wife is impressive. (=la) a'.*Brown's deftly painting of his wife is impressive. b. Brown's deftly painting his wife is impressive. b'.* Brown's deft painting his wife is impressive.  
We describe an N-gram based syntactic analysis using a dependency grammar. Instead of generalizing syntactic rules, N-gram information of parts of speech is used to segment a sequence of words into two clauses. A special part of speech, called segmentation word, which corresponds to the beginning or end symbol of clauses is introduced to express a sentence structure. Segmentation words for each clause were learned using the hill climbing method and a small bracketed corpus. Experimental results for Japanese sentences showed that N-gram based syntactic parser achieved 72.2% recall, which is about the same level of performance as a probabilistic context-free grammar based parser with human-made language-dependent information.  
With treebanks becoming available for more and more languages, their usage for the development of natural language parser has become a topical issue in NLP. This paper tries to give a new spin to this this stream of research, proposing a new direction in corpus-based parsing. Contrary to competitive approaches, this approach does not involve a chart parser which reassembles phrases extracted form a treebank. Instead, parsing proceeds via the extraction of example trees from the treebank using fuzzy pattern matching techniques. A set of adaptation rules modify the extracted example trees so as to produce the best possible parse given the current set of examples. 
Abstract Two methods for stochastically modelling bidirectionality in chart parsing are presented. A probabilistic islanddriven parser which uses such models (either isolated or in combination) has been built and tested on wide-coverage corpora. The best results are accomplished by the hybrid approaches that combine both methods. 
We de ne a new model of automata for the description of bidirectional parsing strategies for tree adjoining grammars and a tabulation mechanism that allow them to be executed in polynomial time. This new model of automata provides a modular way of describing bidirectional parsing strategies for TAG, separating the description of a strategy from its execution.  
 In this paper, we introduce a novel MDL-based grammar learning algorithms, which can automatically induce a good amount of high quality parsing-oriented grammar rules from a tagged corpus with a minimal annotation. Comparing between the basic best-first MDL induction algorithm and a pseudo-grammar induction process, we identify problems associated with the current MDL-based grammar induction approaches. Based on this, we present a novel two-stage grammar induction algorithm to overcome a localminimal problem by clustering the left hand sides of the induced grammar rules with a classifier trained through a seed grammar. Preliminary experimental results show that the resulting induction curve is very close to its upper bound and outperforms the traditional MDL-based grammar induction.  1. Introduction  With the increasing demand for natural language processing in the various Internet applications, such as automatic speech recognition and dialog systems, the acquisition of large amount of high quality grammar rules become more important. The availability of hand-annotated corpora, such as Penn Treebank Project, offers the possibilities for overcoming this knowledge-engineering bottleneck. However, the parsers based on such grammar rules have the risk of becoming too tailored to these labeled training data so as not be able to reliably process sentences from other domains. To parse sentences from a new domain, one would then try to obtain a new set of grammar rules from that domain, which often would require hand-parsed sentences for the new domain. Because to (semi-)manually parse a large corpus is both a labor-intensive and a timeconsuming task, it would be beneficial to automatically derive grammar rules from raw text data from that domain with minimal annotations. In this paper, we report our ongoing research work in automatic grammar acquisition within the minimal description length (MDL) [ Ris78, Ris89] paradigm, together with contextual distribution classification to infer the LHS of those induced rules. Particularly, we want to address three MDL-related grammar induction issues: 1) What problems do current MDL-based grammar induction approaches have? 2) What MDL values may we obtain using the basic MDL induction approach when both the grammar rules and their application order is known? This way, we can have a good idea about the upper bound for the basic MDL-based grammar induction. 3) Are there any new approaches that can lead to a performance close to that upper bound? To answer these questions, we conducted a set of experiments that compare the induction curves under different settings using both automatically induced grammar rules and hand-annotated rules in Treebank.  The results show that the MDL principle alone induces reasonable phrase grammar rules at the beginning, but quickly leads to a local-minimal and most of induced rules then are not adequate. However, applying the rules from Treebank by MDL principle in bottom-up order shows monotonous and sharp decrease of MDL values, compared with the results from the basic MDL principle. We speculate that it may be due to the vagueness of the LHS symbols from the MDL principle alone, and therefore, we improve our algorithm to determine LHS using distributional classification. The experimental results show that the new approach is very close to the hand-annotated rule induction approach in term of MDL values. The rest of paper is organized as follows: section 2 presents the MDL principle, with an emphasis on description length gain (DLG), described in [Kit98] following classic information theory [Shannon49, Cover&Thomas91]. The next section presents two grammar induction strategies, i.e. the basic induction algorithm, which aims to find optimal grammar rules from the scratch with the guide of MDL principle alone, and a two-stage improved induction algorithm that first explores the context distribution of five syntactic categories from some “seed grammar”. During the next induction stage, those induced grammar rules are dynamically classified as one of the five categories to avoid the deficiency of MDL-based search strategy. Section 4 reports experimental settings, results and algorithm evaluation. Section 5 reviews previous MDL-based research on grammar learning and gives our conclusions. 2. Grammar induction by MDL Principle Grammar induction can be viewed as a process that searches for the best grammar in a predefined grammar (or hypothesis) space. If a set of permissible rules or rule formats, e.g., context-free grammar (CFG) rules, are given, it is widely adopted to use the Baum-Welch (or forward-backward) algorithm and its extension, the inside-outside algorithm [Baker79, Lari&Young90], to estimate the probabilistic parameters for these grammars. Essentially, there are two sub-tasks in obtaining a CFG rule. One is to decide what the right-hand terminals or non-terminals should be, and the other is to decide the left-hand symbol (LHS).  2.1. MDL Principle  Researchers have proposed various techniques and criteria to constrain the grammar space and to guide the  search process. For example, [Hol75] used genetic algorithm and [KVG83] applied stimulated annealing  algorithm to facilitate the search process. However, at the core of the search process, the goodness criterion  for search is a critical issue, this is because it tells which grammar rule is better. Among those approaches,  the minimal description length (MDL) principle, which is based on the classic and algorithmic information  theory [Shannon49, Solo64, and Kol65], has received a wide attention.  For any given set of data, i.e. legal sentences, there are usually multiple theories (i.e. a set of grammar  rules) that can account for the data, and we need to decide which one to select. An often used principle is the  Occam’s razor principle, which states that given a choice of the theories, the simplest is best. There are two  aspects associated with the simplicity. One is that how simple is the theory describes the data, and the other  is that how simple is the description of the theory itself. There is clearly a tradeoff between these two aspects.  [Ris78] formalized this as follows: given some data D, we should pick that theory T which minimizes. That  is:  L(T) +L(D|T)  (1)  where L(T) is the number of bits needed to minimally encode the theory T, and L(D|T) is the number of  bits needed to minimally encode the data D given the theory T.  From Shannon’s information theory [Shannon49], we know that if we have a discrete set X of items with a probability distribution P(x), then to send a message identifying x∈ X, we need approximately L(x) = -  log2(P(x)) bits. In other words  P(x) = 2 –L(x)  (2)  This enables us to interpret the MDL principle in Bayesian framework. From the equation it can easily be  seen that minimizing L(T) + L(D|T) corresponds to maximizing P(T)*P(D|T) and hence P(T|D). This shows,  theoretically, searching for the most likely theory for a given data in a Bayesian modeling framework is  equivalent to searching for a model with the minimal description length.  It should be noted that the MDL principle enables us to assign prior probabilities to items in a meaningful  way, even if we do not really have enough prior knowledge. We can do this through minimal length encoding  for the items.  2.2. Description Length Gain  The application of MDL is independent of encoding scheme [Ris89]. To calculate the description length  L(T) +L(D|T), what we need is an ideal encoding scheme, instead of a real compression program. This can be  formulated in terms of token counts in the corpus as below for empirical calculation [Kit 98], following  classic information theory [Shannon49, Cover&Thomas91]:  ∑ ∑ ∧ DL( X ) = n H ( X ) = −n  ∧  ∧  p(x) log p(x) = −  c(x) log c(x)  x∈V  x∈V  |X |  (3)  where V is the set of distinct tokens (i.e. the vocabulary) in corpus X and c(x) is the count of x in X. Accordingly, the description length gain of selecting the substring xixi+1…xj (denoted as xi…j, i < j) as possible RHS candidate of a grammar in a given corpus is defined as  x x x DLG ( ∈ X ) = DL ( X ) − DL ( X [r → ] ⊗ )  i... j  x ... j  i ... j  (4)  x where X [r →  ] x... j  represents  a  resultant  corpus  from  the  operation  of  replacing  all  instances  of  xi…j  with r throughout X, and ⊗ denotes a string concatenation operation with a delimiter inserted between its two operands, the current corpus and newly learned phrase xi…j. It is worth to note that we can choose the substring with maximum DLG value at each iteration without carrying out the real string substitution throughout the original corpus. The calculation is based on the token count change involved in the substitution to derive the new corpus. After finding the substring with maximum DLG, we replace the substring with a new string r in the original corpus. Another problem is that we need to derive the count of x, for all possible sub-strings x in the corpus X, because during the induction process, it is necessary to consider all segments (i.e. all n-gram) in the corpus in order to select a set of good candidates. For one thing, MDL principle itself prefers short grammar rules over long grammar rules and long rules normally occur less frequent than short rules and hence less possible to become good grammar rules in the induction. In addition, it is too computationally expensive to consider each possible of these n-grams at every point in the search. Therefore, we use only bi-gram and tri-gram in the induction process. However, we will consider two cases: using bi-gram and tri-gram RHS with automatic MDL principle alone, and using the same number of RHS hand-annotated rules.  3. Learning Strategies 3.1. Basic Induction Algorithm  Accordingly, the best first learning algorithm using the goodness criterion is illustrated in figure 1. Given an utterance U = t0t1…tn as input string of some linguistic token, e.g. part-of-speech tags, the unsupervised grammar induction looks for the substring with maximum description length decrease, i.e. maximum DLG, at each iteration and then replaces the n-gram (bi-gram or the tri-gram in our work) with a random symbol in the whole corpus, at the same time, output the learned rules in this iteration. It loops until description length value does not decrease, or DLG have a zero or negative value. 1. set k = 0 and extract all 2-gram and 3-gram in Xk with their counts; 2. for every n-gram (n = 2,3) in Xk, examine: (a) If no more xij (2 ≤ j-i ≤ 3) that DL (Xk[r-> xij ]) < DL(Xk), output the phrase and exit; (b) Else rk = argmax ∆DL (Xk[r-> xij ]); 3. Xk+1 = Xk[rk -> xij], output the rk, go to step 2; Figure 1 Basic MDL Induction Algorithm 3.2. Pseudo-Grammar Learning by MDL Principle One may see that the learning algorithm may not reach the real shortest description length, since it is a best first strategy that stops at local minima. To evaluate if the MDL principle is applicable for those handannotated rules in Treebank corpus, we implement a pseudo-grammar induction algorithm to gain insights. Figure 2 outlines this pseudo-grammar induction algorithm. 1. Extract all hand-annotated grammar rules from Treebank corpus, sort those rules according to bottom-up parsing order, mark grammar rules as “hidden”, except for leaf grammars in the every sentence tree and add them to the rule pool; 2. For all rules in the rule pool, apply step 2 and 3 of basic induction algorithm in figure 1 in each run to choose the rule with maximum DLG; 3. Output the learned grammar and apply it in order to involve more higher level grammars, that is , if all the children of one grammar are applicable, the grammar can be marked as “visible” and hence add to the rule pool; 4. Go to step2 if there are rules in the rule poor. Figure 2 pseudo-grammar induction algorithm In the algorithm, by simple extracting all hand-annotated rules from the corpus, the rule form (RHS and LHS) and the rule application order are predetermined based on the parse trees. We only use MDL principle to pick a rule in the current step so to get maximum description length gain. When all the child rules are selected and applied, their parent rule will be considered subsequently. Thus, we apply these hand-annotated rules roughly in the bottom-up parsing order, guided by the MDL criterion. From this experiment, we want to figure out how the basic induction algorithm differs from the pseudo-induction, where the rule and the order of the application are already known, under the same criterion. In addition, through this experiment, we try to find an upper bound for MDL-based grammar induction.  NP->DT JJ NN NP->JJ NN NP->DT CD NNS ADJP->RBS JJ VP->TO VP  NP->DT JJ NNS NP->NNP POS NP->NP NP ADJP->RBR JJ VP->MD VP  NP->DT NN NP->DT NN POS NP->NP CC NP PP->IN NP VP->VBD NP  NP->DT NNS NP->JJ JJ NN ADJP->RB JJ PP->TO NP VP->VBZ NP  NP->PRP NP->JJ JJ NNS ADJP->RB JJR PP->IN S S->NP VP  Table 1 the Seed Grammar Rules  NP->JJ NNS NP->DT NN NN ADJP->RB JJS VP->VB NP S->PP NP VP  3.3. MDL Induction by Dynamic Distributional Classification Comparing the results of the two experiments, we discover that the basic MDL induction algorithm alone does infer reasonable phrasal grammar rules at the beginning, but after getting about a hundred of rules, it quickly reaches a local-minimum and most of the induced rules are not adequate. We suspect that it may be due to the random labeling of LHS for those induced rules, because if all the LHS symbols of the induced rules are different, the repetition of certain patterns becomes less and therefore its MDL value decreases less dramatically. Based on this observation, we come up with a new algorithm. We decide to integrate some linguistic information into the search strategy, which tries to classify the LHS symbols of the induced rules, using distributional analysis, and to help the search process to infer more syntactic plausible rules. The algorithm is divided into two stages, one is the context vector training stage and the other is an improved MDL induction process. The context vector training algorithm is based on the assumption that similar grammar rules tend to occur in similar contexts. The contexts of the rules from “VP” category, for instance, differ greatly from those of the “NP” rules. If the context is restricted to a fixed sliding window (e.g., three part-of-speech tags in our work, on either side of rules), then we can define the context distribution over all rules in that syntactic category. The context distribution of one category can be estimated from the observed contexts of sample sentences in category. In the next MDL induction stage, we measure the similarity of each MDL induced rule to the center of context vectors of each syntactic category using Kullback-Leibler (KL) divergence as the distance function and assign to the LHS of the rule the category with the shortest distance. At each iteration, we also dynamically update the contexts and their centers of the induced rules for every syntactic category. For simplicity, the syntactic categories are limited to five non-terminals, i.e., “NP”, “VP”, “S”, “ADJP”, and “PP”, which are main syntactic components in the syntactic parsing. We take, as “a seed grammar”, a set of most frequently used grammar rules for each of 5 syntactic categories and analyze the contexts (3 left and 3 right part-of-speech tags) for each of those rules in the sample corpus. The thirty seed rules we used in the algorithm are illustrated in Table 1. Note that we not only use the base grammar rules, i.e. the rules at the leaves of the parsing tree, but also the ones at the upper levels, since exploiting the context of these rules on the fly will make the search process more robust. Another critical issue concerning the selection of the seed grammar rules is to decide the number of rules for each category. The ratio we choose is roughly the same ratio for these five categories in the training corpus. In addition, we discover that “NP” rules alone account for two thirds of all rules in the hand-annotated Treebank corpus and they dominate other kind of rules especially in the bottom level of parsing trees. Therefore, we choose many “NP” rules as seed grammar rules.      QrqB hh Ã D qp v  7hv pÃ H9GÃ D qp v                                        Figure3 The Two Grammar Induction Curves (MDL Value vs. Rules Induced) 4. Experiments and Results A number of preliminary experiments on unsupervised phrase and lexical learning have been conducted on parts of Treebank corpus. These experiments show promising results by DLG measure [Kit 98]. It shows certain ability to capture the regularities in the data. Since it takes a learning-via-compression approach, i.e. MDL principle approach, the result is a set of deterministic CFG rules. We perform four experiments and all of them use 2,500 sentences extracted from Treebank corpus with hand-annotated part-of-speech tag for each word as input. Backing off to POS tags is necessary because it alleviates the sparse data problem. 4.1. Experiment 1: Basic MDL Grammar Induction The first experiment is the basic MDL principle induction. The testing corpus contains 2,500 short sentences and the vocabulary set is made up of 32 POS tags, a subset of 47 tags used in Treebank corpus. We apply MDL principle on the grammar space, where the RHS of a CFG is bi-gram or tri-gram. The first thirty of induced rules are given in appendix A. From the appendix, we can see that most learned grammar rules are reasonable, such as [NNP NNP], [TO VB] and [MD VB]. However, some other rules seem to be quite “flat”, i.e. lack of internal structures of the rule. The rule [PRP RB VBD], for instance, should be broken down into [PRP [RB VBD]]. In addition, we plot MDL value curve to show the MDL decrease trend along the search procedure. The curve is given in figure 3. It is clear that having induced about a hundred rules, the basic MDL induction algorithm reaches the local minimal quickly. For comparison and verification of whether the MDL principle is useful for real world data, we perform another experiment using the pseudo-induction algorithm in subsection 3.2. 4.2. Experiment 2: Learning by Pseudo-Grammar Induction  In this experiment, we use the same search strategy but to the manually annotated rules found in Treebank corpus. The search process only chooses the rule with the maximum description length decrease, while the rule forms are all predetermined in advance. The MDL curve is illustrated in the figure 3. The detail of the algorithm is described in section 3.2. From the figure, we observe that although the two curves are very closely to each other in the beginning, they differ greatly in the whole process in that: firstly, no local minimal is found in this case, while the basic induction process quickly becomes flat and has to be terminated because it is too computational expensive to infer new rules – large amount of randomly selected LHS of the induced rules lead to a very sparse distribution. Secondly, for the pseudo-grammar induction case, the curve decreases irregularly, that is, a few “critical” rules make the description length drop dramatically than the rest of them. We still work on understanding the effects. To verify if it is just a special case for pseudo-grammar induction, we perform similar experiment on different sentence sets, illustrated in figure 4. The three curves are obtained by applying pseudo-grammar induction algorithm on different number of sentences extracted from Treebank corpus, namely 2,500, 5,000 and 10,000 sentences respectively. The chart shows the consistency among varied numbers of sentences. Figure 4 Different Data Set used in Pseudo-Grammar Induction Experiment 4.3. Experiment 3: MDL Induction by Dynamic Distributional Classification In this section, we give experiment results of the two-stage induction algorithm described in section 3.3. The goal of the experiment is to incorporate some linguistic knowledge into the search process to get more syntactic plausible grammar and overcome the local-minimal problem. After training the classifier on 1,000 POS tagged sentences using the seed grammar rules, we obtain the context vector centers for all the 5 syntactic categories. Then, we construct the induction sets using a different set of 2,500 sentences (which is the same as the previous experiements). The curve labeled with “MDL Induction by DDC” in Figure 5 summarizes the outcome of the experiment. From the chart, we see that it decreases monotonically, however, with no local minimal found at this time in the curve; the search process repeats until no rules can be induced by MDL framework. In addition, the curve is very close to the pseudo-grammar induction case, i.e. the upper bound of the algorithm. This algorithm learns not only the right-hand sequences of terminals/non-terminals, but also their LHS syntactic categories.  We also conduct another experiment to see the effect when both the classification of syntactic category and the MDL induction are accurate, that is, assume the syntactic category of every induced rule is correctly identifiable using extra knowledge, and also assume all induced rules are subset of the grammar rules found in the hand-annotated Treebank corpus. We vary the experiment settings in this algorithm to explore the upper bound for experiment 3. Because many induced grammar rules, which reduce the description length dramatically, are not syntactic plausible rules and are not found in manual rule set, however, the algorithm assigns a syntactic category for them and updates the center of the context vector for that category in the search process. This, in turn, impacts on the classification in the later part of the process. Although we try to use several high-level grammar rules as seed grammar and explore their context in the induction process to compensate this effect, how to improve the robustness needs further investigation. Another major factor to certain poor performance is the limited number of syntactic category, (five in our work, but more than fifteen in the Treebank), and the restricted number of n-gram (many ‘flat’ grammar rules are found in Treebank [Gai95]) that we applied in the experiment all impact the induction procedure. To Figure 5 The MDL Curve investigate their impact, we loose the restriction on the limited number of LHS categories. In the each induction process, as described in section 2.3, we sort the candidate rules by their DLGs in a descending order, choose only the first rule found in hand-annotated Treebank grammar and apply it by replacing its RHS sequence with the correct LHS. In such procedure, no classification is performed, therefore the classifier is always assumed to be accurate, which removes the effect that grammar rules induced early influence the rules induced later. On the other hand, all rules learned are syntactic plausible ones, since they are subset of the manual-annotated grammar rules. The only difference between them is the number of the right-hand symbol and the syntactic category of the left-hand symbol. The experiment result is illustrated in the figure 5 with the label, “Simulated induction”. From the figure, we find that the different number of n-grams and the syntactic categories does affect the results, especially in  the later search process, when compared with the “Pseudo-Grammar Induction” curve. On the other hand, compared with “Grammar Induction by DDC” curve, the classifier is really distracted by the previously induced “bad” grammar rules. This is the place where future research work can be directed. In addition, we also calculate the precision and recall for those induced rules, in contrast with the handannotated grammar rules extracted from the same set of 2,500 sentences. The result is illustrated in table2.  Rules after  100  200  500  Precision  0.92  0.89  0.82  Recall  0.13  0.16  0.18  Table2 the precision and recall for induced rules  1000 0.74 0.22  5. Related Work and Conclusions  The difficulty of grammar induction depends greatly on the amount of supervision provided. [Charniak96], for example, has shown that a grammar rule can be easily constructed when the examples are fully labeled parse tree. However, if the examples consist of only raw sentences with no extra structural information, grammar induction is very difficult, even theoretically impossible [Gold67]. Part of our work explores the in-between case, where the category of learned rules could be decided by the result of a supervised learning algorithm. Second, the search criterion also impacts the induction process. Besides the MDL principle, there are other search criteria, similar to us, to guide the “guessing game”. Cook et al. [Cra76] explores a hill-climbing search for a grammar of a smaller weighted sum of grammar complexity and the discrepancy between the grammar and corpus; Brill et al. [BMMS90] derive phrase structures from a bracket corpus by generalized mutual information approach; and Brill and Marcus [BM92] attempt to induce binary branching phrases with distribution analysis using the information-theoretical measure divergence, derived from relative entropy. de Marcken gives an in-depth discussion on the kind of issues involved in the pure distribution analysis and on the disadvantages of the inside-outside algorithm for grammar induction in his PhD thesis [deMa95]. Recently, following Cook’s work, Stolcke [Sto94] worked under the Bayesian modeling framework, whereas Chen [Chen95, Chen 96] uses the universal prior probability p(G) = 2–l(G) for grammar induction. Their learning strategy reports to work well on small to medium size artificial corpora, using measures such as entropy, perplexity or likelihood. But, to our knowledge, no one has tried to induce all levels of syntactic grammar rules on large scale real corpora before. In the paper, we have shown two MDL-based grammar induction algorithms. Both of them try to infer syntactic plausible grammar rules for parsing with one focusing on a best-first search strategy with minimal supervision and the other focusing on integration of language constrains into the learning model. Comparing these two approaches through experiments, we show that MDL principle alone could induce phrase-level grammar well, but fails to learn high-level grammar rules. In addition, with integrated language constraints, the MDL principle could infer not only the grammar rules, but also the categories of the LHS of the learned rules. The experiments show that the result of the second algorithm is very close to that of the pseudogrammar induction algorithm. To further improve the grammar learning algorithms for high performance parsing, we still need to investigate the failed instances and come up with more sophisticated learning algorithms. Evaluating learned rules for parsing and further improving learning algorithms are the two main tasks in our future work.  Reference [Baker79] Baker, James K. "Trainable grammars for speech recognition". In Speech Communication Papers for the 97th Meeting of the Acoustical Society of America, edited by Jared J. Wolf and Dennis H. Klatt, 547--550, MIT, Cambridge, Mass, 1979 [BM92] Brill, E. and Marcus, M. Automatically Acquiring Phrase Structure Using Distributional Analysis, In proceedings of 1992 DARPA Speech and Language Workshop, Harriman, N.Y., 1992 [BMMS90] E. Brill, D. Magerman, M. Marcus, and B. Santorini. Deducing Linguistic Structure from the Statistics of Large Corpora. In Proceedings of DARPA Speech and Natural Language Workshop, Hidden Valley, Pennsylvania, June 1990 [Charniak96] Charniak, E. Tree-bank grammars. In Proceedings of the National Conference on Artificial Intelligence (AAAI), 1996 [Chen95] S.F. Chen. Bayesian grammar induction for language modeling. Technical Report TR-01-95, Harvard University, Center for Research in Computing Technology, January 1995. [Chen96] S. F. Chen. Building probabilistic models for Natural Language. PhD thesis, Harvard University, Cambridge, Massachusetts, Cambridge, Mass., 1996 [Cover&Thomas91] Cover, T. and Thomas, J. Elements of Information Theory. John Wiley & Sons, New York, NY, 1991 [Cra76] Craig M. Cook, Azriel Rosenfeld, and Alan R. Aronson. Grammatical inference by hill climbing. Information Sciences, 10:59—80, 1976 [deMa95] C. de Marken. The Unsupervised Acquisition of a Lexicon from Continuous Speech. Technical Report A.I. Memo No. 1558, AI Lab., MIT, Cambridge, MA, November, 1995 [Gai95] Robert Gaizauskas. Investigations into the grammar underlying the Penn Treebank II. Research Memorandum CS-95-25, University of Sheffield, 1995 [Gold67] E. M. Gold. Language identification in the limit. Information and Control, 10:447--474, 1967 [Hol75] J. Holland. Adaptation in Natural and Artificial Systems. MIT Press, 1975 [KVG83] Kirkpatrick, S., Gelatt, C. and Vecchi, M., "Optimisation by Simulated Annealing," Science, No. 220, pp.671-680, 1983 [Kit98] Kit, C. A goodness measure for phrase learning via compression with the MDL principle. In The ESSLLI-98 Student Session, Chapter 13, pp.175-187. Aug. 17-28, Saarbruken, 1998 [Kol65] Kolmogorov, A. N. Three approaches to the definition of the concept "quantity of information." Problemy Peredachi Informatsii 1, 3—11, 1965 [Lari&Young90] K. Lari and S. J. Young. The Estimation of Stochastic Context-Free Grammars Using the Inside-Outside Algorithm. Computer Speech and Language, 4:35--56, 1990 [Ris78] Rissanen, J. Modeling by shortest data description. Automatica, 14:465--471, 1978. [Ris89] J. Rissanen. Stochastic Complexity in Statistical Inquiry. World Scientific Publishing Company, 1989. [Shannon49] C. Shannon and W. Weaver. The mathematical theory of communication. University of Illinois Press, 1949. [Solo64] R. J. Solomonoff, "A formal theory of inductive inference," Inform. and Control, vol. 7, pp. 1-22, March 1964; pp. 224--254, June 1964. [Sto94] A. Stolcke. Bayesian Learning of Probabilistic Language Models. Dissertation, U. California, Berkeley, 1994.  Appendix A The first 30 grammar rules learned by basic MDL grammar induction algorithm from 2,500 sentences in the Treebank corpus are given below. The rules are marked as true (t), false (f) and unsure (u) respectively according to human evaluators. There are four columns in the table, namely, rule number, the current description length with model and data combined when the grammar rule is acquired, the rule and the evaluation flag. The POS tags in these rules are listed below.  
Automatic acquisition of the prosodic phrase boundary detecting rules from the text and speech corpora has always been a difficulty for TTS systems. We collected over 5,000 sentences as the corpus, introduced a method based on the transform-based error -driven learning to get the rules for detecting prosodic phrase boundaries, and then used trees to organize the rules in the TTS system. For using the transformation -based error -driven learning, we designed a set of templates especially. Using 1,000 sentences to get rules for the TTS system can reach 92% accuracy in close -test and 73% accuracy in open-test. 
 **Department of Psychology Carnegie Mellon University Pittsburgh, PA 15213, USA  {sagae, alavie}@cs.cmu.edu  macw@cmu.edu  Abstract This paper discusses the process of parsing adult utterances directed to a child, in an effort to produce a syntactically annotated corpus of the verbal input to a human language learner. In parsing the Eve corpus of the CHILDES database, we encountered several challenges relating to parser coverage and ambiguity, for which we describe solutions that result in a system capable of analyzing almost 80% of the adult utterances in the corpus correctly. We describe characteristics of the language in the corpus that make this task unique, and present specific ways to deal with the analysis of this type of language. We discuss each step of the corpus analysis in detail, focusing on how selected techniques, such as part-of-speech tagging, rule-based robust parsing and statistical disambiguation, affect the trade-off between coverage and accuracy. Finally, we present a detailed evaluation of the performance of our system. A parsed corpus resulting from the research described in this paper is available to the research community.  
Since the early 90s, there has been a rising interest in applying probabilistic models in syntactic parsing, and significant progress has been achieved. The most influential work include, among others, research using probabilistic lexicalized context free grammars in the form of production rules (or similarly, tree-adjoining grammars and its derivatives), probabilistic dependency grammars; and probabilistic constraints-based grammars [Lafferty, J., Sleator, D., and Temperley, D., 1992; Magerman, D., 1995; Collins, M, 1997; Eisner, J., 1997; Chelba and Jelinek, 1997; Ratnaparkhi, A., 1999; Charniak, E., 2000]. Based on the discussion of different methods, we propose a new bottom-up link unification parsing algorithm in the link grammar framework together with a novel probabilistic model. In addition, we also try to provide some intuitive justification for the assumptions made in the probabilistic model. Initial results in terms of bracket recall and precision on the Penn TreeBank close-test set have reached beyond 91%, which shows that our proposed probabilistic model is leading to an encouraging direction. 1. Introduction Since the early 90s, there has been a rising interest in apply probabilistic models in natural language processing, particularly in syntactic parsing, and significant progress has been achieved. The most influential 
This paper describes a generic approach to compute dependencies from a variety of input ranging from raw text to syntactic trees. The dependency calculus is incremental and combines topological constraints on sub-tree patterns together with logical constraints de ned as Boolean expressions over the set of dependencies. This formalism has been successfully implemented and tested with a broad coverage grammar for French, and leads to computationally e cient and linguistically deep parsing. 
This paper describes an eﬃcient method of incremental dependency parsing based on phrase structure grammar with the dependency relation. The reachability relation between syntactic categories is utilized for connecting a head word with a dependent word simultaneously with the inputs. The method does not need to construct the whole parse tree of an initial fragment on the word-by-word basis, and thus can be expected to be usable for simultaneous spoken language processing. An experiment on the ATIS corpus has shown the technique of utilizing the reachability to be eﬀective for reducing processing time of the incremental dependency parsing. 
Research and Technology Center, Palo Alto, CA94304 {pcluk, hmmeng}@se.cuhk.edu.hk, fuliang.weng@rtc.bosch.com Abstract This paper begins by reviewing an effort in grammar decomposition (or grammar partitioning) for natural language to achieve modular parsing. We then propose a novel automatic approach for grammar partitioning, instead of the manual partitioning used in systems. Experiments are conducted with GLR parsing for the Wall Street Journal (WSJ) corpus in the Penn Treebank where single grammar GLR parsing is impractical. Our results show that the automatic grammar partitioning method based on the mutual information criterion fares better than a random partitioning method, and exhibits efficiency in parsing and high parse coverage. 
Abstract In practical applications, the speed with which grammars are loaded into a parser can be of prime importance, particularly when they have to be loaded dynamically. In this paper we address this issue by describing an extension to a bottom-up chart parser that enables it to directly process input utterances using context-free grammars represented in abbreviated form. As a result of this extension, the expensive process of eliminating abbreviatory notation can be avoided, leading to faster grammar loading and parsing times. Our parser is evaluated with respect to a number of realistic grammars of varying sizes, by comparing it with a parser that first expands these grammars during grammar loading. 
Faced with growing volume and accessibility of electronic textual information, information retrieval, and in general, automatic documentation require updated terminological resources that are ever more voluminous. A current problem is the automated construction of these resources from a corpus. Various linguistic and statistical methods to handle this problem are coming to light. One problem that has been less studied is that of updating these resources, in particular, of classifying a term extracted from a corpus in a subject field of an existing thesaurus. This experiment compares different models for representing a term from a corpus for its automatic classification in the subject fields of a thesaurus. Terms are first extracted from a corpus by combining a linguistic processing and statistical filtering The classification method used then is linear discriminatory analysis. The most effective model is the one based on mutual information between terms, which typifies the fact that two terms often appear together in the corpus, but rarely apart.  
The basic flow of input tokens is that instead of being directly moved onto the symbol stack, they are first shifted into the buffer queue, where they may be reduced to other symbols and may also result other sequential reductions in the buffer and/or on the parsing stack. Symbols contained in the buffer are not limited to a fixed number, and will be finally shifted onto the symbol stack, where they are to be reduced to other symbols. Since the parser can look ahead any number of symbols before making shift/reduce decisions, and make reductions at tow separate nodes in the parse tree, it gets more control to handle conflicts and ambiguities. * This work is supported by National Natural Science Foundation of China, project No. 69973005. 
This paper presents a data-driven lexicalist framework for machine translation based on alignment. In addition to word alignment that provides the word correspondences of source and target language, the system uses classifications of the correspondences based on superlags. We lirsL give an overview of the framework, pointing out its fundamental concepts and then report some results from a pilot implementation and evaluation on the ATTS domain. 1. Introduction Automatic word alignment systems have reached a level of pe1t'ormance that make them useful for a number of applications including machine translation. A problem with many current systems, though, especially when applied in the construction of machine translaLion systems, is that they align smface strings with no information on properties such as part-or-speech or sense. Moreover, Lhey do not provide information on the context of a word correspondence. Unless Lhis inrormaLion is provided somehow, it is impossible to select the correct alternative translation(s) for a given context. To illustrate, consider the following extract from a (constructed) parallel text: (a) They will win. De kommer att vinna. (b) Did they win? Vann de? (c) They did win. De vann faktiskt. (d) They never ,vin. De vinner aldrig. 
 When the VISL project started in 1996, its primary goal was to further the integration  of IT tools and IT based communication routines into the university language  
We will in this paper present an evaluation1 of how much stemming improves precision in information retrieval for Swedish texts. To perform this, we built an information retrieval tool with optional stemming and created a tagged corpus in Swedish. We know that stemming in information retrieval for English, Dutch and Slovenian gives better precision the more inflecting the language is, but precision depends also on query length and document length. Our final results were that stemming improved both precision and recall with 15 respectively 18 percent for Swedish texts having an average length of 181 words. Keywords Stemming, Swedish, Information Retrieval, Evaluation 1. INTRODUCTION Stemming is a technique to transform different inflections and derivations of the same word to one common "stem". Stemming can mean both prefix and suffix removal. Stemming can, for example, be used to ensure that the greatest number of relevant matches is included in search results. A word's stem is its most basic form: for example, the stem of a plural noun is the singular; the stem of a past-tense verb is the present tense. The stem is, however, not to be confused with a word lemma, the stem does not have to be an actual word itself. Instead the stem can be said to be the least common denominator for the morphological variants. The motivation for using stemming instead of lemmatization, or indeed tagging of the text, is mainly a question of cost. It is considerably more expensive, in terms of time and effort, to develop a well performing lemmatizer than to develop a well performing stemmer. It is also more expensive in terms of computational power and run time to use a lemmatizer than to use a stemmer. The reason for this is that the stemmer can use ad-hoc suffix and prefix stripping rules and exception lists while the lemmatizer must do a complete morphological analysis (based on a actual grammatical rules and a dictionary). Another point of motivation is that a stemmer can deliberately “bring together” semantically related words belonging to different word classes to the same stem, which a lemmatizer cannot. A problem concerning stemming is the issue of overstemming. If the stemmer removes too much in its quest for the stem the result is that, morphologically or semantically, unrelated words are conjoined under the same stem. For example, if both the words tiden (“time”) and tidning (“newspaper”) are stemmed to tid, a search for tidning also would return documents containing tiden. The graveness of this problem depends on both the set of stemming rules and the document collection 
The purpose of the study reported in this paper is to investigate how local focusing structure, analysed in terms of Centering Theory (Grosz, Joshi & Weinstein, 1995), and global d iscourse structure, analysed in terms of discourse segments and discourse segment purposes (Grosz & Sidner, 1986), interact. Swedish dialogue was analysed according to Centering Theory and Grosz and Sidners (1986) discourse theory. The results indicate an interaction between locally implicit elements and global intentions. Also indications concerning discourse markers varying intonation were found. Introduction Discourse can be described as built up from discourse building blocks called discourse segments (hereafter DS). These DS are the units for forming a hierarchical discourse structure. They are described in e.g. Grosz & Sidner (1986, hereafter G&S) where claims are made about the use of the DS in the global discourse structure as well as their connect ion to the coherence of the discourse, and in Centering Theory (Grosz, Joshi & Weinstein, 1995, hereafter CT), where claims are made about the internal structure and coherence of the DS:s. Grosz & Sidner (1986) have applied their discourse theory to both argumentative text and task oriented dialogue, while CT traditionally has been applied to narrative text. In recent times, however, an interest for applying CT to dialogue has arisen, and some attempts to do that has been carried out (e.g. Brennan, 1998, B yron & Stent, 1998, Eckert & Strube, 1999). In applying G&S theory, a problematic point is the importance of speaker intention, which governs both the discourse segmenting and the discourse structure. It is unclear whose perspective, that should be taken; the speakers original intention, the listeners understanding of the intention or the analysers interpretation of the intention. However, one thing that is for sure is that the analyser will certainly face a challenge if attempting to find out original speaker/listener intentions. The major problematic issues in applying CT the issue of both utterance segmenting and discourse segmenting effects almost all other aspects of the analysis. Another problem with CT is to decide what concepts that are accessible, or realised, in an utterance. The pilot study presented in this paper is of explorative character, and addresses a range of problems encountered in a combined G&S-type analysis and CT analysis of task oriented dialogue, i.e.: • Utterance segmenting, i.e. the units between which local coherence is computed • Discourse segmenting, i.e. larger constituents affecting the global discourse structure. These segments correlates with what Carletta et al. (1997) calls "game". • What items that are possible centers • The CT notion of a realised item The aim of this paper is to give a picture of how those problems are connected to each other, and to outline how to refine a multiple -level analysis. It is also an attempt to apply a global and local analysis to spok en language data, and to give account for specific problems that arises by such an analysis. It is the hope of the author that results from investigations like this should help to develop e.g. instructions for more extensive investigations in the field.  
2. Statistical measures and other grammar checkers The Norwegian grammar checker has been developed using Constraint Grammar rules. This method has previously been used to develop a Swedish grammar checker (see Birn 2000, Arppe 2000). The resulting precision for this grammar checker is reported to be 70% (according to Birn 2000:38), counted as good alarms divided by the sum of good alarms and false alarms. The resulting precision for the Norwegian grammar checker is 75 % from a test corpus of 890 000 words from the newspapers Nordlys and Sarbsborg Blad. However, depending on what kind of text that is used as a test corpus and what kind of rules that are included, these numbers can vary a great deal. For example, if we include a rule that tests whether there are any finite verbs in a sentence, and apply the grammar checker on newspaper text with a lot of verb-less headlines, the precision rises to approximately 91 % (and the same goes for the Swedish grammar checker). Online Proceedings of NODALIDA 2001  3. The structure of the grammar checker 
This paper describes the automatic building of a corpus of short Swedish news texts from the Internet, its application and possible future use. The corpus is aimed at research on Information Retrieval, Information Extraction, Named Entity Recognition and Multi Text Summarization. The corpus has been constructed by using an Internet agent, the so called newsAgent, downloading Swedish news text from various sources. A small part of this corpus has then been manually tagged with keywords and named entities. The newsAgent is also used as a workbench for processing the abundant flows of news texts for various users in a customized format in the application Nyhetsguiden. Keywords News text, Corpus, Swedish, Internet Introduction Two years ago we built an automatic text summarizer called SweSum (Dalianis, 2000) for Swedish text. We wanted to evaluate SweSum but there were no tagged Swedish corpus available to evaluate text summarizers or information retrieval tools processing Swedish as it is for the English speaking community, mainly through the TREC, (Vorhees & Tice 2000), MUC and TIPSTER-SUMMAC evaluation conferences (Mani et al. 1998, Krenn & Samuelsson 1997). The purpose of this project1 was to construct test bed for new natural language technology tools, i.e. automatic text summarization, named entity tagging, stemming, information retrieval/extraction, etc. In the process of building this system, Nyhetsguiden (Hassel 2001), we also made it capable of gathering the news texts into a corpus, a corpus we have used to train and evaluate such tools as mentioned above. As this corpus is aimed at research on information and language technology applied on redundant text, the system does not, contrary to (Hofland 2000), remove duplicated concordance lines 1. Nyhetsguiden – A user centred news delivery system The system has a modular design and consists of three parts, the user interface, the user database and the main application, newsAgent. Being modular, the system can be run as a distributed system or on a single web server. When run as a distributed system, at least newsAgent must be run on a computer with Internet access. The user interface (Nyhetsguiden) and the user database can reside on either an Internet or Intranet capable server depending on the desired public access to the system. newsAgent is the core of the system and is basically a web spider that is run in a console window. The spider is implemented in Perl, which makes it platform independent, that is, it can run on any platform running Perl (Unix/Linux, Windows, Macintosh, BeOS, Amiga, etc). On intervals of 3-5 minutes newsAgent searches the designated news sources (Appendix A) for new news texts, that is news texts not seen by the system before. When a new news text is encountered it is fetched, the actual news text and accompanying illustrations are extracted (by removing navigation panels, banners, tables of links, etc). 
In this paper we consider the possible use of existing linguistic (mainly morphological) analyzers for written Finnish in order to create a system that uses speech as its interface. We also present means to enhance the usability of these analyzers in this respect. 
This paper presents a model for synergistic integration of multimodal speech and pen information. The model consists of an algorithm for matching and integrating interpretations of inputs from different modalities, as well as of a grammar that constrains integration. Integration proper is achieved by unifying feature structures. The integrator is part of a general framework for multimodal information systems with dialogue capabilities. Those parts of this framework that are relevant and affects the design of the integrator are also presented. 
If we conceive of a Constraint Grammar as an ordered sequence of transformation rules of a particular kind – as reduction rules rather than replacement rules – the transformation-based learning method used to train Brill taggers can, with minor modifications, be used to train Constraint Grammar taggers as well. This paper makes a few observations based on this approach, and presents some initial and rather promising experimental results. 
 Text  Title  No. of No. of Transl.  type  source target method  words words  User’s Microsoft Access 179,631 157,302 Human  Guide UG  User’s Microsoft Excel 141,381 127,436 Human  Guide UG  User’s IBM OS2 UG 127,499 99,853 TM  Guide  User’s IBM  69,428 53,619 TM  Guide InfoWindows UG  UGsueidr’es IUBGM Client Access 21,321 16,752 TM  Novel Gordimer: A  197,078 210,350 Human  Guest of Honour  Novel BJeerluloswal:emToand  66,760 65,268 Human  Back  Dialog ATIS dialogues 2,179 2,048 MT  ToanfhIerBxeeMtroa’sfTdttiohrmeatenatnsrlslaainotsinolanttoimotnhesem8wcoo0err5rype,u2ttosr7.oa7lns(7Tla3Mte2d),,6ww2i8hthichthegaavide  Guwctrhnhaiavcnoroeslavnelca,etatreoirotrritnshaestneodslc(asaNhetsiaeoerwnsawocmchtuoeaerrrtrcpihkesu-etosircr1ssitu9ehnc8oeht8fe)tad.trshaLenoTsTrhtlaCrettaa,inorosnganlnseaetta-itclooayonsrsukiielsdanisstebtodoaef  tfdtcsfwwrriroyeharoeialnstotalmanathlsttsaeifsfloatcunarthstnleotialeispcm-oathbssonaiilpsmoonttckhswfhdropeeobesrntlsurneecpseostchdaoumwtuhneusgcraoaercaanhlecnanysnetntesihhca,itasbeohnnlncieweaoandolmfmtgsnyatothusseaarhutioildgrsrsyerwgeetascaijteieeciunnttnsathdattstnsenee.letdaxaxaotIvnlttrtntyeaescaarsrlonrtyageigmhsnalssileivirdsaspstecetetaeontpoprseprraeehxrso(pnosse,tlfede)suwsgrx.eprh,onnifoacittatnnhhvacalgydleeees, 2 Step 1: Source and target texts TuUotinenshnxdineittevhsgp,meeiernntaLshddijii.oteeeneyr.nkp.iDttöeWlypytAnhi,oindVeitfngheEtcnhTltsuhteotrdloeuaytionrnrDlcasgbenlAoadstxVlaia,ottEteanidxofteCtonosvrooearllsnpoaaupnwlsedyedsfieersaxtsatttcrraogLasmceitntseeeksdpötafpderrioxaantmttgaes · Word type/token ratio · Sentence type/token ratio · Average number of words per sentence · Number of repeated sentences · Recurrent sentence rate Trehspeseectdivaetalyf,oarrtehleisstoeduricneTanabdltea2rgaentdte3xbtse,low. Tttnrsmassnacsr(iIhsAehBeeceeeoniuohxgacnrnnlendcMcvmcauyeuthtttcosoettreeetrbet“eimwmdleiennnvriA.srnevtaetescccopmruFiytlereneeeslaataenx,onstsssooercnrnti.rionfaismusisnwdtoibgveFe,rwstetielhehnorTEcxluhtemoleatitthucasrixcah.nrrenaoa”trmhabcidne(schsngrnteGlooecepaeesatltwemclsc)rslaIorrya;ec2tmtnieaeorrpren,eurbt,ftdlgauwheirooreelreuaktisteertedWasltheia,twsetpnttnshnsaeht,eile.egiiedtchoavuronnaxtieTlnnaaemhtuthyBnttrnefgigehettobagteidcunhhesehetrllaehersnehltexlirseottfiwmneetaatoicfwoxmidtlsscuotcoistfhtepti)hkhrtrrhwsheaneweireteecatslteoostcheoroposhoenuetns“igGnaearennI,lcmMtlt(utdddacloie13aketsbteetr81iibihinsthcrnnuvdcnp4.ehearpo1etoteieaotetmiawrtnnnronnhpesherraeoss.eneooedtsree”tis.trraeifved,psenltrOnlpsoeeeacd“ncoegiotlretfYretslfoweiuaagoentt1avennhtbtttxrsieetn8shhheveccoe)lasst.ydd4.leeeeeerst”ttt,  
In the future e-Home, information from various sources, located both globally and locally, are at hand for a wide range of tasks. Many of these tasks involve finding out about public authorities' rules and regulations. The Public Tax authorities, for instance, provide hundreds of documents on their web site (forms, FAQ’s, tax rules, etc.). Currently, the user is restricted to navigating and searching these information sources by clicking hyperlinks or typing in keywords in a search box. Suppose a citizen needs to know what the local tax in his area is. By providing the keywords “kommunalskatt” (local tax) and “Linköping” to the search engine five documents are retrieved and the user can continue clicking on the provided links to see if the answer is provided in the documents found. On the other hand, supposing that the user had the ability to state the information problem in natural language, and the background system was something more than just a document retrieval system, this interaction could look like the following: Citizen: Hi, how much do I pay in local taxes in Linköping? System: Are you a member of the Swedish Church? Citizen: Yes, I think so. System: What parish do you belong to? Citizen: Slaka. System: You pay 31,55 per cent in local taxes. Figure 1. Sample dialogue In order to allow for such interaction a number of research issues must be addressed. From a language technology perspective we foresee a fruitful, and necessary, co-operation between two main application areas: · Multimodal interaction. This means that the user and system can utilise various types of modalities in order to present information, not only natural language (spoken or written) but also graphics, images, videos, and tables. In this paper we will, however, only consider natural language processing aspects, and especially dialogue and domain knowledge management. In the scenario in Figure 1, above, the use of dialogue allows formulation of the information needs in a fragmented fashion. The system had to collect further information before it could present something useful and this also often involves  clarification sub-dialogues. When such a request has been completed, it can be used to retrieve the required information. · Information processing of documents. In this paper we use information processing to mean interpretation and adaptation of information stored as natural language documents. Dialogue systems need structured information in order to support advanced information retrieval and problem solving. Such structured knowledge bases are often hand crafted. For the vast amount of public information available on the Internet it is not feasible to manually create structured knowledge sources. Instead, we must be able to find the relevant information stored in unstructured formats and in a systematic way convert it to a suitable form. The problem is not only to bring structure to the information, a prerequisite for doing that is also to locate the relevant information, which, as the information is unstructured is a complex task. In a newly-started project at Linköping University we are investigating and developing multimodal interaction systems which utilise knowledge and methods from these two areas of language technology, see Figure 2. The long-term vision is to integrate such systems within a common e-home framework, but we believe that a fruitful research strategy will start from specific examples, and work towards a common framework, instead of trying to develop such a framework in a top-down fashion. We will work iteratively based on a method that unifies issues of conceptual design with a clear correspondence to the components of the customisation of a generic framework (Degerstedt & Jönsson, 2001). The method advocates that coding and design go together and that a dialogue system is implemented iteratively with cumulatively added capabilities. Coding should be carried out as soon as possible, before all details of the system's design are ready. A prototype is developed from the start and is gradually refined based on evaluations of its behaviour. Furthermore, dialogue systems are knowledge intensive and much knowledge is acquired during the development of the system. The evolutionary development, thus, mainly involves refining the knowledge sources.  Online Proceedings of NODALIDA 2001  Interpreter Generator  Interaction component  Dialogue Manager  Domain Knowledge Manager  Structured Data A Structured Data B  Lexicon  Grammar  Ontology  Structured Data C  Unstructured Data as Documents  Information extraction component  Preprocessor  Coreference resolver  Semantic classifier (wrappers)  POS-tagger  Lemmatizer  Phrase identifier  Figure 2. Interaction and information extraction combined  2. Interaction component The interaction component interprets user utterances in context, access the background information system1, integrate that with the interpreted utterance and generate a response. In this project the interaction will be handled by the MALIN dialogue system framework (Degerstedt & Jönsson, 2001). Dialogue systems often have a modular architecture with processing modules for interpretation, dialogue management, background system access, and generation, see Figure 2. The processing modules utilise a number of knowledge sources, such as, grammars, lexicons, a dialogue model, a domain model, and task models. 2.1 The Interpreter The parser is an incremental chart parser that is modified to handle a grammar with rules that allow a partial and shallow parsing (Jönsson & Strömbäck, 1998) The interpretation is driven by the information needed by the background system and guided by expectations from the dialogue 
The main idea of the Constraint Grammar is that it determines the surface-level syntactic analysis of the text, which has gone through prior morphological analysis. The process of syntactic analysis consists of three stages: morphological disambiguation, identification of clause boundaries, and identification of syntactic functions of words. The underlying principle in determining both the morphological interpretation and the syntactic functions is the same: first all the possible labels are attached to words and then the ones that do not fit the context are removed by applying special rules called constraints. Constraint Grammar consists of hand written rules, which by checking the context decide whether an interpretation is correct or has to be removed. 2. Preceding steps For morphological analysis of Estonian, we use the morphological analyser ESTMORF (Kaalep, 1997) that assigns adequate morphological descriptions to about 99% of tokens in a text. In Estonian fiction texts about 45% of morphologically analysed word-forms have more than one reading. Morphologically analysed text is disambiguated by Constraint Grammar disambiguator (Puolakainen, 1998). The development of the disambiguator is in process but 85-90% of words become morphologically unambiguous and the error rate of this disambiguator is less than 2%. The disambiguating grammar consists of more than 1200 hand written rules, almost half of them treat concrete word forms (e.g. 'on' - verb be in simple present 3rd person singular or plural), the others cover broader ambiguity classes. The difficult problem is the choice between the readings of a noun in nominative, genitive, partitive or short illative (aditive) case. The other sources of Online Proceedings of NODALIDA 2001  errors and ambiguities are participles and readings of adposition, adverb and noun of some word-forms. 3. Determination of syntactic functions 27 syntactic tags of ESTCG represent syntactic functions of traditional Estonian grammar (Erelt et al., 1993), although there are some modifications considering the specialities of Constraint Grammar: CG annotates every word with some syntactic label while linguistic grammar has a more general view treating multiple words as units. The syntax used in Constraint Grammar is word based, this means that no hierarchical phrase structure is constructed. The phrasal heads are labelled as subjects, objects, adverbials or predicatives. The modifiers have tags that indicate the direction where the head of phrase could be found but the modifiers and heads are not formally connected. The verb chain is marked by five labels: finite or infinite auxiliary or main verb and a label for negation. Determination of syntactic functions is implemented in two modules. First, the parser adds all possible function tags to each morphological reading, and after that, syntactic constraints remove incorrect tags in the current context. Syntactic tags are added to words by 180 morphosyntactic mapping rules. These rules describe which combination of syntactic tags should to be attached to the current morphological reading. For example, a noun in nominative case can be a subject, an object, a predicative, a premodifying or postmodifying attribute or an adverbial. In this stage of parsing at least one syntactic tag is assigned to every word but usually many more (approximately 3.8 tags per word in the case of Estonian). After the mapping operation syntactic constraints are applied. ESTCG contains 1118 syntactic constraints. The rules were devised using training corpus of 20,000 words. Most of these rules have linguistic background, this means that they are generated using grammar books and author's personal linguistic intuition. Only some of them are compiled using statistical information about word order tendencies. As known, any natural language tends to have somehow irregular nature - it is very difficult (if not impossible) to describe a language with fixed rules. So ca 20% of syntactic constraints in ESTCG are heuristic rules - they are not 100% true but ease to solve some complicated ambiguity classes. ESTCG heuristic rules help to raise unambiguity rate from 79% to 91%, reducing correctness from 99.46% to 99.24% (results from training corpus). Attempts are made to devise rules, which will remove as few correct interpretations as possible and so result in as error-free analyses as feasible. The syntactic part of Estonian Constraint Grammar is fully documented in author's Ph.D. thesis (Müürisep, 2000). A disambiguated and syntactically analysed sentence is shown in fig. 1. Morphological description is between "//"-symbols, syntactic tag begins with @symbol. The direct translation is given after #-symbol. The last word in the sentence remains ambiguous between adverbial and postmodifying attribute. The phrase koht infootsingul ('place on the information retrieval') has no meaning but the attribute tag can't be removed since the phrase with some other attribute in adessive case is quite usual, e.g. koht laeval - 'place on the ship'. 2 Online Proceedings of NODALIDA 2001  $LA$  ####  Dokumenditöötluses  # in the document processing  dokumendi_töötlus+s //_S_ com sg in #cap // **CLB @ADVL  on  # is  ole+0 //_V_ main indic pres ps3 sg ps af #FinV #Intr // @+FMV  oluline  # important  olu=line+0 //_A_ pos sg nom #line // @AN>  koht  # place  koht+0 //_S_ com sg nom // @SUBJ  infootsingul  # on the information retrieval  info_otsing+l //_S_ com sg ad // @ADVL @<NN  $.  . //_Z_ Fst //  $LL$  ####  
 ¤1  In this paper, we will show that the performance of vector-based semantic analysis can be improved by considering basic linguistic structures in the data— e.g. morphology. For this purpose, we have used a new method for vector-based semantic analysis that computes semantic word vectors based on distributed representations by means of random labeling of words in narrow context windows. This form of representation is more natural than previously reported techniques, and, as we will show, equivalent or even superior in performance when subjected to a standardized synonym test. AA¦v¦cCATvCy¦iyCC  The use of vector-based models of information for the  purpose of semantic analysis is an area of research that  has gained substantial recognition over the last decade.  Pioneering techniques such as Latent Semantic  Analysis (LSA; Landauer & Dumais, 1997) and  Hyperspace Analogue to Language (HAL; Lund &  Burgess, 1996) have demonstrated the viability of  computing semantic word vectors from the co-  occurrence statistics of words in large text data.  However, the prevailing techniques have been  almost exclusively statistical, and consequently paid  little or no attention to the linguistic structures of the  data used in the experiments. This negligence regarding  linguistics has, of course, been at least partly deliberate,  as one of the primary goals of the techniques has been  to develop representations of word meanings from text  data “that was minimally preprocessed, not unlike  human human-concept acquisition” (Burgess & Lund,  1998).  LSA and HAL are both purely statistical methods  that treat the text data simply as a bag-of-words in  which the only relevant piece of structural information  is the words-by-contexts co-occurrence frequencies.   ¢¡T What separates the two approaches is their treatment  and conception of  . In LSA, the text data is  represented as a words-by-documents co-occurrence  matrix where each cell indicates the frequency of a  given word in a given text sample of approximately  150 words. The frequencies are normalized, and the  normalized matrix is transformed with Singular Value  Decomposition (SVD) into a smaller matrix with  reduced dimensionality. The purpose of using SVD to  reduce the dimensions of the normalized frequency  matrix is that this operation appears to accomplish  inductive effects that capture latent semantic  structures in the text data. Words are thus  £ represented in the reduced matrix by semantic vectors of dimensionality (300 proving to be  optimal in Landauer & Dumais’ (1997)  experiments).  In HAL, the data is represented as a words-by-  words co-occurrence matrix where each cell  indicates the co-occurrence counts for a single  ¤¢¥ ¦T§ word pair (a word pair being an asymmetrical relation so that “ ” and “ ” represent different  entries in the matrix). Each word is thus  represented in the matrix by both a row and a  column, and these row/column pairs may be  ¨ © concatenated to produce a co-occurrence vector for each word. Assuming an × co-occurrence  ª matrix, words are thus represented as semantic vectors of 2 dimensionality.  The point in all of this is that the word vectors  capture relative meaning, thereby deserving the  epithet “semantic”. The semantic content is relative  «¬­® rather than absolute since it is only in relation to  each other that the vectors  anything, so  semantic similarity between words can be  established by comparing the vectors with each  other. That the vectors in this way capture word  meaning has been verified in a number of  experiments where the high-dimensional vectors  are used for executing different kinds of linguistic  tasks pertaining to semantic knowledge, such as  passing a standardized synonym test (Landauer &  Dumais, 1997), comparing vector similarities with  reaction times from lexical priming studies (Lund  & Burgess, 1996), or evaluating the quality of  content of student essays on given topics  (Landauer, Laham, Rehder & Schreiner, 1997). ¯7°¤±0²0³¤´¶µv±0²C·p¸T¹g±0º  We have studied the use of high-dimensional  random distributed representations for  accumulating a words-by-contexts co-occurrence  matrix from which semantic word vectors can be  extracted. In Kanerva, Kristofersson & Holst  (2000), 1,800-dimensional semantic word vectors  » ¼½¾¢¿2ÀT¾ÁÂ ÃÅÄ1Æ were computed using 1,800-dimensional sparse  random  representing documents of  approximately 150 words each. The index vectors  were accumulated into a words-by-contexts matrix  by adding a document’s index vector to the row for  Online Proceedings of NODALIDA 2001  a given word every time the word appeared in that  document. This Random Indexing method is  comparable to LSA except that the resulting matrix is  significantly smaller than the words-by-documents  matrix of LSA, since the dimensionality of the index  vectors is smaller than the number of documents. By  comparison, assuming a vocabulary of 60,000 words  divided into 30,000 text samples, LSA would represent  the data in a 60,000 × 30,000 words-by-documents  matrix, whereas the matrix in Random Indexing would  be 60,000 × 1,800, when 1,800-dimensional index  vectors are used. This seems to accomplish the same  inductive effects as those attained in LSA by applying  SVD to the matrix, but in a more efficient way.  In the present experiment, the high-dimensional  random vectors of Random Indexing have been used to  ÇÈÅÉ1ÉËÊÅÌ index words and to calculate semantic word vectors by  means of  context windows consisting of only a  few adjacent words on each side of the focus word. As  an example, imagine that the number of adjacent words  in the context window is set to two. This would imply a  window size of five space-separated linguistic units, i.e.  the focus word and the two words preceding and  succeeding it—a what we may call a “2 + 2 sized”  Í Î context window. Thus, the context for the word in  Ï¤ÐÑÒÔÓ%ÕÅÖ1ÖË×ØÙÑÒÛÚ×Ü×ÅÖËÝ the sentence  would be “This  parrot” and “no more,” as denoted by:  [(This parrot) is (no more)]  The reason for using narrow context windows as  opposed to whole documents is the assumption that the  semantically most significant context is the immediate  vicinity of a word. Computing semantic word vectors  using random indexing of words in narrow context  Þ windows is done by first assigning an -dimensional  ßËàáâãä¶å àæçå sparse random vector called a  to each  è word type in the text data. These random labels have a small number of randomly distributed –1s and +1s,  é with the rest set to 0. The present experiment has utilized 1,800-dimensional random labels with = 8.7  (±2.9). Thus, a label might have, for example, four –1s  êë and five +1s. Next, every time a given word—the focus word —  ìípîï ð¢ñTïyòTðìï íÅó occurs in the text data, the labels for the words in its  context window are added to its  . For  example, assuming a 2 + 2 sized context window as  represented by: ôÔõ öÔ÷ øù úÔû üÔý [( -2 -1) ( +1 +2)] þÿ the context vector of would be updated with:  (¡£¢ -2) + ¤ (¥£¦ -1) + § (¨£© +1) +  (£ +2)  where  ( ) is the random label of  . This summation  has also been weighted to reflect the distance of the  words to the focus word. The weights were distributed  so that the words immediately preceding and  succeeding the focus word would get more significance  in the computation of the context vectors. For the four  different window sizes used in these experiments, the  window slots were given weights as follows:  
This paper reports on the development of a finite state system for finding grammar errors in Swedish text written by children. The writing problems are more frequent for this group and the distribution of the error types is different from texts written by adults. The detection approach involves subtraction of automata that represent two “positive” grammars with varying degree of detail. The difference between the automata corresponds to the search for writing problems that violate the grammars. The constituents of various fragments are identified by the use of lexical-prefix-first strategy and application of incremental parsing. While the grammatical coverage is still rather small, yielding rather large numbers of false alarms, the technique can be applied to agreement phenomena, verb selection phenomena and some word order phenomena. The aim is to include also detection of missing sentence boundaries. Introduction Research and development of grammar checking techniques has been carried out since the 1980’s, mainly for English and also for other languages, e. g. French (Chanod, 1993), Dutch (Vosse, 1994), Czech (Kirschner, 1994), Spanish and Greek (Bustamente and León, 1996). In the case of Swedish, the development of grammar checkers started not until the later half of the 1990’s with several independent projects, resulting in the first product release in November 1998 - Grammatifix (Arppe, 2000; Birn, 2000), now part of the Swedish Microsoft Office 2000. The work described here is a continuation of a project from this starting period, that explores the use of finite state techniques for grammar checking. The approach differs from the other Swedish projects not only in the choice of technique used, but also in that the grammatical errors are found without any description of the erroneous patterns. The detection process involves partial parsing as writing of ”positive” grammars at different accuracy levels that as transducers can be subtracted from each other. The difference between the automata corresponds to the search for writing problems that violate the Online Proceedings of NODALIDA 2001  grammars. Karttunen et al (1996) use this technique to find instances of invalid dates. The current system, using the Xerox Finite-State Tool (XFST) (Karttunen et al, 1997), is divided in four main modules: the lexicon lookup, the grammar, the parser and the error finder. A simple emacs environment is used both for testing and development of finite state grammars. The environment shows the results of an XFST process run on the current emacs buffer in a separate buffer. An XFST mode allows for menus to be used and recompile files in the system. The Corpus Data The analyses of writing problems is based on a corpus of 31 756 words (3 361 word types), composed of (mostly) computer written and hand written essays written by children between 9 to 13 year old. In general, the text structure of the compositions reveals clearly the influence of spoken language and performance difficulties in spelling, segmentation of words, the use of capitals and punctuation, varying both by individuals and age. In total, 306 grammatical error instances were found in the 134 narratives. The most recurrent grammar problem concerns the omission of finite verb inflection (28%), i. e. when the main finite verb in a clause is in infinitive form and the appropriate present or past tense endings are dropped (a quite usual phenomena in spoken Swedish). Other more frequent grammar problems are wrong choice of a pronoun (15%), wrong pronoun case (8%), extra or missing words (11%), errors in verb chains (6%) and agreement in noun phrases (5 %). Punctuation problems are also included in the analyses. In general, the use of punctuation varies from no usage at all (mostly among the youngest children) to rather sparse marking. The omission of end of sentence marking is quite obvious problem (35%), but the most frequent punctuation problem concerns the comma (81%). The Lexicon Lookup The lexicon is built as a finite state transducer, using the Xerox tool FiniteState Lexicon Compiler (Karttunen, 1993). It takes a string and maps inflected surface form to a tag containing part-of-speech and feature information, e. g. applying the transducer to the string kvinna ‘woman’ will return [nn utr sin ind nom]. The word analyser is based on LEXIN (58 326 word forms; Skolverket, 1992) and the lexicon developed in Daniel Ridings’ lexicon project in the Language Technology Programme, HSFR/NUTEK (100 000 word forms). The network does not handle unknown words. The size of the Online Proceedings of NODALIDA 2001  lexicon is rather small, but the LEXIN-part includes valuable information on valency (see further in Andersson et al, 1998 and 1999). The system uses a simple lookup without any disambiguation. The reason for that is that the disambiguation heuristics of a tagger may fail with a text that contains errors, because the information needed for the detection of errors is (often) filtered out (see Cooper and Sofkova Hashemi, 1998). The strategy of a lexical lookup which leaves all lexical information present without attempting any disambiguation seems to be the most safe strategy in order to ensure that no information needed is lost.  The Grammar and Parsing  The grammar module is further subdivided into a broad grammar and a narrow grammar, that include regular expressions reflecting truths about the grammatical structure of Swedish, differing in the level of detail. For example the simple regular expression in the broad grammar:  define VC [Verb Adv* Verb (Verb)];  recognises potential verb clusters (both grammatical and ungrammatical), consisting of a sequence of two or three verbs in combination with some adverbs (zero or more). On the other hand, the following rules in the narrow grammar take into account the internal structure of a verb cluster, i. e. the rules define the grammar of (modal or temporal) auxiliary verbs followed by optional sentence adverb(s) and main verb:  define VC1 define VC2 define VC3 define VC4  [Mod Adv* VerbInf]; [Mod Adv* PerfInf VerbSup]; [Perf Adv* VerbSup]; [Perf Adv* ModSup VerbInf];  kan (inte) springa ”can (not) run ” skall (inte) ha sprungit ”will (not) have run[sup]” har (nog) sprungit ”have (not) run[sup]” har (inte) velat springa ”have (not) want[sup] run[sup]”  The various kinds of constituents are marked out in a text using the lexicalprefix-first method, i. e. parsing first from left margin of a phrase to the head and then extending the phrase by adding on complements. The actual parsing (based on the broad grammar definitions) is incremental similarly to the methods described in Ait-Mokhtar and Chanod (1997). With this method, the parsing results vary depending on the order in which the various grammatical facts are applied. The system recognises the higher phrases in the first phase (e. g. vp, pp, np) and then applies the second phase in the reverse order (e. g. np, pp, vp). This method gives a greater efficiency and flexibility to the system by decreasing the size of the nets when compiling and that (some) false parses can be blocked.  Online Proceedings of NODALIDA 2001  Error Detection The error finder is a separate module in the system which means that the grammar and parser could be used directly in a different application. The nets of this module correspond to the difference between the two grammars, broad and narrow. For example, the regular expression: [ ”<vc>” [VC - VCgram] ”</vc>” ] where VC is the part of the broad grammar describing broad structure of a verb cluster and VCgram is the part of the narrow grammar describing the grammar of auxiliary verbs, will find verb clusters that violate these constraints within the VC-boundaries that have been previously marked out by applying the broad grammar. So far the technique was used to detect agreement errors in noun phrases, verb selection phenomena (in particular selection of finite and non-finite verb forms in main and subordinate clauses and infinitival complements) and local word order phenomena (e. g. placement of negation). Also some attempts were maid to detect missing sentence boundaries, starting with clause and verb subcategorisation and trying to make use of the valency information stored in the lexicon of the system. Further, it is possible to use this method to perform error diagnostics. This is achieved by subtraction of small parts of the narrow grammar representing specific constraints that can be violated. Evaluation The grammatical coverage of the system is still rather small, yielding a large number of false alarms. There are in general two kinds of false alarms that occur: either because of the “smallness” of grammar for the particular constituents occurring in the phrase, or due a false parse depending either on the ambiguity of constituents or the “wideness” of the parse, i. e. too many constituents are included when applying the longest-match strategy. The following example shows an ambiguous parse: Linda , brukade ofta vara i stallet.1 ”Linda , used to be often in the stable.” <Error finite verb> <vp><vpHead><np>Linda </np></vpHead></vp> </Error> , <vp><vpHead> <vc> brukade ofta <np>vara </np> </vc> </vpHead><pp><ppHead>i </ppHead><np>stallet </np></pp></vp>. 
CG and FDG CG is a reductionistic constraint rule formalism whose input is lexically analysed ambiguous text and whose output is disambiguated text. Disambiguation is carried out by constraints on lemmas and tags that discard alternative analyses on the basis of contextual information, typically coded by a linguist. The ENGCG morphosyntactic tagger was introduced in 1992 (Voutilainen & al.) and compared with a state-of-the-art statistical tagger in 1997 (Samuelsson & Voutilainen). CG was successful in word-class tagging but not adequate for full-scale parsing. A considerable effort on finite-state parsing was made by Koskenniemi, Tapanainen and Voutilainen (see their articles in Roche & Schabes, eds., 1997). A more successful effort was made by Tapanainen and Järvinen, who extended CG into a functional dependency grammar formalism and interpreter/compiler capable of introducing explicit functional dependencies and of applying large grammars efficiently. Earlier work on Swedish tagging and parsing As discussed in Voutilainen (forthcoming 2001), most efforts at Swedish tagging and parsing have focused on wordclass tagging, mostly in the statistical paradigm. A somewhat more informative analysis is given by Lingsoft's SWECG (morphology + function tags) and shallow finite-state Abney-style parsers (Kokkinakis & Johansson 1999). The Swedish Core Language Engine (Gambäck 1997) produces full syntactic parses, but, as argued by Gambäck, it seems to work only for texts from very restricted domains. Swedish Light Syntax In design, SweLite follows Conexor's EngLite (see demo at www.conexor.fi). The first major component is the morphological analyser, based on a recent extension of Koskenniemi's Two-Level formalism. The analyser contains a large lexicon, morphology and guesser for unknown words. The morphological analyser produces analyses, many of them ambiguous. The parser uses mapping statements to introduce light syntactic ambiguity, so before any disambiguation is done, an ambiguous analysis looks like this: "<tvingas>" "tvinga" <Pass> V INF &MV "tvinga" <Pass> V PRES &MV "tving#as" <Neu> <Indef> N SG/PL NOM &>N &NH "tv#in|gas" <Utr> <Indef> N SG NOM &>N &NH Online Proceedings of NODALIDA 2001  Morphological alternatives are given on lines of their own; syntactic ambiguity is shown as the occurrence of several syntactic tags (here: &MV = main verb; &NH = nominal head; &>N = premodifier). Disambiguation is carried out with hand-coded contextual constraints. Here is a sample parse in tabular format:  Man  ställer  upp  verkligt  höga  mål  ,  som  tränarna  och  skidåkarna  tvingas  leva  med  .  .  man ställa upp verkligt hög mål , som tränare och skid#åkare tvinga leva med  &NH PRON SG NOM &MV V PRES &AH ADV &>A ADV &>N A NOM &NH N NOM &NH PRON NOM &NH N PL NOM &CC CC &NH N PL NOM &MV V PRES &MV V INF &AH ADV &AH PREP  On the basis of light syntactic tags and morphology, identification of basic linguistic entities, e.g. nominal phrases, is possible. Identifying relations between the entities requires more information.  Swedish FDG  Tapanainen and Järvinen (1997) give examples of indexing rules whereby functional dependencies between words can be introduced. In the best case, a successful grammar gives a complete dependency structure; in practice many sentences receive only partial dependencies (e.g. due to gaps in the grammar or structural peculiarities in the sentence). A dependency grammar was written for Swedish. The goal of the present grammar is to show the main nominal arguments as well as relations between clauses. The functional description of adverb phrases and prepositional phrases (e.g. agent, source, goal, benefactive, time) remains to be described in a future version. Here is a sample parse for a newspaper sentence (`One puts up really high goals that trainers and skiiers are forced to live with.'):  
SE-405 30 Göteborg, SWEDEN {jens,leifg,eliza,mgunnar}@ling.gu.se  Introduction The paper contains a description of the Spoken Language Corpus of Swedish at the Department of Linguistics, Göteborg University (GSLC), and a summary of the various types of analysis and tools that have been developed for work on this corpus. Work on the corpus was started in the late 1970:s. It is incrementally growing and presently consists of 1.3 million words from about 25 different social activities. The corpus was initiated to meet a growing interest in naturalistic spoken language data. It is based on the fact that spoken language varies considerably in different social activities with regard to pronunciation, vocabulary, grammar and communicative functions. The goal of the corpus is to include spoken language from as many social activities as possible to get a more complete understanding of the role of language and communication in human social life. This type of spoken language corpus is still fairly unique even for English, since many spoken language corpora (certainly for Swedish) have been collected for special purposes, like speech recognition, phonetics, dialectal variation or interaction with a computerized dialog system in a very narrow domain, e.g. (Map Task (Isard and Carletta (1995), TRAINS (Heeman and Allen 1994), Waxholm (Blomerg et al. 1993). Compared to English corpora, the Göteborg corpus is most similar to the Wellington Corpus of Spoken New Zealand English (Holmes, Vine and Johnson 1998), but also has traits in common with the BNC, the London/Lund corpus (Svartvik  1990) and the Danish BySoc corpus (Gregersen 1991, Henrichsen 1997). The corpus is based on audio (50%) or video/audio (50%) recordings of naturalistically occurring interactions. The recordings have been transcribed according to a transcription standard consisting of a language neutral part – presently Göteborg transcription standard, GTS 6.2 (Nivre 1999a) (it has been tested on Chinese, Arabic, English, Spanish, Bulgarian and Finnish) and a language particular part concerned with Swedish – presently Modified Standard Orthography, MSO6 (Nivre 1999b). Both parts have undergone 6 major revisions and several minor ones, In order to enhance the reliability, all transcriptions are manually checked by another person than the transcriber. They are also checked for correctness of format, before they are inserted into the corpus. In MSO, standard orthography is used unless there are several spoken language pronunciation variants of a word. When there are several variants, these are kept apart graphically. Although the goal is to keep transcription simple, the standard includes features of spoken language such as contrastive stress, overlaps and pauses. It also includes procedures for anonymizing transcriptions and for introducing comments on part of the transcription. Below, we will also describe several tools we have developed for using the corpus. The tools have, like the corpus, been incrementally developed since the early 1980:s and are all  
We describe an approach (\variant transduction") aimed at reducing the e ort and skill involved in building spoken language interfaces. Applications are created by specifying a relatively small set of example utterance-action pairs grouped into contexts. No intermediate semantic representations are involved in the speci cation, and the con rmation requests used in the dialog are constructed automatically. These properties of variant transduction arise from combining techniques for paraphrase generation, classi cation, and examplematching. We describe how a spoken dialog system is constructed with this approach and also provide some experimental results on varying the number of examples used to build a particular application. 
We have developed a discourse level tagging tool for spoken dialogue corpus using machine learning methods. As discourse level information, we focused on dialogue act, relevance and discourse segment. In dialogue act tagging, we have implemented a transformation-based learning procedure and resulted in 70% accuracy in open test. In relevance and discourse segment tagging, we have implemented a decision-tree based learning procedure and resulted in about 75% and 72% accuracy respectively. 
Since van der Sandt and Geurts have put forward and extensively applied the notion of a fundamental identity of presupposition and anaphora, something like a universal consensus seems to have developed that this view is basically correct. Supposing that it is, and further supposing that it entails an empirical hypothesis, there are a number of facts that have so far remained unaccounted for. This paper presents some of these facts and argues that more differentiated notions of anaphora and presupposition may well be more fruitful for further research in the semantics-pragmatics interface. Introduction Since van der Sandt & Geurts (1991), van der Sandt (1992) and Geurts (1995, 1999) have put forward the notion of a fundamental identity of presupposition and anaphora, a fairly universal consensus seems to have developed that this view is basically correct. Assuming that this notion is intended to have empirical consequences I shall refer to this view in the following as the 'Presupposition is Anaphora Hypothesis', for short: the PIA hypothesis. There is surely no doubt that phenomena of presupposition and anaphora are not unrelated. Understanding anaphora quite conventionally as a way of resuming a previously established reference more or less involves the assumption that this reference has actually been established beforehand and in this sense is presupposed. In this rough and general sense anaphora is presuppositional. Conversely, in presupposing that one or the other proposition is true, a relation is established  to something that either was said before, follows from something that was said before, or at least could reasonably have been said before; and this relation may in some rough and general sense be called anaphorical. Now Geurts and van der Sandt do not stay at a rough and general level, but turn these relations between anaphora and presupposition into a venerable theory of presupposition that yields certain technical advantages for the treatment of presupposition when it is implemented in (a variety of) Discourse Representation Theory (van der Sandt (1992) and Geurts (1995, 1999)). I shall argue below that despite this technical progress the PIA hypothesis obscures both the notion of presupposition and the notion of anaphora. The Procrustean relationship actually harms our understanding of both phenomena. At the same time, I shall argue, the Hypothesis is empirically wrong with respect to linguistic data. I shall start with the latter. 
 We describe our experience in  developing a discourse-annotated  corpus for community-wide use.  Working in the framework of  Rhetorical Structure Theory, we were  able to create a large annotated  resource with very high consistency,  using a well-defined methodology and  protocol. This resource is made  publicly available through the  Linguistic Data Consortium to enable  researchers to develop empirically  grounded,  discourse-specific  applications.  
The development of spoken dialogue systems is often limited by the performance of their speech recognition component. The impact of speech recognition errors on dialogue systems is often studied at the global level of task completion. In this paper, we carry an empirical study on the consequences of speech recognition errors on a fully-implemented dialogue prototype, based on a speech acts formalisms. We report the impact of speech recognition errors on speech act identification and discuss how standard control mechanisms can participate to robustness by assisting the user in repairing the consequences of speech recognition errors. Introduction The development of spoken dialogue systems is faced with limitations in speech recognition technologies that make recognition errors a recurring problem for any dialogue system. Several studies have shown little correlation between speech recognition scores and user satisfaction, or the ability to complete the tasks underlying spoken dialogue [Yankelovich et al., 1995] [Dybkjaer et al., 1997], suggesting that a certain level of errors should not prevent spoken dialogue systems from being successful. However, most of the studies on speech recognition errors have concentrated either on parsing incomplete utterances or on global dialogue robustness, i.e. at task completion level [Allen et al., 1996] [Stromback and Jonsson, 1998] [Brandt-Pook et al., 1996]. In this paper, we investigate the impact of speech recognition errors on a fully-  implemented prototype for a task-oriented dialogue system. This system supports a conversational character for Interactive Television and is based on a speech acts formalism. We report a first empirical study on the consequences of speech recognition errors on the identification of speech acts, and the conditions under which the system can be robust to those errors. 
While researchers have many intuitions about the diﬀerences between humancomputer and human-human interactions, most of these have not previously been subject to empirical scrutiny. This work presents some initial experiments in this direction, with the ultimate goal being to use what we learn to improve computer dialogue systems. Working with data from the air travel domain, we identiﬁed a number of striking differences between the human-human and human-computer interactions. 
This paper describes an application of state-of-the-art spoken language technology (OAA/Gemini/Nuance) to a new problem domain: engaging students in automated tutorial dialogues in order to evaluate and improve their performance in a training simulator. 
This paper deals with user corrections and aware sites of system errors in the TOOT spoken dialogue system. We rst describe our corpus, and give details on our procedure to label corrections and aware sites. Then, we show that corrections and aware sites exhibit some prosodic and other properties which set them apart from `normal' utterances. It appears that some correction types, such as simple repeats, are more likely to be correctly recognized than other types, such as paraphrases. We also present evidence that system dialogue strategy a ects users' choice of correction type, suggesting that strategy-speci c methods of detecting or coaching users on corrections may be useful. Aware sites tend to be shorter than other utterances, and are also more difcult to recognize correctly for the ASR system. 
We present a tool for the annotation of anaphoric and bridging relations in a corpus of written texts. Based on differences as well as similarities between these phenomena, we deﬁne an annotation scheme. We then implement the scheme within an annotation tool and demonstrate its use. 
Manually verified pitch data were compared with output from a commonly used pitchtracking algorithm. The manual pitch data made statistically significantly better “final rise” predictions than the automatic pitch data, in spite of great similarity between the two sets of measurements. Pitch Tracking doubling/halving errors are described. Introduction Automatic ally captured pros odic information is relevant to both automatic speech recognition and speech synthesis. Pitch information, though regarded as highly relevant, has not been scrutinized in detail as with respect to automatic pitch trackers. This study presents a comparison of hand-verified pitch measurements (“hand”) with measurements from a commonly used pitch tracking algorithm (“automatic”), Talkin (1995). For this paper pitch will be defined as the aurally perceived information that loosely correlates with the fundamental frequency of a section of a speech waveform. The organization of this paper is as follows: First, the corpus used is described and justified. The next section describes comparisons of the hand-corrected pitch measurements and the automatic pitch tracker output. Next, results are presented with respect to the detection of utterance-final rises and falls. Lastly, the future work section connects conclusions from this specific study to related work on pitch and perception, and describes discourse-related applications that could benefit from a study of this kind. 
We examine what purpose a dialog metric serves and then propose empirical methods for evaluating systems that meet that purpose. The methods include a protocol for conducting a wizardof-oz experiment and a basic set of descriptive statistics for substantiating performance claims using the data collected from the experiment as an ideal benchmark or “gold standard” for comparative judgments. The methods also provide a practical means of optimizing the system through component analysis and cost valuation. 
This paper addresses recent results on Mandarin spoken dialogues and introduces the collection of a large Mandarin conversational dialogue corpus. In the context of data processing, principles of transcription are proposed and accordingly a transcription tool is specifically developed for Mandarin spoken conversations.  Introduction Large speech corpora have become indispensable for current linguistic research and information science applications dealing with spoken data (Gibbon et al. 1997). Concretely, they provide real phonetic data and empirical data-driven knowledge on linguistic features of spoken language. The corpus presented here is composed of conversational dialogues. Conversations contain a considerable variety of linguistic phenomena as well as phonetic-acoustic variations. Furthermore, they open up a wide range of research issues such as dialogue acts, turn-taking, lexical use of spoken language and prosodic use in conversation. From a diachronic point of view, such a large dialogue corpus archives the contemporary daily conversational use of a given language.  
This paper presents a study of the effects of syntax and melodic configuration on turntaking in Southern British English. Using dialogue materials, two perception experiments were carried out. In the first, subjects heard dialogue fragments in which syntactic completeness and melodic contour were systematically varied, and were asked whether they expected a subsequent turn exchange or not. In the second, subjects were presented with short speaker exchanges taken from the same material, and asked whether they thought the first speaker had intended to cede the turn or not. The results suggest that syntactic completion or non-completion is the main factor in predicting turn-taking behaviour. Only one melodic contour, the high level tone H* %, appears to operate as a turn holding device, regardless of whether the utterance is grammatically complete or not. The results of this study were found to be similar to those of a study of Dutch turn-taking. Introduction Most studies of the intonational cues to turntaking have been carried out qualitatively within the theoretical framework of Conversation Analysis (e.g. Wells & Macfarlane 1998, Selting 1996). An exception to this is the study by Ford and Thompson (1996), who found that turnchanges in American English mostly appear when melodic, syntactic and pragmatic completion coincide. Two recent studies of the melodic cues to turn-taking in Dutch (Caspers 2000, 2001) motivate the present study, which uses comparable English data to replicate as far as possible the perception experiments carried  out in Caspers (2001). On the basis of the findings for Dutch we expected syntactic completeness to be the overriding predictor of a possible turn change. Where melody has an effect, we hypothesised that, as in Dutch, the high level tone was likely to signal more to come and that no subsequent turn change would be expected (Caspers 1998). We also hoped to gain some insight into possible similarities and differences between the two languages. 
This paper presents a corpus study of evaluative and speculative language. Knowledge of such language would be useful in many applications, such as text categorization and summarization. Analyses of annotator agreement and of characteristics of subjective language are performed. This study yields knowledge needed to design e ective machine learning systems for identifying subjective language. 
Dialogue analysis is widely used in oncology for training health professionals in communication skills. Parameters and tagsets have been developed independently of work in natural language processing. In relation to emergent standards in NLP, syntactic tagging is minimal, semantics is domain-speciﬁc, pragmatics is comparable, and the analysis of cognitive aﬀect is richly developed. We suggest productive directions for convergence. 
This paper proposes a new dialogue control method for spoken dialogue systems. The method conﬁgures a dialogue plan so as to minimize the estimated number of turns to complete the dialogue. The number of turns is estimated depending on the current speech recognition accuracy and probability distribution of the true user’s request. The proposed method reduces the number of turns to complete the task at almost any recognition accuracy. 
Each of these proposed workshops had a history of their own. 
This short paper presents some motivations behind the organization of the ACL/EACL01 “Workshop on Sharing Tools and Resources for Research and Education”, concentrating on the possible connection of Tools and Resources repositories. Taking some papers printed in this volume and the ACL Natural Language Software Registry as a basis, we outline some of the steps to be done on the side of NLP tool repositories in order to achieve this goal. 
In the scope of the TELRI concerted action a working group is investigating the formation of a tool catalogue and repository. The idea is similar to that of the ACL Natural Language Software Registry, but the contents should be mostly limited to corpus processing tools available free of cost for research use. The catalogue should also offer a help-line for installing and using the software. The paper reports on the setup of this catalogue, and concentrates on the technical issues involved in its creation, storage and display. This involves the form interface on the Web, the XML DocBook encoding, and the XSL stylesheets used to present the catalogue either on the Web or in print. The paper lists the current entries in the catalogue and discusses plans for their expansion and maintenance. 
 TRACTOR is the TELRI Research  Archive of Computational Tools and  Resources. It features monolingual,  bilingual, and multilingual corpora  and lexicons in a wide variety of  languages, as well as tools for  language processing. TRACTOR is a  key element of TELRI II, a pan-  European alliance of focal national  language technology institutions with  the emphasis on Central and Eastern  European and NIS countries.  TRACTOR hopes to complement  other archives by providing a service  for languages and users who are  currently under-represented in  existing archives. TRACTOR's unique  strength lies in the amount of  resources provided by centres in  Central and Eastern Europe, and its  role at the hub of a network of  resource creation, standardisation and  distribution which links the EU and  non-EU  European  research  communities. The TRACTOR User  Community brings together resource  providers, academic users and  industrial users in an ongoing  relationship, which is designed to  foster the emergence of joint research  projects in language engineering.  The TRACTOR philosophy is to  accept deposits of resources in any  format, and to distribute them in the  form in which they are received (with  small changes if possible such as  additional documentation, and putting  a browsable version or sample online.)  In addition, certain standards are  recommended and help is offered to  providers who wish to make their  resources conformant with the  standards. This lack of standardisation  is not simply a pragmatic measure in  the face of problems of heterogeneity,  but is based on a profound scepticism  towards  current  resource  standardisation practice.  In the future, TRACTOR aims to  build up particularly parallel corpora  and tools for processing and  extracting meaning from such  resources.  
Recently there has been a growing interest in infrastructures for sharing NLP tools and resources. This paper presents SiSSA, a project that aims at developing an infrastructure for prototyping, editing and validation of NLP application architectures. The system will provide the user with a graphical environment for (1) selecting the NLP activities relevant for the particular NLP task and the associated linguistic processors that execute them; (2) connecting new linguistic processors to SiSSA; (3) checking that the chosen architectural hypothesis corresponds to the functional speciﬁcations of the given application. 
As language data and associated technologies proliferate and as the language resources community rapidly expands, it has become difﬁcult to locate and reuse existing resources. Are there any lexical resources for such-and-such a language? What tool can work with transcripts in this particular format? What is a good format to use for linguistic data of this type? Questions like these dominate many mailing lists, since web search engines are an unreliable way to ﬁnd language resources. This paper describes a new digital infrastructure for language resource discovery, based on the Open Archives Initiative, and called OLAC – the Open Language Archives Community. The OLAC Metadata Set and the associated controlled vocabularies facilitate consistent description and focussed searching. We report progress on the metadata set and controlled vocabularies, describing current issues and soliciting input from the language resources community. 
The ISLE project is a continuation of the long standing EAGLES initiative, carried out under the Human Language Technology (HLT) programme in collaboration between American and European groups in the framework of the EU-US International Research Co-operation, supported by NSF and EC. We concentrate in this paper on the current position of the ISLE Computational Lexicon Working Group. We provide a short description of the EU SIMPLE lexicons built on the basis of previous EAGLES recommendations. We then point at a few basic methodological principles applied in previous EAGLES phases, and describe a few principles to be followed in the definition of a Multilingual ISLE Lexical Entry (MILE). 
 respect to metadata within the DOBES [3] and the CGN [4] projects.  The increasing amount and complexity of  multi-media/multi-modal  language  resources (MMLR) poses a problem in  many respects. This paper wants to discuss  metadata descriptions that can be used to  easy find and locate suitable MMLRs in the  Internet and how these descriptions may be  used to discover and apply suitable tools on  the data.  
This paper presents a workbench for Tree Adjoining Grammars that we are currently developing. This workbench includes several tools and resources based on the markup language XML, used as a convenient language to format and exchange linguistic resources. 
This paper describes the RenTAL system, which enables sharing resources in LTAG and HPSG formalisms by a method of grammar conversion from an FB-LTAG grammar to a strongly equivalent HPSG-style grammar. The system is applied to the latest version of the XTAG English grammar. Experimental results show that the obtained HPSG-style grammar successfully worked with an HPSG parser, and achieved a drastic speed-up against an LTAG parser. This system enables to share not only grammars and lexicons but also parsing techniques. 
This paper introduces GLARF, a framework for predicate argument structure. We report on converting the Penn Treebank II into GLARF by automatic methods that achieved about 90% precision/recall on test sentences from the Penn Treebank. Plans for a corpus of hand-corrected output, extensions of GLARF to Japanese and applications for MT are also discussed. 
We demonstrate the open-source LKB system which has been used to teach the fundamentals of constraint-based grammar development to several groups of students. 
We present three applications which share some of their linguistic processor. The first application “FILES” – Fully Integrated Linguistic Environment for Syntactic and Functional Annotation - is a fully integrated linguistic environment for syntactic and functional annotation of corpora currently being used for the Italian Treebank. The second application is a shallow parser – the same used in FILES – which has been endowed with a feedback module in order to inform students about their grammatical mistakes, if any, in German. Finally an LFGbased multilingual parser simulating parsing strategies with ambiguous sentences. We shall present the three applications in that sequence. 1. FILES FILES has been used to annotate a number of corpora of Italian within the National Project currently still work in progress. Input to FILES is the output of our linguistic modules for the automatic analysis of Italian, a tokenizer, a morphological analyser, a tagger equipped with a statistic and syntactic disambiguator and finally a shallow parser. All these separate modules contribute part of the input for the system which is then used by human annotators to operate at syntactic level on constituent structure, or at function level on head-features functional representation. We don’t have here space to describe the linguistic processors – but see [8, 9, 10, 11, 12]. As to tag disambiguation, this is carried out in a semi-automatic manner by the human annotator, on the basis of the automatic redundant morphological tagger. The disambiguator takes each token and in case of ambiguity it alerts the annotator to decide which is the tag to choose: the annotator is presented with the best candidate computed on the basis both of syntactic and statistical information. Low level representations are integrated in a relational database and shown in the FILES environment  which is an intelligent browser allowing the annotation to operate changes and create XML output automatically for each file. Here below is a snapshot of the six relational databases where all previously analysed linguistic material has been inputted. It contains tokens, lemmata, POS tagging, empty categories, sentences containing each token, tokens regarded as heads as separated from tokens regarded as features and verb subacategorization list. Fig.1 Relational databases to be used as input for the Syntactic and Functional Annotation An interesting part of the browser is the availability of subcategorization frames for verbs: these are expressed in a compact format which are intended to help the annotator in the most difficult task, i.e. that of deciding whether a given constituent head must be interpreted as either an argument or an adjunct; and in case it is an argument, whether it should be interpreted as predicative or “open” in LFG terms, or else as non-predicative or “close”. The list of subcategorization frames contains 17,000 entries. Of course the annotator can add new entries either as new lexical items or simply as new subcategorizations frames, which are encoded in the current list. Notable features of the browser are the subdivision into two separate columns of  verbal heads from non verbal ones, whereas the actual sentence highlights all heads verbal and non verbal in bold. On the righthand side there is a scrollable list of relations and the possibility to move from one sentence to another at will. Finally the XML button to translate the contents of each or any number of sentences into xml format. Fig.2 Browser for Functional Annotation with Structural representation 2. GRAMMCHECK The second application is a Grammar Checker for Italian students of German and English. The one for students of English is based on GETARUNS and uses a highly sophisticated grammar which is however a completely separated system from the one presented here and requires a lot more space for its presentation – see [13, 14]. It is available under Internet and will be shown as such. The one for students of German on the contrary, is based on the shallow parser of Italian used to produce the syntactic constituency for the National Treebank. The output of the parser is a bracketing of the input tagged word sequence which is then passed to the higher functional processor. This is an LFG-based c-structure to fstructure mapping algorithm which has three tasks: the first one is to compute features from heads; the second one is to compute agreement. The third task is to impose LFG’s grammaticality principles: those of Coherence and Consistency, i.e. number and type of arguments are constrained by the lexical form of the governing predicate. The parser is an RTN which has been endowed with a grammar and a lexicon of German of about 8K entries. The grammar is written in the usual  arc/transition nodes formalism, well-known in ATNs. However, the aim of the RTN is that of producing a structured output both for wellformed and illformed grammatical sentences of German. To this end, we allowed the grammar to keep part of the rules of Italian at the appropriate structural level, though. Grammar checking is not accomplished at the constituent structure building level, but at the f-structure level. 2.1 THE SHALLOW PARSER The task of the Shallow Parser is that of creating syntactic structures which are eligible for Grammatical Function assignment. This task is made simpler given the fact that the disambiguator will associate a net/constituency label to each disambiguated tag. Parsing can then be defined as a Bottom-Up collection of constituents which contain either the same label, or which may be contained in/be member of the same net/higher constituent. No attachment is performed in order to avoid being committed to structural decisions which might then reveal themselves to be wrong. We prefer to perform some readjustment operations after structures have been built rather than introducing errors from the start. Readjustment operations are in line with LFG theoretical framework which assumes that fstructures may be recursively constituted by subsidiary f-structures, i.e. by complements or adjuncts of a governing predicate. So the basic task of the shallow parser is that of building shallow structures for each safely recognizable constituent and then pass this information to the following modules. 2.2 Syntactic Readjustment Rules Syntactic structure is derived from shallow structures by a restricted and simple set of rewriting operations which are of two categories: deletions, and restructuring. Here are some examples of both: a. Deletion Delete structural labels internally with the same constituent label that appears at the beginning as in Noun Phrases, whenever a determiner is taken in front of the head noun; b. Restructuring As explained above, we want to follow a policy of noncommittal as to attachment of constituents: nonetheless, there are a number of restructuring operations which can be safely executed in order to simplify the output without running the risk of  taking decisions which shall have later to be modified. Restructuring is executed taking advantage of agreement information which in languages like Italian or German, i.e. in morphologically rich languages, can be fruitfully used to that aim. In particular, predicative constituents may belong to different levels of attachment from the adjacent one. More Restructuring is done at sentence level, in case the current sentence is a coordinate or subordinate sentence. 3 FROM C-STRUCTURE TO FSTRUCTURE Before working at the Functional level we collected 2500 grammatical mistakes taken from real student final tests. We decided to keep trace of the following typical grammatical mistakes: - Lack of Agreement NP internally; - Wrong position of Argument Clitic; - Wrong Subject-Verb Agreement; - Wrong position of finite Verbs both in Main, Subordinate and Dependent clauses; - Wrong case assignment. Example 1. Heute willst ich mich eine bunte Krawatte umbinden. cp-[ savv-[avv-[heute]], vsec-[vsupp-[willst], fvsec-[sogg2-[sn-[pers-[ich]]], ogg-[sn-[clitdat-[mich]]], ogg1-[snsempl-[art-[eine],ag-[bunte], n-[krawatte]]], ibar2-[vit-[umbinden]]] ], punto-[.]] The parser will issue two error messages: The first one is relative to Case assignment, “mich” is in the accusative while dative is required. The second one is relative to SubjectVerb agreement, “willst” is second person singular while the subject “ich” is first person singular. As to the use of f-structure for grammar checking the implementation we made in GETARUN – a complete system for text understanding, is a case where parsing strategies are used. This is a web-based multilingual parser which is based mainly on LFG theory and partly on Chomskian theories, incorporating a number of Parsing Strategies which allow the student to parse ambiguous sentences using the appropriate strategy in order to obtain an adequate grammatical output. The underlying idea was that  of stimulating the students to ascertain and test by themselves linguistic hypotheses with a given linguistically motivated system architecture. The parser builds c-structure and f-structure and computer anaphoric binding at sentence level; it also has provision for quantifier raising and temporal local interpretation. Predicates are provided for all lexical categories, noun, verb, adjective and adverb and their description is a lexical form in the sense of LFG. It is composed both of functional and semantic specifications for each argument of the predicate: semantic selection is operated by means both of thematic role and inherent semantic features or selectional restrictions. Moreover, in order to select adjuncts appropriately at each level of constituency, semantic classes are added to more traditional syntactic ones like transitive, unaccusative, reflexive and so on. Semantic classes are of two kinds: the first class is related to extensionality vs intensionality, and is used to build discourse relations mainly; the second class is meant to capture aspectual restrictions which decide the appropriateness and adequacy of adjuncts, so that inappropriate ones are attached at a higher level.  SYSTEM ARCHITECTURE I°  Top-Down DGC-based Grammar Rules  Deterministic Policy: Look-ahead WFST 
Annotation graphs and annotation servers offer infrastructure to support the analysis of human language resources in the form of time-series data such as text, audio and video. This paper outlines areas of common need among empirical linguists and computational linguists. After reviewing examples of data and tools used or under development for each of several areas, it proposes a common framework for future tool development, data annotation and resource sharing based upon annotation graphs and servers. 
 Annotation graphs provide an efﬁcient and expressive data model for linguistic annotations of time-series data. This paper reports progress on a complete open-source software infrastructure supporting the rapid development of tools for transcribing and annotating time-series data. This generalpurpose infrastructure uses annotation graphs as the underlying model, and allows developers to quickly create special-purpose annotation tools using common components. An application programming interface, an I/O library, and graphical user interfaces are described. Our experience has shown us that it is a straightforward task to create new special-purpose annotation tools based on this general-purpose infrastructure. 
We propose a new approach under the example-based machine translation paradigm. First, the proposed approach retrieves the most similar example by carrying out DP-matching of the input sentence and example sentences while measuring the semantic distance of the words. Second, the approach adjusts the gap between the input and the most similar example by using a bilingual dictionary. We show the results of a computational experiment. 
We describe MSR-MT, a large-scale hybrid machine translation system under development for several language pairs. This system’s ability to acquire its primary translation knowledge automatically by parsing a bilingual corpus of hundreds of thousands of sentence pairs and aligning resulting logical forms demonstrates true promise for overcoming the so-called MT customization bottleneck. Trained on English and Spanish technical prose, a blind evaluation shows that MSR-MT’s integration of rule-based parsers, example based processing, and statistical techniques produces translations whose quality exceeds that of uncustomized commercial MT systems in this domain. 
This paper describes a novel approach to inducing lexico-structural transfer rules from parsed bi-texts using syntactic pattern matching, statistical cooccurrence and error-driven ﬁltering. We present initial evaluation results and discuss future directions. 
Existing studies show that a weighted context-free transduction of reasonable quality can be effectively learned from examples. This paper investigates the approximation of such transduction by means of weighted rational transduction. The advantage is increased processing speed, which beneﬁts realtime applications involving spoken language. 
This paper gives an overview of the stochastic modelling approach to machine translation. Starting with the Bayes decision rule as in pattern classiﬁcation and speech recognition, we show how the resulting system architecture can be structured into three parts: the language model probability, the string translation model probability and the search procedure that generates the word sequence in the target language. We discuss the properties of the system components and report results on the translation of spoken dialogues in the VERBMOBIL project. The experience obtained in the VERBMOBIL project, in particular a largescale end-to-end evaluation, showed that the stochastic modelling approach resulted in signiﬁcantly lower error rates than three competing translation approaches: the sentence error rate was 29% in comparison with 52% to 62% for the other translation approaches. 
Translation systems that automatically extract transfer mappings (rules or examples) from bilingual corpora have been hampered by the difficulty of achieving accurate alignment and acquiring high quality mappings. We describe an algorithm that uses a bestfirst strategy and a small alignment grammar to significantly improve the quality of the transfer mappings extracted. For each mapping, frequencies are computed and sufficient context is retained to distinguish competing mappings during translation. Variants of the algorithm are run against a corpus containing 200K sentence pairs and evaluated based on the quality of resulting translations. 
In statistical machine translation, correspondences between the words in the source and the target language are learned from bilingual corpora on the basis of so called alignment models. Existing statistical systems for MT often treat different derivatives of the same lemma as if they were independent of each other. In this paper we argue that a better exploitation of the bilingual training data can be achieved by explicitly taking into account the interdependencies of the different derivatives. We do this along two directions: Usage of hierarchical lexicon models and the introduction of equivalence classes in order to ignore information not relevant for the translation task. The improvement of the translation results is demonstrated on a German-English corpus. 
In this paper, we describe an efﬁcient A* search algorithm for statistical machine translation. In contrary to beamsearch or greedy approaches it is possible to guarantee the avoidance of search errors with A*. We develop various sophisticated admissible and almost admissible heuristic functions. Especially our newly developped method to perform a multi-pass A* search with an iteratively improved heuristic function allows us to translate even long sentences. We compare the A* search algorithm with a beam-search approach on the Hansards task.  
We report on our experience with building a statistical MT system from scratch, including the creation of a small parallel TamilEnglish corpus, and the results of a taskbased pilot evaluation of statistical MT systems trained on sets of ca. 1300 and ca. 5000 parallel sentences of Tamil and English data. Our results show that even with apparently incomprehensible system output, humans without any knowledge of Tamil can achieve performance rates as high as 86% accuracy for topic identiﬁcation, 93% recall for document retrieval, and 64% recall on question answering (plus an additional 14% partially correct answers). 
Grammar Association is a technique for Machine Translation and Language Understanding introduced in 1993 by Vidal, Pieraccini and Levin. All the statistical and structural models involved in the translation process are automatically built from bilingual examples, and the optimal translation of new sentences can be efﬁciently found by Dynamic Programming algorithms. This paper presents and discusses Grammar Association state of the art, including a new statistical model: Loco C. 
We report on a project to derive word translation relationships automatically from parallel corpora. Our effort is distinguished by the use of simpler, faster models than those used in previous high-accuracy approaches. Our methods achieve accuracy on singleword translations that seems comparable to any work previously reported, up to nearly 60% coverage of word types, and they perform particularly well on a class of multi-word compounds of special interest to our translation effort.  text is parsed into logical forms employing the source language grammar and lexicon used in constructing the logical-form training corpus, and the logical-form transfer patterns are used to construct target language logical forms. These logical forms are transformed into target language strings using the target-language lexicon, and a generation grammar written by a linguist. The principal roles played by the translation relationships derived by the methods discussed in this paper are to provide correspondences between content word lemmas in logical forms to assist in the alignment process, and to augment the lexicons used in parsing and generation, for a special case described in Section 4.  
This paper presents on-going research on automatic extraction of bilingual lexicon from English-Japanese parallel corpora. The main objective of this paper is to examine various Ngram models of generating translation units for bilingual lexicon extraction. Three N-gram models, a baseline model (Bound-length N-gram) and two new models (Chunk-bound Ngram and Dependency-linked N-gram) are compared. An experiment with 10000 English-Japanese parallel sentences shows that Chunk-bound Ngram produces the best result in terms of accuracy (83%) as well as coverage (60%) and it improves approximately by 13% in accuracy and by 5-9% in coverage from the previously proposed baseline model. 
We present a system for extracting an English translation of a given Japanese technical term by collecting and scoring translation candidates from the web. We ﬁrst show that there are a lot of partially bilingual documents in the web that could be useful for term translation, discovered by using a commercial technical term dictionary and an Internet search engine. We then present an algorithm for obtaining translation candidates based on the distance of Japanese and English terms in web documents, and report the results of a preliminary experiment. 
In the development of a machine translation system, one important issue is being able to adapt to a specific domain without requiring timeconsuming lexical work. We have experimented with using a statistical word-alignment algorithm to derive word association pairs (French-English) that complement an existing multipurpose bilingual dictionary. This word association information is added to the system at the time of the automatic creation of our translation pattern database, thereby making this database more domain specific. This technique significantly improves the overall quality of translation, as measured in an independent blind evaluation.  The transfer component consists only of correspondences learned during the alignment process. Training takes place on aligned sentences which have been analyzed by the French and English analysis systems to yield dependency structures specific to our system entitled Logical Forms (LF). The LF structures, when aligned, allow the extraction of lexical and structural translation correspondences which are stored for use at runtime in the transfer database. The transfer database can also be thought of as an example-base of conceptual structure representations. See Figure 2 for an illustration of the training process. The transfer database for French-English was trained on approximately 200,000 pairs of aligned sentences from computer manuals and help files. In these aligned pairs, the French text was produced by human translators from the original English version. Sample sentences from the training set are:  
This paper describes a prototype system to visualize and animate 3D scenes from car accident reports, written in French. The problem of generating such a 3D simulation can be divided into two subtasks: the linguistic analysis and the virtual scene generation. As a means of communication between these two modules, we ﬁrst designed a template formalism to represent a written accident report. The CarSim system ﬁrst processes written reports, gathers relevant information, and converts it into a formal description. Then, it creates the corresponding 3D scene and animates the vehicles. 
 The results of the study demonstrate  that numerous object-specific  restrictions on the use of projective  prepositions in English and Russian are  predicted by their interactional  (functional) semantic properties.  Object-independent  perceptual  properties (such as distance between  objects, direction of their motion, etc)  that seemingly guide the use of the  expressions, are also found to be  presupposed by their interactional  properties. Based on these findings, it  is suggested that in addition to a basic  geometrical specification, the semantic  representation should contain  functional  information.  A  computational procedure of matching  an expression with a spatial scene  should thus include detection of the  interactional properties of the scene.  They can be determined through (1)  retrieval of information about  interactional properties of specific  objects and (2) determining  functionally  relevant  object-  independent perceptual properties of  the scene.  
This work presents the semantical analysis of the two spatial prepositions and associated prefixes, the French sur, sur- (on) and the Polish przez, prze- (across). We propose a theory of abstract places (loci), as a method of description which helps to build an invariant meanings of the two linguistics units. 
In this paper, I will argue that the wellknown ambiguity of directional prepositions between the intrinsic and relative readings is not lexical, but can be interpreted as a framework assignment ambiguity of the type observed in the temporal domain. A DRT semantics will then be constructed around a unified model of framework assignment which can be applied across the board to all three universal frame types. 
Conventional information systems cannot cater for temporal information e ectively. For this reason, it is useful to capture and maintain the temporal knowledge (especially the relative knowledge) associated to each action in an information system. In this paper, we propose a model to mine and organize temporal relations embedded in Chinese sentences. Three kinds of event expressions are accounted for, i.e. single event, multiple events and declared event(s). Experiments are conducted to evaluate the mining algorithm using a set of news reports and the results are signi cant. Error analysis has also been performed opening up new doors for future research. 
This paper is concerned with the identification of two semantically close categories – temporal locating adverbials and time-denoting expressions. The dividing line between these categories is difficult to draw, inasmuch as there are several phrases that occur with the same surface form in the typical contexts of both of them (e.g. in adverbial position and as the complement of verbs like to date from). These ambivalent phrases include relatively simple expressions like yesterday or last week, but also – a fact that has gone practically unnoticed in the literature – structurally complex ones, like those headed by before, after, when or ago. In this paper, a uniform semantic categorisation of these phrases as mere time-denoting expressions is advocated and some of its consequences for the grammatical system are assessed. The analysis postulates a null locating preposition (with a value close to that of in) in the contexts where the ambivalent forms occur adverbially. A corollary is the partition of the set of particles traditionally classified as temporal locating into two sets: the truly locating ones – like in, during, since or until – and those that are mere heads of (structurally complex) time-denoting expressions – like before, after, between, when, ago, or from.  
This paper deals with the way temporal connectives affect Temporal Structure as well as Discourse Structure in Narratives. It presents a contrastive study of French connectives puis (then, afterwards) and un peu plus tard (a bit later) within the framework of Segmented Discourse Representation Theory. It shows that puis is a marker of the Narration discourse relation, whereas un peu plus tard blocks Narration and licenses only a weaker discourse relation, that can be considered as a “weak Narration” involving only temporal succession. In addition, puis blocks Result, while un peu plus tard does not. 
This paper computes the semantic representation of while as the pragmatically most relevant one which speakers select from a variety of grammatical constructions in which while may occur in current English. The semantic representation of while provides the condition for translating it into the adequate German equivalent. This computation is implemented in a uniﬁcation–based formalism and may thus be applied in a machine translation system. 
We present a semantic tagging system for temporal expressions and discuss how the temporal information conveyed by these expressions can be extracted. The performance of the system was evaluated wrt. a small hand-annotated corpus of news messages. 
This paper introduces a set of guidelines for annotating time expressions with a canonicalized representation of the times they refer to, and describes methods for extracting such time expressions from multiple languages. 
 The aim of this paper is to present  a language-neutral, theory-neutral  method for annotating sentence-  internal temporal relations. The  annotation method is simple and  can be applied without special  training. The annotations are  provided with a well-defined  model-theoretic interpretation for  use in the content-based  comparison of annotations.  Temporally annotated corpora  have a number of applications, in  lexicon/induction, translation and  linguistic investigation. A  searchable  multi-language  database has already been created.  
In this paper we analyze two question answering tasks : the TREC-8 question answering task and a set of reading comprehension exams. First, we show that Q/A systems perform better when there are multiple answer opportunities per question. Next, we analyze common approaches to two subproblems: term overlap for answer sentence identiﬁcation, and answer typing for short answer extraction. We present general tools for analyzing the strengths and limitations of techniques for these subproblems. Our results quantify the limitations of both term overlap and answer typing to distinguish between competing answer candidates. 
 Sogang University  and Engineering,  Sogang University  
This paper describes machine learning based parsing and question classiﬁcation for question answering. We demonstrate that for this type of application, parse trees have to be semantically richer and structurally more oriented towards semantics than what most treebanks offer. We empirically show how question parsing dramatically improves when augmenting a semantically enriched Penn treebank training corpus with an additional question treebank. 
Mining the answer of a natural language open-domain question in a large collection of on-line documents is made possible by the recognition of the expected answer type in relevant text passages. If the technology of retrieving texts where the answer might be found is well developed, few studies have been devoted to the recognition of the answer type. This paper presents a uniﬁed model of answer types for open-domain Question/Answering that enables the discovery of exact answers. The evaluation of the model, performed on real-world questions, considers both the correctness and the coverage of the answer types as well as their contribution to answer precision.  
Answering precise questions requires applying Natural Language techniques in order to locate the answers inside retrieved documents. The QALC system, presented in this paper, participated to the Question Answering track of the TREC8 and TREC9 evaluations. QALC exploits an analysis of documents based on the search for multi-word terms and their variations. These indexes are used to select a minimal number of documents to be processed and to give indices when comparing question and sentence representations. This comparison also takes advantage of a question analysis module and recognition of numeric and named entities in the documents. 
We propose a question answering system which uses an encyclopedia as a knowledge base. However, since existing encyclopedias lack technical/new terms, we use an encyclopedia automatically generated from the World Wide Web. For this purpose, we ﬁrst search the Web for pages containing a term in question. Then linguistic patterns and HTML structures are used to extract text fragments describing the term. Finally, extracted term descriptions are organized based on word senses and domains. We also evaluate our system by way of experiments, where the Japanese Information-Technology Engineers Examination is used as a test collection. 
 This paper outlines the central role a  range of human language technologies  play in the emerging discipline of  knowledge management.  We  articulate several grand challenges,  illustrate some early successes, and  recommend areas of continued  research.  
 A large and fast growing part of corporate knowledge is encoded in electronic texts. Although digital information repositories are becoming truly multimedial, human language will remain the only medium for preserving and sharing complex concepts, experiences and ideas. It is also the only medium suited for expressing metainformation. For a human reader a text has a rich structure, for a data processing machine it is merely a string of symbols. Classical information retrieval helps to sort and find information in large libraries of documents by matching strings of characters. Effective information management is a building block of modern knowledge management. However, language technology can contribute much more than methods for finding information. A number of areas in which language technologies can improve knowledge management are described in Maybury (in this volume). We will concentrate on examples in which language technologies can facilitate the creation of new knowledge from large volumes of textual information and the sharing of knowledge accross language boundaries. 
AKT is a major research project applying a variety of technologies to knowledge management. Knowledge is a dynamic, ubiquitous resource, which is to be found equally in an expert's head, under terabytes of data, or explicitly stated in manuals. AKT will extend knowledge management technologies to exploit the potential of the semantic web, covering the use of knowledge over its entire lifecycle, from acquisition to maintenance and deletion. In this paper we discuss how HLT will be used in AKT and how the use of HLT will affect different areas of KM, such as knowledge acquisition, retrieval and publishing. 
Though the utility of domain Ontologies is now widely acknowledged in the IT (Information Technology) community, several barriers must be overcome before Ontologies become practical and useful tools. One important achievement would be to reduce the cost of identifying and manually entering several thousand-concept descriptions. This paper describes a text mining technique to aid an Ontology Engineer to identify the important concepts in a Domain Ontology. 
This paper presents the semi-automatic construction method of a practical ontology by using various resources. In order to acquire a reasonably practical ontology in a limited time and with less manpower, we extend the Kadokawa thesaurus by inserting additional semantic relations into its hierarchy, which are classified as case relations and other semantic relations. The former can be obtained by converting valency information and case frames from previously-built computational dictionaries used in machine translation. The latter can be acquired from concept co-occurrence information, which is extracted automatically from large corpora. The ontology stores rich semantic constraints among 1,110 concepts, and enables a natural language processing system to resolve semantic ambiguities by making inferences with the concept network of the ontology. In our practical machine translation system, our ontology-based word sense disambiguation method achieved an 8.7% improvement over methods which do not use an ontology for Korean translation. 
Categorization of text in IR has traditionally focused on topic. As use of the Internet and e−mail increases, categorization has become a key area of research as users demand methods of prioritizing documents. This work investigates text classification by format style, i.e. "genre", and demonstrates, by complementing topic classification, that it can significantly improve retrieval of information. The paper compares use of presentation features to word features, and the combination thereof, using Naïve Bayes, C4.5 and SVM classifiers. Results show use of combined feature sets with SVM yields 92% classification accuracy in sorting seven genres. 
This paper describes a fully implemented system for fusing related news stories into a single comprehensive description of an event. The basic components and the underlying algorithm are explained. The system uses a computationally feasible and robust notion of entailment for comparing information stemming from different documents. We discuss the issue of evaluating document fusion and provide some preliminary results. 
We present a system for the automatic extraction of salient information from email messages, thus providing the gist of their meaning. Dealing with email raises several challenges that we address in this paper: heterogeneous data in terms of length and topic. Our method combines shallow linguistic processing with machine learning to extract phrasal units that are representative of email content. The GIST-IT application is fully implemented and embedded in an active mailbox platform. Evaluation was performed over three machine learning paradigms. Introduction The volume of email messages is huge and growing. A qualitative and quantitative study of email overload [Whittaker and Sidner (1996)] shows that people receive a large number of email messages each day (~ 49) and that 21% of their inboxes (about 334 messages) are long messages (over 10 Kbytes). Therefore summarization techniques adequate for realworld applications are of great interest and need [Berger and Mittal (2000), McKeown and Radev (1995), Kupiec et al (1995), McKeown et al (1999), Hovy (2000)]. In this paper we present GIST-IT, an automatic email message summarizer that will convey to the user the gist of the document through topic phrase extraction, by combining linguistic and machine learning techniques.  Email messages and web documents raise several challenges to automatic text processing, and the summarization task addresses most of them: they are free-style text, not always syntactically or grammatically well-formed, domain and genre independent, of variable length and on multiple topics. Furthermore, due to the lack of well-formed syntactic and grammatical structures, the granularity of document extracts presents another level of complexity. In our work, we address the extraction problem at phrase-level [Ueda et al (2000), Wacholder et al (2000)], identifying salient information that is spread across multiple sentences and paragraphs. Our novel approach first extracts simple noun phrases as candidate units for representing document meaning and then uses machine learning algorithms to select the most prominent ones. This combined method allows us to generate an informative, generic, “at-a-glance” summary. In this paper, we show: (a) the efficiency of the linguistic approach for phrase extraction in comparing results with and without filtering techniques, (b) the usefulness of vector representation in determining proper features to identify contentful information, (c) the benefit of using a new measure of TF*IDF for the noun phrase and its constituents, (d) the power of machine learning systems in evaluating several classifiers in order to select the one performing the best for this task.  
 Decanter illustrates a heuristic  approach to extraction for information  retrieval and question answering.  Generic  information  about  argumentative text is found and  stored, easing user-focused, question-  driven access to the core information.  The emphasis is placed on the  argumentative dimension, to address  in particular three types of questions:  “What are the points?”, “Based on  what?” “What are the comments?”.  The areas of application of this  approach include: question-answering,  information retrieval, summarization,  critical thinking and assistance to  speed reading.  
With increasing amounts of electronic information available, and the increase in the variety of languages used to produce documents of the same type, the problem of how to manage similar documents in different languages arises. This paper proposes an approach to processing/structuring text so that Multilingual Authoring (creating hypertext links) can be eﬀectively carried out. This work, funded by the European Union, is applied to the Multilingual Authoring of news agency text. We have applied methods from Natural Language Processing, especially Information Extraction technology, to both monolingual and Multilingual Authoring. 
In interpreting multilingual queries to databases whose domain information is described in a particular language, we must address the problem of word sense disambiguation. Since full-ﬂedged semantic classiﬁcation information is difﬁcult to construct either automatically or manually for this purpose, we propose to disambiguate the senses of the source lexical items by automatically augmenting a simple translation dictionary with database terminologies and describe an implemented multilingual query interpretation system in a combinatory categorial grammar framework.1 
This paper addresses two related topics: Firstly, it presents building-blocks for ﬂexible multimodal dialog interfaces based on standardized components (VoiceXML, XML) to indicate that thanks to well-supported standardizations, mobile multimodal interfaces to heterogeneous data sources are becoming ready for mass-market deployment, provided that adequate modularization is respected. Secondly, this is put in the perspective of a discussion of knowledge management in ﬁrms, and the paper argues that multimodal dialog systems and the naturalized mobile access to company data they offer will trigger a new knowledge management practice of importance for knowledgeintensive companies. 
We describe in this paper the MUMIS Project (Multimedia Indexing and Searching Environment)1, which is concerned with the development and integration of base technologies, demonstrated within a laboratory prototype, to support automated multimedia indexing and to facilitate search and retrieval from multimedia databases. We stress the role linguistically motivated annotations, coupled with domain-speciﬁc information, can play within this environment. The project will demonstrate that innovative technology components can operate on multilingual, multisource, and multimedia information and create a meaningful and queryable database. 
We examine what purpose a dialog metric serves and then propose empirical methods for evaluating systems that meet that purpose. The methods include a protocol for conducting a wizardof-oz experiment and a basic set of descriptive statistics for substantiating performance claims using the data collected from the experiment as an ideal benchmark or “gold standard” for making comparative judgments. The methods also provide a practical means of optimizing the system through component analysis and cost valuation. 
The paper first addresses a series of issues basic to evaluating the usability of spoken language dialogue systems, including types and purpose of evaluation, when to evaluate and which methods to use, user involvement, how to evaluate and what to evaluate. We then go on to present and discuss a comprehensive set of usability evaluation criteria for spoken language dialogue systems. 
In this paper we discuss the need for corpora with a variety of annotations to provide suitable resources to evaluate different Natural Language Processing systems and to compare them. A supervised machine learning technique is presented for translating corpora between syntactic formalisms and is applied to the task of translating the Penn Treebank annotation into a Categorial Grammar annotation. It is compared with a current alternative approach and results indicate annotation of broader coverage using a more compact grammar. 
The QALC question-answering system, developed at LIMSI, has been a participant for two years in the QA track of the TREC conference. In this paper, we present a quantitative evaluation of various modules in our system, based on two criteria: first, the numbers of documents containing the correct answer and selected by the system; secondly, the number of answers found. The first criterion is used for evaluating locally the modules in the system, which contribute in selecting documents that are likely to contain the answer. The second one provides a global evaluation of the system. As such, it also serves for an indirect evaluation of various modules. 
If Natural Language Processing (NLP) systems are viewed as intelligent systems then we should be able to make use of verification and validation (V&V) approaches and methods that have been developed in the intelligent systems community. This paper addresses language engineering infrastructure issues by considering whether standard V&V methods are fundamentally different than the evaluation practices commonly used for NLP systems, and proposes practical approaches for applying V&V in the context of language processing systems. We argue that evaluation, as it is performed in the NL community, can be improved by supplementing it with methods from the V&V community. 
This paper describes the work achieved in the Concerted Research Project ARC A3 supported and coordinated by the AUF1, former Aupelf-Uref2. The project deals with the evaluation of term and semantic relation extraction from corpora in French. Eight participants, both from public institutions and industrial corporations were involved in this project and were responsible for producing corpora suitable for extraction tasks and elaborating a protocol in order to evaluate objectively terminology acquisition tools. This expression covers respectively, term extractors, classifiers and semantic relation extractors. The paper also reports on the methodology used for comparing four term extractors, one classifier and three semantic relation extractors during the 2000 evaluation campaign. There are also several by-products of this campaign: first, two corpora which can be used for NLP system development and evaluation as the AUF recommended; and then terminology products: for each corpus a list of terms characterizing the field is available. We are not giving details about the results but rather an assessment of what the evaluation of Terminology Extraction Tools is: how was it done, what were the difficulties, which are the advantages and disadvantages of the adopted protocol, what are the limits and how should we proceed for future testing.  
Statistical NLP systems are frequently evaluated and compared on the basis of their performances on a single split of training and test data. Results obtained using a single split are, however, subject to sampling noise. In this paper we argue in favour of reporting a distribution of performance ﬁgures, obtained by resampling the training data, rather than a single number. The additional information from distributions can be used to make statistically quantiﬁed statements about diﬀerences across parameter settings, systems, and corpora. 
Many Natural Language Processing applications require semantic knowledge about topics in order to be possible or to be efﬁcient. So we developed a system, SEGAPSITH, that acquires it automatically from text segments by using an unsupervised and incremental clustering method. In such an approach, an important problem consists of the validation of the learned classes. To do that, we applied another clustering method, that only needs to know the number of classes to build, on the same subset of text segments and we reformulate our evaluation problem in comparing the two classiﬁcations. So, we established different criteria to compare them, based either on the words as class descriptors or on the thematic units. Our ﬁrst results lead to show a great correlation between the two classiﬁcations. 
Determining which linguistic forms are appropriate in what contexts is a hard task. The introspective grammaticality judgment that (perhaps) is legitimate in the study of syntax is methodologically suspect in the study of language use in context, and most work in linguistic pragmatics is in fact corpus-based, such as Prince’s work using the Watergate transcripts and similar corpora (Prince, 1981). Thus, it is clear that the role of corpus-based methods in NLG is not to displace traditional methods, but rather to accelerate them. If indeed corpus-based methods are necessary in any case, we may as well use automated procedures for discovering regularities; we no longer need to use multi-colored pencils to mark up paper copies. For the researcher, there is enough left to do: the corpus-based techniques still require 
In this paper we describe a two-stage model for content determination in systems that summarise time series data. The first stage involves building a qualitative overview of the data set, and the second involves using this overview, together with the actual data, to produce summaries of the timeseries data. This model is based on our observations of how human experts summarise time-series data.  
In this paper, the issue of document structuring is addressed. To achieve this task, we advocate that Segmented Discourse Representation Theory (SDRT) is a most expressive discourse framework. Then we sketch a discourse planning mechanism which aims at producing as many paraphrastic document structures as possible from a set of factual data encoded into a logical form.  
We examine the principle of coextensivity which underlies current algorithms for the generation of referring expressions, and investigate to what extent the principle allows these algorithms to be generalized. The discussion focusses on the generation of complex Boolean descriptions and sentence aggregation.  
This paper describes a new approach to the generation of referring expressions. We propose to formalize a scene as a labeled directed graph and describe content selection as a subgraph construction problem. Cost functions are used to guide the search process and to give preference to some solutions over others. The resulting graph algorithm can be seen as a meta-algorithm in the sense that deﬁning cost functions in different ways allows us to mimic — and even improve— a number of wellknown algorithms. 
We present an algorithm which improves the efﬁciency of a search for the optimally aggregated paragraph which summarises a ﬂat structured input speciﬁcation. We model the space of possible paraphrases of possible paragraphs as the space of sequences of compositions of a set of tree-adjoining grammar (TAG) elementary trees. Our algorithm transforms this to a set with equivalent paraphrasing power but better computational properties. Also, it identiﬁes an explicit mapping between input propositions and their possible surface realisations. 
 This paper presents an overview of a  robust, broad-coverage, and  application-independent  natural  language generation system. It  demonstrates how the different  language generation components  function within a multilingual  Machine Translation (MT) system,  using the languages that we are  currently working on (English,  Spanish, Japanese, and Chinese).  Section 1 provides a system  description. Section 2 focuses on the  generation components and their core  set of rules. Section 3 describes an  additional layer of generation rules  included to address application-  specific issues. Section 4 provides a  brief description of the evaluation  method and results for the MT system  of which our generation components  are a part.  The MT process starts with a source sentence being analyzed by the source-language parser, which produces as output a syntactic tree. This tree is input to the Logical Form module, which produces a deep syntactic representation of the input sentence, called the LF (Heidorn, G. E., 2000). The LF uses the same basic set of relation types for all languages. Figure 1 gives the syntactic tree and LF for the simple English sentence, “I gave the pencils to John”. Tree LF  
This paper presents the implementation of the Vietnamese generation module in ITS3, a multilingual machine translation (MT) system based on the Government & Binding (GB) theory. Despite well-designed generic mechanisms of the system, it turned out that the task of generating Vietnamese posed non-trivial problems. We therefore had to deviate from the generic code and make new design and implementation in many important cases. By developing corresponding bilingual lexicons, we obtained prototypes of French-Vietnamese and English-Vietnamese MT, the former being the first known prototype of this kind. Our experience suggests that in a principle-based generation system, the parameterized modules, which contain language-specific and lexicalized properties, deserve more attention, and the generic mechanisms should be flexible enough to facilitate the integration of these modules. 
We propose a multilingual approach to characterizing word order at the clause level as a means to realize information structure. We illustrate the problem with three languages which differ in the degree of word order freedom they exhibit: Czech, a free word order language in which word order variation is pragmatically determined; English, a ﬁxed word order language in which word order is primarily grammatically determined; and German, a language which is between Czech and English on the scale of word order freedom. Our work is theoretically rooted in previous work on information structuring and word order in the Prague School framework as well as on the systemic-functional notion of Theme. The approach we present has been implemented in KPML. 
The aim of this talk is to show to what extent the work on text generation by computer (TGBC) does not address some of the fundamental problems people struggle with when generating language (TGBP). We will substantiate this claim by taking two tasks on which a lot of research has been carried out during the last 15 years: discourse planning and lexicalisation. 
A relatively self-contained subtask of natural language generation is sentence realization: the process of generating a grammatically correct sentence from an abstract semantic / logical representation. We propose a method where sentence realization is carried out using a simplified (context free) version of a large analysis grammar, combined with a statistical language model from the full (context sensitive) version of the same grammar. The statistical model provides a measure of the probability of syntactic substructures, derived from the analysis of a corpus with the full grammar, and is used to guide both subsequent analysis and generation. 
The task of creating indicative summaries that help a searcher decide whether to read a particular document is a difﬁcult task. This paper examines the indicative summarization task from a generation perspective, by ﬁrst analyzing its required content via published guidelines and corpus analysis. We show how these summaries can be factored into a set of document features, and how an implemented content planner uses the topicality document feature to create indicative multidocument query-based summaries. 
This paper proposes an empirical approach to the development of a computational model for assessing texts according to cohesiveness. We argue that the NLG technologies for the generation of structural paraphrases can be used to efﬁciently create what we call a cohesion-variant parallel corpus, which would serve as a good resource for empirical acquisition of cohesiveness criteria. We also present our pilot case study, in which we took a particular type of paraphrasing that separates a relative clause from a sentence. We have so far created a cohesion-variant parallel corpus containing 499 cohesive instances and 841 incohesive instances. Based on this corpus, we conducted a preliminary experiment on cohesion evaluation, obtaining encouraging results. 
We consider how far two attributes of text quality commonly used in MT evaluation – intelligibility and fidelity – apply within NLG. While the former appears to transfer directly, the latter needs to be completely re-interpreted. We make a crucial distinction between the needs of symbolic authors and those of end-readers. We describe a form of textual feedback, based on a controlled language used for specifying software requirements that appears well suited to authors’ needs, and an approach for incrementally improving the fidelity of this feedback text to the content model. 
Memory-based learning (MBL) has enjoyed considerable success in corpus-based natural language processing (NLP) tasks and is thus a reliable method of getting a high-level of performance when building corpus-based NLP systems. However there is a bottleneck in MBL whereby any novel testing item has to be compared against all the training items in memory base. For this reason there has been some interest in various forms of memory editing whereby some method of selecting a subset of the memory base is employed to reduce the number of comparisons. This paper investigates the use of a modiﬁed self-organising map (SOM) to select a subset of the memory items for comparison. This method involves reducing the number of comparisons to a value proportional to the square root of the number of training items. The method is tested on the identiﬁcation of base noun-phrases in the Wall Street Journal corpus, using sections 15 to 18 for training and section 20 for testing. 
 Selectional preference learning  methods have usually focused on word-  to-class relations, e.g., a verb selects as  its subject a given nominal class. This  papers extends previous statistical  models to class-to-class preferences,  and presents a model that learns  selectional preferences for classes of  verbs. The motivation is twofold:  different senses of a verb may have  different preferences, and some classes  of verbs can share preferences. The  model is tested on a word sense  disambiguation task which uses  subject-verb  and  object-verb  relationships extracted from a small  sense-disambiguated corpus.  
This paper describes a Natural Language Learning method that extracts knowledge in the form of semantic patterns with ontology elements associated to syntactic components in the text. The method combines the use of EuroWordNet’s ontological concepts and the correct sense of each word assigned by a Word Sense Disambiguation(WSD) module to extract three sets of patterns: subject-verb, verb-direct object and verb-indirect object. These sets deﬁne the semantic behaviour of the main textual elements based on their syntactic role. On the one hand, it is shown that Maximum Entropy models applied to WSD tasks provide good results. The evaluation of the WSD module has revealed a accuracy rate of 64% in a preliminary test. On the other hand, we explain how an adequate set of semantic or ontological patterns can improve the success rate of NLP tasks such us pronoun resolution. We have implemented both modules in C++ and although the evaluation has been performed for English, their general features allow the treatment of other languages like Spanish.  
 ing to be explicitly hand-coded.  We propose the use of multilingual corpora In this paper, we explore the use of multilin-  in the automatic classi cation of verbs. We ex- gual corpora in the automatic learning of verb  tend the work of Merlo and Stevenson, 2001, classi cation. We extend the work of Merlo  in which statistics over simple syntactic fea- and Stevenson, 2001, in which statistics over  tures extracted from textual corpora were used simple syntactic features extracted from syn-  to train an automatic classi er for three lexical tactically annotated corpora were used to train  semantic classes of English verbs. We hypoth- an automatic classi er for a set of sample lex-  esize that some lexical semantic features that ical semantic classes of English verbs. This  are di cult to detect super cially in English work had two potential limitations: rst, only  may manifest themselves as easily extractable a small number  ve of syntactic features that  surface syntactic features in another language. correlate with semantic class were proposed;  Our experimental results combining English second, a very large corpus was needed 65M  and Chinese features show that a small bilin- words to extract su ciently discriminating  gual corpus may provide a useful alternative statistics.  to using a large monolingual corpus for verb We address both of these issues in the cur-  classi cation.  rent study by exploiting the use of a parallel  
Signiﬁcant amount of work has been devoted recently to develop learning techniques that can be used to generate partial (shallow) analysis of natural language sentences rather than a full parse. In this work we set out to evaluate whether this direction is worthwhile by comparing a learned shallow parser to one of the best learned full parsers on tasks both can perform — identifying phrases in sentences. We conclude that directly learning to perform these tasks as shallow parsers do is advantageous over full parsers both in terms of performance and robustness to new and lower quality texts. 
We present a general model for PP attachment resolution and NP analysis in French. We make explicit the different assumptions our model relies on, and show how it generalizes previously proposed models. We then present a series of experiments conducted on a corpus of newspaper articles, and assess the various components of the model, as well as the different information sources used. 
One of the particular characteristics of text classiﬁcation tasks is that they present large class imbalances. Such a problem can easily be tackled using resampling methods. However, although these approaches are very simple to implement, tuning them most effectively is not an easy task. In particular, it is unclear whether oversampling is more effective than undersampling and which oversampling or undersampling rate should be used. This paper presents a method for combining different expressions of the re-sampling approach in a mixture of experts framework. The proposed combination scheme is evaluated on a very imbalanced subset of the REUTERS-21578 text collection and is shown to be very effective on this domain. 
This paper reports on the LEARNING COMPUTATIONAL GRAMMARS (LCG) project, a postdoc network devoted to studying the application of machine learning techniques to grammars suitable for computational use. We were interested in a more systematic survey to understand the relevance of many factors to the success of learning, esp. the availability of annotated data, the kind of dependencies in the data, and the availability of knowledge bases (grammars). We focused on syntax, esp. noun phrase (NP) syntax. 
An algorithm is presented for learning a phrase-structure grammar from tagged text. It clusters sequences of tags together based on local distributional information, and selects clusters that satisfy a novel mutual information criterion. This criterion is shown to be related to the entropy of a random variable associated with the tree structures, and it is demonstrated that it selects linguistically plausible constituents. This is incorporated in a Minimum Description Length algorithm. The evaluation of unsupervised models is discussed, and results are presented when the algorithm has been trained on 12 million words of the British National Corpus. 
Unsupervised grammar induction systems commonly judge potential constituents on the basis of their effects on the likelihood of the data. Linguistic justiﬁcations of constituency, on the other hand, rely on notions such as substitutability and varying external contexts. We describe two systems for distributional grammar induction which operate on such principles, using part-of-speech tags as the contextual features. The advantages and disadvantages of these systems are examined, including precision/recall trade-offs, error analysis, and extensibility. 
Information about the animacy of nouns is important for a wide range of tasks in NLP. In this paper, we present a method for determining the animacy of English nouns using WordNet and machine learning techniques. Our method ﬁrstly categorises the senses from WordNet using an annotated corpus and then uses this information in order to classify nouns for which the sense is not known. Our evaluation results show that the accuracy of the classiﬁcation of a noun is around 97% and that animate entities are more difﬁcult to identify than inanimate ones. 
This paper shows that linguistic techniques along with machine learning can extract high quality noun phrases for the purpose of providing the gist or summary of email messages. We describe a set of comparative experiments using several machine learning algorithms for the task of salient noun phrase extraction. Three main conclusions can be drawn from this study: (i) the modiﬁers of a noun phrase can be semantically as important as the head, for the task of gisting, (ii) linguistic ﬁltering improves the performance of machine learning algorithms, (iii) a combination of classiﬁers improves accuracy.  The comparative evaluation of several machine learning models in the settings of our experiments indicates that : (i) for the task of gisting the modiﬁers of the noun phrase are equally as important as the head, (ii) noun phrases are better than ngrams for the phrase-level representation of the document, (iii) linguistic ﬁltering enhances machine learning techniques, (iv) a combination of classiﬁers improves accuracy. Section 2 of the paper outlines the machine learning aspect of extracting salient noun phrases, emphasizing the features used for classiﬁcation and the symbolic machine learning models used in the comparative experiments. Section 3 presents the linguistic ﬁltering steps that improve the accuracy of the machine learning algorithms. Section 4 discusses in detail our conclusions stated above.  
Computational learning of natural language is often attempted without using the knowledge available from other research areas such as psychology and linguistics. This can lead to systems that solve problems that are neither theoretically or practically useful. In this paper we present a system CLL which aims to learn natural language syntax in a way that is both computationally effective and psychologically plausible. This theoretically plausible system can also perform the practically useful task of unsupervised learning of syntax. CLL has then been applied to a corpus of declarative sentences from the Penn Treebank (Marcus et al., 1993; Marcus et al., 1994) on which it has been shown to perform comparatively well with respect to much less psychologically plausible systems, which are signiﬁcantly more supervised and are applied to somewhat simpler problems. 
Many classiﬁcation problems require decisions among a large number of competing classes. These tasks, however, are not handled well by general purpose learning methods and are usually addressed in an ad-hoc fashion. We suggest a general approach – a sequential learning model that utilizes classiﬁers to sequentially restrict the number of competing classes while maintaining, with high probability, the presence of the true outcome in the candidates set. Some theoretical and computational properties of the model are discussed and we argue that these are important in NLP-like domains. The advantages of the model are illustrated in an experiment in partof-speech tagging. 
We evaluate empirically a scheme for combining classifiers, known as stacked generalization, in the context of anti-spam filtering, a novel cost-sensitive application of text categorization. Unsolicited commercial email, or “spam”, floods mailboxes, causing frustration, wasting bandwidth, and exposing minors to unsuitable content. Using a public corpus, we show that stacking can improve the efficiency of automatically induced anti-spam filters, and that such filters can be used in reallife applications. Introduction This paper presents an empirical evaluation of stacked generalization, a scheme for combining automatically induced classifiers, in the context of anti-spam filtering, a novel cost-sensitive application of text categorization. The increasing popularity and low cost of email have intrigued direct marketers to flood the mailboxes of thousands of users with unsolicited messages, advertising anything, from vacations to get-rich schemes. These messages, known as spam or more formally Unsolicited Commercial E-mail, are extremely annoying, as they clutter mailboxes, prolong dial-up connections, and often expose minors to unsuitable content (Cranor & Lamacchia, 1998).  Legal and simplistic technical countermeasures, like blacklists and keyword-based filters, have had a very limited effect so far.1 The success of machine learning techniques in text categorization (Sebastiani, 2001) has recently led to alternative, learning-based approaches (Sahami, et al. 1998; Pantel & Lin, 1998; Drucker, et al. 1999). A classifier capable of distinguishing between spam and non-spam, hereafter legitimate, messages is induced from a manually categorized learning collection of messages, and is then used to identify incoming spam e-mail. Initial results have been promising, and experiments are becoming more systematic, by exploiting recently introduced benchmark corpora, and cost-sensitive evaluation measures (Gomez Hidalgo, et al. 2000; Androutsopoulos, et al. 2000a, b, c). Stacked generalization (Wolpert, 1992), or stacking, is an approach for constructing classifier ensembles. A classifier ensemble, or committee, is a set of classifiers whose individual decisions are combined in some way to classify new instances (Dietterich, 1997). Stacking combines multiple classifiers to induce a higher-level classifier with improved performance. The latter can be thought of as the president of a committee with the ground-level classifiers as members. Each unseen incoming message is first given to the members; the president then decides on the category of the 
We are developing corpus-based techniques for identifying semantic relations at an intermediate level of description (more speciﬁc than those used in case frames, but more general than those used in traditional knowledge representation systems). In this paper we describe a classiﬁcation algorithm for identifying relationships between two-word noun compounds. We ﬁnd that a very simple approach using a machine learning algorithm and a domain-speciﬁc lexical hierarchy successfully generalizes from training instances, performing better on previously unseen words than a baseline consisting of training on the words themselves. 
In this paper we describe a morphological analysis method based on a maximum entropy model. This method uses a model that can not only consult a dictionary with a large amount of lexical information but can also identify unknown words by learning certain characteristics. The model has the potential to overcome the unknown word problem. 
We investigated the applicability of probabilistic context-free grammars to syllabi cation and grapheme-to-phoneme conversion. The results show that the standard probability model of context-free grammars performs very well in predicting syllable boundaries. However, our results indicate that the standard probability model does not solve grapheme-to-phoneme conversion su ciently although, we varied all free parameters of the probabilistic reestimation procedure. 
Most work in statistical parsing has focused on a single corpus: the Wall Street Journal portion of the Penn Treebank. While this has allowed for quantitative comparison of parsing techniques, it has left open the question of how other types of text might a ect parser performance, and how portable parsing models are across corpora. We examine these questions by comparing results for the Brown and WSJ corpora, and also consider which parts of the parser's probability model are particularly tuned to the corpus on which it was trained. This leads us to a technique for pruning parameters to reduce the size of the parsing model. 
SENSEVAL-2: The Second International Workshop on Evaluating Word Sense Disambiguation Systems was held on July 5-6, 2001. This paper gives an overview of SENSEVAL-2, discussing the evaluation exercise, the tasks, the scoring system, and the results. It ends with some recommendations for future evaluation exercises. 
In this paper we describe the Senseval 2 Basque lexical-sample task. The task comprised 40 words (15 nouns, 15 verbs and 10 adjectives) selected from Euskal Hiztegia, the main Basque dictionary. Most examples were taken from the Egunkaria newspaper. The method used to hand-tag the examples produced low inter-tagger agreement (75%) before arbitration. The four competing systems attained results well above the most frequent baseline and the best system scored 75% precision at 100% coverage. The paper includes an analysis of the tagging procedure used, as well as the performance of the competing systems. In particular, we argue that inter-tagger agreement is not a real upperbound for the B,asque WSD task. 
We describe the Dutch word sense disambiguation data submitted to SENSEVAL-2, and give preliminary results on the data using a WSD system based on memory-based learning and statistical keyword selection. 
We describe our experience in preparing the lexicon and sense-tagged corpora used in the English all-words and lexical sample tasks of SENSEVAL-2. 
This paper describes the all-word sense disambiguation task provided by Estonian team at SENSEVAL-2. About 10,000 words are manually disambiguated according to Estonian WordNet word senses. Language-specific problems and lexicon features are discussed. 
 encoding format (XML) proved to facilitate re-use and sharing of the data.  In this paper we give an overall description of the Italian lexical sample task for SENSEVAL-2, together with some general reflections about on the one hand the overall task of lexical-semantic annotation and on the other about the adequacy of existing lexical-semantic reference resources. Introduction In this paper we give an overall description of the Italian lexical sample task for SENSEVAL-2. In the first two sections, the corpus and reference lexicon used are illustrated; the last section contains some general reflections on the basis of the Senseval experience about on the one hand, the overall task of lexical-semantic annotation and on the other, about the adequacy of existing lexical-semantic reference resources. Dictionary and Corpus The dictionary and corpus used for the Italian lexical sample task were provided by the resources developed in the framework of the SITAL project1• The data had not been adapted in order to be used for the Senseval task, apart from the necessary format conversions. A common 
This paper reports an overview of the SENSEVAL-2 Japanese dictionary task. It was a lexical sample task, and word senses are defined according to a Japanese dictionary, the Iwanami Kokugo Jiten. The Iwanami Kokugo Jiten and a training corpus were distributed to all participants. The number of target words was 100, 50 nouns and 50 verbs. One hundred instances of each target word were provided, making for a total of 10,000 instances for evaluation. Seven systems of three organizations participated in this task. 
This paper reports an overview of SENSEVAL-2 Japanese translation task. In this task, word senses are defined according to translation distinction. A translation Memory .· (TM) was constructed, which contains, for each Japanese head word, a list of typical Japanese expressions and their English translations. For each target word instance, a TM record best approximating that usage had to be submitted. Alternatively, submission could take the form of actual target word translations. 9 systems from 7 organizations participated in the task. 
In this paper we describe the structure, organisation and results of the SENSEVAL exercise for Spanish. We present several design decisions we taked for the exercise, we describe the creation of the goldstandard data and finally, we present the results of the evaluation. Twelve systems from five different universities were evaluated. Final scores ranged from 0.56 to 0.65. 
In this paper we describe the organisation and results of the SENSEVAL-2 exercise for Swedish. We present some of the experiences we gained by participating as developers and organisers in the exercise. We particularly focus on the choice of the lexical and corpus material, the annotation process, the scoring scheme, the motivations for choosing the lexical-sample branch of the exercise, the participating systems and the official results. Introduction Word sense ambiguity is a potential source for errors in human language technology applications, such as Machine Translation, and it is considered as the great open problem at the lexical level of Natural Language Processing (NLP). There are, however, several computer programs for automatically determining which sense of a word is being used in a given context, according to a variety of semantic, or defining dictionaries as demonstrated in the SENSEVAL-l exercise; (Kilgarriff and Palmer, 2000). The purpose of SENSEVAL is to be able to say which programs and methods perform better, which worse, which words, or varieties of language, present particular problems to which programs; when modifications improve performance of systems, and how much and what combinations of modifications are optimal. Specifically for Swedish, we would also like to investigate to what extent sense disambiguation can be accomplished and the potential resources available for the task. We would thus be creating a framework that can be shared both within the  exercise and for future evaluation exercises of similar kind, national and international. 
This paper describes two distinct attempts at the SENSEVAL-2 Japanese translation task. The first implementation is based on lexical similarity and builds on the results of Baldwin (2001b; 2001a), whereas the second is based on structural similarity via the medium of parse trees and includes a basic model of conceptual similarity. Despite its simplistic nature, the lexical method was found to perform the better of the two, at 49.1% accuracy, as compared to 41.2% for the structural method and 36.8% for the baseline. 
We describe the University of Maryland's supervised sense tagger, which participated in the SENSEVAL-2 lexical sample evaluations for English, Spanish, and Swedish; we also present unofficial results for Basque. We designed a highly modular combination of language-independent feature extraction and supervised learning using support vector machines in order to permit rapid ramp-up, language independence, and capability for future expansion. 
We present the techniques used in the word sense disambiguation (WSD) system that was submitted to the SENSEVAL-2 workshop. The system builds a probabilistic network per sentence to model the dependencies between the words within the sentence, and the sense tagging for the entire sentence is computed by performing a query over the network. The salient context used for disambiguation is based on sentential structure and not positional information. The parameters are established automatically and smoothed via training data, which was compiled from the SemCor corpus and the WordNet glosses. Lastly, the One-sense-per-discourse (OSPD) hypothesis is incorporated to test its effectiveness. The results from two parameterization techniques and the effects of the OSPD hypothesis are presented. 
The approach presented in this paper for Word Sense Disambiguation (WSD) is based on a combination of different views of the context. Semantic Classification Trees (SCT) are employed over a short and a multi-level view of context, including rough semantic features, while a similarity measure is used in some particular cases to rely on a larger view of the context. We also describe our two-step approach based on HMM for the all-word task. Introduction In the tracks of SENSEVAL-l (Kilgarriff and Rosenzweig, 2000), the second edition of the word sense disambiguation evaluation campaign offers a new set of words to test improvements in the domain of WSD. It also includes a new task, aimed at disambiguating each word of a text. Our approach for the lexical sample task is based on three different views of the context, which allows us to consider more information for sense tagging. In order to deal with shortrange view of the context, we have chosen to use Semantic Classification Trees (SCT) (Kuhn and De Mori, 1995), which are binary decision trees. Moreover, based on our experience, we will show, that using rough semantic features as a higher-level view of the context yields substantial increases in perfom1ance. Finally, a similarity distance is employed in order to capture longer-range context information. The paper is organized as follows: in the first part (Section I), the work we have done on the lexical sample task is presented. This part includes a brief overview of the SCT approach (Section 1. 1) and we show how the coverage it yields could be increased while using more or less rough semantic features thanks to a multi-level view of the context (Section 1.2). In  Section 1.3, we propose to use a similarity measure like those used in document retrieval in order to select a sense among those proposed by the SCT systems. The second part (Section 2) is dedicated to the all-words task. A two-step approach based on a trisem-bisem model is presented (Section 2.1 ). Then, we propose to apply a special process on the most frequent words in the task (Section 2.2). In conclusion, the results for both tasks are presented. 
This paper describes the architecture and results of the TALP system presented at the SENSEVAL-2 exercise for the English lexical-sample task. This system is based on the LazyBoosting algorithm for Word Sense Disambiguation (Escudero et al., 2000), and incorporates some improvements and adaptations to this task. The evaluation reported here includes an analysis of the contribution of each component to the overall system performance. 
We have participated in the SENSEVAL-2 English tasks (all words and lexical sample) with an unsupervised system based on mutual information measured over a large corpus (277 million words) and some additional heuristics. A supervised extension of the system was also presented to the lexical sample task. Our system scored first among unsupervised systems in both tasks: 56.9% recall in all words, 40.2% in lexical sample. This is slightly worse than the first sense heuristic for all. words and 3.6% better for the lexical sample, a strong indication that unsupervised Word Sense Disambiguation remains being a strong challenge. 
This paper describes IITl, IIT2, and IIT3, three versions of a semantic tagging system basing its sense discriminations on WordNet examples. The system uses WordNet relations aggressively, both in identifying examples of words with similar lexical constraints and matching those examples to the context. 
The Stanford-CS224N system is an ensemble of simple classifiers. The first-tier systems are heterogeneous, consisting primarily of naive-Bayes variants, but also including vector space, memory-based, and other classifier types. These simple classifiers are combined by a second-tier classifier, which variously uses majority voting, weighted voting, or a maximum entropy model. Results from SENSEVAL-2 lexical sample tasks indicate that, while the individual classifiers perform at a level comparable to middlescoring team's systems, the combination achieves high performance. In this paper, we discuss both our system and lessons learned from its behavior. 
This paper describes the Sprakdata-ML system as used in the SENSEVAL-2 exercise. The main focus of the paper is devoted to the process of feature extraction, preparation and organization of the test and training data. Introduction The methodology followed for sense disambiguation of the Swedish data by the Sprakdata-ML system is supervised, based on Machine Learning (ML) techniques, particularly Memory Based Learning (MBL). The MBL implementation we used originates from the university of Tilburg in a system called TiMBL; details can be found in Daelemans et al. (1999). Thus, our main contribution in this task has been the effort to try and isolate a set of features that could maximize the performance of the MBL software. However, it is rather difficult to give the exact number of features and examples required for an adequate description of a word's sense or which algorithm performs best. We think that there is space for improvement of our system's performance by better modeling of the available resources (e.g. context, annotations), choice of parameters and algorithms, a claim that we have not explored to its full potential, further exploration is required. Intelligent example selection for supervised learning is an important issue in ML, an issue that we have not fully explored. In previous experiments for a similar problem for Swedish, the algorithm that performed best in TiMBL was a variant of the knearest neighbor (Mitchell, 1997) called IB 1, an algorithm that we also used in the exercise; (Kokkinakis & Johansson Kokkinakis, 1999).  
We propose a translation selection system based on the vector space model. When each translation candidate of a word is given as a pair of expressions containing the word and its translation, selecting the translation of the word can be considered equivalent to selecting the expression having the most similar context among candidate expressions. The proposed method expresses the context information in "context vectors" constructed from content words co-occurring with the target word. Context vectors represent detailed information composed of lexical attributes(word forms, semantic codes, etc.) and syntactic relations (syntactic dependency, etc.) of the co-occurring words. We tested the proposed method with the SENSEVAL-2 Japanese translation task. Precision/recall was 45.8% to the gold standard m the experiment with the evaluation set. 
This paper describes our use of Prolog Word Experts (PWEs) in the SENSEVAL-2 competition. We explain how we specify our PWEs as sequences of transformation rules and how they can be trained on sense tagged corpus data. We give a semantics of PWEs by translating them into first order predicate logic, and we describe how PWEs can be compiled into Prolog procedures. We finally present our results for the Swedish lexical sample task: 63% (fine-grained score) for our best PWE, and a second place in the ranking. 
This paper describes a descriptivesemantic-primitive-based method for word sense disambiguation (WSD) with a machine-tractable dictionary and conceptual distance data among primitives. This approach is using unsupervised learning algorithm and focuses only on the immediately surrounding words and basis morphological form to disambiguate a word sense. This approach also agrees with past observations that human only requires a small window of a few words to perform WSD. (Choueka & Lusignan, 1985). In additional, this paper also describes our experience in doing the English all-word task in SENSEVAL-2. Then, we will discuss the results in the SENSEVAL-2 evaluation. Apart from the description of current system, possibilities for future work are explored 
CL Research's word-sense disambiguation (WSD) system is part of the DIMAP dictionary software, designed to use any full dictionary as the basis for unsupervised disambiguation. Official SENSEVAL-2 results were generated using WordNet, and separately using the New Oxford Dictionary of English (NODE). The disambiguation functionality exploits whatever information is made available by the lexical database. Special routines examined multiword units and contextual clues (both collocations, definition and example content words, and subject matter analyses); syntactic constraints have not yet been employed. The official coarsegrained precision was 0.367 for the lexical sample task and 0.460 for the all-words task (these are actually recall, with actual precision of 0.390 and 0.506 for the two tasks). NODE definitions were automatically mapped into WordNet, with precision of0.405 and 0.418 on 75% and 70% mapping for the lexical sample and all-words tasks, respectively, comparable to WordNet. Bug fixes and implementation of incomplete routines have increased the precision for the lexical sample to 0.429 (with many improvements still likely). Introduction CL Research's participation in SENSEVAL-2 was designed to ( 1) extend WSD techniques from SENSEVAL-l (Litkowski, 2000), (2) generalize WSD mechanisms to rely on a full dictionary rather than a small set of entries where individual crafting might intrude, and (3) investigate WSD using one dictionary mapped into another (WordNet). Results indicate positive achievements for each of these goals. Time constraints precluded a complete  assessment of the upper limits that can be achieved. In particular, although the general architecture from SENSEVAL-l was retained, several specific WSD routines were notreimplemented. Incomplete testing, debugging, and implementation of new routines significantly affected the official results. Several of these problems are investigated more fully below. CL Research's WSD functionality is implemented in DIMAP1, designed primarily for creation and maintenance of lexicons for natural language processing. In particular, DIMAP is designed to make machine-readable dictionaries (MRDs) tractable and to create semantic networks (similar to WordNet (Fellbaum, 1998) and MindNet (Richardson, 1997)) automatically by analyzing and parsing definitions. Section 1 describes the dictionary preparation techniques for WordNet and NODE (The New Oxford Dictionary of English, 1998), as well as the mapping from NODE to WordNet. Section 2 describes the WSD techniques used in SENSEVAL-2. Section 3 describes the SENSEVAL-2 results and section 4 discusses these results .. 
The major goal in ITC-irst's participation at SENSEVAL-2 was to test the role of domain information in word sense disambiguation. The underlying working hypothesis is that domain labels, such as MEDICINE, ARCHITECTURE and SPORT provide a natural way to establish semantic relations among word senses, which can be profitably used during the disambiguation process. For each task in which we participated (i.e. English all words, English 'lexical sample' and Italian 'lexical sample') a different mix of knowledge based and statistical techniques were implemented. 
In this paper we describe the systems we developed for the English (lexical and allwords) and Basque tasks. They were all supervised systems based on Yarowsky's Decision Lists. We used Semcor for training in the English all-words task. We defined different feature sets for each language. For Basque, in order to extract all the information from the text, we defined features that have not been used before in the literature, using a morphological analyzer. We also implemented systems that selected automatically good features and were able to obtain a prefixed precision (85%) at the cost of coverage. The systems that used all the features were identified as BCU-ehu-dlist-all and the systems that selected some features as BCU-ehu-dlistbest. 
Our system for the SENSEVAL-2 all words task uses automatically acquired selectional preferences to sense tag subject and object head nouns, along with the associated verbal predicates. The selectional preferences comprise probability distributions over WordNet nouns, and these distributions are conditioned on WordNet verb classes. The conditional distributions are used directly to disambiguate the head nouns. We use prior distributions and Bayes rule to compute the highest probability verb class, given a noun class. We also use anaphora resolution and the 'one sense per discourse' heuristic to cover nouns and verbs not occurring in these relationships in the target text. The selectional preferences are acquired without recourse to sense tagged data so our system is unsupervised. 
This paper describes a system for word sense disambiguation that participated in the Swedish Lexical Sample task of SENSEVAL-2. The system LIU-WSD is based on letting different contextual features cast votes on preferred senses according to a ranking scheme. Introduction The addition of new languages to the SENSEVAL-2 workshop, among these languages also Swedish, presented an opportunity to learn more about WSD applied to Swedish by participation in the event. Previously, we had had no experience of building word sense disambiguation software, but the Swedish Lexical Sample task seemed like a suitable occasion for trying another field of NLP (in recent years our focus has been on word alignment and parallel corpora). Due to time constraints our initial plans of implementing some kind of version of decision lists (Yarowsky, 2000; Pedersen 2001) were abandoned in the end and we decided to go for a slightly simpler approach based on a general algorithm and voting strategies for contextual features on different levels. The contextual features that were being considered were unigrams and bigrams, both in fixed and variable positions, together with possibilities to include parts-of-speech, lemmas and graph words (inflected words). 
We present here the main ideas of the algorithm employed in the SMUls and SMUaw systems. These systems have participated in the SENSEVAL-2 competition attaining the best performance for both English all words and English lexical sample tasks1. The algorithm has two main components (1) pattern learning from available sense tagged corpora (SemCor) and dictionary definitions (WordNet), and (2) instance based learning with active feature selection, when training data is available for a particular word. 
The WSD system presented at SENSEVAL-2 uses a knowledge-based method for noun disambiguation and a corpus-based method for verbs and adjectives. The methods are, respectively, Specification Marks and Maximum Entropy probability models. So, we can say that this is a hybrid system which joins an unsupervised method with a supervised method. The whole system has been used in lexical sample english task and lexical sample spanish task. 
We submitted four systems to the Japanese dictionary-based lexical-sample task of SENSEVAL-2. They were i) the support vector machine method ii) the simple Bayes method, iii) a method combining the two, and iv) a method combining two kinds of each. The combined methods obtained the best precision among the submitted systems. After the contest, we tuned the parameter used in the simple Bayes method, and it obtained higher preciSIOn. An explanation of these systems used in Japanese word sense disambiguation was provided. 
This paper describes the sixteen Duluth entries in the SENSEVAL-2 comparative exercise among word sense disambiguation systems. There were eight pairs of Duluth systems entered in the Spanish and English lexical sample tasks. These are all based on standard machine learning algorithms that induce classifiers from sense-tagged training text where the context in which ambiguous words occur are represented by simple lexical features. These are highly portable, robust methods that can serve as a foundation for more tailored approaches. 
We describe a simple word sense disambiguation system equipped with the Kennedy and Boguraev (1996) anaphora resolution algorithm, evaluated on the SENSEVAL-2 English all-words task. The system relies on the structure of the WordNet hierarchy to pick optimal senses for nouns in the text. Since anaphoric references are known to indicate the topic of the text (Boguraev et al., 1998), they may aid disambiguation. 
The classification information model or CIM classifies instances by considering the discrimination ability of their features, which was proven to be useful for word sense disambiguation at SENSEVAL-1. But the CIM has a problem of information loss. KUNLP system at SENSEVAL-2 uses a modified version of the CIM for word sense disambiguation. We used three types of features for word sense disambiguation: local, topical, and bigram context. Local and topical context are similar to Chodorow's context and refer to only unigram information. The window of a bigram context is similar to that of a local context but a bigram context refers to only bigram information. We participated in the English lexical sample task and the Korean lexical sample task, where our systems ranked high.  
We present WASP-Bench: a novel approach to Word Sense Disambiguation, also providing a semi-automatic environment for a lexicographer to compose dictionary entries based on corpus evidence. For WSD, involving lexicographers tackles the twin obstacles to high accuracy: paucity of training data and insufficiently explicit dictionaries. For lexicographers, the computational environment fills the need for a corpus workbench which supports WSD. Results under simulated lexicographic use on the English lexical-sample task show precision comparable with supervised systems1, without using the laboriously-prepared training data. 
SENSEVAL-2 was held in Spring, 2001. It consisted of several tasks in various languages. In this paper, we describe our system used for one of these tasks: the Japanese translation task. With an accuracy of 63.4%, our system was the third best system in the contest among nine systems developed by seven groups. 
This paper describes a fully automatic Estonian word sense disambiguation system called semyhe which is based on Estonian WordNet (EstWN) hyponymjhypernym hierarchies and meant to disambiguate both nouns and verbs. 
This article describes the Johns Hopkins University (JHU) sense disambiguation systems that participated in seven SENSEVAL2 tasks: four supervised lexical choice systems (Basque, English, Spanish, Swedish), one unsupervised lexical choice system (Italian) and two supervised all-words systems (Czech, Estonian). The common core supervised system utilizes voting-based classifier combination over several diverse systems, including decision lists (Yarowsky, 2000), a cosine-based vector model and two Bayesian classifiers. The classifiers employed a rich set of features, including words, lemmas and part-of-speech informatino modeled in several syntactic relationships (e.g. verb-object), bag-of-words context and local collocational n-grams. The allwords systems relied heavily on morphological analysis in the two highly inflected languages. The unsupervised Italian system was a hierarchical class model using the Italian WordNet. 
The 3 billion base pair sequence of the human genome is now available, and attention is focusing on annotating it to extract biological meaning. I will discuss what we have obtained, and the methods that are being used to analyse biological sequences. In particular I will discuss approaches using stochastic grammars analogous to those used in computational linguistics, both for gene finding and protein family classification.   
This paper addresses recent progress in speaker-independent, large vocabulary, continuous speech recognition, which has opened up a wide range of near and mid-term applications. One rapidly expanding application area is the processing of broadcast audio for information access. At LIMSI, broadcast news transcription systems have been developed for English, French, German, Mandarin and Portuguese, and systems for other languages are under development. Audio indexation must take into account the speciﬁcities of audio data, such as needing to deal with the continuous data stream and an imperfect word transcription. Some near-term applications areas are audio data mining, selective dissemination of information and media monitoring. 
In this paper, we propose adding long-term grammatical information in a Whole Sentence Maximun Entropy Language Model (WSME) in order to improve the performance of the model. The grammatical information was added to the WSME model as features and were obtained from a Stochastic Context-Free grammar. Finally, experiments using a part of the Penn Treebank corpus were carried out and signiﬁcant improvements were acheived.  
In this paper, we compare the relative eﬀects of segment order, segmentation and segment contiguity on the retrieval performance of a translation memory system. We take a selection of both bag-of-words and segment order-sensitive string comparison methods, and run each over both characterand word-segmented data, in combination with a range of local segment contiguity models (in the form of N-grams). Over two distinct datasets, we ﬁnd that indexing according to simple character bigrams produces a retrieval accuracy superior to any of the tested word Ngram models. Further, in their optimum conﬁguration, bag-of-words methods are shown to be equivalent to segment ordersensitive methods in terms of retrieval accuracy, but much faster. We also provide evidence that our ﬁndings are scalable. 
The amount of readily available on-line text has reached hundreds of billions of words and continues to grow. Yet for most core natural language tasks, algorithms continue to be optimized, tested and compared after training on corpora consisting of only one million words or less. In this paper, we evaluate the performance of different learning methods on a prototypical natural language disambiguation task, confusion set disambiguation, when trained on orders of magnitude more labeled data than has previously been used. We are fortunate that for this particular application, correctly labeled training data is free. Since this will often not be the case, we examine methods for effectively exploiting very large corpora when labeled data comes at a cost.  potentially large cost of annotating data for those learning methods that rely on labeled text. The empirical NLP community has put substantial effort into evaluating performance of a large number of machine learning methods over fixed, and relatively small, data sets. Yet since we now have access to significantly more data, one has to wonder what conclusions that have been drawn on small data sets may carry over when these learning methods are trained using much larger corpora. In this paper, we present a study of the effects of data size on machine learning for natural language disambiguation. In particular, we study the problem of selection among confusable words, using orders of magnitude more training data than has ever been applied to this problem. First we show learning curves for four different machine learning algorithms. Next, we consider the efficacy of voting, sample selection and partially unsupervised learning with large training corpora, in hopes of being able to obtain the benefits that come from significantly larger training corpora without incurring too large a cost.  
In this paper we argue that comparative evaluation in anaphora resolution has to be performed using the same pre-processing tools and on the same set of data. The paper proposes an evaluation environment for comparing anaphora resolution algorithms which is illustrated by presenting the results of the comparative evaluation of three methods on the basis of several evaluation measures. 
The theoretical study of the range concatenation grammar [RCG] formalism has revealed many attractive properties which may be used in NLP. In particular, range concatenation languages [RCL] can be parsed in polynomial time and many classical grammatical formalisms can be translated into equivalent RCGs without increasing their worst-case parsing time complexity. For example, after translation into an equivalent RCG, any tree a¢¤d£¦jo¥¨i§n© ing grammar can be parsed in time. In this paper, we study a parsing technique whose purpose is to improve the practical efﬁciency of RCL parsers. The non-deterministic parsing  choices of the main parser for a lan- guage are directed by a guide which uses the shared derivation forest output  by a prior RCL parser for a suitable su- perset of . The results of a practical evaluation of this method on a wide coverage English grammar are given.  An oracle always indicates all the good ways that  will eventually lead to success, and those good  ways only, while a guide will indicate all the good  ways but may also indicate some wrong ways. In  other words, an oracle is a perfect guide (Kay,  2000), and the worst guide indicates all possi-    ble ways. Given two problems and and    their respective solutions and , if they are   "!#¨ such that  , any algorithm which solves    is a candidate guide for nondeterministic al-   gorithms solving . Obviously, supplementary  $ conditions have to be fulﬁlled for to be a guide.  The ﬁrst one deals with relative efﬁciency: it as- $ sumes that problem can be solved more efﬁ  ciently than problem . Of course, parsers are  privileged candidates to be guided. In this pa-  per we apply this technique to the parsing of a  subset of RCLs that are the languages deﬁned by  RCGs. The syntactic formalism of RCGs is pow-  erful while staying computationally tractable. In-  deed, the positive version of RCGs [PRCGs] de-  ﬁnes positive RCLs [PRCLs] that exactly cover  the class PTIME of languages recognizable in de-  terministic polynomial time. For example, any  mildly context-sensitive language is a PRCL.  
While paraphrasing is critical both for interpretation and generation of natural language, current systems use manual or semi-automatic methods to collect paraphrases. We present an unsupervised learning algorithm for identiﬁcation of paraphrases from a corpus of multiple English translations of the same source text. Our approach yields phrasal and single word lexical paraphrases as well as syntactic paraphrases. 
This paper presents a formal analysis for a large class of words called alternative markers, which includes other(than), such(as), and besides. These words appear frequently enough in dialog to warrant serious attention, yet present natural language search engines perform poorly on queries containing them. I show that the performance of a search engine can be improved dramatically by incorporating an approximation of the formal analysis that is compatible with the search engine’s operational semantics. The value of this approach is that as the operational semantics of natural language applications improve, even larger improvements are possible. 
We aim at finding the minimal set of fragments which achieves maximal parse accuracy in Data Oriented Parsing. Experiments with the Penn Wall Street Journal treebank show that counts of almost arbitrary fragments within parse trees are important, leading to improved parse accuracy over previous models tested on this treebank (a precis ion of 90.8% and a recall of 90.6%). We isolate some dependency relations which previous models neglect but which contribute to higher parse accuracy. 
For ambiguous sentences, traditional semantics construction produces large numbers of higher-order formulas, which must then be -reduced individually. Underspeciﬁed versions can produce compact descriptions of all readings, but it is not known how to perform -reduction on these descriptions. We show how to do this using -reduction constraints in the constraint language for ¡ -structures (CLLS). 
We address the issue of on-line detection of communication problems in spoken dialogue systems. The usefulness is investigated of the sequence of system question types and the word graphs corresponding to the respective user utterances. By applying both ruleinduction and memory-based learning techniques to data obtained with a Dutch train time-table information system, the current paper demonstrates that the aforementioned features indeed lead to a method for problem detection that performs signiﬁcantly above baseline. The results are interesting from a dialogue perspective since they employ features that are present in the majority of spoken dialogue systems and can be obtained with little or no computational overhead. The results are interesting from a machine learning perspective, since they show that the rule-based method performs signiﬁcantly better than the memory-based method, because the former is better capable of representing interactions between features. 
Educators are interested in essay evaluation systems that include feedback about writing features that can facilitate the essay revision process. For instance, if the thesis statement of a student’s essay could be automatically identified, the student could then use this information to reflect on the thesis statement with regard to its quality, and its relationship to other discourse elements in the essay. Using a relatively small corpus of manually annotated data, we use Bayesian classification to identify thesis statements. This method yields results that are much closer to human performance than the results produced by two baseline systems. 
The RAGS proposals for generic speciﬁcation of NLG systems includes a detailed account of data representation, but only an outline view of processing aspects. In this paper we introduce a modular processing architecture with a concrete implementation which aims to meet the RAGS goals of transparency and reusability. We illustrate the model with the RICHES system – a generation system built from simple linguisticallymotivated modules. 
 This paper addresses the issue of  designing embodied conversational  agents that exhibit appropriate posture  shifts during dialogues with human  users. Previous research has noted the  importance of hand gestures, eye gaze  and head nods in conversations  between embodied agents and humans.  We present an analysis of human  monologues and dialogues that  suggests that postural shifts can be  predicted as a function of discourse  state in monologues, and discourse and  conversation state in dialogues. On the  basis of these findings, we have  implemented  an  embodied  conversational agent that uses  Collagen in such a way as to generate  postural shifts.  1. Introduction  This paper provides empirical support for the relationship between posture shifts and discourse structure, and then derives an algorithm for generating posture shifts in an animated embodied conversational agent from discourse states produced by the middleware architecture known as Collagen [18]. Other nonverbal behaviors have been shown to be correlated with the underlying conversational structure and information structure of discourse. For example, gaze shifts towards the listener correlate with a shift in conversational turn (from the conversational participants’  perspective, they can be seen as a signal that the floor is available). Gestures correlate with rhematic content in accompanying language (from the conversational participants’ perspective, these behaviors can be seen as a signal that accompanying speech is of high interest). A better understanding of the role of nonverbal behaviors in conveying discourse structures enables improvements in the naturalness of embodied dialogue systems, such as embodied conversational agents, as well as contributing to algorithms for recognizing discourse structure in speech-understanding systems. Previous work, however, has not addressed major body shifts during discourse, nor has it addressed the nonverbal correlates of topic shifts. 2. Background Only recently have computational linguists begun to examine the association of nonverbal behaviors and language. In this section we review research by non-computational linguists and discuss how this research has been employed to formulate algorithms for natural language generation or understanding. About three-quarters of all clauses in descriptive discourse are accompanied by gestures [17], and within those clauses, the most effortful part of gestures tends to co-occur with or just before the phonologically most prominent syllable of the accompanying speech [13]. It has been shown that when speech is ambiguous or in a speech situation with some noise, listeners rely on  gestural cues [22] (and, the higher the noise-tosignal ratio, the more facilitation by gesture). Even when gestural content overlaps with speech (reported to be the case in roughly 50% of utterances, for descriptive discourse), gesture often emphasizes information that is also focused pragmatically by mechanisms like prosody in speech. In fact, the semantic and pragmatic compatibility in the gesture-speech relationship recalls the interaction of words and graphics in multimodal presentations [11]. On the basis of results such as these, several researchers have built animated embodied conversational agents that ally synthesized speech with animated hand gestures. For example, Lester et al. [15] generate deictic gestures and choose referring expressions as a function of the potential ambiguity and proximity of objects referred to. Rickel and Johnson [19]'s pedagogical agent produces a deictic gesture at the beginning of explanations about objects. André et al. [1] generate pointing gestures as a sub-action of the rhetorical action of labeling, in turn a sub-action of elaborating. Cassell and Stone [3] generate either speech, gesture, or a combination of the two, as a function of the information structure status and surprise value of the discourse entity. Head and eye movement has also been examined in the context of discourse and conversation. Looking away from one’s interlocutor has been correlated with the beginning of turns. From the speaker’s point of view, this look away may prevent an overload of visual and linguistic information. On the other hand, during the execution phase of an utterance, speakers look more often at listeners. Head nods and eyebrow raises are correlated with emphasized linguistic items – such as words accompanied by pitch accents [7]. Some eye movements occur primarily at the ends of utterances and at grammatical boundaries, and appear to function as synchronization signals. That is, one may request a response from a listener by looking at the listener, and suppress the listener’s response by looking away. Likewise, in order to offer the floor, a speaker may gaze at the listener at the end of the utterance. When the listener wants the floor, s/he may look at and slightly up at the  speaker [10]. It should be noted that turn taking only partially accounts for eye gaze behavior in discourse. A better explanation for gaze behavior integrates turn taking with the information structure of the propositional content of an utterance [5]. Specifically, the beginning of themes are frequently accompanied by a look-away from the hearer, and the beginning of rhemes are frequently accompanied by a look-toward the hearer. When these categories are co-temporaneous with turn construction, then they are strongly predictive of gaze behavior. Results such as these have led researchers to generate eye gaze and head movements in animated embodied conversational agents. Takeuchi and Nagao, for example, [21] generate gaze and head nod behaviors in a “talking head.” Cassell et al. [2] generate eye gaze and head nods as a function of turn taking behavior, head turns just before an utterance, and eyebrow raises as a function of emphasis. To our knowledge, research on posture shifts and other gross body movements, has not been used in the design or implementation of computational systems. In fact, although a number of conversational analysts and ethnomethodologists have described posture shifts in conversation, their studies have been qualitative in nature, and difficult to reformulate as the basis of algorithms for the generation of language and posture. Nevertheless, researchers in the non-computational fields have discussed posture shifts extensively. Kendon [13] reports a hierarchy in the organization of movement such that the smaller limbs such as the fingers and hands engage in more frequent movements, while the trunk and lower limbs change relatively rarely. A number of researchers have noted that changes in physical distance during interaction seem to accompany changes in the topic or in the social relationship between speakers. For example Condon and Osgton [9] have suggested that in a speaking individual the changes in these more slowly changing body parts occur at the boundaries of the larger units in the flow of speech. Scheflen (1973) also reports that posture shifts and other general body  movements appear to mark the points of change between one major unit of communicative activity and another. Blom & Gumperz (1972) identify posture changes and changes in the spatial relationship between two speakers as indicators of what they term "situational shifts" -- momentary changes in the mutual rights and obligations between speakers accompanied by shifts in language style. Erickson (1975) concludes that proxemic shifts seem to be markers of 'important' segments. In his analysis of college counseling interviews, they occurred more frequently than any other coded indicator of segment changes, and were therefore the best predictor of new segments in the data. Unfortunately, in none of these studies are statistics provided, and their analyses rely on intuitive definitions of discourse segment or “major shift”. For this reason, we carried out our own empirical study. 3. Empirical Study Videotaped “pseudo-monologues” and dialogues were used as the basis for the current study. In “pseudo-monologues,” subjects were asked to describe each of the rooms in their home, then give directions between four pairs of locations they knew well (e.g., home and the grocery store). The experimenter acted as a listener, only providing backchannel feedback (head nods, smiles and paraverbals such as "uh-huh"). For dialogues, two subjects were asked to generate an idea for a class project that they would both like to work on, including: 1) what they would work on; 2) where they would work on it (including facilities, etc.), and 3) when they would work on it. Subjects stood in both conditions and were told to perform their tasks in 5-10 minutes. The pseudo-monologue condition (pseudo- because there was in fact an interlocutor, although he gave backchannel feedback only and never took the turn) allowed us to investigate the relationship between discourse structure and posture shift independent of turn structure. The two tasks were constructed to allow us to identify exactly where discourse segment boundaries would be placed. The video data was transcribed and coded for three features: discourse segment boundaries,  turn boundaries, and posture shifts. A discourse segment is taken to be an aggregation of utterances and sub-segments that convey the discourse segment purpose, which is an intention that leads to the segment initiation [12]. In this study we chose initially to look at high-level discourse segmentation phenomena rather than those discourse segments embedded deeper in the discourse. Thus, the time points at which the assigned task topics were started served as segmentation points. Turn boundaries were coded (for dialogues only) as the point in time in which the start or end of an utterance cooccurred with a change in speaker, but excluding backchannel feedback. Turn overlaps were coded as open-floor time. We defined a posture shift as a motion or a position shift for a part of the human body, excluding hands and eyes (which we have dealt with in other work). Posture shifts were coded with start and end time of occurrence (duration), body part in play (for this paper we divided the body at the waistline and compared upper body vs. lower body shifts), and an estimated energy level of the posture shift. Energy level was normalized for each subject by taking the largest posture shift observed for each subject as 100% and coding all other posture shift energies relative to the 100% case. Posture shifts that occurred as part of gesture or were clearly intentionally generated (e.g., turning one's body while giving directions) were not coded. 4. Results Data from seven monologues and five dialogues were transcribed, and then coded and analyzed independently by two raters. A total of 70.5 minutes of data was analyzed (42.5 minutes of dialogue and 29.2 minutes of monologue). A total of 67 discourse segments were identified (25 in the dialogues and 42 in the monologues), which constituted 407 turns in the dialogue data. We used the instructions given to subjects concerning the topics to discuss as segmentation boundaries. In future research, we will address the smaller discourse segmentation. For posture shift coding, raters coded all posture shifts independently, and then calculated reliability on the transcripts of one monologue (5.2 minutes) and both speakers from one dialogue (8.5  minutes). Agreement on the presence of an upper body or lower body posture shift in a particular location (taking location to be a 1second window that contains all of or a part of a posture shift) for these three speakers was 89% (kappa = .64). For interrater reliability of the coding of energy level, a Spearman’s rho revealed a correlation coefficient of .48 (p<.01). 4.1 Analysis Posture shifts occurred regularly throughout the data (an average of 15 per speaker in both pseudo-monologues and dialogues). This, together with the fact that the majority of time was spent within discourse segments and within turns (rather than between segments), led us to normalize our posture shift data for comparison purposes. For relatively brief intervals (interdiscourse-segment and inter-turn) normalization by number of inter-segment occurrences was sufficient (ps/int), however, for long intervals (intra-discourse segment and intra-turn) we needed to normalize by time to obtain meaningful comparisons. For this normalization metric we looked at posture-shifts-per-second (ps/s). This gave us a mean average of .06 posture shifts/second (ps/s) in the monologues (SD=.07), and .07 posture shifts/second in the dialogues (SD=.08).  Table 4.1.1. Posture WRT Discourse Segments  Monologues  Dialogues  ps/s ps/int energy  inter- 0.340 0.837 0.832 dseg  intra- 0.039 dseg  0.701  ps/s 0.332 0.053  ps/int energy 0.533 0.844 0.723  Our initial analysis compared posture shifts made by the current speaker within discourse segments (intra-dseg) to those produced at the boundaries of discourse segments (inter-dseg). It can be seen (in Table 4.1.1) that posture shifts occur an order of magnitude more frequently at discourse segment boundaries than within discourse segments in both monologues and dialogues. Posture shifts also tend to be more energetic at discourse segment boundaries (F(1,251)=10.4; p<0.001).  Table 4.1.2 Posture Shifts WRT Turns  ps/s  ps/int  energy  inter-turn 0.140  0.268  0.742  intra-turn 0.022  0.738  Initially, we classified data as being inter- or intra-turn. Table 4.1.2 shows that turn structure does have an influence on posture shifts; subjects were five times more likely to exhibit a shift at a boundary than within a turn.  Table 4.1.3 Posture by Discourse and Turn Breakdown  inter-dseg/start-turn inter-dseg/mid-turn inter-dseg/end-turn intra-dseg/start-turn intra-dseg/mid-turn intra-dseg/end-turn  ps/s 0.562 0.000 0.130 0.067 0.041 0.053  ps/int 0.542 0.000 0.125 0.135 0.107  An interaction exists between turns and discourse segments such that discourse segment boundaries are ten times more likely to co-occur with turn changes than within turns. Both turn and discourse structure exhibit an influence on posture shifts, with discourse having the most predictive value. Starting a turn while starting a new discourse segment is marked with a posture shift roughly 10 times more often than when starting a turn while staying within discourse segment. We noticed, however, that posture shifts appeared to congregate at the beginnings or ends of turn boundaries, and so our subsequent analyses examined start-turns, midturns and end-turns. It is clear from these results that posture is indeed correlated with discourse state, such that speakers generate a posture shift when initiating a new discourse segment, which is often at the boundary between turns. In addition to looking at the occurrence and energy of posture shifts we also analyzed the distributions of upper vs. lower body shifts and the duration of posture shifts. Speaker upper body shifts were found to be used more frequently at the start of turns (48%) than at the middle of turns (36%) or end of turns (18%) (F(2,147)=5.39; p<0.005), with no significant  dependence on discourse structure. Finally, speaker posture shift duration was found to change significantly as a function of both turn and discourse structure (see Figure 4.1.3). At the start of turns, posture shift duration is approximately the same whether a new topic is introduced or not (2.5 seconds). However, when ending a turn, speakers move significantly longer (7.0 seconds) when finishing a topic than when the topic is continued by the other interlocutor (2.7 seconds) (F(1,148)=17.9; p<0.001).  8  7  6  5  4  3  start  2  
We present two language models based upon an “immediate-head” parser — our name for a parser that conditions all events below a constituent c upon the head of c. While all of the most accurate statistical parsers are of the immediate-head variety, no previous grammatical language model uses this technology. The perplexity for both of these models signiﬁcantly improve upon the trigram model base-line as well as the best previous grammarbased language model. For the better of our two models these improvements are 24% and 14% respectively. We also suggest that improvement of the underlying parser should signiﬁcantly improve the model’s perplexity and that even in the near term there is a lot of potential for improvement in immediatehead language models. 
We consider the question “How much strong generative power can be squeezed out of a formal system without increasing its weak generative power?” and propose some theoretical and practical constraints on this problem. We then introduce a formalism which, under these constraints, maximally squeezes strong generative power out of context-free grammar. Finally, we generalize this result to formalisms beyond CFG. 
We develop a framework for formalizing semantic construction within grammars expressed in typed feature structure logics, including HPSG. The approach provides an alternative to the lambda calculus; it maintains much of the desirable ﬂexibility of uniﬁcationbased approaches to composition, while constraining the allowable operations in order to capture basic generalizations and improve maintainability. 
We present a machine learning approach to evaluating the wellformedness of output of a machine translation system, using classifiers that learn to distinguish human reference translations from machine translations. This approach can be used to evaluate an MT system, tracking improvements over time; to aid in the kind of failure analysis that can help guide system development; and to select among alternative output strings. The method presented is fully automated and independent of source language, target language and domain. 
 Polarized dependency (PD-) grammars  are proposed as a means of efﬁcient  treatment of discontinuous construc-  tions. PD-grammars describe two kinds  of dependencies : local, explicitly de-  rived by the rules, and long, implicitly  speciﬁed by negative and positive va-  lencies of words. If in a PD-grammar  the number of non-saturated valencies  in derived structures is bounded by a  ¢¡¤£¦¥¨§ constant, then it is weakly equivalent  to a cf-grammar and has a  -  time parsing algorithm. It happens that  such bounded PD-grammars are strong  enough to express such phenomena as  unbounded raising, extraction and ex-  traposition.  
Current alternatives for language modeling are statistical techniques based on large amounts of training data, and hand-crafted context-free or ﬁnite-state grammars that are difﬁcult to build and maintain. One way to address the problems of the grammar-based approach is to compile recognition grammars from grammars written in a more expressive formalism. While theoretically straight-forward, the compilation process can exceed memory and time bounds, and might not always result in accurate and efﬁcient speech recognition. We will describe and evaluate two approaches to this compilation problem. We will also describe and evaluate additional techniques to reduce the structural ambiguity of the language model. 
In a language generation system, a content planner embodies one or more “plans” that are usually hand–crafted, sometimes through manual analysis of target text. In this paper, we present a system that we developed to automatically learn elements of a plan and the ordering constraints among them. As training data, we use semantically annotated transcripts of domain experts performing the task our system is designed to mimic. Given the large degree of variation in the spoken language of the transcripts, we developed a novel algorithm to ﬁnd parallels between transcripts based on techniques used in computational genomics. Our proposed methodology was evaluated two–fold: the learning and generalization capabilities were quantitatively evaluated using cross validation obtaining a level of accuracy of 89%. A qualitative evaluation is also provided. 
We describe a new framework for dependency grammar, with a modular decomposition of immediate dependency and linear precedence. Our approach distinguishes two orthogonal yet mutually constraining structures: a syntactic dependency tree and a topological dependency tree. The syntax tree is nonprojective and even non-ordered, while the topological tree is projective and partially ordered. 
This paper presents methods for a qualitative, unbiased comparison of lexical association measures and the results we have obtained for adjective-noun pairs and preposition-noun-verb triples extracted from German corpora. In our approach, we compare the entire list of candidates, sorted according to the particular measures, to a reference set of manually identiﬁed “true positives”. We also show how estimates for the very large number of hapaxlegomena and double occurrences can be inferred from random samples. 
We propose a method to generate large-scale encyclopedic knowledge, which is valuable for much NLP research, based on the Web. We ﬁrst search the Web for pages containing a term in question. Then we use linguistic patterns and HTML structures to extract text fragments describing the term. Finally, we organize extracted term descriptions based on word senses and domains. In addition, we apply an automatically generated encyclopedia to a question answering system targeting the Japanese InformationTechnology Engineers Examination. 
Typically, the lexicon models used in statistical machine translation systems do not include any kind of linguistic or contextual information, which often leads to problems in performing a correct word sense disambiguation. One way to deal with this problem within the statistical framework is to use maximum entropy methods. In this paper, we present how to use this type of information within a statistical machine translation system. We show that it is possible to signiﬁcantly decrease training and test corpus perplexity of the translation models. In addition, we per- form a rescoring of ¢ -Best lists us- ing our maximum entropy model and thereby yield an improvement in translation quality. Experimental results are presented on the so-called “Verbmobil Task”. 
While the generative view of language processing builds bigger units out of smaller ones by means of rewriting steps, the axiomatic view eliminates invalid linguistic structures out of a set of possible structures by means of wellformedness principles. We present a generator based on the axiomatic view and argue that when combined with a TAG-like grammar and a ﬂat semantics, this axiomatic view permits avoiding drawbacks known to hold either of top-down or of bottom-up generators. 
This paper proposes a description of German word order including phenomena considered as complex, such as scrambling, (partial) VP fronting and verbal pied piping. Our description relates a syntactic dependency structure directly to a topological hierarchy without resorting to movement or similar mechanisms.1 
A good decoding algorithm is critical to the success of any statistical machine translation system. The decoder’s job is to ﬁnd the translation that is most likely according to set of previously learned parameters (and a formula for combining them). Since the space of possible translations is extremely large, typical decoding algorithms are only able to examine a portion of it, thus risking to miss good solutions. In this paper, we compare the speed and output quality of a traditional stack-based decoding algorithm with two new decoders: a fast greedy decoder and a slow but optimal decoder that treats decoding as an integer-programming optimization problem. 
We offer a computational analysis of the resolution of ellipsis in certain cases of dialogue clariﬁcation. We show that this goes beyond standard techniques used in anaphora and ellipsis resolution and requires operations on highly structured, linguistically heterogeneous representations. We characterize these operations and the representations on which they operate. We offer an analysis couched in a version of Head-Driven Phrase Structure Grammar combined with a theory of information states (IS) in dialogue. We sketch an algorithm for the process of utterance integration in ISs which leads to grounding or clariﬁcation. 
This paper describes automatic techniques for mapping 9611 entries in a database of English verbs to WordNet senses. The verbs were initially grouped into 491 classes based on syntactic features. Mapping these verbs into WordNet senses provides a resource that supports disambiguation in multilingual applications such as machine translation and cross-language information retrieval. Our techniques make use of (1) a training set of 1791 disambiguated entries, representing 1442 verb entries from 167 classes; (2) word sense probabilities, from frequency counts in a tagged corpus; (3) semantic similarity of WordNet senses for verbs within the same class; (4) probabilistic correlations between WordNet data and attributes of the verb classes. The best results achieved 72% precision and 58% recall, versus a lower bound of 62% precision and 38% recall for assigning the most frequently occurring WordNet sense, and an upper bound of 87% precision and 75% recall for human judgment. 
We introduce a new categorial formalism based on intuitionistic linear logic. This formalism, which derives from current type-logical grammars, is abstract in the sense that both syntax and semantics are handled by the same set of primitives. As a consequence, the formalism is reversible and provides different computational paradigms that may be freely composed together. 
We describe the use of XML tokenisation, tagging and mark-up tools to prepare a corpus for parsing. Our techniques are generally applicable but here we focus on parsing Medline abstracts with the ANLT wide-coverage grammar. Hand-crafted grammars inevitably lack coverage but many coverage failures are due to inadequacies of their lexicons. We describe a method of gaining a degree of robustness by interfacing POS tag information with the existing lexicon. We also show that XML tools provide a sophisticated approach to pre-processing, helping to ameliorate the ‘messiness’ in real language data and improve parse performance. 
A hybrid system is described which combines the strength of manual rulewriting and statistical learning, obtaining results superior to both methods if applied separately. The combination of a rule-based system and a statistical one is not parallel but serial: the rule-based system performing partial disambiguation with recall close to 100% is applied ﬁrst, and a trigram HMM tagger runs on its results. An experiment in Czech tagging has been performed with encouraging results. 
This paper presents an open-domain textual Question-Answering system that uses several feedback loops to enhance its performance. These feedback loops combine in a new way statistical results with syntactic, semantic or pragmatic information derived from texts and lexical databases. The paper presents the contribution of each feedback loop to the overall performance of 76% human-assessed precise answers. 
We present conditions under which verb phrases are elided based on a corpus of positive and negative examples. Factor that affect verb phrase ellipsis include: the distance between antecedent and ellipsis site, the syntactic relation between antecedent and ellipsis site, and the presence or absence of adjuncts. Building on these results, we examine where in the generation architecture a trainable algorithm for VP ellipsis should be located. We show that the best performance is achieved when the trainable module is located after the realizer and has access to surfaceoriented features (error rate of 7.5%). 
In this paper we address the problem of extracting key pieces of information from voicemail messages, such as the identity and phone number of the caller. This task differs from the named entity task in that the information we are interested in is a subset of the named entities in the message, and consequently, the need to pick the correct subset makes the problem more difﬁcult. Also, the caller’s identity may include information that is not typically associated with a named entity. In this work, we present three information extraction methods, one based on hand-crafted rules, one based on maximum entropy tagging, and one based on probabilistic transducer induction. We evaluate their performance on both manually transcribed messages and on the output of a speech recognition system. 
It is widely recognized that the proliferation of annotation schemes runs counter to the need to re-use language resources, and that standards for linguistic annotation are becoming increasingly mandatory. To answer this need, we have developed a representation framework comprised of an abstract model for a variety of different annotation types (e.g., morpho-syntactic tagging, syntactic annotation, co-reference annotation, etc.), which can be instantiated in different ways depending on the annotators approach and goals. In this paper we provide an overview of our representation framework and demonstrate its applicability to syntactic annotation. We show how the framework can contribute to comparative evaluation and merging of parser output and diverse syntactic annotation schemes. 
Named entity (NE) recognition is a task in which proper nouns and numerical information in a document are detected and classiﬁed into categories such as person, organization, location, and date. NE recognition plays an essential role in information extraction systems and question answering systems. It is well known that hand-crafted systems with a large set of heuristic rules are difﬁcult to maintain, and corpus-based statistical approaches are expected to be more robust and require less human intervention. Several statistical approaches have been reported in the literature. In a recent Japanese NE workshop, a maximum entropy (ME) system outperformed decision tree systems and most hand-crafted systems. Here, we propose an alternative method based on a simple rule generator and decision tree learning. Our experiments show that its performance is comparable to the ME approach. We also found that it can be trained more efﬁciently with a large set of training data and that it improves readability. 
This paper compares two different ways of estimating statistical language models. Many statistical NLP tagging and parsing models are estimated by maximizing the (joint) likelihood of the fully-observed training data. However, since these applications only require the conditional probability distributions, these distributions can in principle be learnt by maximizing the conditional likelihood of the training data. Perhaps somewhat surprisingly, models estimated by maximizing the joint were superior to models estimated by maximizing the conditional, even though some of the latter models intuitively had access to “more information”. 
We present a rule−based shallow− parser compiler, which allows to generate a robust shallow−parser for any language, even in the absence of training data, by resorting to a very limited number of rules which aim at identifying constituent boundaries. We contrast our approach to other approaches used for shallow−parsing (i.e. finite−state and probabilistic methods). We present an evaluation of our tool for English (Penn Treebank) and for French (newspaper corpus "LeMonde") for several tasks (NP−chunking & "deeper" parsing) . 
This paper presents empirical studies and closely corresponding theoretical models of the performance of a chart parser exhaustively parsing the Penn Treebank with the Treebank’s own CFG grammar. We show how performance is dramatically affected by rule representation and tree transformations, but little by top-down vs. bottom-up strategies. We discuss grammatical saturation, including analysis of the strongly connected components of the phrasal nonterminals in the Treebank, and model how, as sentence length increases, the effective grammar rule size increases as regions of the grammar are unlocked, yielding super-cubic observed time behavior in some conﬁgurations. 
Chunk parsing has focused on the recognition of partial constituent structures at the level of individual chunks. Little attention has been paid to the question of how such partial analyses can be combined into larger structures for complete utterances. Such larger structures are not only desirable for a deeper syntactic analysis. They also constitute a necessary prerequisite for assigning function-argument structure. The present paper offers a similaritybased algorithm for assigning functional labels such as subject, object, head, complement, etc. to complete syntactic structures on the basis of prechunked input. The evaluation of the algorithm has concentrated on measuring the quality of functional labels. It was performed on a German and an English treebank using two different annotation schemes at the level of function-argument structure. The results of 89.73 % correct functional labels for German and 90.40 % for English validate the general approach. 
Previous research has shown that the plausibility of an adjective-noun combination is correlated with its corpus co-occurrence frequency. In this paper, we estimate the co-occurrence frequencies of adjective-noun pairs that fail to occur in a 100 million word corpus using smoothing techniques and compare them to human plausibility ratings. Both class-based smoothing and distance-weighted averaging yield frequency estimates that are signiﬁcant predictors of rated plausibility, which provides independent evidence for the validity of these smoothing techniques. 
We provide a logical deﬁnition of Minimalist grammars, that are Stabler’s formalization of Chomsky’s minimalist program. Our logical deﬁnition leads to a neat relation to categorial grammar, (yielding a treatment of Montague semantics), a parsing-asdeduction in a resource sensitive logic, and a learning algorithm from structured data (based on a typing-algorithm and type-uniﬁcation). Here we emphasize the connection to Montague semantics which can be viewed as a formal computation of the logical form. 
This paper focuses on the analysis and prediction of so-called aware sites, deﬁned as turns where a user of a spoken dialogue system ﬁrst becomes aware that the system has made a speech recognition error. We describe statistical comparisons of features of these aware sites in a train timetable spoken dialogue corpus, which reveal signiﬁcant prosodic differences between such turns, compared with turns that ‘correct’ speech recognition errors as well as with ‘normal’ turns that are neither aware sites nor corrections. We then present machine learning results in which we show how prosodic features in combination with other automatically available features can predict whether or not a user turn was a normal turn, a correction, and/or an aware site.  a user and a spoken dialogue system (SDS) exhibit more frequent communication breakdowns, due mainly to errors in the Automatic Speech Recognition (ASR) component of these systems. In such interactions, however, there is also evidence showing prosodic information may be used as a resource for error recovery. In previous work, we identiﬁed new procedures to detect recognition errors. In particular, we found that prosodic features, in combination with other information already available to the recognizer, can distinguish user turns that are misrecognized by the system far better than traditional methods used in ASR rejection (Litman et al., 2000; Hirschberg et al., 2000). We also found that user corrections of system misrecognitions exhibit certain typical prosodic features, which can be used to identify such turns (Swerts et al., 2000; Hirschberg et al., 2001). These ﬁndings are consistent with previous research showing that corrections tend to be hyperarticulated — higher, louder, longer ...than other turns (Wade et al., 1992; Oviatt et al., 1996; Levow, 1998; Bell and Gustafson, 1999).  
This paper presents an approach to automatically build a semantic perceptron net (SPN) for topic spotting. It uses context at the lower layer to select the exact meaning of key words, and employs a combination of context, co-occurrence statistics and thesaurus to group the distributed but semantically related words within a topic to form basic semantic nodes. The semantic nodes are then used to infer the topic within an input document. Experiments on Reuters 21578 data set demonstrate that SPN is able to capture the semantics of topics, and it performs well on topic spotting task. 1. Introduction Topic spotting is the problem of identifying the presence of a predefined topic in a text document. More formally, given a set of n topics together with a collection of documents, the task is to determine for each document the probability that one or more topics is present in the document. Topic spotting may be used to automatically assign subject codes to newswire stories, filter electronic emails and online news, and pre-screen document in information retrieval and information extraction applications. Topic spotting, and its related problem of text categorization, has been a hot area of research for over a decade. A large number of techniques have been proposed to tackle the problem, including: regression model, nearest neighbor classification, Bayesian probabilistic model, decision tree,  inductive rule learning, neural network, on-line learning, and, support vector machine (Yang & Liu, 1999; Tzeras & Hartmann, 1993). Most of these methods are word-based and consider only the relationships between the features and topics, but not the relationships among features. It is well known that the performance of the word-based methods is greatly affected by the lack of linguistic understanding, and, in particular, the inability to handle synonymy and polysemy. A number of simple linguistic techniques has been developed to alleviate such problems, ranging from the use of stemming, lexical chain and thesaurus (Jing & Tzoukermann, 1999; Green, 1999), to word-sense disambiguation (Chen & Chang, 1998; Leacock et al, 1998; Ide & Veronis, 1998) and context (Cohen & Singer, 1999; Jing & Tzoukermann, 1999). The connectionist approach has been widely used to extract knowledge in a wide range of information processing tasks including natural language processing, information retrieval and image understanding (Anderson, 1983; Lee & Dubin, 1999; Sarkas & Boyer, 1995; Wang & Terman, 1995). Because the connectionist approach closely resembling human cognition process in text processing, it seems natural to adopt this approach, in conjunction with linguistic analysis, to perform topic spotting. However, there have been few attempts in this direction. This is mainly because of difficulties in automatically constructing the semantic networks for the topics. In this paper, we propose an approach to automatically build a semantic perceptron net (SPN) for topic spotting. The SPN is a connectionist model with hierarchical structure. It uses a combination of context, co-occurrence  statistics and thesaurus to group the distributed but semantically related words to form basic semantic nodes. The semantic nodes are then used to identify the topic. This paper discusses the design, implementation and testing of an SPN for topic spotting. The paper is organized as follows. Section 2 discusses the topic representation, which is the prototype structure for SPN. Sections 3 & 4 respectively discuss our approach to extract the semantic correlations between words, and build semantic groups and topic tree. Section 5 describes the building and training of SPN, while Section 6 presents the experiment results. Finally, Section 7 concludes the paper.  2. Topic Representation  The frame of Minsky (1975) is a well-known knowledge representation technique. A frame represents a high-level concept as a collection of slots, where each slot describes one aspect of the concept. The situation is similar in topic spotting. For example, the topic “water ” may have many aspects (or sub-topics). One sub-topic may be about “water supply ”, while the other is about “water and environment protection”, and so on. These sub-topics may have some common attributes, such as the word “water”, and each subtopic may be further sub-divided into finer sub- topics, etc. The above points to a hierarchical topic representation, which corresponds to the hierarchy of document classes (Figure 1). In the model, the contents of the topics and sub-topics (shown as circles) are modeled by a set of attributes, which is simply a group of semantically related words (shown as solid elliptical shaped bags or rectangles). The context (shown as dotted ellipses) is used to identify the exact meaning of a word.  topic  common attribute  Sub - topic  a word  Aspect attribute the context of a word Figure 1. Topic representation  Hofmann (1998) presented a word occurrence based cluster abstraction model that learns a hierarchical topic representation. However, the method is not suitable when the set of training examples is sparse. To avoid the problem of automatically constructing the hierarchical model, Tong et al (1987) required the users to supply the model, which is used as queries in the system. Most automated methods, however, avoided this problem by modeling the topic as a feature vector, rule set, or instantiated example (Yang & Liu, 1999). These methods typically treat each word feature as independent, and seldom consider linguistic factors such as the context or lexical chain relations among the features. As a result, these methods are not good at discriminating a large number of documents that typically lie near the boundary of two or more topics. In order to facilitate the automatic extraction and modeling of the semantic aspects of topics, we adopt a compromise approach. We model the topic as a tree of concepts as shown in Figure 1. However, we consider only one level of hierarchy built from groups of semantically related words. These semantic groups may not correspond strictly to sub-topics within the domain. Figure 2 shows an example of an automatically constructed topic tree on “water ”.  water  Topic  provide rain  water  costumer corporation  rainfall dry  a b plant  c  waste  Basic Semantic  environment Nodes bank  d  price agreement water e ton  water river tourist f  Contexts  Figure 2. An example of a topic tree  In Figure 2, node “a” contains the common feature set of the topic; while nodes “b”, “c” and “d” are related to sub-topics on “water supply”, “rainfall”, and “water and environment protection” respectively. Node “e” is the context of the word “plant”, and node “f” is the context of the word “bank”. Here we use training to automatically resolve the corresponding relationship between a node and an attribute, and the context word to be used to select the exact meaning of a word. From this representation, we observe that: a) Nodes “c” and “d” are closely related and may not be fully separable. In fact, it is sometimes difficult even for human experts to decide how to divide them into separate topics.  b) The same word, such as “water ”, may appear in both the context node and the basic semantic node. c) Some words use context to resolve their meanings, while many do not need context. 3. Semantic Correlations Although there exists many methods to derive the semantic correlations between words (Lee, 1999; Lin, 1998; Karov & Edelman, 1998; Resnik, 1995; Dagan et al, 1995), we adopt a relatively simple and yet practical and effective approach to derive three topic -oriented semantic correlations: thesaurus-based, co-occurrence-based and contextbased correlation. 3.1 Thesaurus based correlation WordNet is an electronic thesaurus popularly used in many researches on lexical semantic acquisition, and word sense disambiguation (Green, 1999; Leacock et al, 1998). In WordNet, the sense of a word is represented by a list of synonyms (synset), and the lexical information is represented in the form of a semantic network. However, it is well known that the granularity of semantic meanings of words in WordNet is often too fine for practical use. We thus need to enlarge the semantic granularity of words in practical applications. For example, given a topic on “children education”, it is highly likely that the word “child” will be a key term. However, the concept “child” can be expressed in many semantically relate d terms, such as “boy”, “girl ”, “kid”, “child”, “youngster”, etc. In this case, it might not be necessary to distinguish the different meaning among these words, nor the different senses within each word. It is, however, important to group all these words into a large synset {child, boy, girl, kid, youngster}, and use the synset to model the dominant but more general meaning of these words in the context. In general, it is reasonable and often useful to group lexically related words together to represent a more general concept. Here, two words are considered to be lexically related if they are related to by the “is_a”, “part_of”, “member_of”, or “antonym” relations, or if they belong to the same synset. Figure 3 lists the lexical relations that we considered, and the examples. Since in our experiment, there are many antonyms co-occur within the topic, we also group antonyms together to identify a topic. Moreover, if a word had two senses of, say, sense-1 and sense-2. And if there are two separate words that are  lexically related to this word by sense-1 and sense2 respectively, we simply group these words together and do not attempt to distinguish the two different senses. The reason is because if a word is so important to be chosen as the keyword of a topic, then it should only have one dominant meaning in that topic. The idea that a keyword should have only one dominant meaning in a topic is also suggested in Church & Yarowsky (1992).  synset is_a  part_of member_of antonym  corn  metal  tree  family  import  maize zinc  leaf  son  export  Figure 3p:eEr xamples of lexicapl ererlationshpieprso  Based on the above discussion, we compute the  thesaurus-based correlation between the two terms  t1 and t2, in topic Ti, as:  
We present a set of algorithms that enable us to translate natural language sentences by exploiting both a translation memory and a statistical-based translation model. Our results show that an automatically derived translation memory can be used within a statistical framework to often ﬁnd translations of higher probability than those found using solely a statistical model. The translations produced using both the translation memory and the statistical model are signiﬁcantly better than translations produced by two commercial systems: our hybrid system translated perfectly 58% of the 505 sentences in a test collection, while the commercial systems translated perfectly only 40-42% of them. 
In this paper we discuss our approach toward establishing a model of the acquisition of English grammatical structures by users of our English language tutoring system, which has been designed for deaf users of American Sign Language. We explore the correlation between a corpus of error-tagged texts and their holistic proﬁciency scores assigned by experts in order to draw initial conclusions about what language errors typically occur at different levels of proﬁciency in this population. Since errors made at lower levels (and not at higher levels) presumably represent constructions acquired before those on which errors are found only at higher levels, this should provide insight into the order of acquisition of English grammatical forms. 
An approach to automatic detection of syllable boundaries is presented. We demonstrate the use of several manually constructed grammars trained with a novel algorithm combining the advantages of treebank and bracketed corpora training. We investigate the effect of the training corpus size on the performance of our system. The evaluation shows that a hand-written grammar performs better on ﬁnding syllable boundaries than does a treebank grammar. 
This paper considers three assumptions conventionally made about signatures in typed feature logic that are in potential disagreement with current practice among grammar developers and linguists working within feature-based frameworks such as HPSG: meet-semilatticehood, unique feature introduction, and the absence of subtype covering. It also discusses the conditions under which each of these can be tractably restored in realistic grammar signatures where they do not already exist. 
This paper presents a method that assists in maintaining a rule-based named-entity recognition and classification system. The underlying idea is to use a separate system, constructed with the use of machine learning, to monitor the performance of the rule-based system. The training data for the second system is generated with the use of the rule-based system, thus avoiding the need for manual tagging. The disagreement of the two systems acts as a signal for updating the rule-based system. The generality of the approach is illustrated by applying it to large corpora in two different languages: Greek and French. The results are very encouraging, showing that this alternative use of machine learning can assist significantly in the maintenance of rulebased systems. 
Techniques for automatically training modules of a natural language generator have recently been proposed, but a fundamental concern is whether the quality of utterances produced with trainable components can compete with hand-crafted template-based or rulebased approaches. In this paper We experimentally evaluate a trainable sentence planner for a spoken dialogue system by eliciting subjective human judgments. In order to perform an exhaustive comparison, we also evaluate a hand-crafted template-based generation component, two rule-based sentence planners, and two baseline sentence planners. We show that the trainable sentence planner performs better than the rule-based systems and the baselines, and as well as the handcrafted system. 
The STOP system, which generates personalised smoking-cessation letters, was evaluated by a randomised controlled clinical trial. We believe this is the largest and perhaps most rigorous task effectiveness evaluation ever performed on an NLG system. The detailed results of the clinical trial have been presented elsewhere, in the medical literature. In this paper we discuss the clinical trial itself: its structure and cost, what we did and did not learn from it (especially considering that the trial showed that STOP was not effective), and how it compares to other NLG evaluation techniques. 
In this paper we present a thorough evaluation of a corpus resource for Portuguese, CETEMPúblico, a 180million word newspaper corpus free for R&D in Portuguese processing. We provide information that should be useful to those using the resource, and to considerable improvement for later versions. In addition, we think that the procedures presented can be of interest for the larger NLP community, since corpus evaluation and description is unfortunately not a common exercise.  ,QWURGXFWLRQ CETEMPúblico is a large corpus of European Portuguese newspaper language, available at no cost to the community dealing with the processing of Portuguese.1 It was created in the framework of the Computational Processing of Portuguese project, a government funded initiative to foster language engineering of the Portuguese language.2 Evaluating this resource, we have two main goals in mind: To contribute to improve its usefulness; and to suggest ways of going about as far as corpus evaluation is concerned in general (noting that most corpora projects are simply described and not evaluated). 
We describe a biographical multidocument summarizer that summarizes information about people described in the news. The summarizer uses corpus statistics along with linguistic knowledge to select and merge descriptions of people from a document collection, removing redundant descriptions. The summarization components have been extensively evaluated for coherence, accuracy, and non-redundancy of the descriptions produced. 
In a headed tree, each terminal word can be uniquely labeled with a governing word and grammatical relation. This labeling is a summary of a syntactic analysis which eliminates detail, reﬂects aspects of semantics, and for some grammatical relations (such as subject of ﬁnite verb) is nearly uncontroversial. We deﬁne a notion of expected governor markup, which sums vectors indexed by governors and scaled by probabilistic tree weights. The quantity is computed in a parse forest representation of the set of tree analyses for a given sentence, using vector sums and scaling by inside probability and ﬂow. 
The standard pipeline approach to semantic processing, in which sentences are morphologically and syntactically resolved to a single tree before they are interpreted, is a poor ﬁt for applications such as natural language interfaces. This is because the environment information, in the form of the objects and events in the application’s runtime environment, cannot be used to inform parsing decisions unless the input sentence is semantically analyzed, but this does not occur until after parsing in the single-tree semantic architecture. This paper describes the computational properties of an alternative architecture, in which semantic analysis is performed on all possible interpretations during parsing, in polynomial time. 
We propose a statistical method that ﬁnds the maximum-probability segmentation of a given text. This method does not require training data because it estimates probabilities from the given text. Therefore, it can be applied to any text in any domain. An experiment showed that the method is more accurate than or at least as accurate as a state-of-the-art text segmentation system. 
Multi-processor systems are becoming more commonplace and aﬀordable. Based on analyses of actual parsings, we argue that to exploit the capabilities of such machines, uniﬁcation-based grammar parsers should distribute work at the level of individual uniﬁcation operations. We present a generic approach to parallel chart parsing that meets this requirement, and show that an implementation of this technique for LinGO achieves considerable speedups. 
This paper describes the application of the PARADISE evaluation framework to the corpus of 662 human-computer dialogues collected in the June 2000 Darpa Communicator data collection. We describe results based on the standard logﬁle metrics as well as results based on additional qualitative metrics derived using the DATE dialogue act tagging scheme. We show that performance models derived via using the standard metrics can account for 37% of the variance in user satisfaction, and that the addition of DATE metrics improved the models by an absolute 5%. 
We present a syntax-based statistical translation model. Our model transforms a source-language parse tree into a target-language string by applying stochastic operations at each node. These operations capture linguistic differences such as word order and case marking. Model parameters are estimated in polynomial time using an EM algorithm. The model produces word alignments that are better than those produced by IBM Model 5. 
In this paper, a new language model, the Multi-Class Composite N-gram, is proposed to avoid a data sparseness problem for spoken language in that it is difﬁcult to collect training data. The Multi-Class Composite N-gram maintains an accurate word prediction capability and reliability for sparse data with a compact model size based on multiple word clusters, called MultiClasses. In the Multi-Class, the statistical connectivity at each position of the N-grams is regarded as word attributes, and one word cluster each is created to represent the positional attributes. Furthermore, by introducing higher order word N-grams through the grouping of frequent word successions, Multi-Class N-grams are extended to Multi-Class Composite N-grams. In experiments, the Multi-Class Composite N-grams result in 9.5% lower perplexity and a 16% lower word error rate in speech recognition with a 40% smaller parameter size than conventional word 3-grams. 
Many machine learning methods have recently been applied to natural language processing tasks. Among them, the Winnow algorithm has been argued to be particularly suitable for NLP problems, due to its robustness to irrelevant features. However in theory, Winnow may not converge for nonseparable data. To remedy this problem, a modiﬁcation called regularized Winnow has been proposed. In this paper, we apply this new method to text chunking. We show that this method achieves state of the art performance with signiﬁcantly less computation than previous approaches. 
We describe a set of supervised machine learning experiments centering on the construction of statistical models of WH-questions. These models, which are built from shallow linguistic features of questions, are employed to predict target variables which represent a user’s informational goals. We report on different aspects of the predictive performance of our models, including the inﬂuence of various training and testing factors on predictive performance, and examine the relationships among the target variables. 
Captions in videos contain valuable information for video retrieval. Although texts in captions can be obtained easily in the new image compression formats like MPEG2, there still are many video programs encoded in older formats. Thus, video OCR is indispensable for content-based video retrieval. This paper proposes a simple video OCR method for Chinese captions, including image capturing, caption region deciding, background removing, character segmentation, OCR and post-processing. We employed Discovery Channel films as training and testing corpus. In an outside test, the accuracy of the video OCR was 84.1%. The hardware used in the experiment consisted of a computer with a P4-1.7G CPU, 256MB RAM and a 40G, 7200rpm hard disk. On average, it took 29 minutes and 11 seconds to process a film 495MB in size. We also applied the results of video OCR to video retrieval and question answering. Keywords: digital library, question answering, Chinese video OCR, video retrieval 1. Introduction In the new information era, multimedia is widely used, and the amount of existing video data is huge. How to extract the content of video data for further application has become an important issue. The well-known project “Informedia” [Wactlar, 2000] in digital library is a typical example. Captions in videos contain valuable information for video retrieval. Although texts in captions can be easily obtained in the new image compression formats like MPEG2, there still are many video programs encoded in older formats. Thus, video OCR is indispensable for content-based video retrieval. This paper proposes a simple video OCR method for Chinese captions and demonstrates its application in video search and question answering.  * Department of Computer Science and Information Engineering, National Taiwan University, Taipei, TAIWAN, R.O.C. E-mail: {cjlin, jjliu}@nlg2.csie.ntu.edu.tw, hh_chen@csie.ntu.edu.tw  12  C. J. Lin et al.  OCR research started very early and has achieved many good results. In a traditional OCR system, textual data is scanned and saved as images, and then transformed into text files [Lee and Chen, 1996]. There have also been many researches on handwriting OCR. In contrast, video OCR is more challenging than traditional OCR because we have to recognize small characters on a colorful background instead of black characters on a white background. Several approaches have been proposed to video OCR. Wu et al. [1997, 1998] tried to find characters in pictures by means of connected components. Their method performs well on pictures but not films because the background of a film is more complicated, and text will also connect with other objects in the film. Lienhart et al. [1998, 2000] found text by means of color segmentation, contrast segmentation, geometry analysis, and texture analysis. Li, Doermann and Kia [2000] adopted a neural network to detect strings in images. Li and Doermann [1999] also employed multiple images to enhance resolution. Smith and Kande [1997] used text and object shifting, and facial recognition to reduce the size of images. Sato et al. [1998] achieved higher OCR correctness by means of image improving and multi-frame integration. This paper focuses on Chinese captions in videos. Section 2 introduces several issues concerning video OCR and the architecture of our system. Sections 3 to 8 describe each strategy and each module in detail. The performance was evaluated using films made by the Discovery Channel. Section 9 demonstrates an application for question answering. Section 10 presents conclusion. 2. Architecture There are two kinds of texts in videos, i.e., captions and image texts. Captions often appear at specific positions, such as a textual line in the lower part of a screen, or a vertical text line in the left or right part of a screen. Image texts consist of characters appearing in an image, such as shop signs, automobile registration numbers, etc. They are themselves part of the image, so they change their positions when the camera moves. Captions are narratives or dialogues in a film, so they often carry valuable information. The focus of this paper is how to extract texts in captions. Complex backgrounds often show up behind captions; thus, the first problem is how to remove backgrounds. After backgrounds are removed, the remaining captions are black characters on a white background. That will make the following OCR task easier. We also apply a post-processing procedure to improve OCR performance. Figure 1 shows the architecture of the whole system.  A Simple Method for Chinese Video OCR and Its Application to Question Answering 13 Deciding Caption Regions Removing Backgrounds within Single Images 2-Level Binary Image Removing Large Black Areas Removing Backgrounds by Means of Multiple Images Detecting Changes of Captions Removing Backgrounds Character Segmentation and OCR Figure 1 The Architecture of the Video OCR System. To evaluate the performance of the system, some films produced by the Discovery Channel were used as experimental materials. Their topics vary widely from natural science to history, military, adventures and human life. 3. Deciding Caption Regions The characteristics of captions are: (1) they are always in a straight line from left to right or up to down; (2) the characters usually have colors which contrast with the background, and often have perceivable borders; (3) they are always in the foreground of the image; (4) they usually consist of two or more characters; (5) the height of the caption region is not often higher than one third of the height of the image, because characters cannot be too large or too small for reading; (6) they have fixed height, width, and size; (7) they have fixed colors. We employ these characteristics to locate captions. 3.1 Binary Image Before processing, we first transform the original images into binary images. This technique is often used in video processing. It helps to simplify the background and make the retrieval of captions much easier. When extracting images from a film, we take 2 pictures in a second and save them in the BMP format. In a BMP file, the color of each point is recorded as its RGB-value, (red-value, green-value, blue-value). Each value ranges in brightness from 0 to 255. Here, 0 indicates the darkest value and 255 the brightest value.  14  C. J. Lin et al.  Figure 2 An Example of Binary Image Transformation. Using the RGB-values, we can transform an image into a binary image using the following method: Let the binary-threshold be SegColorScore For each point (red-value, green-value, blue-value) in an image: IF red-value, green-value, and blue-value are larger than SegColorScore THEN change the color of this point to black, i.e., (0, 0, 0) ELSE change the color of this point to white, i.e., (255, 255, 255). In our experiment, SegColorScore was set to 190. Figure 2 shows an example of binary image transformation. The captions are clearly separated from the background. The result is black characters on a white background.  3.2 Deciding Caption Regions  After performing binary image transformation, we decide where the captions are. Here, we employ another characteristic of captions: if we draw a horizontal line across a caption, the line will go through many vertical lines of Chinese characters. As in printed characters, these vertical lines are often of the same width.  Consider every point at the same height heighti. A sequence of black points is called a segment. In this way, a horizontal line at heighti is composed of a set SEGMENTi=(segmenti1, segmenti2, … ) of segments. If the difference between the numbers of black points in two neighboring segments is not larger than a predefined threshold (e.g., 3 in this paper), then we  say these two segments belong to the same group. Thus, we have a set GROUPi=(groupi1, groupi2, … ) at heighti. Seg(groupij) is defined as the number of segments in groupij. Now we define Score As Caption Region (abbreviated as SACR hereafter) of heighti as  ( ) ( ) SACRi  =  GROUPi ∑ Seg  groupij  × log2 Seg groupij  .  (1)  j =1  A Simple Method for Chinese Video OCR and Its Application to Question Answering 15  Figure 3 Examples of Deciding Caption Regions (1).  Figure 4 Examples of Deciding Caption Regions (2).  Consider the following example. Here, 0 denotes a white point and 1 a black point.  points:  00111011111001100011101111111011111111100111101110110111111  segments: --111-22222--33---444-5555555-666666666--7777-888-99-AAAAAA  groups:  |---------1----------||--------2-------||-----3-----||--4--|  Seg(group):  4  2  3  
In a text-to-speech (TTS) conversion system based on the time-domain pitch-synchronous overlap-add (TD-PSOLA) method, accurate estimation of pitch periods and pitch marks is necessary for pitch modification to assure optimal quality of synthetic speech. In general, there are two major tasks in pitch marking: pitch detection and location determination. In this paper, an adaptable filter, which serves as a bandpass filter, is proposed for use in pitch detection to transform voiced speech into a sine-like wave. The pass band of the adaptable filter can be adapted based on the fundamental frequency. Based on the sine-like wave, a peak-valley decision method is proposed to determine the appropriate parts (positive part and negative part) of voiced speech for use in pitch mark estimation. In each pitch period, two possible peaks/valleys are searched, and dynamic programming is performed to obtain pitch marks. Experimental results indicate that our proposed method performs very well if correct pitch information is estimated. 1. Introduction In past years, the concatenative synthesis approach has been adopted for use in many text-to-speech (TTS) systems [Hamon et al. 1989][Iwahashi et al. 1995][Shih et al. 1996][Chen et al. 1998][Chou et al. 1998][Charpentier et al. 1986]. Concatenative synthesis uses real recorded speech segments as synthesis units and concatenates them together during synthesis. In addition, the time-domain pitch-synchronous overlap-add (TD-PSOLA) [Charpentier et al. 1986] method has been employed to perform prosody modification. This method modifies the prosodic features of a synthesis unit according to the target prosodic information. Generally, the prosodic information of a speech unit includes its pitch (the fundamental frequency, f0), intensity, duration, etc. For a synthesis scheme based on the * Advanced Techonlogy Center, Computer and Communication Research Laboratories, Industrial Technology Research Institute, Chutung 310, Taiwan Email: chenjh@itri.org.tw, kya@itri.org.tw  32  J. H. Chen, Y. A. Kao  TD-PSOLA method, it is necessary to obtain a pitch mark for each pitch period in order to assure optimal quality of synthetic speech. The pitch mark is a reference point for the overlap between speech signals. A speech synthesizer with various voices is useful for speech synthesis. Sometimes, it is also important for a service-providing company to have a synthesizer with the voice of its own employee or its favorite speaker. For conventional TTS systems, however, it is a demanding and tedious job to create a new voice. Recently, corpus-based TTS systems have been developed which use a large number of speech segments. Some approaches select speech segments as candidates for synthesis units. Establishing synthesis units involves speech segmentation, pitch estimation, pitch marking, and so on. Moreover, pitch marking is very labor-intensive task if no automatic mechanism is available. In general, there are two major tasks in pitch marking: pitch detection and location determination. Compared to the literature on pitch detection [Rabiner et al. 1976][Rabiner 1977][Noll 1967][Markel 1972][Barnard et al. 1991][Kadambe et al. 1991][Barner 2000][Huang et al. 2000], few papers have focused on pitch marking [Moulines et al. 1990][Kobayashi et al. 1998], which is also a difficult problem because of the great variability of speech signals. Moulines et al. [Moulines et al. 1990] proposed a pitch-marking algorithm based on the detection of abrupt changes at glottal closure instants. In each period, they assumed that the speech waveform could be represented by the concatenation of the response of two all-pole systems. On the other hand, Kobayashi et al. [Kobayashi et al. 1998] used dyadic wavelets for pitch marking. The glottal closure instant was detected by searching for a local peak in the wavelet transform of the speech waveform. In this paper, we propose a pitch-marking method based on an adaptable filter and a peak-valley estimation method. The block diagram of our method is shown in Fig. 1. The input signals are limited to voiced speech because only the periodic parts are of interest. We introduce an adaptable filter, which serves as a bandpass filter, to transform voiced speech into a sine-like wave. FFT (Fast Fourier Transform) is used to transform voice to the frequency domain, and the filter’s pass band is determined by finding the spectral peak of the fundamental frequency. Consequently, the pass band can be adapted based on the fundamental frequency. The autocorrelation method is then used to estimate the pitch periods on the sine-like wave. In addition, a peak-valley decision method is employed to determine which part of the voiced speech is suitable for pitch mark estimation. The positive part (the speech with positive amplitude) and the negative part (the speech with negative amplitude) are investigated in this method. This is demonstrated by Fig. 3(a), which shows an example of a waveform having a negative part that reveals explicit periodicity. In general, it is possible to achieve better speech quality if the pitch marks are labeled at the positions of the extreme  Pitch Marking Based on an Adaptable Filter and a Peak-Valley Estimation Method 33 points (peaks and valleys) of speech. In each pitch period, two possible peaks/valleys are searched. Finally, the pitch marks are obtained through dynamic programming by calculating the degree of pitch distortion. Voiced Speech Pitch Detection Adaptable Filter Autocorrelation Pitch Periods Pitch Mark Determination Peak-Valley Decision Peak/Valley Searching Dynamic Programming Pitch Marks Figure 1 Block diagram of the proposed pitch-marking method.  34  J. H. Chen, Y. A. Kao  2. Pitch Detection Using an Adaptable Filter Followed by Application of the Autocorrelation Method The proposed adaptable filter serves as a bandpass filter in which the pass band extends from 50 Hz to the detected fundamental frequency, up to 500 Hz, of the voiced speech. First, we will define the following symbols, which are used in this algorithm: N: frame size in sample. Consecutive frames do not overlap. sm[n]: the voiced speech of the m-th frame, where 0≤n< N. SFm[k]: the frequency response of sm[n], where 0≤k< N. YFm[k]: the pass band frequency response of SFm[k], where 0≤k< N. om[l]: the adaptable filter’s output signal of the m-th frame, where 0≤l<N. The algorithm of the adaptable filter is described as follows: Step 1. Use FFT to transform the signal sm[n] to obtain the frequency response SFm[k]. Step 2. Find the position kp of the spectral peak of the fundamental frequency for SFm[k] by searching the first forty points of ⏐SFm[k]⏐. Step 3. Decide on the filter’s pass band. Let YFm[k]=SFm[k] if 3≤k≤kp+2 or 3≤N-k≤kp+2; otherwise, let YFm[k]=0. Step 4. Normalize YFm[k] by multiplying a scale of Maxk(⏐YFm[k]⏐)/⏐YFm[kp]⏐. Step 5. Use IFFT (Inverse FFT) to transform the normalized YFm[k] to the time domain. Let om[n] be the real part of the time domain signal. Finally, the refined pitch periods are obtained by analyzing the filtered speech o[n] using the conventional autocorrelation method. The waveform of om[n] after IFFT may be discontinuous at the frame boundaries. A typical example is shown in Fig. 2. However, such waveform discontinuity is not very significant and does not significantly affect the results of pitch period estimation. Discontinuity  Figure 2 A typical example of waveform discontinuity after IFFT. An example of an adaptable filter is displayed in Fig. 3. Panels (a) and (b) show the waveforms of the original speech and the filtered speech, respectively. It can be seen that the  Pitch Marking Based on an Adaptable Filter and a Peak-Valley Estimation Method 35 filtered speech is generally a sine-like wave with clear periodicity than the original speech waveform. For a frame in the middle of the voiced speech, the spectral contour is depicted in panel (d). Note that the frequency axis is not linearly plotted to allow inspection of the first spectral peak. The first peak was found at 168 Hz, which was the fundamental frequency. Finally, the pitch periods were obtained by analyzing the filtered speech using the conventional autocorrelation method. (a) (b) (c) The first peak (d) Figure 3 Results obtained using the adaptable filter and pitch mark determination. (a) Waveform of the voiced speech with explicit periodicity in the negative part. (b) Waveform of the filtered speech. (c) Detected pitch marks. (d) Spectral contour (note that the frequency axis is not linearly plotted). 3. Pitch Mark Determination Using a Peak-Valley Decision Method and Dynamic Programming 3.1 Peak-Valley Decision From observations, we have found that voiced speech, s[·], is synchronous with filtered  36  J. H. Chen, Y. A. Kao  speech, o[·], either at peaks or at valleys. The cases illustrated in Figs. 3 (a) and 2 (b) are  synchronous at valleys having explicit periodicity instead of at peaks. As a result, the pitch  marks can be more easily determined in the negative part than in the positive part. In the  following, the peak-valley decision method is used to calculate two costs by summing the  amplitudes of s[q], where q represents the position of the local extreme point of o[·] over each  pitch period:  ∑ C peak  =  
Machine Translation is one of the most difficult problems in the field of natural language processing. In the past, MT has been applied to professional communication in the process of translating technical and corporate document on a specific domain. Recent years saw the rapid development of Internet as a new form of communication and information exchange, and the need to access information across the language barrier became apparent. People began to look into the role that MT can play in Cross Language Information Retrieval. The prevalent approach to CLIR is based on translation of query, in particular query  *  E-mail: jschang@cs.nthu.edu.tw  +  44 phrases. However, for CLIR there is an additional new objective of translating into something that is relevant to the collection being searched upon. Therefore, the current approach of using general bilingual word list or an off-the-shelf commercial MT software is bound to be very ineffective in terms of retrieving relevant documents. We propose a new approach to Statistical Phrase Translation Model (SPTM), aimed at achieving a tighter estimation of phrase translation. Experiments were conducted using bilingual phrases in BDC Electronic Chinese-English Dictionary. Preliminary results shows the approach is much faster and produces better word alignment for phrases, which has not been possible using previous approaches. Keywords: Statistical Machine Translation; Phrase Translation; Cross-language Information Retrieval. 1.  Gaussier, and Daille, 1997] Chen, 1997] 2001]  Machine Assisted Human Translation [Lange, Cross-Language Information Retrieval [Gey and Computer Assisted Language Learning [Shei and Pain,  query translation  document translation  [McCarley 1999]  NTCIR-2  [Kando et al. 2001]  l Assembly Parade Law l Parade and Demonstration l Constitution l Freedom of speech l Communism l Council of Grand Justices l Legislation l Amendments  45  Veronis 1998, Chen and Chang 1998] Collection  Word Sense Disambiguation  [Ide and  Phrase Translation  Text  Lexical Choice  demonstration  1.  [Gey and Chen 1997, Kwok 2001]  2.  [Oard 1999, Kwok 2001]  Kwok  Michael Jordan  statistics  Chen [1999]  [Knight and Graehl 1997, Chang et al. 2001]  occurrence  Example-based  Approach  data-driven  Statistical Translation Model  [Jones and Havrilla, 1998]  [Gale and Church  1992, Melamed 2000]  [Melamed 2000] IBM Watson  Brown  [1988, 1990, 1993]  Wu [1997]  Wang [1998] Och  [1999]  Yamata Knight [2001]  Machine Translation Probability  Statistical Alignment and Assignment  46  2.  Brown [1993] Translation Probability  Direct Approach  Transfer Approach  1980  Empirical Approach  Brown  Model 3  S  T  Pr( T | S )  (a)  Lexical Translation Probability  Pr( Tj | Si )  (b)  Fertility Probability  Pr( a | Si )  (c)  Distortion Probability  Pr(j | i, k, m )  Brown  Si S Tj T a Tj kS mT  i j Expectation and Maximization Algorithm  Estimation EM Noisy Channel Model Language Model  Maximum Likelihood N-gram Beam Search  47  3. IBM Model 3  alignment  j  
Cluster-based n-gram modeling is a variant of normal word-based n-gram modeling. It attempts to make use of the similarities between words. In this paper, we present an empirical study of clustering techniques for Asian language modeling. Clustering is used to improve the performance (i.e. perplexity) of language models as well as to compress language models. Experimental tests are presented for cluster-based trigram models on a Japanese newspaper corpus and on a Chinese heterogeneous corpus. While the majority of previous research on word clustering has focused on how to get the best clusters, we have concentrated our research on the best way to use the clusters. Experimental results show that some novel techniques we present work much better than previous methods, and achieve more than 40% size reduction at the same level of perplexity. 1. Introduction Statistical language modelling (SLM) has been successfully applied in many domains, such as speech recognition, optical character recognition, machine translation, spelling correction, information retrieval, and spoken language understanding [Jelinek, 1990; Church, 1988; Brown et al., 1990; Kernighan et al., 1990; Miller et al., 1999; Zue, 1995]. The dominant technology in SLM is n-gram models. Typically, n-gram models are trained on very large corpora. In constructing n-gram models, we always face two problems. First, for a general domain model, large amounts of training data can lead to models that are too large for realistic applications. On the other hand, for specific domains, n-gram models usually suffer from the data sparseness problem because large amounts of domain-specific data are usually not available. When n-gram models are used, we can define clusters for similar words in a corpus. We * Microsoft Research, Asia, Beijing, 100080, P.R.C. E-mail: jfgao@microsoft.com + Microsoft Research, Redmond Washington 98052, USA. E-mail: joshuago@microsoft.com ** Department of Computer & Information Sciences University of Delaware, USA. This work was done while the author was visiting Microsoft Research Asia.  28  J. Gao et al.  thus extend word-based n-gram models to cluster-based n-gram models. This has been demonstrated as an effective way to handle the data sparseness problem. Recent research also shows that cluster-based n-gram models are effective for rapid domain adaptation, training on small data sets, and reducing the memory requirements for realistic applications. Extending our previous work in [Goodman, 2001; Gao et al., 2001; Goodman and Gao, 2000], this paper presents an empirical study of clustering techniques for Asian language modeling. Clustering is used to improve the performance (i.e. perplexity) of language models as well as to compress language models. Experimental tests will be presented for cluster-based trigram models on a Japanese newspaper corpus of more than 10 million words, and on a Chinese heterogeneous corpus of more than 11 million characters. The majority of the previous research on word clustering has focused on how to get the best clusters. We have concentrated our research on the best way to use the clusters. Experimental results show that some novel techniques work much better than previous methods. This paper is structured as follows: In the remainder of this section, we present an introduction to n-gram models, smoothing, and performance evaluation. In Section 2, we briefly review previous work on word clustering and cluster-based n-gram models. In Section 3, we present our technique of using clusters for trigram models. In Section 4, we describe our method for finding clusters. In Section 5, we present the results of our main experiments. Finally, we present our conclusions in Section 6.  1.1 N-gram models The classic task of language modeling is to predict the next word given the previous words. The n-gram model is the usual approach. It states the task of predicting the next word as attempting to estimate the conditional probability:  P(wn ) = P(wn | w1 " wn−1 ) .  (1)  In practice, the cases of n-gram models that people usually use are n=2,3,4, referred to as a bigram, a trigram, and a four-gram model, respectively. For example, in trigram models, the probability of a word is assumed to depend only on the two previous words:  P(wn | w1 "wn−1) ≈ P(wn | wn−2wn−1) .  (2)  An estimate of the probability P(wi | wi−2wi−1) is given by Equation (3), called the maximum likelihood estimation (MLE):  P(wi  | w i− 2 w i−1 )  =  C (w i− 2 w i−1 w i ) C (w i− 2 w i−1 )  ,  (3)  where C(wi−2wi−1wi ) represents the number of times the sequence wi−2 wi−1wi occurs in the  The Use of Clustering Techniques for Language Modeling  29  training text. A difficulty with this approximation is that for word sequences that do not occur in the training text, where C (wi−2 wi−1wi ) = 0 , the predicted probability is 0. This makes it impossible for a system, such as a speech recognition system, to accept such a 0 probability sequence. Thus, these probabilities are typically smoothed [Chen and Goodman, 1999]: some probability is removed from all non-zero counts, and used to add probability to the 0 count items. The added probability is typically in proportion to some less specific, but less noisy model. For trigram models, typically, a formula of the following form is used:  P ( wi  |  wi − 2 wi −1 )  =  ⎪⎧ C (wi −2 wi −1wi ) − D (C (wi −2 wi −1wi ))  ⎨  C (wi − 2 wi −1 )  if C (wi− 2 wi−1wi ) > 0 ,  (4)  ⎪⎩α (wi− 2 wi−1 ) P(wi | wi −1 )  otherwise  where α ( wi−2 wi−1 ) is a normalization factor, and is defined in such a way that the probabilities sum to 1. The function D(C (wi−2 wi−1wi )) is a discount function. It can, for instance, have a constant value, in which case the technique is called “Absolute Discounting”,  or it can be a function estimated using the Good-Turing method, in which case the technique  is called Good-Turing or Katz smoothing [Katz, 1987; Chen and Goodman 1999].  1.2 Performance evaluation The most common metric for evaluating a language model is perplexity. Formally, the word perplexity PPW of a model is the reciprocal of the geometric average probability assigned by the model to each word in the test set. It is defined as  PPW  = 2 ∑ , − 1 NW  NW log 2 P ( wi |wi −2wi −1 ) i =1  (5)  where NW is the total number of words in the test set. The perplexity can be roughly interpreted as the geometric mean of the branching factor of the test document when presented to the language model. Clearly, lower perplexities are better.  For applications, such as speech recognition, handwriting recognition, and spelling correction, it is generally assumed that lower perplexity correlates with better performance. In [Gao et al., 2001], we presented results that indicate this correlation is especially strong when the n-gram model is applied to the application of pinyin to Chinese character conversion, which is a similar problem to speech recognition.  2. Word Cluster and Cluster-based N-grams  For any given assignment of a word wi to a cluster (also called a class) ci, there may be many to many mappings; i.e. a word wi may belong to more than one cluster, and a cluster ci will  30  J. Gao et al.  typically contain more than one word. For the sake of simplicity, in this paper, we assume that a word wi can only be uniquely mapped to its own cluster ci, which is called hard clustering. The cluster-based n-gram model is a variant of the word-based n-gram model that uses the frequency of sequences of clusters to help produce a more knowledgeable estimate of the probability of word strings. The basic cluster-based n-gram model defines the conditional probability of a word wi based on its history as the product of the two factors: the probability of the cluster given the preceding clusters, and the probability of a particular word given the cluster [Brown et al., 1990]. For example, in cluster-based trigram models, we have  P(wi | wi−2 wi−1 ) = P(wi | ci ) × P(ci | ci−2 ci−1 ) .  (6)  The MLE of the probability of the word given the cluster, and the probability of the cluster given the two previous clusters can be computed as follows:  P(wi  |  ci  )  =  C(wi ) C(ci )  ,  (7)  P(ci  |  ci−2 ci−1 )  =  C(ci−2 ci−1ci ) C(ci−2 ci−1 )  .  (8)  A large amount of previous research has focused on how to best cluster similar words together. The proposed methods can be roughly grouped into two categories: (1) knowledge based clustering, and (2) data-driven clustering.  In knowledge based clustering, words are clustered based on the syntactic/semantic information we have for the language and the task [Jelinek, 1990; Heeman, 1999; Heeman and Allen, 1997; Placeway et al., 1993; Issar and Ward, 1994; Ward and Young, 1993]. For example, part of speech (POS) tags can be generally used to produce a small number of clusters although this may lead to significantly increased perplexity [Srinivas, 1996; Niesler et al., 1998]. Alternatively, if we have domain knowledge, it is often advantageous to cluster words that have a similar semantic functional role together. For example, [Issar and Ward, 1994] used tags like CITY and AIRLINE for an airline information system. There has also been some interesting research on word clustering for Chinese language. For example, [Yang et al., 1994] present a method in which Chinese words are simply clustered according to their starting and ending characters. It assumes that because almost every Chinese character is a morpheme with its own meaning, very often words having the same starting or ending characters share some common linguistic properties and, thus, can form a word cluster. A good example is the cluster containing “yesterday” (昨天), “tomorrow” (明天), “everyday” (每天), “Sunday” (星期天) etc.  In data-driven clustering, words are clustered automatically in a such way that the overall perplexity of the corpus is minimized [Brown et al., 1992]. A greedy search algorithm  The Use of Clustering Techniques for Language Modeling  31  is generally used for clustering. It basically works as follows. First, each word is initialized to a random cluster. Then, at each iteration, every word is moved to a cluster such that the resulting model has the minimum perplexity. The algorithm converges when no single word can be moved to another cluster in a way that reduces the perplexity of the cluster-based ngram model. Most previous research has found only small differences between different techniques for finding clusters [Kneser and Ney, 1993; Yamamoto and Sagisaka, 1999; Ueberla, 1996; Pereira et al., 1993; Bellegarda et al., 1996; Bai et al., 1998]. One result, however, is that automatically derived clusters outperform POS tags [Niesler et al., 1998], at least when there is enough training data [Ney et al., 1994]. While cluster-based n-gram models often offer no perplexity reduction in comparison to word-based n-gram models, it is beneficial to smooth the word-based n-gram model via either backoff or interpolation methods (although the improvement is marginal) [Maltese and Mancini, 1992; Miller and Alleva, 1997]. One typical example is a combined model where the cluster-based n-gram model can be linearly interpolated with a normal word-based n-gram model [Brown et al., 1992]:  λP(wi | wi−2 wi−1 ) + (1 − λ )P(wi | ci ) × P(ci | ci−2 ci−1 )  (9)  where λ is the interpolation weight optimized on heldout data.  In this study, we focused our research on novel techniques for using clusters rather than different ways of finding clusters. We also noticed that all realistic applications have memory constraints. Therefore, we concentrated our experiments on finding the best way to use cluster-based n-gram models together with word-based n-gram models to seek the optimum balance between memory storage and perplexity. In Section 5, most of our experimental results will be presented in the form of size/perplexity curves.  3. Using Clusters  In this section, we will describe our techniques for using clusters, which are a bit different than traditional clustering as shown in Equation (6). As a typical example, consider the trigram probability P(w3|w1w2), where w3 is the word to be predicted, called the predicted word, and w1 and w2 are context words used to predict w3, called the conditional word. Either the predicted word or the conditional word can be clustered when building cluster-based trigram models. Therefore, there are three basic forms of cluster-based trigram models. When using clusters for the predicted word as shown in Equation (10), we get the first kind of cluster-based trigram model, called predictive clustering. When using clusters for the conditional word as shown in Equation (11), we get the second model, called conditional clustering. When using clusters for both the predicted word and the conditional word, we get Equation (12), called combined clustering:  32  J. Gao et al.  P(wi | wi−2 wi−1 ) = P(ci | wi−2 wi−1 ) × P(wi | wi−2 wi−1ci ) ,  (10)  P(wi | wi−2 wi−1 ) = P(wi | ci−2 ci−1 ) ,  (11)  P(wi | wi−2 wi−1 ) = P(ci | ci−2 ci−1 ) × P(wi | ci−2 ci−1ci ) .  (12)  In what follows, each technique will be discussed in detail, and illustrated by an example.  3.1 Predictive clustering Consider a probability such as P(Tuesday| party on). Perhaps the training data contains no instances of the phrase “party on Tuesday”, although other phrases such as “party on Wednesday” and “party on Friday” do appear. We can put words into clusters, such as the word “Tuesday” into the cluster WEEKDAY. Now, we can consider the probability of the word “Tuesday” given the phrase “party on”, and also given that the next word is a WEEKDAY. We will denote this probability by P(Tuesday | party on WEEKDAY). We can then decompose the probability  P(Tuesday | party on) = P(WEEKDAY | party on) × P(Tuesday | party on WEEKDAY). When each word belongs to only one cluster, this decomposition is a strict equality. This can be trivially proven as follows:  P(ci  | wi−2 wi−1 ) × P(wi  | wi−2 wi−1ci )  =  P(wi−2 wi−1ci ) × P(wi−2 wi−1 )  P(wi−2 wi−1ci wi ) P(wi−2 wi−1ci )  = P(wi−2 wi−1ci wi ) .  (13)  P(wi−2 wi−1 )  Now, since each word belongs to a single cluster, P(ci|wi)=1, it follows that  P(wi−2 wi−1ci wi ) = P(wi−2 wi−1 wi ) × P(ci | wi−2 wi−1 wi ) = P(wi−2 wi−1wi ) × P(ci | wi )  = P(wi−2 wi−1wi ) .  (14)  Substituting Equation (14) into Equation (13), we get  P(ci  | wi−2 wi−1 ) × P(wi  | wi−2 wi−1ci )  =  P(wi−2 wi−1wi ) P(wi−2 wi−1 )  =  P(wi  | wi−2 wi−1 )  .  (15)  Now, although Equation (15) is a strict equality, when smoothing is taken into consideration, using the clustered probability will be more accurate than using the nonclustered probability. For instance, even if we have never seen an example of “party on  The Use of Clustering Techniques for Language Modeling  33  Tuesday”, perhaps we have seen examples of other phrases, such as “party on Wednesday”; thus, the probability P(WEEKDAY | party on) will be relatively high. Furthermore, although we may never have seen an example of “party on WEEKDAY Tuesday”, after we backoff or interpolate with a lower order model, we may able to accurately estimate P(Tuesday|on WEEKDAY). Thus, our smoothed clustered estimate may be a good one. We call this particular kind of clustering predictive clustering. The general form is Equation (10). 3.2 Conditional clustering On the other hand, we can also cluster the words we are conditioning on. For instance, if “party” is in the cluster EVENT and “on” is in the cluster “PREPOSITION”, then we can write P(Tuesday | party on) ≈ P(Tuesday | EVENT PREPOSITION). We call this kind of clustering conditional clustering. The general form is Equation (11). 3.3 Combined clustering It is also possible to combine both predictive and conditional clustering, and, in fact, for some applications, this combination works better than either one separately. Thus, we can compute P(Tuesday | party on) = P(WEEKDAY | EVENT PREPOSITION) × P(Tuesday | EVENT PREPOSITION WEEKDAY). We call this kind of clustering combined clustering. The general form is Equation (12). Equation (12) is a generalization of predictive clustering of Equation (10), in which case we used no clustering for conditional words. Equation (12) is also a generalization of conditional clustering of Equation (11), in which case we used no clustering for predicted words. Also notice that the combined cluster-based trigram model of Equation (12) is actually a generalization of a technique invented at IBM (Brown et al., 1992), which uses the approximation P(wi|ci-2 ci-1 ci) ≈P(wi|ci) to get P(Tuesday | party on) ≈ P(WEEKDAY| EVENT PREPOSITION) × P(Tuesday | WEEKDAY). The approximation is suboptimal unless we use high (count) cutoffs for bigrams and trigrams. Given that combined clustering uses more information than regular IBM clustering, we assumed that it would lead to improvements. As will be shown in Section 5, it works about the same or a little better, at least when interpolated with a normal word-based trigram model. 4. Finding Clusters As described in Section 2, a large number of techniques for finding clusters have been proposed, but previous studies showed that no one technique outperforms other significantly. In this study, we did not explore different techniques for finding clusters, but simply picked  34  J. Gao et al.  one we thought would be good, based on previous research.  There is no need for the clusters used for different positions to be the same. In particular, for a model like IBM clustering, with P(wi|ci)×P(ci|ci-2 ci-1), we call the cluster ci a predictive cluster, and the clusters ci-2 and ci-1 conditional clusters. The predictive and conditional clusters can be different [Yamamoto and Sagisaka, 1999]. For instance, consider a pair of words like “a” and “an”. In general, “a” and “an” can follow the same words, and so, for predictive clustering, belong to the same cluster. However, there are very few words that can follow both “a” and “an”, and so, for conditional clustering, they belong to different clusters. We have also found in experiments that the optimal numbers of clusters used for predictive and conditional clustering are different. In this paper, we always optimize both the number of conditional and predictive clusters separately, and reoptimize for each technique on each training data set. This is a very time consuming task, since each time the number of clusters is changed, the models must be rebuilt from scratch. We always try numbers of clusters that are powers of 2, e.g. 1, 2, 4, etc. This seems to produce numbers of clusters that are close enough to optimal.  The clusters are found automatically using a tool that attempts to minimize perplexity.  In particular, for conditional clusters, we try to minimize the perplexity of the training data for  a bigram of the form P(wi|ci-1), which is equivalent to maximizing  N  ∏ P(wi | ci−1 ) .  (16)  i =1  For predictive clusters, we try to minimize the perplexity of the training data of P(ci|wi1)×P(wi|ci). We do not minimize P(ci|wi-1)×P(wi|wi-1 ci) because we are doing our minimization on unsmoothed training data, and the latter formula would, thus, be equal to P(wi|wi-1) for any clustering. If we were to use the method of leaving-one-out (Kneser and Ney, 1993), then we could use the latter formula, but the approach would be more difficult. Now,  ∏ ∏ N i =1  P(ci  | wi−1 ) × P(wi  | ci ) =  N i =1  P(wi−1ci ) × P(ci wi ) P(wi−1 ) P(ci )  ∏ = N P(ci wi ) × P(wi−1ci ) i=1 P(wi−1 ) P(ci )  (17)  ∏ =  N i =1  P(wi ) P(wi−1 )  ×  P(wi−1  |  ci  )  .  Now, P(wi ) is independent of the clustering used. Therefore, for selection of the best P(wi−1 )  ∏ clusters, it is sufficient to try to maximize  N i =1  P(wi−1  |  ci  )  .  This  is  very  convenient  since  it  is exactly the opposite of what was done for conditional clustering. It means that we can use  The Use of Clustering Techniques for Language Modeling  35  the same clustering tool for both, and simply switch the order used by the program used to get the raw counts for clustering. We give more details about the clustering algorithm in Appendix B. 5. Results and Discussion In this section, we will report our main experiments. In Section 5.1, we will describe the text corpus we used. In Section 5.2, we will compare the performance of word-based trigram models with cluster-based n-gram models. We will give perplexity results of cluster-based ngram models alone, as well as of combined models, where the cluster-based n-gram models were interpolated with word-based n-gram models. In Section 5.3, we will present a fairly thorough comparison of different techniques for using clusters in language model compression. We will then show that our novel clustering techniques can produce much smaller models at a given level of perplexity. 5.1 Corpora We performed our experiments on both Chinese and Japanese text corpora. In both cases, we built language models on training data sets of medial size. We performed parameter optimization on a separate set of heldout data, and performed testing on a set of test sets. None of the three data sets overlapped. Out-of-vocabulary words were not included in perplexity computations. For the Chinese corpus, we used the IME corpus for language model training. It is a balanced corpus, and it exhibits great variety in domain as well as in style. It was collected from the Microsoft input method editor (IME – a software layer that converts keystrokes into Chinese character) tasks. It consists of 11 million characters (or 7 million words after word segmentation). We used 10,000 words for heldout data, and 20,000 words for testing data. The heldout and test data set were every 50th sentence from two non-overlapping sets of an independent open test set. The open test set was carefully designed, and contains approximately half a million characters that have been proofread and balanced among domains, styles, and time [Gao et al., 2001]. The lexicon we used is defined by Chinese linguists, with 50,180 entries. The experiments on the Chinese corpus were fairly open tests since we used heterogeneous (in terms of domain and style) data sets from different sources for language model training and testing. Thus, we assumed that problems due to data sparseness and training-test mismatch would be relatively serious. For experiments on Japanese language modeling, we used a subset of the Nikkei newspaper corpus. In particular, we used the most recent ten million words of the Nikkei corpus for training. As in the Chinese case, we used 10,000 words for heldout data, and 20,000 words for testing data. The heldout and test data sets were every 50th sentence from  36  J. Gao et al.  two non-overlapping sets, taken from another section of the Nikkei corpus. The lexicon we used contains 180,187 Japanese words. The experiments on the Japanese corpus were more like closed tests since we used homogeneous (at least in terms of style) data sets from the same corpus for language model training and testing. We then assumed that data sparseness and training-test mismatch would be less serious than they would be for the Chinese corpus. We also assumed that the Japanese lexicon was far more complete than the Chinese one. A certain number of the entries in the Japanese lexicon are expressions (e.g. of time and date). Using the abovementioned Chinese and Japanese text corpora, we sought to test the robustness of our clustering techniques for different languages, corpora, and word sets (e.g. lexicons). 5.2 Clustering for language model improvement The techniques for finding clusters described in Section 4 were applied to the training corpus to determine suitable word clusters. The word clusters obtained were used to define a clusterbased trigram model and to compute the perplexity on the test sets. In the experiments, the clustering technique we used created a binary branching tree with words at the leaves. By cutting the tree at a certain level, it was possible to achieve a wide variety of different numbers of clusters. For instance, if the tree was cut after the 8th level, there would be roughly 2^8=256 clusters. Since the tree would not be balanced, the actual number of clusters could be somewhat smaller. Therefore, in what follows, we will use the level of the tree to represent approximately the number of clusters, such as 2^1, 2^2, 2^3, etc. Many more details about the clustering techniques used are given in Appendix B. 5.2.1 Using cluster-based trigram models alone In the first series of experiments, we used the traditional cluster-based trigram model of Equation (6) to compute the perplexity. The results are shown in Table 1 for the Chinese and the Japanese corpora. For the sake of comparison, the perplexities of the word trigram models are included. In addition, the perplexities of several human defined word clusters sets are shown as well. These include (1) the 28 POS tags of the Chinese corpus [Zhou, 1996] and (2) the 1428 semantic clusters of the Chinese corpus, which were taken from “同义词词林” (TongYiCi CiLing), a widely used Chinese thesaurus [Mei, 1983]. As shown in Table 1, the perplexity was drastically decreased by increasing the number of word clusters. The best results on both Chinese and Japanese corpora are still the word-based trigram values. It turns out that human defined clusters work much worse than automatically derived clusters with similar numbers of word clusters. The results are consistent with those of Ney et al. [1994], who observed that for small amounts of training data (100,000 words), hand clustering outperformed automatic clustering, but that for larger amounts (1.1 million words), automatic  The Use of Clustering Techniques for Language Modeling  37  clustering was better. Notice that although the perplexity of the hand clustering model is much higher than the perplexity of the automatic clustering model, this does not mean that human defined clusters are unreasonable or worse than automatically derived clusters. The two cluster sets were generated by different criteria and motivations. Hand clustering is usually based on semantic/syntactic similarity, while automatic clustering uses the perplexity measurement directly. Therefore, the former is more widely used for knowledge systems, such as spoken language understanding, while the latter is good for statistical systems, such as speech recognition. As shown in table 4, although most of the automatically derived clusters look reasonable, there are also clusters which are difficult to interpret from a linguistic point of view. Table 1. Test set perplexities with cluster-based trigram models.  Number of clusters 2^5 2^6 2^7 2^8 2^9 2^10 28 (POS clusters ) 1428 (semantic clusters) Word trigram  Chinese 644.31 542.13 464.76 405.92 358.57 322.13 1038.56 676.87 242.74  Japanese 346.05 268.92 223.70 194.26 172.63 155.39 ----------106.33  5.2.2 Using combined models In the second series of experiments, we used the combined models of Equation (9), where the cluster-based trigram model is linearly interpolated with the word-based trigram model. The interpolation constant λ is optimized on heldout data. The results are shown in Table 2. We still used word-based trigram models as baseline systems. It turns out that combined models consistently outperform baseline models. Unlike the case shown in the Table 1, the perplexity is decreased slowly at first by increasing the number of word clusters. We thus have an optimum at about 2^9 clusters for both the Chinese and the Japanese corpus. Beyond these numbers, the perplexity increases slightly again. Depending on the corpus, we have different  38  J. Gao et al.  levels of perplexity reduction: about 3% for the Chinese corpus (at 2^9 clusters), and more than 10% for the Japanese corpus (at 2^9 clusters). Table 2. Test set perplexities with combined trigram models.  Number of clusters 2^5 2^6 2^7 2^8 2^9 2^10 2^11 2^12 2^13 Word trigram  Chinese 236.01 235.02 234.21 233.53 233.42 234.11 234.81 235.53 236.58 242.74  Japanese 100.01 98.21 96.68 95.73 95.41 95.66 96.72 97.60 99.58 106.33  5.2.3 Using higher-order n-gram models While trigram approximation has been proven, in practice, to be reasonable, there is disagreement about whether longer contexts can be helpful. This has led to research on using n-gram models in which n>3, called higher-order n-grams. Most of the previous experiments with higher-order n-grams showed little improvement because of the data sparseness problem. For example, [Goodman, 2001] showed that even using a very large corpus for n-gram model training (e.g. 280 million words), very small improvements occurred for n-gram models, where n is larger than 5. Clustering is an alternative way of dealing with the data sparseness problem besides smoothing. It was, thus, interesting to explore the effectiveness of clusterbased higher-order n-gram models. We performed the third series of experiments on the relationship between cluster-based n-gram order and perplexity. We fixed the number of clusters at 2^8, and built a series of ngram models, with n ranging from 2 to 20. The cluster-based higher-order n-gram models were then linearly interpolated with normal word-based trigram models. The perplexity results are shown in Table 3. We can see that although we used training corpora of medial size, improvement still occurred even for very high order n-gram models. After 10-gram models were used, depending on the corpus, we obtained approximately 10% perplexity reduction for  The Use of Clustering Techniques for Language Modeling  39  the Chinese corpus, and obtained more than 11% perplexity reduction on the Japanese corpus. It then turns out that clustering works significantly better with higher-order n-gram models than the traditional smoothing methods as described in [Chen and Goodman, 1999]. Table 3. Test set perplexities with cluster-based higher-order n-gram models.  Order of cluster-based n-gram model 2 3 4 5 6 7 8 9 10 12 20 word trigram  Chinese 238.93 233.53 230.17 226.79 224.52 223.01 221.37 220.05 219.44 219.14 219.13 242.74  Japanese 99.00 95.73 93.74 93.62 93.91 94.19 94.33 94.47 94.47 94.53 94.53 106.33  5.2.4 Analysis of words in clusters We divided the 50,180-entry Chinese lexicon into 2^8 clusters by means of automatic clustering. The number of words in each cluster varied greatly from 0 to more than 2000. Table 4 gives 11 examples of word clusters. For each cluster from A to C, we randomly selected 10 two-character Chinese words, and removed those words that occurred less than 10 times in the training corpus. For each remaining cluster shown in table 4, we give the top 15 to 30 two-character Chinese words with the highest frequency (at lest 10 times) in the training corpus. We can see that most of the words in each cluster belong to the same syntactic class, namely, verbs for cluster A, nouns for clusters B and C, etc. Furthermore, there are some semantic similarities between the words in a cluster. The majority of the words in cluster A are verbs expressing some kind of motion, some of the words in cluster B are titles, and some of the words in cluster C are games. There are also words which appear to be in the wrong  40  J. Gao et al.  cluster: words like “earth” and “banquet” are not games, and words like “parsimony” and “mournful” are not verbs. Although most of the clusters look reasonable, there are also clusters that are difficult to interpret from a linguistic point of view. The other 8 clusters, which contain only high frequency words, look quite reasonable. It turns out that, given a sufficient large training corpus, the degree to which the clusters capture both syntactic and semantic aspects of Chinese is quite impressive although they were constructed from nothing more than counts of bigrams. Table 4. Most frequent words of some sample clusters from the Chinese corpus.  Cluster  Words  A  走(walk), 飞跑(run), 奔腾(rush), 高攀(climb up), 推翻(overset), 跳动 (jump), 流淌(flow), 吝惜(parsimony), 凄然(mournful), …  老师(teacher), 先生(sir), 小姐(miss), 同志(comrade), 父亲(father), 母亲 B (mother), 讨伐(crusade against), 发誓(promise), …  C  篮球(basketball), 棒球(baseball), 乒乓球(ping-pang), 铅球(shot), 地球 (earth), 宴会(banquet), …  进行(conduct), 建立(build), 提出(bring forth), 实现(accomplish), 取得  D  (gain), 提供(provide), 出现(advent), 得到(annex), 形成(form), 发生 (occur), 发挥(develop), 产生(accrue), 完成(complete), 获得(get), 发表  (publish), 创造(create), 召开(convene), 出席(attend), 所有(all), …  继续(keep on), 再次(once more), 重新(over again), 坚决(determined), 第  一次(the first time), 多次(several times), 经常(often), 纷纷(one after  E  another), 突然(suddenly), 立即(at once), 刚刚(a moment ago), 逐渐 (gradually), 尽快(as soon as possible), 主动(active), 从中(from it), 亲自  (personally), 彻底(thoroughly), 提前(advanced), 反复(again and again), 马  上(immediately), …  汽车(automobile), 石油(petroleum), 建筑(architecture), 制造  (manufacture), 加工(process), 食品(food), 化学(chemistry), 化工(chemical  F  engineering), 机械(mechanics), 本地(native), 广告(advertisement), 航空 (aerial), 制作(manufacture), 航天(spaceflight), 示范(demonstration), 电力  (power), 服装(garment), 纺织(spinning), 钢铁(steel and iron), 走私  (smuggle), …  The Use of Clustering Techniques for Language Modeling  41  重要(important), 主要(dominant), 群众(mass), 一定(certain), 基本  (elementary), 重大(fatal), 实际(practical), 一切(the whole), 高度(high), 人  G  类(mankind), 一般(general), 具体(concrete), 根本(basic), 自然(natural), 核 心(kernel), 特殊(special), 自身(oneself), 客观(objective), 各自(respective),  唯一(unique), 最好(best), 自我(self), 周围(surrounding), 军人(soldier), 绝  对(absolute), 历史性(historic), 彼此(one another), 最低(lowest), …  广州(Guangzhou), 贫困(poor), 深圳(Shenzhen), 天津(Tientsin), 纽约(New  York), 南京(Nanking), 厦门(Xiamen), 重庆(Chongqing), 巴黎(Paris), 东北  H  (northeast), 西安(Xi’an), 福州(Fuzhou), 长江(Yangtze River), 华盛顿 (Washington), 东京(Tokyo), 成都(Chengdu), 大连(Dalian), 珠海(Zhuhai),  武汉(Wuhan), 沿海(coastal), 西南(southwest), 南方(south), 黄河(Yellow  River), 整顿(put to order), 山区(mountain region), …  所谓(so-called), 称为(call), 誉为(call), 可谓(call), 叫做(call), 称之为(call),  I  致函(address a letter), 评为(call), 人称(namely), 发给(hand out), 称作 (call), 素有(have), 号称(reputed), 鼓吹(promote by publicity), 当作(regard  as), …  过去(past), 以后(after), 之后(later on), 当时(at that time), 一天(one day), 后来(later on), 如今(now), 为此(for the purpose), 另外(for the purpose), 当 J 年(that year), 晚上(night), 不久(soon), 面前(in front), 之前(before), 身上 (body), 这时(this time), 拒绝(reject), 中间(intermediate), 随后(later on), 那 天(that day), …  积极(positively), 不断(constantly), 充分(adequately), 认真(seriously), 广 泛(widely), 深入(deeply), 正确(correctly), 有效(availably), 真正(really), 逐步(stepwise), 健康(healthily), 明显(obviously), 迅速(promptly), 严格 K (sternly), 明确(explicitly), 顺利(smoothly), 普遍(generally), 热烈(warmly), 热情(passionately), 合理(reasonably), 及时(timely), 切实(practically), 更 好(get better), 有力(strongly), 大大(greatly), 显著(significantly), 自觉 (voluntarily), 相应(correspondingly), …  5.3 Clustering for language model compression As shown in the last subsection, Equation (9) leads to better results (lower perplexities) than a simple trigram model does. But at the same time, the combined model is larger, since it includes both a cluster-based trigram model and a normal trigram model. In this subsection, we will explain how we took memory constraints into consideration, and concentrated our experiments on using clustering for language model compression. We performed experiments on the three basic cluster-based trigram models described in Section 3, and we found that our novel clustering techniques could be combined with language model pruning methods to  42  J. Gao et al.  produce much smaller models at a given level of perplexity than could be produced using pruning methods without clustering.  Since we are seeking the correct balance between memory storage and perplexity, all experimental results below are presented in the form of size/perplexity curves. The size was measured as the total number of parameters of the language model: one parameter for each bigram and trigram one parameter for each normalization parameter α that was needed, and one parameter for each unigram. In the pruning experiments, bigrams and trigrams were both pruned, unigrams never were. This resulted in the smallest possible number of parameters being equal to the vocabulary size, e.g. 50,187 unigrams for Chinese models, and 180,187 unigrams for Japanese models.  In our experiments described below, we used Stolcke’s [1998] pruning method to  produce a series of language models of different sizes. This method is an entropy-based cutoff  method, and can be considered an extension of the work of Seymore and Rosenfeld [1996]  and of Kneser [1996]. The basic idea is to remove as many “useless” probabilities as possible,  and at the same time to keep the perplexity increase as small as possible. This is achieved by  examining the weighted relative entropy or Kullback-Leibler distance between each  probability P(w| h) and its value from the backoff distribution, P(w | h) :  D(P(w | h) || P(w | h)) = P(w | h) ∗ log P(w | h) , P(w | h)  (18)  where h is the reduced history. When the Kullback-Leibler distance is small, the backoff probability is a good approximation, and the probability P(w|h) does not carry much additional information and can be deleted. The Kullback-Leibler distance was calculated for each n-gram entry, and we removed entries and reassigned the deleted probability mass to backoff mass for any n-gram entry whose deletions increased the Kullback-Leibler distance by less than a specified threshold value. Compared to the traditional count cutoff methods, Stolcke pruning performed a little better [Goodman and Gao, 2000]. More importantly, the Stolcke method could prune a model to a specific size, simply by finding the threshold level that resulted in a model of that size. For all the models, we used a smoothing method called modified absolute discounting for backoff. We give more details about Stolcke pruning and modified absolute discounting in Appendix A. We then performed a number of experiments to compare our different models. In these experiments, the baseline system was the word-based trigram model.  5.3.1 Predictive clustering We first used predictive clustering of Equation (10). The results are shown in Figures 1 and 2. It turns out that we got the best result at about 2^6 clusters for both the Chinese and Japanese corpora. Depending on the corpus, compared to the baseline systems, at the same size, we got  The Use of Clustering Techniques for Language Modeling  43  a maximum 6.6% perplexity reduction for the Chinese corpus, and a maximum 5.1% perplexity reduction for the Japanese corpus; at the same perplexity, we got a maximum 54% size reduction for the Chinese corpus, and a maximum 57% size reduction for the Japanese corpus. We notice that for these two corpora, although we got the best result at 2^6 clusters for both of them, the results at other numbers of clusters (e.g. 2^4, 2^7) were very different. For the Chinese corpus, all the predictive clustering models performed about the same. For the Japanese corpus, models at larger numbers of clusters performed much better than models at small numbers of clusters (e.g. 2^4). In general, with our clustering, when there was only a small amount of training data, the clusters became less useful. Perhaps this was because there was a more serious data sparseness problem for the Chinese corpus, and many parameters were out of training, thus larger clusters did not bring benefits. As for the Japanese corpus, the data sparseness problem was much less serious, so a large number of clusters led to significant perplexity reduction. We also tried to set different pruning threshold values for the two components of the predictive clustering models. We could not obtain any improvement. Therefore, in what follows, we will always assume that we used the same pruning threshold value for both components in the predictive clustering and combined clustering models. 5.3.2 Conditional clustering We used the conditional clustering of Equation (11). As shown in Figures 3 and 4, the results for the two languages are qualitatively very similar. The performance was consistently improved by increasing the number of clusters. But no conditional clustering model was superior to the baseline model. This is not surprising because the conditional clustering model always discards information for predicting words, and even with smoothing, it does not bring any additional benefits.  44  J. Gao et al.  size  3.0E+06 2.5E+06 2.0E+06 1.5E+06  2^4 clusters 2^5 clusters 2^6 clusters  1.0E+06  2^8 clusters  5.0E+05  0.0E+00  Baseline: Word Trigram  400  500  600  700  800  900  perplexity  Figure 1 Comparison of predictive models applied with different numbers of clusters to the Chinese corpus.  The Use of Clustering Techniques for Language Modeling  45  size  2.0E+06 1.8E+06  2^4 clusters  1.6E+06  1.4E+06 1.2E+06  2^6 clusters  1.0E+06  8.0E+05 6.0E+05  2^8 clusters  4.0E+05  2.0E+05 0.0E+00  Baseline: Word Trigram  100  120  140  160  180  200  perplexity  Figure 2. Comparison of predictive models applied with different numbers of clusters to the Japanese corpus.  46  J. Gao et al.  size  3.0E+06 2.5E+06 2.0E+06 1.5E+06 1.0E+06  2^10 clusters 2^12 clusters 2^14 clusters 2^16 cluster  5.0E+05 0.0E+00  Baseline: Word Trigram  400  500  600  700  800  900  perplexity  Figure 3 Conditional models applied with different numbers of clusters to the Chinese corpus.  The Use of Clustering Techniques for Language Modeling  47  size  2.0E+06 1.8E+06 1.6E+06 1.4E+06 1.2E+06 1.0E+06 8.0E+05 6.0E+05 4.0E+05 2.0E+05 100 110 120 130 140 150 160 170 180 perplexity  2^10 clusters 2^12 clusters 2^14 clusters Baseline: Word Trigram  Figure 4 Comparison of conditional models applied with different numbers of clusters to the Japanese corpus.  48  J. Gao et al.  5.3.3 Combined clustering We also used the combined clustering of Equation (12). As mentioned earlier, we can use different numbers of cluster for predictive clusters and conditional clusters. This leads to a very large number of possible parameter settings. We presented detailed analysis of the parameter settings of the combined clustering model in [Goodman and Gao, 2000]. In this paper, we will only report the results of some sample parameter settings. For the Chinese corpus, as shown in Figure 5, we set the number of predictive clusters to 2^4, 2^6, and 2^8, and set the number of conditional clusters to 2^12, 2^14, and 2^16. We then built a large number of models. Rather than graph all the points of all the models, we show only the outer envelope of the points for each number of predictive clusters in Figure 5. That is, if for a model with a given number of predictive clusters, there was some other point with the same number of predictive clusters (and perhaps a different number of conditional clusters) with both lower perplexity and smaller size than the first model, then we did not graph the first, worse point. We show the outer envelope of the size/perplexity curves of 2^4, 2^6, and 2^8 predictive clusters. For the Japanese corpus, as shown in Figure 6, we do not show the outer envelopes as in Figure 5. Instead, we show results of the top three best parameter settings we obtained; for instance, (2^4, 2^12) represents the combined cluster-based trigram model with 2^4 predictive clusters and 2^12 conditional clusters. It turns out that, for the Japanese corpus, the best combined clustering models outperformed the baseline model. At small model sizes, we got the best result at 2^14 conditional clusters and 2^6 predictive clusters. At large model sizes, we got the best result at 2^12 conditional clusters and 2^4 predictive clusters. We achieved the maximum 6.5% perplexity reduction at the same size, and the maximum 40% size reduction at the same perplexity. But for the Chinese corpus, no improvement over the baseline model was achieved until we used models with very large numbers of conditional clusters. This is not difficult to explain. Recall that predictive clustering is a special case of combined clustering. Actually, in most combined clustering models for Chinese, it turns out to be no less optimal to use conditional clusters than the vocabulary size, i.e., no conditional clustering. Now, consider the IBM clustering of Equation (6), which is a special case of the combined clustering model. As shown in Figure 6, the performance is by far the worst, roughly an order of magnitude worse than the performance of the other approaches. We hypothesized that this was because the IBM model throws away too much useful information. We thus tried a variation on the IBM model:  λP(wi | wi−2 wi−1 ) + (1 − λ)P(wi | ci−2ci−1ci ) × P(ci | ci−2ci−1 ) .  (20)  The Use of Clustering Techniques for Language Modeling  49  This model is just like the standard IBM model, but it also conditions the probability of the word on the previous clusters. We compared this model with a standard IBM model. The results are shown in Tables 5 and 6. It turns out that, for the Chinese corpus, models in the form of Equation (20) consistently outperformed the standard IBM models (e.g. we achieved 4% perplexity reduction at 2^9 clusters), while for the Japanese corpus, they worked about the same. Notice that in these experiments, no pruning was done. We summarize the results of all the experiments described in this subsection in Figures 7 and 8. It is clearly seen that our novel clustering techniques produce much smaller models than do previous methods (i.e. baseline systems) at the same perplexity level. In addition, several more conclusions can be drawn: 1. Conditional clustering did not help for either the Chinese or the Japanese corpus since it always discards information. 2. For closed tests on homogeneous text corpora (e.g. the Japanese corpus), both combined clustering and predictive clustering outperformed the baseline system consistently. Combined clustering is better at small model sizes, while predictive clustering is better at larger sizes. 3. For open tests on heterogeneous text corpora (e.g. the Chinese corpus), predictive clustering outperformed the baseline system consistently. Although the results presented in this paper show that combined clustering achieved no improvement, in [Goodman and Gao, 2000], we showed that with more sophisticated techniques, it appears possible to make combined clustering better than predictive clustering.  50  J. Gao et al.  size  3.0E+06  2.5E+06  2.0E+06  1.5E+06  1.0E+06  5.0E+05  0.0E+00  450  550  650  750  850  perplexity  combined (2^4,2^n) n=12,14,16 combined (2^6,2^n) n=12,14,16 combined (2^8,2^n) n=12,14,16 Baseline: Word Trigram  Figure 5 Comparison of combined clustering models applied with different numbers of clusters to the Chinese corpus.  The Use of Clustering Techniques for Language Modeling  51  size  3.00E+06  combined (2^4,2^12)  2.50E+06  combined (2^6,2^14)  2.00E+06  combined (2^8,2^18)  1.50E+06  IBM model (2^12)  1.00E+06  IBM model (2^14)  5.00E+05  IBM model (2^18)  Baseline:  0.00E+00  Word  100  120  140  160  180  200  Trigram  perplexity  Figure 6 Comparison of combined clustering models applied with different numbers of clusters to the Japanese corpus and the IBM model.  Table 5. Comparison of different combined trigram models applied to the Chinese corpus.  Number of clusters 2^6  Equation (9) 235.02  Equation (20) 226.65  2^7  234.21  224.65  2^8  233.53  224.29  2^9  233.42  224.99  2^10  234.11  226.53  2^11  234.81  228.26  2^12  235.53  230.95  2^13  236.58  234.78  word trigram  242.74  242.74  52  J. Gao et al.  Table 6. Comparison of different combined trigram models applied to the Japanese corpus.  Number of clusters 2^6  Equation (9) 98.21  Equation (20) 97.06  2^7  96.68  96.42  2^8  95.73  96.33  2^9  95.41  96.41  2^10  95.66  96.82  2^11  96.72  97.57  2^12  97.60  98.50  2^13  99.58  100.52  word trigram  106.33  106.33  size  3.0E+06  2^14 conditional clusters  2.5E+06 2.0E+06 1.5E+06 1.0E+06  2^6 predictive clusters 2^6 predictive clusters and 2^18 conditional clusters (combined clusters) Baseline: Word Trigram  5.0E+05  0.0E+00 400 450 500 550 600 650 700 750 800 850 perplexity Figure 7 Summary of the results obtained by applying clustering models to the Chinese corpus.  The Use of Clustering Techniques for Language Modeling  53  3.00E+06 2.50E+06 2.00E+06  Predictive Model (2^6) combined Model (2^6, 2^14)  size  1.50E+06  Conditional Model (2^14)  1.00E+06 5.00E+05  IBM model (2^14)  0.00E+00 100  120  140  160  perplexity  Baseline: Word Trigram 180  Figure 8 Summary of the results obtained by applying clustering models to the Japanese corpus.  6. Conclusion  Cluster-based n-gram models are variations on traditional word-based n-gram models. They attempt to make use of the similarities between words. In this paper, we have presented an empirical study on clustering techniques for Asian language modeling. While the majority of the previous research on word clustering has focused on how to get the best clusters, we have concentrated our research on the best way to use the clusters. We have studied in detail three cluster-based n-gram models, namely, predictive clustering, conditional clustering, and combined clustering. In our experiments, clustering was used to improve the performance (i.e. perplexity) of language models as well as to compress language models. We performed experimental tests on a Japanese newspaper corpus of more than 10 million words, and on a Chinese mixed-domain corpus of more than 7 million words. Results show that our novel techniques worked much better than previous methods. They not only showed better performance when interpolated with normal n-gram models, but could also be combined with Stolcke pruning to produce models much smaller than unclustered models with the same perplexity level. Most language modeling improvements reported previously required significantly more space than the normal trigram baseline model, or had higher perplexity. Their practical value  54  J. Gao et al.  is questionable. In this paper, we have proposed a technique that results in lower perplexity than the traditional trigram models do at every memory size. In other research [Gao et al., 2001], we have shown that cluster-based models of this form can be applied effectively to pinyin to Chinese character conversion. One area we consider promising for future research is the combination of human defined and automatically derived clustering. While human defined clusters alone generally work worse than automatically derived clusters, there has been little research on their combination. It is an open question whether such a combination can lead to further improvements. Acknowledgements We would like to thank Prof. Changning Huang, Dr. Ming Zhou, and other colleagues at Microsoft Research, Yoshiharu Sato, and Hiroaki Kanokogi at the Microsoft (Japan) IME group, for their help in developing the ideas and implementation presented in this paper. We would also like to thank Jiang Zhu and Miyuki Seki for their help in our experiments and for providing Chinese and Japanese text corpora. References Bai, S., Li, H., Lin, Z., and Yuan, B. “Building class-based language models with contextual statistics.” In ICASSP-98, 1998. pp. 173-176. Bellegarda, J. R., Butzberger, J. W., Chow, Y. L., Coccaro, N. B., and Naik, D. “A novel word clustering algorithm based on latent semantic analysis.” In ICASSP-96, 1996. pp. I172I175. Brown, P. F., Cocke, J., DellaPietra, S. A., DellaPietra, V. J., Jelinek, F., Lafferty, J. D., Mercer, R. L., and Roossin, P. S. “A statistical approach to machine translation.” Computational Linguistics, 16(2), 1990. pp. 79-85. Brown, P. F., DellaPietra V. J., deSouza, P. V., Lai, J. C., and Mercer, R. L. “Class-based ngram models of natural language.” Computational Linguistics, 18(4), 1992. pp. 467479. Chen, S. F., and Goodman, J. “An empirical study of smoothing techniques for language modeling.” Computer Speech and Language, 13:359-394, October. 1999. Church, K. “A stochastic parts program and noun phrase parser for unrestricted text.” In Proceedings of the Second Conference on Applied Natural Language Processing, 1988. pp. 136-143. Cutting, D. R., Karger, D. R., Pedersen, J. R., and Tukey, J. W. “Scatter/gather: A clusterbased approach to browsing large document collections.” In SIGIR 92. 1992. Gao, J., Goodman, J., Li, M., and Lee, K. F. “Toward a unified approach to statistical language modeling for Chinese.” To appear in ACM Transactions on Asian Language  The Use of Clustering Techniques for Language Modeling  55  Information  Processing.  2001.  http://www.research.microsoft.com/~jfgao  Draft  available  from  Goodman, J. “A bit of progress in language modeling.” Submitted to Computer Speech and Language. 2001. Draft available from http://www.research.microsoft.com/~joshuago  Goodman, J., and Gao, J. “Language model compression by predictive clustering.” ICSLP2000, Beijing, October. 2000.  Heeman, P. “POS tags and decision trees for language modeling.” In ACL-99, 1999. pp. 129137.  Heeman, P., and Allen, J. “Incorporating POS tagging into language modeling.” In Eurospeech-97, Ghodes, Greece, 1997. pp. 2767-2770.  Huang, X. D., Acero, A., and Hon, H. Spoken language processing. Prentice Hall PTR. 2001.  Issar, S., and Ward, W. “Flexible parsing: CMU’s approach to spoken language understanding.” In Proceedings of the ARPA Spoken Language Technology Workshop, pp. 53-58. 1994.  Jelinek, F. Self-organized language modeling for speech recognition. In Readings in Speech Recognition, A. Waibel and K. F. Lee, eds., Morgan-Kaufmann, San Mateo, CA, 1990, pp. 450-506.  Jurafsky, D., and Martin, J. H. Speech and language processing. Prentice Hall.  Katz, S. M. 1987. “Estimation of probabilities from sparse data for the language model component of a speech recognizer.” IEEE Transactions on Acoustics, Speech and Signal Processing, ASSP-35(3):400-401, March. 2000.  Kernighan, M. D., Church, K. W., and Gale, W. A. “A spelling correction program based on a noisy channel model.” In Proceedings of the Thirteenth International Conference on Computational Linguistics, 1990. pp. 205-210.  Kneser, R. and Ney, H. “Improved clustering techniques for class-based statistical language modeling.” In Eurospeech, Vol. 2, 1993. pp. 973-976, Berlin, Germany.  Kneser, R. “Statistical language modeling using a variable context length.” Proc. ICSLP, volume 1, pages 494-497, Oct. 1996.  Maltese, B., and Mancini, F. “An automatic technique to include grammatical and morphological information in a trigram-based statistical language model.” In ICASSP-92, 1992. pp. I157-I160.  Manning, C. D., and Schutze, H. “Foundations of Statistical Natural Language Processing.” MIT Press, 1999. Cambridge, MA.  Mei, J. Z. Tongyici Cilin. Shanghai Cishu Publishing House, 1983. Shanghai.  Miller, D., Leek, T., and Schwartz, R. M. “A hidden Markov model information retrieval system.” In Proc. 22nd International Conference on Research and Development in Information Retrieval, Berkeley, CA, 1999, pp. 214-221.  Miller, J. W., and Alleva, F. “Evaluation of a language model using a clustered model back off.” In ICASSP-97, 1997. pp. 390-393.  56  J. Gao et al.  Ney, H., Essen, U., and Kneser, R. “On structuring probabilistic dependences in stochastic language modeling.” Computer Speech and Language, 8:1-38. 1994. Niesler, T. R., Whittaker, E. W. D., and Woodland, P. C. “Comparison of part-of-speech and automatically derived category-based language models for speech recognition.” In ICASSP-98, 1998. pp. I177-I180. Pereira, F., Tishby, N., and Lee L. “Distributional clustering of English words.” In Proceedings of the 31st Annual Meeting of the ACL. 1993. Placeway, P., Schwartz, R., Fung, P., and Nguyen, L. “The estimation of powerful language models from small and large corpora.” In ICASSP-93, 1993. II33-36. Seymore, K. and Rosenfeld, R. “Scalable backoff language models”, Proc. ICSLP, Vol. 1., 1996. pp.232-235, Philadelphia, Srinivas, B. “Almost parsing techniques for language modeling.” In ICSLP-96, 1996. pp. 1169-1172. Stolcke, A. “Entropy-based Pruning of Backoff Language Models.” In Proc. DARPA News Transcription and Understanding Workshop, Lansdowne, VA. 1998. pp. 270-274. See corrections at http://www.speech.sri.com/people/stolcke Ueberla, J. P. “An extended clustering algorithm for statistical language models.” IEEE Transactions on Speech and Audio Processing, 4(4): 313-316. 1996. Ward, W., and Young, S. “Flexible use of semantic constraints in speech recognition.” In ICASSP-93, 1993. pp. II49-50. Yamamoto, H., and Sagisaka, Y. “Multi-class Composite N-gram based on Connection Direction.” In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, May, 1999. Phoenix, Arizona. Yang, Y. J., et al., “An intelligent and efficient word-class-based Chinese language model for Mandarin speech recognition with very large vocabulary.” In ICSLP-94, Yokohama, Japan, 1994. pp. 1371-1374. Zhou, Q. “Phrase bracketing and annotating on Chinese language corpus.” Ph.D. dissertation. Beijing University. 1996. Zue, V. W. Navigating the information superhighway using spoken language interfaces. IEEE Expert, vol. 10, no. 5, pp. 39-43, October, 1995  The Use of Clustering Techniques for Language Modeling  57  A. Methods of Trigram Training We will describe methods for language model training below. These include the modified absolute discounting smoothing method and Stolcke’s entropy-based pruning method.  Absolute Discounting Trigram Language models make the assumption that the probability of a word depends only on the identity of the immediately two preceding words, say P(wi|w1 w2… wi-1) ≈ P(wi|wi-2 wi1). Smoothing is used to address the problem of data sparseness. Experimental results show that a novel variation of absolute discounting, Kneser-Ney smoothing, consistently outperforms all others [Chen and Goodman, 1999]. However, because Kneser-Ney smoothing is less commonly used, slightly more difficult to implement, and, we suspect, may not work as well when pruning is done, we used a slightly different technique in this research, modified absolute discounting. First, we will describe basic absolute discounting. Letting D represent a discount, we set the probability as follows:  Pabsolute (wi  |  wi−2 wi−1 )  =  ⎧ ⎪  max(C  (wi  −  2  wi  −1  wi  )  −  D,0)  ⎨ ⎪⎩α  (wi  −2  C(wi−2 wi−1 wi−1 )Pabsolute  ) (  wi  |  wi−1 )  if C(wi−2 wi−1wi ) > 0 . otherwise  (21)  α (wi−2 wi−1 ) is defined in such a way that the probabilities sum to 1:  1−  ∑ Pabsolute (wi | wi−2 wi−1 )  α (wi−2 wi−1 ) =  wi :C ( wi − 2 wi −1wi )>0  1−  ∑ Pabsolute (wi | wi−1 )  .  (22)  wi :C ( wi − 2 wi −1wi )>0  The trigram backs off to the bigram, and the bigram backs off to the unigram. The unigram does not need to be smoothed although it can be smoothed with the uniform distribution. In practice, different D’s are used for the bigram and trigram.  A further improvement is to use multiple discount D. Taking the trigram as an example, D1 stands for counts C(wi−2 wi−1wi ) = 1 , D2 for C(wi−2 wi−1wi ) = 2 , and a final one, D3 for C(wi−2 wi−1wi ) ≥ 3 . Chen and Goodman [1999] introduced an estimate for the optimal D for absolute discounting smoothing as a function of training data counts1. In practice, we can use Equation (23) to Equation (26) to approximately estimate the optimal values for D1, D2, and  
Knowledge acquisition is a bottleneck in machine translation and many NLP tasks. A method for automatically acquiring translation templates from bilingual corpora is proposed in this paper. Bilingual sentence pairs are first aligned in syntactic structure by combining a language parsing with a statistical bilingual language model. The alignment results are used to extract translation templates which turn out to be very useful in real machine translation. Keywords: Bilingual corpus, Translation template acquisition, Structure alignment, Machine translation 1. Introduction Bilingual corpora have been recognized as a valuable resource for knowledge acquisition in machine translation and many other NLP tasks. To make better use of them, bilingual corpora are often aligned first. Intensive researches have been done on sentence and word level alignment [Brown et al. 1991, Church 1993, Ker et al. 1997, Huang et al. 2000]. These alignments have been proven to be very useful in machine translation, word sense disambiguation, information retrieval, translation lexicon extraction, and so on. With a sentence aligned parallel English-Chinese corpus ready in hand, this paper extends word-level alignment to syntactic structure alignment with the aim of acquiring structural translation templates automatically.  
Fusion and clustering are two approaches to improving the effectiveness of information retrieval. In fusion, ranked lists are combined together by various means. The motivation is that different IR systems will complement each other, because they usually emphasize different query features when determining relevance and retrieve different sets of documents. In clustering, documents are clustered either before or after retrieval. The motivation is that similar documents tend to be relevant to the same query so that this approach is likely to retrieve more relevant documents by identifying clusters of similar documents. In this paper, we present a novel fusion technique that can be combined with clustering to achieve consistent improvements over conventional approaches. Our method involves three steps: (1) clustering similar documents, (2) re-ranking retrieval results, and (3) combining retrieval results. 1. Introduction In terms of the overall performance on a large query set, none of the typical IR systems outperform others substantially, while for each individual query, the performance that different systems achieve varies greatly [Voorhees 1997]. This observation leads to the idea of combining results obtained by different IR systems to improve overall performance. Fusion is a technique that combines retrieval results (or ranked lists) obtained by * This work was done while the author worked for Microsoft Research Asia as a visiting student. Department of Computer Science and Technology of Tsinghua University, China Email:ajian@s1000e.cs.tsinghua.edu.cn + Microsoft Research Asia E-mail:jfgao@microsoft.com ** Microsoft Research Asia E-mail:mingzhou@microsoft.com ++ Department of Computer Science and Technology of Tsinghua University, China E- mail:wjx@s1000e.cs.tsinghua.edu.cn  110  J. Zhang et al.  different systems. However, conventional fusion techniques only consider retrieval results, while the information embedded in the document collection (e.g. the similarity between documents) is ignored. On the other hand, document clustering applies the structure of a document collection, but it usually considers each individual ranked list separately and is not able to take advantage of multiple ranked lists. In this paper, we present a novel fusion technique that can be combined with clustering. Given multiple retrieval results obtained by different IR systems, we first perform clustering on each ranked list and obtain a set of clusters. We then identify the clusters that contain the most relevant documents. Each of these clusters is evaluated based on a metric called reliability. Documents in reliable clusters are re-ranked. That is, we set higher scores for these documents. Finally, a conventional fusion method is applied to combine multiple retrieval results, which are re-ranked. Our experiments on the TREC-5 Chinese collection show that the above approach achieves consistent improvements over conventional approaches. The remainder of this paper is organized as follows. Section 2 gives a brief survey of related work. In Section 3, we describe our method in detail. In Section 4, a series of experiments are presented to show the effectiveness of our approach. Finally, we present our conclusions in Section 5. 2. Related Work Fusion and clustering have been important research topics for many researchers. Fox and Shaw [Fox 1994] reported on their work on result sets fusion. Their method for combining the evidence from multiple retrieval runs is based on document-query similarities in different sets. Five combining strategies were investigated, as summarized in Table 1. In their experiments, CombSUM and CombMNZ were better than the others.  Improving the Effectiveness of Information Retrieval with Clustering and Fusion 111  Table 1. Formulas proposed by Fox & Shaw.  Name  Combined Similarity =  CombMAX  MAX(Individual Similarities)  CombMIN  MIN(Individual Similarities)  CombSUM  SUM(Individual Similarities)  CombANZ  SUM(Individual Similarities)  Number of Nonzero Similarities  CombMNZ  SUM(Individual Similarities) * Number of Nonzero Similarities  Thompson’s work [Thompson 1990] includes assigning to each ranked list a variable weight based on the prior performance of the system. His idea is that a retrieval system should be considered preferable to others if its prior performance is better. Thompson’s results were slightly better than Fox’s.  Bartell [Bartell 1994] used numerical optimization techniques to determine optimal scalars (weights) for a linear combination of results. The idea is similar to Thompson’s except that Bartell obtained the optimal scalars from training data, while Thompson constructed scalars based on their prior performance. Bartell achieved good results on a relatively small collection (less than 50MB).  To perform fusion more effectively, researchers began to investigate whether two result  sets are suitable for fusion by examining some critical characteristics. Lee [Lee 1997] found  that the overlap of the result sets was an important factor for fusion. Overlap ratios of  relevant and non-relevant documents are calculated as follows:  Roverlap  =  Rcommon × 2 , RA + RB  N overlap  =  Ncommon × 2 , NA + NB  where RA and N A are, respectively, the numbers of relevant and irrelevant documents in result set RLA 1. Rcommon is the number of common relevant documents in RLA and RLB . Ncommon is the number of common irrelevant documents in RLA and RLB .  
In a text-to-speech (TTS) conversion system based on the time-domain pitch-synchronous overlap-add (TD-PSOLA) method, accurate estimation of pitch periods and pitch marks is necessary for pitch modification to assure an optimal quality of the synthetic speech. In general, there are two major issues on pitch marking: pitch detection and location determination. In this paper, an adaptable filter, which serves as a bandpass filter, is proposed for pitch detection to transform the voiced speech into a sine-like wave. Based on the sine-like wave, a peak-valley decision method is investigated to determine the appropriate part (positive part and negative part) of the voiced speech for pitch mark estimation. At each pitch period, two possible peaks/valleys are searched and the dynamic programming is performed to obtain the pitch marks. Experimental results indicate that our proposed method performed very well if correct pitch information is estimated. 1. Introduction In past years, the approach of concatenative synthesis has been adopted by many text-to-speech (TTS) systems [1]–[6]. The concatenative synthesis uses real recorded speech segments as the synthesis units and concatenates them together during synthesis. Also, the time-domain pitch-synchronous overlap-add (TD-PSOLA) [6] method has been employed to perform prosody modification. This method modifies the prosodic features of the synthesis unit according to the target prosodic information. Generally, the prosodic information of the speech includes pitch (the fundamental frequency), intensity, and duration, etc. For a synthesis scheme based on TD-PSOLA method, it is necessary to obtain a pitch mark for each pitch period in order to assure an optimal quality of the synthetic speech. The pitch mark is a reference point for the overlap of the speech signals. 
In this paper, we describe the creation of Chinese zero anaphora resolution rules by performing experiments. The rules were constructed based on the centering model. In the experiments, we selected several texts as testing examples. We compared the referents of zero anaphors in the testing texts identified by hand with the ones resolved by using an algorithm employing a resolution rule. Three rules were used to carry out the experiment. The results show that the rule considering grammatical role criteria and domain knowledge obtained the best result: 85% of zero anaphors in the test texts were correctly resolved. We investigate problems of miss-resolution of zero anaphors in the test text and propose solution to deal with them.  1. Introduction  In Chinese text, anaphors are frequently eliminated, termed zero anaphor (ZA) hereafter, due to their prominence in discourse [LT81]. For example in (1), the topic  of the utterance (1a) is  ‘Electronics stocks,’ which is eliminated in the  second utterance and the topic of utterance (1c),  ‘Securities stocks,‘ is  eliminated in the utterance (1d).  (1) a.  i  Electronics stocks were affected by high-tech stocks fallen heavily in America.  b. i (Electronics stocks) continued falling down today.  c.  j  Securities stocks also had respondence.  d. j (Securities stocks) fell by close one after another on the market.  A simple rule, Rule 1, can be formulated by observing the phenomenon of topic chain in Chinese text. This rule can be used to correctly resolve the referent of the ZA in  
 jschang@cs.nthu.edu.tw  њ]  ք ¤±Ͳ5ʂ ɸ ο]̗Ն‫ & ڜ‬жǂjǮք  ¤±)Ь2Ƽ̗B Ͳ ̗ӊG( ̗¤± ʞ wyw  Х DƗ̗Fȥ  ο ք ¤± Фʂ‫ װ‬ÒCross  Ó ̗ʁȡ Фʂ‫ ڜ>̗ װ‬Language Information Retrieval  Ͳ ΁h \˄Ʀ ȥ¤±ÒQuery Ó Translation Ș¤±  ̗0ˤǞ D̗( ̗Ȅ T̗ ‫ں‬ˀ аʱ‫̗ װ‬  ˤ 1 ̗΁h‫\ں‬¤±̗ ˼ ˄ Ǯ 2̗¤± ¸ ˄  BǮ& ˀ̗‫\ژ‬ʪ ˼>ǭ ( ‫̗ں‬¤± 3 a   ж άǸք ¤±̗ ˼ʞ ȥ΁h‫̗\ں‬¤± Ʈ  Фʂ‫ ̗ װ‬9 ԛՋ( a ƸA̗ άǸƦ¤±  ̴ ʔ ȥ ¸  cȄ̗ άǸք ¤±̴̗   Ʀ ‫פ‬¤±̗ 9  ٩ 1. ք ¤±Ͳ5ʂ ɸ ο]̗Ն‫ & ڜ‬ȄɁH‫ס‬ɁBǮ Ф ʂ( ̗y жǂjǮք ¤±)Ь2Ƽ̗B Ͳ ̗ӊG( ̗¤± ˀ̗BǮ(+ 6: yքp̗ ƛ( ʞ wywХ DƗ̗Fȥ ο ք ¤±  
Keywords: Construction Grammar, metaphorical transfer, conceptualization V-diao constructions, according to their semantics, fall into three categories: A) Physical disappearance from its original position, with the V slot filled by physical verbs, such as tao-diao “escape”, diu-diao “throw away”, and so on. B) Disappearance from a certain conceptual domain, rather than from the physical space, with the V slot filled by less physically perceivable verbs, such as jie-diao “quit”, wang-diao “forget” and the like. C) The third category of V-diao involves speaker’s subjective, always negative, attitude to the result. Examples include: lan-diao “rot”, ruan-diao “soften”, huang-diao “turn yellow”, and so forth. The meaning in Type C constructions cannot be gained by simply putting their component parts together, so in this study, I shall term V-diao as a construction (Goldberg 1995) rather than merely a resultative compound (Li and Thompson 1981). Metaphor, as a mechanism of semantic change (Sweetser 1990, Bybee, Perkins and Pagliuca 1994, Heine, Claudi and Hunnemeyer 1991), is a plausible account of the polysemy between Type A and B. Type A denotes disappearance from physical space, while Type B disappearance from the conceptual space. I thus speculate on the mapping relation between the physical and the abstract, conceptual domain. Other than metaphor, pragmatic inference is claimed to be a major mechanism of semantic change (Hopper and Traugott 1993, Bybee, Perkins and Pagliuca 1994). In such changes, context plays a crucial role. Frequent use of a grammatical or lexical unit in a particular context may lead to the inference that the context is an integral part of its meaning. The development of Type C V-diao may relate to frequent co-occurrence of negative verbs and -diao. (The reason why only negative verbs are allowed in the construction will be further addressed in the next section.) Consequently, negative connotation may spread to the entire construction and give rise to the constructional meaning. * I’m grateful to Dr. Lily I-wen Su for her insightful comment on this paper. 
Sentence planning is a set of inter-related but distinct tasks, one of which is sentence scoping, i.e. the choice of syntactic structure for elementary speech acts and the decision of how to combine them into one or more sentences. In this paper, we present SPoT, a sentence planner, and a new methodology for automatically training SPoT on the basis of feedback provided by human judges. We reconceptualize the task into two distinct phases. First, a very simple, randomized sentence-plangenerator (SPG) generates a potentially large list of possible sentence plans for a given text-plan input. Second, the sentence-plan-ranker (SPR) ranks the list of output sentence plans, and then selects the top-ranked plan. The SPR uses ranking rules automatically learned from training data. We show that the trained SPR learns to select a sentence plan whose rating on average is only 5% worse than the top human-ranked sentence plan. 
We present two methods for learning the structure of personal names from unlabeled data. The ﬁrst simply uses a few implicit constraints governing this structure to gain a toehold on the problem — e.g., descriptors come before ﬁrst names, which come before middle names, etc. The second model also uses possible coreference information. We found that coreference constraints on names improve the performance of the model from 92.6% to 97.0%. We are interested in this problem in its own right, but also as a possible way to improve named entity recognition (by recognizing the structure of different kinds of names) and as a way to improve noun-phrase coreference determination. 
In this paper we investigate polysemous adjectives whose meaning varies depending on the nouns they modify (e.g., fast). We acquire the meanings of these adjectives from a large corpus and propose a probabilistic model which provides a ranking on the set of possible interpretations. We identify lexical semantic information automatically by exploiting the consistent correspondences between surface syntactic cues and lexical meaning. We evaluate our results against paraphrase judgments elicited experimentally from humans and show that the model’s ranking of meanings correlates reliably with human intuitions: meanings that are found highly probable by the model are also rated as plausible by the subjects. 
This paper describes a lexicon organized around systematic polysemy: a set of word senses that are related in systematic and predictable ways. The lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree-cut. We compare our lexicon to WordNet cousins, and the inter-annotator disagreement observed between WordNet Semcor and DSO corpora. 
I present a method of identifying cognates in the vocabularies of related languages. I show that a measure of phonetic similarity based on multivalued features performs better than “orthographic” measures, such as the Longest Common Subsequence Ratio (LCSR) or Dice’s coefﬁcient. I introduce a procedure for estimating semantic similarity of glosses that employs keyword selection and WordNet. Tests performed on vocabularies of four Algonquian languages indicate that the method is capable of discovering on average nearly 75% percent of cognates at 50% precision. 
Using ﬁnite-state automata for the text analysis component in a text-to-speech system is problematic in several respects: the rewrite rules from which the automata are compiled are difﬁcult to write and maintain, and the resulting automata can become very large and therefore inefﬁcient. Converting the knowledge represented explicitly in rewrite rules into a more efﬁcient format is difﬁcult. We take an indirect route, learning an efﬁcient decision tree representation from data and tapping information contained in existing rewrite rules, which increases performance compared to learning exclusively from a pronunciation lexicon. 
We present a simple architecture for parsing transcribed speech in which an edited-word detector ﬁrst removes such words from the sentence string, and then a standard statistical parser trained on transcribed speech parses the remaining words. The edit detector achieves a misclassiﬁcation rate on edited words of 2.2%. (The NULL-model, which marks everything as not edited, has an error rate of 5.9%.) To evaluate our parsing results we introduce a new evaluation metric, the purpose of which is to make evaluation of a parse tree relatively indiﬀerent to the exact tree position of EDITED nodes. By this metric the parser achieves 85.3% precision and 86.5% recall. 
In automatic speech recognition (ASR) enabled applications for medical dictations, corpora of literal transcriptions of speech are critical for training both speaker independent and speaker adapted acoustic models. Obtaining these transcriptions is both costly and time consuming. Non-literal transcriptions, on the other hand, are easy to obtain because they are generated in the normal course of a medical transcription operation. This paper presents a method of automatically generating texts that can take the place of literal transcriptions for training acoustic and language models. ATRS1 is an automatic transcription reconstruction system that can produce near-literal transcriptions with almost no human labor. We will show that (i) adapted acoustic models trained on ATRS data perform as well as or better than adapted acoustic models trained on literal transcriptions (as measured by recognition accuracy) and (ii) language models trained on ATRS data have lower perplexity than language models trained on non-literal data. Introduction Dictation applications of automatic speech recognition (ASR) require literal transcriptions of speech in order to train both speaker independent and speaker adapted acoustic models. Literal transcriptions may also be used to train stochastic language models that need to perform well on spontaneous or disfluent speech. With the exception of personal desktop systems, however, obtaining these transcriptions is costly and time consuming since they must be produced manually 
This paper describes an approach to Machine Translation that places linguistic information at its foundation. The difficulty of translation from English to Japanese is illustrated with data that shows the influence of various linguistic contextual factors. Next, a method for natural language transfer is presented that integrates translation examples (represented as typed feature structures with source-target indices) with linguistic rules and constraints. The method has been implemented, and the results of an evaluation are presented. Introduction High-quality automatic translation requires the disambiguation of common, highly ambiguous verbs, such as to have, to take, or to get. It also requires the correct handling of non-compositional, idiomatic expressions with varying degrees of “fixedness”. We view Machine Translation in terms of linguistic information represented as typed feature structures. By integrating translation information represented as example pairs with other types of linguistic information represented as rules, our approach extends the capabilities of current machine translation methods, and solves a number of key problems. 1. Linguistic Context for Translation In translating different words, phrases, and expressions, different types and amounts of information from the context need to be considered. (Only the sentential context is considered here.) So far, a systematic solution to this problem has not been found. This section illustrates the extent of this problem, and the remainder of this paper describes our approach.  1.1. Expressions with to have We examined the problem of translating the English main verb to have into Japanese. The verb to have was selected because it is quite common in colloquial English, yet forms a large variety of senses, collocations, and idioms. 615 different expressions containing the English verb to have were extracted from a 7000-sentence corpus from the “international travel” domain. Each English expression was manually translated into Japanese in the most general way possible. 
In human sentence processing, cognitive load can be deﬁned many ways. This report considers a deﬁnition of cognitive load in terms of the total probability of structural options that have been disconﬁrmed at some point in a sentence: the surprisal of word wi given its preﬁx w0...i−1 on a phrase-structural language model. These loads can be eﬃciently calculated using a probabilistic Earley parser (Stolcke, 1995) which is interpreted as generating predictions about reading time on a word-by-word basis. Under grammatical assumptions supported by corpusfrequency data, the operation of Stolcke’s probabilistic Earley parser correctly predicts processing phenomena associated with garden path structural ambiguity and with the subject/object relative asymmetry. Introduction What is the relation between a person’s knowledge of grammar and that same person’s application of that knowledge in perceiving syntactic structure? The answer to be proposed here observes three principles. Principle 1 The relation between the parser and grammar is one of strong competence. Strong competence holds that the human sentence processing mechanism directly uses rules of grammar in its operation, and that a bare minimum of extragrammatical machinery is necessary. This hypothesis, originally proposed by Chomsky (Chomsky, 1965, page 9) has been pursued by many researchers (Bresnan, 1982) (Stabler, 1991) (Steedman, 1992) (Shieber and Johnson, 1993), and stands in contrast with an approach directed towards the discovery of autonomous principles unique to the processing mechanism. Principle 2 Frequency aﬀects performance. The explanatory success of neural network and constraint-based lexicalist theories (McClelland and St. John, 1989) (MacDonald et al., 1994) (Tabor et al., 1997) suggests a statistical theory of language  performance. The present work adopts a numerical view of competition in grammar that is grounded in probability. Principle 3 Sentence processing is eager. “Eager” in this sense means the experimental situations to be modeled are ones like self-paced reading in which sentence comprehenders are unrushed and no information is ignored at a point at which it could be used. The proposal is that a person’s diﬃculty perceiving syntactic structure be modeled by word-toword surprisal (Attneave, 1959, page 6) which can be directly computed from a probabilistic phrasestructure grammar. The approach taken here uses a parsing algorithm developed by Stolcke. In the course of explaining the algorithm at a very high level I will indicate how the algorithm, interpreted as a psycholinguistic model, observes each principle. After that will come some simulation results, and then a conclusion. 
We propose a novel Co-Training method for statistical parsing. The algorithm takes as input a small corpus (9695 sentences) annotated with parse trees, a dictionary of possible lexicalized structures for each word in the training set and a large pool of unlabeled text. The algorithm iteratively labels the entire data set with parse trees. Using empirical results based on parsing the Wall Street Journal corpus we show that training a statistical parser on the combined labeled and unlabeled data strongly outperforms training only on the labeled data. 
We propose an algorithm to automatically induce the morphology of inflectional languages using only text corpora and no human input. Our algorithm combines cues from orthography, semantics, and syntactic distributions to induce morphological relationships in German, Dutch, and English. Using CELEX as a gold standard for evaluation, we show our algorithm to be an improvement over any knowledge-free algorithm yet proposed. 
We apply Support Vector Machines (SVMs) to identify English base phrases (chunks). SVMs are known to achieve high generalization performance even with input data of high dimensional feature spaces. Furthermore, by the Kernel principle, SVMs can carry out training with smaller computational overhead independent of their dimensionality. We apply weighted voting of 8 SVMsbased systems trained with distinct chunk representations. Experimental results show that our approach achieves higher accuracy than previous approaches. 
Developing dialogue systems is a complex process. In particular, designing eﬃcient dialogue management strategies is often diﬃcult as there are no precise guidelines to develop them and no sure test to validate them. Several suggestions have been made recently to use reinforcement learning to search for the optimal management strategy for speciﬁc dialogue situations. These approaches have produced interesting results, including applications involving real world dialogue systems. However, reinforcement learning suﬀers from the fact that it is state based. In other words, the optimal strategy is expressed as a decision table specifying which action to take in each speciﬁc state. It is therefore diﬃcult to see whether there is any generality across states. This limits the analysis of the optimal strategy and its potential for re-use in other dialogue situations. In this paper we tackle this problem by learning rules that generalize the state-based strategy. These rules are more readable than the underlying strategy and therefore easier to explain and re-use. We also investigate the capability of these rules in directing the search for the optimal strategy by looking for generalization whilst the search proceeds. 
Recent contributions to statistical language modeling for speech recognition have shown that probabilistically parsing a partial word sequence aids the prediction of the next word, leading to “structured” language models that have the potential to outperform n-grams. Existing approaches to structured language modeling construct nodes in the partial parse tree after all of the underlying words have been predicted. This paper presents a different approach, based on probabilistic left-corner grammar (PLCG) parsing, that extends a partial parse both from the bottom up and from the top down, leading to a more focused and more accurate, though somewhat less robust, search of the parse space. At the core of our new structured language model is a fast context-sensitive and lexicalized PLCG parsing algorithm that uses dynamic programming. Preliminary perplexity and word-accuracy results appear to be competitive with previous ones, while speed is increased. 
* School of Humanities, Language and Social Sciences, Stafford Street, Wolverhampton WV1 1SB, UK. E-maih r.mitkov@wlv.ac.uk 
Volume 27, Number 4  tachment ambiguities, role uncertainty, and the instantiation of empty categories. Based on this observation, Kennedy and Boguraev (1996) have suggested an adaptation of Lappin and Leass's approach to the shallow analysis frontend of English Constraint Grammar (Karlsson et al. 1995), which provides a part-of-speech tagging comprising an assignment of syntactic function but no constituent structure. This information deficiency is partly overcome by the application of a regular filter that heuristically reconstructs parts of the constituent structure. An alternative solution, which is based on the possibly partial but potentially more comprehensive and reliable output of a conventional parser, has been suggested in Stuckardt (1997). In the present paper, an approach to robust anaphor resolution is developed that enhances the latter work. The coreference resolution algorithm ROSANAl is developed, the core of which consists of a set of rule patterns by means of which the verification of disjoint reference rules is generalized in order to make it applicable to deficient (fragmentary) syntactic descriptions. Based on this algorithm, the ROSANA system, which works on the partial syntactic descriptions generated by the robust FDG (Functional Dependency Grammar of English) parser of J~irvinen and Tapanainen (1997), is implemented. By a formal evaluation on two text corpora that differ with respect to genre and domain, it is proven that ROSANA achieves robust (truly operational) high-quality coreference resolution on unrestricted texts. An in-depth analysis shows that the robust implementation of syntactic disjoint reference is nearly optimal. Compared with approaches that rely on a combination of shallow preprocessing and heuristic syntactic disjoint reference, the largely nonheuristic disjoint reference algorithmization employed by ROSANA opens up the possibility for a slight improvement. The performance study of the ROSANA system crucially rests on an enhancement of the evaluation methodology for coreference resolution systems, the development of which constitutes the second major contribution of the paper. As a supplement to the coreference class scoring scheme that was developed for the CO-task evaluation of the Message Understanding Conferences (Vilain et al. 1996), two additional eval- uation disciplines are defined that, on one hand, aim at supporting the developer of anaphor resolution systems, and, on the other hand, shed light on applicationaspects of pronoun interpretation. The evaluation of ROSANA according to the refined scoring scheme gives evidence that the interpretation quality may be improved by a genrespecific choice of the preference factors and their relative weights. This demonstrates the usefulness of enhancing the evaluation methodology for coreference resolution systems. The paper is organized as follows. In Section 2, the robustness issue of natural language processing is briefly discussed at a general level, and two models of robust anaphor resolution are introduced. In Section 3, by deriving a set of disjoint reference rule patterns for fragmentary syntax, the core component of a robust, operational anaphor resolution algorithm is developed. In Section 4, the ROSANA algorithm is designed, and an implementation, the ROSANA system, is described. In Section 5, an enhanced set of evaluation disciplines for coreference resolution systems is advocated for, and the respective formal measures are defined. In Section 6, the evaluation results of ROSANA are discussed. Finally, in section 7, ROSANA is compared with other approaches to anaphor resolution and, in particular, robust syntactic disjoint reference.  
2. Algorithms 2.1 Hobbs's Algorithm Hobbs (1978) presents two algorithms: a naive one based solely on syntax, and a more complex one that includes semantics in the resolution method. The naive one (henceforth, the Hobbs algorithm) is the one analyzed here. Unlike the other three algorithms analyzed in this project, the Hobbs algorithm does not appeal to any discourse models for resolution; rather, the parse tree and grammatical rules are the only information used in pronoun resolution. The Hobbs algorithm assumes a parse tree in which each NP node has an N type node below it as the parent of the lexical object. The algorithm is as follows: 1. Begin at the NP node immediately dominating the pronoun. 2. Walk up the tree to the first NP or S encountered. Call this node X, and call the path used to reach it p. 3. Traverse all branches below node X to the left of path p in a left-to- right, breadth-first manner. Propose as the antecedent any NP node that is encountered which has an NP or S node between it and X. If no antecedent is found, proceed to Step 4. 4. If node X is the highest S node in the sentence, traverse the surface parse trees of previous sentences in order of recency, the most recent first; each tree is traversed in a left-to-right, breadth-first manner, and when an NP node is encountered, propose it as the antecedent. If X is not the highest S node in the sentence, continue to Step 5. 5. From node X, go up the tree to the first NP or S node encountered. Call this new node X, and call the path traversed to reach it p. 6. If X is an NP node and if the path p to X did not pass through the 51 node that X immediately dominates, propose X as the antecedent. 7. Traverse all branches below node X to the left of path p in a left-to- right, breadth-first manner. Propose any NP node encountered as the antecedent. 8. If X is an S node, traverse all branches of node X to the right of path p in a left-to-right, breadth-first manner, but do not go below any NP or S node encountered. Propose any NP node encountered as the antecedent. 9. Go to Step 4. A match is "found" when the NP in question matches the pronoun in number, gender, and person. The algorithm amounts to walking the parse tree from the pronoun in question by stepping through each NP and S on the path to the top S and running a breadth-first search on NP's children left of the path. If a referent cannot be found in the current utterance, then the breadth-first strategy is repeated on preceding utterances. Hobbs did a hand-based evaluation of his algorithm on three different texts: a history chapter, a novel, and a news article. Four pronouns were considered: he, she, it, and they. Cases where it refers to a nonrecoverable entity (such as the time or weather) were not counted. The algorithm performed successfully on 88.3% of the 300 pronouns in the corpus. Accuracy increased to 91.7% with the inclusion of selectional constraints. 2.2 Centering Theory and BFP's Algorithm Centering theory is part of a larger theory of discourse structure developed by Grosz and Sidner (1986). These researchers assert that discourse structure has three compo-  508  Tetreault  Centering and Pronoun Resolution  nents: (1) a linguistic structure, which is the structure of the sequence of utterances; (2) the intentional structure, which is a structure of discourse-relevant purposes; and (3) the attentional state, which is the state of focus. The attentional state models the discourse participants' focus of attention determined by the other two structures at any one time. Also, it has global and local components that correspond to the two levels of discourse coherence. Centering models the local component of attentional state--namely, how the speaker's choice of linguistic entities affects the inference load placed upon the hearer in discourse processing. For example, referring to an entity with a pronoun signals that the entity is more prominently in focus. As described by Brennan, Friedman, and Pollard (1987) (henceforth, BFP) and Walker, Iida, and Cote (1994), entities called centers link an utterance with other utterances in the discourse segment. Each utterance within a discourse has one backwardlooking center (Cb) and a set of forward-looking centers (Cf). The Cf set for an utterance /do is the set of discourse entities evoked by that utterance. The Cf set is ranked according to discourse salience; the most accepted ranking is by grammatical role (by subject, direct object, indirect object). The highest-ranked element of this list is called the preferred center (Cp). The Cb represents the most highly ranked element of the previous utterance that is found in the current utterance. Essentially, it serves as a link between utterances. Abrupt changes in discourse topic are reflected by a change of Cb between utterances. In discourses where the change of Cb is minimal, the Cp of the utterance represents a prediction of what the Cb will be in the next utterance. Grosz, Joshi, and Weinstein (1986, 1995) proposed the following constraints of centering theory: Constraints For each utterance Ui, in a discourse segment D, consisting of utterances of U1 ... Urn: . There is precisely one Cb. 2. Every element of the Cf-list for Ui must be realized in Ui. 3. The center, Cb(Ui, D), is the highest-ranked element of Cf(Ui_I, D) that is realized in Ui. In addition, they proposed the following rules: Rules For each utterance U/, in a discourse segment D, consisting of utterances of U1 ... Urn: . If some element of Cf(Ui-1, D) is realized as a pronoun in Ui, then so is Cb(Ui, D). . Transition states (defined below) are ordered such that a sequence of Continues is preferred over a sequence of Retains, which are preferred over sequences of Shifts. The relationship between the Cb and Cp of two utterances determines the coherence between the utterances. Centering theory ranks the coherence of adjacent utterances with transitions that are determined by the following criteria: 1. whether or not the Cb is the same from Un-1 to Un; 2. whether or not this entity coincides with the Cp of U,.  509  Computational Linguistics  Table 1 Centering transition table. Cb(U,,) = Cp(U,~) Cb(Un) = Cp(Un-1)  Cb(U.) = Cb(U._O Continue Retain  Volume 27, Number 4 Cb(U.) # Cb(U._~) Smooth Shift Rough Shift  BFP and Walker, Iida, and Cote (1994) identified a finer gradation in the Shift transition, stating that Retains were preferred over Smooth Shifts, which were preferred over Rough Shifts. Table 1 shows the criteria for each transition. Given these constraints and rules, BFP proposed the following pronoun-binding algorithm based on centering: I. Generate all possible Cb - C f combinations. 2. Filter combinations by contraindices and centering rules. 3. Rank remaining combinations by transitions. Walker (1989) compared Hobbs and BFP on three small data sets using hand evaluation. The results indicated that the two algorithms performed equivalently over a fictional domain of 100 utterances; and Hobbs outperformed BFP over domains consisting of newspaper articles (89% to 79%) and a task domain (Tasks) (51% to 49%). 2.3 The S-List Approach The third approach (Strube 1998) discards the notions of backward- and forwardlooking centers but maintains the notion of modeling the attentional state. This method, the Sqist (salience list), was motivated by the BFP algorithm's problems with incrementality and computational overhead (it was also difficult to coordinate the algorithm with intrasentential resolution). 2.3.1 The S-List. The model has one structure, the S-list, which "describes the attentional state of the hearer at any given point in processing a discourse" (Strube 1998, page 1252). At first glance, this definition is quite similar to that of a Cfqist; however, the two differ in ranking and composition. First, the S-list can contain elements from both the current and previous utterance while the Cf-list contains elements from the previous utterance alone. Second, the S-list's elements are ranked not by grammatical role but by information status and then by surface order. The elements of the S-list are separated into three information sets--hearer-old discourse entities (OLD), mediated discourse entities (MED), and hearer-new discourse entities (NEW)--all of which are based on Prince's (1981) familiarity scale. The three sets are further subdivided: OLD consists of evoked and unused entities; MED consists of inferrables, containing inferrables, and anchored brand-new discourse intrasentential entities; NEW consists solely of brand-new entities. What sorts of NPs fall into these categories? Pronouns and other referring expressions, as well as previously mentioned proper names, are evoked. Unused entities are proper names. Inferrables are entities that are linked to some other entity in the hearer's knowledge, but indirectly. Anchored brand-new discourse entities have as their anchor an entity that is OLD. 510  Tetreault  Centering and Pronoun Resolution  The three sets are ordered by their information status. OLD entities are preferred over MED entities, which are preferred over NEW entities. Within each set, the ordering is by utterance and position in utterance. Basically, an entity of utterance x is preferred over an entity of utterance y if utterance x follows utterance y. If the entities are in the same utterance, they are ranked by position in the sentence: an entity close to the beginning of the sentence is preferred over one that is farther away. 2.3.2 Algorithm. The resolution algorithm presented here comes from Strube (1998) and personal communication with Michael Strube. For each u t t e r a n c e (U1 . . . UN): for each entity within Ui: 1. If Ui is a pronoun, then find a referent by looking through the S-list left to right for one that matches in gender, number, person, and binding constraints. Mark entity as EVOKED. I 2. If Ui is preceded by an indefinite article, then mark Ui as BRAND-NEW. 3. If Ui is not preceded by a determiner, then mark Ui as UNUSED. 4. Else mark Ui as ANCHORED BRAND-NEW. 5. Insert Ui into the S-list given the ranking described above. 6. Upon completion of Ui remove all entities from the S-list that were not realized in Ui. In short, the S-list method continually inserts new entities into the S-list in their proper positions and "cleanses" the list after each utterance to purge entities that are unlikely to be used again in the discourse. Pronoun resolution is a simple lookup in the S-list. Strube did perform a hand test of the S-list algorithm and the BFP algorithm on three short stories b y H e m i n g w a y a n d three articles f r o m the New York Times. BFP, with intrasentential centering added, successfully resolved 438 pronouns out of 576 (76%). The S-list approach performed much better (85%). 2.4 Left-Right Centering Algorithm Left-Right Centering (Tetreault 1999) is an algorithm built upon centering theory's constraints and rules as detailed in Grosz, Joshi, and Weinstein (1995). The creation of the LRC algorithm is motivated by BFP's limitation as a cognitive model in that it makes no provision for incremental resolution of pronouns (Kehler 1997). Psycholinguistic research supports the claim that listeners process utterances one word at a time. Therefore, when a listener hears a pronoun, he or she will try to resolve it immediately; if new information appears that makes the original choice incorrect (such as a violation of binding constraints), the listener will go back and find a correct antecedent. Responding to the lack of incremental processing in the BFP model, we have constructed an incremental resolution algorithm that adheres to centering constraints. It works by first searching for an antecedent in the current utterance; 2 if one is not found, then the previous Cf-lists (starting with the previous utterance) are searched  
 Computational Linguistics  Volume 27, Number 4  Freetext :~ TokenizatiOnsentenc&e Ii ~ Morphological ~I POS tagger Segmentation [ ~ Processing  Noun Phrase Identification  Named Entity Recognition  ~ NoNunePstherdase ~ Extraction  i [:~[ Semantic Class Determination  Figure 1 System architecture of natural language processing pipeline.  Markables  2. A Machine Learning Approach to Coreference Resolution We adopt a corpus-based, machine learning approach to noun phrase coreference resolution. This approach requires a relatively small corpus of training documents that have been annotated with coreference chains of noun phrases. All possible markables in a training document are determined by a pipeline of language-processing modules, and training examples in the form of feature vectors are generated for appropriate pairs of markables. These training examples are then given to a learning algorithm to build a classifier. To determine the coreference chains in a new document, all markables are determined and potential pairs of coreferring markables are presented to the classifier, which decides whether the two markables actually corefer. We give the details of these steps in the following subsections. 2.1 Determination of Markables A prerequisite for coreference resolution is to obtain most, if not all, of the possible markables in a raw input text. To determine the markables, a pipeline of natural language processing (NLP) modules is used, as shown in Figure 1. They consist of tokenization, sentence segmentation, morphological processing, part-of-speech tagging, noun phrase identification, named entity recognition, nested noun phrase extraction, and semantic class determination. As far as coreference resolution is concerned, the goal of these NLP modules is to determine the boundary of the markables, and to provide the necessary information about each markable for subsequent generation of features in the training examples. Our part-of-speech tagger is a standard statistical tagger based on the Hidden Markov Model (HMM) (Church 1988). Similarly, we built a statistical HMM-based noun phrase identification module that determines the noun phrase boundaries solely based on the part-of-speech tags assigned to the words in a sentence. We also implemented a module that recognizes MUC-style named entities, that is, organization, person, location, date, time, money, and percent. Our named entity recognition module uses the H M M approach of Bikel, Schwartz, and Weischedel (1999), which learns from a tagged corpus of named entities. That is, our part-of-speech tagger, noun phrase identification module, and named entity recognition module are all based on HMMs and learn from corpora tagged with parts of speech, noun phrases, and named entities, respectively. Next, both the noun phrases determined by the noun phrase identification module and the named entities are merged in such a way that if the noun phrase overlaps with a named entity, the noun phrase boundaries will be adjusted to subsume the named entity. 522  Soon, Ng, and Lira  Coreference Resolution  The nested noun phrase extraction module subsequently accepts the noun phrases and determines the nested phrases for each noun phrase. The nested noun phrases are divided into two groups: . Nested noun phrases from possessive noun phrases. Consider two possessive noun phrases marked by the noun phrase module, his long-range strategy and Eastern's parent. The nested noun phrase for the first phrase is the pronoun his, while for the second one, it is the proper name Eastern. . Nested noun phrases that are modifier nouns (or prenominals). For example, the nested noun phrase for wage reductions is wage, and for Union representatives, it is Union. Finally, the markables needed for coreference resolution are the union of the noun phrases, named entities, and nested noun phrases found. For markables without any named entity type, semantic class is determined by the semantic class determination module. More details regarding this module are given in the description of the semantic class agreement feature. To achieve acceptable recall for coreference resolution, it is most critical that the eligible candidates for coreference be identified correctly in the first place. In order to test our system's effectiveness in determining the markables, we attempted to match the markables generated by our system against those appearing in the coreference chains annotated in 100 SGML documents, a subset of the training documents available in MUC-6. We found that our system is able to correctly identify about 85% of the noun phrases appearing in coreference chains in the 100 annotated SGML documents. Most of the unmatched noun phrases are of the following types: 1. Our system generated a head n o u n that is a subset of the n o u n phrase in the annotated corpus. For example, Saudi Arabia, the cartel's biggest producer was annotated as a markable, but our system generated only Saudi Arabia. . Our system extracted a sequence of words that cannot be considered as a markable. . Our system extracted markables that appear to be correct but do not match what was annotated. For example, our system identified selective wage reductions, but wage reductions was annotated instead. 2.2 Determination of Feature Vectors To build a learning-based coreference engine, we need to devise a set of features that is useful in determining whether two markables corefer or not. In addition, these features must be generic enough to be used across different domains. Since the MUC-6 and MUC-7 tasks define coreference guidelines for all types of noun phrases and different types of noun phrases behave differently in terms of how they corefer, our features must be able to handle this and give different coreference decisions based on different types of noun phrases. In general, there must be some features that indicate the type of a noun phrase. Altogether, we have five features that indicate whether the markables are definite noun phrases, demonstrative noun phrases, pronouns, or proper names. There are many important knowledge sources useful for coreference. We wanted to use those that are not too difficult to compute. One important factor is the distance  523  Computational Linguistics  Volume 27, Number 4  between the two markables. McEnery, Tanaka, and Botley (1997) have done a study on how distance affects coreference, particularly for pronouns. One of their conclusions is that the antecedents of pronouns do exhibit clear quantitative patterns of distribution. The distance feature has different effects on different noun phrases. For proper names, locality of the antecedents may not be so important. We include the distance feature so that the learning algorithm can best decide the distribution for different classes of noun phrases. There are other features that are related to the gender, number, and semantic class of the two markables. Such knowledge sources are commonly used for the task of determining coreference. Our feature vector consists of a total of 12 features described below, and is derived based on two extracted markables, i and j, where i is the potential antecedent and j is the anaphor. Information needed to derive the feature vectors is provided by the pipeline of language-processing modules prior to the coreference engine. 1. Distance Feature (DIST): Its possible values are 0,1, 2, 3. . . . . This feature captures the distance between i and j. If i and j are in the same sentence, the value is 0; if they are one sentence apart, the value is 1; and so on. 2. i-Pronoun Feature (I_PRONOUN): Its possible values are true or false. If i is a pronoun, return true; else return false. Pronouns include reflexive pronouns (himself, herself), personal pronouns (he, him, you), and possessive p r o n o u n s (hers, her). 3. j-Pronoun Feature (J_PRONOUN): Its possible values are true or false. If j is a pronoun (as described above), then return true; else return false. 4. String Match Feature (STR_MATCH): Its possible values are true or false. If the string of i matches the string of j, return true; else return false. We first remove articles (a, an, the) and demonstrative p r o n o u n s (this, these, that, those) from the strings before performing the string comparison. Therefore, the license matches this license, that computer matches computer. 5. Definite Noun Phrase Feature (DEF_NP): Its possible values are true or false. In our definition, a definite noun phrase is a noun phrase that starts with the w o r d the. For example, the car is a definite n o u n phrase. If j is a definite noun phrase, return true; else return false. 6. Demonstrative Noun Phrase Feature (DEM_NP): Its possible values are true or false. A demonstrative noun phrase is one that starts with the w o r d this, that, these, or those. If j is a demonstrative n o u n phrase, then return true; else return false. 7. Number Agreement Feature (NUMBER): Its possible values are true or false. If i and j agree in number (i.e., they are both singular or both plural), the value is true; otherwise false. Pronouns such as they and them are plural, while it, him, and so on, are singular. The morphological root of a noun is used to determine whether it is singular or plural if the noun is not a pronoun. 8. Semantic Class Agreement Feature (SEMCLASS): Its possible values are true, false, or unknown. In our system, we defined the following semantic classes: "female," "male," "person," "organization," "location," "date," "time," "money,.... percent," and "object." These semantic classes  524  Soon, Ng, and Lim  Coreference Resolution  are arranged in a simple ISA hierarchy. Each of the "female" and "male" semantic classes is a subclass of the semantic class "person," while each of the semantic classes "organization," "location," "date," "time," "money," and "percent" is a subclass of the semantic class "object." Each of these defined semantic classes is then mapped to a WordNet synset (Miller 1990). For example, "male" is mapped to the second sense of the n o u n male in WordNet, "location" is m a p p e d to the first sense of the norm location, and so on. The semantic class determination module assumes that the semantic class for every markable extracted is the first sense of the head noun of the markable. Since WordNet orders the senses of a noun by their frequency, this is equivalent to choosing the most frequent sense as the semantic class for each norm. If the selected semantic class of a markable is a subclass of one of our defined semantic classes C, then the semantic class of the markable is C; else its semantic class is "unknown." The semantic classes of markables i and j are in agreement if one is the parent of the other (e.g., chairman with semantic class " p e r s o n " and Mr. Lim with semantic class "male"), or they are the same (e.g., Mr. Lim and he, both of semantic class "male"). The value returned for such cases is true. If the semantic classes of i and j are not the same (e.g., IBM with semantic class "organization" and Mr. Lim with semantic class "male"), return false. If either semantic class is "unknown," then the head noun strings of both markables are compared. If they are the same, return true; else return unknown. . Gender Agreement Feature (GENDER): Its possible values are true, false, or unknown. The gender of a markable is determined in several ways. Designators and pronouns such as Mr., Mrs., she, and he, can determine the gender. For a markable that is a person's name, such as Peter H. Diller, the gender cannot be determined b y the above method. In our system, the gender of such a markable can be determined if markables are f o u n d later in the d o c u m e n t that refer to Peter H. Diller b y using the designator form of the name, such as Mr. Diller. If the designator form of the name is not present, the system will look through its database of common human first names to determine the gender of that markable. The gender of a markable will be unknown for noun phrases such as the president and chief executive officer. The gender of other markables that are not "person" is determined by their semantic classes. Unknown semantic classes will have unknown gender while those that are objects will have "neutral" gender. If the gender of either markable i or j is unknown, then the gender agreement feature value is unknown; else if i and j agree in gender, then the feature value is true; otherwise its value is false. 10. B o t h - P r o p e r - N a m e s Feature (PROPER_NAME): Its possible values are true or false. A proper name is determined based on capitalization. Prepositions appearing in the n a m e such as of and and need not be in uppercase. If i and j are both proper names, return true; else return false. 11. Alias Feature (ALIAS): Its possible values are true or false. If i is an alias of j or vice versa, return true; else return false. That is, this feature value is true if i and j are named entities (person, date, organization, etc.) that refer to the same entity. The alias module works differently depending  525  Computational Linguistics  Volume 27, Number 4  on the n a m e d entity type. For i a n d j that are dates (e.g., 01-08 a n d Jan. 8), b y using string comparison, the day, m o n t h , a n d year values are extracted a n d c o m p a r e d . If they match, then j is an alias of i. For i a n d j that are "person," such as Mr. Simpson and Bent Simpson, the last w o r d s of the noun phrases are compared to determine whether one is an alias of the other. For organization names, the alias function also checks for a c r o n y m m a t c h such as IBM and International Business Machines Corp. In this case, the longer string is chosen to be the one that is converted into the acronym form. The first step is to remove all postmodifiers such as Corp. a n d Ltd. Then, the a c r o n y m function considers each w o r d in turn, and if the first letter is capitalized, it is used to form the acronym. Two variations of the acronyms are produced: one with a period after each letter, and one without. 12. A p p o s i t i v e Feature (APPOSITIVE): Its possible values are true or false. If j is in apposition to i, return true; else return false. For example, the m a r k a b l e the chairman of Microsoft Corp. is in apposition to Bill Gates in the sentence Bill Gates, the chairman of Microsoft Corp. . . . . . O u r s y s t e m determines whether j is a possible appositive construct by first checking for the existence of verbs and proper punctuation. Like the above example, most appositives do not have any verb; and an appositive is separated b y a c o m m a from the most immediate antecedent, i, to which it refers. Further, at least one of i and j must be a proper name. The MUC-6 and MUC-7 coreference task definitions are slightly different. In MUC-6, j needs to be a definite noun phrase to be an appositive, while both indefinite and definite noun phrases are acceptable in MUC-7. As an example, Table 1 shows the feature vector associated with the antecedent i, Frank Newman, and the a n a p h o r j, vice chairman, in the following sentence: (1) Separately, Clinton transition officials said that Frank Newman, 50, vice chairman a n d chief financial officer of BankAmerica Corp., is expected to be nominated as assistant Treasury secretary for domestic finance.  Table 1 Feature vector of the markable pair (i = FrankNewman, j = vicechairman).  Feature  Value  DIST  0  I_PRONOUN  -  J~RONOUN  -  STR_MATCH  -  DEF_NP  -  DEMaNP  -  NUMBER  +  SEMCLASS  
University of Alicante Maximiliano Saiz-Noeda* University of Alicante  This paper presents an algorithmfor identifying noun phrase antecedents of third person personal pronouns, demonstrative pronouns, reflexive pronouns, and omitted pronouns (zero pronouns) in unrestricted Spanish texts. We define a list of constraints and preferences for different types of pronominal expressions, and we document in detail the importance of each kind of knowledge (lexical, morphological, syntactic, and statistical) in anaphora resolution for Spanish. The paper also provides a definition for syntactic conditions on Spanish NP-pronoun noncoreference using partial parsing. The algorithm has been evaluated on a corpus of 1,677 pronouns and achieved a success rate of 76.8%. We have also implemented four competitive algorithms and tested their performance in a blind evaluation on the same test corpus. This new approach could easily be extended to other languages such as English, Portuguese, Italian, or Japanese. 1. Introduction We present an algorithm for identifying noun phrase antecedents of personal pronouns, demonstrative pronouns, reflexive pronouns, and omitted pronouns (zero pronouns) in Spanish. The algorithm identifies both intrasentential and intersentential antecedents and is applied to the syntactic analysis generated by the slot unification parser (SUP) (Ferr~ndez, Palomar, and Moreno 1998b). It also combines different forms of knowledge by distinguishing between constraints and preferences. Whereas constraints are used as combinations of several kinds of knowledge (lexical, morphological, and syntactic), preferences are defined as a combination of heuristic rules extracted from a study of different corpora. We present the following main contributions in this paper: • an algorithm for anaphora resolution in Spanish texts that uses different kinds of knowledge * Departmentof Softwareand ComputingSystems,Alicante,Spain.E-mail:(Palomar) mpalomar@dlsi.ua.es,(Ferr~ndez)antonio@dlsi.ua.es,(Martfnez-Barco)patricio@dlsi.ua.es,(Peral) jperal@dlsi.ua.es, (Saiz-Noeda) max@dlsi.ua.es, (Mufioz) rafael@dlsi.ua.es t Departmentof InformationSystemsand Computation,Valencia,Spain.E-mail:hnoreno@dsic.upv.es @ 2001Associationfor ComputationalLinguistics  Computational Linguistics  Volume 27, Number 4  • an exhaustive study of the importance of each kind of knowledge in Spanish anaphora resolution • a proposal concerning syntactic conditions on NP-pronoun noncoreference in Spanish that can be evaluated on a partial parse tree • a proposal regarding preferences that are appropriate for resolving anaphora in Spanish and that could easily be extended to other languages • a blind test of the algorithm • a comparison with other approaches to anaphora resolution that we have applied to Spanish texts using the same blind test  In Section 2, we show the classification scheme we used to identify the different types of anaphora that we would be resolving. In Section 3, we present the algorithm and discuss its main properties. In Section 4, we evaluate the algorithm. In Section 5, we compare our algorithm with several other approaches to anaphora resolution. Finally, we present our conclusions.  2. Our Classification Scheme for Pronominal Expressions in Spanish  In this section, we present our classification scheme for identifying the different types of anaphora that we will be resolving. Personal pronouns (PPR), demonstrative pronouns (DPR), reflexive pronouns (RPR), and omitted pronouns (OPa) are some of the most frequent types of anaphoric expressions found in Spanish and are the main subject of this study. Personal and demonstrative pronouns are further classified according to whether they appear within a prepositional phrase (PP) or whether they are complement personal pronouns (clitic pronouns1). We present examples for each of the four types of common anaphora. Each example is presented in three forms: as a Spanish sentence, as a word-to-word translation into English, and correctly translated into English.2  2.1 Clitic Personal Pronouns (CPPR) In the case of clitic personal pronouns, I0, la, le 'him, her, it' and los, las, les 'them', we consider that the third person personal pronoun plays the role of the complement.  (1) Ana abre [la verja]i y lai cierra tras de si. Ana opens [the gate]/ and it/ closes after herself 'Ana opens the gate and closes it after herself.'  2.2 Personal Pronouns Not Included in a PP (PPanotPP) We include in this class the personal pronouns ~l, ella, ello 'he, she, it' and ellas, ellos 'they'.  (2)  Andr6si es mi vecino, t~li vive en el segundo piso.  Andr6si is my neighbor Hei lives on the second floor  'Andr6s is my neighbor. He lives on the second floor.'  
 Volume 27, Number 4  2. Reporting Pronoun Resolution Performance This squib is necessary because past reports of pronoun resolution performance have included inconsistent amounts of detail. Some provide complete details of the experimental design and results, while others (e.g., Byron and Stent 1998) fail to answer even basic questions: What pronouns did this study address? Which pronouns were resolved correctly? In order for a reader to assess performance scores, a report must describe its test data so that the reader knows exactly what the study includes and what it excludes from T. This section briefly discusses what details should be provided. 2.1 Describing the Test Data 2.1.1 Corpus Type. Each pronoun resolution evaluation is carried out over an evaluation corpus, for example a set of human conversations or a number of pages from a book. Details about the evaluation corpus's genre (written or spoken, news or fiction, etc.) and size (e.g., word count, number of discourse units) should be provided to help the reader understand how the corpus chosen for evaluation affected the results obtained. 2.1.2 Lexical Coverage. A report should clearly indicate which pronouns the study included, called the coverage, by listing each distinct pronoun type (e.g., it, itself, and its are shown separately). Some past reports give no coverage details at all, while others (e.g., Popescu-Belis and Robba 1997, page 97) precisely state their coverage: "/il/,/elle/,/le/,/la/,/1'/,/lui/,/ils/,~/elles/. '' A categorical description, such as "[results are shown for] personal and possessive pronouns" (Strube 1998, page 1256) is insufficient because the author might assume that his exclusion of certain pronouns (e.g., first person pronouns) need not be mentioned since they are excluded by most other studies. 2.1.3 Exclusions. Before pronoun resolution is executed, any evaluation corpus must be brought into line with the goals of the study by marking individual pronoun tokens as included or excluded.1Even tokens of pronoun types covered in the study might be excluded from the evaluation. The reasons for considering tokens to be out of scope for a study are called exclusion criteria, and the set of pronouns remaining after all exclusions are applied to the corpus is the evaluation set. Different studies apply different exclusions, and pronoun tokens that were excluded in one study might be counted as errors in another. Cataphors are a case in point. Some pronoun resolution techniques address cataphora (e.g., Lappin and Leass 1994), so the cataphors are included w h e n calculating the performance for these techniques. Other techniques are not designed to identify cataphors, and for some of those the authors exclude cataphors from their test data (e.g., Ge, Hale, and Charniak 1998) while others include them but count the cataphors as errors (e.g., Strube and Hahn 1999). There are no standard guidelines for w h a t exclusions are reasonable to apply, although it would be beneficial for such a standard to exist. Since performance measures are based on the number of pronouns in the evaluation set, such inconsistencies make recall scores from separate studies difficult to compare. Because each study defines its own idiosyncratic set of exclusion criteria, it is important that performance reports clearly list which criteria were applied. Some  
 Computational Linguistics  Volume 27, Number 4  Table 1 Centering constraints and rules (adapted from Walker,Joshi, and Prince [1998, pages 3-4]). Constraints C1. There is precisely one Cb. C2. Every element of Cf(Un)must be realized in U,. C3. Cb(U,) is the highest-ranked element of Cf(Un-1)that is realized in Un. Rules R1. If some element of Cf(Un_l) is realized as a pronoun in U,, then so is Cb(Un). R2. Continue is preferred over Retain, which is preferred over Smooth Shift, which is preferred over Rough Shift. account of CT both over- and undergenerates. On the one hand, the stipulated preference for Retain over Smooth Shift has not been confirmed by empirical evidence, and cannot be naturally incorporated in standard generation architectures. On the other hand, there is no mechanism within the theory to predict specific cases where a Retain or a Shift may be preferred over a Continue transition, as in the Retain-Shift pattern that has been argued to signal the introduction of a new discourse topic (Brennan, Friedman, and Pollard 1987 [BFP]; Strube and Hahn 1999). I aim to overcome these difficulties under an analysis that gives a partial ordering of the classic transitions, incorporating a "streamlined" version of Strube and Hahn's notion of "cheapness" to handle the Retain-Shift pattern. I do not claim to offer new empirical results; the aim is rather to consolidate existing results in a more economic and principled formulation of Rule 2 itself. Finally, given that CT addresses local rather than global coherence, we need to consider the question, How local is "local"? Two possible notions of local coherence are (1) overall coherence of a multi-utterance discourse segment (as in the original GJW model, which stipulates preferences for coherent sequences of transitions)--this has been called "not psychologically plausible from a speaker's perspective" (Brennan 1998, page 231); (2) coherence between immediately adjacent utterances, as in the BFP algorithm, which replaces the original preference for sequences of transitions with a preference ordering on transitions. In this paper, I explore an intermediate position put forward by Strube and Hahn (1999), which is a preference over pairs of transitions or triples of utterances, which may or may not cross segment boundaries. 2. Transition Rules The main claims of CT are formalized in terms of Cb, the backward-looking center; Cf, a list of forward-looking centers for each utterance Un; and Cp or preferred center, the most salient candidate for subsequent utterances. Cf(Un) is a partial ordering on the entities mentioned (or "realized") in Un, ranked by grammatical role; for example, SUBJ > DIR-OBJ > INDIR-OBJ > COMP(S) > ADJUNCT(S). Cb(Un) is defined as the highest-ranked member of Cf(Un-1) that is realized in U,. Cp(U,) is the highest-ranked member of Cf(Un), and is predicted to be Cb(Un+l). The ranking of Cf by grammatical role has been widely adopted in the litera- ture following BFE though it is questioned by some researchers including Strube and 580  Kibble  Rule 2 of Centering Theory  Table 2 Centering transitions (Walker,Joshi, and Prince 1998, page 6).  Cb(Un) = Cb(Un_l) or Cb(Un-1) undefined Cb(Un) = Cp(Un) Continue Cb(Un) # Cp(Un) Retain  Cb(Un) 76 Cb(Un_l) Smooth Shift Rough Shift  Hahn (1999), who propose a ranking based on "functional information structure," a combination of degrees of "givenness" and left-to-right order. They note that the BFP ranking is not appropriate for German, which they say is a free-word-order language (page 310); more accurately, relative order of NPs within a clause is not determined by grammatical role to the extent that it is in English. For the purposes of this paper, there is no need to commit to either BFP's or Strube and Hahn's rankings, or to go into the details of the latter's "functional centering" model, as both make the same predictions for the examples considered. Strube and Hahn themselves (page 334) state that the grammatical and functional analyses achieved consistent results for all examples in GJW. I adopt the ranking by grammatical role for purposes of exposition. 2.1 "Salience" and "Cohesion" Transitions are defined in terms of two tests: whether the Cb stays the same (Cb(Un) = Cb(U,~-I)), and whether the Cb is realized as the most prominent NP (grammatically or otherwise): Cb(Un) = Cp(Un). For the sake of convenience and concision, I refer to the first of these tests as cohesion and the second as salience; it is important to keep in mind that I use the terms in these defined and limited ways, and to disregard (for the time being) other uses of the terms in the literature. There are four possible combinations, which are displayed in Table 2, resulting in the named transitions Continue, Retain, Smooth Shift, and Rough Shift. The optimal case, where both salience and cohesion obtain, is Continue; the least preferred is Rough Shift. Walker, Joshi, and Prince (1998), following BFP, stipulate that Retain is preferred over Smooth Shift, which implies that cohesion is a stronger requirement than salience. However, corpus analyses reported by di Eugenio (1998, page 127), Hurewitz (1998, pages 280ff.), and Passoneau (1998, pages 338ff.) do not support this claim. In fact, all these researchers found a higher percentage of Smooth Shifts than Retains. In a spoken corpus, Passoneau found more Shifts than Continues. A preponderance of Shifts over Continues may reflect the domain and content of a text rather than the author's organizational goals. In fact, it can be seen that sequences of Smooth Shifts are rather natural in certain kinds of narrative or descriptive texts; see Example 1 (adapted from a pharmaceutical leaflet). Example 1 a. The name of your medicine/ is Compound X. b. Iti contains budesonideJ. (Continue) 581  Computational Linguistics  Volume 27, Number 4  c. Thisj is one of a group of medicines called corticosteroidsk. (Smooth Shift) d. Thesek can help to relieve the symptoms of hay fever or rhinitis. (Smooth Shift) This does not appear to be an incoherent text, but there is no way that the content could be rearranged to turn the Shifts into Continues. However, we can see that the author has maintained centering coherence as far as the content allows. We may conclude that not only does corpus evidence fail to confirm the canonical ordering, but in fact corpus analysis itself is not sufficient to evaluate the claims of CT without taking into account the underlying semantic content of a text. That is, statistics about the relative frequency of occurrences of different transition types do not in themselves tell us much about which transitions are preferred in particular situations since they do not take account of the choices available to an author.1 A more promising approach is that of Brennan (1998), who gave subjects the controlled narrative task of providing a running commentary on a video recording of a basketball game, and used the videotape itself to construct a "propositional script" listing the sequence of events and their participants, and identifying players who were likely to continue as the center of attention over a sequence of incidents. 2.2 Rule 2 Applied to Generation Reiter (1994) claimed that existing generation systems converged on a "consensus," generic natural language generation (NLG) architecture consisting of the following tasks: • Content determination/text planning: deciding the content of a message and organizing the component propositions into a text tree; • Sentence planning: aggregating propositions into clausal units and choosing lexical items corresponding to concepts in the knowledge base, including referring expressions; • Linguistic realization: taking care of surface details such as agreement and orthography. I have argued elsewhere (Kibble 1999) that if CT is to be implemented in an NLG system, the principles I call "salience" and "cohesion" belong to different tasks within this scheme: "salience" is a matter for sentence planning, choosing a verb form or some other construction that makes the Cb prominent within a clause or sentence, while "cohesion"---ordering propositions in a text to maintain referential continuity-is a matter for text planning. So there may be no single point in the generation process where the system has a choice between Retain and Shift, for instance: rather, the terms retain and shift describe the composite results of choices made at different stages of the generation task. This point is discussed in more detail in the cited paper. Referential continuity as determined by CT is only one of a number of factors determining the fluency and acceptability of generated text; see Kibble and Power (2000) for further discussion.  
• Department of Computer Science, 211 Regent Court, Portobello Street, Sheffield $1 4DP, UK (~) 2001 Association for Computational Linguistics  Computational Linguistics  Volume 27, Number 3  that the techniques employed will be applicable when a larger vocabulary is tackled. However it is likely that mark-up for a restricted vocabulary can be carried out more rapidly since the subject has to learn the possible senses of fewer words. Among the researchers mentioned above, one must distinguish between, on the one hand, supervised approaches that are inherently limited in performance to the words over which they evaluate because of limited training data and, on the other hand, approaches whose unsupervised learning methodology is applied to only small numbers of words for evaluation, but which could in principle have been used to tag all content words in a text. Others, such as Harley and Glennon (1997) and ourselves Wilks and Stevenson (1998a, 1998b; Stevenson and Wilks 1999), have concentrated on approaches that disambiguate all content words.1In addition to avoiding the problems inherent in restricted vocabulary systems, wide coverage systems are more likely to be useful for NLP applications, as discussed by Wilks et al. (1990). A third difference concerns the granularity of WSD attempted, which one can illustrate in terms of the two levels of semantic distinctions found in many dictionaries: homograph and sense (see Section 3.1). Like Cowie, Guthrie, and Guthrie (1992), we shall give results at both levels, but it is worth pointing out that the targets of, say, work using translation equivalents (e.g., Brown et al. 1991; Gale, Church, and Yarowsky 1992c; and see Section 2.3) and Roget categories (Yarowsky 1992; Masterman 1957) correspond broadly to the wider, homograph, distinctions. In this paper we attempt to show that the high level of results more typical of systems trained on many instances of a restricted vocabulary can also be obtained by large vocabulary systems, and that the best results are to be obtained from an optimization of a combination of types of lexical knowledge (see Section 2). 1.1 Lexical Knowledge and WSD Syntactic, semantic, and pragmatic information are all potentially useful for WSD, as can be demonstrated by considering the following sentences: (1) John did not feel well. (2) John tripped near the well. (3) The bat slept. (4) He bought a bat from the sports shop. The first two sentences contain the ambiguous word well; as an adjective in (1) where it is used in its "state of health" sense, and as a noun in (2), meaning "water supply". Since the two usages are different parts of speech they can be disambiguated by this syntactic property. Sentence (3) contains the word bat, whose nominal readings are ambiguous between the "creature" and "sports equipment" meanings. Part-of-speech information cannot disambiguate the senses since both are nominal usages. However, this sentence can be disambiguated using semantic information, such as preference restrictions. The verb sleep prefers an animate subject and only the "creature" sense of bat is animate. So Sentence (3) can be effectively disambiguated by its semantic behaviour but not by its syntax.  
t Department of Computer Science; University of Toronto; 6 King's College Road; Toronto, ON M5S 3H5 Canada; suzanne@cs.toronto.edu Q 2001 Association for Computational Linguistics  Computational Linguistics  Volume 27, Number 3  Table 1 Examples of verbs from the three optionally intransitive classes.  Unergative  The horse raced past the barn. The jockey raced the horse past the barn.  Unaccusative The butter melted in the pan. The cook melted the butter in the pan.  Object-Drop The boy played. The boy played soccer.  Other work has attempted to learn deeper semantic properties such as selectional restrictions (Resnik 1996; Riloff and Schmelzenbach 1998), verbal aspect (Klavans and Chodorow 1992; Siegel 1999), or lexical-semantic verb classes such as those proposed by Levin (1993) (Aone and McKee 1996; McCarthy 2000; Lapata and Brew 1999; Schulte im Walde 2000). In this paper, we focus on argument structure--the thematic roles assigned by a verb to its arguments--as the way in which the relational semantics of the verb is represented at the syntactic level. Specifically, our proposal is to automatically classify verbs based on argument structure properties, using statistical corpus-based methods. We address the problem of classification because it provides a means for lexical organization which can effectively capture generalizations over verbs (Palmer 2000). Within the context of classification, the use of argument structure provides a finer discrimination among verbs than that induced by subcategorization frames (as we see below in our example classes, which allow the same subcategorizations but differ in thematic assigmnent), but a coarser classification than that proposed by Levin (in which classes such as ours are further subdivided according to more detailed semantic properties). This level of classification granularity appears to be appropriate for numerous language engineering tasks. Because knowledge of argument structure captures fundamental participant/event relations, it is crucial in parsing and generation (e.g., Srinivas and Joshi [1999]; Stede [1998]), in machine translation (Dorr 1997), and in information retrieval (Klavans and Kan 1998) and extraction (Riloff and Schmelzenbach 1998). Our use of statistical corpus-based methods to achieve this level of classification is motivated by our hypothesis that class-based differences in argument structure are reflected in statistics over the usages of the component verbs, and that those statistics can be automatically extracted from a large annotated corpus. The particular classification problem within which we investigate this hypothesis is the task of learning the three major classes of optionally intransitive verbs in English: unergative, unaccusative, and object-drop verbs. (For the unergative/unaccusative distinction, see Perlmutter [1978]; Burzio [1986]; Levin and Rappaport Hovav [1995]). Table 1 shows an example of a verb from each class in its transitive and intransitive usages. These three classes are motivated by theoretical linguistic properties (see discussion and references below, and in Stevenson and Merlo [1997b]; Merlo and Stevenson [2000b]). Furthermore, it appears that the classes capture typological distinctions that are useful for machine translation (for example, causative unergatives are ungrammatical in many languages), as well as processing distinctions that are useful for generating naturally occurring language (for example, reduced relatives with unergative verbs are awkward, but they are acceptable, and in fact often preferred to full relatives for unaccusative and object-drop verbs) (Stevenson and Merlo 1997b; Merlo and Stevenson 1998). 374  Merlo and Stevenson  Statistical Verb Classification  Table 2 Summary of thematic role assignments by class.  Transitive  Intransitive  Classes  Subject  Object Subject  Unergative Agent (of Causation) Agent  Unaccusative Agent (of Causation) Theme  Object-Drop  Agent  Theme  Agent Theme Agent  The question then is what underlies these distinctions. We identify the property that precisely distinguishes among these three classes as that of argument structure-i.e., the thematic roles assigned by the verbs. The thematic roles for each class, and their mapping to subject and object positions, are summarized in Table 2. Note that verbs across these three classes allow the same subcategorization frames (taking an NP object or occurring intransitively); thus, classification based on subcategorization alone would not distinguish them. On the other hand, each of the three classes is comprised of multiple Levin classes, because the latter reflect more detailed semantic distinctions among the verbs (Levin 1993); thus, classification based on Levin's labeling would miss generalizations across the three broader classes. By contrast, as shown in Table 2, each class has a unique pattern of thematic assignments, which categorize the verbs precisely into the three classes of interest. Although the granularity of our classification differs from Levin's, we draw on her hypothesis that semantic properties of verbs are reflected in their syntactic behavior. The behavior that Levin focuses on is the notion of diathesis alternation--an alternation in the expression of the arguments of a verb, such as the different mappings between transitive and intransitive that our verbs undergo. Whether a verb participates in a particular diathesis alternation or not is a key factor in Levin's approach to classification. We, like others in a computational framework, have extended this idea by showing that statistics over the alternants of a verb effectively capture information about its class (Lapata 1999; McCarthy 2000; Lapata and Brew 1999). In our specific task, we analyze the pattern of thematic assignments given in Table 2 to develop statistical indicators that are able to determine the class of an optionally intransitive verb by capturing information across its transitive and intransitive alternants. These indicators serve as input to a machine learning algorithm, under a supervised training methodology, which produces an automatic classification system for our three verb classes. Since we rely on patterns of behavior across multiple occurrences of a verb, we begin with the problem of assigning a single class to the entire set of usages of a verb within the corpus. For example, we measure properties across all occurrences of a word, such as raced, in order to assign a single classification to the lexical entry for the verb race. This contrasts with work classifying individual occurrences of a verb in each local context, which have typically relied on training that includes instances of the verbs to be classified--essentially developing a bias that is used in conjunction with the local context to determine the best classification for new instances of previously seen verbs. By contrast, our method assigns a classification to verbs that have not previously been seen in the training data. Thus, while we do not as yet assign different classes to the instances of a verb, we can assign a single predominant class to new verbs that have never been encountered. To preview our results, we demonstrate that combining just five numerical indicators, automatically extracted from large text corpora, is sufficient to reduce the error 375  Computational Linguistics  Volume 27, Number 3  rate in this classification task by more than 50% over chance. Specifically, we achieve almost 70% accuracy in a task whose baseline (chance) performance is 34%, and whose expert-based upper bound is calculated at 86.5%. Beyond the interest for the particular classification task at hand, this work addresses more general issues concerning verb class distinctions based in argument structure. We evaluate our hypothesis that such distinctions are reflected in statistics over corpora through a computational experimental methodology in which we investigate as indicated each of the subhypotheses below, in the context of the three verb classes under study: • Lexical features capture argument structure differences between verb classes. 1 • The linguistically distinctive features exhibit distributional differences across the verb classes that are apparent within linguistic experience (i.e., they can be collected from text). • The statistical distributions of (some of) the features contribute to learning the classifications of the verbs. In the following sections, we show that all three hypotheses above are borne out. In Section 2, we describe the argument structure distinctions of our three verb classes in more detail. In support of the first hypothesis above, we discuss lexical correlates of the underlying differences in thematic assignments that distinguish the three verb classes under investigation. In Section 3, we show how to approximate these features by simple syntactic counts, and how to perform these counts on available corpora. We confirm the second hypothesis above, by showing that the differences in distribution predicted by the underlying argument structures are largely found in the data. In Section 4, in a series of machine learning experiments and a detailed analysis of errors, we confirm the third hypothesis by showing that the differences in the distribution of the extracted features are successfully used for verb classification. Section 5 evaluates the significance of these results by comparing the program's accuracy to an expertbased upper bound. We conclude the paper with a discussion of its contributions, comparison to related work, and suggestions for future extensions. 2. Deriving Classification Features from Argument Structure Our task is to automatically build a classifier that can distinguish the three major classes of optionally intransitive verbs in English. As described above, these classes are differentiated by their argument structures. In the first subsection below, we elaborate on our description of the thematic role assignments for each of the verb classes under investigation--unergative, unaccusative, and object-drop. This analysis yields a distinctive pattern of thematic assignment for each class. (For more detailed discussion concerning the linguistic properties of these classes, and the behavior of their component verbs, please see Stevenson and Merlo [1997b]; Merlo and Stevenson [2000b].) Of course, the key to any automatic classification task is to determine a set of useful features for discriminating the items to be classified. In the second subsection below,  
The desirability of combining text, layout, graphics, diagrams, punctuation, and typesetting in order to present information most effectively is uncontroversial--indeed, in traditional graphic design and publishing, they could scarcely be conceived of as separate. It is therefore natural that computational attempts to synthesize texts, diagrams, and layout automatically should also now converge. In this paper, we argue that effective and coherent information presentation is best supported by adopting a common framework for physical layout and language/diagram generation. Whereas previous research has made this point convincingly for graphical and textual representations-particularly, for example, in the WIP (Andr4 et al. 1993), COMET (Feiner and McKeown 1993), and SAGE (Kerpedjiev et al. 1997; Green, Carenini, and Moore 1998) systems--we take this further and demonstrate that the same commonalities extend to include overall page layout, an area that has not previously received sufficient attention.  • FBIO,Sprach-und Literaturwissenschaften,Universityof Bremen,28334Breman,Germany,E-mail: bateman@uni-bremen.de. t Intelligent ViewsGmbH,Julius-Reiber-Str.17 G423Darmstadt, Germany,E-mail: {t.kamps,j.kleinz, k.reichenberger}@i-views.de. (~)2001Associationfor ComputationalLinguistics  Computational Linguistics  Volume 27, Number 3  The paper focuses on two aspects of automatic information presentation new in our work: • a general mechanism for organizing presentations around informational regularities in the data to be expressed--the regularities then inform the presentational strategies used for natural language, diagrarn, and layout generation; • the construction of an indirect relationship between structured communicative intentions (typically represented in both mono- and multimodal work by some kind of rhetorical structure) and their expression in page layout. The former allows us to ensure broad consistency of perspective and informational organization across elements presented using different media e.g., across diagram and text; the latter allows us to draw closer to the kind of sophisticated layout that is observable in human-produced presentations. We organize the paper as follows. We first introduce the mechanism for datadriven aggregation that we have developed, since this underlies our approach to both natural language generation and diagram design (Section 2). We then sketch the place of layout as an organizing framework within our approach as a whole (Section 3), setting out by means of examples some of the issues focused upon in the empirical investigation (Section 4). We then summarize the results of the empirical study in terms of an abstract specification for performing page layout (Section 5) and provide a first illustration of its application within the prototype multimodal information-presentation system DArtbio (Dictionary of Art: biographies) (Section 6). We conclude b y summa- rizing the main contributions of our work and some of the follow-up research and development to which it is now leading (Section 7). 2. Data-driven Aggregation for Visualization and Natural Language Generation It is commonly recognized in work on multimodal information presentation that much of the true value of such presentations lies in appropriate juxtapositions of nonidentical but overlapping information. Textual presentations and graphical presentations have differing strengths and weaknesses and so their combination can achieve powerful synergies. Conversely, simply placing textual and graphical information together is no guarantee that one view is supportive of another: if the perspective on the data taken in a graphic and that taken in a text have no relation (or, worse, even clash), then the result is incoherence rather than synergy--cf, the discussions by authors such as Arens, Hovy, and Vossers (1993), Fasciano and Lapalme (1996), Green et al. (1998), and Fasciano and Lapalme (2000). One means of ensuring mutually compatible presentations across modes is to drive both the language and the graphic generation from the same communicative intentions. If an automatic natural language generator and an automatic graphic generator both receive the task of expressing broadly similar, or compatible, intentions then there is a good chance that the resulting presentations will also be perceived to be compatible and mutually supportive. This has been used to good affect in systems such as CGS (Caption Generation System) of Mittal et al. (1998), where it is clearly crucial that the text and the graphic be in close correspondence. Another, in some ways related, approach is to derive both the graphic and textual elements from different components of a single presentation plan: thus, for example, one part of the  410  Bateman, Kamps, Kleinz and Reichenberger  Constructive Page Generation  presentation plan might express textually an instruction that must be carried out (turn the dial), while another part of the plan elaborates on that instruction by showing a diagram in which the location of the action to be performed is identified graphically. This has been explored extensively in systems such as WIP (Andr~ et al. 1993), PPP (AndrG Rist, and Mfiller 1998), and COMET (Feiner and McKeown 1993). While both of these approaches are essentially top-down, or goal driven, effective presentations can also be produced by responding to regularities found in the data to be presented. Such regularities are difficult to predict as they are strongly contingent on what set of data happens to have been selected. "Data-driven" methods of this kind are commonly found in automatic visualization, where the goal is to present users with some comprehensible view of large collections of data. Utilizing regularities in the data is essential for effective visualization. In previous work (Reichenberger, Kamps, and Golovchinsky 1995), a set of techniques for generative diagram design were developed for precisely this task, i.e., for presenting overviews of datasets. We subsequently recognized that this approach also has applications to the task of aggregation in natural language generation, and we thus adapted it for use across both textual and graphical presentation modes. This provides a further technique for ensuring consistency between graphical and textual presentations--if both the graphical and textual presentations express the same regularities, or redundancies, that have been found in a dataset, then they are necessarily compatible in this respect. This allows us to use contingent data-driven organizations for generating information while nevertheless preserving coherent and mutually supportive views across presentation modalities. 2.1 Data-driven Aggregation: the Mechanism The original generative diagram-design algorithms developed by Reichenberger, Kamps, and Golovchinsky (1995) built on the landmark work of Mackinlay (1986). Here, a data-classification algorithm flexibly links relational data with elements of a graphical language. These elements are allocated particular degrees of expressiveness so that appropriate graphical resources can be selected as required to capture the data being described. Reichenberger et al. extended this approach by employing a general type hierarchy of data properties to determine algorithmically the most specific property subtype (e.g., transitive, acyclic directed graph, inclusion, etc.) that accurately describes a dataset to be visualized. This subtype allows in turn selection of the particular forms of diagrammatic representation (e.g., trees, nested boxes, directed arrows, etc.) that are expressively adequate, but not over-expressive, for that dataset. The theoretical basis of these methods is given in detail in Kamps (1997; 1998). They rest on a new application of Formal Concept Analysis (FCA) (Wille 1982). FCA is an applied mathematical discipline based on a formal notion of concepts and concept hierarchies and allowing the exploitation of mathematical reasoning for conceptual data analysis and processing. In particular, FCA permits the efficient construction of dependency lattices that effectively represent the functional and set-valued dependencies established among the domains of some data relation. Such dependency lattices can then motivate the differential selection of appropriate graphical presentations. FCA starts from the notion of a formal context (G,M,I) representing a dataset in which G is a set of objects, M is a set of attributes, and I establishes a binary re- lation between the two sets. I(g, m) is read object g has property m where g c G and m E M. Such a context is called a one-valued context. For illustration, we draw on the domain of the DArtbio system that we discuss below: an example of a one-valued context corresponding to the attribute Profession for a set of artists is shown in the table to the left of Figure 1. Concepts in FCA are defined in accordance with the  411  Computational Linguistics  Volume 27, Number 3  Gropius Breuer A. Albers J. Albers Moholy-Nagy Hilberseimer  Architect X X X  Designer X  Urban Planner X X X X  ({Groplus, Breuer, A. Albers,J.Albers, Moholy-Nagy,Hllberselmer}, 0)  {Gropius, Breuer,  ( Hilberseimer} /  ~  [Architect] )  .AI  ({A. Albers},  ({Groplus, K Breuer}, - ~ [Architect, Urban Planner] )  / / ~.~ (0, [Architect, Designer, Urban Planner] )  Figure 1 Example of a one-valued context and its corresponding lattice.  traditional theory of concepts, and consist of an extension and an intension. The extension is a subset A of the set of objects G and the intension is a subset B of the set of attributes M. We call the pair (A,B) a formal concept if each object of the extension has all the properties of the intension. Thus, for the data shown in Figure 1, the pair ({Gropius, Breuer}, {Urban Planner, Architect}) represents a formal concept: each of the members of the extension possesses all the attributes mentioned in the intension. The set of all concepts for some formal context can be computed effectively using the Next Closure algorithm developed by Ganter and Wille (1996). The main theorem of concept analysis then shows that the set of concepts for a formal context can be organized into a complete lattice structure under the following definition of the subconcept relation: a concept (A,B) is a subconcept of (A*,B*) if and only if A c A* ~ B* _c B (Wille 1982). The concept lattice may be constructed by starting from the top concept (the one that has no superconcepts) and proceeding top-down recursively. In each step we compute the set of direct subconcepts and link them to the respective superconcept until we reach the greatest lower bound of the lattice itself (the existence of which is always guaranteed for finite-input data structures). An efficient implementation of this algorithm is given in Kamps (1997). The lattice corresponding to our example one-valued context is given to the right of Figure 1. This lattice shows the full labeling of formal concepts in order to ease comparison with the originating table. Much of this information is redundant, however, and so we generally use variations on the abbreviated, more concise, form shown in Figure 2. Such lattices naturally capture similarities and differences between the values of the specified attributes of objects: each concept of the lattice indicates objects with some set of values in common. Moreover, the generalizations are organized by subsumption, which supports the selection of most-specific subtypes. When considering datasets in general, we typically need to express more information than that of single attributes and for this we require multi-valued contexts. An example of a multi-valued context is shown in Table 1, which includes our previous one-valued context as one of its columns; for ease of discussion, however, we will for the present restrict the Profession attribute so that each artist has only one profession. The table shows the subject areas/professions, institutions, and time periods in which the indicated artists were active. Formally, a multivalued context is a generalisation of a one-valued context and may be represented as a quadruple (G,M,W,I) where G, M, and I are as before, and W represents the set of values of the attributes-- 412  Bateman, Kamps, Kleinz and Reichenberger • Architect Hilberseimer  Constructive Page Generation __ Designer  Breuer ~ Figure 2 Concept lattice example, more succinctly labeled. Here, the extension label for each node consists of just those elements which are added at that node moving up the lattice; conversely the members of the intensions are shown moving down the lattice, again adding just those elements that are new for that node. For example, the node labeled simply Gropius, Breuer corresponds to the full form ({Gropius, Breuer}, {Architect, Urban Planner}) since both Gropius and Breuer are newly added to the extension at that node, while no new elements are added to the intension--'Architect' and 'Urban Planner' are both inherited from above.  Table 1 A collection of facts concerning artists and their professions drawn from the frame-based domain model used for the Dictionary of Art: biographies and re-expressed as a table of facts and attributes. (The facts are for illustrative purposes only and should not be taken as reliable statements of art history!)  Person  Profession  School  Workperiod  gl Gropius  Architect  Harvard  g2 Breuer  Architect  Harvard  g3 A. Albers  Designer  Black Mountain College  g4 J. Albers  Urban Planner  Black Mountain College  g5 Moholy-Nagy Urban Planner  New Bauhaus  g6 Hilberseimer  Architect Illinois Institute of Technology  1937-1951 1937-1946 1933-1949 1933-1949 1937-1938 1938-1967  which are, in contrast to the one-valued-context case, not trivially either true or false, applicable or not. To identify the value w c W of attribute m E M for an object g E G, we a d o p t the notation rn(g) = w and read this as attribute m of object g has value w. Kamps (1997) renders multivalued contexts amenable to the techniques for dependency-lattice construction by deriving a one-valued context that captures the functional dependencies of the original multivalued context. To see how this works, we first note that a functional dependency in a relation table is established when the following implication is always true: for two arbitrary objects g, h E G and two domain sets D,D* E M, then D(g) = D(h) ~ D*(g) = D*(h). This implication suggests the following construction for an appropriate one-valued dependency context: for the set of objects take the set of subsets of two elements of the given multi-valued context P2(G); for the set of attributes take the set of domains M; and for the connecting incidence relation take IN( {g,h}, m) :~=~m(g) = m(h). The required d e p e n d e n c y context is then represented b y the triple (P2(G),M, IN). This is illustrated in the table to the left of Figure 3, which shows the one-valued context corresponding to the multivalued context of Table 1. An entry here indicates that the identified attribute has the same 413  Computational Linguistics  Volume 27, Number 3  Person Profession School Workperiod ~  glg2  X X  Scho  glg6  X  gag6 g3g4  x  ]_ Period  X  X  m(g3)=m(g4~)' ~ L  g4g5  X  Figure3 Example dependency context and corresponding lattice.  Pmro(gfel)s=smio(ng6) mm((gg42)=)=mm(g(Sg)6) [ m(gl):m(g2) _ t'erson  value for both the facts identified in the object labels of the leftmost column: for example, gl and g2 share the values of their Profession and School attributes. This provides a wholistic view of the dependency structure of the original data and is, moreover, computationally simple to achieve. It is then straightforward to construct a dependency lattice as described above; this is shown to the right of Figure 3. The arcs in this lattice now represent the functional dependencies between the involved domains, and the equalities (e.g., m(gl)=m(g2)) represent the redundancies present in the data. For example, the lower left node labeled Period indicates not only that the third- and fourth-row entries under Period (g3 and g4) are identical but also, following the upward arc, that these entries are equal with respect to School; similarly, following upward arcs, the middle node (m(gl)=m(g2)) indicates that the first- and second-row entries (e.g., gl and g2) are equal with respect to both School and Profession. The lattice as a whole indicates that there are functional relationships from the set of persons into the set of professions, the set of periods, and the set of schools. A further functional relationship exists from the set of periods into the set of schools. Once such a lattice has been constructed, we also have as a consequence a set of classifications of the original relational input, or dataset. This can directly drive visualization as follows. For graphics generation, it is important that all domains of the relation become graphically encoded: this means the encoding is complete. Kamps (1997) proposes a corresponding graphical encoding algorithm that starts encoding the bottom domain and walks up the lattice employing a bottom-up/left-to-right strategy for encoding the upper domains. The idea of this model, much abbreviated, is that the cardinality of the bottom domain is the largest, whereas the domains further up in the lattice contain fewer elements. Thus, the bottom domain is graphically encoded using so-called graphical elements (rectangle, circle, line, etc.), whereas the upper domains are encoded using graphical attributes (color, width, radius) and set-valued attributes that must be attached to graphical elements. In general, it is preferable to maximize graphical attributes over set-valued attributes as this keeps graphical complexity moderate. Figure 4 shows two example diagrams that are produced from the dataset of Table 1 via the dependency lattice shown to the right of Figure 3. Informally, from the lattice we can see directly that artists (Person) can be classified on the one hand according to work period (following the lefthand arc upwards) and, on the other hand, jointly according to school and profession (following the vertical arc). The algorithm first allocates the attribute Person, indicated in the lowest node of the lattice, to the basic graphical element rectangle; the individual identities of the set members are given by a graphical attachment: a string giving the artist's name. The functional relationship between the set of persons and the set of time periods is then represented by the further 414  Bateman, Kamps, Kleinz and Reichenberger  Constructive Page Generation  ', [  [ a.Albers i  '_ ........ ......................  . . . . . . . . . . . . . . . . . . . . .~_~_..p.q~___~  E l Uan'ard [Z3 ~,c  [- .  I J.Aibe~  lib ,rr  [ ] Mohoiy-Nagy  .. .. .. .. .. .. .. .. .. .. . . . . . . . . . ...................... . . . . . . . . . . . . . . . . . . . . . . . . . . . . :. . . .  [  J Breuer  arct~mcts  I  I Gropius  Hiiberseir~r .....................................................  (a)  J.Albers i [ ] ~ F . ~ ,. ~ _ , .~o,y~.g.L. _:  193Q 1940 Z950 I ~  19/0  (b)  Figure 4 Example diagrams generated for the example data. Alternatives are produced by two distinct traversals of the aggregation lattice.  graphical attribute of the length of the rectangle. This is motivated by the equivalence of the properties of temporal intervals in the data and the properties of the graphical relationship of spatial intervals on the page. Two paths are then open: following the functional relationship first to either a set of schools or to a set of professions. Diagram (a) in Figure 4 adopts the first path and encodes the school relationship by means of the further graphical attribute of the colorof the rectangle, followed by a nesting rectangle for the relationship to Professions; diagram (b) illustrates the second path, in which the selection of graphical encodings is reversed. Both the selection of color and of nesting rectangles are again motivated by the correspondence between the formal properties of the graphical relations and those of the dependencies observed in the data. Reinstating the multiple professions of Gropius and Breuer mentioned in Figure 1 gives rise to a rather different dependency lattice in which the second solution is no longer possible. All of these mechanisms were implemented and used extensively for visualization in the context of an Editor's Workbench for supporting editorial decisions during the design of large-scale publications such as encyclopedias (Rostek, M6hr, and Fischer 1994; Kamps et al. 1996).1 2.2 The Partial Equivalence of Diagram Design and Text Design A selection of particular graphical elements entails the expression of particular functional dependencies. This is similar to decisions that need to be made when generating text. For instance, the equality rn(gl) = re(g2) in the lattice of Figure 3 above can also motivate a particular grouping of information in a corresponding linguistic presentation. That is, whereas graphically the equality motivates an association of both Gropius and Breuer with the graphical attributes allocated to Professions and Schools, textually we may connect both artists in a single sentence: i.e., gl (concerning Gropius) and g2 (concerning Breuer) can be compactly expressed by collapsing their (identical) school and profession attributes: Both Gropius and Breuer were architects and taught at Harvard. A similar phenomenon holds for the grouping re(g3) = rn(g4); here, g3 (concerning A. Albers) and g4 (concerning J. Albers) may be succinctly expressed by collapsing their identical period and school attributes. 
It is interesting to read Ladd's paper right after Pierrehumbert's. Ladd plays devil's advocate and raises three questions directly addressing the issue of intonation representation in Pierrehumbert's framework: 1. What is a tone? There is a level of abstraction in Pierrehumbert's tonal inventory involving mismatches in the assignment of H and L on the one hand, and turning points in surface pitch contours on the other hand. Should one take the turning points in surface intonation contour more seriously? 2. What is the meaning of the starred tone, as in the "*" of H*+L or L*+H? Ladd suggests that the starred tone should be the landing site of emphasis, and he expects H* to go higher and L* to go lower under emphasis. Under this assumption, he raises the issue of Bruce's (1987) re-analysis of his Swedish data, which assigned H+L* to accent I and H*+L to accent II. Fant and Krukenberg (1994) reported that, under focus, the pitch of the low tone in accent II (H*+L) is lower, while the pitch of all targets in accent I (H+L*) are raised. 3. Is the tune-text association convention as mediated by the star really meaningful? Ladd raises the question of Greek L+H tone, where L is aligned right before the stressed syllable and H is aligned right after the stressed syllable. Ladd's concern is that, in this case, nothing is actually aligned with the stressed syllable. It is also implied that in languages such as English or Greek, the tune-text association may not be as rigid as in a tone language such as Yoruba (see below). Ladd's questions reiterate the theme raised by Pierrehumbert on the difficulty of separating phonetic variation from phonological representations. In a sense, the Greek L+H case remains true to the autosegmental spirit of tune-text association: the star of the text provides an anchor, and alignment of the tune is made with reference to this anchor. As long as the facts are clear, an alignment model can be built successfully, referring to the whole contour or to any number of points along the contour. The 451  Computational Linguistics  Volume 27, Number 3  alignment model can be quite complicated (see van Santen and M6bius [2000]) and the predicted tone landing site can be quite far from the anchor. In addition, it turns out that the tune-text misalignment in a tone language can be substantially more dramatic than the Greek case described by Ladd. In Mandarin Chinese (Shih and Kochanski 2000; Xu 2001), it is not uncommon to have a tone target shifted completely off the syllable it originates from. In Yoruba (Laniran 1992), a target can be delayed by several syllables. In a tone language, both the tonal inventory and the ideal, phonological tune-text alignment are known. So when the tune and text alignment are off by a few syllables, one has no choice but to acknowledge the misalignment and to zero in on an alignment model predicting the alignment pattern. When dealing with a non-tone language, it is not as easy to ascertain a case of longdistance misalignment and one often ends up with a phonological analysis that is closer to surface observables. The paper of Hirst, Di Cristo, and Espesser is an overview of their intonation model, providing a contrast to the ToBI-based articles in the book. Theirs is an automated intonation model used for speech analysis and synthesis. The MOMEL algorithm analyzes f0 curves, smoothing out some of the micro-prosody and finding target points in the f0 contours. The target points are converted to the INTSINT transcription system. Basically, INTSINT defines the highest and lowest targets in the utterance as H (high) and B (bottom), respectively. Other phrase-initial targets are labeled as M (mid). The rest of the targets in the utterance are assigned labels reflecting relative relations with preceding and following targets: L (low), U (up), D (down), S (same). It should be apparent from this description that the analysis part of the system is language-independent: it depends on speech signals rather than on language knowledge. For the purpose of synthesis, some level of language knowledge is needed to write or to train a language-dependent intonation grammar. The fO contours will then be generated from the targets predicted by the grammar. Terken and Hermes provide a comprehensive review of the literature on prominence perception; readers get a clear sense of what questions should be asked and have been asked, even though the answers may not always be clear. One central issue addressed in this article is the question of what makes two accents sound equally prominent. Interesting experiments have been done comparing different pitch accents, as well as pitch accents in different pitch registers (e.g., male vs. female), in different positions of a sentence, and with or without a declining baseline. Technical questions revolve around what the correct thing is to measure (pitch excursion, or pitch-level difference), and in which scale (Hz, BART, Mel, or ERB). The data support a general declination model, in which early accents are bigger and higher than later, equally prominent accents. The tilt of the baseline has an effect on the perception of prominence. Among different pitch accents with equal excursion size, falling pitch accents lend more prominence than rising or rise-fall accents. Gussenhoven's contribution presents new work on a lexical tone contrast in the Roermond dialect of Dutch. Roermond Dutch, like other Limburg dialects, has a lexical accentual system reminiscent of that of Swedish. There are two types of accent. Words with Accent I have no lexically prespecified tone. Accent II words, following Gussenhoven's analysis, have a high (H) tone linked to the second mora of the accented syllable: note, therefore, that Accent II can only occur in words where the syllable that would bear the accent is bimoraic. Accent II displays a number of interesting features. First, if no boundary tone and no pitch accent is associated with the accented syllable, the H does not surface. Second, if a low (L) pitch accent is associated with the syllable, the H transmutes to a L so that you get a sequence L*L. Third, and most interesting, a boundary tone--Li or HiLi--apparently shows up before the Accent II high. So an  452  Book Reviews "underlying" sequence such as H*HLi--where H* is an intonationally assigned pitch accent, H is the Accent II tone, and Li is the boundary tone--surfaces as an HLH tone sequence rather than the expected HHL. As Gussenhoven notes, this is the first documented instance of a boundary tone being anything other than peripheral, and it has somewhat the flavor of morphological infixation. Gussenhoven argues that a derivational account--what he terms an "SPE" (Choresky and Halle 1968) account--would lead to an ordering paradox between two rules, both of which one would seemingly need: the rule that transmutes L*H to L'L, and the metathesis rule that reorders the boundary tone before an Accent II H. Note that in this derivational account, Gussenhoven assumes that the metathesis rule is a transformational rule that refers to the tone sequence and the boundary, and nothing else. Having rejected such an account, Gussenhoven presents a constraint-based analysis within the Optimality Theory (OT) framework, making use of about ten ranked constraints. Central to the boundary tone reordering is the assumption of two constraints, one (ALIGNTiRT) that states that the boundary tone wants to align to the right of its phrasal domain, and the other (ALIGNLEXRT) which states that the Accent II H wants to align to the right of its syllable, which of course coincides with the right of the phrasal domain if that syllable is final. The ranking ALIGNLEXRT ~ ALIGNTiRT achieves the desired result that the Accent II H comes after the boundary tone. Though Gussenhoven presents what seems to be a particularly compelling argument for OT, the Roermond data are actually grist for any number of theoretical mills. For example, taking Gussenhoven's proposal for a lexical H tone at face value, one could explore the possibility of a more traditional autosegmental analysis: the straw man derivational analysis that Gussenhoven presents is hardly fair to the quarter of a century of phonology between SPE and the advent of OT. Then there is the possibility of not taking Gussenhoven's analysis at face value. Indeed, two properties of Accent II--the transmutation of the H to L after L*, and the complete loss of H in accent-free non-boundary contrasts--suggests the possibility that Accent II may not involve a lexically specified H at all, but rather merely a different timing specification for whatever accent (if any) gets associated with the syllable. Such an approach would not be without complications, but it seems nonetheless worth exploring. Beckman and Cohen's is the second of the articles in this book to report new data. This article is a follow up of earlier work by Beckman and Edwards (1994) on the differences in the jaw-opening movements of two types of lengthening: lengthening for accent, and phrase-final lengthening. The data consist of the syllable pop, stressed/accented, stressed/unaccented, and unstressed. These stimuli are embedded in phrase-final as well as non-final positions. Beckman and Cohen consider three articulatory models to account for the jaw-tracing differences between full and reduced vowels: a truncation model, a rescaling model, and a hybrid model. The preliminary analysis suggests that the hybrid model works best. This study supports earlier findings that not all lengthenings are accomplished in the same way. Contrasting with an unstressed syllable, a stressed syllable is longer, and has a more extreme displacement of the jaw, but with higher velocity in the movement; this is not accounted for by the truncation model, which predicts the velocity of the movement to be the same. In phrase-final position, lengthening is accompanied by slower movement. Shattuck-Hufnagel's paper presents three types of arguments, based on stress shift, glottalization, and rhythmic pattern in speech--with data obtained from a speech corpus--to support the prosodic planning hypothesis, namely that speech production is planned with reference to prosodic structure. She suggests that the so-called stressshift rule is not really a rule that shifts stress to avoid stress clash. The main argument is that the "stress shift" effect may be achieved by the addition of an accent without 453  Computational Linguistics  Volume 27, Number 3  shifting the original stress; it may also occur when there is no stress clash. There is a tendency for speakers to use a pair of accents to frame a prosodic phrase. The first landing site is the earliest full vowel in the phrase, and the last one is the nuclear accent. The stress-shift rule can be subsumed under this mechanism. Glottalization of a vowel-initial word such as apple is more common in phrase-initial position. Also, phrase-final position is frequently marked by glottalization. The use of accents and glottalization both have the effect of framing a prosodic phrase. Further evidence for prosodic structure comes from the preference for the use of alternating stress. Although sentences in natural speech often do not show strictly alternating stress, comparisons of sentences with and without rhythmic patterns show that sentences with rhythmic patterns are easier to produce and less prone to speech errors. Selkirk presents an interesting analysis of English phonological phrasing in terms of OT. She starts by reviewing evidence from Bantu languages for the universality of two constraints, namely ALIGNR XP, which states that "the right edge of any XP (maximal projection) in syntactic structure must be aligned with the right edge of a MaP (major phrase) in prosodic structure", and a constraint proposed by Truckenbrodt (1995), WRAP XP, which states that "the elements of an input morphosyntactic constituent of type XP must be contained within a prosodic constituent of type MaP in output representation". Since the first constraint requires the right edges of MaPs to align with the right edge of each XP, whereas the second requires all XPs to be contained within a (single) MaP, the two constraints are inherently in conflict. In English, it seems that there is no evidence for ranking between ALIGNR XP and WRAP XP. This is because a sentence such as She 16anedher r6llerblades to R6bin--where, crucially, each of the words loaned, rollerblades and Robin are accented--can be phrased as either (She 16aned her r6llerbladeS)Ma P (to R6bin)Ma P, with two MaPs, or (She 16aned her r6llerblades to R6bin)Ma p, with one. The first case violates WRAP XP, the second violates ALIGNR XP, but both violations seem to be equal. What is not accounted for is the failure of an "overphrased" version, namely (She 16aned)MaP (her r61lerbladeS)MaP (to R6bin)Ma P, which is a violation of WRAP XP, but not ALIGNR XP. This motivates the introduction of a third, lower-ranked constraint BINMAP, which requires MaPs with just two accentual phrases; the overphrased version then has three violations of this constraint. Selkirk then turns to ALIGNR FOCUS, which requires the alignment of a MaP with a focused constituent, and she argues that it is higher ranked than the other constraints. Thus a focused version of She ldaned her rdllerblades to Robin has one optimal candidate: (She ldaned)Ma P (her r6llerbladeS)Ma P (to R6bin)MaP. Here, the boundary after loaned is favored by ALIGNR FOCUS, and the boundary after rollerblades is favored by ALIGNR XP. Selkirk's data are based on intuitive judgments concerning the putative presence or absence of a MaP boundary tone, the diagnostic she adopts (following Beckman and Pierrehumbert [1986]) for deciding whether a phrase boundary is present. It would be interesting to see to what extent these data hold up under experimental conditions. Ostendorf presents a brief review of linguistic and engineering issues related to the automatic detection of prosodic boundaries. As she points out, automatic boundary detection is desirable in a number of areas of speech technology. For example, in speech recognition and understanding, prosodic information can, in principle, be used to prune the search space (since some hypotheses are likely to be incompatible with a given phrasing) and to score linguistic hypotheses. Automatic detection is also desirable in text-to-speech synthesis to aid in the rapid development of prosodically labeled databases. A holy grail of this enterprise, as Ostendorf notes, is an approach that is robust enough to work on spontaneous speech: current automatic phrase de-  454  Book Reviews  tection methods work well only on read speech and perform considerably less well on the kind of conversational speech found in the Switchboard corpus. Campbell presents an overview of work on duration modeling, starting with a rather detailed account of Klatt's (1973) rule-based model, and covering various statistical approaches such as Riley's (1990) CART-based models and van Santen's (1994) sum-of-products models. Campbell's own view is that segment-based models of duration are misguided because they are based on the notion of a segment's "inherent duration," and that instead one should model higher levels of prosodic structure (syllables, feet, or even prosodic phrases), deriving segmental durations once the higher-level durations are set up. The second half of the article describes earlier work of Campbell that provides support for a syllable-based approach from English and Japanese. 1 Hirschberg concludes the book with a review of earlier work on prosodic cues that differentiate speaking style--or, more properly, two particular speaking styles, namely read speech and spontaneous speech. She catalogs differences in rate (read speech is faster), differences in the distribution of different boundary tones, and differences in the rates of disfluencies, as well as a few other factors. Disfluencies, while more common in spontaneous speech, as one might expect, are nonetheless sufficiently rare in both styles that they are not a particularly useful cue to distinguishing the two. Finally, a word on production quality, which unfortunately is mediocre. There are a variety of problems, particularly with the presentation of some of the figures and the equations. So, the shaded areas of the tableaux in Selkirk's paper are too dark, though the ones in Gussenhoven's paper are fine. In at least a couple of the papers-Gussenhoven, Campbell--there are some quite annoying changes in font size between successive linguistic examples or equations. On the whole the production quality is not what you would expect for a volume that lists at over US$150. But there is presumably nothing to do here but lament the fact that as academic publishers continue to up the prices of their wares, they also seem to be taking less and less care in their production.  References Beckman, Mary and Jan Edwards. 1994. Articulatory evidence for differentiating stress categories. In P. A. Keating, editor, Phonological Structure and Phonetic Form: Papers in Laboratory Phonology II1. Cambridge University Press, Cambridge, pages 7-33. Beckman, Mary and Janet Pierrehumbert. 1986. Intonational structure in English and Japanese. Phonology, 3:255-309. Bruce, GOsta. 1987. How floating is focal accent? In K. Gregersen and H. Basboll, editors, Nordic ProsodyIV. Odense University Press, Odense, pages 41-49. Chomsky, Noam and Morris Halle. 1968. The Sound Pattern of English. Harper and Row, New York, NY. Fant, Gunnar and A. Krukenberg. 1994. Notes on stress and word accent in Swedish. SpeechTransmission Laboratory Quarterly Status Report. Technical report, Department of Speech  Communication and Music Acoustics, KTH, Stockholm. Klatt, Dennis. 1973. Interaction between two factors that influence vowel duration. Journal of the Acoustical Society of America, 54:1102-1104. Laniran, Yettmde. 1992. Intonation in Tone Languages: The PhoneticImplementation of Tones in Yoruba. Ph.D. thesis, Cornell University, Ithaca, NY. Riley, M. D. 1990. Tree-based modeling for speech synthesis. In Proceedingsof the ESCA Workshopon SpeechSynthesis, 229-232, European Speech Communication Association. Shih, Chilin and Greg Kochanski. 2000. Chinese tone modeling with Stem-ML. In International Conferenceon Spoken Language Processing, 2000, article 1232, Beijing. Truckenbrodt, H. 1995. PhonologicalPhrases: Their Relation to Syntax, Prominenceand Focus. Ph.D. thesis, Massachusetts Institute of Technology, Cambridge, MA.  
Volume 27, Number 3  Mark-Jan Nederhof's "Regular approximation of CFLs: A grammatical view" is similar to Eisner's contribution in that its focus is primarily mathematical. It describes an attractive approach to finite-state approximation of regular grammars. The essential idea is to characterize properties that make grammars non-regular, and to develop schemes for systematically removing such properties. This helps to keep the approximation process perspicuous. Experimental work with this approximation scheme is absent from the current article, but is reported elsewhere (Nederhof 2000). In "Probabilistic GLR parsing," Kentaro Inui, Virach Sornlertlamvanich, Hozumi Tanaka, and Takenobu Tokunaga provide a careful analysis of the process of LR parsing. This leads to a probabilistic parsing scheme having the desirable property, not previously achieved for LR parsers, that the sum over all parses of the probability is unity. Once again experimental work is not present here but is reported elsewhere (Sornlertlamvanich, Inui, Tokunaga, Tanaku, and Takezawa 1999). Eisner's paper does not report experiments either, but addresses a problem with profound practical significance. It analyses the computational properties of grammars in which potentially idiosyncratic word-to-word relationships play a key role. The framework used is general enough to capture the essence of many recent statistical parsers and clean enough to make it easy (and interesting) to compare one with another. I like Eisner's paper for the insight it provides into the options available to the lexically minded probabilistic modeler. This aspect is also present in "Encoding frequency information in lexicalized grammars," where John Carroll and David Weir, using lexicalized tree adjoining grammar (LTAG) as an example, analyze the problem of providing practically useful estimates of the large number of parameters that are potentially present in lexicalized grammars. Similarly, in "Towards a reduced commitment, D-theory style TAG parser," John Chen and K. Vijay-Shanker describe an approach to TAG parsing whose goal is to delay attachment decisions. This is a design sketch, not an implemented parser, but the design is well fleshed out, and looks worth testing. Several articles do have extensive evaluation data. Joshua Goodman contributes "Probabilistic feature grammars," developing an implemented and efficient stochastic feature-based grammar formalism. The key idea, prefigured in, for example, Stolcke's (1994) doctoral dissertation, is to choose a feature formalism that does not impede dynamic programming implementations of the usual inside, outside, and Viterbi probability calculations. Goodman includes extensive quantitative evaluation, which is greatly to be welcomed. "A new parsing method using a global association table" by Juntae Yoon, Seonho Kim, and Mansuk Song, is a description and evaluation of a semi-deterministic parsing algorithm designed to exploit the fact that Korean is an SOV language with many surface cues to syntactic dependency. Extensive evaluation is provided. Bangalore Srinavas's "Performance evaluation of SuperTagging for partial parsing" exploits the author's SuperTagging idea (i.e., employing part-ofspeech-tagger technology to "almost parse," using the elementary trees of lexicalized tree adjoining grammar) for the now-standard task of partial parsing. Given the title, the plethora of interesting performance figures is to be expected. For example, connecting to the discussion of the Penn Treebank above, Bangalore reports that 35% of the sentences tested have no dependency-link errors, while 89.8% have three errors or less. Two papers give evaluations that are based on the measurement of run-time behavior. In "Parsing by successive approximation," Helmut Schmid describes an efficient parsing technology that is nonetheless able to process grammars that make significant use of features. The efficiency of this algorithm is demonstrated by appeal to a range of empirical performance statistics. Udo Hahn, Norbert BrOker, and Peter Neuhaus  460  Book Reviews  take a similar approach to evaluation. Their contribution describes "Message-passing protocols for object-oriented parsing," and shows how to derive different heuristically guided parsing algorithms from variations in the communication patterns in an object-oriented parser. They report a variety of performance statistics for a set of 41 challenging-looking sentences from German computer magazines. Since a version of the material of the book has already been presented at a workshop with proceedings (Bunt and Nijholt 1997), it is relevant to ask what has been gained (or lost) in the transition to (an expensive) book form. The articles average 20 pages--longer than the original conference presentation--and several authors have made good use of the opportunity to update and revise their work. The editors have selected an interesting group of papers, and provide a clear introduction with useful summaries of the chapters, pointing out some interesting relationships between the different lines of research. 1 On the other hand, despite the high price of the book, there is no evidence that a competent professional copy editor was involved in the process of publication. This is a shame, since several of the contributions (especially Hektoen's) deserve to be more widely known.  References Bunt, Harry and Anton Nijholt. 1997. Proceedings of the Fifth International Workshop on Parsing Technologies. Massachusetts Institute of Technology, Boston, MA. Carroll, John, Ted Briscoe, and Antonio Sanfilippo. 1998. Parser evaluation: a survey and a new proposal. In Proceedings of the First International Conferenceon Language Resources and Evaluation, pages 447-454, Granada, Spain. Carroll, John, Guido Minnen, and Ted Briscoe. 1999. Corpus annotation for parser evaluation. In Proceedingsof the EACL-99 Post-Conference Workshop on Linguistically Interpreted Corpora (LINC-99), pages 35--41, Bergen, Norway. Grishman, Ralph, Catherine Macleod,  and J. Sterling. 1992. Evaluating parsing strategies using standardized parse files. In Proceedingsof the Third Conferenceon Applied Natural Language Processing, pages 156-161, Trento, Italy. Nederhof, Mark-Jan. 2000. Practical experiments with regular approximation of context-free languages. Computational Linguistics, 26(1): 17-44, March. Sornlertlamvanich, Virach, Kentaro Inui, Takenobu Tokunaga, Hozumi Tanaka, and Toshiyuki Takezawa. 1999. Empirical support for new probabilistic generalized LR parsing. Journal of Natural Language Processing, 6(2): 3-22. Stolcke, Andreas. 1994. Bayesian Learning of Probabilistic Language Models. Ph.D. thesis, University of California at Berkeley.  Chris Brew is an assistant professor of computational linguistics and language technology at the Ohio State University. His recent research has concerned the use of corpus-based methods in psycholinguistics and in natural language generation. Brew's address is: Department of Linguistics, Oxley Hall, 1712 Neil Avenue, Columbus, OH 43210; e-maih cbrew@ling.ohio-state.edu.  
2. Derivational geometry Sproat provides very little in the way of an introduction to writing systems; after pointing the reader to reliable sources, he jumps boldly into his model of reading devices. The geometry of his model is traditional derivational phonology, involving a mapping through various levels from an underlying representation U to the surface phonology. Sproat makes two major claims: 1. Consistency: Every orthography corresponds to a single Orthographically Relevant Level (ORL) in the derivation; and . Regularity: MOaL~r, the mapping from the ORL to F, the spelling itself, is regular. In the current academic world, derivational approaches to phonology are out of fashion, overshadowed by Optimality Theory (Prince and Smolensky 1993) and other mono-stratal models. The Two-Level finite-state model of computational morphology (Koskenniemi 1983) also eschews derivation. Nevertheless, a great deal of computational phonology and morphology continues to be done with derivational cascades of rewrite rules, which Johnson (1972) and Kaplan and Kay (1994) showed to be only  464  Book Reviews finite-state in power. Computational linguists from this tradition have challenged OT (Karttunen 1998; Gerdemann and van Noord 2000), assuming that as long as linguists stay within finite-state power, they can construct their grammars flatly or derivationally as they find most convenient and perspicuous. The interesting contribution of Sproat to this ongoing debate is his argument that the derivation has explanatory power, modeling "orthographical depth." In this model, English orthography reflects a fairly deep ORL, Russian a shallower one, Belarusian a level slightly shallower than Russian, Spanish quite shallow, and so on. Sproat's claim that his derivations are regular is testable and potentially disprovable; he is forthright in discussing challenges and apparent counterexamples. The second claim, that an orthography always represents a consistent level of representation, and not sometimes one level and sometimes another, is a stronger claim, especially given the messy history of borrowing and adapting scripts. Scripts, as Sproat himself points out, are contingent "artifacts" or technologies, not something inherently human like phonology. He argues credibly, however, that a viable everyday writing system must bear a "sensible relationship" to the language it represents, that we can expect a natural pressure in the direction of consistency. 3. Derivational breakdown Sproat's MORL~r rules are divided into two main subgroups: ME,,code, which are (morpho)phonological mappings, and Mspell, which are "autonomous spelling rules" or rules reflecting the conventions of the orthography itself. As the rules are regular, they can simply be composed together, and the result can be encoded as a finite-state transducer and applied bidirectionally. MORL--*P ~ MEncode o MSpelI This distinction is easy to defend. Mspell covers phenomena such as the conventions for representing phonologically long vowels in Dutch orthography, and parallel examples in other orthographies are easy to find. Sproat then makes another, less-obvious, distinction, splitting up Mspell, so that aspellrnap is a mapping, encoded as a finite-state transducer, but MSpellconstrainis encoded as a regular-language filter. Again, the two subsystems are regular and are composed together. Mspell =MSpellma p o Id(Mspellco.st~.,~i,,) As composition is defined only for transducers, the composition must technically involve the identity relation on the filter as shown. The examples of MSpellco,,s,rani involve alternate representations that appear in com- plementary distribution in the surface orthography. In Malagasy, the vowel / i / is represented as either (i) or (y), with (i) occurring only in nonfinal position and (y) occurring only at the end of words. If MEncode contains the rule that is, vowel / i / is realized as either orthographical (i/ or as (Yl, then Mspellco,~str,~i. would include the following regular filter to constrain the variants to appear only in appropriate contexts. ~[(G*(i/ #) ] (G*(Y/ -7#)] 465  
* P.O. Box 9103, 6500 HD Nijmegen, The Netherlands. E-mail: hvh@let.kun.nl. t Universiteitsplein 1, 2610 Wilrijk, Belgium. E-mail: zavrel@textkerneLnl. :~Universiteitsplein 1, 2610 Wilrijk, Belgium. E-mail: daelem@uia.ua.ac.be. Q 2001 Association for Computational Linguistics  Computational Linguistics  Volume 27, Number 2  very little additional effort by exploiting the disagreement between different language models. Although the approach is applicable to any type of language model, we focus on the case of statistical disambiguators that are trained on annotated corpora. The examples of the task that are present in the corpus and its annotation are fed into a learning algorithm, which induces a model of the desired input-output mapping in the form of a classifier. We use a number of different learning algorithms simultaneously on the same training corpus. Each type of learning method brings its own "inductive bias" to the task and will produce a classifier with slightly different characteristics, so that different methods will tend to produce different errors. We investigate two ways of exploiting these differences. First, we make use of the gang effect. Simply by using more than one classifier, and voting between their outputs, we expect to eliminate the quirks, and hence errors, that are due to the bias of one particular learner. However, there is also a way to make better use of the differences: we can create an arbiter effect. We can train a second-level classifier to select its output on the basis of the patterns of co-occurrence of the outputs of the various classifiers. In this way, we not only counter the bias of each component, but actually exploit it in the identification of the correct output. This method even admits the possibility of correcting collective errors. The hypothesis is that both types of approaches can yield a more accurate model from the same training data than the most accurate component of the combination, and that given enough training data the arbiter type of method will be able to outperform the gang type.1 In the machine learning literature there has been much interest recently in the theoretical aspects of classifier combination, both of the gang effect type and of the arbiter type (see Section 2). In general, it has been shown that, when the errors are uncorrelated to a sufficient degree, the resulting combined classifier will often perform better than any of the individual systems. In this paper we wish to take a more empirical approach and examine whether these methods result in substantial accuracy improvements in a situation typical for statistical NLP, namely, learning morphosyntactic word class tagging (also known as part-of-speech or POS tagging) from an annotated corpus of several hundred thousand words. Morphosyntactic word class tagging entails the classification (tagging) of each token of a natural language text in terms of an element of a finite palette (tagset) of word class descriptors (tags). The reasons for this choice of task are several. First of all, tagging is a widely researched and well-understood task (see van Halteren [1999]). Second, current performance levels on this task still leave room for improvement: "state-of-the-art" performance for data-driven automatic word class taggers on the usual type of material (e.g., tagging English text with single tags from a low-detail tagset) is at 96-97% correctly tagged words, but accuracy levels for specific classes of ambiguous words are much lower. Finally, a number of rather different methods that automatically generate a fully functional tagging system from annotated text are available off-the-shelf. First experiments (van Halteren, Zavrel, and Daelemans 1998; Brill and Wu 1998) demonstrated the basic validity of the approach for tagging, with the error rate of the best combiner being 19.1% lower than that of the best individual tagger (van Halteren, Zavrel, and Daelemans 1998). However, these experiments were restricted to a single language, a single tagset and, more importantly, a limited amount of training data for the combiners. This led us to perform further, more extensive,  
This passage highlights three major roles an evaluation method can serve. First, the method may be used during the process of task definition, to assess interannotator agreement on proposed task specifications and revise them accordingly, and to subsequently validate the final answer keys. Second, the method may be used to drive * Department of Linguistics#0108, University of California,San Diego,9500Gilman Drive, La Jolla,CA 92093-0108. E-mail:kehler@ling.ucsd.edu j"ArtificialIntelligenceCenter,333 RavenswoodAvenue,Menlo Park, CA 94025. E-mail:bear@ai.sri.com. ~:ArtificialIntelligenceCenter,333 RavenswoodAvenue,Menlo Park, CA 94025. E-maih appelt@ai.sri.com. © 2001Associationfor Computational Linguistics  Computational Linguistics  Volume 27, Number 2  the system development process, as system developers and machine learning algorithms rely heavily on the feedback it provides to determine whether proposed system changes should be adopted. Third, the results may be used for final cross-system comparison, on which judgments concerning the adequacy of competing technologies are based. Given their pervasiveness in the entire technology development process, the need for adequate evaluation methods is of the essence, and has thus become a prominent topic for research in itself (Sparck-Jones and Galliers 1996; CSL Special Issue 1998, inter alia). The need to serve all these roles has necessitated the development of automated evaluation methods that are capable of being run repeatedly with little time and cost overhead. Automated methods are commonly used for application tasks, including speech recognition, information retrieval, and IE, as well as for natural language component technologies, including part-of-speech tagging, syntactic annotation, and coreference resolution. Such methods generally consist of a two-step process--first, systemgenerated responses are aligned with corresponding human-generated responses encoded in an answer key, and then a predetermined scoring procedure is applied to the aligned response pairs. The second of these tasks (scoring procedure) has been the focus of most previous research on evaluation. In this paper, we focus instead on the less well studied problem of alignment. The relative inattention to problems in alignment is no doubt a result of the fact that alignment is relatively unproblematic in many natural language evaluation scenarios. In evaluations of component technologies such as part-of-speech tagging and treebank-style syntactic annotation systems, for instance, a system-generated annotation is simply scored against the human-generated annotation for the same word or sentence, regardless of whether matching it to the annotation for a different word or sentence would improve the overall score assigned. Alignment is similarly trivial in evaluations of applications such as information retrieval, since the notion of document identity is well defined. Alignment in speech recognition evaluations can be a bit more complex, but constraints inherent in the methods nonetheless prohibit clear misalignments, such that a system cannot receive credit for recognizing a word from an acoustic signal that occurred several utterances later, for instance. As the field progresses to address higher-level interpretation tasks, however, the problem of determining alignments may become less straightforward. Such tasks may require that the output contain information that is synthesized (and perhaps even inferred) from disparate parts of the input signal, making the correspondence between information in the system output and information in the answer key more difficult to recover. A case in point is the evaluation IE technology, as most prominently carried out by the series of MUCs. For a typical MUC-style IE task, a text may contain several extractable events, and thus a method is required for aligning the (often only partially correct) event descriptions extracted by a system to the appropriate ones in the answer key. It is therefore important to investigate the issues involved in the definition of alignment criteria in such tasks, and we can use the MUC experience as a basis for such an investigation. In this paper, we focus specifically on the criterion used for alignment in MUC-6. In light of difficulties in identifying a perfect alignment criterion for the MUC-6 task, the MUC-6 community agreed upon on a rather weak and forgiving criterion, leaving the resolution of alignment ambiguities to a mechanism that sought to maximize the score assigned. While this decision may have been thought to be relatively benign, we report on the results of an extensive, post hoc analysis of a 13 1/2-month effort focused on the MUC-6 task which reveals several unforeseen and negative consequences associated with this decision, particularly with respect to its influence on the incremental system  232  Kehler, Bear, and Appelt  Accurate Alignment in Evaluation  development process. While we do not argue that these consequences are so severe that they call the integrity of the MUC-6 evaluation into question, they are substantial enough that they demonstrate the potential of such an alignment strategy to have a significantly adverse impact on the goals of an evaluation. It is therefore important that these lessons be brought to bear in the design of future evaluations for IE and other high-level language processing tasks. We begin with an overview of IE tasks, systems, and evaluation, including the alignment procedure used for MUC-6. We then provide an example from the MUC-6 development corpus that illustrates properties of the alignment process that interfere with the stated goals of the evaluation. We assess the pervasiveness of these problems based on data compiled from an extended development effort centered on the MUC-6 task, and conclude that the effect of the alignment criterion is robust enough that it could potentially undermine the technology development process. We conclude that these results argue strongly for the use of strict and accurate alignment criteria in future natural language evaluations---evaluations in which alignment problems will become exacerbated as the natural language applications addressed become more complex-and for maintaining the independence of alignment criteria and the mechanisms used to calculate scores. 2. Information Extraction, MUC, and the F-score Metric IE systems process streams of natural language input and produce representations of the information relevant to a particular task, typically in the form of database templates. In accordance with the aforementioned array of roles served by evaluation methods, the MUCs have been very influential, being the primary driving force behind IE research in the past decade:  The MUCs have helped to define a program of research and development .... The MUCs are notable ... in that they have substantially shaped the research program in information extraction and brought it to its current state. (Grishman and Sundheim 1995, 1-2) There have been seven MUCs, starting with MUC-1 in 1987, and ending with MUC-7 in 1997. The metrics used--precision, recall, and F-score--are probably the most exhaustively used metrics for any natural language understanding application; precision and recall have been in use since MUC-2 in 1989 (Grishman and Sundheim 1995), and F-score since MUC-4 in 1992 (Hirschman 1998). IE evaluation has thus been extensively thought out, revised, and experimented with:  For the natural language processing community in the United States, the pre-eminent evaluation activity has been the series of Message Understanding Conferences (MUCs).... the MUC conferences provide us with over a decade of experience in evaluating language understanding. (Hirschman 1998, 282) The MUCs therefore provide a rich and established basis for the study of the effects of evaluation with respect to its roles noted above. As previously indicated, we will focus in this paper on MUC-6, held in 1995, and exclusively on the procedure used for aligning system-generated responses with those in an answer key.  233  Computational Linguistics  Volume 27, Number 2  (TEMPLATE) CONTENT (succession_event)  (SUCCESSION_EVENT} SUCCESSION_ORE (organization) POST string IN_AND_OUT (in_and_out}  (IN_AND_OUT) IO_PERSON (person} NEW_STATUS { IN, m_ACT~NG,  VACANCY_REASON { DEPART_WORKFORCE,  ON_THE_JOB  OUT~ OUT-ACTING } { YES, NO, UNCLEAR }  REASSIGNMENT, NEW _POST _CREATED I OTH_UNK }  OTHER_ORE (organization) REL_OTHER_ORG { SAME_ORE, RELATED-ORE t  (ORGANIZATION)  OUTSIDE_ORE }  ORE_NAME string ORE_ALIAS string ORG_DESCRIPTOR string ORG_TYPE { COMPANY~GOVERNMENT, OTHER } ORE_LOCALE string  {PERSON} PER_NAME string PER_ALIAS string PER_TITLE string  ORE_COUNTRY string  Figure 1 Output template for MUC-6.  Marketing & Media: Star TV Chief Steps Down After News Corp. Takeover In the wake of a takeover by News Corp., the chief executive officer of Star TV resigned after less than six months in that post, industry executives said. Last week, News Corp. bought 63.6% of the satellite broadcaster, which serves the entire Asian region, for $525 million in cash and stock from Hutchison Whampoa Ltd. and the family of Li Ka-shing. At the time of the purchase, News Corp. executives said they would like the executive, Julian Mounter, to stay. However, Star's chief balked at the prospect of not reporting directly to Rupert Murdoch, News Corp.'s chairman and chief executive officer, people close to Mr. Mounter said. Both Mr. Mounter and a Star spokesman declined to comment. It is likely that Star's new chief executive will report to either Sam Chisholm, chief executive of News Corp.'s U.K.-based satellite broadcaster, British Sky Broadcasting, or Chase Carey, chief operating officer of News Corp.'s Fox Inc. film and television unit. Mr. Mounter's departure is expected to be formally announced this week. Although there are no obvious successors, it is expected that Mr. Murdoch will choose someone from either British Sky Broadcasting or Fox to run Star, said a person close to News Corp. Figure 2 Example text from MUC-6 development set (9308040024).  2.1 Task Definition The MUC-6 task was, roughly speaking, to identify information in business news that describes executives moving in and out of high-level positions within companies (Grishman and Sundheim 1995). The template structure that MUC-6 systems populated is s h o w n in Figure 1. There are three types of values used to fill template slots. String fills, shown italicized, are simply strings taken from the text, such as CEO for the POST slot in a SUCCESSION_EVENT. Set fills are chosen from a fixed set of values, such as DEPART_WORKFORCEfor the VACANCY_REASON slot in a SUCCESSION_EVENT. Finally, Pointer fills, shown in angle brackets, hold the identifier of another template structure, e.g., the SUCCESSION_ORE slot in a SUCCESSION_EVENT will contain a pointer to an ORGANIZATION template. Figure 2 displays a text from the MUC-6 development corpus. When a participating system encounters this passage, it should extract the information that Julian 234  Kehler, Bear, and Appelt  Accurate Alignment in Evaluation  TEMPLATE-024-1)  (TEMPLATE-024-1)  cor  CONTENT  (SUCC_EVENT-024- i}  CONTENT (SUCC_EVENT-024-I}  SUCC_EVENT-024-1}  (SUCC_EVENT-024-1)  cor  SUCCESSION_ORG  (ORG-024-1}  SUCCESSION_ORG  (ORG-024-11  cor  POST "CEO"  POST "CEO"  cor  IN_AND_OUT  (IN_AND_OUT-024- i}  IN_AND_OUT  (IN_AND_OUT-024-1}  inc  VACANCY_REASON  REASSIGNMENT  VACANCY_REASON  OTH_UNK  ORG-024-1)  (ORG-024-1)  cor  ORG_NAME "STARTv"  ORG_NAME "STARTV"  mis  ORG_ALIAS "STAR"  mis  O R G _ D E S C R I P T O R "THE SAT BDSTR"  cor  ORG_TYPE COMPANY  ORG_TYPE COMPANY  spu  ORG_LOCALE WHAMPOA  spu  O R G _ C O U N T R Y UNITED KINGDOM  IN_AND_OUT-024-1}  cor  IO_PERSON (PERSON-024-1}  inc  NEW_STATUS OUT  inc  ON_THE_JOB UNCLEAR  (IN_AND_OUT-024-1} IO_PERSON (PERSON-024-1) NEW_STATUS IN ON_THE_JOB No  PERSON-024-1}  cor  P E R _ N A M E "JULIAN MOUNTER"  mis  PER_ALIAS "MouNTER"  mis  PER_TITLE "MR."  (PERSON-024-1) P E R _ N A M E "JULIAN MOUNTER"  Figure 3 Target and hypothetical outputs for the example text.  Mounter is "out" of the position of CEO of company Star TV, along with other information associated with the event. The correct results for this passage, as encoded in a human-annotated answer key, are shown in the middle column of Figure 3. 2.2 Evaluation: Alignment The rightmost column of Figure 3 shows hypothetical output of an IE system. The first step of the evaluation algorithm, alignment, determines which templates in the system's output correspond to which ones in the key. Generally speaking, there can be any number of templates in the key and system response for a given document; all, some, or no pairs of which may be descriptions of the same event. The alignment algorithm must thus determine the correct template pairing. This process is not necessarily straightforward, since there may be no slot in a template that uniquely identifies the event or object which it describes. In response to this problem, the MUC community decided to adopt a relatively lax alignment criterion, leaving it to the alignment algorithm to find the alignment that optimizes the resulting score. The procedure has two major steps. First, it determines which pairs of templates are possible candidates for alignment; the criterion for candidacy was only that the templates share a common value for at least one slot. (Pointer fills share a common value if they point to objects that are aligned by the algorithm.) This criterion often results in alignment ambiguities--a key template will often share a common slot value with several templates in the system's output, and vice versa-and thus a method for selecting among the alternative mappings is necessary. The candidate pairs are rank ordered by a mapping score, which simply counts the number of slot values the templates have in common. 1 The scoring algorithm then considers 
 each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar. A left-toright parser whose derivations are not rooted, i.e., with derivations that can consist of disconnected tree fragments, such as an LR or shift-reduce parser, cannot incrementally calculate the probability of each prefix string being generated by the probabilistic grammar, because their derivations include probability mass from unrooted structures. Only at the point when their derivations become rooted (at the end of the string) can generative string probabilities be calculated from the grammar. These parsers can calculate word probabilities based upon the parser state--as in Chelba and Jelinek (1998a)--but such a distribution is not generative from the probabilistic grammar. A parser that is not left to right, but which has rooted derivations, e.g., a headfirst parser, will be able to calculate generative joint probabilities for entire strings; however, it will not be able to calculate probabilities for each word conditioned on previously generated words, unless each derivation generates the words in the string in exactly the same order. For example, suppose that there are two possible verbs that could be the head of a sentence. For a head-first parser, some derivations will have the first verb as the head of the sentence, and the second verb will be generated after the first; hence the second verb's probability will be conditioned on the first verb. Other derivations will have the second verb as the head of the sentence, and the first verb's probability will be conditioned on the second verb. In such a scenario, there is no way to decompose the joint probability calculated from the set of derivations into the product of conditional probabilities using the chain rule. Of course, the joint probability can be used as a language model, but it cannot be interpolated on a word-by-word basis with, say, a trigram model, which we will demonstrate is a useful thing to do. Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis II 1970). A second key feature of our approach is that top-down guidance improves the efficiency of the search as more and more conditioning events are extracted from the derivation for use in the probabilistic model. Because the rooted partial derivation is fully connected, all of the conditioning information that might be extracted from the top-down left context has already been specified, and a conditional probability model built on this information will not impose any additional burden on the search. In contrast, an Earley or left-corner parser will underspecify certain connections between constituents in the left context, and if some of the underspecified information is used in the conditional probability model,- it will have to become specified. Of course, this can be done, but at the expense of search efficiency; the more that this is done, the less benefit there is from the underspecification. A top-down parser will, in contrast, derive an efficiency benefit from precisely the information that is underspecified in these other approaches. Thus, our top-down parser makes it very easy to condition the probabilistic grammar on an arbitrary number of values extracted from the rooted, fully specified derivation. This has lead us to a formulation of the conditional probability model in terms of values returned from tree-walking functions that themselves are contextually sensitive. The top-down guidance that is provided makes this approach quite efficient in practice. The following section will provide some background in probabilistic context-free grammars and language modeling for speech recognition. There will also be a brief review of previous work using syntactic information for language modeling, before we introduce our model in Section 4.  250  Roark  Top-Down Parsing  (a)  (b)  (c)  S ~  S t  S t  S  S  STOP  S  STOP  NP  VP  NP  VP (/s)  NP VP  Spot VBD  NP  Spot VBD  NP  Spot  chased DT NN I I the ball  chased DT NN f J the ball  Figure 1 Three parse trees: (a) a complete parse tree; (b) a complete parse tree with an explicit stop symbol; and (c) a partial parse tree.  2. Background 2.1 Grammars and Trees This section will introduce probabilistic (or stochastic) context-flee grammars (PCFGs), as well as such notions as complete and partial parse trees, which will be important in defining our language model later in the paper. 1 In addition, we will explain some simple grammar transformations that will be used. Finally, we will explain the notion of c-command, which will be used extensively later as well. PCFGs model the syntactic combinatorics of a language by extending conventional context-free grammars (CFGs). A CFG G = (V, T, P, St), consists of a set of nonterminal symbols V, a set of terminal symbols T, a start symbol St E V, and a set of rule productions P of the form: A ~ a, where a c (V U T)*. These context-free rules can be interpreted as saying that a nonterminal symbol A expands into one or more either nonterminal or terminal symbols, a = Xo... Xk} A sequence of context-free rule expansions can be represented in a tree, with parents expanding into one or more children below them in the tree. Each of the individual local expansions in the tree is a rule in the CFG. Nodes in the tree with no children are called leaves. A tree whose leaves consist entirely of terminal symbols is complete. Consider, for example, the parse tree s h o w n in (a) in Figure 1: the start s y m b o l is St, which e x p a n d s into an S. The S n o d e e x p a n d s into an N P followed b y a VP. These n o n t e r m i n a l n o d e s each in turn expand, and this process of expansion continues until the tree generates the terminal string, "Spot chased the ball", as leaves. A CFG G defines a language Lc, which is a subset of the set of strings of terminal symbols, including only those that are leaves of complete trees rooted at St, built with rules f r o m the g r a m m a r G. We will denote strings either as w or as WoW1 . . . wn, w h e r e wn is u n d e r s t o o d to be the last terminal s y m b o l in the string. For simplicity in displaying equations, f r o m this point f o r w a r d let w / b e the substring w i . . . wj. Let Twg 
* School of Computer Science, Telecommunications and Information Systems, Chicago, IL 60604. E-mail: tomuro@cs.depaul.edu t School of Computer Science, Telecommunications and Information Systems, Chicago, IL 60604. E-maih lytinen@cs.depaul.edu 
fundamental to AI systems, to object-oriented systems, and to every dictionary from the earliest days to the present" (p. 4). • Leibniz's "definition of modality in terms of possible worlds is still used today in the semantics for modal logic" (p. 7). • "Leibniz saw that accounting machines could also be used for mechanical reasoning--an insight that should entitle him to be called the grandfather of artificial intelligence" (p. 8). • "Modern systems of predicate calculus are based on the algebraic notation developed by C. S. Peirce in 1883" (p. 10). • "Peano began the practice of turning letters upside-down or backward to represent the logical symbols ... Bertrand Russell adopted this notation from Peano for the Principia Mathematica" (p. 11). Then he suggests that "readers who have not had a course in logic or who would like a quick review should read [Appendix A.1]" (p. 11). However, this suggestion is probably premature; reading Appendix A.1 is probably best left for after all of Chapter 1 has been read. The title of Section 1.2 is "Representing knowledge in logic" (pp. 11-18). This phrase and others like it, such as "Logic itself is a simple language" (p. 15) and "Logic is an ontologically neutral notation" (p. 16, italics in the original), have always bothered me. To me, logic is not a particular knowledge representation language, but is the study of correct reasoning. There are many systems of logic, each of which may be called a logic, and knowledge representation research may be viewed as a search for an appropriate logic to underlie commonsense reasoning. I am sure that Sowa does not actually disagree with this. Section 1.3 is titled "Varieties of logic" (pp. 18-29), and at the beginning of Section 1.5, he says, "many notations for logic have been invented over the years ... To be a logic, a knowledge representation language must have four essential features: Vocabulary ... Syntax ... Semantics ... Rules of inference" (pp. 39-40). The logics and variants of logics discussed in Chapter 1 include typed (sometimes called "sorted") logics, larnbda calculus, modal logic, higherorder logic, KIF, and conceptual graphs. There are also discussions of important issues in logical representation, including the choice of predicates, expressing definitions, object- vs. meta-language, names, types, measures, and the unique-naming convention. An unusual application that is first discussed in this chapter, and then reappears throughout the book, is the representation of a musical piece. Appendix A supplements Chapter 1 by providing more complete introductions to propositional and predicate logic (Appendix A.1), conceptual graphs (Appendix A.2), and the Knowledge Interchange Format (Appendix A.3). Appendix A.1, "Predicate Calculus" (pp. 467-476), is a review of propositional and predicate logic that starts at the very basics, such as the truth tables for conjunction, disjunction, negation, material implication, and equivalence, but uses the terms "theorem" and "proof" without defining them, and even though Sowa had said that "to be a 287  Computational Linguistics  Volume 27, Number 2  Figure 1 Conceptual graph for A cat is on a mat (p. 477). logic, a knowledge representation language must have ... Semantics" (p. 39), he does not give the semantics of predicate logic, beyond the truth tables of the propositional connectives, either in Chapter I or in Appendix A.1. Sowa's choice of notations prefigures the techniques of conceptual graphs. For example, he presents a typed predicate logic, and both the "exactly-one quantifier" (3!) and the "unique existential quantifier" (3!!) (I did not know about the latter--it looks useful). Sowa mentions modal logic in Appendix A.1 without discussing it. He does introduce it in Section 1.3, which is one reason Appendix A.1 shouldn't be read right after reading Section 1.1, when Sowa suggests it. Appendix A.2 (pp. 476-489) is an introduction to conceptual graphs. Again, I was disappointed to find this organized by a set of ten definitions, instead of by a specification of vocabulary, syntax, semantics, and rules of inference. The first definition is, "A conceptual graph g is a bipartite graph that has two kinds of nodes called concepts and conceptual relations" (p. 477). A simple example of a conceptual graph is shown in Figure 1. In the official linear notation, this conceptual graph is written as [Oat] -~ (On) -~ [Mat] In either notation, [Cat] and [Mat] are concepts, and (0n) is a "conceptual relation." The use of the term conceptual relation is not fully justified. When I first introduced the distinction between conceptual and structural relations (Shapiro 1971), the idea was that structural relations were represented by arcs, and conceptual relations were conceptual entities in their own right, were represented by nodes, and could participate in relationships with other conceptual entities. It is true that the "conceptual relations" of conceptual graphs are nodes rather than arcs, but since conceptual graphs are bipartite, "there are no arcs that link ... relations to relations" (p. 478), and so conceptual graphs cannot represent information about so-called conceptual relations. The concepts [Cat] and [Mat] are the simplest variety of concept: "Every concept has a type t and a referent r ... In the concept [Bus], 'Bus' is the type, and the referent is a blank . . . . In the concept [person: John], 'Person' is the type, and the referent 'John' is the name of some person" (p. 478, italics in the original). Not only are conceptual graphs not defined by a specification of vocabulary, syntax, semantics, and rules of inference, they do not have their own independent semantics at all! 1 For example, I would expect the concept [Cat] to denote some cat, but instead we read, "The concept [Cat] by itself simply means There is a cat" (p. 484, italics in the original). The real significance of conceptual graphs is that they are a notational variant of standard predicate logic: "There is a formula operator ~, which translates conceptual graphs to formulas in predicate calculus ... For Figure [1], generates the following formula: (3x: Cat)(3y: Mat)on(x, y)" (p. 476-477, italics in the original). However, there are still problems. Figure 2 (in which the referents V and @1 are quantifiers) is said to be the conceptual graph for the sentence Every employee 
 The next three papers form the first subtheme of Part II of the book, a discussion of uniform and nonuniform representation and reasoning frameworks. These three contributions describe significant natural language systems for machine translation, reasoning about space and time, and dialogue processing in a tutoring system, respectively. Bonnie J. Dorr and Clare R. Voss, in "A multi-level approach to interlingual machine translation: Defining the interface between representational languages," argue that a variety of knowledge representation types is advantageous in their machine translation task. Lucja M. Iwa~ska, in "Uniform natural (language) spatio-temporal logic: Reasoning about absolute and relative space and time," proposes a uniform knowledge representation and reasoning approach for spatio-temporal reasoning, again returning to her conjecture from Chapter 1 that natural language provides the appropriate representation and reasoning mechanism. Susan W. McRoy, Syed S. Ali, and Susan M. Haller, in "Mixed depth representations for dialog processing," use a uniform knowledge-representation framework to reason about the domain knowledge and the discourse in their tutoring system. The final three papers constitute the second subtheme of Part II. The underlying theme of two of the papers is the use of linguistic knowledge to enrich knowledge representation or knowledge acquisition techniques (the paper by Iwa~ska, Mata, and Kruger does not use "a priori existing knowledge" [p. 336] to acquire taxonomic knowledge from texts). Sanda M. Harabagiu and Dan I. Moldovan, in "Enriching the WordNet taxonomy with contextual knowledge acquired from text," discuss their contextualization of concepts in WordNet, enabling pragmatic inferences such as Gricean implicatures to be derived. Lucja M. Iwa~ska, Naveen Mata, and Kellyn Kruger, in "Fully automatic acquisition of taxonomic knowledge from large corpora of texts: Limited-syntax knowledge representation system based on natural language," employ weak, local-context-based methods to extract taxonomic knowledge from text to be represented using the UNO representation language that was introduced in Iwa~ska's first paper in the book. William J. Rapaport and Karen Ehrlich, in "A computational theory of vocabulary acquisition," use context (surrounding text, grammatical information, and background knowledge) to learn the meaning of new words (or word senses) encountered in text. The papers in this collection, for the most part, describe major projects, some that span one or two decades (and for some, their roots can be traced further). The comments that I make below should not be taken to reflect on the authors' contributions to this book. In the preface, the editors state that this book "contains the most recent theoretical and practical computational approaches to representing and utilizing the meaning of natural language." Unfortunately, the book seems to have taken some time to be published. This delay is evident in a number of ways: Most of the papers have no references beyond 1997 and the few that do appear are self-references. Also, a table in the preface noting some professional activities associated with the topic of the book ends with a 1997 entry. The editors' claim is also undermined by the fact that two of the papers are updated versions of papers from Expert Systems 1996 and one is an update of a 1992 Artificial Intelligence paper. If the 1997 date is not a coincidence, there has been more recent work done. For example, the publication of Blackburn et al. 1998 marks the beginning of the DORIS project at the University of the Saarland (http://www.coli.uni-sb.de/,-~bos/doris/). DRT-oriented and based on classical logics (representation and inference), it would provide a different perspective on this discussion, somewhat akin to the uniform-nonuniform dichotomy in Part II. In addition to the eleven papers, three appendices are included "to make this book self-contained" (p. xvii). Unfortunately, Appendix A, entitled "Propositional, First-  296  Book Reviews  Order and Higher-Order Logics," gives eight pages each to propositional and firstorder logic (which the editors state are inadequate for natural language) and only half a page to the last topic which is much better suited to the task. Also contributing to the lack of self-containedness (but in a minor way) is the lack of cross-references to the papers contained in the book even though references to prior work by contributors are made (especially in Part I). The editors might have chosen a slightly different set of papers (four of the eleven papers are authored or coauthored by the editors). The DORIS project, mentioned earlier, would be a candidate for Part L Because all of the bibliographies have been moved to the end of the book, I would have liked to have seen the bibliographic entries in the index. Having a reverse index of the references is appealing when the bibliography is presented in this manner.  Reference Blackburn, Patrick, Johan Bos, Michael Kohlhase, and Hans de Nivelle. 1998. Automated theorem proving for natural  language understanding. CADE-15 Workshop Problem-solvingMethodologies with Automated Deduction.  RobertMercer is an Associate Professor in the Department of Computer Scienceat The University of Western Ontario. His interests include nolxmonotonic logics and their use in the representation of natural language knowledge and reasoning. Mercer's address is Department of Computer Science,Middlesex College, The University of Western Ontario, London, Ontario, Canada N6A 5B7; e-mail: mercer@csd.uwo.ca.  297  
Another strength of the volume comes from the expertise in language modeling developed at CIIR. The contribution of language models to several aspects of IR is well represented, appealing to researchers interested in statistical natural language processing (NLP). Two different language models are presented. The first model, originally introduced by Ponte and Croft (1998), is described from two different perspectives. In "Combining approaches to information retrieval," by Bruce Croft, the Ponte and Croft language model is considered when dealing with combinations of evidence generated by merging retrieval models and strategies. This paper also contains a wealth of information, representative of more than ten years of IR research into combining retrieval representations, retrieval algorithms, and search results. The same language model is also considered in "Language models for relevance feedback" by Jay Ponte, where relevance feedback and routing techniques are derived. Ponte's paper supports the theoretical findings with extensive experimental data. A different, unigram language model is described in "Topic-based language models for distributed retrieval" by Jinxi Xu and Bruce Croft. This new language model, called topic model, is used to characterize the content of a specific topic from a given collection. In this paper, topics are approximated as document clusters, produced with the k-means clustering algorithm (Jain and Dubes 1988). This representation entails three new methods for distributed IR, suitable for different environments, including dynamic ones. Readers interested in other aspects of distributed IR are presented with an excellent account of the techniques involved in resource selection and merging of document rankings in Jamie Callan's paper "Distributed information retrieval." This paper lists experimental data that demonstrates the effectiveness of distributed IR techniques. The architecture of distributed IR systems is considered in "The effect of collection organization and query locality on information retrieval system performance" by Zhihong Lu and Kathryn McKinley. The authors' expertise in distributed and parallel systems ports new, interesting research perspectives to the problem of distributed IR. The volume also contains papers on automatic derivations of concept hierarchies, cross-language retrieval and image retrieval. In "Building, testing, and applying concept hierarchies," Mark Sanderson and Dawn Lawrie present a method of automatically devising concept hierarchies based on document term frequencies, a metric that was recently used by Caraballo and Charniak (1999) to determine the specificity of nouns in texts. In "Cross-language retrieval via transitive translation," Lisa Ballesteros presents the CIIR efforts in cross-lingual IR, focusing on a dictionary-based approach. In "Appearance-based global similarity retrieval of images," S. Chandu Ravela and C. Luo present a technique of computing global appearance similarity, which enables appearance-based retrieval of images. To the computational linguist, this collection of articles has interest for at least three reasons. First, the past years have shown that several NLP techniques play a central role in one of the most exciting new applications in IR: Open-domain textual question answering (Q/A)--that is, the task of producing answers in the form of text snippets from a corpus whenever an open-domain natural language question is posed. 1 The development of recent Q/A systems paved the way to new models of IR and especially to novel semantic processing mechanisms, able to operate on more and more complex questions and text passages. It is clear that this new interest in semantic processing will find its way into mainstream NLP.  
 304  Book Reviews the sets of finite labeled trees definable in MSO are exactly those that can be generated by context-free grammars. (Engelfriet [1991] presents an elegant further generalization of the models for MSO, this time to arbitrary graphs; the results on trees follow as a special case.) Rogers connects this work to the post-1980 trend in theoretical linguistics focused on grammars composed of (or incorporating) declarative statements about the syntactic structure of natural language expressions. Linguists of most theoretical persuasions today talk about languages in a way that is at least partially reminiscent of the work of the computer scientists mentioned above: they state (however informally) conditions applying directly to syntactic structures. For example, consider a statement of binding theory in GB, such as "An anaphor is governed in its binding domain." This is a condition on sentence structure, not a component of a device for building structure. A representation that displays binding relations either satisfies it or does not. Description in such terms can be called model-theoretic syntax (the term appears to originate with Rogers's title for a class he gave at the European Summer School in Logic, Language, and Information in 1996). By contrast, the dynamic structure-building syntax of Post production systems and transformational grammars can be referred to as rewriting syntactic description. But most current theories are actually hybrids of model-theoretic and rewriting syntax. For example, the "Move" operation of GB and the more recent minimalist program is interpreted as a schema over instructions for building objects: "Move" is not a statement that is true or false of any given structure. Rogers is interested in the program of describing syntactic structure in a way that is purely and rigorously model-theoretic (as hardly any work in linguistics has been; Rogers [1997] correctly notes Johnson and Postal [1980] as one of the few exceptions). 
 Computational Linguistics  Volume 27, Number 1  retrieval literature (Sparck Jones 1972); it counts the number of documents that contain a type at least once. Term frequency is an integer between 0 and N; document frequency is an integer between 0 and D, the number of documents in the corpus. The statistics, tf and df, and functions of these statistics such as mutual information (MI) and inverse document frequency (IDF), are usually computed over short n-grams such as unigrams, bigrams, and trigrams (substrings of 1-3 tokens) (Charniak 1993; Jelinek 1997). This paper will show how to work with much longer n-grams, including million-grams and even billion-grams. In corpus-based NLP, term frequencies are often converted into probabilities, using the maximum likelihood estimator (MLE), the Good-Turing method (Katz 1987), or Deleted Interpolation (Jelinek 1997, Chapter 15). These probabilities are used in noisy channel applications such as speech recognition to distinguish more likely sequences from less likely sequences, reducing the search space (perplexity) for the acoustic recognizer. In information retrieval, document frequencies are converted into inverse document frequency (IDF), which plays an important role in term weighting (Sparck Jones 1972). dr(t) IDF(t) = -log 2 D  IDF(t) can be interpreted as the number of bits of information the system is given if it is told that the document in question contains the term t. Rare terms contribute more bits than common terms. Mutual information (MI) and residual IDF (RIDF) (Church and Gale 1995) both compare tf and df to what would be expected by chance, using two different notions of chance. MI compares the term frequency of an n-gram to what would be expected if the parts combined independently, whereas RIDF combines the document frequency of a term to what would be expected if a term with a given term frequency were randomly distributed throughout the collection. MI tends to pick out phrases with noncompositional semantics (which often violate the independence assumption) whereas RIDF tends to highlight technical terminology, names, and good keywords for information retrieval (which tend to exhibit nonrandom distributions over documents). Assuming a random distribution of a term (Poisson model), the probability p~(k) that a document will have exactly k instances of the term is: e-O Ok pa(k) = rc(O,k) - k! '  where 0 = np, n is the average length of a document, and p is the occurrence probability  of the term. That is,  8-- Ntf tf DN D"  Residual IDF is defined as the following formula.  Residual IDF = observed IDF - predicted IDF = - log df + log(1 - ---- - l o g ~df +log(1 - e-~)  The rest of the paper is divided into two sections. Section 2 describes the algorithms and the code that were used to compute term frequencies and document frequencies  Yamamoto and Church  Term Frequency and Document Frequency for All Substrings  for all substrings in two large corpora, an English corpus of 50 million words of the Wall Street Journal, and a Japanese corpus of 216 million characters of the Mainichi Shimbun. Section 3 uses these frequencies to find "interesting" substrings, where what counts as "interesting" depends on the application. MI finds phrases of interest to lexicography, general vocabulary whose distribution is far from random combination of the parts, whereas RIDF picks out technical terminology, names, and keywords that are useful for information retrieval, whose distribution over documents is far from uniform or Poisson. These observations may be particularly useful for a Japanese word extraction task. Sequences of characters that are high in both MI and RIDF are more likely to be words than sequences that are high in just one, which are more likely than sequences that are high in neither. 2. Computing tf and df for All Substrings 2.1 Suffix Arrays This section will introduce an algorithm based on suffix arrays for computing tf and df and many functions of these quantities for all substrings in a corpus in O(NlogN) time, even though there are N(N + 1)/2 such substrings in a corpus of size N. The algorithm groups the N(N + 1)/2 substrings into at most 2N - 1 equivalence classes. By grouping substrings in this way, many of the statistics of interest can be computed over the relatively small number of classes, which is manageable, rather than over the quadratic number of substrings, which would be prohibitive. The suffix array data structure (Manber and Myers 1990) was introduced as a database indexing technique. Suffix arrays can be viewed as a compact representation of suffix trees (McCreight 1976; Ukkonen 1995), a data structure that has been extensively studied over the last thirty years. See Gusfield (1997) for a comprehensive introduction to suffix trees. Hui (1992) shows how to compute df for all substrings using generalized suffix trees. The major advantage of suffix arrays over suffix trees is space. The space requirements for suffix trees (but not for suffix arrays) grow with alphabet size: O(N]~]) space, where ]~] is the alphabet size. The dependency on alphabet size is a serious issue for Japanese. Manber and Myers (1990) reported that suffix arrays are an order of magnitude more efficient in space than suffix trees, even in the case of relatively small alphabet size (IGI = 96). The advantages of suffix arrays over suffix trees become much more significant for larger alphabets such as Japanese characters (and English words). The suffix array data structure makes it convenient to compute the frequency and location of a substring (n-gram) in a long sequence (corpus). The early work was motivated by biological applications such as matching of DNA sequences. Suffix arrays are closely related to PAT arrays (Gonnet, Baeza-Yates, and Snider 1992), which were motivated in part by a project at the University of Waterloo to distribute the Oxford English Dictionary with indexes on CD-ROM. PAT arrays have also been motivated by applications in information retrieval. A similar data structure to suffix arrays was proposed by Nagao and Mori (1994) for processing Japanese text. The alphabet sizes vary considerably in each of these cases. DNA has a relatively small alphabet of just 4 characters, whereas Japanese has a relatively large alphabet of more than 5,000 characters. The methods such as suffix arrays and PAT arrays scale naturally over alphabet size. In the experimental section (Section 3) using the Wall Street Journal corpus, the suffix array is applied to a large corpus of English text, where the al- phabet is assumed to be the set of all English words, an unbounded set. It is sometimes assumed that larger alphabets are more challenging than smaller ones, but ironically,  Computational Linguistics  Volume 27, Number 1  it can be just the reverse because there is often an inverse relationship between the size of the alphabet and the length of meaningful or interesting substrings. For expository convenience, this section will use the letters of the alphabet, a-z, to denote tokens. This section starts by reviewing the construction of suffix arrays and how they have been used to compute the frequency and locations of a substring in a sequence. We will then show how these methods can be applied to find not only the frequency of a particular substring but also the frequency of all substrings. Finally, the methods are generalized to compute document frequencies as well as term frequencies. A suffix array, s, is an array of all N suffixes, sorted alphabetically. A suffix, s[i], also known as a semi-infinite string, is a string that starts at position i in the corpus and continues to the end of the corpus. In practical implementations, it is typically denoted by a four-byte integer, i. In this way, a small (constant) a m o u n t of space is used to represent a very long substring, which one might have thought would require N space. A substring, sub(i,j), is a prefix of a suffix. That is, sub(i,j), is the first j characters of the suffix s[i]. The corpus contains N ( N + 1)/2 substrings. The algorithm, suffix_array, presented below takes a corpus and its length N as input, and outputs the suffix array, s. suffix_array *-- function(corpus, N){ Initialize s to be a vector of integers from 0 to N - 1. Let each integer denote a suffix starting at s[i] in the corpus. Sort s so that the suffixes are in alphabetical order. Return s. } The C program below implements this algorithm. char *corpus ; int suffix_compare(int *a, int *b) { return strcmp(corpus+*a, corpus+*b);} int *suffix_array(int n){ int i, *s = (int *)malloc(n*sizeof(int)); for(i=O; i < n; i++) s[i] = i; /* initialize */ qsort(s, n, sizeof(int), suffix_compare); /* sort */ return s;}  Figures i and 2 illustrate a simple example where the corpus ("to_be_ormot_to_be") consists of N = 18 (19 bytes): 13 alphabetic characters plus 5 spaces (and 1 null termination). The C program (above) starts by allocating memory for the suffix array (18 integers of 4 bytes each). The suffix array is initialized to the integers from 0 to 17. Finally, the suffix array is sorted by alphabetical order. The suffix array after initialization is shown in Figure 1. The suffix array after sorting is shown in Figure 2. As mentioned above, suffix arrays were designed to make it easy to compute the frequency (tf) and locations of a substring (n-gram or term) in a sequence (corpus). Given a substring or term, t, a binary search is used to find the first and last suffix that start with t. Let s[i] be the first such suffix and s~] be the last such suffix. Then tf(t) = j - i + 1 and the term is located at positions: {s[i], s[i + 1]. . . . ,s~]}, and only these positions. Figure 2 shows how this procedure can be used to compute the frequency and locations of the term "to_be" in the corpus "to_be_or_not_to_be". As illustrated in the figure, s[i = 16] is the first suffix to start with the term "to_be" and s~ = 17] is the last  Yamamoto and Church  Term Frequency and Document Frequency for All Substrings  Input corpus: "to_be_or_not_to_be"  Position: Characters:  It0lo1l_2lbl3el4_l5ol6rl_7 ln8 lo9ltl10_l1t1 l1o2 l_13lb14le1l5  16  17  .~"  Initialized : : : : Suffix Array ' ' ' ! Suffixes denoted by s [ i]  ""{ i i i ! ii i i i  s[O] 0 -" / ." / to b~ o~ not to b~ I/ / i i i  s[l] 1  s[2] s[3]  2 3  II........"....."~j....,.'":  o or to j :" ' i i ' ' I be or not to be I ." / / / / beo r N o t _ t o _ b e ......,'"'"...'/" .y ./" /  s [ 1 3 ] 13 s [ 1 8 ] 14 s [15] 15 s [16] 15 s [ 1 7 ] 17 Figure 1 Illustration of a suffix array, s, that has just been initialized and not yet sorted. Each element in the suffix array, s[i], is an integer denoting a suffix or a semi-infinite string, starting at position i in the corpus and extending to the end of the corpus.  Suffix Array s[0] 1151 s [1] 2., s[2] 8 s[3] 5 s [4] 12 s [5] 16 s[6] 3 s [7] 17 s [8] 4 s[9] 9 s [10] 14 sill] 1 s[12] 6 s[13] 10 s [14] 7 s [15] 11 s [16] 13 s[17] 0  Suffixes denoted by s [ i ] _De be_or_not_to_be _not to be _or_not_to_be _to_be be be_or_no t_t o_be e e_or_no t_t o_be not_to_be obe o be or_not_to_be or_not to be ot_to_be r_not_to_be t to be to_be to be or_not_to_be  Figure 2 Illustration of the suffix array in Figure 1 after sorting. The integers in s are sorted so that the semi-infinite strings are now in alphabetical order.  Computational Linguistics  Volume 27, Number 1  suffix to start with this term. Consequently, tf("to_be") = 1 7 - 1 6 + 1 = 2. Moreover, the term appears at positions("to_be") = {s[16], s[17]} = {13, 0}, and only these positions. Similarly, the substring "to" has the same tf and positions, as do the substrings, "to_" and "to_b". Although there may be N(N + 1)/2 ways to pick i and j, it will turn out that we need only consider 2N - 1 of them when computing tf for all substrings. Nagao and Mori (1994) ran this procedure quite successfully on a large corpus of Japanese text. They report that it takes O(NlogN) time, assuming that the sort step performs O(N log N) comparisons, and that each comparison takes constant time. While these are often reasonable assumptions, we have found that if the corpus contains long repeated substrings (e.g., duplicated articles), as our English corpus does (Paul and Baker 1992), then the sort can consume quadratic time, since each comparison can take order N time. Like Nagao and Mori (1994), we were also able to apply this procedure quite successfully to our Japanese corpus, but for the English corpus, after 50 hours of CPU time, we gave up and turned to Manber and Myers's (1990) algorithm, which took only two hours. 1Manber and Myers' algorithm uses some clever, but difficult to describe, techniques to achieve O(N log N) time, even for a corpus with long repeated substrings. For a corpus that would otherwise consume quadratic time, the Manber and Myers algorithm is well worth the effort, but otherwise, the procedure described above is simpler, and can even be a bit faster. The "to_be_or_not_to_be" example used the standard English alphabet (one byte per character). As mentioned above, suffix arrays can be generalized in a straightforward way to work with larger alphabets such as Japanese (typically two bytes per character). In the experimental section (Section 3), we use an open-ended set of English words as the alphabet. Each token (English word) is represented as a four-byte pointer into a symbol table (dictionary). The corpus "to_be_or_not_to_be", for example, is tokenized into six tokens: "to", "be", "or", "not", "to", and "be", where each token is represented as a four-byte pointer into a dictionary. 2.2 Longest Common Prefixes (LCPs) Algorithms from the suffix array literature make use of an auxiliary array for storing LCPs (longest common prefixes). The lcp array contains N + 1 integers. Each element, lcp[i], indicates the length of the common prefix between s[i - 1] and s[i]. We pad the lcp vector with zeros (lcp[O] = lcp[N] = 0) to simplify the discussion. The padding avoids the need to test for certain end conditions. Figure 3 shows the lcp vector for the suffix array of "to_be_or_not_to_be". For example, since s[10] and s[11] both start with the substring "o_be", lcp[11] is set to 4, the length of the longest common prefix. Manber and Myers (1990) use the Icp vector in their O(P+log N) algorithm for computing the frequency and location of a substring of length P in a sequence of length N. They showed that the Icp vector can be computed in O(N log N) time. These algorithms are much faster than the obvious straightforward implementation when the corpus contains long repeated substrings, though for many corpora, the complications required to avoid quadratic behavior are unnecessary. 2.3 Classes of Substrings Thus far we have seen how to compute tf for a single n-gram, but how do we compute tf and df for all n-grams? As mentioned above, the N(N + 1)/2 substrings will be clustered into a relatively small number of classes, and then the statistics will be  
Prosodic cues are known to be relevant to discourse structure in spontaneous speech (cf. Section 2.3) and can therefore be expected to play a role in indicating topic transitions. Furthermore, prosodic cues, by their nature, are relatively unaffected by word identity, and should therefore improve the robustness of lexical topic segmentation methods based on automatic speech recognition. • Departmentof ComputerEngineering,BilkentUniversity,Ankara,06533,Turkey.E-mail:{tur, hakkani}@cs.bilkent.edu.tr.The researchreportedhere was carriedout whilethe authorswere InternationalFellowsat SRI International. t SpeechTechnologyand ResearchLaboratory,SRIInternational,333 RavenswoodAve.,MenloPark,CA 94025. E-maih {stolcke,ees}@speech.sri.com.  Computational Linguistics  Volume 27, Number 1  •.. tens of thousands of people are homeless in northern china tonight after a powerful earthquake hit an earthquake registering six point two on the richter scale at least forty seven people are dead few pictures available from the region but we do know temperatures there will be very cold tonight minus seven degrees <TOPIC_CHANGE> peace talks expected to resume on monday in belfast northern ireland former u. s. senator george mitchell is representing u. s. interests in the talks but it is another american center senator rather who was the focus of attention in northern ireland today here's a. b. c.'s richard gizbert the senator from america's best known irish catholic family is in northern ireland today to talk about peace and reconciliation a peace process does not mean asking unionists or nationalists to change or discard their identity or aspirations... Figure 1 An example of a topic boundary in a broadcast news transcript.  Topic segmentation research based on prosodic information has generally relied on hand-coded cues (with the notable exception of Hirschberg and Nakatani [1998]), or has not combined prosodic information with lexical cues (Litman and Passonneau [1995] is one example where lexical information was combined with hand-coded prosodic features for a related task). Therefore, the work presented here is the first that combines automatic extraction of both lexical and prosodic information for topic segmentation. The general framework for combining lexical and prosodic cues for tagging speech with various kinds of "hidden" structural information is a further development of our earlier work on sentence segmentation and disfluency detection for spontaneous speech (Shriberg, Bates, and Stolcke 1997; Stolcke and Shriberg 1996; Stolcke et al. 1998), conversational dialogue tagging (Stolcke et al. 2000), and information extraction from broadcast news (Hakkani-T~ir et al. 1999). In the next section, we review previous work on topic segmentation. In Section 3, we describe our prosodic and language models as well as methods for combining them. Section 4 reports our experimental procedures and results. We close with some general discussion (Section 5) and conclusions (Section 6). 2. Previous Work Work on topic segmentation is generally based on two broad classes of cues. On the one hand, one can exploit the fact that topics are correlated with topical content-word usage, and that global shifts in word usage are indicative of changes in topic. Quite independently, discourse cues, or linguistic devices such as discourse markers, cue phrases, syntactic constructions, and prosodic signals are employed by speakers (or writers) as generic indicators of endings or beginnings of topical segments. Interestingly, most previous work has explored either one or the other type of cue, but only rarely both. In automatic segmentation systems, word usage cues are often captured by statistical language modeling and information retrieval techniques. Discourse cues, on the other hand, are typically modeled with rule-based approaches or classifiers derived by machine learning techniques (such as decision trees). 2.1 Approaches Based on Word Usage Most automatic topic segmentation work based on text sources has explored topical word usage cues in one form or other. Kozima (1993) used mutual similarity of words in a sequence of text as an indicator of text structure. Reynar (1994) presented a method that finds topically similar regions in the text by graphically modeling the distribution 32  Ttir, Hakkani-Tiir, Stolcke, and Shriberg  Integrating Prosodic and Lexical Cues  of word repetitions. The method of Hearst (1994, 1997) uses cosine similarity in a word vector space as an indicator of topic similarity. More recently, the U.S. Defense Advanced Research Projects Agency (DARPA) initiated the Topic Detection and Tracking (TDT) program to further the state of the art in finding and following new topics in a stream of broadcast news stories. One of the tasks in the TDT effort is segmenting a news stream into individual stories. Several of the participating systems rely essentially on word usage: Yamron et al. (1998) model topics with unigram language models and their sequential structure with hidden Markov models (HMMs). Ponte and Croft (1997) extract related word sets for topic segments with the information retrieval technique of local context analysis, and then compare the expanded word sets. 2.2 Approaches Based on Discourse and Combined Cues Previous work on both text and speech has found that cue phrases or discourse particles (items such as now or by the way), as well as other lexical cues, can provide valuable indicators of structural units in discourse (Grosz and Sidner 1986; Passonneau and Litman 1997, among others). In the TDT framework, the UMass HMM approach described in Allan et al. (1998) uses an HMM that models the initial, middle, and final sentences of a topic segment, capitalizing on discourse cue words that indicate beginnings and ends of segments. Aligning the HMM to the data amounts to segmenting it. Beeferman, Berger, and Lafferty (1999) combined a large set of automatically selected lexical discourse cues in a maximum entropy model. They also incorporated topical word usage into the model by building two statistical language models: one static (topic independent) and one that adapts its word predictions based on past words. They showed that the log likelihood ratio of the two predictors behaves as an indicator of topic boundaries, and can thus be used as an additional feature in the exponential model classifier. 2.3 Approaches Using Prosodic Cues Prosodic cues form a subset of discourse cues in speech, reflecting systematic duration, pitch, and energy patterns at topic changes and related locations of interest. A large literature in linguistics and related fields has shown that topic boundaries (as well as similar entities such as paragraph boundaries in read speech, or discourselevel boundaries in spontaneous speech) are indicated prosodically in a manner that is similar to sentence or utterance boundaries--only stronger. Major shifts in topic typically show longer pauses, an extra-high F0 onset or "reset," a higher maximum accent peak, greater range in F0 and intensity (Brown, Currie, and Kenworthy 1980; Grosz and Hirschberg 1992; Nakajima and Allen 1993; Geluykens and Swerts 1993; Ayers 1994; Hirschberg and Nakatani 1996; Nakajima and Tsukada 1997; Swerts 1997) and shifts in speaking rate (Brubaker 1972; Koopmans-van geinum and van Donzel 1996; Hirschberg and Nakatani 1996). Such cues are known to be salient for human listeners; in fact, subjects can perceive major discourse boundaries even if the speech itself is made unintelligible via spectral filtering (Swerts, Geluykens, and Terken 1992). Work in automatic extraction and computational modeling of these characteristics has been more limited, with most of the work in computational prosody modeling dealing with boundaries at the sentence level or below. However, there have been some studies of discourse-level boundaries in a computational framework. They differ in various ways, such as type of data (monologue or dialogue, human-human or human-computer), type of features (prosodic and lexical versus prosodic only), which features are considered available (e.g., utterance boundaries or no boundaries), to  33  Computational Linguistics  Volume 27, Number 1  what extent features are automatically extractable and normalizable, and the machine learning approach used. Because of these vast difference, the overall results cannot be compared directly to each other or to our work, but we describe three of the approaches briefly here. An early study by Litman and Passonneau (1995) used hand-labeled prosodic boundaries and lexical information, but applied machine learning to a training corpus and tested on unseen data. The researchers combined pause, duration, and hand-coded intonational boundary information with lexical information from cue phrases (such as and and so). Additional knowledge sources included complex relations, such as coref- erence of noun phrases. Work by Swerts and Ostendorf (1997) used prosodic features that in principle could be extracted automatically, such as pitch range, to classify utterances from human-computer task-oriented dialogue into two categories: initial or noninitial in the discourse segment. The approach used CART-style decision trees to model the prosodic features, as well as various lexical features that, in principle, could also be estimated automatically. In this case, utterances were presegmented, so the task was to classify segments rather than find boundaries in continuous speech; some of the features included, such as type of boundary tone, may not be easy to extract robustly across speaking styles. Finally, Hirschberg and Nakatani (1998) proposed a prosodyonly front end for tasks such as audio browsing and playback, which could segment continuous audio input into meaningful information units. They used automatically extracted pitch, energy, and "other" features (such as the cross-correlation value used by the pitch tracker in determining the estimate of F0) as inputs to CART-style trees, and aimed to predict major discourse-level boundaries. They found various effects of frame window length and speakers, but concluded overall that prosodic cues could be useful for audio browsing applications.  3. The Approach  Topic segmentation in the paradigm used in this study and others (Allan et al. 1998) proceeds in two phases. In the first phase, the input is divided into contiguous strings of words assumed to belong to the same topic. We refer to this step as chopping. For example, in textual input, the natural units for chopping are sentences (as can be inferred from punctuation and capitalization), since we can assume that topics do not change in mid sentence. 1For continuous speech input, the choice of chopping criteria is less obvious; we compare several possibilities in our experimental evaluation. Here, for simplicity, we will use "sentence" to refer to units of chopping, regardless of the criterion used. In the second phase, the sentences are further grouped into contiguous stretches belonging to one topic, i.e., the sentence boundaries are classified into topic boundaries and nontopic boundaries. 2 Topic segmentation is thus reduced to a boundary classification problem. We will use B to denote the string of binary boundary classifications. Furthermore, our two knowledge sources are the (chopped) word sequence W and the stream of prosodic features F. Our approach aims to find the segmentation B with highest probability given the information in W and F  argmax P(BIW, F)  (1)  B  using statistical modeling techniques.  
 Computational Linguistics  Volume 27, Number 1  The work reported in this paper describes the process of building and refining morphological analyzers using data elicited from human informants and machine learning. The main use of machine learning in our current approach is in the automatic learning of formal rewrite or replace rules for morphographemic changes derived from the examples provided by the informant. The subtask of accounting for morphographemic changes is perhaps one of the more complicated aspects of building an analyzer; by automating it, we expect to improve productivity. After a review of related work, we very briefly describe the Boas project, of which the current work is a part. Subsequent sections describe the details of the approach, the architecture of the morphological analyzer, the elicited descriptive data, and the computational processes performed on this data, including segmentation and the induction of morphographemic rules. We then provide a detailed example of applying this approach to developing a morphological analyzer for Polish. Finally, we provide some conclusions and ideas for future work. 2. Related Work Machine learning techniques are widely employed in many aspects of language processing. The availability of large, annotated corpora has fueled a significant amount of work in the application of machine learning techniques to language processing problems, such as part-of-speech tagging, grammar induction, and sense disambiguation, as witnessed by recent workshops and journal issues dedicated to this topic. 1The current work attempts to contribute to this literature by describing a human-supervised machine learning approach to the induction of morphological analyzers--a problem that, surprisingly, has received little attention. There have been a number of studies on inducing morphographemic rules from a list of inflected words and a root word list. Johnson (1984) presents a scheme for inducing phonological rules from surface data, mainly in the context of studying certain aspects of language acquisition. The premise is that languages have a finite number of alternations to be handled by morphographemic rules and a fixed number of contexts in which they appear; so if there is enough data, phonological rewrite rules can be generated to account for the data. Rules are ordered by some notion of "surfaciness", and at each stage the most surfacy rule--the rule with the most transparent context-is selected. Golding and Thompson (1985) describe an approach for inducing rules of English word formation from a corpus of root forms and the corresponding inflected forms. The procedure described there generates a sequence of transformation rules,2 each specifying how to perform a particular inflection. More recently, Theron and Cloete (1997) have presented a scheme for obtaining two-level morphology rules from a set of aligned segmented and surface pairs. They use the notion of string edit sequences, assuming that only insertions and deletions are applied to a root form to get the inflected form. They determine the root form associated with an inflected form (and consequently the suffixes and prefixes) by exhaustively matching the inflected form against all root words. The motivation is that "real" suffixes will appear frequently in the corpus of inflected forms. Once common suffixes and prefixes are identified, the segmentation for an inflected word can be determined by choosing the segmentation with the most frequently occurring affix segments; the remainder is then considered the root. While this procedure seems to  
* Istituto di Informatica,Via BrecceBianche1-60131Ancona, Italy.E-mail:alex@inform.unian.it t Dipartimento di Scienzedell'Informazione,Via Salaria 113,1-00198Roma,Italy.E-mail:velardi@ dsi.uniromal.it  Computational Linguistics  Volume 27, Number 1  2.1 Learning Contextual Sense Indicators Our method proceeds as follows: first, by means of any available NE recognition technique (which we will call an early NE classifier), at least some examples of PNs in each category are detected. Second, through an unsupervised corpus-based technique, typical PN syntactic and semantic contexts are learned. Syntactic and semantic cues can then be used to extend the coverage of the early NE classifier, increasing its robustness to the limitations of the gazetteers (PN dictionaries) and domain shifts. In phase one, a learning corpus in the application domain is morphologically processed. The gazetteer lookup and the early NE classifier are then used to detect PNs. At the end of this phase, "some" PNs are recognized and classified, depending upon the size of the gazetteer and the actual performance (in the domain) of the NE classifier. In phase two, the objective is to learn a contextual model of each PN category, augmented with syntactic and semantic features. Since the algorithm is unsupervised, statistical techniques are applied to smooth the weight of acquired examples as a function of semantic and syntactic ambiguity.1 Syntactic processing is applied over the corpus. A shallow parser (see details in Basili, Pazienza, and Velardi [1994]) extracts from the learning corpus elementary syntactic relations such as Subject-Object, Noun-Preposition-Noun, etc.2 An elementary syntactic link (esl) is represented as: esl(wi, mod( typei, Wk)) where wj is the headword, Wk is the modifier, and type i is the type of syntactic relation (e.g. Prepositional Phrase, Subject-Verb, Verb-Direct-Object, etc.). For example, esl(close mod(G_N_V_Act Xerox)) reads: Xerox is the modifier of the head close in a Subject-Verb (G_N_V_Act) syntactic relation. In our study, the context of a word w in a sentence S is represented by the esls including w as one of its arguments (wj or Wk). The esls that include semantically classified PNs as one of their arguments are grouped in a database, called PN_esl. This database provides contextual evidence for assigning a category to unknown PNs. 2.2 Tagging Unknown PNs A corpus-driven algorithm is used to classify unknown proper nouns recognized as such, but not semantically classified by the early NE recognizer.3 • Let U_PN be an unknown proper noun, i.e., a single word or a complex nominal. Let Cpn = (Cp~l,Cpn2. . . . . CpnN) be the set of semantic categories for proper nouns (e.g. Person, Organization, Product, etc.). Finally, let ESL be the set of esls (often more than one in a text) that include U_PN as one of their arguments. • For each esli in ESL let: esli( wj, mod( typei, Wk)) = esli(x, U_PN)  
 Book Reviews fiction (5.0 million words), news (10.7 million words), and academic prose (5.3 million words); in addition, there are two supplementary registers: 5.7 million words of nonconversational speech and 6.9 million words of general prose. The transcribed conversations are of particular note. British and American informants surreptitiously tape-recorded all their conversations for a week. (Other participants in the conversations were asked post hoc for consent to the use of the conversation in the corpus.) Throughout the book, the differences between conversation and the written registers are treated in detail, and Chapter 14 is devoted to particular aspects of conversation, such as how the constraints of real-time, interactive language production influence the language user's choice of syntactic constructions. While the more mechanical parts of the corpus analysis were largely automatic (beginning with part-of-speech tagging), the main analyses were carried out manually by the authors with the aid of software for searching and manipulating the contents of the corpus. But manual analysis was also required for many of the relatively low-level tasks, where semantic judgment was required to determine the function of a linguistic element. For example, it still requires a human to reliably distinguish instances of the hedging use of sort of from instances of the regular noun-phrase use (you can sort of wedge it in; what sort of ideas have you come up with? (p. 36)) and to tease out the semantics of the genitive in a coordinate construction (we present Berg and Muscat's definition refers to just one definition; but Andrew and Horatia's eyes met refers to two separate pairs of eyes (p. 298)). Table 1 lists the titles of the chapters, along with my gloss of the contents of each. A complete listing of even the first-level subheadings would take much more space. 2. The Analysis A typical section of LGSWE takes a syntactic phenomenon of English, describing it in general terms. It is then subcategorized, each subcategory is explained, and a crossregister corpus analysis of the phenomenon is presented, highlighting its distribution in each register and the distribution of the subcategories. If the subcategories are semantic, a corpus analysis is presented of the various possible syntactic realizations of each. Tables, sometimes quite long, are often given of relevant words and the frequency of some specific behavior. The results are then discussed qualitatively, emphasizing the functional aspects of syntactic choice. Constructions are analyzed with respect to the work or tasks that they perform, their relation to cognitive constraints on language production, and their social indexing (pp. 41-44). The tasks of linguistic features are divided into the ideational, textual (serving to mark information structure or textual cohesion), personal, interpersonal, contextual, and aesthetic. The following quotations exemplify the style of the analysis: That-clauses functioning as noun complements are one of the primary devices used to mark stance in academic prose. (p. 648) There is a clear stylistic difference between interrogative /f-clauses, which are strongly favored in the more colloquial style of conversation and fiction, and interrogative whether-clauses, which are more neutral in their stylistic range. (p. 691) In [academic prose,] fronting serves to juxtapose items which through semantic repetition cohesively tie the sentences together.... Conversation and fiction, by contrast, strive for greater impact and stylistic 133  Computational Linguistics  Volume 27, Number 1  Table 1 Synopsis of the contents of Longman Grammar of Spoken and Written English. A. Introductory 1. A corpus-based approach to English grammar Introduction to the basic concepts of the work, including the use of the corpus. K Basic grammar 2. Word and phrase grammar The characteristics of words; the basic word classes; function words; the characteristics of phrases; types of phrase; embedding and coordination of phrases. 3. Clause grammar The major elements of clauses and their patterns; peripheral elements; ellipsis; negation; subject-verb concord; dependent and independent clauses. C. Key word classes and their phrases 4. Nouns, pronouns, and the simple noun phrase Types of nouns; determiners; number; case; gender; derived nouns; pronouns. 5. Verbs Single-word and multiword verbs; semantic domains; valency; auxiliaries; copulas. 6. Variation in the verb phrase Tense; aspect; voice; modality. 7. Adjectives and adverbials Types of adjectives; comparatives and superlatives; formation of adjectives; syntactic roles of adverbs; semantic categories of adverbs. D. More complex structures 8. Complex noun phrases Premodification; nominal sequences; restrictive and nonrestrictive postmodifiers; postmodification by relative clauses, prepositional phrases, and appositives. 9. The form and function of complement clauses that, wh-, -ing, and infinitive clauses. 10. Adverbials Circumstance, stance, and linking adverbials. E. Grammar in a wider perspective 11. Word order and related syntactic choices Marked word orders, such as inversions; passive constructions; existential there; clefts. 12. The grammatical marking of stance Kinds of stance; attribution of stance. 13. Lexical expressions in speech and writing Collocations ("lexical bundles"); idioms; free verb-particle combinations; binomial phrases. 14. The grammar of conversation Differences between conversation and writing; dysfluencies; grammatical characteristics of sentences constructed in real time. End matter Appendix: Contraction End notes Bibliography Lexical and conceptual indexes  134  Book Reviews effect, so we find types of fronting which chiefly convey special emphasis and contrast especially fronting of objects but also some fronting of predicatives. (p. 910). At the lexical level, there is a strong emphasis on collocation, which the authors generalize to the notion of lexical bundles, "sequences of word forms that commonly go together in natural discourse" (p. 990)--for example, are shown in table, should be noted that, you want a cup of tea, thank you very much, got nothing to do with. Chapter 13 includes long tables of four- to six-word lexical bundles common in conversation and academic prose, classified by structure. The authors adopt a prototype view of word classes in the style of Lakoff (1987, Chap. 3) (though it is not described in those terms), with classes having core and peripheral members: "nouns can be more or less 'nouny'" (p. 59). Nonetheless, the authors categorize words as much as they possibly can, introducing novel classes such as semi-determiners: "determiner-like words which are often described as adjectives ... [but] have no descriptive meaning" (p. 280)--for example, same, other,former, latter. This varies from CGEL's analysis, in which the adjectival aspect of such words is emphasized (e.g., pp. 430-431). Despite the emphasis on frequency data, there are few actual numbers. In the text itself, most of the results are given qualitatively, with just occasional, isolated numeric data points that are usually approximations. For example, the past perfect is compared with the simple past tense in these three bullet points (p. 469): • The past perfect aspect has accompanying time adverbials a greater percentage of the time than does the simple past tense. • Past perfect verb phrases often occur in dependent clauses. • Taken together, these two factors (time adverbials and dependent clauses) account for c. 70% of all occurrences of past perfect verb phrases. (Statistical significance is never noted; the authors promise in the introduction (p. 40) that anything reported is statistically significant.) Even the tables and figures are rarely numeric. Most quantitative data are reported as histograms, but the histograms, either by their small size or by their design (see below), are very low in precision and are apparently intended only for quick visual impressions. A representative section is 10.2 (pp. 776ff.), on circumstance adverbials. The first subsection describes the different semantic categories of these adverbials (place, time, means, etc). The second subsection gives corpus results on the frequencies of each of the categories in each of the four registers of the corpus, with discussion of the functional significance of the findings; for example, "News is particularly concerned with current events. Time adverbials are therefore commonly used to make clear when events happened or to give background leading up to the current event" (p. 785). The next three subsections give the frequencies of different kinds of phrasal and lexical realizations of each of the semantic categories, including a 2.5-page table of the frequencies in each of the registers of the most common lexical realizations. Differences between British and American usage are highlighted; for example, a large difference is found in the choice between relative and absolute references to time in the news register: yesterday is eight times more common, and this morning is more than twice as common, in the British news subcorpus than in the American news subcorpus, whereas names of days of the week are 28 times more frequent in the latter. The next two subsections look at the frequencies with which the circumstance adverbials occur 135  Computational Linguistics  Volume 27, Number 1  in different positions within the sentence and the correlation between their position, semantic category, and length, and then at the order of occurrence of multiple circumstance adverbials within the same sentence. The final subsection, which is almost as long as all the others together, repeats much of this analysis for clausal realizations of circumstance, which the authors find to be different enough from phrasal and lexical realization to warrant a separate presentation of the analysis. In total, this section is 77 pages long. The authors are generous with the use of examples from the corpus to illustrate their descriptions. Often, the selection of examples is as large as, or even larger than, the description itself; for example, Section 10.2.1.2, on circumstance adverbials of time, consists of five sentences by the authors and 14 example sentences from the corpus. And as anyone who has worked with corpora knows, it is sometimes hard to avoid the distraction of reading the data for its own sake; similarly, in LGSWE, it's easy to start skipping over the authors' text in order to hurry to the next example. One suspects the authors of having sometimes deliberately chosen the most enticing examples from the corpus to illustrate each point; I got particularly hung up trying to construct a story around one example on page 809: One day I left the Yale on the latch accidentally, and when I came back I found a brand-new shelf, with a brass rod below it, high up in the shallow recess beside the fire. Other examples highlight the drama of daily life (pp. 1104, 746, 1119): A: Can I have a drink? B: Yeah, what would you like? A: I would like strawberry. B: With or without ice? A: With. She keeps smelling the washing powder. Yeah, he went "Oh!" He goes, "Who put that there?" And the bit where he goes he goes "Urgh, cobwebs", and she goes "Piss off!" She goes "Mum, come and sit here", she goes "Piss off!" like that and the mother goes "You talking to me?"  3. Empiricism versus prescriptivism Since many lay readers of LGSWE will turn to the book for guidance in "correct English," the authors are careful to emphasize that their work is descriptive, not prescriptive. Of course, this is not to say that the book is not of use as a guide for those who are uncertain in their usage; any writer who wants to ensure that his or her usage is in accord with English norms, be they prescriptive or statistical, will find it extremely helpful. For example, the section on verb concord with existential there (p. 186) spends some time discussing the fact that the prescriptively deprecated use of a singular verb followed by a plural noun phrase (There's apples if you want one) is actually more common in conversation than the prescriptively approved form (There are apples... ). But there is less emphasis than in CGEL on the identification of prescriptively deprecated or stigmatized forms. Indeed, whenever prescriptivism is mentioned in LGSWE, a certain tension arises. For example, the section on dangling participles  136  Book Reviews (p. 829) first mentions the prescriptivist proscription of such participles as if preparing to discard it, but then concedes that violation of the rule can lead to absurd interpretations; and the examples offered are constructed rather than selected from the corpus, implying that none were found in the corpus and hence that dangling participles have no part in even a purely descriptive grammar. (The section closes with some corpus examples of unattached participles that don't "dangle" in the proscribed manner.) In another round of prescriptivism-bashing on pages 83-84, we read: There is a well-known prescriptive reaction against beginning an orthographic sentence with a coordinator [such as but]. Nevertheless, in actual texts, we quite frequently find coordinators in this position.... The prescription against initial coordinators seems most influential in academic prose. The higher frequencies in fiction and news reportage probably reflect the fact that these registers often include more spontaneous discourse, including fiction dialogue and quoted speech in news, evidencing the lack of attention to prescriptive rules of ordinary speech. But while plausible, this is just speculation, and is not in keeping with the authors' own stated goal of explaining their corpus observations in terms of linguistic function; it simply cannot be determined from the corpus data whether or not prescriptivism influences the observed tendency of academic writers to avoid sentence-initial coordinators. (It certainly didn't influence my writing of the previous sentence.1) 4. Pragmatics LGSWE far exceeds CGEL in the quality of its index, which was prepared by Meg Davies. The authors have learned from the problems of CGEL's enormous index (by David Crystal), which, while very comprehensive, frustrates the user by being very sparing with its subheadings. It is not unusual for a single CGEL index entry or subentry to list more than 50 locators without differentiation; moreover, the locators are paragraph numbers, not page numbers, although the design of the book makes it hard to find the paragraph numbers when flipping pages. By contrast, the index of LGSWE uses descriptive subheadings for every small group of locatorsmand the locators are page numbers. In addition, the lexical index is separate from the conceptual index, which also adds to ease of use. LGSWE is an attractive, well-designed, and well-typeset book. I noticed no significant typos or errors of copyediting. But I must admit to being both mystified and somewhat annoyed by the histograms. Five different styles are used, all of them low in resolution: cumulative vertical bars, adjacent vertical bars, horizontal bars of a few discrete lengths, horizontal rows of squares, and horizontal rows of circles. The vertical styles require the reader to distinguish up to six shades of grey. The horizontal styles have no scale, but rather a legend such as "each mrepresents 5%," so that one has to count the squares or circles. Confusingly, the style chosen for any particular histogram seems often to be random and unrelated to either the nature of the data shown or the logic of the display. For example, Tables 5.26-28 (pp. 432-3) show the distribution across registers of various forms of do; the first two tables use horizontal 
responds to what many authors would call an ontology. The meaning base, however, represents categories of experience with a topmost node called phenomenon instead of categories of existence with a topmost node called entity. The first subdivision of phenomena is a three-way partitioning according to levels of complexity:  . Elementary ideas or elements are realized by the lexicogrammar as words or short groups of words, such as rain,from the west, or 8 hard-boiled eggs. Computationally, elements may be represented by slots in a frame, nodes in a graph, or typed variables in logic.  140  Book Reviews . Configurations of elements or figures are realized by phrases or clauses, such as rain ending from the west or chopfinely. Computationally, a figure may be represented by various data structures, such as a frame, list, or graph. . Complexes of figures or sequences are realized by complex sentences or paragraphs, such as Take8 hard-boiled eggs, chopfinely, mash with 3 tablespoons of soft butter, and add salt and pepper. Computationally, a sequence could be represented by a network of frames, a list of lists, a graph of graphs, or structures of objects in an object-oriented language. Each of these categories is further divided and subdivided by various distinctions, some dyadic and some triadic. Elements are classified as participant, circumstance, or process. Figures are classified by another triad of relational (being or having), material (doing or happening), and mental (sensing or saying). These categories are further elaborated and illustrated with numerous examples. To demonstrate the generality of the approach, Chapter 7 shows how the semantic categories realized in English can also be realized in Chinese and other languages. Part III consists of two chapters that show how the theory can be implemented in a computational system for language generation, with examples of weather reports and cooking recipes. Part IV consists of three chapters that compare the theoretical and descriptive techniques of systemic-functional theory to other approaches. The concluding Part V consists of three chapters that apply systemic theory to an analysis of how humans construe experience through language. Chapter 14 has an intriguing analysis of the evolution of linguistic expressions from folk theories to scientific theories. Instead of drawing a sharp dichotomy between commonsense and scientific ways of thinking, the authors show how the basic linguistic mechanisms of abstraction and metaphor are used to systematize and formalize scientific language. Metaphor is fundamental to both science and poetry. The primary difference is that poets constantly strive to create novel metaphors, while scientists recycle, perfect, and build on the most successful of their colleagues' metaphors. In summary, this book makes a strong case for the systemic approach as a fruitful alternative to Chomsky's view of autonomous syntax. The authors demonstrate that semantics has important structures that are cross-linguistic and formalizable. Although they present their data with the terminology, notation, and viewpoint of the systemicfunctional approach, their analyses, distinctions, and categories can be adapted to semantic theories based on other approaches. The authors criticize the logic-based, model-theoretic approaches for their limited ontologies and neglect of important aspects of language, such as metaphor. Yet logicians recognize the need for richer ontologies, and many, if not most, would agree that semantics is the proper starting point for a study of natural language. The authors try to draw a sharp distinction between the deductive methods of logic and the method of inheritance used in frame-based systems. A logician, however, would reply that inheritance is the oldest of all rules of inference; it was introduced by Aristotle for syllogisms, and it is a derived rule in every modern system of logic. The methods of unification used in many logic-based systems implement inheritance in ways that are equivalent to or more general than frame systems. Rather than being a competitor, the systemic approach can be a valuable complement to the logic-based approaches. The authors consider language as a semiotic system, but they only mention the dyadic version of semiotics developed by Saussure and linguists influenced by Saussure, such as Hjelmslev and Firth. Peirce analyzed the sign relation in greater depth 141  Computational Linguistics  Volume 27, Number 1  than Saussure and emphasized its irreducible triadic nature. Although Halliday and Mattheissen never mention Peirce, they have rediscovered many of Peirce's triads in their systemic analysis (Peirce 1991-1998). Their choice of phenomenon as the most general category is an unconscious endorsement of Peirce's point that his categories were primarily phenomenological rather than ontological. The systemic triad of beinghaving, doing-happening, and sensing-saying corresponds to Peirce's fundamental triad of Quality, Reaction, and Representation. Most of the other triads in the systemic meaning base also have a strong Peircean flavor, and a more conscious application of Peirce's version of semiotics might help clarify and refine many of the triadic distinctions in the systemic approach. Perhaps the least attractive feature of the book is its formatting. The authors used a conventional word processor to print camera-ready copy on A4 paper, which the publisher reproduced without change. The result is a heavy, unwieldy tome with a great deal of wasted paper, a generally unfinished appearance, but a price tag of $102. With that price and format, the book is destined to sell very few copies, the authors will get little or nothing in royalties, the publisher's high price will seem to be justified, and a potentially important book will never be read by students who might profit from it. The book would get better distribution if the authors had simply put the electronic version on their Web site; better yet, professional societies such as the ACL should put books such as these on their Web sites.  References Bateman, John A., Robert Kasper, Johanna Moore, and Richard Whitney. 1990. A general organization of knowledge for natural language processing: The Penman upper model. Research Report. Information Sciences Institute, University of Southern California, Marina del Rey. Hovy, Eduard H. 1988. Generating Natural Language under Pragmatic Constraints. Lawrence Erlbaum, Hillsdale, NJ. Mann, William C. 1982. An overview of the Penman text generation system. Research Report 83-114. Information Sciences Institute, University of Southern  California, Marina del Rey. Mann, William C., and Sandra A. Thompson, editors. 1992. Discourse Description: Diverse Linguistic Analysis of a Fund-Raising Text. Benjamins, Amsterdam. Matthiessen, Christian M. I. M. 1995. Lexicogrammatical Cartography: English Systems. International Language Sciences Publishers, Tokyo. Peirce, C. S. 1991-1998. The Essential Peirce, volumes 1 and 2. Nathan Houser and Christian Kloesel, editors. Indiana University Press, Bloomington. Winograd, Terry 1972. Understanding Natural Language. Academic Press, New York.  John E Sowa worked for 30 years on research and development projects at IBM. Since then, he has been teaching, writing, and consulting. He has published and edited several books and numerous articles on knowledge representation, computational linguistics, and related areas of artificial intelligence. He is a Fellow of the AAAI, best known for his work on the theory of conceptual graphs and their application to natural language semantics. E-mail: sowa@bestweb.net; URL:www.bestweb.net / ~sowa / direct/.  142  Book Reviews Computing Meaning, volume 1 Harry Bunt and Reinhard Muskens (editors) (Tilburg University) Dordrecht: Kluwer Academic Publishers (Studies in linguistics and philosophy, edited by Gennaro Chierchia, Pauline Jacobson, and Francis J. Pelletier, volume 73), 1999, vi+360 pp; hardbound, ISBN 0-7923-6108-3, $149.00, £93.00, Dfl 280.00 Reviewed by Yoad Winter Technion--Israel Institute of Technology Manipulating meanings of natural language texts and utterances is one of the main objectives of any large-scale NLP system. However, at present there is no general theory that explains what natural language meanings precisely are, and how they are to be effectively computed for purposes of practical NLP. Moreover, in the first place there is not even an overall agreement as to a general notion of "meaning" that is computationally relevant. Two prominent approaches to the question can be recognized. . The machine learning approach: In this view, meanings should be defined according to whatever representation practical NLP systems find useful. For instance, if a system is to extract travel information from free text, then meanings in this context can be defined as records in a database containing fields such as "destination", "time of arrival", "means of transportation", etc. Such a representation has to be defined ad hoc for any relevant purpose, but the mapping from natural language to this formal representation is performed automatically using general learning algorithms. . The formal semantics approach: According to this line, meanings are logical objects and should be manipulated using logical tools. Work in the formal-semantic school that developed from Montague grammar specifies the logically relevant parts of meaning and how to derive them from a natural language input, while the field of computational semantics deals with the algorithmic realization of these formal techniques as NLP systems. The volume under review is a collection of 16 articles that adopt the second view as their starting point. In a clear and instructive introduction, the editors present an overview of the formal approach to the computation of meaning, illustrate it using a small calculus, and discuss a number of general problems for this approach. Of special importance is the ambiguity problem: the spurious multiplicity of meanings that even the most sophisticated syntactic and semantic theories derive. The construction of underspecified representations of meaning, which is one of the prominent techniques 143  Computational Linguistics  Volume 27, Number 1  for tackling the ambiguity problem, is addressed from different angles by five of the articles in the volume. Another prominent issue in computational semantics is the dynamic nature of many natural language phenomena, especially those related to anaphora and presupposition. Six articles in this volume address dynamic semantics from different perspectives. The other articles in the book deal with different topics in semantics: compositionality, speech events, belief utterances, motion verbs, and the interpretation of German compounds. These topics are vast and highly varied, and a fair description of even the core ideas in these papers is impossible within the space limits of this review. (A good overview of the articles in this volume can be found in the introduction.) Many of the works have important implications for formal semantics or theoretical linguistics, but those papers that are most relevant for computational linguistics are those that succeed in extending an existing computational framework to treat phenomena that it had not previously handled. One such contribution is the paper by Richter and Sailer, who develop an underspecified semantics in HPSG. Of similar significance is an interesting paper by Van Genabith and Crouch, who give a semantics of crosssentential anaphora using LFG glue language semantics. Other papers would be of interest mainly to theoretical linguists or to logicians and philosophers of language. Good examples of works of the first kind are a paper by Ginzburg on ellipsis resolution and a paper by Stone and Hardt on the anaphoric properties of modals. Examples of more logically oriented papers are the contribution by Meyer Viol et al. on the use of epsilon terms for underspecified semantics and the paper by Asher and Fernando on underspecification using labeled representations. Although these works do not give algorithmic implementations of their ideas, they include enough formal details to make small but illustrative computer applications feasible. The book also contains two articles of a more programmatic nature: on underspecified semantics (by Pinkal) and on compositionality and minimum description length (by Zadrozny). These works and others would be of interest to any researcher occupied with problems of natural language semantics from a formal or computational perspective. It is also important to make clear what the book does not include: • It does not provide a unified framework. To understand many of the proposals in this book, the reader has to become familiar with a considerable number of notations, techniques, and theoretical standpoints, sometimes with no real justification for this variety. • The book does not contain contributions that would be of direct relevance to the NLP engineer who is especially interested in the development of practical "real-world" applications. In general, the editors did a good job in projecting a collection of works representing the state of the art in computational semantics. The book contains material that will be of value especially to experts in this field. However, most of the papers in the volume will also be relevant to researchers from other branches of computational linguistics who are interested in theoretical aspects of the computation of meaning in natural language. Acknowledgment Work on this review was partly supported by a visit grant from NWO (the Netherlands Organization for Scientific Research). 
Oral communication is ubiquitous and carries important information yet it is also time consuming to document. Given the development of storage media and networks one could just record and store a conversation for documentation. The question is, however, how an interesting information piece would be found in a large database. Traditional information retrieval techniques use a histogram of keywords as the document representation but oral communication may oﬀer additional indices such as the time and place of the rejoinder and the attendance. An alternative index could be the activity such as discussing, planning, informing, story-telling, etc. This paper addresses the problem of the automatic detection of those activities in meeting situation and everyday rejoinders. Several extensions of this basic idea are being discussed and/or evaluated: Similar to activities one can deﬁne subsets of larger database and detect those automatically which is shown on a large database of TV shows. Emotions and other indices such as the dominance distribution of speakers might be available on the surface and could be used directly. Despite the small size of the databases used some results about the eﬀectiveness of these indices can be obtained. Keywords activity, dialogue processing, oral communication, speech, information access ∗We would like to thank our lab, especially Klaus Zechner, Alon Lavie and Lori Levin for their discussions and support. We would also like to thank our sponsors at DARPA. Any opinions, ﬁndings and conclusions expressed in this material are those of the authors and may not reﬂect the views of DARPA, or any other party. .  1. INTRODUCTION Information access to oral communication is becoming an interesting research area since recording, storing and transmitting large amounts of audio (and video) data is feasible today. While written information is often available electronically (especially since it is typically entered on computers) oral communication is usually only documented by constructing a new document in written form such as a transcript (court proceedings) or minutes (meetings). Oral communications are therefore a large untapped resource, especially if no corresponding written documents are available and the cost of documentation using traditional techniques is considered high: Tutorial introductions by a senior staﬀ member might be worthwhile to attend by many newcomers, oﬃce meetings may contain informations relevant for others and should be reproducable, informal and formal group meetings may be interesting but not fully documented. In essence the written form is already a reinterpretation of the original rejoinder. Such a reinterpretation are used to • extract and condense information • add or delete information • change the meaning • cite the rejoinder • relate rejoinders to each other Reinterpretation is a time consuming, expensive and optional step and written documentation is combining reinterpretation and documentation step in one 1. If however reinterpretation is not necessary or unwanted a system which is producing audiovisual records is superior. If reinterpretation is wanted or needed a system using audiovisual records may be used to improve the reinterpretation by adding all audiovisual data and the option to go back to the unaltered original. Whether reinterpretation is done or not it is crucial to be able to navigate eﬀectively within an audiovisual document and to ﬁnd a speciﬁc document. 1The most important exception is the literal courtroom transcript, however one could argue that even transcripts are reinterpretations since they do not contain a number of informations present in the audio channel such as emotions, hesitations, the use of slang and certain types of hetereglossia, accents and so forth. This is speciﬁcally true if transcription machines are used which restrict the transcriber to standard orthography.  Database Meeting Segment  Meetings TV shows Lectures Speeches Data collection, last Wednesday Dialogue detection, with Hans Language modeling tutorial for Tim Setting up the new hard drive Willie is sick Need new coding scheme Personal stuff  Figure 1: Information access hierarchy: Oral communications take place in very diﬀerent formats and the ﬁrst step in the search is to determine the database (or sub-database) of the rejoinder. The next step is to ﬁnd the speciﬁc rejoinder. Since rejoinders can be very long the rejoinder has to segmented and a segment has to be selected.  While keywords are commonly used in information access to written information the use of other indices such as style is still uncommon (but see Kessler et al. (1997); van Bretan et al. (1998)). Oral communication is richer than written communication since it is an interactive real time accomplishment between participants, may involve speech gestures such as the display of emotion and is situated in space and time. Bahktin (1986) characterizes a conversation by topic, situation and style. Information access to oral communication can therefore make use of indices that pertain to the oral nature of the discourse (Fig. 2). Indices other than topic (represented by keywords) increase in importance since browsing audio documents is cumbersome which makes the common interactive retrieval strategy “query, browse, reformulate” less eﬀective. Finally the topic may not be known at all or may not be that relevant for the query formulation, for example if one just wants to be reminded what was being discussed last time a person was met. Activities are suggested as an alternative index and are a description of the type of interaction. It is common to use “action-verbs” such as story-telling, discussing, planning, informing, etc. to describe activities 2. Items similar to activities have been shown to be directly retrievable from autobiographic memory (Herrmann, 1993) and are therefore indices that are available to participants of the conversation. Other indices may be very eﬀective but not available: The frequency of the word “I” in the conversation, the histogram of word lengths or the histogram of pitch per participant. In Fig. 1 the information access hierarchy is being introduced which allows to understand the problem of information access to oral communication at diﬀerent levels. In Ries (1999) we have shown that the detection of general di- 2 The deﬁnition of activities such as planning may vary vastly across general dialogue genres, for example compare a military combat situation with a mother child interaction. However it is often possible to develop activities and dialogue typologies for a speciﬁc dialogue genre. The related problem of general typologies of dialogues is still far from being settled and action-verbs are just one potential categorization (Fritz and Hundschnur, 1994).  Topic Semantics / pragmatics Keywords  Overlap between speakers Emotion Parts of speech Style  Related rejonders Time Speaker Location Situation  Figure 2: Bahktin’s characterization of dialogue: Bahktin (1986) describes a discourse along the three major properties style, situation and topic. Current information retrieval systems focus on the topical aspect which might be crucial in written documents. Furthermore, since throughout text analysis is still a hard problem, information retrieval has mostly used keywords to characterize topic. Many features that could be extracted are therefore ignored in a traditional keyword based approach.  alogue genre (database level in Fig. 1) can be done with high accuracy if a number of diﬀerent example types have been annotated; in Ries et al. (2000) we have shown that it is hard but not impossible to distinguish activities in personal phone calls (segment level in Fig. 1) . In this paper we will address activities in meetings and other types of dialogues and show that these activities can be distinguished using certain features and a neural network based classiﬁer (Sec. 2, segment level in Fig. 1). The concept of information retrieval assessment using information theoretic measures is applied to this task (Sec. 3). Additionally we will introduce a level somewhat below the database level in Fig. 1 that we call “sub-genre” and we have collected a large database of TV-shows that are automatically classiﬁed for their showtype (Sec. 4). We also explore whether there are other indices similar to activities that could be used and we are presenting results on emotions in meetings (Sec. 5). 2. ACTIVITY DETECTION We are interested in the detection of activities that are described by action verbs and have annotated those in two databases: meetings have been collected at Interactive Systems Labs at CMU (Waibel et al., 1998) and a subset of 8 meetings has been annotated. Most of the meetings are by the data annotation group itself and are fairly informal in style. The participants are often well acquainted and meet each other a lot besides their meetings. Santa Barbara (SBC) is a corpus released by the LDC and 7 out of 12 rejoinders have been annotated. The annotator has been instructed to segment the rejoinders into units that are coherent with respect to their topic  Activity Discussion Information Story-telling Planning Undetermined Advising Not meeting Interrogation Evaluation Introduction Closing  SBC 35 25 24 7 5 5 3 2 1 0 0  Meeting 58 23 10 19 8 17 2 1 0 1 1  Table 1: Distribution of activity types: Both databases contain a lot of discussing, informing and story-telling activities however the meeting data contains a lot more planning and advising.  and activity and annotate them with an activity which follows the intuitive deﬁnition of the action-verb such as discussing, planning, etc. Additionally an activity annotation manual containing more speciﬁc instructions has been available (Ries et al., 2000; Thym´e-Gobbel et al., 2001) 3. The list of tags and the distribution can be seen in Tab. 1. The set of activities can be clustered into “interactive” activities of equal contribution rights (discussion,planning), one person being active (advising, information giving, story-telling), interrogations and all others.  Measure κ Mutual inf.  Meeting all inter 0.41 0.51 0.35 0.25  SBC all inter 0.49 0.56 0.65 0.32  CallHome Spanish 0.59 0.61  Table 2: Intercoder agreement for activities: The meeting dialogues and Santa Barbara corpus have been annotated by a semi-naive coder and the ﬁrst author of the paper. The κ-coeﬃcient is determined as in Carletta et al. (1997) and mutual information measures how much one label “informs” the other (see Sec. 3). For CallHome Spanish 3 dialogues were coded for activities by two coders and the result seems to indicate that the task was easier.  Both datasets have been annotated not only by a seminaive annotator but also by the ﬁrst author of the paper. The results for κ-statistics (Carletta et al., 1997) and mutual information between the coders can be seen in Tab. 2. The intercoder agreement would be considered moderate but compares approximately to Carletta et al. (1997) agreement on transactions (κ = 0.59), especially for the interactive activities and CallHome Spanish. For classiﬁcation a neural network was trained that uses the softmax function as its output and KL-divergence as 3 In contrast to (Ries et al., 2000; Thym´e-Gobbel et al., 2001) the “consoling” activity has been eliminated and an “informing” activity has been introduced for segments where one or more than one member of the rejoinder give information to the others. Additionally an “introducing” activity was added to account for a introduction of people or topics at the beginning of meetings.  Feature baseline dialogue acts per channel dialogue acts words dominance style style + words dominance + words dominance + style + words dialogue acts + words dialogue acts + style + words Wordnet Wordnet + words ﬁrst author  all SBC meet 32.7 41.1 28.1 37.6 28.0 36.2 38.3 39.7 32.7 44.7 24.3 35.5 42.1 38.3 41.1 41.1 42.1 39.7 42.1 37.6 39.3 40.4 37.4 37.6 49.5 39.0 59.8 57.9  interactive SBC meet 50.5 54.6 47.7 56.7 46.7 65.3 53.3 54.6 64.5 58.2 53.3 58.9 52.3 57.5 52.3 58.9 53.3 60.3 57.0 61.0 57.9 61.0 46.7 52.5 53.3 57.5 73.8 72.7  Table 3: Activity detection: Activities are detected on the Santa Barbara Corpus (SBC) and the meeting database (meet) either without clustering the activities (all) or clustering them according to their interactivity (interactive) (see Sec. 2 for details).  the error function. The network connects the input directly to the output units. Hidden units have not been used since they did not yield improvements on this task. The network was trained using RPROP with momentum (Riedmiller and Braun, 1993) and corresponds to an exponential model (Nigam et al., 1999). The momentum term can be interpreted as a Gaussian prior with zero mean on the network weights. It is the same architecture that we used previously (Ries et al., 2000) for the detection of activities on CallHome Spanish. Although some feature sets could be trained using the iterative scaling algorithm if no hidden units are being used the training times weren’t high enough to justify the use of the less ﬂexible iterative scaling algorithm. The features used for classiﬁcation are words the 50 most frequent words / part of speech pairs are used directly, all other pairs are replaced by their part of speech 4. stylistic features adapted from Biber (1988) and contain mostly syntactic constructions and some word classes. Wordnet a total of 40 verb and noun classes (so called lexicographers classes (Fellbaum, 1998)) are deﬁned and a word is replaced by the most frequent class over all possible meanings of the word. dialogue acts such as statements, questions, backchannels, . . . are detected using a language model based detector trained on Switchboard similar to Stolcke et al. (2000) 5 4Klaus Zechner trained an English part of speech tagger tagger on Switchboard that has been used. The tagger uses the code by Brill (1994). 5The model was trained to be very portable and therefore the following choices were taken: (a) the dialogue model is context-independent and (b) only the part of speech are taken as the input to the model plus the 50 most likely word/part of speech types.  dominance is described as the distribution of the speaker dominance in a conversation. The distribution is represented as a histogram and speaker dominance is measured as the average dominance of the dialogue acts (Linell et al., 1988) of each speaker. The dialogue acts are detected and the dominance is a numeric value assigned for each dialogue act type. Dialogue act types that restrict the options of the conversation partners have high dominance (questions), dialogue acts that signal understanding (backchannels) carry low dominance. First author The activities used for classiﬁcation are those of the semi-naive coder. The “ﬁrst author” column describes the “accuracy” of the ﬁrst author with respect to the naive coder. The detection of interactive activities works fairly well using the dominance feature on SBC which is also natural since the relative dominance of speakers should describe what kind of interaction is exhibited. The dialogue act distribution on the other hand works fairly well on the more homogeneous meeting database were there is a better chance to see generalizations from more speciﬁc dialogue based information. Overall the combination of more than one feature is really important since word level, Wordnet and stylistic information, while sometimes successful, seem to be able to improve the result while they don’t provide good features by themselves. The meeting data is also more diﬃcult which might be due to its informal style.  Another option is to assume that the labels of one coder are part of D. If the query by the other coder is R we are interested in the reduction of the document entropy given the query. If we furthermore assume that H(R|D) = H(R|R ) where R is the activity label embedded in D: H(D) − H(D|R) = H(R) − H(R|D) = M I(R, R ) Tab. 2 shows that the labels of the semi-naive coder and the ﬁrst author only inform each other by 0.25 − 0.65 bits. However, since all constraints are important to apply, it might be important to include manual annotations to be matched by a query or in a graphical presentation of the output results. Another interesting question to consider is whether the activity is correlated with the rejoinder or not. This question is important since a correlation of the activity with the rejoinder would mean that the indexing performance of activities needs to be compared to other indices that apply to rejoinders such as attendance, time and place (for results on the correlation with rejoinders see Waibel et al. (2001)). The correlation can be measured using the mutual information between the activity and the meeting identity. The mutual information is moderate for SBC (≈ 0.67 bit) and much lower for the meetings (≈ 0.20 bit). This also corresponds to our intuition since some of the rejoinders in SBC belong to very distinct dialogue genre while the meeting database is homogeneous. The conclusion is that activities are useful for navigation in a rejoinder if the database is homogeneous and they might be useful for ﬁnding conversations in a more heterogeneous database.  3. INFORMATION ACCESS ASSESSMENT  Assuming a probabilistic information retrieval model a  query r – in our example an activity – predicts a docu-  ment d with the probability q(d|r) =  q(r|d)q(d) q(r)  .  Let p(d, r)  be the real probability mass distribution of these quanti-  ties. The probability mass function q(r|d) is estimated on  a separate training set by a neural network based classiﬁer 6. The quantity we are interested in is the reduction  in expected coding length of the document using the neural network based detector 7:  q(D)  
We describe an Example-Based Machine Translation (EBMT) system and the adaptations and enhancements made to create a ChineseEnglish translation system from the Hong Kong legal code and various other bilingual resources available from the Linguistic Data Consortium (LDC). 1. BACKGROUND We describe an Example-Based Machine Translation (EBMT) system and the adaptations and enhancements made to create a Chinese-English translation system from the Hong Kong legal code and various other bilingual resources available from the Linguistic Data Consortium (LDC). The EBMT software [1, 3] used for the experiments described here is a shallow system which can function using nothing more than sentence-aligned plaintext and a bilingual dictionary; and given sufﬁcient parallel text, the dictionary can be extracted statistically from the corpus [2]. To perform a translation, the program looks up all matching phrases in the source-language half of the parallel corpus and performs a word-level alignment on the entries containing matches to determine a (usually partial) translation. Portions of the input for which there are no matches in the corpus do not generate a translation. Because the EBMT system does not generate translations for 100% of the text it is given as input, a bilingual dictionary and phrasal glossary are used to ﬁll any gaps. Selection of a “best” translation is guided by a trigram model of the target language [6]. Supporting Chinese required a number of changes to the program and training procedures; those changes are discussed in the next section. 2. ENHANCEMENTS The ﬁrst change required of the translation software was support for the two-byte encoding used for the Chinese text (GB-2312, “GB” for short). Further, the EBMT (as well as dictionary and glossary) approaches are word-based, but Chinese is ordinarily written without breaks between words. Thus, Chinese input must be .  segmented into individual words. The initial baseline system used the segmenter made available by the LDC. This segmenter uses a word-frequency list to make segmentation decisions, but although the list provided by the LDC is large, it did not completely cover the vocabulary of the EBMT training corpus (described below). As a result, many sentences had incorrect segmentations or included long sequences which were not segmented at all or were broken into single characters. Almost every Chinese character has at least one meaning, and its meaning may be entirely different from the meaning of the word containing it. The mis-segmenting of Chinese words due to the inadequate dictionary makes it very hard to build a statistical dictionary and properly index the EBMT corpus. To improve the performance of the Chinese segmenter, we augmented its word list by ﬁnding sequences of characters in the training corpus that belong together, based on their frequency and high mutual information. We developed a form of term extraction to ﬁnd English phrases which should be treated as atomic units for translation, thus increasing the average length of “words” in both source and target languages. Finally, we also created an augmented bilingual dictionary for use in word-level alignment for EBMT by applying statistical dictionary extraction techniques to the training corpus. As the improved segmenter and the term ﬁnder may be producing excessively long phrases or phrases which are impossible to match in the other language, we repeat the procedure of segmenting/bracketing/dictionary-building several times. On each successive iteration, the segmenter and bracketer are limited to words and phrases for which the statistical dictionary from the previous iteration contains translations. Through this iteration, we increased the size of the statistical dictionary from each step and guaranteed that all Chinese words generated by the segmenter have translations in the dictionary. This helps ensure that the EBMT engine can perform word-level alignments. 3. EXPERIMENTAL DESIGN The primary purpose of this experiment was to determine the effect of each enhancement by operating with various subsets of the enhancements. Since it rapidly becomes impractical to test all possible combinations, we opted for the following test conditions: 1. baseline: parallel corpus segmented with the LDC segmenter and LDC dictionary/glossary 2. baseline plus improved segmenter 3. baseline plus improved segmenter and term ﬁnder 4. baseline plus improved segmenter and statistical dictionary 5. baseline plus improved segmenter, term ﬁnder, and statistical dictionary  For training, we had available two parallel Chinese-English corpora distributed by the LDC: the complete Hong Kong legal code (after cleaning: 47.86 megabytes, 5.5 million English words, 9 million Chinese characters) where 85% of the content (by sentence) is unique, and a collection of Hong Kong news articles (after cleaning: 24.58 megabytes, 2.67 million English words, 4.5 million Chinese characters). In addition, LDC distributes a bilingual dictionary/phrasebook, which we also used. To determine the effects of varying amounts of training data on overall performance, we divided the bilingual training corpus into ten nearly equal slices. Each test condition was then run ten times, each time increasing the number of slices used for training the system. After each training pass, the test sentences were translated and the system’s performance evaluated automatically; selected points were then manually evaluated for translation quality. The automatic performance evaluation measured coverage of the input and average phrase length. Coverage is the percentage of the input text for which a translation is produced by a particular translation method (since the EBMT engine does not generally produce hypotheses that cover every word of input), while average phrase length is a crude indication of translation quality – the longer the phrase that is translated, the more context is incorporated and the less likely it is that the wrong sense will be used in the translation or that (for EBMT) the alignment will be incorrect. Since the dictionary and glossary remain constant for a given test condition, only the EBMT coverage will be presented. Manual grading of the output was performed using a web-based system with which the graders could assign one of three scores (“Good”, “OK”, “Bad”) in each of two dimensions: grammatical correctness and meaning preservation. This type of quality scoring is commonly used in assessing translation quality, and is used by other TIDES participants. Fifty-two test sentences were translated for each of four points from the automated evaluation and these sets of four alternatives presented to the graders. The four points chosen were the baseline system with 100% of the training corpus, the full system with 20% and 100% training, and the full system trained on a corpus of Hong Kong news text (cross-domain); only four points were selected due to the difﬁculty and expense of obtaining large numbers of manual quality judgements. To assess the performance of the system in a different domain, as well as the effect of the trigram language model on the selection of translated fragments for the ﬁnal translation, we obtained manual judgements for 44 sentences on an additional four test conditions, each trained with the entire available parallel text and tested on Hong Kong news text rather than legal sentences. These points were the cross-domain case (trained on the legal corpus) and three different language models for within-domain training: an English language model derived from the legal corpus, one derived from the news corpus, and a pre-existing model generated from two gigabytes of newswire and broadcast news transcriptions. 4. RESULTS We discovered that there is a certain amount of synergy between some of the improvements, particularly the term ﬁnder and statistical dictionary extraction. Applying the term ﬁnder modiﬁes the parallel corpus in such a way that it becomes more difﬁcult for the EBMT engine to ﬁnd matches which it can align, while adding dictionary entries derived from the modiﬁed corpus eliminates that effect. As a result, we will not present the performance results for Test Condition 3 (improved segmenter plus term ﬁnder); further, the data for Test Conditions 2 (improved segmenter only) and 4 (improved segmenter plus statistical dictionary) may not accurately reﬂect the contribution of those two components to the full system  System Training Syntactic Semantic  Translating Legal Code  Baseline Full  Full  100% 20% 100%  42.31% 54.81% 61.06%  43.75% 61.54% 64.42%  X-Dom 100% 39.42% 34.62%  Translating Hong Kong News  Training  News News News  LangModel Legal News Prior  Syntactic 45.67% 44.71% 47.60%  Semantic 50.00% 50.96% 51.92%  Legal Legal 34.62% 47.12%  Figure 1: Judgements – Acceptable Translations  used for Test Condition 5. Figure 2 shows the proportion of the words in the test sentences for which the EBMT engine was able to produce a translation, while Figure 3 shows the average number of source-language words per translated fragment. These curves do not increase monotoni- N cally because, for performance reasons, the EBMT engine does not attempt to align every occurrence of a phrase, only the (currently 12) most-recently added ones; as a result, adding more text to the corpus can cause EBMT to ignore matches that successfully align in favor of newer occurrences which it is unable to align. Examining Figure 3, it is clear that the ﬁfth slice (from 40 to 50%) is much more like the test data than other slices, resulting in longer matches. In general, the closer training and test text are to each other, the longer the phrases they have in common. Figure 1 summarizes the results of human quality assessments. The “Good” and “OK” judgements were combined into “Acceptable” and the the percentage of “Acceptable” judgements was averaged across sentences and graders. As hoped and expected, the improvements do in fact result not only in better coverage by EBMT, but also in better quality assessments by the human graders. Further, the results on Hong Kong news text show that the choice of language model does have a deﬁnite effect on quality. These results also conﬁrm the adage that there is no such thing as too much training text for language modeling, since the model generated from the EBMT corpus was unable to match the performance of the preexisting model generated from two orders of magnitude more text. 5. CONCLUSIONS AND FUTURE WORK As seen in Figure 2, the enhancements described here cumulatively provide a 12% absolute improvement in coverage for EBMT translations without requiring any additional knowledge resources. Further, the enhanced coverage does, in fact, result in improved translations, as veriﬁed by human judgements. We can also conclude that when we combine words into larger chunks on both sides of the corpus, the possibility of ﬁnding larger matches between the source language and the target language increases, which leads to the improvement of the translation quality for EBMT. We will do further research on the interaction between the improved segmenter, term ﬁnder and statistical dictionary builder, utilizing the information provided by the statistical dictionary as feedback for the segmenter and term ﬁnder to modify their results. We are also investigating the effects of splitting the EBMT training into multiple sets of topic-speciﬁc sentences, automatically separated using clustering techniques. The relatively low slope of the coverage curve also indicates that the training corpus is sufﬁciently large. Our prior experience with Spanish (using the UN Multilingual Corpus [5]) and French (using  Coverage of EBMT  0.8 0.75 0.7 0.65 0.6 0.55 0.5 0.45 0.4 0.35 0.3 0.25 0.2 0.15 0.1 0.05 0 0  Improved Segmenter, term finder, statDict Improved Segmenter, statDict Improved Segmenter Baseline system Trained on News tested On Legalcode  10  20  30  40  50  60  70  80  90  100  Percentage of corpus used for training (%)  Figure 2: EBMT Coverage with Varying Training  the Hansard corpus [7]) was that the curve ﬂattens out at between two and three million words of training text, which appears also to be the case for Chinese (each training slice contains approximately one million words of total text). We have not yet taken full advantage of the features of the EBMT software. In particular, it supports equivalence classes that permit generalization of the training text into templates for improved coverage. We intend to test automatic creation of equivalence classes from the training corpus [4] in conjunction with the other improvements reported herein. 6. ACKNOWLEDGEMENTS We would like to thank Alon Lavie and Lori Levin for their comments on drafts of this paper. 7. REFERENCES [1] R. D. Brown. Example-Based Machine Translation in the PANGLOSS System. In Proceedings of the Sixteenth International Conference on Computational Linguistics, pages 169–174, Copenhagen, Denmark, 1996. http://www.cs.cmu.edu/˜ralf/papers.html.  [2] R. D. Brown. Automated Dictionary Extraction for “Knowledge-Free” Example-Based Translation. In Proceedings of the Seventh International Conference on Theoretical and Methodological Issues in Machine Translation (TMI-97), pages 111–118, Santa Fe, New Mexico, July 1997. http://www.cs.cmu.edu/˜ralf/papers.html. [3] R. D. Brown. Adding Linguistic Knowledge to a Lexical Example-Based Translation System. In Proceedings of the Eighth International Conference on Theoretical and Methodological Issues in Machine Translation (TMI-99), pages 22–32, Chester, England, August 1999. http://www.cs.cmu.edu/˜ralf/papers.html. [4] R. D. Brown. Automated Generalization of Translation Examples. In Proceedings of the Eighteenth International Conference on Computational Linguistics (COLING-2000), pages 125–131, 2000. [5] D. Graff and R. Finch. Multilingual Text Resources at the Linguistic Data Consortium. In Proceedings of the 1994 ARPA Human Language Technology Workshop. Morgan Kaufmann, 1994.  Average phrase length (words)  
 Carnegie Mellon University, Pittsburgh, PA, USA Universita¨ t Karlsruhe, Fakulta¨ t fu¨ r Informatik, Karlsruhe, Germany  http://www.is.cs.cmu.edu/  tanja@cs.cmu.edu  1. INTRODUCTION Speech recognition has advanced considerably, but has been limited almost entirely either to situations in which close speaking microphones are natural and acceptable (telephone, dictation, command&control, etc.) or in which high-quality recordings are ensured. Furthermore, most recognition applications involve controlled recording environments, in which the user turns the recognition event on and off and speaks cooperatively for the purpose of being recognized. Unfortunately, the majority of situations in which humans speak with each other fall outside of these limitations. When we meet with others, we speak without turning on or off equipment, or we don’t require precise positioning vis a vis the listener. Recognition of speech during human encounters, or “meeting recognition”, therefore represents the ultimate frontier for speech recognition, as it forces robustness, knowledge of context, and integration in an environment and/or human experience. 2. CHALLENGES Over the last three years we have explored meeting recognition at the Interactive Systems Laboratories [5, 6, 7]. Meeting recognition is performed as one of the components of a “meeting browser”; a search retrieval and summarization tool that provides information access to unrestricted human interactions and encounters. The system is capable of automatically constructing a searchable and browsable audiovisual database of meetings. The meetings can be described and indexed in somewhat unorthodox ways, including by what has been said (speech), but also by who said it (speaker&face ID), where (face, pose, gaze, and sound source tracking), how (emotion tracking), and why, and other meta-level descriptions such as the purpose and style of the interaction, the focus of attention, the relationships between the participants, to name a few (see [1, 2, 3, 4]). The problem of speech recognition in unrestricted human meetings is formidable. Error rates for standard recognizers are 5-10 times higher than for dictation tasks. Our explorations based on LVCSR systems trained on BN, reveal that several types of mis- .  matches are to blame [6]: ¥ Mismatched and/or degraded recording conditions (remote, different microphone types), ¥ Mismatched dictionaries and language models (typically ideosynchratic discussions highly specialized on a topic of interest for a small group and therefore very different from other existing tasks), ¥ Mismatched speaking-style (informal, sloppy, multiple speakers talking in a conversational style instead of single speakers reading prepared text). In the following sections, we describe experiments and improvements based on our Janus Speech Recognition Toolkit JRTk [8] applied to transcribing meeting speech robustly. 3. EXPERIMENTAL SETUP As a ﬁrst step towards unrestricted human meetings each speaker is equipped with a clip-on lapel microphone for recording. By this choice interferences can be reduced but are not ruled out completely. Compared to a close-talking headset, there is signiﬁcant channel cross-talk. Quite often one can hear multiple speakers on a single channel. Since meetings consist of highly specialized topics, we face the problem of a lack of training data. Large databases are hard to collect and can not be provided on demand. As a consequence we have focused on building LVCSR systems that are robust against mismatched conditions as described above. For the purpose of building a speech recognition engine on the meeting task, we combined a limited set of meeting data with English speech and text data from various sources, namely Wall Street Journal (WSJ), English Spontaneous Scheduling Task (ESST), Broadcast News (BN), Crossﬁre and Newshour TV news shows. The meeting data consists of a number of internal group meeting recordings (about one hour long each), of which fourteen are used for experiments in this paper. A subset of three meetings were chosen as the test set. 4. SPEECH RECOGNITION ENGINE To achieve robust performance over a range of different tasks, we trained our baseline system on Broadcast News (BN). The system deploys a quinphone model with 6000 distributions sharing 2000 codebooks. There are about 105K Gaussians in the system. Vocal Tract Length Normalization and cluster-based Cepstral Mean Normalization are used to compensate for speaker and channel variations. Linear Discriminant Analysis is applied to reduce feature dimensionality to 42, followed by a diagonalization transform (Maximum Likelihood Linear Transform). A 40k vocabulary and trigram  System WER on Different Tasks [%]  BN (h4e98 1) F0-condition  9.6  BN (h4e98 1) all F-conditions  18.5  BN+ESST (h4e98 1) all F-conditions  18.4  Newshour  20.8  Crossﬁre  25.6  Improvements on Meeting Recognition  Baseline ESST system  54.1  Baseline BN system  44.2  + acoustic training BN+ESST  42.2  + language model interpolation (14 meetings)  39.0  Baseline BN system  + acoustic MAP Adaptation (10h meeting data)  40.4  + language model interpolation (14 meetings)  38.7  Table 1: Recognition Results on BN and Meeting Task  language model are used. The baseline language model is trained on the BN corpus. Our baseline system has been evaluated across the above mentioned tasks resulting in the word error rates shown in Table 1. While we achieve a ﬁrst pass WER of 18.5% on all F-conditions and 9.6% on the F0-conditions in the Broadcast News task, the word error rate of 44.2% on meeting data is quite high, reﬂecting the challenges of this task. Results on the ESST system [9] are even worse with a WER of 54.1% which results from the fact that ESST is a highly specialized system trained on noise-free but spontaneous speech in the travel domain. 4.1 Acoustic and Language Model Adaptation The BN acoustic models have been adapted to the meeting data thru Viterbi training, MLLR (Maximum Likelihood Linear Regression), and MAP (Maximum A Posteriori) adaptation. To improve the robustness towards the unseen channel conditions, speaking mode and training/test mismatch, we trained a system “BN+ESST” using a mixed training corpus. The comparison of the results indicate that the mixed system is more robust (44.2% ¦ 42.2%), without loosing the good performance on the original BN test set (18.5% vs. 18.4%). To tackle the lack of training corpus, we investigated linear interpolation of the BN and the meeting (MT) language model. Based on a cross-validation test we calculated the optimal interpolation weight and achieved a perplexity reduction of 21.5% relative compared to the MT-LM and more than 50% relative compared to the BN-LM. The new language model gave a signiﬁcant improvement decreasing the word error rate to 38.7%. Overall the error rate was reduced by §©¨  relative (44.2% ¦ 38.7%) compared to the BN baseline system. 4.2 Model Combination based Acoustic Mapping (MAM) For the experiments on meeting data reported above we have used comparable recording conditions as each speaker in the meeting has been wearing his or her own lapel microphone. Frequently however this assumption does not apply. We have also carried out experiments aimed at producing robust recognition when microphones are positioned at varying distances from the speaker. In this case data, speciﬁc for the microphone distance and SNR found in the test condition is unavailable. We therefore apply a new method, Model Combination based Acoustic Mapping (MAM) to the recognition of speech at different distances. MAM was originally pro-  posed for recognition in different car noise environments, please  refer to [10, 11] for details.  MAM estimates an acoustic mapping on the log-spectral domain  in order to compensate for noise condition mismatches between  tr aining and test. During training, the generic acoustic models  £  §"!#¨!$%%&!#')( and a model combination is  variable applied  noise model to get new  0 are estimated. generic models    1T£ hen ,   £32 0 , which correspond to noisy speech. During decoding of a  gsQ tievpeS.nR 4)iTn!hp1 eu£ t(s.4co,Inrthetehfmeorsaeepcapocinnhgd57ps698Atreo@©pc@ e4CsBEsisrDGerFqeH uc6Cio( rneissstarcucoclmtaespdsuiaﬁteccdcaotairosdniIna£g s  cUWalV£7cX`ulY aIte£ d  4Psc( o T re£b, a  where T1 £ ( .  T  refers to the mean vector: 4 1  a ﬁrst 4P( 
AT&T Communicator is a state-of-the-art speech-enabled telephony-based application that allows the end-user to, among other things, select and reserve airline itineraries. This experiment explores how the amount and structure of information presented in complex lists influences the user experience and the ability of subjects to successfully complete a selection task. Presenting all the relevant information needed for a decision at once was the factor that most positively influenced successful task completion and the user experience. Subjects preferred hearing all of the relevant information about each flight, without initiating additional dialog with the system. Additionally, successful task completion rates improved when all of the flights were presented at once, without any intervening questions from the system. Keywords dialogue, user interface, human factors, spoken language, lists, selection 1. INTRODUCTION AT&T Communicator is a state-of-the-art spoken dialogue system that allows the end-user to select and reserve various travel related resources, in particular, airfare, hotel, and rental cars [2]. One of its most challenging dialog modules is that for presenting  information that allows the user to select a desired air travel itinerary. While selecting between multiple possible itineraries in a visual domain is a relatively simple task because most of the selection criteria are listed in a single scrollable page, the same task is likely to have a much higher cognitive load in an audioonly domain [1, 3]. The selection criteria for each candidate flight must be presented to the user serially, leading to higher cognitive demands that could result in errors in itinerary selection.  A sample AT&T Communicator dialog with the prototype used in this experiment, showing the outbound flight only, follows. System: Hello, this is AT&T Communicator. How may I help with your travel plans?  User: Yeah, I’d like to fly from Newark to San Francisco.  System: OK, a flight from Newark, New Jersey to San Francisco, California. And will this be a round trip?  User:  Yes, and I need to arrive in San Francisco by 4 PM on November 22nd.  System: I found five outbound United Airlines flights.  Option 1 is flight number 635. It has one stop; it departs at 10:15 A.M. and arrives at 2:42 P.M. <pause>  Option 2 is flight number 7535. It has one stop; It departs at 12:20 P.M., and arrives at 4:37 P.M. <pause>  Option 3 is flight number 837. It' s a no-nstop flight; it departs at 2:00 P.M. and arrives at 5:01 P.M. <pause>  Would you like to hold any of these flights?  User: I guess I’ll take option one. System: Got it. Note that the second to the last system utterance, in which a set of itineraries are presented to the user for selection, is by far the longest system utterance in the dialog. This utterance includes a complex list of information. Unlike a more simple list, each list item (option) contains a number of sub-items, for example flight number, number of stops, etc. In early dialog evaluations of the Communicator system, the selection of a single itinerary from such a complex list of flights was a source of substantial usability problems; users frequently became frustrated. In fact, many users dropped out of the dialog before successfully selecting an itinerary. This paper describes an experiment in which we vary the amount and structure of information presented about available itineraries. We predict that the amount and structure of information presented affects the ability of users to successfully select the optimal itinerary within a set, and influences subjective measures such as user satisfaction. 2. METHODS AND PROCEDURES 2.1 Subjects Sixty-four subjects were run at a local shopping mall over a five day period. Subjects were recruited from the shoppers frequenting the mall. 2.2 Wizard of Oz A Wizard of Oz (WOZ) experiment was run to determine the optimal way for the end-user to select a desired itinerary in the Communicator project. A Wizard of Oz experiment is one in which no real automatic speech recognition (ASR) or natural language understanding (NLU) is used. Instead, the user interface is prototyped and a ‘wizard,’ or experimenter, acts in place of the ASR and NLU. Consequently, subjects believe that ASR/NLU is being used. The WOZ methodology allows competing user interface strategies to be prototyped and tested with end users in a shorter period of time than would be required to implement multiple fully-functioning systems with competing user interfaces. 2.3 Apparatus & Materials Relevant aspects of the AT&T Communicator user interface were prototyped using the Unisys Natural Language Speech Assistant (NLSA) software. NLSA runs on a PC using the Windows NT operating system. Subjects called into the Communicator prototype using an analog telephone and interacted with the system by voice. The wizard categorized the subject’s speech using the NLSA Wizard graphical user interface (GUI). Each subject completed 5 surveys in pen and paper format. During the course of the experiment, subjects also had access to a pad of paper. 2.4 Experimental Design All itineraries presented to the subjects were round-trip. 2.4.1 Independent Variables This was a factorial experiment with two factors, one factor between subjects and the other within subject (see Table 1).  Selection Itinerary Content. There were two levels of this between subjects factor: --Terse. The presented itineraries included: airline, number of stops, and departure time1. In order to get additional information, the user could ask the system questions (e.g. “When does that flight arrive?”). --Verbose. The presented itineraries included: airline, flight number, number of stops, departure time, and arrival time. All the information relevant to the tasks specified in the experiment are presented about each flight; the user did not need to ask questions to get additional information. Number of Flights Before Question. Each level is actually a combination of two separate, but related, factors. --Combined vs. Separate. Whether outbound and return flights are presented separately or in combination. --Number of flights. The number of flights that are presented before asking the subject to make a decision. Four levels of this factor were chosen. In all cases (1) the total number of flights ‘found’ was 5, and, (2) the question was, “Would you like to hold [that flight/any of those flights]?”. --Separate 1. The outbound and return flights of the trip are presented separately and after each flight the subject is asked the question. --Separate 3. The outbound and return flights of the trip are presented separately and after the third flight the subject is asked the question. --Separate 5. The outbound and return flights of the trip are presented separately and after the last flight the subject is asked the question. --Combined. The outbound and return flights of the trip are presented at the same time and after each set of two flights the subject is asked the question. Table 1: Factors used in this experiment.  Selection Itinerary Content (Between)  Terse  Verbose  Outbound  # of  / Return  Flights  Before  Question  (Within)  Separate  
Annotation graphs provide an efﬁcient and expressive data model for linguistic annotations of time-series data. This paper reports progress on a complete software infrastructure supporting the rapid development of tools for transcribing and annotating time-series data. This general-purpose infrastructure uses annotation graphs as the underlying model, and allows developers to quickly create special-purpose annotation tools using common components. An application programming interface, an I/O library, and graphical user interfaces are described. Our experience has shown us that it is a straightforward task to create new special-purpose annotation tools based on this general-purpose infrastructure. Keywords transcription, coding, annotation graph, interlinear text, dialogue annotation 1. INTRODUCTION Annotation graphs (AGs) provide an efﬁcient and expressive data model for linguistic annotations of time-series data [2]. This paper reports progress on a complete software infrastructure supporting the rapid development of tools for transcribing and annotating time-series data. This general-purpose infrastructure uses annotation graphs as the underlying model, and allows developers to quickly create special-purpose annotation tools using common components. This work is being done in cooperation with the developers of other widely used annotation systems, Transcriber and Emu [1, 3]. The infrastructure is being used in the development of a series of annotation tools at the Linguistic Data Consortium. Several such tools are shown in the paper: one for dialogue annotation, one for telephone conversation transcription, and one for interlinear transcription aligned to speech. This paper will cover the following points: the application programming interfaces for manipulating annotation graph data and importing data from other formats; the model of inter-component .  communication which permits easy reuse of software components; and the design of the graphical user interfaces, which have been tailored to be maximally ergonomic for the tasks. The project homepage is: [http://www.ldc.upenn.edu/ AG/]. The software tools and software components described in this paper are available through a CVS repository linked from this homepage. 2. ARCHITECTURE 2.1 General Architecture Existing annotation tools are based on a two level model (Figure 1 Top). The systems we demonstrate are based around a three level model, in which annotation graphs provide a logical level independent of application and physical levels (Figure 1 Bottom). The application level represents special-purpose tools built on top of the general-purpose infrastructure at the logical level. The system is built from several components which instantiate this model. Figure 2 shows the architecture of the tools currently being developed. Annotation tools, such as the ones discussed below, must provide graphical user interface components for signal visualization and annotation. The communication between components is handled through an extensible event language. An application programming interface for annotation graphs (AG-API) has been developed to support well-formed operations on annotation graphs. This permits applications to abstract away from ﬁle format issues, and deal with annotations purely at the logical level. 2.2 The Annotation Graph API The complete IDL deﬁnition of the AG-API is provided in the appendix (also online). Here we describe a few salient features of the API. The API provides access to internal objects (signals, anchors, annotations etc) using identiﬁers. Identiﬁers are strings which contain internal structure. For example, an AG identiﬁer is qualiﬁed with an AGSet identiﬁer: AGSetId:AGId. Annotations and anchors are doubly qualiﬁed: AGSetId:AGId:AnnotationId, AGSetId:AGId:AnchorId. Thus, it is possible to determine from any given identiﬁers, its membership in the overall data structure. The functioning of the API will now be illustrated with a series of examples. Suppose we have already constructed an AG and now wish to create a new anchor. We might have the following API call: CreateAnchor( "agSet12:ag5", 15.234, "sec" ); This call would construct a new anchor object and return its identiﬁer: agSet12:ag5:anchor34. Alternatively, if we already  Application Level  Visualization & Exploration  Extraction Systems Conversion Tools  Annotation Tools  Query Systems  Evaluation Software Automatic Aligners  Physical Level  RDB Format  XML  Tab delimited flat files  Application Level  Visualization & Exploration  Extraction Systems Conversion Tools  Annotation Tools  Query Systems  Evaluation Software Automatic Aligners  Logical Level Physical Level  AG-API  RDB Format  XML  Tab delimited flat files  Figure 1: The Two and Three-Level Architectures for Speech Annotation  have an anchor identiﬁer that we wish to use for this new anchor (e.g. because we are reading previously created annotation data from a ﬁle and do not wish to assign new identiﬁers), then we could have the following API call: CreateAnchor( "agset12:ag5:anchor34", 15.234, "sec" ); This call will return agset12:ag5:anchor34. Once a pair of anchors have been created it is possible to create an annotation which spans them: CreateAnnotation( "agSet12:ag5", "agSet12:ag5:anchor34", "agSet12:ag5:anchor35", "phonetic" ); This call will construct an annotation object and return an identiﬁer for it, e.g. agSet12:ag5:annotation41. We can now add features to this annotation: SetFeature( "agSet12:ag5:annotation41", "date", "1999-07-02" ); The implementation maintains indexes on all the features, and also on the temporal information and graph structure, permitting efﬁcient search using a family of functions such as: GetAnnotationSetByFeature( "agSet12:ag5", "date", "1999-07-02" ); 2.3 A File I/O Library A ﬁle I/O library (AG-FIO) to support creation and export of AG data has been developed. This will eventually handle all widely used annotation formats. Formats currently supported by the AGFIO library include the TIMIT, BU, Treebank, AIF (ATLAS Interchange Format), Switchboard and BAS Partitur formats. 2.4 Inter-component Communication Figure 3 shows the structure of an annotation tool in terms of components and their inter-communications. Main program - a small script  Figure 2: Architecture for Annotation Systems  AG-GUI-API Waveform display AG-GUI-API Transcription editor  AG-FIO-API File input / output 
We present the technique of Virtual Annotation as a specialization of Predictive Annotation for answering definitional What is questions. These questions generally have the property that the type of the answer is not given away by the question, which poses problems for a system which has to select answer strings from suggested passages. Virtual Annotation uses a combination of knowledge-based techniques using an ontology, and statistical techniques using a large corpus to achieve high precision. Keywords Question-Answering, Information Retrieval, Ontologies 1. INTRODUCTION Question Answering is gaining increased attention in both the commercial and academic arenas. While algorithms for general question answering have already been proposed, we find that such algorithms fail to capture certain subtleties of particular types of questions. We propose an approach in which different types of questions are processed using different algorithms. We introduce a technique named Virtual Annotation (VA) for answering one such type of question, namely the What is question. We have previously presented the technique of Predictive Annotation (PA) [Prager, 2000], which has proven to be an effective approach to the problem of Question Answering. The essence of PA is to index the semantic types of all entities in the corpus, identify the desired answer type from the question, search for passages that contain entities with the desired answer type as well as the other query terms, and to extract the answer term or phrase. One of the weaknesses of PA, though, has been in dealing with questions for which the system cannot determine the correct answer type required. We introduce here an extension to PA which we call Virtual Annotation and show it to be effective for those “What is/are (a/an) X” questions that are seeking hypernyms of X. These are a type of definition question, which other QA systems attempt to answer by searching in the document collection for textual clues similar to those proposed by [Hearst, 1998], that  are characteristic of definitions. Such an approach does not use the strengths of PA and is not successful in the cases in which a deeper understanding of the text is needed in order to identify the defining term in question. We first give a brief description of PA. We look at a certain class of What is questions and describe our basic algorithm. Using this algorithm we develop the Virtual Annotation technique, and evaluate its performance with respect to both the standard TREC and our own benchmark. We demonstrate on two question sets that the precision improves from .15 and .33 to .78 and .83 with the addition of VA. 2. BACKGROUND For our purposes, a question-answering (QA) system is one which takes a well-formed user question and returns an appropriate answer phrase found in a body of text. This generally excludes How and Why questions from consideration, except in the relatively rare cases when they can be answered by simple phrases, such as “by fermenting grapes” or “because of the scattering of light”. In general, the response of a QA system will be a named entity such as a person, place, time, numerical measure or a noun phrase, optionally within the context of a sentence or short paragraph. The core of most QA systems participating in TREC [TREC8, 2000 & TREC9, 2001] is the identification of the answer type desired by analyzing the question. For example, Who questions seek people or organizations, Where questions seek places, When questions seek times, and so on. The goal, then, is to find an entity of the right type in the text corpus in a context that justifies it as the answer to the question. To achieve this goal, we have been using the technique of PA to annotate the text corpus with semantic categories (QA-Tokens) prior to indexing. Each QA-Token is identified by a set of terms, patterns, or finite-state machines defining matching text sequences. Thus “Shakespeare” is annotated with “PERSON$”, and the text string “PERSON$” is indexed at the same text location as “Shakespeare”. Similarly, “$123.45” is annotated with “MONEY$”. When a question is processed, the desired QAToken is identified and it replaces the Wh-words and their auxiliaries. Thus, “Who” is replaced by “PERSON$”, and “How much” + “cost” are replaced by “MONEY$”. The resulting query is then input to the search engine as a bag of words. The expectation here is that if the initial question were “Who wrote Hamlet”, for example, then the modified query of “PERSON$ write Hamlet” (after lemmatization) would be a  perfect match to text that states “Shakespeare wrote Hamlet” or “Hamlet was written by Shakespeare”. The modified query is matched by the search engine against passages of 1-2 sentences, rather than documents. The top 10 passages returned are processed by our Answer Selection module which re-annotates the text, identifies all potential answer phrases, ranks them using a learned evaluation function and selects the top 5 answers (see [Radev et al., 2000]). The problem with “What is/are (a/an) X” questions is that the question usually does not betray the desired answer type. All the system can deduce is that it must find a noun phrase (the QAToken THING$). The trouble with THING$ is that it is too general and labels a large percentage of the nouns in the corpus, and so does not help much in narrowing down the possibilities. A second problem is that for many such questions the desired answer type is not one of the approximately 50 high-level classes (i.e. QATokens) that we can anticipate at indexing; this phenomenon is seen in TREC9, whose 24 definitional What is questions are listed in the Appendix. These all appear to be calling out for a hypernym. To handle such questions we developed the technique of Virtual Annotation which is like PA and shares much of the same machinery, but does not rely on the appropriate class being known at indexing time. We will illustrate with examples from the animal kingdom, including a few from TREC9.  3. VIRTUAL ANNOTATION If we look up a word in a thesaurus such as WordNet [Miller et al., 1993]), we can discover its hypernym tree, but there is no indication which hypernym is the most appropriate to answer a What is question. For example, the hypernym hierarchy for “nematode” is shown in Table 1. The level numbering counts levels up from the starting term. The numbers in parentheses will be explained later. Table 1. Parentage of “nematode” according to WordNet.  Level 0 1 2 3 4 5  Synset {nematode, roundworm} {worm(13)} {invertebrate} {animal(2), animate being, beast, brute, creature, fauna} {life form(2), organism(3), being, living thing} {entity, something}  At first sight, the desirability of the hypernyms seems to decrease with increasing level number. However, if we examine “meerkat” we find the hierarchy in Table 2. We are leaving much unsaid here about the context of the question and what is known of the questioner, but it is not unreasonable to assert that the “best” answer to “What is a meerkat” is either “a mammal” (level 4) or “an animal” (level 7). How do we get an automatic system to pick the right candidate?  Table 2. Parentage of “meerkat” according to WordNet  Level 0 1 2 3 4 5 6 7 8 9  Synset {meerkat, mierkat} {viverrine, viverrine mammal} {carnivore} {placental, placental mammal, eutherian, eutherian mammal} {mammal} {vertebrate, craniate} {chordate} {animal(2), animate being, beast, brute, creature, fauna} {life form, organism, being, living thing} {entity, something}  It seems very much that what we would choose intuitively as the best answer corresponds to Rosch et al.’s basic categories [Rosch et al., 1976]. According to psychological testing, these are categorization levels of intermediate specificity that people tend to use in unconstrained settings. If that is indeed true, then we can use online text as a source of evidence for this tendency. For example, we might find sentences such as “… meerkats and other Y … ”, where Y is one of its hypernyms, indicating that Y is in some sense the preferred descriptor. We count the co-occurrences of the target search term (e.g. “meerkat” or “nematode”) with each of its hypernyms (e.g. “animal”) in 2-sentence passages, in the TREC9 corpus. These counts are the parenthetical numbers in Tables 1 and 2. The absence of a numerical label there indicates zero co-occurrences. Intuitively, the larger the count, the better the corresponding term is as a descriptor. 3.1 Hypernym Scoring and Selection Since our ultimate goal is to find passages describing the target term, discovering zero co-occurrences allows elimination of useless candidates. Of those remaining, we are drawn to those with the highest counts, but we would like to bias our system away from the higher levels. Calling a nematode a life-form is correct, but hardly helpful. The top levels of WordNet (or any ontology) are by definition very general, and therefore are unlikely to be of much use for purposes of definition. However, if none of the immediate parents of a term we are looking up co-occur in our text corpus, we clearly will be forced to use a more general term that does. We want to go further, though, in those cases where the immediate parents do occur, but in small numbers, and the very general parents occur with such high frequencies that our algorithm would select them. In those cases we introduce a tentative level ceiling to prevent higher-level terms from being chosen if there are suitable lower-level alternatives. We would like to use a weighting function that decreases monotonically with level distance. Mihalcea and Moldovan [1999], in an analogous context, use the logarithm of the number of terms in a given term’s subtree to calculate weights, and they claim to have shown that this function is optimal. Since it is approximately true that the level population increases  exponentially in an ontology, this suggests that a linear function of level number will perform just as well. Our first step is to generate a level-adapted count (LAC) by dividing the co-occurrence counts by the level number (we are only interested in levels 1 and greater). We then select the best hypernym(s) by using a fuzzy maximum calculation. We locate the one or more hypernyms with greatest LAC, and then also select any others with a LAC within a predefined threshold of it; in our experimentation we have found that a threshold value of 20% works well. Thus if, for example, a term has one hypernym at level 1 with a count of 30, and another at level 2 with a count of 50, and all other entries have much smaller counts, then since the LAC 25 is within 20% of the LAC 30, both of these hypernyms will be proposed. To prevent the highest levels from being selected if there is any alternative, we tentatively exclude them from consideration according to the following scheme: If the top of the tree is at level N, where N <= 3, we set a tentative ceiling at N-1, otherwise if N<=5, we set the ceiling at N-2, otherwise we set the ceiling at N-3. If no co-occurrences are found at or below this ceiling, then it is raised until a positive value is found, and the corresponding term is selected. If no hypernym at all co-occurs with the target term, then this approach is abandoned: the “What” in the question is replaced by “THING$” and normal procedures of Predictive Annotation are followed. When successful, the algorithm described above discovers one or more candidate hypernyms that are known to co-occur with the target term. There is a question, though, of what to do when the question term has more than one sense, and hence more than one ancestral line in WordNet. We face a choice of either selecting the hypernym(s) with the highest overall score as calculated by the algorithm described above, or collecting together the best hypernyms in each parental branch. After some experimentation we made the latter choice. One of the questions that benefitted from this was “What is sake”. WordNet has three senses for sake: good (in the sense of welfare), wine (the Japanese drink) and aim/end, with computed scores of 122, 29 and 87/99 respectively. It seems likely (from the phrasing of the question) that the “wine” sense is the desired one, but this would be missed entirely if only the top-scoring hypernyms were chosen. We now describe how we arrange for our Predictive Annotation system to find these answers. We do this by using these descriptors as virtual QA-Tokens; they are not part of the search engine index, but are tagged in the passages that the search engine returns at run time. 3.2 Integration Let us use H to represent either the single hypernym or a disjunction of the several hypernyms found through the WordNet analysis. The original question Q = “What is (a/an) X” is converted to Q’= “DEFINE$ X H” where DEFINE$ is a virtual QA-Token that was never seen at indexing time, does not annotate any text and does not occur in the  index. The processed query Q’ then will find passages that contain occurrences of both X and H; the token DEFINE$ will be ignored by the search engine. The top passages returned by the search engine are then passed to Answer Selection, which reannotates the text. However, this time the virtual QA-Token DEFINE$ is introduced and the patterns it matches are defined to be the disjuncts in H. In this way, all occurrences of the proposed hypernyms of X in the search engine passages are found, and are scored and ranked in the regular fashion. The end result is that the top passages contain the target term and one of its most frequently co-occurring hypernyms in close proximity, and these hypernyms are selected as answers. When we use this technique of Virtual Annotation on the aforementioned questions, we get answer passages such as “Such genes have been found in nematode worms but not yet in higher animals.” and “South African golfer Butch Kruger had a good round going in the central Orange Free State trials, until a mongoose-like animal grabbed his ball with its mouth and dropped down its hole. Kruger wrote on his card: "Meerkat."” 4 RESULTS 4.1 Evaluation We evaluated Virtual Annotation on two sets of questions – the definitional questions from TREC9 and similar kinds of questions from the Excite query log (see http://www.excite.com). In both cases we were looking for definitional text in the TREC corpus. The TREC questions had been previously verified (by NIST) to have answers there; the Excite questions had no such guarantee. We started with 174 Excite questions of the form “What is X”, where X was a 1- or 2-word phrase. We removed those questions that we felt would not have been acceptable as TREC9 questions. These were questions where: o The query terms did not appear in the TREC corpus, and some may not even have been real words (e.g. “What is a gigapop”).1 37 questions. o The query terms were in the corpus, but there was no definition present (e.g “What is a computer monitor”).2 18 questions. o The question was not asking about the class of the term but how to distinguish it from other members of the class (e.g. “What is a star fruit”). 17 questions. o The question was about computer technology that emerged after the articles in the TREC corpus were written (e.g. “What is a pci slot”). 19 questions. o The question was very likely seeking an example, not a definition (e.g. “What is a powerful adhesive”). 1 question plus maybe some others – see the Discussion 
AETHRA Ancona, Italy  1. INTRODUCTION NESPOLE! 1 is a speech-to-speech machine translation research project funded jointly by the European Commission and the US NSF. The main goal of the NESPOLE! project is to advance the state-of-the-art of speech-to-speech translation in a real-world setting of common users involved in e-commerce applications. The project is a collaboration between three European research labs (IRST in Trento Italy, ISL at University of Karlsruhe in Germany, CLIPS at UJF in Grenoble France), a US research group (ISL at Carnegie Mellon in Pittsburgh) and two industrial partners (APT - the Trentino provincial tourism bureau, and Aethra - an Italian tele-communications commercial company). The speech-to-speech translation approach taken by the project builds upon previous work that the research partners conducted within the context of the CSTAR consortium (see http://www.c-star.org). The prototype system developed in NESPOLE! is intended to provide effective multi-lingual speech-to-speech communication between all pairs of four languages (Italian, German, French and English) within broad, but yet restricted domains. The ﬁrst showcase currently under development is in the domain of tourism and travel information. The NESPOLE! speech translation system is designed to be an integral part of advanced e-commerce technology of the next generation. We envision a technological scenario in which multi-modal (speech, video and gesture) interaction plays a signiﬁcant role, in addition to the passive browsing of pre-designed web pages as is common in e-commerce today. The interaction between client and provider will need to support online communication with agents (both real and artiﬁcial) on the provider side. The language barrier then becomes a signiﬁcant obstacle for such online communication between the two parties, when they do not speak a common language. Within the tourism and travel domain, one can imagine a scenario in which users (the clients) are planning a recreational trip and are searching for speciﬁc detailed information about the  1NESPOLE! - NEgotiating through SPOken Lan-  guage in E-commerce.  See the project website at  http://nespole.itc.it/  .  regions they wish to visit. Initial general information is obtained from a web site of a tourism information provider. When more detailed or special information is required, the customer has the option of opening an online video-conferencing connection with a human agent of the tourism information provider. Speech translation is integrated within the video-conference connection; the two parties each speak in their native language and hear the synthesized translation of the speech of the other participant. Text translation (in the form of subtitles) can also be provided. Some multi-modal communication between the parties is also available. The provider agent can send web pages to the display of the customer, and both sides can annotate and refer to pictures and diagrams presented on a shared whiteboard application. In this paper we describe the design considerations behind the architecture that we have developed for the NESPOLE! speech translation system in the scenario described above. In order to make the developed prototype as realistic as possible for use by a common user, we assume only minimal hardware and software is available on the customer side. This does include a PC-type video camera, commercially available internet video-conferencing software (such as Microsoft Netmeeting), standard audio and video hardware and a standard web browser. However, no speech recognition and/or translation software is assumed to reside locally on the PC of the customer. This implies a server-type architecture in which speech recognition and translation are accomplished via interaction with a dedicated server. The extent to which this server is centralized or distributed is one of the major design considerations taken into account in our system. 2. NESPOLE! INTERLINGUA-BASED TRANSLATION APPROACH Our translation approach builds upon previous work that we have conducted within the context of the C-STAR consortium. We use an interlingua-based approach with a relatively shallow task-oriented interlingua representation [2] [1], that was initially designed for the C-STAR consortium and has been signiﬁcantly extended for the NESPOLE! project. Interlingual machine translation is convenient when more than two languages are involved because it does not require each language to be connected by a set of transfer rules to each other language in each direction [3]. Adding a new language that has all-ways translation with existing languages requires only writing one analyzer that maps utterances into the interlingua and one generator that maps interlingua representations into sentences. The interlingua approach also allows each partner group to implement an analyzer and generator for its home language only. A fur-  Figure 1: General Architecture of NESPOLE! System  ther advantage is that it supports a paraphrase generation back into the language of the speaker. This provides the user with some control in case the analysis of an utterance failed to produce a correct interlingua. The following are three examples of utterances tagged with their corresponding interlingua representation: Thank you very much c:thank And we’ll see you on February twelfth. a:closing (time=(february, md12)) On the twelfth we have a single and a double available. a:give-information+availability+room (room-type=(single & double),time=(md12)) 3. NESPOLE! SYSTEM ARCHITECTURE DESIGN Several main considerations were taken into account in the design of the NESPOLE! Human Language Technology (HLT) server architecture: (1) The desire to cleanly separate the actual HLT system from the communication channel between the two parties, which makes use of the speech translation capabilities provided by  the HLT system; (2) The desire to allow each research site to independently develop its language speciﬁc analysis and generation modules, and to allow each site to easily integrate new and improved components into the global NESPOLE! HLT system; and (3) The desire of the research partners to build to whatever extent possible upon software components previously developed in the context of the C-STAR consortium. We will discuss the extent to which the designed architecture achieves these goals after presenting an overview of the architecture itself. Figure 1 shows the general architecture of the current NESPOLE! system. Communication between the client and agent is facilitated by a dedicated module - the Mediator. This module is designed to control the video-conferencing connection between the client and the agent, and to integrate the speech translation services into the communication. The mediator handles audio and video data associated with the video-conferencing application and binary data associated with a shared whiteboard application. Standard H.323 data formats are used for these three types of data transfer. Speechto-speech translation of the utterances captured by the mediator is accomplished through communication with the NESPOLE! global HLT server. This is accomplished via socket connections with language-speciﬁc HLT servers. The communication between the mediator and each HLT server consists mainly of linear PCM audio packets (some text and control messages are also supported and are described later in this section).  . Communication with Mediator  audio  audio  Speech Recognizer text Parser/Analysis Module IF Analysis Chain  Speech Synthsizer text Generation Module IF Generation Chain  Communication with CommSwitch Language X HLT Server  Figure 2: Architecture of NESPOLE! Language-speciﬁc HLT Servers  The global NESPOLE! HLT server comprises four separate language-speciﬁc servers. Additional language-speciﬁc HLT servers can easily be integrated in the future. The internal architecture of each language-speciﬁc HLT server is shown in ﬁgure 2. Each language-speciﬁc HLT server consists of an analysis chain and a generation chain. The analysis chain receives an audio stream corresponding to a single utterance and performs speech recognition followed by parsing and analysis of the input utterance into the interlingua representation (IF). The interlingua is then transmitted to a central HLT communication switch (the CS), that forwards it to the HLT servers for the other languages as appropriate. IF messages received from the central communication switch are processed by the generation chain. A generation module ﬁrst generates text in the target language from the IF. The text utterance is then sent to a speech synthesis module that produces an audio stream for the utterance. The audio is then communicated externally to the mediator, in order to be integrated back into the video-conferencing stream between the two parties. The mediator can, in principle, support multiple one-to-one communication sessions between client and agent. However, the design supports multiple mediators, which, for example, could each be dedicated to a different provider application. Communication with the mediator is initiated by the client by an explicit action via the web browser. This opens a communication channel to the mediator, which contacts the agent station, establishes the videoconferencing connection between client and agent, and starts the whiteboard application. The speciﬁc pair of languages for a dialogue is determined in advance from the web page from which the client initiates the communication. The mediator then establishes a socket communication channel with the two appropriate language speciﬁc HLT servers. Communication between the two language  speciﬁc HLT servers, in the form of IF messages, is facilitated by the NESPOLE! global communication switch (the CS). The language speciﬁc HLT servers may in fact be physically distributed over the internet. Each language speciﬁc HLT server is set to service analysis requests coming from the mediator side, and generation requests arriving from the CS. Some further functionality beyond that described above is also supported. As described earlier, the ability to produce a textual paraphrase of an input utterance and to display it back to the original speaker provides useful user control in the case of translation failures. This is supported in our system in the following way. In addition to the translated audio, each HLT server also forwards the generated text in the output language to the mediator, which then displays the text on a dedicated application window on the PC of the target user. Additionally, at the end of the processing of an input utterance by the analysis chain of an HLT server, the resulting IF is passed internally to the generation chain, which produces a text generation from the IF. The result is a textual paraphrase of the input utterance in the source language. This text is then sent back to the mediator, which forwards it to the party from which the utterance originated. The paraphrase is then displayed to the original speaker in the dedicated application window. If the paraphrase is wrong, it is likely that the produced IF was incorrect, and thus the translation would also be wrong. The user may then use a button on the application interface to signal that the last displayed paraphrase was wrong. This action triggers a message that is forwarded by the mediator to the other party, indicating that the last displayed translation should be ignored. Further functionality is planned to support synchronization between multi-modal events on the whiteboard and their corresponding speech actions. As these are in very preliminary stages of planning we do not describe them here.  4. DISCUSSION AND CONCLUSIONS We believe that the architectural design described above has several strengths and advantages. The clean separation of the HLT server dedicated to the speech translation services from the external communication modules between the two parties allows the research partners to develop the HLT modules with a large degree of independence. Furthermore, this separation will allow us in the future to explore other types of mediators for different types of applications. One such application being proposed for development within the C-STAR consortium is a speech-to-speech translation service over mobile phones. The HLT server architecture described here would be able to generally support such alternative external communication modalities as well. The physical distribution of the individual language speciﬁc HLT servers allows each site to independently develop, integrate and test its own analysis and generation modules. The organization of each language speciﬁc HLT server as an independent module allows each of the research sites to develop its unique approaches to analysis and generation, while adhering to a simple communication protocol between the HLT servers and externally with the mediator. This allowed the research partners to “jump-start” the project with analysis and generation modules previously developed for the CSTAR consortium, and incrementally develop these modules over time. Furthermore, the global NESPOLE! communication switch (the CS) supports testing of analysis and generation among the four languages in isolation from the external parts of the system. Currently, requests for analysis of a textual utterance can be transmitted to the HLT servers via the CS, with the resulting IF sent (via the CS) to all HLT servers for generation. This gives us great ﬂexibility in developing and testing our translation system. The functionality of the CS was originally developed for our previous C-STAR project, and was reused with little modiﬁcation. Support for additional languages is also very easy to incorporate into the system by adding new language-speciﬁc HLT servers. Any new language speciﬁc HLT server needs only to adhere to the communication protocols with both the global NESPOLE! communication switch (the CS) and the external mediator. The C-STAR consortium plans to use the general architecture described here for its next phase of collaboration, with support for at least three asian languages (Japanese, Korean and Chinese) in addition to the languages currently covered by the NESPOLE! project. 
Assuming that the goal of a person name query is to find references to a particular person, we argue that one can derive better relevance scores using probabilities derived from a language model of personal names than one can using corpus based occurrence frequencies such as inverse document frequency (idf). We present here a method of calculating person name match probability using a language model derived from a directory of legal professionals. We compare how well name match probability and idf predict search precision of word proximity queries derived from names of legal professionals and major league baseball players. Our results show that name match probability is a better predictor of relevance than idf. We also indicate how rare names with high match probability can be used as virtual tags within a corpus to identify effective collocation features for person names within a professional class. 1. INTRODUCTION Some of the most common types of queries submitted to search engines both on the internet and on proprietary text search systems consist simply of a person’s name. To improve the way such queries are handled, it would be useful if search engines could estimate the likelihood or belief that a name contained in a document pertains to the name in the query. Traditionally, relevance likelihood for name phrases has been based on inverse document frequency or idf, [3][4]. The idea behind this relevance estimate is that names which rarely occur in the corpus are thought to be more indicative of relevance than names that commonly occur. Assuming that the goal of a person name query is to find references to a particular person, we argue that one can derive better relevance scores using probabilities derived from a language model of personal names than one can using corpus based occurrence frequencies. The reason for this is that finding references to a particular person in text is more dependent upon the relative rarity of the name with respect to the human population than it is on the rarity of the name within a corpus. To get an intuitive idea of this point, consider that, within a corpus of 27,000 Wall Street Journal articles published between  January and August of the year 2000, the name “Trent Lott” occurred in 80 documents while the name “John Smith” occurred in 24. All 80 references to “Trent Lott” referred to the majority leader of the U.S. Senate, while “John Smith” references mapped to 5 different people. This is not surprising. From our experience, we know that “Trent Lott” is an uncommon name and “John Smith” is a common one. We present here evidence that name match probability based on a language model predicts relevance for name queries far better than idf. It may be argued that idf was never intended to be used to measure the relative ambiguity of a name query. However, idf is the standard measure used in probabilistic search engines to measure the degree of relevance terms and phrases within a collection have to the terms and phrases in queries, [1] [5]. For this reason, we take idf to be the standard against which to compare name match probability. Being able to predict relevance through name match probabilities enables us to do three things. First, it tells us when we need to add information to the query to improve precision either by prompting the user for it or automatically expanding the query. Second, and perhaps more importantly, it enables us to use names with high match probabilities as virtual tags that can help us find useful collocation features to disambiguate names within a given class of names, such as the names of attorneys and judges. For purposes of this paper, we define an ambiguous name as one likely to be shared by many people and an unambiguous name as one likely to apply to a single person or to only a few people. And third, match probability can be used as a feature within a name search operator to improve search precision. 2. DESCRIPTION OF MATCH PROBABILITY CALCULATION FOR PERSON NAMES The motivation for our work is an effort to develop a name search operator to find attorneys and judges in the news. In our particular application, we wish to allow users to search for newspaper references to attorneys and judges listed in a directory of U.S. legal professionals. This directory contains the curriculum vitae of approximately one million people. In this section, we show how we calculate person name match probability. To compute the probability of relevance or match probability for a name, we perform three steps. First, we compute a probability distribution for the first and last names in our name directory. This is our language model. Second, we compute a name’s probability by multiplying its first name probability with its last  name probability. Third, we compute its match probability by taking the reciprocal of the product of the name probability and the size of the human population likely to be referenced in the corpus. For our Wall Street Journal test corpus, we estimated this size to be approximately the size of the U.S. population or 300 million. Formulas for the three steps are shown below. (1) P ( first _ name ) = F N P (last _ name ) = L N where F = number of occurrences of first name, L = number of occurrences of last name, and N = number of names in the directory.  (2) P ( name ) = P ( first _ name ) ⋅ P (last _ name )  (3)  P (name  _  match  )  =  (H  ⋅  
One of the central issues for information extraction is the cost of customization from one scenario to another. Research on the automated acquisition of patterns is important for portability and scalability. In this paper, we introduce Tree-Based Pattern representation where a pattern is denoted as a path in the dependency tree of a sentence. We outline the procedure to acquire Tree-Based Patterns in Japanese from un-annotated text. The system extracts the relevant sentences from the training data based on TF/IDF scoring and the common paths in the parse tree of relevant sentences are taken as extracted patterns. Keywords Information Extraction, Pattern Acquisition 1. INTRODUCTION Information Extraction (IE) systems today are commonly based on pattern matching. New patterns need to be written when we customize an IE system for a new scenario (extraction task); this is costly if done by hand. This has led to recent research on automated acquisition of patterns from text with minimal pre-annotation. Riloff [4] reported a successful result for her procedure that needs only a pre-classiﬁed corpus. Yangarber [6] developed a procedure for unannotated natural language texts. One of their common assumption is that the relevant documents include good patterns. Riloff implemented this idea by applying the pre-deﬁned heuristic rules to pre-classiﬁed (relevant) documents and Yangarber advanced further so that the system can classify the documents by itself given seed patterns speciﬁc to a scenario and then ﬁnd the best patterns from the relevant document set. Considering how they represent the patterns, we can see that, in general, Riloff and Yangarber relied on the sentence structure of English. Riloff’s predeﬁned heuristic rules are based on syntactic structures, such as “ subj active-verb” and “active-verb .  dobj ”. Yangarber used triples of a predicate and some of its arguments, such as “ pred subj obj ”. The Challenges Our careful examination of Japanese revealed some of the challenges for automated acquisition of patterns and information extraction on Japanese(-like) language and other challenges which arise regardless of the languages. Free Word-ordering Free word order is one of the most signiﬁcant problems in analyzing Japanese. To capture all the possible patterns given a predicate and its arguments, we need to permute the arguments and list all the patterns separately. For example, for “ subj dobj iobj predicate ” with the constraint that the predicate comes last in the sentence, there would be six possible patterns (permutations of three arguments). The number of patterns to cover even simple facts would rise unacceptably high. Flexible case marking system There is also a difﬁculty in a language with a ﬂexible case marking system, like Japanese. In particular, we found that, in Japanese, some of the arguments that are usually marked as object in English were variously marked by different post-positions, and some case markers (postpositions) are used for marking more than one grammatical category in different situations. For example, the topic marker in Japanese, “wa”, can mark almost any entity that would have been variously marked in English. It is difﬁcult to deal with this variety by simply ﬁxing the number of arguments of a predicate for creating patterns in Japanese. Relationships beyond direct predicate-argument Furthermore, we may want to capture the relationship between a predicate and a modiﬁer of one of its arguments. In previous approaches, one had to introduce an ad hoc frame for such a relationship, such as “verb obj [PP head-noun ]”, to extract the relationship between “to assume” and “ organization ” in the sentence “ person will assume the post of organization ”. Relationships beyond clausal boundaries Another problem lies in relationships beyond clause boundaries, especially if the event is described in a subordinate clause. For example, for a sentence like “ organization announced that person retired from post ,” it is hard to ﬁnd a relationship between organization and the event of retiring without the global view  from the predicate “announce”. These problems lead IE systems to fail to capture some of the arguments needed for ﬁlling the template. Overcoming the problems above makes the system capable of ﬁnding more patterns from the training data, and therefore, more slot-ﬁllers in the template. In this paper, we introduce Tree-based pattern representation and consider how it can be acquired automatically. 2. TREE-BASED PATTERN REPRESENTATION (TBP) Deﬁnition Tree-based representation of patterns (TBP) is a representation of patterns based on the dependency tree of a sentence. A pattern is deﬁned as a path in the dependency tree passing through zero or more intermediate nodes within the tree. The dependency tree is a directed tree whose nodes are bunsetsus or phrasal units, and whose directed arcs denote the dependency between two bunsetsus: A B denotes A’s dependency on B (e.g. A is a subject and B is a predicate.) Here dependency relationships are not limited to just those between a case-marked element and a predicate, but also include those between a modiﬁer and its head element, which covers most relationships within sentences. 1 TBP for Information Extraction Figure 2 shows how TBP is used in comparison with the wordorder based pattern, where A...F in the left part of the ﬁgure is a sequence of the phrasal units in a sentence appearing in this order and the tree in the right part is its dependency tree. To ﬁnd the relationship between B F, a word-order based pattern needs a dummy expression to hold C, D and E, while TBF can denote the direct relationship as B F. TBP can also represent a complicated pattern for a node which is far from the root node in the dependency tree, like C D E, which is hard to represent without the sentence structure. For matching with TBP, the target sentence should be parsed into a dependency tree. Then all the predicates are detected and the subtrees which have a predicate node as a root are traversed to ﬁnd a match with a pattern. Beneﬁt of TBP TBP has some advantages for pattern matching over the surface word-order based patterns in addressing the problems mentioned in the previous section: ¯ Free word-order problem TBP can offer a direct representation of the dependency relationship even if the word-order is different. ¯ Free case-marking problem TBP can freely traverse the whole dependency tree and ﬁnd any signiﬁcant path as a pattern. It does not depend on predeﬁned case-patterns as Riloff [4] and Yangarber [6] did. ¯ Indirect relationships TBP can ﬁnd indirect relationships, such as the relationship between a predicate and the modiﬁer of the argument of the ½In this paper, we used the Japanese parser KNP [1] to obtain the dependency tree of a sentence.  predicate. For example, the pattern “ organization Ó post ØÓ appoint” can capture the relationship between “ organization ” and “to be appointed” in the sentence “ person was appointed to post of organization .” ¯ Relationships beyond clausal boundaries TBP can capture relationships beyond clausal boundaries. The pattern “ post ØÓ appoint ÇÅÈannounce” can ﬁnd the relationship between “ post ” and “to announce”. This relationship, later on, can be combined with the relationship “ organization ” and “to announce” and merged into one event.  3. ALGORITHM In this section, we outline our procedure for automatic acquisition of patterns. We employ a cascading procedure, as is shown in Figure 3. First, the original documents are processed by a morphological analyzer and NE-tagger. Then the system retrieves the relevant documents for the scenario as a relevant document set. The system, further, selects a set of relevant sentences as a relevant sentence set from those in the relevant document set. Finally, all the sentences in the relevant sentence set are parsed and the paths in the dependency tree are taken as patterns.  3.1 Document Preprocessing Morphological analysis and Named Entity (NE) tagging is performed on the training data at this stage. We used JUMAN [2] for the former and a NE-system which is based on a decision tree algorithm [5] for the latter. Also the part-of-speech information given by JUMAN is used in the later stages.  3.2 Document Retrieval The system ﬁrst retrieves the documents that describe the events of the scenario of interest, called the relevant document set. A set of narrative sentences describing the scenario is selected to create a query for the retrieval. For this experiment, we set the size of the relevant document set to 300 and retrieved the documents using CRL’s stochastic-model-based IR system [3], which performed well in the IR task in IREX, Information Retrieval and Extraction evaluation project in Japan 2. All the sentences used to create the patterns are retrieved from this relevant document set.  3.3 Sentence Retrieval  The system then calculates the TF/IDF-based score of relevance to the scenario for each sentence in the relevant document set and retrieves the n most relevant sentences as the source of the patterns, where n is set to 300 for this experiment. The retrieved sentences will be the source for pattern extraction in the next subsection. First, the TF/IDF-based score for every word in the relevant document set is calculated. TF/IDF score of word w is:  × ÓÖ ´Ûµ  ´ Ì ¼  ´Ûµ ¡ ÐÓ ´Æ·¼ µ ÐÓ Æ´Ûµ ´ ·½µ  if w is Noun, Verb or Named Entity otherwise  where N is the number of documents in the collection, TF(w) is the term frequency of w in the relevant document set and DF(w) is the document frequency of w in the collection. Second, the system calculates the score of each sentence based on the score of its words. However, unusually short sentences and ¾IREX Homepage: http://cs.nyu.edu/cs/projects/proteus/irex  Dependency Tree  Tree-Based Pattern  
In this paper, we implemented a set of title generation methods using training set of 21190 news stories and evaluated them on an independent test corpus of 1006 broadcast news documents, comparing the results over manual transcription to the results over automatically recognized speech. We use both F1 and the average number of correct title words in the correct order as metric. Overall, the results show that title generation for speech recognized news documents is possible at a level approaching the accuracy of titles generated for perfect text transcriptions. Keywords Machine learning, title generation 1. INTRODUCTION To create a title for a document is a complex task. To generate a title for a spoken document becomes even more challenging because we have to deal with word errors generated by speech recognition. Historically, the title generation task is strongly connected to traditional summarization because it can be thought of extremely short summarization. Traditional summarization has emphasized the extractive approach, using selected sentences or paragraphs from the document to provide a summary. The weaknesses of this approach are inability of taking advantage of the training corpus and producing summarization with small ratio. Thus, it will not be suitable for title generation tasks. More recently, some researchers have moved toward “learning approaches” that take advantage of training data. Witbrock and Mittal [1] have used Naïve Bayesian approach for learning the document word and title word correlation. However they limited their statistics to the case that the document word and the title word are same surface string. Hauptmann and Jin [2] extended this approach by relaxing the restriction. Treating title generation problem as a variant of Machine translation problem, Kennedy and Hauptmann [3] tried the iterative Expectation-Maximization algorithm. To avoid struggling with organizing selected title words into human readable sentence, Hauptmann [2] used K  nearest neighbour method for generating titles. In this paper, we put all those methods together and compare their performance over 1000 speech recognition documents. We decompose the title generation problem into two parts: learning and analysis from the training corpus and generating a sequence of title words to form the title. For learning and analysis of training corpus, we present five different learning methods for comparison: Naïve Bayesian approach with limited vocabulary, Naïve Bayesian approach with full vocabulary, K nearest neighbors, Iterative ExpectationMaximization approach, Term frequency and inverse document frequency method. More details of each approach will be presented in Section 2. For the generating part, we decompose the issues involved as follows: choosing appropriate title words, deciding how many title words are appropriate for this document title, and finding the correct sequence of title words that forms a readable title ‘sentence’. The outline of this paper is as follows: Section 1 gave an introduction to the title generation problem. The details of the experiment and analysis of results are presented in Section 2. Section 3 discusses our conclusions drawn from the experiment and suggests possible improvements. 2. THE CONTRASTIVE TITLE GENERATION EXPERIMENT In this section we describe the experiment and present the results. Section 2.1 describes the data. Section 2.2 discusses the evaluation method. Section 2.3 gives a detailed description of all the methods, which were compared. Results and analysis are presented in section 2.4. 2.1 Data Description In our experiment, the training set, consisting of 21190 perfectly transcribed documents, are obtain from CNN web site during 1999. Included with each training document text was a human assigned title. The test set, consisting of 1006 CNN TV news story documents for the same year (1999), are randomly selected from the Informedia Digital Video Library. Each document has a closed captioned transcript, an alternative transcript generated with CMU Sphinx speech recognition system with a 64000-word broadcast news language model and a human assigned title. 2.2 Evaluation First, we evaluate title generation by different approaches using the F1 metric. For an automatically generated title Tauto, F1 is  measured against corresponding human assigned title Thuman as follows: F1 = 2×precision×recall / (precision + recall) Here, precision and recall is measured respectively as the number of identical words in Tauto and Thuman over the number of words in Tauto and the number of words in Thuman. Obviously the sequential word order of the generated title words is ignored by this metric. To measure how well a generated title compared to the original human generated title in terms of word order, we also measured the number of correct title words in the hypothesis titles that were in the same order as in the reference titles. We restrict all approaches to generate only 6 title words, which is the average number of title words in the training corpus. Stop words were removed throughout the training and testing documents and also removed from the titles. 2.3 Description of the Compared Title Generation Approaches The five different title generation methods are: 1. Naïve Bayesian approach with limited vocabulary (NBL). It tries to capture the correlation between the words in the document and the words in the title. For each document word DW, it counts the occurrence of title word same as DW and apply the statistics to the test documents for generating titles. 2. Naïve Bayesian approach with full vocabulary (NBF). It relaxes the constraint in the previous approach and counts all the document-word-title-word pairs. Then this full statistics will be applied on generating titles for the test documents. 3. Term frequency and inverse document frequency approach (TF.IDF). TF is the frequency of words occurring in the document and IDF is logarithm of the total number of documents divided by the number of documents containing this word. The document words with highest TF.IDF were chosen for the title word candidates. 4. K nearest neighbor approach (KNN). This algorithm is similar to the KNN algorithm applied to topic classification. It searches the training document set for the closest related document and assign the training document title to the new document as title. 5. Iterative Expectation-Maximization approach (EM). It views documents as written in a ‘verbal’ language and their titles as written a ‘concise’ language. It builds the translation model between the ‘verbal’ language and the ‘concise’ language from the documents and titles in the training corpus and ‘translate’ each testing document into title. 2.4 The sequentializing process for title word candidates To generate an ordered set of candidates, equivalent to what we would expect to read from left to right, we built a statistical trigram language model using the SLM tool-kit (Clarkson, 1997) and the 40,000 titles in the training set. This language model was used to determine the most likely order of the title word candidates generated by the NBL, NBF, EM and TF.IDF methods. 3. RESULTS AND OBSERVATIONS The experiment was conducted both on the closed caption transcripts and automatic speech recognized transcripts. The F1  results and the average number of correct title word in correct order are shown in Figure 1 and 2 respectively. KNN works surprisingly well. KNN generates titles for a new document by choosing from the titles in the training corpus. This works fairly well because both the training set and test set come from CNN news of the same year. Compared to other methods, KNN degrades much less with speech-recognized transcripts. Meanwhile, even though KNN performance not as well as TF.IDF and NBL in terms of F1 metric, it performances best in terms of the average number of correct title words in the correct order. If consideration of human readability matters, we would expect KNN to outperform considerately all the other approaches since it is guaranteed to generate human readable title.  Comparison of F1  30.00% 25.00% 20.00% 15.00% 10.00% 5.00% 0.00%  original documents spoken documents  F1 KNN TFIDF NBL NBF EM  Methods Figure 1: Comparison of Title Generation Approaches on a test corpus of 1006 documents with either perfect transcript or speech recognized transcripts using the F1 score. NBF performs much worse than NBL. NBF performances much worse than NBL in both metrics. The difference between NBF and NBL is that NBL assumes a document word can only generate a title word with the same surface string. Though it appears that NBL loses information with this very strong assumption, the results tell us that some information can safely be ignored. In NBF, nothing distinguishes between important words and trivial words. This lets frequent, but unimportant words dominate the document-word-title-word correlation. Light learning approach TF.IDF performances considerably well compared with heavy learning approaches. Surprisingly, heavy learning approaches, NBL, NBF and EM algorithm didn’t out performance the light learning approach TF.IDF. We think learning the association between document words and title words by inspecting directly the document and its title is very problematic since many words in the document don’t reflect its content. The better strategy should be distilling the document first before learning the correlation between document words and title words.  # of Correct Words in the Correct Order KNN TFIDF NBL NBF EM  Comparison of # of Correct Words in Correct Order  0.8  0.7  original  0.6  documents  0.5  spoken  0.4  documents  0.3  0.2  0.1  0 
We present a deployed, conversational dialog system that assists users in finding computers based on their usage patterns and constraints on specifications. We discuss findings from a market survey and two user studies. We compared our system to a directed dialog system and a menu driven navigation system. We found that the conversational interface reduced the average number of clicks by 63% and the average interaction time by 33% over a menu driven search system. The focus of our continuing work includes developing a dynamic, adaptive dialog management strategy, robustly handling user input and improving the user interface. 1. INTRODUCTION Conversational interfaces allow users to interact with automated systems using speech or typed in text via "conversational dialog". For the purposes of this paper, a conversational dialog consists of a sequence of interactions between a user and a system. The user input is interpreted in the context of previous user inputs in the current session and from previous sessions.  The system consists of three major modules (cf. Figure 1): Presentation Manager, Dialog Manager, and Action Manager. The Presentation Manager interprets user input and generates system responses. It embodies the user interface and contains a shallow semantic parser and a response generator. The semantic parser  PDA  tel ephone  web  speech, text,..  Pr eMsaennatgaetrion M anage r  USER  Conve r s ational Dialog Manager  APIs APIs  NLP Se r vice s Dis cour s e HHiissttoorryy Action M anage r Application Action Tem plates etc...  Conversational interfaces offer greater flexibility to users than menu-driven (i.e., directed-dialog) interfaces, where users navigate menus that have a rigid structure [5,4]. Conversational interfaces permit users to ask queries directly in their own words. Thus, users do not have to understand the terminology used by system designers to label hyperlinks on a website or internalize the hierarchical menus of a telephone system [3] or websites. Recently, conversational interfaces for executing simple transactions and for finding information are proliferating [7,6]. In this paper, we present a conversational dialog system, Natural Language Assistant (or NLA), that helps users shop for notebook computers and discuss the results of user studies that we conducted with this system. 2. NATURAL LANGUAGE ASSISTANT NLA assists users in finding notebooks that satisfy their needs by engaging them in a dialog. At each turn of the dialog, NLA provides incremental feedback about its understanding of the user's constraints and shows products that match these constraints. By encouraging iterative refinement of the user's query, the system finds more user constraints and, ultimately, recommends a product that best matches the user's criteria.  Figure 1. Architecture of the NLA conversational system. identifies concepts (e.g., MULTIMEDIA) and constraints on product attributes (e.g., hard disk size more than 20GB) from the textual user input. The concepts mediate the mapping between user input and available products through product specifications. They implement the business logic. The Dialog Manager uses the current requirements and formulates action plans for the Action Manager to perform back-end operations (e.g., database access1). The Dialog Manager constructs a response to the user based on the results from the Action Manager and the discourse history and sends the system response to the Presentation Manager that displays it to the user. The system prompts for features relevant in the current context. In our mixed initiative dialog system, the user can always answer the specific question put to him/her or provide any constraints. The system has been recently deployed on an external website. Figure 2 shows the start of a dialog.2  
Websites of businesses should accommodate both customer needs and business requirements. Traditional menu-driven navigation and key word search do not allow users to describe their intentions precisely. We have developed a conversational interface to online shopping that provides convenient, personalized access to information using natural language dialog. User studies show significantly reduced length of interactions in terms of time and number of clicks in finding products. The core dialog engine is easily adaptable to other domains. 1. INTRODUCTION Natural language dialog has been used in many areas, such as for call-center/routing application (Carpenter & Chu-Carroll 1998), email routing (Walker, Fromer & Narayanan 1998), information retrieval and database access (Androutsopoulos & Ritchie 1995), and for telephony banking (Zadrozny et al. 1998). In this demonstration, we present a natural language dialog interface to online shopping. Our user studies show natural language dialog to be a very effective means for negotiating user's requests and intentions in this domain. 2. SYSTEM ARCHITECTURE In our system, a presentation manager captures queries from users, employs a parser to transform the user's query into a logical form, and sends the logical form to a dialog manager. The presentation manager is also responsible for obtaining the system's response from the dialog manager and presenting it to the user using template-based generation. The dialog manager formulates action plans for an action manager to perform backend tasks such as database access, business transactions, etc. The dialog manager applies information state-based dialog strategies to formulate responses depending on the current state, discourse history and the action results from the action manager.  The Data Management Subsystem maintains a “concept” repository with common sense “concepts” and a phrasal lexicon that lists possible ways for referring to the concepts. Business Rules map concepts to business specifications by defining concepts using a propositional logic formula of constraints over product specifications. Thus, the Business Rules reflect business goals and decisions. The Extended Database combines product specifications and precompiled evaluations of the concept definitions for each product to provide a representation that guides the natural language dialog. We are investigating automated tools for helping developers and maintainers extract relevant concepts and terms on the basis of user descriptions and queries about products. 3. EVALUATION We conducted several user studies to evaluate the usability of NLA (Chai et al. 2000). In one study, seventeen test subjects preferred the dialog-driven navigation of NLA two to one over menu-driven navigation. Moreover, with NLA, the average number of clicks was reduced by 63.2% and the average time was reduced by 33.3%. Analysis of the user queries (average length = 5.31 words long; standard deviation = 2.62; 85% of inputs are noun phrases) revealed the brevity and relative linguistic simplicity of user input. Hence, shallow parsing techniques were adequate for processing user input. In general, sophisticated dialog management appears to be more important than the ability to handle complex natural language sentences. The user studies also highlighted the need to combine multiple modalities and styles of interaction. 4. REFERENCES [1] Androutsopoulos, Ion & Ritchie, Graeme. Natural Language Interfaces to Databases – An Introduction, Natural Language Engineering 1.1:29-81, 1995. [2] Carpenter, Bob & Chu-Carroll, Jeniffer. Natural Language Call Routing: A Robust, Self-organizing Approach, Proceedings of the 5th International Conference on Spoken Language Processing, 1998. [3] Chai, J., Lin, J., Zadrozny, W., Ye, Y., Budzikowska, M., Horvath, V., Kambhatla, N. & Wolf, C. Comparative Evaluation of a Natural Language Dialog Based System and a Menu-Driven System for Information Access: A Case Study, Proceedings of RIAO 2000, Paris, 2000.  Client HTML  Ne twork (HTTP)  Web Server  Servlet HTTP Se rv e r HTML  Application Server  Data Management (Off line)  Conce pts  Business Rules Concept Interpreter  Product Databas e  Exte nde d PD  Online Inte raction  Quick Parser 
2. CONVERTING PHRASE STRUCTURES TO DEPENDENCY STRUCTURES The notion of head is important in both phrase structures and dependency structures. In many linguistic theories such as X-bar theory and GB theory, each phrase structure has a head that determines the main properties of the phrase and a head has several levels of projections; whereas in a dependency structure the head is linked to its dependents. In practice, the head information is explicitly marked in a dependency Treebank, but not always so in a phrase-structure Treebank. A common way to ﬁnd the head in a phrase structure is to use a head percolation table, as discussed in [7, 1] among others. For example, the entry (S right S/VP) in the head percolation table says that the head child1 of an S node is the ﬁrst child of the node from the right with the label S or VP. Once the heads in phrase structures are found, the conversion from phrase structures to dependency structures is straightforward, as shown below: (a) Mark the head child of each node in a phrase structure, using the head percolation table. 1The head-child of a node XP is the child of the node XP that is the ancestor of the head of the XP in the phrase structure. .  (b) In the dependency structure, make the head of each nonhead-child depend on the head of the head-child. Figure 1 shows a phrase structure in the English Penn Treebank [8]. In addition to the syntactic labels (such as NP for a noun phrase), the Treebank also uses function tags (such as SBJ for the subject) for grammatical functions. In this phrase structure, the root node has two children: the NP and the VP. The algorithm would choose the VP as the head-child and the NP as a non-head-child, and make the head Vinkin of the NP depend on the head join of the VP in the dependency structure. The dependency structure of the sentence is shown in Figure 2. A more sophisticated version of the algorithm (as discussed in [10]) takes two additional tables (namely, the argument table and the tagset table) as input and produces dependency structures with the argument/adjunct distinction (i.e., each dependent is marked in the dependency structure as either an argument or an adjunct of the head).  NP-SBJ NNP Vinken  S VP  MD  VP  VB will join  NP DT NN the board  PP-CLR NP-TMP  NNP CD  IN  NP Nov 29  as DT JJ NN  a  director  nonexecutive  Figure 1: A phrase structure in the Penn Treebank  join  Vinken will  board the  as  29  director Nov  a nonexecutive Figure 2: A dependency tree for the sentence in Figure 1. Heads are marked as parents of their dependents in an ordered tree. It is worth noting that quite often there is no consensus on what the correct dependency structure for a particular sentence should be. To build a dependency Treebank, the Treebank annotators must decide which word depends on which word; for example, they have to decide whether the subject Vinken in Figure 1 depends on the  modal verb will or the main verb join. In contrast, the annotators for phrase-structure Treebanks do not have to make such decisions. The users of phrase-structure Treebanks can modify the head percolation tables to get different dependency structures from the same phrase structure. In other words, phrase structures offer more ﬂexibility than dependency structures with respect to the choices of heads. The feasibility of using the head percolation table to identify the heads in phrase structures depends on the characteristics of the language, the Treebank schema, and the deﬁnition of the correct dependency structure. For instance, the head percolation table for a strictly head-ﬁnal (or head-initial) language is very easy to build, and the conversion algorithm works very well. For the English Penn Treebank, which we used in this paper, the conversion algorithm works very well except for the noun phrases with the appositive construction. For example, the conversion algorithm would choose the appositive the CEO of FNX as the head child of the phrase John Smith, the CEO of FNX, whereas the correct head child should be John Smith. 3. CONVERTING DEPENDENCY STRUCTURES TO PHRASE STRUCTURES The main information that is present in phrase structures but not in dependency structures is the type of syntactic category (e.g., NP, VP, and S); therefore, to recover syntactic categories, any algorithm that converts dependency structures to phrase structures needs to address the following questions: Projections for each category: for a category X, what kind of projections can X have? Projection levels for dependents: Given a category Y depends on a category X in a dependency structure, how far should Y project before it attaches to X’s projection? Attachment positions: Given a category Y depends on a category X in a dependency structure, to what position on X’s projection chain should Y’s projection attach? In this section, we discuss three conversion algorithms, each of which gives different answers to these three questions. To make the comparison easy, we shall apply each algorithm to the dependency structure (d-tree) in Figure 2 and compare the output of the algorithm with the phrase structure for that sentence in the English Penn Treebank, as in Figure 1. Evaluating these algorithms is tricky because just like dependency structures there is often no consensus on what the correct phrase structure for a sentence should be. In this paper, we measure the performance of the algorithms by comparing their output with an existing phrase-structure Treebank (namely, the English Penn Treebank) because of the following reasons: ﬁrst, the Treebank is available to the public, and provides an objective although imperfect standard; second, one goal of the conversion algorithms is to make it possible to compare the performance of parsers that produce dependency structures with the ones that produce phrase structures. Since most state-of-the-art phrase-structure parsers are evaluated against an existing Treebank, we want to evaluate the conversion algorithms in the same way; third, a potential application of the conversion algorithms is to help construct a phrasestructure Treebank for one language, given parallel corpora and the phrase structures in the other language. One way to evaluate the quality of the resulting Treebank is to compare it with an existing Treebank. 3.1 Algorithm 1 According to X-bar theory, a category X projects to X’, which  further projects to XP. There are three types of rules, as shown in Figure 3(a). Algorithm 1, as adopted in [4, 3], strictly follows Xbar theory and uses the following heuristic rules to build phrase structures:  (1) XP -> YP X’ (2) X’ -> X’ WP (3) X’ -> X ZP  XP  X  YP(Spec) X’  Y  ZW  (Spec) (Arg) (Mod)  X’ WP(Mod) X ZP(Arg)  (a) rules in X-bar theory  (b) d-tree  (c) phrase structure  Figure 3: Rules in X-bar theory and Algorithm 1 (which is based on it)  Two levels of projections for any category: any category X has two levels of projection: X’ and XP. Maximal projections for dependents: a dependent Y always projects to Y’ then YP, and the YP attaches to the head’s projection. Fixed positions of attachment: Dependents are divided into three types: speciﬁers, modiﬁers, and arguments. Each type of dependent attaches to a ﬁxed position, as shown in Figure 3(c). The algorithm would convert the d-tree in Figure 3(b) to the phrase structure in Figure 3(c). If a head has multiple modiﬁers, the algorithm could use either a single X’ or stacked X’ [3]. Figure 4 shows the phrase structure for the d-tree in Figure 2, where the algorithm uses a single X’ for multiple modiﬁers of the same head.2  VP  NP  V’  N’ VP  NNP V’ VB  MD  Vinken  join  will  V’  PP  NP  NP  P’  N’  DTP N’ IN  NP  NP N’  DT’ NN  DTP  N’  N’ CD  DT  as board  DT’ ADJP N’ NNP 29  the  DT ADJ’ NN Nov  a JJ director  nonexecutive  Figure 4: The phrase structure built by algorithm 1 for the dtree in Figure 2  3.2 Algorithm 2 Algorithm 2, as adopted by Collins and his colleagues [2] when they converted the Czech dependency Treebank [6] into a phrasestructure Treebank, produces phrase structures that are as ﬂat as possible. It uses the following heuristic rules to build phrase structures: One level of projection for any category: X has only one level of projection: XP. 2To make the phrase structure more readable, we use N’ and NP as the X’ and XP for all kinds of POS tags for nouns (e.g., NNP, NN, and CD). Verbs and adjectives are treated similarly.  Minimal projections for dependents: A dependent Y does not project to Y P unless it has its own dependents. Fixed position of attachment: A dependent is a sister of its head in the phrase structure.3 The algorithm treats all kinds of dependents equally. It converts the pattern in Figure 5(a) to the phrase structure in Figure 5(b). Notice that in Figure 5(b), Y does not project to YP because it does not have its own dependents. The resulting phrase structure for the d-tree in Figure 2 is in Figure 6, which is much ﬂatter than the one produced by Algorithm 1.  X  XP  Y (dep)  ZW (dep) (dep)  Y X ZP WP  (a) d-tree  (b) phrase structure  Figure 5: The scheme for Algorithm 2  VP  NNP MD VB  NP  PP  NP  Vinken will  join DT NN IN  NP NNP CD  DT JJ NN Nov 29 the board as  a  director  nonexecutive  Figure 6: The phrase structure built by Algorithm 2 for the d-tree in Figure 2  3.3 Algorithm 3 The previous two algorithms are linguistically sound. They do not use any language-speciﬁc information, and as a result there are several major differences between the output of the algorithms and the phrase structures in an existing Treebank, such as the Penn English Treebank (PTB). Projections for each category: Both algorithms assume that the numbers of projections for all the categories are the same, whereas in the PTB the number of projections varies from head to head. For example, in the PTB, determiners do not project, adverbs project only one level to adverbial phrases, whereas verbs project to VP, then to S, then to SBAR.4 Projection levels for dependents: Algorithm 1 assumes the maximal projections for all the dependents, while Algorithm 2 assumes minimal projections; but in the PTB, the level of projection of a dependent may depend on several factors such as the categories of the dependent and the head, the position of the dependent with respect to the head, and the dependency type. For example, when a 3If a dependent Y has its own dependents, it projects to YP and YP is a sister of the head X; otherwise, Y is a sister of the head X. 4S is similar to IP (IP is the maximal projection of INFL) in GB theory, so is SBAR to CP (CP is the maximal projection of Comp); therefore, it could be argued that only VP is a projection of verbs in the PTB. Nevertheless, because PTB does not mark INFL and Comp, we treat S and SBAR as projections of verbs.  noun modiﬁes a verb (or VP) such as yesterday in he came yester-  day, the noun always projects to NP, but when a noun N1 modiﬁers  another noun N2, N1 projects to NP if N1 is to the right of N2  (e.g., in an appositive construction) and it does not project to NP if  N1 is to the left of N2.  Attachment positions: Both algorithms assume that all the de-  pendents of the same dependency type attach at the same level (e.g.,  in Algorithm 1, modiﬁers are sisters of X’, while in Algorithm 2,  modiﬁers are sisters of X); but in the PTB, that is not always true.  For example, an ADVP, which depends on a verb, may attach to  either an S or a VP in the phrase structure according to the position  of the ADVP with respect to the verb and the subject of the verb.  Also, in noun phrases, left modiﬁers (e.g., JJ) are sisters of the head  noun, while the right modiﬁers (e.g., PP) are sisters of NP.  For some applications, these differences between the Treebank  and the output of the conversion algorithms may not matter much,  and by no means are we implying that an existing Treebank pro-  vides the gold standard for what the phrase structures should be.  Nevertheless, because the goal of this work is to provide an algo-  rithm that has the ﬂexibility to produce phrase structures that are  as close to the ones in an existing Treebank as possible, we pro-  pose a new algorithm with such ﬂexibility. The algorithm distin-  guishes two types of dependents: arguments and modiﬁers. The  algorithm also makes use of language-speciﬁc information in the  form of three tables: the projection table, the argument table, and  the modiﬁcation table. The projection table speciﬁes the projec-  tions for each category. The argument table (the modiﬁcation table,  resp.) lists the types of arguments (modiﬁers, resp) that a head can  ! ! take and their positions with respect to the head. For example, the entry V V P S in the projection table says that a verb can  project to a verb phrase, which in turn projects to a sentence; the  entry (P 0 1 NP/S) in the argument table indicates that a preposition  can take an argument that is either an NP or an S, and the argument  is to the right of the preposition; the entry (NP DT/JJ PP/S) in the  modiﬁcation table says that an NP can be modiﬁed by a determiner  and/or an adjective from the left, and by a preposition phrase or a  sentence from the right.  Given these tables, we use the following heuristic rules to build  phrase structures:5  One projection chain per category: Each category has a unique  projection chain, as speciﬁed in the projection table.  Minimal projection for dependents: A category projects to a  higher level only when necessary.  Lowest attachment position: The projection of a dependent at-  taches to a projection of its head as lowly as possible.  The last two rules require further explanation, as illustrated in  Figure 7. In the ﬁgure, the node X has three dependents: Y and Z  are arguments, and W is a modiﬁer of X. Let’s assume that the al-  gorithm has built the phrase structure for each dependent. To form  the phrase structure for the whole d-tree, the algorithm needs to  attach the phrase structures for dependents to the projection chain  0 X  
In this paper we report on our work on a prototype route navigation dialogue system for use in a vehicle. The system delivers spoken turn-by-turn directions, and has been developed to accept naturally phrased navigation queries, as part of our overall effort to create an in-vehicle information system which delivers information as requested while placing minimal cognitive load on the driver. Keywords Dialogue Systems, Discourse, Navigation, NLP, Pragmatics, Dialogue Manager 1. INTRODUCTION In this paper we report on our work on a spoken language navigation system which runs in real-time on a high-end laptop or PC, for use in a vehicle. We focus on issues in developing a system which can understand natural conversational queries and respond in such a way as to maximize ease of use for the driver. Because today’s technology has the potential to deliver massive amounts of information to automobiles, it is crucial to deliver this information in such way that the driver’s attention is not diverted from the primary task of safe driving. Our assumption has been that a dialogue system with a near-human conversational ability would place less of a cognitive load on the driver than one which behaves very differently than a human. We have implemented a testbed on which to develop and evaluate driver interfaces to navigation systems. Our approach is multimodal and the interface will include a head-up display, steering hub controls, and spoken language, though it is only the latter modality that we report on here. We first discuss our development phases, and after this we provide an overview of our implementation, emphasizing the natural language processing aspects and application interface to the map databases. Next we provide results of our initial evaluation of the system, and finally we draw conclusions and summarize plans for future work. 2. DEVELOPMENT PHASES One can identify four distinct subproblems which must be solved for a navigation system: 1) the natural language navigation interface, 2) street name recognition, 3) the natural language destination entry interface given street name recognition, and 4) the map database interface. We have partitioned the problem and  have phased our development to progressively implement solutions with increasing complexity. Navigation system implementation is complicated by the potential of having a very large street name vocabulary with many unusual and uncommon pronunciations with significant variations across speakers. The appropriate name space is dynamic since it depends on the location of the vehicle. Our initial system does not accept queries with proper street names. In addition, we assume separate destination entry and route planning systems, and that one or more routes have been loaded into the navigation system. The system relies on open dialogue to resolve the directions at any stage of the journey and may or may not use the Global Positioning System (GPS) to determine the progress along the route. By implementing this system first we could concentrate on the dialogue aspects of the navigation problem and also establish a baseline with which to compare our other implementations. In the second phase we include a limited set of street names as part of the language model and lexicons. Initially we are using a predefined set of names with hand tuning of the pronunciations. Additional research is required to solve the street name recognition problem generally and automatically. We assume invehicle GPS and use a map matching system to determine the vehicle’s position and if it is on-route. This phase includes development of the natural language components for destination entry and also broadens the scope of the navigation queries to include questions with and about street names. More distant plans include on-road route replanning, providing information to requests for specific street names or points of interest along the route, and traffic information and workarounds. 3. IMPLEMENTATION Our implementation is based on the Galaxy-II system [6] from the Massachusetts Institute of Technology (MIT), which is the baseline for the Communicator program of the Defense Advance Research Projects Agency (DARPA). The architecture consists of a hub client that communicates, using a standard protocol, with a number of servers as shown in Figure 1. Each server generally implements a key system function including Speech Recognition, Frame Construction (language parsing), Context Tracking, Dialogue Management, Application Interface, Language Generation, and Text-to-speech.  2001 HRL Laboratories, LLC, All rights reserved  Text-to-speech Conversion  Language Generation  Dialogue Management  Audio Server  Hub  Application  Back-end  Speech Recognition  Frame Construction  Context Tracking  Figure 1. The client-server architecture of the MIT Galaxy-II is used to implement our navigation system testbed.  3.1 Speech Recognition We use the latest MIT SUMMIT recognizer [8] using weighted finite-state transducers for the lexical access search. We have also "plugged in" alternate recognizers such as the Microsoft Speech SDK recognizer and the Sphinx [3] speech recognizer available as open source code from Carnegie Mellon University. We are in the process of developing a large database of in-vehicle utterances collected in various car models under a wide range of road and other background noise conditions. This data collection will be carried out in two phases, the first of which is completed; phase two is underway. Limited speech data will result from the first phase and subtantial speech data (appropriate for training acoustic models to represent in-vehicle noise conditions and testing of recognition engines) will come out of the second phase, and will become available through our partners in this collection effort, CSLR at University of Colorado, Boulder [4]. In the meantime we are using the MIT JUPITER acoustic models. The performance is acceptable for our language and dialogue model development, but we refrain from presenting any detailed recognizer results here since they would not reflect fairly on optimized recognizer performance. Our vocabulary consists of about 400 words without street names. We have an additional 600 street names gleaned from the Los Angeles area where we do much of our system evaluation. Baseforms for the vocabulary are derived from the PRONLEX dictionary from the Linguistic Data Consortium at the University of Pennsylvania. Extensive hand editing is needed especially for the street names. The MIT rule set is used for production of word graphs for the alternate pronunciation forms. We have derived a language model from a set of utterances that were initially generated based our best guess of the query space. As evaluation evolves, we modify the utterance list and retrain the language model. The language model uses classes and includes both bigram and trigram models. 3.2 Application Interface We are building the application interface in several phases. Initially we are only answering queries about turns and distances during navigation. We obtain the database in two steps. First, we access a commercial map database using standard text I/O for destination entry and route planning. This produces a detailed set of instruction that includes many short segments such as on- and off-ramps. We filter this and rewrite the data to provide a set of natural driving instructions suitable for verbal communication.  The result is a flat database, such as the one shown in Figure 2. This is loaded into to the system and used to formulate answers to the route queries. In the example in Figure 2 the estimated driving time is 45 minutes. Each row is a segment of the trip. The first and second columns code right, left, straight, and compass direction information. The third column is the segment length in miles and the last is the segment name.  HRL 00 02 11 01 30 01 04  1.0 13.3 3.4 4.5 0.3 1.7 0.4  Airport  45  Malibu Canyon Road  PCH  East I-10 freeway  South I-405 freeway  Howard Hughes Parkway  Sepulveda Boulevard  Los Angeles International Airport  Figure 2: Example database used as source data to generate guidance instructions for the route between HRL Laboratories and the Los Angeles airport  A sample dialogue is shown in Figure 3 which illustrates the kind of responses the system can generate from a database such as that shown above, given navigation queries of the sort shown; this sample was drawn from our phase I user-system data logs:  S Welcome to Navigator U What routes are loaded? S I have one set of directions loaded. It is for the route to the airport from HRL U What’s my first turn? S From Malibu Canyon Road turn left onto Pacific Coast Highway U And after that? S From Pacific Coast Highway go straight onto East I-10 freeway U How far is that turn? S The distance is 14.3 miles U My odometer reads fifteen point one miles. S Your odometer reading is fifteen point one U What is my next turn? S From East I-10 freeway turn right onto South I-405 freeway U How far is it from here? S The distance is 2.6 miles U How far is the airport from there? S The distance is 6.7 miles U Bye Bye. S Goodbye. Have a nice day. Figure 3. A navigation system dialog illustrating some sample queries and showing the responses derived by the dialogue manager based on the database of Figure 2. Off-line construction of the global navigation database is not unrealistic since it could be done, at least in the near term, by a service organization such as OnStar from General Motors (GM). However as navigation systems become widely deployed, users will expect destination entry including real time route re-planning to be an integral part of system. We are developing a direct voice interface to the commercial map database that includes destination entry, route planning, and map matching using GPS data to determine if the vehicle is on-route or not. During the destination entry phase street names need to be robustly recognized. We are currently working with a subset of street names in the Los Angeles area preloaded in the recognizer  and language models. It is untenable to keep all of the street names in Los Angeles loaded in the recognizer simultaneously (there are around 16,000, including 8,000 base names), thus we are developing a method for dynamic loading of map names local to the vehicle position which we will report on in the near future. We have experimented with using a subset of street names as a filter list, and as a lookup list based on spelling the first few letters, to try to resolve the destination requested. If this fails, or if the trip is outside of the area from which names are loaded, we rely on more complete spelling to determine the destination. The origin for the route plan is generally implied since it is determined by the GPS position of the vehicle most of the time. Once the destination is determined it is straightforward to continuously replan the route based on the current vehicle position and thereby be able to provide remedial instruction if the driver departs from the route plan. 3.3 NL Analysis and Generation The core NLP components in our system are a TINA [5] grammar, context tracking mechanisms, and the GENESIS language generation module. The TINA grammar includes both syntactic and semantic elements and we try to extract as much information as possible from the parse. The information is coded in a hierarchical frame (Figure 4a) as well as a flat key-value pair (Figure 4b). In addition to handcrafting this grammar, a set of rules was also developed for the TINA inheritance mechanism. These rules are applied during context tracking, after the parse, to incorporate information from the dialog history into phrases such as "and after that" and "how about my second turn," and are also used to incorporate modifications that are a result of dialogue management. a) Parse frame {c locate :domain "Nav" :pred {p locate_object :topic {q turn :quantifier "poss_pro" :pred {p ord :topic 2 }}}} b) Key-values Pairs :clause "locate" :locate_object "turn" :ORD 2 c) Reply Frame {c speak_turn :topic {q turn :turn_direction "straight" :current_roadway "PCH" :new_roadway "East I-10 freeway" :domain :Nav" } Figure 4. Example frames produced for the simple query "What’s my second turn?" As noted, we use the MIT GENESIS server for language generation. Again this module is rule driven and we developed the lexicon, templates and rewrite rules needed for the three ways we use GENESIS. We extract the key-value pairs (e.g. Figure 4b) from the TINA parse frame. The key values are used to help  control the dialogue management as well as provide easy access to the variable values. We use GENESIS to produce the English reply string that is spoken by the synthesizer. The example frame in Figure 4c in conjunction with our rules generates the sentence "From Pacific Coast Highway turn straight onto East I-10 freeway" Lastly GENESIS is used to produce an SQL query string for database access. Templates and rewrite rules determine which form the output from GENESIS will take. Technically these three uses (key-value, reply string, and SQL) are just generation of different languages. 3.4 Dialogue Management We have developed servers for dialog management and to control the application interface for database query. The hub architecture supports use of a control table to direct which server function is called. This is especially useful for dialogue management. The control table is specified by a set of rules using logic and arithmetic operations on the key-value pairs. A well-designed set of rules makes it far easier to visualize the flow and debug the dialogue logic. For example, when a control rule such as: Clause "locate" !:from --> turn_from_here fires on the key-value pairs (Figure 4b), the hub calls the turn manager function "turn_from_here". In this simplified case, we are assuming if there is no ":from" key, the request is to locate an object (i.e. "turn") relative to the vehicle’s current position. In this case the function needs only to extract the value of the key ":ORD" and look up the data for the second turn in the database of Figure 2. This data is then written into the response frame, here called "speak_turn" and shown in Figure 4c. GENESIS uses this frame to generate the English language reply that is spoken by the synthesizer as described above. In the examples shown here we communicate with the database from the dialogue manager by downloading a flat database such as that of Figure 2, perhaps via a data link to an off-board service organization such as OnStar. In cases where we access databases directly, we use a separate server for this function. Generally, communications between the dialogue manager and database servers are routed via the Hub. Our dialogue manager has been designed to use GPS data when available (in which case GPS coordinates would also be a part of the database) or to use location information based on current odometer readings provided as input by drivers when GPS is not available. We use this latter method for demonstrating the system in a desktop setting, though we have also recently completed a utility for employing maps generated by our commercial navigation database, graphically displaying a driver’s progress along an imaginary route. We are now employing this tool as part of our current iteration of system testing and revision. 3.4.1 Referential amiguities in driver queries The driver can query to determine turn or distance information relative to current vehicle position, relative to another turn or reference point in the database, or as an absolute reference into the route plan stored in the database. We have devoted considerable effort to dealing with ambiguities which may arise as a result of different ways users may be conceptualizing the route (that is, in absolute or relative terms), as well as the driver being at different points in the route, and at different points in the progression of a discourse segment. Queries such as “what’s next?” can be ambiguous. Determining the correct interpretation  requires consideration of the discourse history and the user’s circumstances. For example, in the following dialog sequence (drawn from our data), there are at least two possible interpretations for “what is next?” in the third turn (U:user, S:system):1 --------------U: what’s my next turn S: From Malibu Canyon Road turn left onto Pacific Coast Highway. --------------U: and after that S: From Pacific Coast Highway go straight onto East I-10 freeway --------------→ U: what’s next Figure 5. Sample dialog containing ambiguous “What is next”. Notice that this query could be requesting information about the next turn from the driver’s current position (i.e. the immediately approaching turn), or it could be requesting information about the third turn from the driver’s current position, that is, the next turn from the most recently referred to turn. We will henceforth refer to these two interpretations as next-from-here and next-after-that, respectively. The factor which appears to have the most influence on which interpretation is given to this utterance originates neither in the utterance itself nor in the preceding dialog, but is purely circumstantial, namely, how much time has passed since the last utterance. Our assumption has been that there is a kind of timedependency factor in coherent discourses: while “what is next” is still within the scope of the preceding discourse context, it may (most likely will) be given the next-after-that interpretation. But after a certain length of time has elapsed, “what is next” cannot be interpreted as referring to some previously uttered instruction, but only as referring to the driver’s current position. If we think of this in terms of the user’s frame of reference for talking about their real or imagined location (we’ll refer to this as the FROM value), then we could characterize this phenomenon as the value of FROM being reset to HERE in the absence of immediate discourse context. Interpretations of numbered turn references (e.g. "what's my second turn") can also vary depending on another purely circumstantial factor, namely whether the driver is querying the system while preparing to begin the trip, or after she has begun driving. Some drivers will want to preview trip information before beginning to drive, and in this situation, interpretation of certain query types may differ from interpretation done during the trip. When the driver is querying the system before beginning to drive, she is more likely to conceive of and speak of the route in an absolute sense (cf. [7]). That is, the driver may conceive of the route as a fixed plan, wherein each turn and segment have a unique and constant order in a sequence. When conceiving of the 
Experiment 1 concerns extension of the coverage of semantic grammars in the medical domain. Semantic grammars are based on semantic constituents such as request information phrases (e.g., I was wondering . . . ) and location phrases (e.g., in my right arm) rather than syntactic constituents such as noun phrases and verb phrases. In other papers [12, 5], we have described how our modular grammar design enhances portability across domains. The portable grammar modules are the cross-domain module, containing rules for things like greetings, and the shared module, containing rules for things like times, dates, and locations. Figure 1 shows a parse tree for the sentence How long have you had this pain? XDM indicates nodes that were produced by cross-domain rules. MED indicates nodes that were produced by rules from the new medical domain grammar. The preliminary doctor-patient grammar focuses on three medical situations: give-information+existence — giving information about the existence of a symptom (I have been getting headaches); give-information+onset – giving information about the onset of a symptom (The headaches started three months ago); and give-information+occurrence — giving information about the onset of an instance of the symptoms (The headaches start behind my ears). Symptoms are expressed as body-state (e.g., pain), body-object (e.g., rash), and body-event (e.g., bleeding). Our experiment on extendibility was based on a hand written seed grammar that was extended by hand and by automatic learning. The seed grammar covered the domain actions mentioned above, but did not cover very many ways to phrase each domain action. For example, it might have covered The headaches started  [request-information+existence+body-state]::MED ( WH-PHRASES::XDM ( [q:duration=]::XDM ( [dur:question]::XDM ( how long ) ) ) HAVE-GET-FEEL::MED ( GET ( have ) ) you HAVE-GET-FEEL::MED ( HAS ( had ) ) [super_body-state-spec=]::MED ( [body-state-spec=]::MED ( ID-WHOSE::MED ( [identifiability=] ( [id:non-distant] ( this ) ) ) BODY-STATE::MED ( [pain]::MED ( pain ) ) ) ) ) Figure 1: Parser output with nodes produced by medical and cross-domain grammars.  Seed  IF  37.2  Domain Action 37.2  Speech Act  Recall  43.3  Precision  71.0  Concept List  Recall  2.2  Precision  12.5  Top-Level Arguments  Recall  0.0  Precision  0.0  Top-Level Values  Recall  0.0  Precision  0.0  Sub-Level Arguments  Recall  0.0  Precision  0.0  Sub-level Values  Recall  1.2  Precision  6.2  Extended 37.2 37.2 48.2 75.0 10.1 42.2 7.2 42.2 8.3 50.0 28.3 48.2 28.3 48.2  Learned 31.3 31.3 49.3 45.8 32.5 25.1 29.6 34.4 29.8 39.2 14.1 12.6 14.1 12.9  Table 1: Comparison of seed grammar, human-extended grammar, and machine-learned grammar on unseen data  three months ago but not I started getting the headaches three months ago. The seed grammar was extended by hand and by automatic learning to cover a development set of 133 utterances. The result was two new grammars, a human-extended grammar and a machine-learned grammar, referred to as the extended and learned grammars in Table 1. The two new grammars were then tested on 132 unseen sentences in order to compare generality of the rules. Results are reported only for 83 of the 132 sentences which were covered by the current interlingua design. The remaining 49 sentences were not covered by the current interlingua design and were not scored. Results are shown in Table 1. The parsed test sentences were scored in comparison to a handcoded correct interlingua representation. Table 1 separates results for six components of the interlingua: speech act, concepts, toplevel arguments, top-level values, sub-level arguments, and sublevel values, in addition to the total interlingua, and the domain action (speech act and concepts combined). The components of the interlingua were described in Section 2. The scores for the total interlingua and domain action are reported as percent correct. The scores for the six components of the interlingua are reported as average percent precision and recall. For example, if the correct interlingua for a sentence has two concepts, and the parser produces three, two of which are correct and one of which is incorrect, the precision is 66% and the recall is 100%.  Several trends are reﬂected in the results. Both the human-extended grammar and the machine-learned grammar show improved performance over the seed grammar. However, the human extended grammar tended to outperform the automatically learned grammar in precision, whereas the automatically learned grammar tended to outperform the human extended grammar in recall. This result is to be expected: humans are capable of formulating correct rules, but may not have time to analyze the amount of data that a machine can analyze. (The time spent on the human extended grammar after the seed grammar was complete was only ﬁve days.) Grammar Induction: Our work on automatic grammar induction for Experiment 1 is still in preliminary stages. At this point, we have experimented with completely automatic induction (no interaction with a user)2 of new grammar rules starting from a core grammar and using a development set of sentences that are not parsable according to the core grammar. The development sentences are tagged with the correct interlingua, and they do not stray from the concepts covered by the core grammar — they only correspond to alternative (previously unseen) ways of expressing the same set of covered concepts. The automatic induction is based on performing tree matching between a skeletal tree representation obtained from the interlingua, and a collection of parse fragments 2Previous work on our project [2] investigated learning of grammar rules with user interaction.  [give-information+onset+symptom]  [manner=]  [symptom-location=]  became  [adj:symptom-name=]  [sudden] 
A GUI is presented with our PIRCS retrieval system for supporting English-Chinese cross language information retrieval. The query translation approach is employed using the LDC bilingual wordlist. Given an English query, different translation methods and their retrieval results can be demonstrated. 1. INTRODUCTION The purpose of cross language information retrieval (CLIR) is to allow a user to search, retrieve, and gain some content understanding of documents written in a language different from the one that the user is familiar with. This is to be accomplished automatically without expert linguist assistance. CLIR is of growing importance because it can literally open up a whole world of information for the user, especially with the ease and convenience of access and delivery of foreign documents provided by Internet logistics nowadays. Searching and retrieving Chinese documents via English is a major sub-problem within CLIR because many people in the world use these two languages. For example, one would expect trade between China and the U.S. (and other countries) to grow significantly in the near future because of the impending WTO membership for China. Monitoring trends and status information from Chinese sources may be an essential operation for organizations interested in these affairs. Chinese is a language completely different from English, and it is conceived to be difficult for foreigners to learn. This paper describes some of the methods that we employ to deal with this problem, and presents a demonstrable system to illustrate the workings of cross language document retrieval. In Section 2, techniques for the query translation approach to CLIR are discussed. Section 3 contains a description of our simplified PIRCS retrieval system that is the basis for monolingual retrieval. Section 4 describes the GUI supporting interactive query input, document output and other implementation issues, and Section 5 contains our conclusion and future work.  2. STRATEGY FOR CROSS LANGUAGE INFORMATION RETRIEVAL When faced with the situation of a language mismatch between the target documents and the query (information need statement) of a user, one could reduce them to a common representation language for retrieval purposes by automatically translating the query to the document language, by translating the documents to the query language, or by converting both to a third representation language [1]. By far the simplest and most common approach seems to be the first method, and probably as effective as the others, and we have also taken this route. The question is what tools to use for query translation.  It is well known that machine translation is generally fuzzy and inaccurate [6]. This is particularly true when translation output are judged by humans, who tend to be unforgiving. However, translation for machine consumption (such as for information retrieval (IR)) may not be so bad because IR can operate with a bag of content terms without grammar, coherence or readability. What IR needs is that important content terms are correctly covered, even at the expense of noise translations. For this purpose, we have combined two different methods of query translation to hedge for errors and improve coverage, viz. dictionary translation and MT software.  2.1 Translation Using LDC Bilingual Wordlist One method we employ is dictionary translation using the LDC Chinese-English bilingual wordlist (www.morph.ldc.edu/Projects/ Chinese) which we label as ldc2ce. It has about 120K entries. Each entry maps a Chinese character sequence (character, word or phrase) into one or more English explanation strings delimited with slashes. Sample entries are shown below:  1) 2) 3) 4) 5)  ï /gather/assembly/meeting/convocation/ ½Ë /parts/components/assembly/.. × /assembly hall/auditorium/.. S7 /legislative assembly/ ª /legislative council/..  When an English word from a query is looked up in the ldc2ce wordlist, it will usually be mapped into many Chinese terms and reduction of the output is necessary. For this disambiguation purpose, we employ several methods in succession as tabulated below:  • Dictionary structure-based: ldc2ce format is employed to select the more correct mappings among word translations. For example, when the word to translate is ‘assembly’, we would pick line 1) and 2) only, rather than the additional 3) or 4) because in the latter two, ‘assembly’ appears in context with other words. • Phrase-based: ld2ce can also be regarded as a phrase dictionary by matching query strings with English explanations of Chinese terms, giving much more accurate phrase translations. For example, if ‘legislative assembly’ appears in a query, it would match line 4) exactly and correctly, and would supersede all other single word translations such as those from lines 1), 2), 3) and 5). • Corpus frequency-based: for single word translations with many candidates, those with higher occurrence frequency usually have higher probability of being correct. • Weight-based: a Chinese term set translated for one English word can be considered as a synonym set, so that each individual Chinese term is weighted with the inverse of the sum of the collection frequencies, and generally gives more effective retrieval. These dictionary disambiguation techniques have been implemented and tested with TREC collections. In general, they accumulatively lead to successively more accurate retrievals [4]. Their output can be demonstrated in our system. 2.2 Translation Using MT Software COTS MT software for English to Chinese (or vice versa) are now quite readily available on the market. They cost from scores to about a thousand dollars for a single license. These software mostly operate on the PC Windows platform. Their codes are proprietary and usually do not come with an API. Interfacing them with a UNIX and C platform thus becomes quite difficult and perhaps impossible. However, if one runs retrieval from a Windows environment, one can ‘cut and paste’ from their translation results. We investigated several and found that one from Mainland China called HuaJian (www.atlan.com) performs quite well. A number of other such packages can also be demonstrated within our system. Once an English query has been translated into Chinese, we can perform monolingual Chinese IR using our PIRCS system described in the next section. The two translation outcomes, from dictionary and MT software, can be combined for retrieval and the final result is usually more effective than single translation method alone [3]. 3. A SIMPLIFIED PIRCS RETRIEVAL SYSTEM PIRCS (Probabilistic Indexing and Retrieval – Components – System) is our in-house developed document retrieval system that has participated in all previous TREC large-scale blind retrieval experiments with consistently good results. It supports both English and Chinese languages. PIRCS retrieval approach is based on the probabilistic indexing and retrieval methods, but extended with the ability to account for the influence of term frequencies and item length of documents or queries. PIRCS can best be viewed as activation spreading in a three- layer network,  doc-focused QTD  qry-focused DTQ  wka  tk wki  qa wak  wik di  . . Q RSVq= k wak*wki  . . . .  .  D  RSVd= k wik*wka T RSV = a*RSVq + (1-a)*RSVd  Figure 1. 3-Layer PIRCS Network  Figure 1, that also supports learning from user-judged or pseudorelevant documents. The details of our model are given in [4, 5]. As shown in Figure 1, PIRCS treats queries and documents as objects of the same kind. They form a Q and a D layer of nodes connecting to the middle layer of term nodes. Retrieval means spreading activation from a query node via common term nodes to a document node and summed into a retrieval status value RSVd for ranking. This operation is gated by intervening edges with weights that are set according to the PIRCS model. An analogous operation is to spread activation from document to query nodes, resulting in another RSVq that has been shown to have similarity to a simple language model [2, 4]. The final retrieval status value RSV is a linear combination of the two. Documents are pre-processed to create a direct file, an inverted file and a master dictionary that contains all the content terms and their usage statistics extracted from the collection. After appropriate processing, the master dictionary helps construct the middle layer T nodes of Figure 1. The direct file facilitates obtaining the terms and statistics contained in a given document, and helps construct the D node and D-to-T edges with weights. The inverted file facilitates obtaining the posting information of a given term, and helps construct the T-to-D edges with weights. At query time, a Q layer of one node is formed and the query terms are located on the T layer and linked in to define the Q-to-T and T-to-Q edges with weights. Once the 3-layer network is defined, ranking of documents for the query is achieved by activation spreading Q-T-D realizing the document-focused retrieval status value RSVd, and vice versa for the query-focused RSVq. They are then linearly combined. This crosslingual PIRCS demonstration runs either on a SUN Solaris or Linux platform. The current implementation is a simplification of our batch PIRCS system and does not support automatic twostage retrieval for pseudo-relevance feedback. However, users can interactively modify their queries to perform manual feedback. 4. GUI FOR INTERACTIVE CLIR A simplified PIRCS system with first stage retrieval will be used for demonstrating English-Chinese CLIR. This system is based  on an applet-servlet model that runs on a UNIX operating system (such as Solaris or Linux). User’s interaction with PIRCS is supported via a GUI based on the Netscape browser (Internet Explorer is a better browser for this GUI, but UNIX has Netscape only). The applet or HTML forms in the browser communicate with the servlet on the Apache server. The servlet works as a bridge between the front-end program (in HTML and applet) and the background programs that do the translation or retrieval. Based on the input from the user, it can dispatch calls to the retrieval system and then format the output and send results back to the applet or directly into user’s browser through a customized applet-servlet channel via HTTP protocol. A GUI software that is modeled on that of ZPRISE (www.itl.nist.gov/iaui/894.02/works/papers/zp2/zp2.html) but enhanced for CLIR will be demonstrated. The GUI supports five windows: one for English query input and editing, a translation window for displaying the Chinese terms mapped from the English query via the ldc2ce wordlist, a search-result window for displaying the fifty top-ranked document identities after retrieval, a document box for displaying the content of the current selected document in Chinese, and another index box showing the index terms used for retrieval together with their frequency statistics. This allows a user to do CLIR interactively.  If run in a Windows environment, the translation box also allows input and editing for those users who know some Chinese. In this test system, all Chinese are GB-encoded. A query of a few words currently takes a few seconds for translation and about 20 seconds or more for retrieval depending on the number of unique terms. This response time can be improved in the future. A typical screen of the GUI is shown in Figure 2. A user starts by typing in an English query in free text. When the ‘Convert to Chinese GB’ button is clicked, translation via the LDC dictionary look-up based on a default (best) option will be displayed. Other options for translation such as using dictionary-structure only, add phrase matching, or include target collection frequency disambiguation, etc. (Section 2.1) can be chosen. If the user finds too many English words left un-translated, s/he can re-phrase the English query wordings and repeat the process. Otherwise, the retrieval button can be clicked and the top 50 document ID’s will be displayed in the search-result box (below the translation) sorted by the retrieval status value shown next to each ID. Content of the top document is also displayed automatically in the large window with index terms high-lighted. Additional documents following the one displayed can also be brought in for browsing purposes.  U.S. to separate the most-favored-nation status from human rights issue in China  Clear Convert into Chinese GB 4 1 6  cÒ Û ^1.00  ^^1.00  ^^1.00  ÕµÒ  Z  ^^1.00  ^^1.00  Å] Z ¦! ^^0.20  0.20  0.20  VØ V¦ 0.20  0.20  ^^1.00  # ^  Clear  Retrieve  pd9202-446300 1.5795 pd9202-446200 1.4633 pd9311-150201 1.4494 pd9304-241800 1.4196 C02bKBBW09Q<m 1.3629 pd9207-484400 1.3556 pd9304-248000 1.2974  Show documents in new window Top 50 documents  .!JN 65537 . !TI pd9311.150201 THE ýº!A;#BÒb à cû;èÒ;bÕcµÝÎÒZÒºÕÃ µmxÒßZÚ«Ãv8ms0VÖyv{&s|ýáÆâÒVýÃ;Ãum6ßuÛõâÚà" HmÒÐáÛ6àïac_|cÛºÒïý;®zë¬}!ÆÛûÒ¸:)T¦±>ª;ËÓúz¢*ãKqÚ¦"*=ËÃÉü||WáqéµïcÒuõ®ÕÃ" ½cüÒ;;àÏÕyÿ¸×|c|Òáé´cÒl;TÖâïü!ÄÒÀ &ý«]'`ÒÎ;ÕqµÒuZ`p|"µÒ *Ò¤V#àH;É#¦|ZH½6;cÒA«]Ò;ÕµÒZ|Ò4Ûï¼Â¦*Ãü|µ Òjà=|9qcHÐµÕµÒZÃmá¬F¢|UK$Û¼Â xay  Keywords used to search are:  Z 7737   16548  Z 15765  µ  
An important first step in developing a cross-lingual question answering system is to understand whether techniques developed with English text will also work with other languages, such as Chinese. The Marsha Chinese question answering system described in this paper uses techniques similar to those used in the English systems developed for TREC. Marsha consists of three main components: the query processing module, the Hanquery search engine, and the answer extraction module. It also contains some specific techniques dealing with Chinese language characteristics, such as word segmentation and ordinals processing. Evaluation of the system is done using a method based on the TREC question-answering track. The results of the evaluation show that the performance of Marsha is comparable to some English question answering systems in TREC 8 track. An English language version of Marsha further indicates that the heuristics used are applicable to the English question answering task. Keywords Question-Answering (QA); Search engine; multilingual retrieval, Chinese QA.  1. Introduction A number of techniques for “question answering” have recently been evaluated both in the TREC environment (Voorhees and Harman, 1999) and in the DARPA TIDES program. In the standard approach to information retrieval, relevant text documents are retrieved in response to a query. The parts of those documents that may contain the most useful information or even the actual answer to the query are typically indicated by highlighting occurrences of query words in the text. In contrast, the task of a questionanswering system is to identify text passages containing the relevant information and, if possible, extract the actual answer to the query. Question answering has a long history in natural language processing, and Salton’s first book (Salton, 1968) contains a detailed discussion of the relationship between information retrieval and questionanswering systems. The focus in recent research has been on extracting answers from very large text databases and many of the techniques use search technology as a major component. A significant number of the queries used in information retrieval experiments are questions, for example, TREC topic 338 “What adverse effects have people experienced while taking aspirin repeatedly?” and topic 308 “What are the advantages and/or disadvantages of tooth implants?” In question-answering experiments, the queries tend to be more restricted questions, where answers are likely to be found in a single text passage, for example, TREC question-answering question 11 “Who was President Cleveland’s wife?” and question 14 “What country is the biggest producer of Tungsten?” The TREC question-answering experiments have, to date, used only English text. As the first step towards our goal of cross-lingual question answering, we investigated whether the general approaches to question answering that have been used in English will also be effective for Chinese. Although it is now well known that statistical information  retrieval techniques are effective in many languages, earlier research, such as Fujii and Croft (1993, 1999), was helpful in pointing out which techniques were particularly useful for languages like Japanese. This research was designed to provide similar information for question answering. In the next section, we describe the components of the Chinese question answering system (Marsha) and the algorithm used to determine answers. In section 3, we describe an evaluation of the system using queries obtained from Chinese students and the TREC-9 Chinese cross-lingual database (164,779 documents from the Peoples Daily and the Xing-Hua news agencies in the period 1991-1995).  2. Overview of the Marsha Question Answering System The Chinese question-answering system consists of three main components. These are the query processing module, the Hanquery search engine, and the answer extraction module. The query processing module recognizes known question types and formulates queries for the search engine. The search engine retrieves candidate texts from a large database. The answer extraction module identifies text passages that are likely to contain answers and extracts answers, if possible, from these passages. This system architecture is very similar to other question-answering systems described in the literature.  More specifically, the query processing module carries out the following steps:  (1) The query is matched with templates to decide the  question type and the “question words” in the query. We  define 9 question types. Most of these correspond to typical  named entity classes used in information extraction systems.  For each question type, there are one or more templates.  Currently there are 170 templates. If more than one  template matches the question, we pick the longest match.  For example, a question may include “  ” (how many  dollars). Then both  (how many dollars) and  (how many) will match the question. In this case, we will  pick  and assign “MONEY” to the question type.  The following table gives examples for each question type:  TEMPLATE QUESTION TYPE  TRANSLATION  PERSON  which person  LOCATION  which city  ORGANIZATIO what organization N  DATE  what date  TIME MONEY PERCENTAGE NUMBER OTHER  what time how many dollars what is the percentage how many what is the meaning of  (2) Question words are removed from the query. This is a  form of “stop word” removal. Words like “  ”  (which person) are removed from the query since they are  unlikely to occur in relevant text.  (3) Named entities in the query are marked up using BBN’s IdentiFinder system. A named entity is kept as a word after segmentation.  (5) The query is segmented to identify Chinese words.  (6) Stop words are removed.  (7) The query is formulated for the Hanquery search engine. Hanquery is the Chinese version of Inquery (Broglio, Callan and Croft, 1996) and uses the Inquery query language that supports the specification of a variety of evidence combination methods. To support question answering, documents containing most of the query words were strongly preferred. If the number of query words left after the previous steps is greater than 4, then the operator #and (a probabilistic AND) is used. Otherwise, the probabilistic passage operator #UWn (unordered window) is used. The parameter n is set to twice the number of words in the query.  Hanquery is used to retrieve the top 10 ranked documents. The answer extraction module then goes through the following steps:  (8) IdentiFinder is used to mark up named entities in the documents.  (9) Passages are constructed from document sentences. We used passages based on sentence pairs, with a 1-sentence overlap. (10) Scores are calculated for each passage. The score is based on five heuristics: · First Rule: Assign 0 to a passage if no expected name entity is present. · Second Rule: Calculate the number of match words in a passage. Assign 0 to the passage if the number of matching words is less than the threshold. Otherwise, the score of this passage is equal to the number of matching words (count_m). The threshold is defined as follows: threshold = count_q if count_q<4 threshold = count_q/2.0+1.0 if 4<=count_q<=8 threshold = count_q/3.0+2.0 if count_q>8 count_q is the number of words in the query. · Third Rule: Add 0.5 to score if all matching words are within one sentence. · Fourth Rule: Add 0.5 to score if all matching words are in the same order as they are in the original question. · Fifth Rule: score = score + count_m/(size of matching window) (11) Pick the best passage for each document and rank them. (12) Extract the answer from the top passage: Find all candidates according to the question type. For example, if the question type is LOCATION, then each location marked by IdentiFinder is an answer candidate. An answer candidate is removed if it appears in the original question. If no candidate answer is found, no answer is returned. Calculate the average distance between an answer candidate and the location of each matching word in the passage.  Pick the answer candidate that has the smallest average distance as the final answer.  3. Evaluating the System We used 51 queries to do the initial evaluation of the question-answering system. We selected 26 queries from 240 questions collected from Chinese students in our department, because only these had answers in the test collection. The other 25 queries were constructed by either reformulating a question or asking a slightly different question. For example, given the question “which city is the biggest city in China?” we also generated the questions “where is the biggest city in China?” and “which city is the biggest city in the world?”.  The results for these queries were evaluated in a similar, but not identical way to the TREC question-answering track. An “answer” in this system corresponds to the 50 byte responses in TREC and passages are approximately equivalent to the 250 byte TREC responses.  For 33 of 51 queries, the system suggested answers. 24 of the 33 were correct. For these 24, the “reciprocal rank” is 1, since only the top ranked passage is used to extract answers. Restricting the answer extraction to the top ranked passage also means that the other 27 queries have reciprocal rank values of 0. In TREC, the reciprocal ranks are calculated using the highest rank of the correct answer (up to 5). In our case, using only the top passage means that the mean reciprocal rank of 0.47 is a lower bound for the result of the 50 byte task.  As an example, the question “  ” (Which city is the biggest city in China?), the answer  
We describe and present evaluation results for Talk’n’Travel, a spoken dialogue language system for making air travel plans over the telephone. Talk’n’Travel is a fully conversational, mixedinitiative system that allows the user to specify the constraints on his travel plan in arbitrary order, ask questions, etc., in general spoken English. The system was independently evaluated as part of the DARPA Communicator program and achieved a high success rate. .  Meaning and task state are represented by the path constraint representation (Stallard, 2000). An inference component is included which allows the system to deduce implicit requirements from explicit statements by the user, and to retract them if the premises change. The system is interfaced to the Yahoo/Travelocity flight schedule website, for access to live flight schedule information. Queries to the website are spawned off in a separate thread, which the dialogue manager monitors ands reports on to the user.  1. INTRODUCTION This paper describes and presents evaluation results for Talk’n’Travel, a spoken language dialogue system for making complex air travel plans over the telephone. Talk’n’Travel is a research prototype system sponsored under the DARPA Communicator program (MITRE, 1999). Some other systems in the program are Ward and Pellom (1999), Seneff and Polifroni (2000) and Rudnicky et al (1999). The common task of this program is a mixed-initiative dialogue over the telephone, in which the user plans a multi-city trip by air, including all flights, hotels, and rental cars, all in conversational English over the telephone. A similar research program is the European ARISE project (Den Os et al, 1999). An earlier version of Talk’n’Travel was presented in (Stallard, 2000). The present paper presents and discusses results of an independent evaluation of Talk’n’Travel, recently conducted as part of the DARPA Communicator program. The next section gives a brief overview of the system. 2. SYSTEM OVERVIEW The figure shows a block diagram of Talk’n’Travel. Spoken language understanding is provided by statistical N-gram speech recognition and a robust language understanding component. A plan-based dialogue manager coordinates interaction with the user, handling unexpected user input more flexibly than conventional finite-state dialogue control networks. It works in tandem with a state management component that adjusts the current model of user intention based on the user’s last utterance  Figure 1 : System Architecture  BYBLOS Recognizer  GEM NL Understander  Phone Speech Synthesizer  Dialog Manager Language Generator  Discourse State Manager  Flight Database  3. DIALOGUE STRATEGY Talk’n’Travel employs both open-ended and directed prompts. Sessions begin with open prompts like "What trip would you to take?". The system then goes to directed prompts to get any information the user did not provide ("What day are you leaving?", etc). The user may give arbitrary information at any prompt, however. The system provides implicit confirmation of the change in task state caused by the user’s last utterance ("Flying from Boston to Denver tomorrow ") to ensure mutual understanding. The system seeks explicit confirmation in certain cases, for example where the user appears to be making a change in date of travel. Once sufficient information is obtained, the system offers a set of candidate flights, one at a time, for the user to accept or reject.  4. EVALUATION 4.1 Evaluation Design The 9 groups funded by the Communicator program (ATT, BBN, CMU, Lucent, MIT, MITRE, SRI, and University of Colorado)  took part in an experimental common evaluation conducted by the National Institute of Standards and Technology (NIST) in June and July of 2000. A pool of approximately 80 subjects was recruited from around the United States. The only requirements were that the subjects be native speakers of American English and have Internet access. Only wireline or home cordless phones were allowed. The subjects were given a set of travel planning scenarios to attempt. There were 7 such prescribed scenarios and 2 open ones, in which the subject was allowed to propose his own task. Prescribed scenarios were given in a tabular format. An example scenario would be a round-trip flight between two cities, departing and returning on given dates, with specific arrival or departure time preferences. Each subject called each system once and attempted to work through a single scenario; the design of the experiment attempted to balance the distributions of scenarios and users across the systems. Following each scenario attempt, subjects filled out a Web-based questionnaire to determine whether subjects thought they had completed their task, how satisfied they were with using the system, and so forth. The overall form of this evaluation was thus similar to that conducted under the ARISE program (Den Os, et al 1999). 4.2 Results Table 1 shows the result of these user surveys for Talk’n’Travel. The columns represent specific questions on the user survey. The first column represents the user’s judgement as to whether or not he completed his task. The remaining columns, labeled Q1-Q5, are Likert scale items, for which a value of 1 signifies complete agreement, and 5 signifies complete disagreement. Lower numbers for these columns are thus better scores. The legend below the table identifies the questions. Table 1 : Survey Results Task Comp% Q1 Q2 Q3 Q4 Q5 BBN 80.5% 2.23 2.09 2.10 2.36 2.84 Mean 62.0% 2.88 2.23 2.54 2.95 3.36 Scale: 1 = strongly agree, 5 = strongly disagree Q1 It was easy to get the information I wanted Q2 I found it easy to understand what the system said Q3 I knew what I could do or say at each point in the dialog Q4 The system worked the way I expected it to Q5 I would use this system regularly to get travel information The first row gives the mean value for the measurements over all 78 sessions with Talk’n’Travel. The second row gives the mean value of the same measurements for all 9 systems participating. Talk’n’Travel’s task completion score of 80.5% was the highest for all 9 participating systems. Its score on question Q5, representing user satisfaction, was the second highest. An independent analysis of task completion was also performed by comparing the logs of the session with the scenario given.  Table 2 shows Talk’n’Travel’s results for this metric, which are close to that seen for the user questionnaire.  Table 2: Objective Analysis  Completion of required scenario 70.5%  Completion of different scenario 11.5%  Total completed scenarios  82.0%  Besides task completion, other measurements were made of system operation. These included time to completion, word error rate, and interpretation accuracy. The values of these measurements are given in Table 3.  Table 3: Other Metrics  Average time to completion 246 secs  Average word error rate  21%  Semantic error rate/utterance 10%  4.3 Analysis and Discussion We analyzed the log files of the 29.5% of the sessions that did not result in the completion of the required scenario. Table 4 gives a breakdown of the causes.  Table 4: Causes of Failure  City not in lexicon  39% (9)  Unrepaired recognition error 22% (5)  User error  17% (4)  System diversion  13% (3)  Other  9% (2)  The largest cause (39%) was the inability of the system to recognize a city referred to by the user, simply because that city was absent from the recognizer language model or language understander’s lexicon. These cases were generally trivial to fix. The second, and most serious, cause (22%) was recognition errors that the user either did not attempt to repair or did not succeed in repairing. Dates proved troublesome in this regard, in which one date would be misrecognized for another, e.g. “October twenty third” for “October twenty first Another class of errors were caused by the user, in that he either gave the system different information than was prescribed by the scenario, or failed to supply the information he was supposed to. A handful of sessions failed because of additional causes, including system crashes and backend failure. Both time to completion and semantic error rate were affected by scenarios that failed because because of a missing city. In such scenarios, users would frequently repeat themselves many times in a vain attempt to be understood, thus increasing total utterance count and utterance error.  An interesting result is that task success did not depend too strongly on word error rate. Even successful scenarios had an average WER of 18%, while failed scenarios had average WER of only 22%. A key issue in this experiment was whether users would actually interact with the system conversationally, or would respond only to directive prompts. For the first three sessions, we experimented with a highly general open prompt ("How can I help you?’), but quickly found that it tended to elicit overly general and uninformative responses (e.g. "I want to plan a trip"). We therefore switched to the more purposeful "What trip would you like to take?" for the remainder of the evaluation. Fully 70% of the time, users replied informatively to this prompt, supplying utterances "I would like an American flight from Miami to Sydney" that moved the dialogue forward. In spite of the generally high rate of success with open prompts, there was a pronounced reluctance by some users to take the initiative, leading them to not state all the constraints they had in mind. Examples included requirements on airline or arrival time. In fully 20% of all sessions, users refused multiple flights in a row, holding out for one that met a particular unstated requirement. The user could have stated this requirement explicitly, but chose not to, perhaps underestimating what the system could do. This had the effect of lengthening total interaction time with the system. 4.4 Possible Improvements Several possible reasons for this behavior on the part of users come to mind, and point the way to future improvements. The synthesized speech was fairly robotic in quality, which naturally tended to make the system sound less capable. The prompts themselves were not sufficiently variable, and were often repeated verbatim when a reprompt was necessary. Finally, the system’s dialogue strategy needs be modified to detect when more initiative is needed from the user, and cajole him with open prompts accordingly.  5. ACKNOWLEDGMENTS This work was sponsored by DARPA and monitored by SPAWAR Systems Center under Contract No. N66001-99-D8615. 6. REFERENCES [1] MITRE (1999) DARPA Communicator homepage http://fofoca.mitre.org/ [2] Ward W., and Pellom, B. (1999) The CU Communicator System. In 1999 IEEE Workshop on Automatic Speech Recognition and Understanding, Keystone, Colorado. [3] Den Os, E, Boves, L., Lamel, L, and Baggia, P. (1999) Overview of the ARISE Project. Proceedings of Eurospeech, 1999, Vol 4, pp. 1527-1530. 
This demonstration will motivate some of the significant properties of the Galaxy Communicator Software Infrastructure and show how they support the goals of the DARPA Communicator program. Keywords Spoken dialogue, speech interfaces 1. INTRODUCTION The DARPA Communicator program [1], now in its second fiscal year, is intended to push the boundaries of speechenabled dialogue systems by enabling a freer interchange between human and machine. A crucial enabling technology for the DARPA Communicator program is the Galaxy Communicator software infrastructure (GCSI), which provides a common software platform for dialogue system development. This infrastructure was initially designed and constructed b y MIT [2], and is now maintained and enhanced by the MITRE Corporation. This demonstration will motivate some of the significant properties of this infrastructure and show how they support the goals of the DARPA Communicator program. 2. HIGHLIGHTED PROPERTIES The GCSI is a distributed hub-and-spoke infrastructure which allows the programmer to develop Communicator-compliant servers in C, C++, Java, Python, or Allegro Common Lisp. This system is based on message passing rather than CORBA- or RPC-style APIs. The hub in this infrastructure supports routing of messages consisting of key-value pairs, but also supports logging and rule-based scripting. Such an infrastructure has the following desirable properties: • The scripting capabilities of the hub allow the programmer to weave together servers which may not otherwise have been intended to work together, b y rerouting messages and their responses and transforming  their keys. • The scripting capabilities of the hub allow the programmer to insert simple tools and filters to convert data among formats. • The scripting capabilities of the hub make it easy t o modify the message flow of control in real time. • The scripting capabilities of the hub and the simplicity of message passing make it simple to build up systems bit by bit. • The standard infrastructure allows the Communicator program to develop platform- and programminglanguage-independent service standards for recognition, synthesis, and other better-understood resources. • The standard infrastructure allows members of the Communicator program to contribute generally useful tools to other program participants. This demonstration will illustrate a number of these properties. 3. DEMO CONFIGURATION AND CONTENT By way of illustration, this demo will simulate a process of assembling a Communicator-compliant system, while at the same time exemplifying some of the more powerful aspects of the infrastructure. The demonstration has three phases, representing three successively more complex configuration steps. We use a graphical display of the Communicator hub to make it easy to see the behavior of this system. As you can see in Figure 1, the hub is connected to eight servers: • MITRE's Java Desktop Audio Server (JDAS) • MIT SUMMIT recognizer, using MIT's Mercury travel domain language model • CMU Sphinx recognizer, with a Communicator-compliant wrapper written by the University of Colorado Center for Spoken Language Research (CSLR), using CSLR's travel domain language model • A string conversion server, for managing incompatibilities between recognizer output and synthesizer input • CSLR's concatenative Phrase TTS synthesizer, using their travel domain voice  • CMU/Edinburgh Festival synthesizer, with a Communicator-compliant wrapper written by CSLR, using CMU's travel domain language model for Festival's concatenative voice • MIT TINA parser, using MIT's Mercury travel domain language model • MIT Genesis paraphraser, using MIT's Mercury travel domain language model Figure 2: Modifying the hub information state 3.3 Phase 2 Now that we've established audio connectivity, we can add recognition and synthesis. In this configuration, we will route the output of the preferred recognizer to the preferred synthesizer. When we change the path through the hub script using the graphical display, the preferred servers are highlighted. Figure 3 shows that the initial configuration of phase 2 prefers SUMMIT and Festival.  Figure 1: Initial demo configuration We will use the flexibility of the GCSI, and the hub scripting language in particular, to change the path that messages follow among these servers. 3.1 Phase 1 In phase 1, we establish audio connectivity. JDAS is MITRE's contribution to the problem of reliable access to audio resources. It is based on JavaSound 1.0 (distributed with JDK 1.3), and supports barge-in. We show the capabilities of JDAS by having the system echo the speaker's input; we also demonstrate the barge-in capabilities of JDAS bye showing that the speaker can interrupt the playback with a new utterance/input. The goal in building JDAS is that anyone who has a desktop microphone and the Communicator infrastructure will be able to use this audio server to establish connectivity with any Communicator-compliant recognizer or synthesizer. 3.2 Changing the message path The hub maintains a number of information states. The Communicator hub script which the developer writes can both access and update these information states, and we can invoke "programs" in the Communicator hub script by sending messages to the hub. This demonstration exploits this capability by using messages sent from the graphical display to change the path that messages follow, as illustrated i n Figure 2. In phase 1, the hub script routed messages from JDAS back to JDAS (enabled by the message named "Echo"). In the next phase, we will change the path of messages from JDAS and send them to a speech recognizer.  Figure 3: Initial recognition/synthesis configuration The SUMMIT recognizer and the Festival synthesizer were not intended to work together; in fact, while there is a good deal of activity in the area of establishing data standards for various aspects of dialogue systems (cf. [3]), there are n o programming-language-independent service definitions for speech. The hub scripting capability, however, allows these tools to be incorporated into the same configuration and t o interact with each other. The remaining incompatibilities (for instance, the differences in markup between the recognizer output and the input the synthesizer expects) are addressed b y the string server, which can intervene between the recognizer and synthesizer. So the GCSI makes it easy both to connect a variety of tools to the hub and make them interoperate, as well as to insert simple filters and processors to facilitate the interoperation. In addition to being able to send general messages to the hub, the user can use the graphical display to send messages associated with particular servers. So we can change the preferred recognizer or synthesizer. (as shown in Figure 4), or change the Festival voice (as shown in Figure 5). All these messages are configurable from the hub script. 
 3. METHODOLOGY For the present experiment the parsing model was trained on the entire treebank (99,720 words). We then prepared a new set of 20,202 segmented, POS-tagged words of Xinhua newswire text, which was blindly divided into 3 sets of equal size (±10 words). Each set was then annotated in two or three passes, as summarized by the following table:  Set  Pass 1  Pass 2  Pass 3  
We describe a novel method for detecting errors in task-based human-computer (HC) dialogues by automatically deriving them from semantic tags. We examined 27 HC dialogues from the DARPA Communicator air travel domain, comparing user inputs to system responses to look for slot value discrepancies, both automatically and manually. For the automatic method, we labeled the dialogues with semantic tags corresponding to "slots" that would be filled in "frames" in the course of the travel task. We then applied an automatic algorithm to detect errors in the dialogues. The same dialogues were also manually tagged (by a different annotator) to label errors directly. An analysis of the results of the two tagging methods indicates that it may be possible to detect errors automatically in this way, but our method needs further work to reduce the number of false errors detected. Finally, we present a discussion of the differing results from the two tagging methods. Keywords Dialogue, Error detection, DARPA Communicator. 1. INTRODUCTION In studying the contrasts between human-computer (HC) and human-human (HH) dialogues [1] it is clear that many HC dialogues are plagued by disruptive errors that are rarely seen in HH dialogues. A comparison of HC and HH dialogues may help us understand such errors. Conversely, the ability to detect errors in dialogues is critical to understanding the differences between HC and HH communication. Understanding HC errors is also crucial to improving HC interaction, making it more robust, trustworthy and efficient. The goal of the work described in this paper is to provide an annotation scheme that allows automatic calculation of misunderstandings and repairs, based on semantic information presented at each turn. If we represent a dialogue as a sequence of pairs of partially-filled semantic frames (one for the user’s  utterances, and one for the user’s view of the system state), we can annotate the accumulation and revision of information in the paired frames. We hypothesized that, with such a representation, it would be straightforward to detect when the two views of the dialogue differ (a misunderstanding), where the difference originated (source of error), and when the two views reconverge (correction). This would be beneficial because semantic annotation often is used for independent reasons, such as measurements of concepts per turn [8], information bit rate [9], and currently active concepts [10]. Given this, if our hypothesis is correct, then by viewing semantic annotation as a representation of filling slots in user and system frames, it should be possible to detect errors automatically with little or no additional annotation. 2. SEMANTIC TAGGING We tagged 27 dialogues from 4 different systems that participated in a data collection conducted by the DARPA Communicator program in the summer of 2000. These are dialogues between paid subjects and spoken language dialogue systems operating in the air travel domain. Each dialogue was labeled with semantic tags by one annotator. We focused on just the surface information available in the dialogues, to minimize inferences made by the annotator. The semantic tags may be described along two basic dimensions: slot and type. The slot dimension describes the items in a semantic frame that are filled over the course of a dialogue, such as DEPART_CITY and AIRLINE (see Table 1 for the complete list). The type dimension describes whether the tag is a PROMPT, a FILL, or an OFFER. This type dimension is critical to semantic analysis since it allows one to describe the effect a tag has on slots in the frame. PROMPTs are attempts to gather values to fill slots, e.g., "what city do you want to fly to". FILLs are actual slot fills, e.g., "I’d li ke to fly to San Francisco". OFFERs represent actual flight information based on previous slot FILLs, e.g., "there is a 9:45 flight to San Francisco on Delta". However, OFFERs often do not exactly match slot FILLs (e.g., the user requests a flight at 9:30, but the closest match flight is at 9:45), and thus must be distinguished from FILLs. In addition to the two basic dimensions of slot and type, each tag takes a leg attribute to indicate which leg of a trip is being discussed. There is also an initial USER_ID slot which has two types (PROMPT_USER_ID and FILL_USER_ID), but no leg attribute.  Figure 1. Portion of a Sample Dialogue Labeled with Semantic Tags S0: System said: Welcome to our travel planning system. <prompt_user_id>Please say your PIN number</prompt_user_id>. U1: User said: My PIN is <fill_user_id>four four four four</fill_user_id>. S2: Sys tem said: What are your travel plans? U3: User said: I want to go to <fill_arrive_city leg=”1”>San Francisco</fill_arrive_city>. S4: System said: Traveling to <fill_arrive_city leg=”1”>San Francisco</fill_arrive_city>. <prompt_depart_city leg=”1”>Where are you departing from?</prompt_depart_city> U5: User said: <fill_depart_city leg=”1”>Boston</fill_depart_city>. S6: System said: <prompt_date leg=”1”>When would you like to leave</prompt_date> <fill_depart_city leg=”1”>Boston</fill_depart_city>? U7: User said: <fill_date leg=”1”>July twenty-fifth</fill_date> at <fill_depart_time leg=”1”>12 noon</fill_depart_time>. S8: System said: I have a <offer_airline leg=”1”>Delta</offer_airline> flight that leaves <offer_depart_city leg=”1”>Boston</offer_depart_city> at <offer_depart_time leg=”1”>11:10AM</offer_depart_time> on <offer_date leg=”1”>July twenty-fifth</offer_date> and arrives in <offer_arrive_city leg=”1”>San Francisco</offer_arrive_city> at <offer_arrive_time>5:19PM</offer_arrive_time>. Is that OK?  Our semantic tag set also includes two special tags, YES and NO, for annotating responses to offers and yes/no questions. Finally, we have two tags, PROMPT_ERASE_ FRAMES and FILL_ERASE_FRAMES, for annotating situations where the frames are erased and the dialogue is restarted (e.g., the user says "start over"). Figure 1 shows part of a sample dialogue with semantic tags. Our semantic tagset is summarized in Table 1. Table 1. Semantic Tagset  DEPART_CITY ARRIVE_CITY DEPART_AIRPORT ARRIVE_AIRPORT DATE DEPART_TIME ARRIVE_TIME 
We present the design and development of a Hidden Markov Model for the division of news broadcasts into story segments. Model topology, and the textual features used, are discussed, together with the non-parametric estimation techniques that were employed for obtaining estimates for both transition and observation probabilities. Visualization methods developed for the analysis of system performance are also presented. 1. INTRODUCTION Current technology makes the automated capture, storage, indexing, and categorization of broadcast news feasible allowing for the development of computational systems that provide for the intelligent browsing and retrieval of news stories [Maybury, Merlino & Morey ‘97; Kubula, et al., ‘00]. To be effective, such systems must be able to partition the undifferentiated input signal into the appropriate sequence of news-story segments. In this paper we discuss an approach to segmentation based on the use of a fine-grained Hidden Markov Model [Rabiner, `89] to model the generation of the words produced during a news program. We present the model topology, and the textual features used. Critical to this approach is the application of non-parametric estimation techniques, employed to obtain robust estimates for both transition and observation probabilities. Visualization methods developed for the analysis of system performance are also presented. Typically, approaches to news-story segmentation have been based on extracting features of the input stream that are likely to be different at boundaries between stories from what is observed within the span of individual stories. In [Beeferman, Berger, & Lafferty ‘99], boundary decisions are based on how well predictions made by a long-range exponential language model compare to those made by a short range trigram model. [Ponte and Croft, ‘97] utilize Local Context Analysis [Xu, J. and Croft, ‘96]  to enrich each sentence with related words, and then use dynamic programming to find an optimal boundary sequence based on a measure of word-occurrence similarity between pairs of enriched sentences. In [Greiff, Hurwitz & Merlino, `99], a naïve Bayes classifier is used to make a boundary decision at each word of the transcript. In [Yamron, et al., ‘98], a fully connected Hidden Markov Model is based on automatically induced topic clusters, with one node for each topic. Observation probabilities for each node are estimated using smoothed unigram statistics. The approach reported in this paper goes further along the lines of find-grained modeling in two respects: 1) differences in feature patterns likely to be observed at different points in the development of a news story are exploited, in contrast to approaches that focus on boudary/no-boundary differences; and 2) a more detailed modeling of the story-length distribution profile, unique to each news source (for example, see the histogram of story lengths for ABC World News Tonight shown in the top graph of Figure 3, below). 2. GENERATIVE MODEL  
In this paper, we explore the effects of data fusion on First Story Detection [1] in a broadcast news domain. The data fusion element of this experiment involves the combination of evidence derived from two distinct representations of document content in a single cluster run. Our composite document representation consists of a concept representation (based on the lexical chains derived from a text) and free text representation (using traditional keyword index terms). Using the TDT1 evaluation methodology we evaluate a number of document representation strategies and propose reasons why our data fusion experiment shows performance improvements in the TDT domain. Keywords Lexical Chaining, Data Fusion, First Story Detection. 1. INTRODUCTION The goal of TDT is to monitor and reorganize a stream of broadcast news stories in such a way as to help a user recognize and explore different news events that have occurred in the data set. First story detection (or online new event detection [1]) is one aspect of the detection problem which constitutes one of the three technical tasks defined by the TDT initiative (the other two being segmentation and tracking). Given a stream of news stories arriving in chronological order, a detection system must group or cluster articles that discuss distinct news events in the data stream. The TDT initiative has further clarified the notion of topic detection by differentiating between classification in a retrospective (Event Clustering) and an online environment (First Story Detection). In FSD the system must identify all stories in the data stream that discuss novel news events. This classification decision is made by considering only those documents that have arrived prior to the current document being evaluated, forcing the system to adhere to the temporal constraints of a real-time news stream.  In other words the system must make an irrevocable classification decision (i.e. either the document discusses a new event or previously detected event) as soon as the document arrives on the input stream. The goal of event clustering on the other hand is to partition the data stream into clusters of related documents that discuss distinct events. This decision can be made after the system has considered all the stories in the input stream. In addition to defining three research problems associated with broadcast news, the TDT initiative also attempted to formally define an event with respect to how it differs from the traditional IR notion of a subject or a topic as defined by the TREC community. An event is defined as ‘something that happens at some specific time and place (e.g. an assassination attempt, or a volcanic eruption in Greece)’. A topic on the other hand is a ‘seminal event or activity along with all directly related events and activities (e.g. an investigation or a political campaign)’ [1]. Initial TDT research into event tracking and detection focused on developing a classification algorithm to address this subtle distinction between an event and a topic. For example successful attempts were made to address the temporal nature of news stories1 by exploiting the time between stories when determining their similarity in the detection process [1]. However current research is now focusing on the use of NLP techniques such as language modeling [2, 3], or other forms of feature selection like the identification of events based on the domain dependencies between words [4], or the extraction of certain word classes from stories i.e. noun phrases, noun phrases heads [5]. All these techniques offer a means of determining the most informative features about an event as opposed to classifying documents based on all the words in the document. The aim of our research is also based on this notion of feature selection. In this paper we investigate if the use of lexical chains to classify documents can better encapsulate this notion of an event. In particular we look at the effect on FSD when a composite document representation (using a lexical chain representation and free text representation) is used to represent events in the TDT domain. 
CCC&(¦¨Hf¤§242¤§24HfH1#)£6C24''01#H!H!H!)1#''1#!!)Y¤§78))Y)£U¨FDPc C¤Q9§PC¦¨)'Hi`¦28H9PCU2419`X6))4)4'1h74HiPC¤§6¤§D7828''!)`C)£91#))D24¤PCDh¤§W'C1#P¦¨PC)241#P¦P28)£283)£c)4)4tq!35D1#E¦¨2878'19RT'D'''9DP))¤§2!w6¤BPD¨EX§782)'Y¤8'A!EC48PC19CRTH!78R©F¨)H)¤BR2©C©q 2¦¨b9Y62) ¦hC28d'2'F`PQ'678'Dp%2£)24@)i!¦¨!CFT16C¦6¦¨¦¨¦782D'!¤dH9r PC28HfCC' C)X')EC`2H9H`74D!7I¤§¤R¦¨C24cP%9224U)¤§PCD)4)DYPC'h2tHPC'C9F@'2PCb#6Hf¨DH9)4')74'u b#H289R0 '2478H¤B'Y'¤Hi 28!)CA!7¦¨7C)¦¨7Q¦¨24!¤c3'28h¤B)£9¤b2c4)420 )£C)'7£'9!¤§)2)')DC')'1AD'20¤§U@R6¨FPC¦¤¤§@PRT)21H0 24¨¦20©'©28`b9')£b3X)EGc¦¨289PC!V'ET2¨¤§Aq 'P§DxyC¦¨¦!H2C'¤'74¦¨21H!C128¨T'A!H974I!§RT1#'!)Ih)1#PC)eD¦1#CD9)!2`A ¦¨24¦¨aPH9¤PC¤§24!2£7474¦' 2¦¨)6D)v!PCCa¤a24¤§'9`C¦¨PC'P%H9D7£742PC'I7e)¤B¨` CCPCCb9PC2£9¦¨CCCH9)2¨H!28¤§1!¤§)P¥H9b#¤gH9¤§UC''@DQe)`D78¦PCYyH¨¨¤¥CPb#7e!228¤P'Cb#6Dc'6'b!24I¤e'62S2wf¤%)£28D'¤§Y'!¦¨)`')PeI'28H¦¨CCRH2£RT28¦¨¦¨h2eC28)d2PC282)£C7£Sh2H!)4H9c78C24'74's ')4AF`aC@) 'aH9¨!¨PC9H'H9¤BCU24)1#)e§Q)¤§¨)HV)dD'PeCP¥782£Dc8b#¤@2c¨7e)PDP§62)2C'F¤§@) cP281' H9¦¨`1#'¤BC¤§)4)4)4D)'A!!!D3)IC¨2£'9'9'PC24¦¨)C¤wf¤P§¤B`28C¤ g))¤C1!PCCCI6©)4¤Tcc78CH!24¨7474D¦¨`PCR'2!H!D&24DDD)£22 ¦¦'''wPPPPWWWW``  1. INTRODUCTION CPCC1#!7£YCH6DH!klP¤§D D)'PCD&(DC'F28PSCD7t¦9!¤TCAu)`hf¤§782 9chD!¤H' ¤§e¦¨PD¦1#¦' )AAeP§¤B)a24¤§!A§¤§35)£Y2PET9!DPC')' 2UPC24¤B'CXIHe'CP' 2!E%!74Y1!7Q7£2fAvPCfhPC)`DC ¦¨¨H©m 28)¤¥')¨I)74CC24CD'9pn ¨22@2P CC`ET'¦H!1!A`¤BCPC¥'!PCA6H¤§D`2424`¤B9PC¤B) 24PCC28A6!blIPCg ¤B2461!FCCF'9dH9)) E6C) g vCE¦¨CH9C228H9¨6b9'278¤BP'6!A6Y)' 9Eg¤BP§F¦d2!CPC¤§¤§`)24DDC 9'i PCPCHPCb!¤B¤CH9) CC)©D¤§¥XH92¨P¤pj`o2fh) I@9))16C28 b!tUHPC¤B' H!¦¨¨d24) vCRP§¨¤I¦!24CPCA 28C69Db#92f20DP¤c `¦hC' DDPCC¦¤§PCElAS9'H9¨ D)¦¤§7`24A6282¦¨'D9¦h) c¦¨ 24)2¨78'YDRP¤ HH7PWWWW  u¥$ xyv8H9v4w8¨¤@w6xYPCW(Dy8¤§y£ W() z4PCy8Uw4Hu{ R §PC24¦ )£' F~¥D )4'v  
The limited coverage of available translation lexicons can pose a serious challenge in some cross-language information retrieval applications. We present two techniques for combining evidence from dictionary-based and corpus-based translation lexicons, and show that backoff translation outperforms a technique based on merging lexicons. 1. INTRODUCTION The effectiveness of a broad class of cross-language information retrieval (CLIR) techniques that are based on term-by-term translation depends on the coverage and accuracy of the available translation lexicon(s). Two types of translation lexicons are commonly used, one based on translation knowledge extracted from bilingual dictionaries [1] and the other based on translation knowledge extracted from bilingual corpora [8]. Dictionaries provide reliable evidence, but often lack translation preference information. Corpora, by contrast, are often a better source for translations of slang or newly coined terms, but the statistical analysis through which the translations are extracted sometimes produces erroneous results. In this paper we explore the question of how best to combine evidence from these two sources. 2. TRANSLATION LEXICONS Our term-by-term translation technique (described below) requires a translation lexicon (henceforth tralex) in which each word f is as- f g sociated with a ranked set e1 e2 : : : en of translations. We used two translation lexicons in our experiments. 2.1 WebDict Tralex We downloadeda freely available, manually constructed EnglishFrench term list from the Web1 and inverted it to French-English 1 http://www.freedict.com .  format. Since the WebDict translations appear in no particular order, we ranked the ei based on target language unigram statistics calculated over a large comparable corpus, the English portion of the Cross-LanguageEvaluation Forum (CLEF) collection, smoothed with statistics from the Brown corpus, a balanced corpus covering many genres of English. All single-word translations are ordered by decreasing unigram frequency, followed by all multi-word translations, and ﬁnally by any single-word entries not found in either corpus. This ordering has the effect of minimizing the effect of infrequent words in non-standard usages or of misspellings that sometimes appear in bilingual term lists. 2.2 STRAND Tralex Our second lexical resource is a translation lexicon obtained fully automatically via analysis of parallel French-English documents from the Web. A collection of 3,378 document pairs was obtained using STRAND, our technique for mining the Web for bilingual text [7]. These document pairs were aligned internally, using their HTML markup, to produce 63,094 aligned text “chunks” ranging in length from 2 to 30 words, 8 words on average per chunk, for a total of 500K words per side. Viterbi word-alignments for these paired chunks were obtained using the GIZA implementation of the IBM statistical translation models.2 An ordered set of translation pairs was obtained by treating each alignment link between words as a co-occurrence and scoring each word pair according to the likelihood ratio [2]. We then rank the translation alternatives in order of decreasing likelihood ratio score. 3. CLIR EXPERIMENTS Ranked tralexes are particularly well suited to a simple ranked term-by-term translation approach. In our experiments, we use top2 balanced document translation, in which we produce exactly two English terms for each French term. For terms with no known translation, the untranslated French term is generated twice (often appropriate for proper names). For French terms with one translation, that translation is generated twice. For French terms with two or more translations, we generate the ﬁrst two translations in the tralex. Thus balanced translation has the effect of introducing a uniform weighting over the top n translations for each term (here n = 2). Beneﬁts of the approach include simplicity and modularity — notice that a lexicon containing ranked translations is the only requirement, and in particular that there is no need for access to the internals of the IR system or to the document collection in order to 2 http://www.clsp.jhu.edu/ws99/projects/mt/  perform computations on term frequencies or weights. In addition, the approach is an effective one: in previous experiments we have found that this balanced translation strategy signiﬁcantly outperforms the usual (unbalanced) technique of including all known translations [3]. We have also investigated the relationship between balanced translation and Pirkola’s structured query formulation method [6]. For our experiments we used the CLEF-2000 French document collection (approximately 21 million words from articles in Le Monde). Differences in use of diacritics, case, and punctuation can inhibit matching between tralex entries and document terms, so we normalize the tralex and the documents by converting characters to lowercase and removing all diacritic marks and punctuation. We then translate the documents using the process described above, index the translated documents with the Inquery information retrieval system, and perform retrieval using “long” queries formulated by grouping all terms in the title, narrative, and description ﬁelds of each English topic description using Inquery’s #sum operator. We report mean average precision on the 34 topics for which relevant French documents exist, based on the relevance judgments provided by CLEF. We evaluated several strategies for using the WebDict and STRAND tralexes. 3.1 WebDict Tralex Since a tralex may contain an eclectic mix of root forms and morphological variants, we use a four-stage backoff strategy to maximize coverage while limiting spurious translations: 1. Match the surface form of a document term to surface forms of French terms in the tralex. 2. Match the stem of a document term to surface forms of French terms in the tralex. 3. Match the surface form of a document term to stems of French terms in the tralex. 4. Match the stem of a document term to stems of French terms in the tralex. We used unsupervisedinduction of stemming rules based on the French collection to build the stemmer [5]. The process terminates as soon as a match is found at any stage, and the known translations for that match are generated. The process may produce an inappropriate morphological variant for a correct English translation, so we used Inquery’s English kstem stemmer at indexing time to minimize the effect of that factor on retrieval effectiveness. 3.2 STRAND Tralex One limitation of a statistically derived tralex is that any term has some probability of aligning with any other term. Merely sorting translation alternatives in order of decreasing likelihood ratio will thus ﬁnd some translation alternatives for every French term that appeared at least once in the set of parallel Web pages. In order to limit the introduction of spurious translations, we included only translation pairs with at least N co-occurrences in the set used to build the tralex. We performed runs with N = 1 2 3, using the four-stage backoff strategy described above. 3.3 WebDict Merging using STRAND When two sources of evidence with different characteristics are available, a combination-of-evidence strategy can sometimes outperform either source alone. Our initial experiments indicated that the WebDict tralex was the better of the two (see below), so we adopted a reranking strategy in which the WebDict tralex was reﬁned according a voting strategy to which both the original WebDict and STRAND tralex rankings contributed.  Condition STRAND (N = 1) STRAND (N = 2) STRAND (N = 3) Merging WebDict Backoff  MAP 0.2320 0.2440 0.2499 0.2892 0.2919 0.3282  Table 1: Mean Average Precision (MAP), averaged over 34 topics  For each French term that appeared in both tralexes, we gave the top-ranked translation in each tralex a score of 100, the next a score of 99, and so on. We then summed the WebDict and STRAND scores for each translation, reranked the WebDict translations based on that sum, and then appended any STRAND-only translations for that French term. Thus, although both sources of evidence were weighted equally in the voting, STRAND-only evidence received lower precedence in the merged ranking. For French terms that appeared in only one tralex, we included those entries unchanged in the merged tralex. In this experiment run we used a threshold of N = 1, and applied the four-stage backoff strategy described above to the merged resource. 3.4 WebDict Backoff to STRAND A possible weakness of our merging strategy is that inﬂected forms are more common in our STRAND tralex, while root forms are more common in our WebDict tralex. STRAND tralex entries that were copied unchanged into the merged tralex thus often matched in step 1 of the four-stage backoff strategy, preventing WebDict contributions from being used. With the WebDict tralex outperforming the STRAND tralex, this factor could hurt our results. As an alternative to merging, therefore, we also tried a simple backoff strategy in which we used the original WebDict tralex with the four-stage backoff strategy described above, to which we added a ﬁfth stage in the event that fewer than two WebDict tralex matches were found: 5. Match the surface form of a document term to surface forms of French terms in the STRAND tralex. We used a threshold of N = 2 for this experiment run. 4. RESULTS Table 1 summarizes our results. Increasing thresholds seem to be helpful with the STRAND tralex, although the differences were not found to be statistically signiﬁcant by a paired two-tailed t-test with p < 0:05. Merging the tralexes provided no improvement over using the WebDict tralex alone, but our backoff strategy produced a statistically signiﬁcant 12% improvement in mean average precision (at p < 0:01) over the next best tralex (WebDict alone). As Figure 1 shows, the improvement is remarkably consistent, with only four of the 34 topics adversely affected and only one topic showing a substantial negative impact. Breaking down the backoff results by stage (Table 2), we ﬁnd that the majority of query-to-document hits are obtained in the ﬁrst stage, i.e. matches of the term’s surface form in the document to a translation of the surface form in the dictionary. However, the backoff process improves by-token coverage of terms in documents by 8%, and gives a 3% relative improvement in retrieval results; it also contributed additional translations to the top-2 set in approximately 30% of the cases, leading to the statistically signiﬁcant 12% relative improvement in mean average precision as compared to the baseline using WebDict alone with 4-stage backoff.  Figure 1: WebDict-to-tralex backoff vs. WebDict alone, by query  Stage (forms) 
In this paper we describe a technique for improving the performance of an information extraction system for speech data by explicitly modeling the errors in the recognizer output. The approach combines a statistical model of named entity states with a lattice representation of hypothesized words and errors annotated with recognition conﬁdence scores. Additional reﬁnements include the use of multiple error types, improved conﬁdence estimation, and multipass processing. In combination, these techniques improve named entity recognition performance over a textbased baseline by 28%. Keywords ASR error modeling, information extraction, word conﬁdence 1. INTRODUCTION There has been a great deal of research on applying natural language processing (NLP) techniques to text-based sources of written language data, such as newspaper and newswire data. Most NLP approaches to spoken language data, such as broadcast news and telephone conversations, have consisted of applying text-based systems to the output of an automatic speech recognition (ASR) system; research on improving these approaches has focused on either improving the ASR accuracy or improving the textbased system (or both). However, applying text-based systems to ASR output ignores the fact that there are fundamental differences between written texts and ASR transcriptions of spoken language: the style is different between written and spoken language, the transcription conventions are different, and, most importantly, there are errors in ASR transcriptions. In this work, we focus on the third problem: handling errors by explicitly modeling uncertainty in ASR transcriptions.  The idea of explicit error handling in information extraction (IE) from spoken documents was introduced by Grishman in [1], where a channel model of word insertions and deletions was used with a deterministic pattern matching system for information extraction. While the use of an error model resulted in substantial performance improvements, the overall performance was still quite low, perhaps because the original system was designed to take advantage of orthographic features. In looking ahead, Grishman suggests that a probabilistic approach might be more successful at handling errors. The work described here provides such an approach, but introduces an acoustically-driven word conﬁdence score rather than the word-based channel model proposed in [1]. More speciﬁcally, we provide a uniﬁed approach to predicting and using uncertainty in processing spoken language data, focusing on the speciﬁc IE task of identifying named entities (NEs). We show that by explicitly modeling multiple types of errors in the ASR output, we can improve the performance of an IE system, which beneﬁts further from improved error prediction using new features derived from multi-pass processing. The rest of the paper is organized as follows. In Section 2 we describe our error modeling, including explicit modeling of multiple ASR error types. New features for word conﬁdence estimation and the resulting performance improvement is given in Section 3. Experimental results for NE recognition are presented in Section 4 using Broadcast News speech data. Finally, in Section 5, we summarize the key ﬁndings and implications for future work. 2. APPROACH Our approach to error handling in information extraction involves using probabilistic models for both information extraction and the ASR error process. The component models and an integrated search strategy are described in this section.  2.1 Statistical IE  We use a probabilistic IE system that relates a word se-  quence £¥¤§¦©¨¢¦ to a sequence of information  states ¤! ¨ "  that provide a simple parse of the  word sequence into phrases, such as name phrases. For  the work described here, the states $# correspond to dif-  .  ferent types of NEs. The IE model is essentially a phrase  language model:  %'& '"£)(0¤ ¤  %1& ¨ "  2¦ ¨ 3¦  (  (1)  4  %'& ¦7#98 ¦7#A@ ¨ "¢#B( %1& $#"8 ¢#A@ ¨ ¦7#C@ ¨ (  #65 ¨  with state-dependent bigrams %'& ¦ # 8 ¦ #A@ ¨¢3 # ( that model the types of words associated with a speciﬁc type of NE, and state transition probabilities %'& $#"8 ¢#A@ ¨ 2¦7#A@ ¨ ( that mix the Markov-like structure of an HMM with dependence on the previous word. (Note that titles, such as “President” and “Mr.”, are good indicators of transition to a name state.) This IE model, described further in [2], is similar to other statistical approaches [3, 4] in the use of state dependent bigrams, but uses a different smoothing mechanism and state topology. In addition, a key difference in our work is explicit error modeling in the “word” sequence, as described next.  2.2 Error Modeling  To explicitly model errors in the IE system, we intro-  dD uce new notation for the hypothesized word sequence, ¤FEG¨¢3EH , which may differ from the actual word  sI equence £ , and ¤QPR¨¢GPS  a sequence of , where P # ¤UT  error indicator when E # is an  variables error and  PV#¤XW when EY# is correct. We assume that the hypothe-  sized words from the recognizer are each annotated with  conﬁdence scores  ` #a¤ %'& PV#1¤FWR8 D 3bc(d¤ %'& EY#a¤e¦7#98 D 3bf(9  where b represents the set of features available for initial conﬁdence estimation from the recognizer, acoustic or otherwise.  h t-1  ht  . . .  . . .  ε  ε  Figure 1: Lattice with correct and error paths.  We construct a simple lattice from E ¨ ¢3E  with “error” arcs indicated by g -tokens in parallel with each hypothesized word E # , as illustrated in Figure 1. We then ﬁnd the maximum posterior probability state sequence by summing over all paths through the lattice:  ihp¤ qsr3tvuwy qsx %'& 8 D bf(9  (2)  ¤ qsr3tvuwy qsx A %1& a I 8 D 3bf(  (3)  or, equivalently, marginalizing over the sequence I . Equation 3 thus deﬁnes the decoding of named entities via the state sequence  , which (again) provides a parse of the word sequence into phI rases. D Assuming ﬁrst that and encode all the information from b about  , and then that the speciﬁc value EH# occurring at an error does not provide additional information for  the NE states1  , we can rewrite Equation 3 as:   h ¤ qsr3tvuwy qsx   %'& I 8 D bc( %'& 8 I  D 3bc(  ¤  qsr3tvuwy qsx    %'& I  D 8  bc( %'& 8 I  D   (  ¤  qsr3tvuwy qsx    %'& I  D 8  bc( %'& 8 £   (9  For the error model, %1& I 8 D 3bf( , we assume that erwroorsrdarseeqcuoenndciteioD naallnyditnhdeepeveinddeennctegbiv:en the hypothesized  %'& I 8 D bc(d¤ 4  %1& P # 8 D bc(9  (4)  #5 ¨  where ` # ¤ %1& P # ¤W8 D 3bf( is the ASR word “conﬁ-  dence”. Of course, the errors are not independent, which  we take advantage of in our post-processing of conﬁdence  estimates, described in Section 3. We can ﬁnd %'& 8 £)( directly from the information ex- traction model, %'& '3£)( described in Section 2.1, but there  is no efﬁcient decoding algorithm. Hence we approximate  %1& 78 £)(a¤ %'%'& '& £) "£)(( §%' & '"£)(  (5)  assuming that the different words that could lead to an er-  ror are roughly uniform over the likely set. More speciﬁcally, %'& '"£)( incorporates a scaling term as follows:   %'& g$8 ¦7#C@   ¨  ¤eY"¢#2(d¤  T  %'& g$8 ¦7#A@ ¨ ¤eY"¢#2(  (6)   where is the number of different error words observed after  in the training set and %'& g$8 Y3$#B( is trained by col-  lapsing all different errors into a single label g . Training  this language model requires data that contains g -tokens,  which can be obtained by aligning the reference data and  the ASR output. In fact, we train the language model with  a combination of the original reference data and a dupli-  cate version with g -tokens replacing error words.  Because of the conditional independence assumptions  behind equations 1 and 4, there is an efﬁcient algorithm  for solving equation 3, which combines steps similar to  the forward and Viterbi algorithms used with HMMs. The  search is linear with the length  of the hypothesized  word sequence and the size of the state space (the product  space of NE states and error states). The forward compo-  nent is over the error state (parallel branches in the lattice),  and the Viterbi component is over the NE states.  If the goal is to ﬁnd the words that are in error (e.g. for  subsequent correction) as well as the named entities, then  the objective is  & a I (3hp¤   q$r3tdy uw  qsx %'& ' I q$r3tdy uw  qsx %'& I 8 D  8 D bc( 3bc( %G& a"£   (7)   e (9 (8)  ¨ Clearly, some hypotheses do provide information about  in that a reasonably large number of errors involve simple ending differences. However, our current system has no mechanism for taking advantage of this information explicitly, which would likely add substantially to the complexity of the model.  I which simply involves ﬁnding the best path h through the lattice in Figure 1. Again because of the conditional independence assumption, an efﬁcient solution involves Viterbi decoding over an expanded state spacI e (the product of the names and errors). The sequence h can help us deﬁne a new word sequence £ f that contains g -tokens: ¦ f # ¤UE # if P # h ¤UW , and ¦ f # ¤gg if P # h ¤hT . Joint error and named entity decoding results in a small degradation in named entity recognition performance, since only a single error path is used. Since errors are not used explicitly in this work, all results are based on the objective given by equation 3. Note that, unlike work that uses conﬁdence scores ` # as a weight for the hypothesized word in information retrieval [5], here the conﬁdence scores also provide weights & Ti ` # ( for explicit (but unspeciﬁed) sets of alternative hypotheses. 2.3 Multiple Error Types Though the model described above uses a single error token g and a 2-category word conﬁdence score (correct word vs. error), it is easily extensible to multiple classes of errors simply by expanding the error state space. More speciﬁcally, we add multiple parallel arcs in the lattice in Figure 1, labeled g ¨ , g"j , etc., and modify conﬁdence estimation to predict multiple categories of errors. In this work, we focus particularly on distinguishing out-of-vocabulary (OOV) errors from in-vocabulary (IV) errors, due to the large percentage of OOV words that are names (57% of OOVs occur in named entities). Looking at the data another way, the percentage of name words that are OOV is an order of magnitude larger than words in the “other” phrase category, as described in more detail in [6]. As it turns out, since OOVs are so infrequent, it is difﬁcult to robustly estimate the probability of IV vs. OOV errors from standard acoustic features, and we simply use the relative prior probabilities to scale the single error probability.  3. CONFIDENCE PREDICTION  levAelnceosnsﬁendteinalcecosmcoproen, e%1n&tPSo#"f8 D  our error model is the word3bf( , so one would expect  that better conﬁdence scores would result in better error  modeling performance. Hence, we investigated methods  for improving the conﬁdence estimates, focusing speciﬁ-  cally on introducing new features that might complement  the features used to provide the baseline conﬁdence esti-  mates. The baseline conﬁdence scores used in this study  were provided by Dragon Systems. As described in [7],  the Dragon conﬁdence predictor used a generalized lin-  ear model with six inputs: the word duration, the lan-  guage model score, the fraction of times the word appears  in the top 100 hypotheses, the average number of active  HMM states in decoding for the word, a normalized acous-  tic score and the log of the number of recognized words  in the utterance. We investigated several new features, of  which the most useful are listed below.  First, we use a short window of the original conﬁdence scores: ` # , ` #C@ ¨ and ` #k ¨ . Note that the post-processing paradigm allows us to use non-causal features such as ` #k ¨ . We also deﬁne three features based on the ratios of ` #A@ ¨ , ` # , and ` #6k ¨ to the average conﬁdence for the document  in which E # appears, under the assumption that a low con-  ﬁdence score for a word is less likely to indicate a word error if the average conﬁdence for the entire document is also low. We hypothesized that words occurring frequently in a large window would be more likely to be correct, again assuming that the ASR system would make errors randomly from a set of possibilities. Therefore, we deﬁne features based on how many times the hypothesis word EY# occurs in a window & EY#A@Ylm¢nnn"EY#"onn"EY#kml( for p ¤rq , 10, 25, 50, and 100 words. Finally, we also use the relative frequency of words occurring as an error in the training corpus, again looking at a window of stT around the current word. Due to the close correlation between names and errors, we would expect to see improvement in the error modeling performance by including information about which words are names, as determined by the NE system. Therefore, in addition to the above set of features, we deﬁne a new feature: whether the hypothesis word E # is part of a location, organization, or person phrase. We can determine the value of this feature directly from the output of the NE system. Given this additional feature, we can deﬁne a multi-pass processing cycle consisting of two steps: conﬁdence re-estimation and information extraction. To obtain the name information for the ﬁrst pass, the conﬁdence scores are re-estimated without using the name features, and these conﬁdences are used in a joint NE and error decoding system. The resulting name information is then used, in addition to all the features used in the previous pass, to improve the word conﬁdence estimates. The improved conﬁdences are in turn used to further improve the performance of the NE system. We investigated three different methods for using the above features in conﬁdence estimation: decision trees, generalized linear models, and linear interpolation of the outputs of the decision tree and generalized linear model. The decision trees and generalized linear models gave similar performance, and a small gain was obtained by interpolating these predictions. For simplicity, the results here use only the decision tree model. A standard method for evaluating conﬁdence prediction [8] is the normalized cross entropy (NCE) of the binary correct/error predictors, that is, the reduction in uncertainty in conﬁdence prediction relative to the ASR system error rate. Using the new features in a decision tree predictor, the NCE score of the binary conﬁdence predictor improved from 0.195 to 0.287. As shown in the next section, this had a signiﬁcant impact on NE performance. (See [6] for further details on these experiments and an analysis of the relative importance of different factors.) 4. EXPERIMENTAL RESULTS The speciﬁc information extraction task we address in this work is the identiﬁcation of name phrases (names of persons, locations, and organizations), as well as identiﬁcation of temporal and numeric expressions, in the ASR output. Also known as named entities (NEs), these phrases are useful in many language understanding tasks, such as coreference resolution, sentence chunking and parsing, and summarization/gisting. 4.1 Data and Evaluation Method The data we used for the experiments described in this paper consisted of 114 news broadcasts automatically an-  notated with recognition conﬁdence scores and hand labeled with NE types and locations. The data represents an intersection of the data provided by Dragon Systems for the 1998 DARPA-sponsored Hub-4 Topic, Detection and Tracking (TDT) evaluation and those stories for which named entity labels were available. Broadcast news data is particularly appropriate for our work since it contains a high density of name phrases, has a relatively high word error rate, and requires a virtually unlimited vocabulary. We used two versions of each news broadcast: a reference transcription prepared by a human annotator and an ASR transcript prepared by Dragon Systems for the TDT evaluation [7]. The Dragon ASR system had a vocabulary size of about 57,000 words and a word error rate (WER) of about 30%. The ASR data contained the word-level conﬁdence information, as described earlier, and the reference transcription was manually-annotated with named entity information. By aligning the reference and ASR transcriptions, we were able to determine which ASR output words corresponded to errors and to the NE phrases. We randomly selected 98 of the 114 broadcasts as training data, 8 broadcasts as development test, and 8 broadcasts as evaluation test data, which were kept “blind” to ensure unbiased evaluation results. We used the training data to estimate all model parameters, the development test set to tune parameters during development, and the evaluation test set for all results reported here. For all experiments we used the same training and test data. 4.2 Information Extraction Results Table 1 shows the performance of the baseline information extraction system (row 1) which does not model errors, compared to systems using one and two error types, with the baseline conﬁdence estimates and the improved conﬁdence estimates from the previous section. Performance ﬁgures are the standard measures used for this task: F-measure (harmonic mean of recall and precision) and slot error rate (SER), where separate type, extent and content error measures are averaged to get the reported result. The results show that modeling errors gives a signiﬁcant improvement in performance. In addition, there is a small but consistent gain from modeling OOV vs. IV errors separately. Further gain is provided by each improvement to the conﬁdence estimator. Since the evaluation criterion involves a weighted average of content, type and extent errors, there is an upper bound of 86.4 for the F-measure given the errors in the recognizer output. In other words, this is the best performance we can hope for without running additional processing to correct the ASR errors. Thus, the combined error modeling improvements lead to recovery of 28% of the possible performance gains from this scheme. It is also interesting to note that the improvement in identifying the extent of a named entity actually results in a decrease in performance of the content component, since words that are incorrectly recognized are introduced into the named entity regions. 5. DISCUSSION In this paper we described our use of error modeling to improve information extraction from speech data. Our model is the ﬁrst to explicitly represent the uncertainty inherent in the ASR output word sequence. Two key in-  Table 1: Named entity (NE) recognition results using different error models and feature sets for predicting conﬁdence scores. The baseline conﬁdence scores are from the Dragon recognizer, the secondary processing re-estimates conﬁdences as a function of a window of these scores, and the names are provided by a previous pass of named entity detection.  g -tokens  Conﬁdence Scores  NE  NE  F-Measure SER  none  none  68.4 50.9  
This paper describes a system and set of algorithms for automatically inducing stand-alone monolingual part-of-speech taggers, base noun-phrase bracketers, named-entity taggers and morphological analyzers for an arbitrary foreign language. Case studies include French, Chinese, Czech and Spanish. Existing text analysis tools for English are applied to bilingual text corpora and their output projected onto the second language via statistically derived word alignments. Simple direct annotation projection is quite noisy, however, even with optimal alignments. Thus this paper presents noise-robust tagger, bracketer and lemmatizer training procedures capable of accurate system bootstrapping from noisy and incomplete initial projections. Performance of the induced stand-alone part-of-speech tagger applied to French achieves 96% core part-of-speech (POS) tag accuracy, and the corresponding induced noun-phrase bracketer exceeds 91% F-measure. The induced morphological analyzer achieves over 99% lemmatization accuracy on the complete French verbal system. This achievement is particularly noteworthy in that it required absolutely no hand-annotated training data in the given language, and virtually no language-speciﬁc knowledge or resources beyond raw text. Performance also signiﬁcantly exceeds that obtained by direct annotation projection. Keywords multilingual, text analysis, part-of-speech tagging, noun phrase bracketing, named entity, morphology, lemmatization, parallel corpora 1. TASK OVERVIEW A fundamental roadblock to developing statistical taggers, bracketers and other analyzers for many of the world’s 200+ major languages is the shortage or absence of annotated training data for the large majority of these languages. Ideally, one would like to lever-  Annotations From Existing English Tools  [ JJ National 0  [PLACE]  ] [ ] NNS  VBG  IN  NNP  NNP  laws applying in Hong Kong  
2. BACKGROUND Architecturally, the question answering system is simple. First the parser analyses the question and generates a query for the passage retrieval component. It also provides selection rules for the information extraction component. Next, the passage retrieval component executes the query over the target corpus and retrieves a ranked list of passages for the answer IE component to process. Thirdly, the information extraction component ﬁnds the answers’ extracts in the passages retrieved. The parser is a probabilistic version of Earley’s algorithm. It determines all possible parses of the grammar and selects the most probable. The grammar contains only 80 production rules[3]. .  The passage retrieval component collects arbitrary substrings of  a document in the corpus. These substrings are considered passages  and given a score. Passage scores are based on the terms contained  in the query and the passage length. Passages with a length of one  thousand words were retrieved in the TREC-9 system.  The information extraction component locates possible answers  in the top ten passages. It then selects the best answer extracts of a  predetermined length.  The overall approach of question analysis followed by IR suc-  ceeded by IE is nearly universal in QA systems[1, 2, 5, 6, 7, 8, 9].  The TREC-9 question answering track required the QA system to  ﬁnd solutions to 693 questions. Two different runs were judged:  
In this paper, we describe a process used for guiding the evaluation and transformation process for language processing research and development. The Integrated Feasibility Experiment process is explained by describing the key six steps, and then providing a specific example to help understand how to implement the steps. 1. INTRODUCTION The objective of this paper is to describe a reliable and repeatable process used to guide the development of information systems where technology teams must come together to implement a concept. This paper describes an “IFE process” that has been used successfully multiple times over the last eight years, and has served as both a framework for language experimentation, and as a vehicle for integrating and applying language technology components. 2. DESCRIBING THE IFE SIX KEY STEPS The IFE process consists of six steps that guide development and experimentation. The emphasis placed on each step depends on the maturity of the technology and the involvement of the users. The six steps are as follows (note that the six steps are summarized in Figure 1) 2.1 Step #1: Scenario Describe a scenario for employing the information technology that will allow everyone to visualize how the technology is to be used during a real situation. This step places emphasis on making the technology look and behave like a system. This is a critical step for two main reasons:  a. For the technology teams that are to integrate technology, the scenario provides a real and accessible description of how the technology should be used. This assists the teams directly in describing the architecture and components needed to build an information system for the given scenario. b. The scenario is key in describing the intent of the information system to the operational user. Typically, operational users become involved in this scenario building process to give early and helpful feedback to the technology development teams. 2.2 Step #2: Architecture Many people believe that describing the architecture is the key step in building and information system. However, if the ideas about components and interconnections are vague or incomplete, then the architecture step is actually best developed using a hypothesis and test process. In all cases the architecture must allow plug-and-play concepts that support the inclusion and reuse of mature processing components, plus the inclusion of new components that will be the focus of experimentation. 2.3 Step #3: Reuse Components: The third step is to identify and make plans to reuse components that one will depend on during the IFE. This step is critical for the technology teams because many of the components to be used come from years of development and experimentation. With mature components populating a large share of the architecture, the development teams are then free to experiment with new components that are considered to be necessary for end-to-end processing. Moreover, the developers can experiment with data flow and interconnection strategies. This experimentation step is critical in order to transform into tomorrows’ network-centric processing models supported by communication interoperability provide by TCP/IP processing. 2.4 Step #4: User Involvement Obtaining operational user involvement early-on is an important step to support a technology transformation objective. The operational user will have insights and needs that cannot be predicted by the technology developers. Moreover, user involvement improves the interest, understanding and potential commitment to the technology. If user centered final exam metrics are stated clearly then they provide a useful objective to help  focus technology development and implementation. This all may sound like motherhood, but it is a critical step that is missing often from technology development projects large and small. 2.5 Step #5: Rapid Prototyping The use of a rapid prototype approach is not new. In the mid 1980s it became the key focus for specifying and building information systems. However, the rapid prototype process must be used in conjunction with other steps of the IFE, or else the development effort will end up as a simple demonstration that does not scale to real user needs. The spiral development model for development that emphasizes the “build a little, test a little” approach, should be used to keep development on track and headed toward the target needs of the user. 2.6 Step #6: Evaluation and Feedback Metric-based evaluation is important for any development process. For an IFE the specification of usable metrics is not easy because the teams are coming together to build a “new” capability. The best approach comes by making an early commitment and following through with the measurement process and then later changing the evaluation process to better represent the emerging information processing capability. One should have measures for technology accomplishment and such measures should focus on component performance. In addition, one must have an overall “task performance” metric or metrics that reflect the needs of the operational user and the intent of the scenario. Integrated Feasibility Experiment Steps 1. Scenario .. Helps to visualize the use of new technology 2. Architecture .. Components, interconnects, data flow, and processing model 3. Reuse components .. Must build on past accomplishments 4. User .. The user provides application pull, as opposed to technology push 5. Rapid prototype .. Build a little, test a little strategy to keep effort on track and on target 6. Evaluation and feedback: Metrics-based evaluations are key to understanding accomplishment Figure 1: The six steps of an Integrated Feasibility Experiment 2.7 Historical Note An Integrated Feasibility Development (IFD) process was first used in 1990 by Steve Cross and his team to guide development of the DARPA and Rome Labs replanning system called DART (Dynamic Adaptive Replanning Technology). DART was developed to assist logistics and transportation planners in scheduling the movement and deliver of people and materials. An operational prototype was actually used during the Persian Gulf 
As part of MITRE’s work under the DARPA TIDES (Translingual Information Detection, Extraction and Summarization) program, we are preparing a series of demonstrations to showcase the TIDES Integrated Feasibility Experiment on Bio-Security (IFE-Bio). The current demonstration illustrates some of the resources that can be made available to analysts tasked with monitoring infectious disease outbreaks and other biological threats. Keywords Translation, information extraction, summarization, topic detection and tracking, system integration. 1. INTRODUCTION The long-term goal of TIDES is to provide delivery of information on demand in real-time from live on-line sources. For IFE-Bio, the resources made available to the analyst include email, news groups, digital library resources, and eventually (in later versions), topic-specific segments from broadcast news. Because of the emphasis on global monitoring, there is a need to process incoming information in multiple languages. The system must deliver the appropriate information content in the appropriate form and in the appropriate language (taken for now to be English). This means that the IFE-Bio system will have to deliver news stories, clusters of relevant documents, threaded discussions, alerts on new events, tables, summaries (particularly over document collections), answers to questions, graphs and geospatial temporal displays of information. The demonstration system for the Human Language Technology Conference in March 2001 represents an early stage of the full IFE-Bio system, with an emphasis on end-to-end processing. Future demonstrations will make use of MITRE’s Catalyst architecture, providing an efficient, scalable architecture to  facilitate integration of multiple stages of linguistic processing. By June 2001, the IFE-Bio system will provide richer linguistic processing through the integration of modules contributed by other TIDES participants. By June 2002, the IFE-Bio system will include additional functionality, such as real-time broadcast news feeds, new machine translation components, support for questionanswering, cross-language information retrieval, multi-document summarization, automatic extraction and normalization of temporal and spatial information, and automated geospatial and temporal displays. 2. The IFE-Bio System The current demonstration (March 2001) highlights the basic functionality required by an analyst, including: • Capture of sources, including e-mail, digital library material, news groups, and web-based resources; • Categorizing of the sources into multiple orthogonal hierarchies useful to the analyst, e.g., disease, region, news source, language; • Processing of the information through various stages, including “zoning” of the text to select the relevant portions for processing; named entity detection, event detection, extraction of temporal information, summarization, and translation from Spanish, Portuguese, and Chinese into English; • Access to the information through use of any mail and news group reader, which allows the analyst to organize, save, and share the information in a familiar, readily accessible environment; • Display of the information in alternate forms, including color-tagged documents, tables, summaries, graphs, and geospatial, map-based displays. Figure 1 below shows the overall functionality envisioned for the IFE-Bio system, including capture, categorizing, processing, access and display. Collection capability for the current IFE-Bio system includes email, news groups, journals, and Web resources. We have a complete copy of the ProMED mailings (a moderated source  Translingual Information Detection Extraction Summarization  CDC WHO  Internl News Sources  ~ 2500 stories/day Email: ProMed  Medical literature  Capture  Interaction  What is the status of the current Ebola outbreak?  News Repository 
hcd@cs.cmu.edu  1. INTRODUCTION Most current IR research is focused on speciﬁc technologies, such as ﬁltering, classiﬁcation, entity extraction, question answering, etc. There is relatively little research on merging multiple technologies into sophisticated applications, due in part to the high cost of integrating independently-developed text processing modules. In this paper, we present the Integrated Information Management (IIM) architecture for component-based development of IR applications1. The IIM architecture is general enough to model different types of IR tasks, beyond indexing and retrieval. Rather than providing a single framework or toolkit, our goal is to create a higherlevel framework which is used to build a variety of different class libraries or toolkits for different problems. Another goal is to promote the educational use of IR software, from an “exploratory programming” perspective. For this reason, it is also important to provide a graphical interface for effective task visualization and realtime control. Prior architecture-related work has focused on toolkits or class libraries for speciﬁc types of IR or NLP problems. Examples include the SMART system for indexing and retrieval [17], the FIRE [18] and InfoGrid [15] class models for information retrieval applications, and the ATTICS [11] system for text categorization and machine learning. Some prior work has also focused on the user interface, notably FireWorks [9] and SketchTrieve [9]2. Other systems such as GATE [4] and Corelli [20] have centered on speciﬁc approaches to NLP applications. The Tipster II architecture working group summarized the requirements for an ideal IR architecture [6], which include: Standardization. Specify a standard set of functions and interfaces for information services. Rapid Deployment. Speed up the initial development of new applications. ¡ This work is supported by National Science Foundation (KDI) g¢ rant number 9873009. For further discussion on how these systems compare with the present work, see Section 7. .  Maintainability. Use standardized modules to support plugand-play updates. Flexibility. Enhance performance by allowing novel combinations of existing components. Evaluation. Isolate and test speciﬁc modules side-by-side in the same application. One of the visions of the Tipster II team was a “marketplace of modules”, supporting mix-and-match of components developed at different locations. The goals of rapid deployment and ﬂexibility require an excellent user interface, with support for drag-and-drop task modeling, real-time task visualization and control, and uniform component instrumentation for cross-evaluation. The modules themselves should be small, downloadable ﬁles which run on a variety of hardware and software platforms. This vision is in fact a specialized form of component-based software engineering (CBSE) [14], where the re-use environment includes libraries of reusable IR components, and the integration process includes realtime conﬁguration, control, and tuning. Section 2 summarizes the architectural design of IIM. Section 3 provides more detail regarding the system’s current implementation in Java. In Section 5 we describe three different task libraries that have been constructed using IIM’s generic modules. Current instrumentation, measurement, and results are presented in Section 6. We conclude in Section 7 with some relevant comparisons of IIM to related prior work. 2. ARCHITECTURAL DESIGN IIM uses a ﬂow-based (pipe and ﬁlter [16]) processing model. Information processing steps are represented as nodes in a graph. Each edge in the graph represents a ﬂow connection between a parent node and a child node; the documents produced by the parent node are passed to each child node. In IIM, the ﬂow graph is referred to as a node chain. A sample node chain is shown in Figure 1. The IIM class model includes six basic node types, which can be used to model a variety of IR problems: 1. Source. Generates a document stream (from a static collection, web search, etc.) and passes documents one at a time to its child node(s). 2. Filter. Passes only documents which match the ﬁlter to its child node(s). 3. Annotator. Adds additional information to the document regarding a particular region in the document body.  Figure 1: IIM User Interface  4. Sink. Creates and passes either a single document or a collection to its child node(s), after pooling the input documents it receives. 5. Transformer. Creates and passes on a single new document, presumably the result of processing its input document. 6. Renderer. Produces output for documents received (to disk, to screen, etc.). The IIM class model is embedded in a Model-View-Controller architecture [5], which allows the system to be run with or without the graphical interface. Pre-stored node chains can be executed directly from the shell, or as a background process, completely bypassing all user interaction when optimal performance is required. The Controller subsystem and interface event dispatching subsystem must run as separate threads to support dynamic update of parameters in a running system. The View (user interface) should support: a) plug-and-play creation of new node chains; b) support for saving, loading and importing new node chains; c) dynamic visualization of a task’s status; and d) direct manipulation of a node’s parameters at any time. In addition to the nodes themselves, IIM supports two other important abstractions for IR task ﬂows: Macro Nodes. Certain sequences of nodes are useful in more than one application, so it is convenient to store them together as a single reusable unit, or macro node. IIM allows the user to export a portion of a node chain as a macro node to be loaded into the Node Library and inserted into a new chain as a single node. The user may specify which of the properties of the original nodes are visible in the exported macro node (see Figure 3). Controllers. Some IR tasks require iteration through multiple runs; the system’s behavior on each successive trial is modiﬁed based on feedback from a previous run. For example, a system might wish to ask for more documents or perform query expansion if the original query returns an insufﬁcient number of relevant documents. IIM includes a Controller interface, which speciﬁes methods for sending feedback from  Figure 2: Node Interface and Subtypes. one node to another. The user can implement a variety of controllers, depending on the needs of the particular application. 3. JAVA IMPLEMENTATION In the IIM Java implementation, nodes are speciﬁed by the abstract interface Node and its six abstract subinterfaces: Source, Filter, Annotator, Transformer, Sink and Renderer (see Figure 2). Any user-deﬁned Java class which implements one of the Node subinterfaces can be loaded into IIM and used in a node chain. The visualization of a node is represented by a separate Java class, Box, which handles all of the details related to drawing the node and various visual cues in the node chain display. 
In this paper we show how two standard outputs from information extraction (IE) systems – named entity annotations and scenario templates – can be used to enhance access to text collections via a standard text browser. We describe how this information is used in a prototype system designed to support information workers’ access to a pharmaceutical news archive as part of their “industry watch” function. We also report results of a preliminary, qualitative user evaluation of the system, which while broadly positive indicates further work needs to be done on the interface to make users aware of the increased potential of IE-enhanced text browsers. 1. INTRODUCTION Information extraction (IE) technology, as promoted and deﬁned by the DARPA Message Understanding Conferences [4, 5] and the current ACE component of TIDES [1], has resulted in impressive new abilities to extract structured information from texts, and complements more traditional information retrieval (IR) technology which retrieves documents or passages of relevance from text collections and leaves information seekers to browse the retrieved sub-collection (e.g. [2]). However, while IR technology has been readily incorporated into end-user applications (e.g. web search engines), IE technology has not yet been as successfully deployed in end-user systems as its proponents had hoped. There are several reasons for this, including: 1. Porting cost. Moving IE systems to new domains requires considerable expenditure of time and expertise, either to create/modify domain-speciﬁc resources and rule bases, or to annotate texts for supervised machine learning approaches. 2. Sensitivity to inaccuracies in extracted data. IE holds out the promise of being able to construct structured databases from text sources automatically, but extraction results are by no means perfect. Thus, the technology is only appropriate .  for applications where some error is tolerable and readily detectable by end users. 3. Complexity of integration into end-user systems. IE systems produce results (named entity tagged texts, ﬁlled templates) which must be incorporated into larger, more sophisticated application systems if end users are to gain beneﬁt from them. In this paper we present the approach taken in the TRESTLE project (Text Retrieval Extraction and Summarisation Technologies for Large Enterprises) which addresses the second and third of these problems; and also preliminary results from the user testing evaluation of the TRESTLE interface. The goal of the TRESTLE project is to develop an advanced text access facility to support information workers at GlaxoSmithKline (GSK), a large pharmaceutical corporation. Speciﬁcally, the project aims to provide enhanced access to Scrip1, the largest circulation pharmaceutical industry newsletter, in order to increase the effectiveness of employees in their “industry watch” function, which involves both broad current awareness and tracking of people, companies and products, particularly the progress of new drugs through the clinical trial and regulatory approval process. 2. IE AND INFORMATION SEEKING IN LARGE ENTERPRISES While TRESTLE aims to support information workers in the pharmaceutical industry, most of the functionality it embodies is required in any large enterprise. Our analysis of user requirements at GlaxoSmithKline has led us to distinguish various categories of information seeking. At the highest level we must distinguish requirements for current awareness from those for retrospective search. Current awareness requirements can be further split into general updating (what’s happened in the industry news today/this week) and entity or event-based tracking (e.g. what’s happened concerning a speciﬁc drug or what regulatory decisions have been made). Retrospective search tends to break down into historical tracking of entities or events of interest (e.g. where has a speciﬁc person been reported before, what is the clinical trial history of a particular drug) and search for a speciﬁc event or a remembered context in which a speciﬁc entity played a role. ¤ Scrip is the trademark of PJB Publications Ltd. See http://www.pjbpub.co.uk.  Scrip  Off-line System  Information  Internet  User  Seeking  Web Browser  LaSIE Information Extraction System  NE Tagged Texts  
At MIT Lincoln Laboratory, we have been developing a Koreanto-English machine translation system CCLINC (Common Coalition Language System at Lincoln Laboratory). The CCLINC Korean-to-English translation system consists of two core modules, language understanding and generation modules mediated by a language neutral meaning representation called a semantic frame. The key features of the system include: (i) Robust efficient parsing of Korean (a verb final language with overt case markers, relatively free word order, and frequent omissions of arguments). (ii) High quality translation via word sense disambiguation and accurate word order generation of the target language. (iii) Rapid system development and porting to new domains via knowledge-based automated acquisition of grammars. Having been trained on Korean newspaper articles on “missiles” and “chemical biological warfare,” the system produces the translation output sufficient for content understanding of the original document. 1. SYSTEM OVERVIEW The CCLINC The CCLINC Korean-to-English translation system is a component of the CCLINC Translingual Information System, the focus languages of which are English and Korean, [11,17]. Translingual Information System Structure is given in Figure 1. Given the input text or speech, the language understanding system parses the input, and transforms the parsing output into a language neutral meaning representation called a semantic frame, [16,17]. The semantic frame  the key properties of which will be discussed in Section 2.3  becomes the input to the generation system. The generation system produces the target to the generation system, the semantic frame can be utilized for other applications such as translingual information extraction and  language translation output after word order arrangement, vocabulary replacement, and the appropriate surface form realization in the target language, [6]. Besides serving as the input question-answering, [12].∗ In this paper, we focus on the Koreanto-English text translation component of CCLINC.1  UUNNDDEERRSSTTAANNDDININGG  CC44I I ININFFOORRMMAATTIOIONN AACCCCEESSSS  UUNNDDEERRSSTTAANNDDININGG  ENGLISH TEXT OR SPEECH  SSEEMMAANNTTICICFFRRAAMMEESS (C(COOMMMMOONN CCOOAALLITITIOIONN LLAANNGGUUAAGGEE) )  KOREAN TEXT OR SPEECH  GGEENNEERRAATTIOIONN  GGEENNEERRAATTIOIONN  OTHER LANGUAGES  Figure 1. CCLINC Translingual Information System Structure  2. ROBUST PARSING, MEANING REPRESENTATION, AND AUTOMATED GRAMMAR ACQUISITION ∗ This work was sponsored by the Defense Advanced Research Project Agency under the contract number F19628-00-C-0002. Opinions, interpretations, conclusions, and recommendations are those of the authors and are not necessarily endorsed by the United States Air Force. 
The purpose of this research is to test the efficacy of applying automated evaluation techniques, originally devised for the evaluation of human language learners, to the output of machine translation (MT) systems. We believe that these evaluation techniques will provide information about both the human language learning process, the translation process and the development of machine translation systems. This, the first experiment in a series of experiments, looks at the intelligibility of MT output. A language learning experiment showed that assessors can differentiate native from non-native language essays in less than 100 words. Even more illuminating was the factors on which the assessors made their decisions. We tested this to see if similar criteria could be elicited from duplicating the experiment using machine translation output. Subjects were given a set of up to six extracts of translated newswire text. Some of the extracts were expert human translations, others were machine translation outputs. The subjects were given three minutes per extract to determine whether they believed the sample output to be an expert human translation or a machine translation. Additionally, they were asked to mark the word at which they made this decision. The results of this experiment, along with a preliminary analysis of the factors involved in the decision making process will be presented here. Keywords Machine translation, language learning, evaluation. 1. INTRODUCTION Machine translation evaluation and language learner evaluation have been associated for many years, for example [5, 7]. One attractive aspect of language learner evaluation which recommends it to machine translation evaluation is the expectation that the produced language is not perfect, well-formed language. Language learner evaluation systems are geared towards determining the specific kinds of errors that language learners make. Additionally, language learner evaluation, more than many MT evaluations, seeks to build models of language acquisition which could parallel (but not correspond directly to) the development of MT systems. These models frequently are  feature-based and may provide informative metrics for diagnostic evaluation for system designers and users. In a recent experiment along these lines, Jones and Rusk [2] present a reasonable idea for measuring intelligibility, that of trying to score the English output of translation systems using a wide variety of metrics. In essence, they are looking at the degree to which a given output is English and comparing this to humanproduced English. Their goal was to find a scoring function for the quality of English that can enable the learning of a good translation grammar. Their method for accomplishing this is through using existing natural language processing applications on the translated data and using these to come up with a numeric value indicating degree of “Englishness”. The measures they utilized included syntactic indicators such as word n-grams, number of edges in the parse (both Collins and Apple Pie parser were used), log probability of the parse, execution of the parse, overall score of the parse, etc. Semantic criteria were based primarily on WordNet and incorporated the average minimum hyponym path length, path found ratio, percent of words with sense in WordNet. Other semantic criteria utilized mutual information measures. Two problems can be found with their approach. The first is that the data was drawn from dictionaries. Usage examples in dictionaries, while they provide great information, are not necessarily representative of typical language use. In fact, they tend to highlight unusual usage patterns or cases. Second, and more relevant to our purposes, is that they were looking at the glass as half-full instead of half-empty. We believe that our results will show that measuring intelligibility is not nearly as useful as finding a lack of intelligibility. This is not new in MT evaluation – as numerous approaches have been suggested to identify translation errors, such as [1, 6]. In this instance, however, we are not counting errors to come up with a intelligibility score as much as finding out how quickly the intelligibility can be measured. Additionally, we are looking to a field where the essence of scoring is looking at error cases, that of language learning. 2. SIMPLE LANGUAGE LEARNING EXPERIMENT The basic part of scoring learner language (particularly second language acquisition and English as a second language) consists of identifying likely errors and understanding the cause of them. From these, diagnostic models of language learning can be built and used to effectively remediate learner errors, [3] provide an excellent example of this. Furthermore, language learner testing  seeks to measure the student's ability to produce language which is fluent (intelligible) and correct (adequate or informative). These are the same criteria typically used to measure MT system capability.1 In looking at different second language acquisition (SLA) testing paradigms, one experiment stands out as a useful starting point for our purposes. One experiment in particular serves as the model for this investigation. In their test of language teachers, Meara and Babi [3] looked at assessors making a native speaker (L1) / language learner (L2) distinction in written essays.2 They showed the assessors essays one word at a time and counted the number of words it took to make the distinction. They found that assessors could accurately attribute L1 texts 83.9% of the time and L2 texts 87.2% of the time for 180 texts and 18 assessors. Additionally, they found that assessors could make the L1/L2 distinction in less than 100 words. They also learned that it took longer to confirm that an essay was a native speaker’s than a language learner’s. It took, on average, 53.9 words to recognize an L1 text and only 36.7 words to accurately distinguish an L2 text. While their purpose was to rate the language assessment process, the results are intriguing from an MT perspective. They attribute the fact that L2 took less words to identify to the fact that L1 writing “can only be identified negatively by the absence of errors, or the absence of awkward writing.” While they could not readily select features, lexical or syntactic, on which evaluators consistently made their evaluation, they hypothesize that there is a “tolerance threshold” for low quality writing. In essence, once the pain threshold had been reached through errors, missteps or inconsistencies, then the assessor could confidently make the assessment. It is this finding that we use to disagree with Jones and Rusk [2] basic premise. Instead of looking for what the MT system got right, it is more fruitful to analyze what the MT system failed to capture, from an intelligibility standpoint. This kind of diagnostic is more difficult, as we will discuss later. We take this as the starting point for looking at assessing the intelligibility of MT output. The question to be answered is does this apply to distinguishing between expert translation and MT output? This paper reports on an experiment to answer this question. We believe that human assessors key off of specific error types and that an analysis of the results of the experiment will enable us to do a program which automatically gets these. 
This paper describes a method to construct a case frame dictionary automatically from a raw corpus. The main problem is how to handle the diversity of verb usages. We collect predicate-argument examples, which are distinguished by the verb and its closest case component in order to deal with verb usages, from parsed results of a corpus. Since these couples multiply to millions of combinations, it is difﬁcult to make a wide-coverage case frame dictionary from a small corpus like an analyzed corpus. We, however, use a raw corpus, so that this problem can be addressed. Furthermore, we cluster and merge predicate-argument examples which does not have diﬀerent usages but belong to diﬀerent case frames because of diﬀerent closest case components. We also report on an experimental result of case structure analysis using the constructed case frame dictionary. 1. INTRODUCTION Syntactic analysis or parsing has been a main objective in Natural Language Processing. In case of Japanese, however, syntactic analysis cannot clarify relations between words in sentences because of several troublesome characteristics of Japanese such as scrambling, omission of case components, and disappearance of case markers. Therefore, in Japanese sentence analysis, case structure analysis is an important issue, and a case frame dictionary is necessary for the analysis. Some research institutes have constructed Japanese case frame dictionaries manually [2, 3]. However, it is quite expensive, or almost impossible to construct a wide-coverage case frame dictionary by hand. Others have tried to construct a case frame dictionary automatically from analyzed corpora. However, existing syntactically analyzed corpora are too small to learn a dictionary, since case frame information consists of relations between nouns and verbs, which multiplies to millions of combinations. Based on such a consideration, we took the  unsupervised learning strategy to Japanese case frame construction1 . To construct a case frame dictionary from a raw corpus, we parse a raw corpus ﬁrst, but parse errors are problematic in this case. However, if we use only reliable modiﬁer-head relations to construct a case frame dictionary, this problem can be addressed. Verb sense ambiguity is rather problematic. Since verbs can have diﬀerent cases and case components depending on their meanings, verbs which have different meanings should have diﬀerent case frames. To deal with this problem, we collect predicate-argument examples, which are distinguished by the verb and its closest case component, and cluster them. That is, examples are not distinguished by verbs such as naru ‘make, become’ and tsumu ‘load, accumulate’, but by couples such as tomodachi ni naru ‘make a friend’, byouki ni naru ‘become sick’,nimotsu wo tsumu ‘load baggage’, and keiken wo tsumu ‘accumulate experience’. Since these couples multiply to millions of combinations, it is diﬃcult to make a wide-coverage case frame dictionary from a small corpus like an analyzed corpus. We, however, use a raw corpus, so that this problem can be addressed. The clustering process is to merge examples which does not have diﬀerent usages but belong to diﬀerent case frames because of diﬀerent closest case components. 2. VARIOUS METHODS FOR CASE FRAME CONSTRUCTION We employ the following procedure of case frame construction from raw corpus (Figure 1): 1. A large raw corpus is parsed by KNP [5], and reliable modiﬁer-head relations are extracted from the parse results. We call these modiﬁer-head relations examples. 2. The extracted examples are distinguished by the verb and its closest case component. We call these data example patterns. 3. The example patterns are clustered based on a thesaurus. We call the output of this process example case frames, which is the ﬁnal result of the system. We call words which compose case components case examples, and a group of case examples case example group. In Figure 1, nimotsu ‘baggage’, busshi 1In English, several unsupervised methods have been proposed[7, 1]. However, it is diﬀerent from those that combinations of nouns and verbs must be collected in Japanese.  raw corpus  tagging or analysis+extraction of reliable relations  I. examples  kuruma ni nimotsu wo tsumu  car  baggage load  jugyoin ga nimotsu wo tsumu worker baggage load  truck ni busshi wo tsumu  truck  supply  load  hikoki ni busshi wo tsumu  airplane supply  load  sensyu ga keiken wo tsumu player experience accumulate  example patterns jugyoin ga kuruma ni nimotsu wo tsumu  truck ni hikoki sensyu ga  busshi wo tsumu keiken wo tsumu  II. co-occurrences nimotsu wo tsumu kuruma ni tsumu jugyoin ga tsumu busshi wo tsumu truck ni tsumu hikoki ni tsumu sensyu ga tsumu keiken wo tsumu  III. merged frame  jugyoin ga kuruma ni nimotsu wo tsumu  sensyu truck  busshi  hikoki  keiken  example case frames  jugyoin ga kuruma ni nimotsu wo tsumu  truck  busshi  hikoki  sensyu ga  keiken wo tsumu  thesaurus by hand or learning  IV. semantic case frames <person> ga <vehicle> ni <thing> wo tsumu  <person> ga  <mind> wo tsumu  Figure 1: Several methods for case frame construction.  ‘supply’, and keiken ‘experience’ are case examples, and {nimotsu ‘baggage’, busshi ‘supply’} (of wo case marker in the ﬁrst example case frame of tsumu ‘load, accumulate’) is a case example group. A case component therefore consists of a case example and a case marker (CM).  Let us now discuss several methods of case frame construction as shown in Figure 1. First, examples (I of Figure 1) can be used individually, but this method cannot solve the sparse data problem. For example,  (1) kuruma ni nimotsu wo  tsumu  car dat-CM baggage acc-CM load  (load baggage onto the car)  (2) truck ni  busshi wo  tsumu  truck dat-CM supply acc-CM load  (load supply onto the truck)  even if these two examples occur in a corpus, it cannot be judged whether the expression “kuruma ni busshi wo tsumu” (load supply onto the car) is allowed or not. Secondly, examples can be decomposed into binomial relations (II of Figure 1). These co-occurrences are utilized by statistical parsers, and can address the sparse data problem. In this case, however, verb sense ambiguity becomes a serious problem. For example,  (3) kuruma ni nimotsu wo  tsumu  car dat-CM baggage acc-CM load  (load baggage onto the car)  (4) keiken wo  tsumu  experience acc-CM accumulate  (accumulate experience)  from these two examples, three co-occurrences (“kuruma ni  tsumu”, “nimotsu wo tsumu”, and “keiken wo tsumu”) are extracted. They, however, allow the incorrect expression “kuruma ni keiken wo tsumu” (load experience onto the car, accumulate experience onto the car). Thirdly, examples can be simply merged into one frame (III of Figure 1). However, information quantity of this is equivalent to that of the co-occurrences (II of Figure 1), so verb sense ambiguity becomes a problem as well. We distinguish examples by the verb and its closest case component. Our method can address the two problems above: verb sense ambiguity and sparse data. On the other hand, semantic markers can be used as case components instead of case examples. These we call semantic case frames (IV of Figure 1). Constructing semantic case frames by hand leads to the problem mentioned in Section 1. Utsuro et al. constructed semantic case frames from a corpus [8]. There are three main diﬀerences to our approach: they use an annotated corpus, depend deeply on a thesaurus, and did not resolve verb sense ambiguity. 3. COLLECTING EXAMPLES This section explains how to collect examples shown in Figure 1. In order to improve the quality of collected examples, reliable modiﬁer-head relations are extracted from the parsed corpus. 3.1 Conditions of case components When examples are collected, case markers, case examples, and case components must satisfy the following conditions. Conditions of case markers Case components which have the following case markers (CMs) are collected: ga (nominative), wo (accusative), ni (dative), to (with, that), de (optional), kara (from), yori (from), he (to), and made (to). We also handle compound case markers such as ni-tsuite ‘in terms of’, wo-megutte ‘concerning’, and others. In addition to these cases, we introduce time case marker. Case components which belong to the class <time>(see below) and contain a ni, kara, or made CM are merged into time CM. This is because it is important whether a verb deeply relates to time or not, but not to distinguish between surface CMs. Generalization of case examples Case examples which have deﬁnite meanings are generalized. We introduce the following three classes, and use these classes instead of words as case examples. <time> • nouns which mean time e.g. asa ‘morning’, haru ‘spring’, rainen ‘next year’ • case examples which contain a unit of time e.g. 1999nen ‘year’, 12gatsu ‘month’, 9ji ‘o’clock’ • words which are followed by the suﬃx mae ‘before’, tyu ‘during’, or go ‘after’ and do not have the semantic marker <place> on the thesaurus e.g. kaku mae ‘before · · · write’, kaigi go ‘after the meeting’  <quantity> • numerals e.g. ichi ‘one’, ni ‘two’, juu ‘ten’  • numerals followed by a numeral classiﬁer2 such as tsu, ko, and nin. They are expressed with pairs of the class <quantity> and a numeral classiﬁer: <quantity>tsu, <quantity>ko, and <quantity>nin. e.g. 1tsu → <quantity>tsu 2ko → <quantity>ko  <clause> • quotations (“· · · to” ‘that · · · ’) and expressions which function as quotations (“· · · koto wo” ‘that · · · ’). e.g. kaku to ‘that · · · write’, kaita koto wo ‘that · · · wrote’  Exclusion of ambiguous case components We do not use the following case components:  • Since case components which contain topic markers (TMs) and clausal modiﬁers do not have surface case markers, we do not use them. For example,  sono giin  wa · · · wo teian-shita.  the assemblyman TM acc-CM proposed  wa is a topic marker and giin wa ‘assemblyman TM’ depends on teian-shita ‘proposed’, but there is no case marker for giin ‘assemblyman’ in relation to teianshita ‘proposed’.  · · · wo teian-shiteiru giin ga · · · acc-CM proposing assemblyman “· · · wo teian-shiteiru” is a clausal modiﬁer and teianshiteiru ‘proposing’ depends on giin ‘assemblyman’, but there is no case marker for giin ‘assemblyman’ in relation to teian-shiteiru ‘proposing’.  • Case components which contain a ni or de case marker are sometimes used adverbially. Since they have the optional relation to their verbs, we do not use them. e.g. tame ni ‘because of’, mujouken ni ‘unconditionally’, ue de ‘in addition to’  For example,  30nichi ni souri daijin ga 30th on prime minister nom-CM  sono 2nin  ni  those two people dat-CM  syou wo  okutta  award acc-CM gave  2Most nouns must take a numeral classiﬁer when they are quantiﬁed in Japanese. An English equivalent to it is ‘piece’.  (On 30th the prime minister gave awards to those two people.)  from this sentence, the following example is acquired.  <time>:time-CM daijin:ga minister:nom-CM  <quantity>nin:ni syou:wo  okuru  people:dat-CM award acc-CM give  3.2 Conditions of verbs We collect examples not only for verbs, but also for adjectives and noun+copulas3 . However, when a verb is followed by a causative auxiliary or a passive auxiliary, we do not collect examples, since the case pattern is changed.  3.3 Extraction of reliable examples When examples are extracted from automatically parsed results, the problem is that the parsed results inevitably contain errors. Then, to decrease inﬂuences of such errors, we discard modiﬁer-head relations whose parse accuracies are low and use only reliable relations. KNP employs the following heuristic rules to determine a head of a modiﬁer:  HR1 KNP narrows the scope of a head by ﬁnding a clear boundary of clauses in a sentence. When there is only one candidate verb in the scope, KNP determines this verb as the head of the modiﬁer.  HR2 Among the candidate verbs, verbs which rarely take case components are excluded.  HR3 KNP determines the head according to the preference: a modiﬁer which is not followed by a comma depends on the nearest candidate, and a modiﬁer with a comma depends on the second nearest candidate.  Our approach trusts HR1 but not HR2 and HR3. That is, modiﬁer-head relations which are decided in HR1 (there is only one candidate of the head in the scope) are extracted as examples, but relations which HR2 and HR3 are applied to are not extracted. The following examples illustrate the application of these rules.  (5) kare wa kai-tai  hon wo  he TM want to buy book acc-CM  takusan mitsuketa node, a lot found because  Tokyo he okutta. Tokyo to sent (Because he found a lot of books which he wants to buy, he sent them to Tokyo.)  In this example, an example which can be extracted without ambiguity is “Tokyo he okutta” ‘sent φ to Tokyo’ at the end of the sentence. In addition, since node ‘because’ is analyzed as a clear boundary of clauses, the head candidate of hon wo ‘book acc-CM’ is only mitsuketa ‘ﬁnd’, and this is also extracted. Verbs excluded from head candidates by HR2 possibly become heads, so we do not use the examples which HR2 is applied to. For example, when there is a strong verb right 3In this paper, we use ’verb’ instead of ’verb/adjective or noun+copula’ for simplicity.  after an adjective, this adjective tends not to be a head of a case component, so it is excluded from head candidates.  (6) Hi no mawari ga  hayaku  ﬁre of spread nom-CM rapidly  sukuidase-nakatta. could not save (The ﬁre spread rapidly, so φ1 could not save φ2.) In this example, the correct head of mawari ga ‘spread’ is hayaku ‘rapidly’. However, since hayaku ‘rapidly’ is excluded from the head candidates, the head of mawari ga ‘spread’ is analyzed incorrectly. We show an example of the process HR3:  (7) kare ga  shitsumon ni  he nom-CM question acc-CM  sentou wo  kitte kotaeta.  lead acc-CM take answered  (He took the lead to answer the question.)  In this example, head candidates of shitsumon ni ‘question acc-CM’ are kitte ‘take’ and kotaeta ‘answered’. According to the preference “modify the nearer head”, KNP incorrectly decides the head is kitte ‘take’. Like this example, when there are many head candidates, the decided head is not reliable, so we do not use examples in this case. We extracted reliable examples from Kyoto University Corpus[6], that is a syntactically analyzed corpus, and evaluated the accuracy of them. The accuracy of all the case examples which have the target cases was 90.9%, and the accuracy of the reliable examples was 97.2%. Accordingly, this process is very eﬀective.  4. CONSTRUCTION OF EXAMPLE CASE FRAMES As shown in Section 2, when examples whose verbs have diﬀerent meanings are merged, a case frame which allows an incorrect expression is created. So, for verbs with diﬀerent meanings, diﬀerent case frames should be acquired. In most cases, an important case component which decides the sense of a verb is the closest one to the verb, that is, the verb sense ambiguity can be resolved by coupling the verb and its closest case component. Accordingly, we distinguish examples by the verb and its closest case component. We call the case marker of the closest case component closest case marker. The number of example patterns which one verb has is equal to that of the closest case components. That is, example patterns which have almost the same meaning are individually handled as follows:  (8) jugyoin:ga  kuruma :ni  worker:nom-CM car:dat-CM  nimotsu :wo  tsumu  baggage:acc-CM load  (9) {truck,hikoki }:ni {truck,airplane}:dat-CM  busshi :wo  tsumu  supply:acc-CM load  In order to merge example patterns that have almost the same meaning, we cluster example patterns. The ﬁnal ex-  jugyoin3 ga kuruma 5 ni  worker  car  1.0 0.86  nimotsu8 wo  tsumu  baggage load  0.91  {truck3, hikoki 2} ni busshi10 wo  tsumu  truck airplane  supply load  1/2  1/2  1.0 ¡ (5¡ 3) + 0.86 ¡ (5¡ 2) = 0.94  1/2  1/2  0.91  (5¡ 3) + (5¡ 2)  1/2  1/2 1/2  1/4  similarity between case example groups  :  0.94 ¡  ( (5¡ 3) + (5¡ 2) ) + 0.91 ¡ (8¡ 10)  = 0.92  1/2  1/2 1/2  1/4  ( (5¡ 3) + (5¡ 2) ) + (8¡ 10)  ( ) ratio of common cases :  ( 5 + 8 ) + ( 3 + 2 + 10 ) 1/2 = 0.90 ( 3 + 5 + 8 ) + ( 3 + 2 + 10 )  similarity between example patterns :  0.92 ¡ 0.90 = 0.83  Figure 2: Example of calculating the similarity between example patterns (Numerals in the lower right of examples represent their frequencies.)  ample case frames consist of the example pattern clusters. The detail of the clustering is described in the following section. 4.1 Similarity between example patterns The clustering of example patterns is performed by using the similarity between example patterns. This similarity is based on the similarities between case examples and the ratio of common cases. Figure 2 shows an example of calculating the similarity between example patterns. First, the similarity between two examples e1, e2 is calculated using the NTT thesaurus as follows: sime(e1, e2) = maxx∈s1,y∈s2 sim(x, y)  sim(x, y)  =  2L lx + ly  where x, y are semantic markers, and s1, s2 are sets of semantic markers of e1, e2 respectively4. lx, ly are the depths of x, y in the thesaurus, and the depth of their lowest (most speciﬁc) common node is L. If x and y are in the same node of the thesaurus, the similarity is 1.0, the maximum score based on this criterion. Next, the similarity between the two case example groups E1, E2 is the normalized sum of the similarities of case examples as follows:  È È simE(E1, E2)  √  È È = √ e1∈E1 e2∈E2 |e1||e2| sime(e1,e2)  e1 ∈E1 e2∈E2 |e1||e2|  where |e1| , |e2| represent the frequencies of e1, e2 respectively. The ratio of common cases of example patterns F1, F2 is  4In many cases, nouns have many semantic markers in NTT thesaurus.  calculated as follows:  ×ÈÈ ÈÈ cs =  n i=1  |E1cci  |  +  l i=1  |E1c1i  |  +  n i=1  |E2cci  |  m i=1  |E2c2i  |  where the cases of example pattern F1 are c11, c12, · · · , c1l, the cases of example pattern F2 are c21, c22, · · · , c2m, and the common cases of F1 and F2 is cc1, cc2, · · · , ccn. E1cci is the case example group of cci in F1. E2cci , E1c1i , and E2c2i are deﬁned in the same way. The square root in this equation decreases inﬂuences of the frequencies.  The similarity between F1 and F2 is the product of the ratio of common cases and the similarities between case ex-  ample groups of common cases of F1 and F2 as follows:  È È Ô score = cs ·  n i=1  √wi  sinim=1E√(Ew1icci  ,  E2cci  )  wi =  |e1| |e2|  e1∈E1cci e2∈E2cci  where wi is the weight of the similarities between case example groups.  4.2 Selection of semantic markers of example patterns The similarities between example patterns are deeply inﬂuenced by semantic markers of the closest case components. So, when the closest case components have semantic ambiguities, a problem arises. For example, when clustering example patterns of awaseru ‘join, adjust’, the pair of example patterns (te ‘hand’, kao, ‘face’)5 is created with the common semantic marker <part of an animal>, and (te ‘method’, syouten ‘focus’) is created with the common semantic marker <logic, meaning>. From these two pairs, the pair (te ‘hand’, kao ‘face’, syouten ‘focus’) is created, though <part of an animal> is not similar to <logic, meaning> at all. To address this problem, we select one semantic marker of the closest case component of each example pattern in order of the similarity between example patterns as follows:  1. In order of the similarity of a pair, (p, q), of two example patterns, we select semantic markers of the closest case components, np, nq of p, q. The selected semantic markers sp, sq maximize the similarity between np and nq . 2. The similarities of example patterns related to p, q are recalculated.  3. These two processes are iterated while there are pairs of two example patterns, of which the similarity is higher than a threshold.  4.3 Clustering procedure The following is the clustering procedure:  1. Elimination of example patterns which occur infrequently  Target example patterns of the clustering are those whose closest case components occur more frequently than a threshold. We set this threshold to 5. 5Example patterns are represented by the closest case components.  2. Clustering of example patterns which have the same closest CM  (a) Similarities between pairs of two example patterns which have the same closest CM are calculated, and semantic markers of closest case components are selected. These two processes are iterated as mentioned in 4.2. (b) Each example pattern pair whose similarity is higher than some threshold is merged.  3. Clustering of all the example patterns  The example patterns which are output by 2 are clustered. In this phase, it is not considered whether the closest CMs are the same or not. The following example patterns have almost the same meaning, but they are not merged by 2 because of the diﬀerent closest CM. This clustering can merge these example patterns. (10) {busshi,kamotsu}:wo {supply,cargo}:acc-CM  truck :ni  tsumu  truck:dat-CM load  (11) {truck,hikoki }:ni {truck,airplane}:dat-CM  {nimotsu,busshi }:wo  tsumu  {baggage,supply}:acc-CM load  5. SELECTION OF OBLIGATORY CASE MARKERS If a CM whose frequency is lower than other CMs, it might be collected because of parsing errors, or has little relation to it√s verb. So, we set the threshold for the CM frequency as 2 mf, where mf means the frequency of the most found CM. If the frequency of a CM is less than the threshold, it is discarded. For example, suppose the most frequent CM for a verb is wo, 100 times, and the frequency of ni CM for the verb is 16, ni CM is discarded (since it is less than the threshold, 20). However, since we can say that all the verbs have ga (nominative) CMs, ga CMs are not discarded. Furthermore, if an example case frame do not have a ga CM, we supplement its ga case with semantic marker <person>.  6. CONSTRUCTED CASE FRAME DICTIONARY We applied the above procedure to Mainichi Newspaper Corpus (9 years, 4,600,000 sentences). We set the threshold of the clustering 0.80. The criterion for setting this threshold is that case frames which have diﬀerent case patterns or diﬀerent meanings should not be merged into one case frame. Table1 shows examples of constructed example case frames. From the corpus, example case frames of 71,000 verbs are constructed; the average number of example case frames of a verb is 1.9; the average number of case slots of a verb is 1.7; the average number of example nouns in a case slot is 4.3. The clustering led a decrease in the number of example case frames of 47%.  Table 1: Examples of the constructed case frames(* means the closest CM).  verb CM  case examples  kau 1 ‘buy’ kau 2 ...  ga person, passenger  wo* stock, land, dollar, ticket  de shop, station, yen  ga treatment, welfare, postcard  wo* anger, disgust, antipathy  ...  ...  yomu 1 ‘read’ yomu 2 yomu 3 ...  ga student, prime minister  wo* book, article, news paper  ga <person>  wo talk, opinion, brutality  de* news paper, book, textbook  ga <person>  wo* future  ...  ...  tadasu1 ga member, assemblyman  ‘examine’ wo* opinion, intention, policy  ni tsuite problem, <clause>, bill  tadasu2 ga chairman, oneself  ‘improve’ wo* position, form  ...  ...  ...  kokuchi 1 ga doctor ‘inform’ ni * the said person kokuchi 2 ga colleague wo* infection, cancer ni* patient, family  sanseida1 ga <person> ‘agree’ ni * opinion, idea, argument sanseida2 ga <person> ni* <clause>  As shown in Table1, example case frames of noun+copulas such as sanseida ‘positiveness+copula (agree)’, and compound case markers such as ni-tsuite ‘in terms of’ of tadasu ‘examine’ are acquired. 7. EXPERIMENTS AND DISCUSSION Since it is hard to evaluate the dictionary statically, we use the dictionary in case structure analysis and evaluate the analysis result. We used 200 sentences of Mainichi Newspaper Corpus as a test set. We analyzed case structures of the sentences using the method proposed by [4]. As the evaluation of the case structure analysis, we checked whether cases of ambiguous case components (topic markers and clausal modiﬁers) are correctly detected or not. The evaluation result is presented in Table 2. The baseline is the result by assigning a vacant case in order of ’ga’, ’wo’, and ’ni’. When we do not consider parsing errors to evaluate the case detection, the accuracy of our method for topic markers was 96% and that for clausal modiﬁers was 76%. The baseline accuracy for topic markers was 91% and that for clausal modiﬁers was 62%. Thus we see our method is superior to the baseline.  Table 2: The accuracy of case detection.  correct case detection  incorrect case parsing error detection  our method  topic marker clausal modiﬁer  85 48  4 15  13 2  baseline  topic marker clausal modiﬁer  81 39  8 24  13 2  The following are examples of analysis results6:  (1)  1ookurasyo ga  wa  the Ministry of Finance TM  ginko ga bank nom-CM  2 tsumitate-teiru 2ryuhokin wo no  deposit  reserve fund of  torikuzushi wo  3 mitomeru  consume acc-CM consent  3houshin×ni† wo  
In collaboration with colleagues at UW, OGI, IBM, and SRI, we are developing technology to process spoken language from informal meetings. The work includes a substantial data collection and transcription effort, and has required a nontrivial degree of infrastructure development. We are undertaking this because the new task area provides a signiﬁcant challenge to current HLT capabilities, while offering the promise of a wide range of potential applications. In this paper, we give our vision of the task, the challenges it represents, and the current state of our development, with particular attention to automatic transcription. 1. THE TASK We are primarily interested in the processing (transcription, query, search, and structural representation) of audio recorded from informal, natural, and even impromptu meetings. By “informal” we mean conversations between friends and acquaintances that do not have a strict protocol for the exchanges. By “natural” we mean meetings that would have taken place regardless of the recording process, and in acoustic circumstances that are typical for such meetings. By “impromptu” we mean that the conversation may take place without any preparation, so that we cannot require special instrumentation to facilitate later speech processing (such as close-talking or array microphones). A plausible image for such situations is a handheld device (PDA, cell phone, digital recorder) that is used when conversational partners agree that their discussion should be recorded for later reference. Given these interests, we have been recording and transcribing a series of meetings at ICSI. The recording room is one of ICSI’s standard meeting rooms, and is instrumented with both close-talking and distant microphones. Close-mic’d recordings will support research on acoustic modeling, language modeling, dialog modeling, etc., without having to immediately solve the difﬁculties of far-ﬁeld microphone speech recognition. The distant microphones are included to facilitate the study of these deep acoustic problems, and to provide a closer match to the operating conditions ultimately envisaged. These ambient signals are col- .  lected by 4 omnidirectional PZM table-mount microphones, plus a “dummy” PDA that has two inexpensive microphone elements. In addition to these 6 distant microphones, the audio setup permits a maximum of 9 close-talking microphones to be simultaneously recorded. A meeting recording infrastructure is also being put in place at Columbia University, at SRI International, and by our colleagues at the University of Washington. Recordings from all sites will be transcribed using standards evolved in discussions that also involved IBM (who also have committed to assist in the transcription task). Colleagues at NIST have been in contact with us to further standardize these choices, since they intend to conduct related collection efforts. A segment from a typical discussion recorded at ICSI is included below in order to give the reader a more concrete sense of the task. Utterances on the same line separated by a slash indicate some degree of overlapped speech. A: Ok. So that means that for each utterance, .. we’ll need the time marks. E: Right. / A: the start and end of each utterance. [a few turns omitted] E: So we - maybe we should look at the um .. the tools that Mississippi State has. D: Yeah. E: Because, I - I - I know that they published .. um .. annotation tools. A: Well, X-waves have some as well, .. but they’re pretty low level .. They’re designed for uh - / D: phoneme / A: for phoneme-level / D: transcriptions. Yeah. J: I should A: Although, they also have a nice tool for - .. that could be used for speaker change marking. D: There’s a - there are - there’s a whole bunch of tools J: Yes. / D: web page, where they have a listing. D: like 10 of them or something. J: Are you speaking about Mississippi State per se? or D: No no no, there’s some .. I mean, there just - there are there are a lot of / J: Yeah. J: Actually, I wanted to mention - / D: (??) J: There are two projects, which are .. international .. huge projects focused on this kind of thing, actually .. one of them’s MATE, one of them’s EAGLES .. and um. D: Oh, EAGLES. D: (??) / J: And both of them have J: You know, I shou-, I know you know about the big book. E: Yeah. J: I think you got it as a prize or something. E: Yeah. / D: Mhm. J: Got a surprise. flaughg fJ. thought “as a prize” sounded like “surprise”g Note that interruptions are quite frequent; this is, in our experience, quite common in informal meetings, as is acoustic overlap  between speakers (see the section on error rates in overlap regions). 2. THE CHALLENGES While having a searchable, annotatable record of impromptu meetings would open a wide range of applications, there are signiﬁcant technical challenges to be met; it would not be far from the truth to say that the problem of generating a full representation of a meeting is “AI complete”, as well as “ASR complete”. We believe, however, that our community can make useful progress on a range of associated problems, including: ASR for very informal conversational speech, including the common overlap problem. ASR from far-ﬁeld microphones - handling the reverberation and background noise that typically bedevil distant mics, as well as the acoustic overlap that is more of a problem for microphones that pick up several speakers at approximately the same level. Segmentation and turn detection - recovering the different speakers and turns, which also is more difﬁcult with overlaps and with distant microphones (although inter-microphone timing cues can help here). Extracting nonlexical information such as speaker identiﬁcation and characterization, voice quality variation, prosody, laughter, etc. Dialog abstraction - making high-level models of meeting ‘state’; identifying roles among participants, classifying meeting types, etc. [2]. Dialog analysis - identiﬁcation and characterization of ﬁnescale linguistic and discourse phenomena [3][10]. Information retrieval from errorful meeting transcriptions topic change detection, topic classiﬁcation, and query matching. Summarization of meeting content [14] - representation of the meeting structure from various perspectives and at various scales, and issues of navigation in thes representations. Energy and memory resource limitation issues that arise in the robust processing of speech using portable devices [7]. Clearly we and others working in this area (e.g., [15]) are at an early stage in this research. However, the remainder of this paper will show that even a preliminary effort in recording, manually transcribing, and recognizing data from natural meetings has provided some insight into at least a few of these problems. 3. DATA COLLECTION AND HUMAN TRANSCRIPTION Using the data collection setup described previously, we have been recording technical meetings at ICSI. As of this writing we have recorded 38 meetings for a total of 39 hours. Note that there are separate microphones for each participant in addition to the 6 far-ﬁeld microphones, and there can be as many as 15 open channels. Consequently the sound ﬁles comprise hundreds of hours of recorded audio. The total number of participants in all meetings is 237, and there were 49 unique speakers. The majority of the meetings recorded so far have either had a focus on “Meeting Recorder”  (that is, meetings by the group working on this technology) or “Robustness” (primarily concerned with ASR robustness to acoustic effects such as additive noise). A smaller number of other meeting types at ICSI were also included. In addition to the spontaneous recordings, we asked meeting participants to read digit strings taken from a TI digits test set. This was done to facilitate research in far-ﬁeld microphone ASR, since we expect this to be quite challenging for the more unconstrained case. At the start or end of each meeting, each participant read 20 digit strings. Once the data collection was in progress, we developed a set of procedures for our initial transcription. The transcripts are wordlevel transcripts, with speaker identiﬁer, and some additional information: overlaps, interrupted words, restarts, vocalized pauses, backchannels, and contextual comments, and nonverbal events (which are further subdivided into vocal types such as cough and laugh, and nonvocal types such as door slams and clicks). Each event is tied to the time line through use of a modiﬁed version of the “Transcriber” interface (described below). This Transcriber window provides an editing space at the top of the screen (for adding utterances, etc), and the wave form at the bottom, with mechanisms for ﬂexibly navigating through the audio recording, and listening and re-listening to chunks of virtually any size the user wishes. The typical process involves listening to a stretch of speech until a natural break is found (e.g., a long pause when no one is speaking). The transcriber separates that chunk from what precedes and follows it by pressing the Return key. Then he or she enters the speaker identiﬁer and utterance in the top section of the screen. The interface is efﬁcient and easy to use, and results in an XML representation of utterances (and other events) tied to time tags for further processing. The “Transcriber” interface [13] is a well-known tool for transcription, which enables the user to link acoustic events to the wave form. However, the ofﬁcial version is designed only for singlechannel audio. As noted previously, our application records up to 15 parallel sound tracks generated by as many as 9 speakers, and we wanted to capture the start and end times of events on each channel as precisely as possible and independently of one another across channels. The need to switch between multiple audio channels to clarify overlaps, and the need to display the time course of events on independent channels required extending the “Transcriber” interface in two ways. First, we added a menu that allows the user to switch the playback between a number of audio ﬁles (which are all assumed to be time synchronized). Secondly, we split the timelinked display band into as many independent display bands as there are channels (and/or independent layers of time-synchronized annotation). Speech and other events on each of the bands can now be time-linked to the wave form with complete freedom and totally independently of the other bands. This enables much more precise start and end times for acoustic events. See [8] for links to screenshots of these extensions to Transcriber (as well as to other updates about our project). In the interests of maximal speed, accuracy and consistency, the transcription conventions were chosen so as to be: quick to type, related to standard literary conventions where possible (e.g., - for interrupted word or thought, .. for pause, using standard orthography rather than IPA), and minimalist (requiring no more decisions by transcribers than absolutely necessary). After practice with the conventions and the interface, transcribers achieved a 12:1 ratio of transcription time to speech time. The amount of time required for transcription of spoken language is known to vary widely as a function of properties of the discourse (amount of overlap, etc.), and amount of detailed encoding (prosod-  ics, etc.), with estimates ranging from 10:1 for word-level with minimal added information to 20:1, for highly detailed discourse transcriptions (see [4] for details). In our case, transcribers encoded minimal added detail, but had two additional demands: marking boundaries of time bins, and switching between audio channels to clarify the many instances of overlapping speech in our data. We speeded the marking of time bins by providing them with an automatically segmented version (described below) in which the segmenter provided a preliminary set of speech/nonspeech labels. Transcribers indicated that the presegmentation was correct sufﬁciently often that it saved them time. After the transcribers ﬁnished, their work was edited for consistency and completeness by a senior researcher. Editing involved checking exhaustive listings of forms in the data, spell checking, and use of scripts to identify and automatically encode certain distinctions (e.g., the distinction between vocalized nonverbal events, such as cough, and nonvocalized nonverbal events, like door slams). This step requires on average about 1:1 - one minute of editing for each minute of speech. Using these methods and tools, we have currently transcribed about 12 hours out of our 39 hours of data. Other data have been sent to IBM for a rough transcription using commercial transcribers, to be followed by a more detailed process at ICSI. Once this becomes a routine component of our process, we expect it to signiﬁcantly reduce the time requirements for transcription at ICSI. 4. AUTOMATIC TRANSCRIPTION As a preliminary report on automatic word transcription, we present results for six example meetings, totalling nearly 7 hours of speech, 36 total speakers, and 15 unique speakers (since many speakers participated in multiple meetings). Note that these results are preliminary only; we have not yet had a chance to address the many obvious approaches that could improve performance. In particular, in order to facilitate efforts in alignment, pronunciation modeling, language modeling, etc., we worked only with the close-mic’d data. In most common applications of meeting transcription (including those that are our chief targets in this research) such a microphone arrangement may not be practical. Nevertheless we hope the results using the close microphone data will illustrate some basic observations we have made about meeting data and its automatic transcription. 4.1 Recognition system The recognizer was a stripped-down version of the largevocabulary conversational speech recognition system ﬁelded by SRI in the March 2000 Hub-5 evaluation [11]. The system performs vocal-tract length normalization, feature normalization, and speaker adaptation using all the speech collected on each channel (i.e., from one speaker, modulo cross-talk). The acoustic model consisted of gender-dependent, bottom-up clustered (genonic) Gaussian mixtures. The Gaussian means are adapted by a linear transform so as to maximize the likelihood of a phone-loop model, an approach that is fast and does not require recognition prior to adaptation. The adapted models are combined with a bigram language model for decoding. We omitted more elaborate adaptation, cross-word triphone modeling, and higher-order language and duration models from the full SRI recognition system as an expedient in our initial recognition experiments (the omitted steps yield about a 20% relative error rate reduction on Hub-5 data). It should be noted that both the acoustic models and the language model of the recognizer were identical to those used in the Hub-5 domain. In particular, the acoustic front-end assumes a telephone channel, requiring us to downsample the wide-band signals  of the meeting recordings. The language model contained about 30,000 words and was trained on a combination of Switchboard, CallHome English and Broadcast News data, but was not tuned for or augmented by meeting data. 4.2 Speech segmentation As noted above, we are initially focusing on recognition of the individual channel data. Such data provide an upper bound on recognition accuracy if speaker segmentation were perfect, and constitute a logical ﬁrst step for obtaining high quality forced alignments against which to evaluate performance for both near- and farﬁeld microphones. Individual channel recordings were partitioned into “segments” of speech, based on a “mixed” signal (addition of the individual channel data, after an overall energy equalization factor per channel). Segment boundary times were determined either by an automatic segmentation of the mixed signal followed by hand-correction, or by hand-correction alone. For the automatic case, the data was segmented with a speech/nonspeech detector consisting of an extension of an approach using an ergodic hidden Markov model (HMM) [1]. In this approach, the HMM consists of two main states, one representing “speech” and one representing “nonspeech” and a number of intermediate states that are used to model the time constraints of the transitions between the two main states. In our extension, we are incorporating mixture densities rather than single Gaussians. This appears to be useful for the separation of foreground from background speech, which is a serious problem in these data. The algorithm described above was trained on the speech/nonspeech segmentation provided manually for the ﬁrst meeting that was transcribed. It was used to provide segments of speech for the manual transcribers, and later for the recognition experiments. Currently, for simplicity and to debug the various processing steps, these segments are synchronous across channels. However, we plan to move to segments based on separate speech/nonspeech detection in each individual channel. The latter approach should provide better recognition performance, since it will eliminate cross-talk in segments in which one speaker may say only a backchannel (e.g. “uhhuh”) while another speaker is talking continuously. Performance was scored for the spontaneous conversational portions of the meetings only (i.e., the read digit strings referred to earlier were excluded). Also, for this study we ran recognition only on those segments during which a transcription was produced for the particular speaker. This overestimates the accuracy of word recognition, since any speech recognized in the “empty” segments would constitute an error not counted here. However, adding the empty regions would increase data load by a factor of about ten— which was impractical for us at this stage. Note that the current NIST Hub-5 (Switchboard) task is similar in this respect: data are recorded on separated channels and only the speech regions of a speaker are run, not the regions in which they are essentially silent. We plan to run all speech (including these “empty” segments) in future experiments, to better assess actual performance in a real meeting task. 4.3 Recognition results and discussion Overall error rates. Table 1 lists word error rates for the six meetings, by speaker. The data are organized into two groups: native speakers and nonnative speakers. Since our recognition system is not trained on nonnative speakers, we provide results only for the native speakers; however the word counts are listed for all partici-  Table 1: Recognition performance by speaker and meeting (MRM = “Meeting Recorder meeting”; ROB = “Robustness meeting”). Speaker gender is indicated by “M” or “F” in the speaker labels. “* : : : *” marks speakers using a lapel microphone; all other cases used close-talking head-mounted microphones. “—” indicates speakers with severely degraded or missing signals due to incorrect microphone usage. Word error rates are in boldface, total number of words in Roman, and out-of-vocabulary (OOV) rates in italics. OOV rate is by token, relative to a Hub-5 language model. WER is for conversational speech sections of meetings only, and are not reported for nonnative speakers.  Meeting Duration (minutes) Native speakers M 004  MRM002 MRM003 MRM004  45  78  60  42.4 4550 2.07  48.1 3087 2.75  MRM005 ROB005  68  81  44.3 3432 1.60  48.4 4912 2.12  ROB004 70 45.1 5512 1.61  M 001  42.4 2311 1.82  50.6 2488 2.09  37.6 1904 2.78  38.6 3400 1.56  F 001  45.2 3008 2.59  43.2 3360 3.18  42.9 2714 4.05  41.9 2705 2.14  M 009  *100.1* 1122 1.59  *115.8* 367 2.45  38.2 1066 1.88  *68.7* 696 2.01  F 002  45.2 1549 2.26  43.7 1481 2.64  *46.0* 2480 1.63  M 002  *55.6*  990  2.12  Speakers with low word counts  M 007  55.6  —  198  69  2.97  2.90  M 008  72.7  59.5  55  121  5.45  5.79  M 015  Non-native speakers (total words only)  M 003 (British)  2189  M 011 (Spanish)  2653  F 003 (Spanish)  M 010 (German)  M 012 (German)  M 006 (French)  1239 28 639  — 59 6.56  663  620  220  3524  2648  pants for completeness.1 The main result to note from Table 1 is that overall word error rates are not dramatically worse than for Switchboard-style data. This is particularly impressive since, as described earlier, no meeting data were used in training, and no modiﬁcations of the acoustic or language models were made. The overall WER for native speakers was 46.5%, or only about a 7% relative increase over a comparable recognition system on Hub-5 telephone conversations. This suggests that from the point of view of pronunciation and language (as opposed to acoustic robustness, e.g., for distant microphones), Switchboard may also be “ASR-complete”. That is, talkers may not really speak in a more “sloppy” manner in meetings than they do in casual phone conversation. We further investigate this claim in the next section, by breaking down results by overlap versus nonoverlap regions, by microphone type and by speaker. Note that in some cases there were very few contributions from a speaker (e.g., speakers M 007, M 008, and M 015), and such speakers also tended to have higher word error rates. We initially suspected the problem was a lack of sufﬁcient data for speaker adaptation; indeed the improvement from adaptation was less than for other speakers. Thus for such speakers it would make sense to pool data across meetings for repeat participants. However, in looking at their word transcripts we noted that their utterances, while few, tended to be dense with information content. That is, these were not the speakers uttering “uhhuh” or short common phrases (which are generally well modeled in the Switchboard recognizer) but rather high-perplexity utterances that are generally harder to recognize. Such speakers also tend to have a generally higher overall OOV rate than other speakers. Error rates in overlap versus nonoverlap regions. As noted in the previous section, the overall word error rate in our sample meetings was slightly higher than in Switchboard. An obvious question to ask here is: what is the effect on recognition of overlapping speech? To address this question, we deﬁned a crude measure of overlap. Since segments were channel-synchronous in these meetings, a segment was either non-overlapping (only one speaker was talking during that time segment), or overlapping (two or more speakers were talking during the segment). Note that this does not measure amount of overlap or number of overlapping speakers; more sophisticated measures based on the phone backtrace from forced alignment would provide a better measure for more detailed analyses. Nevertheless, the crude measure provides a clear ﬁrst answer to our question. Since we were also interested in the interaction if any between overlap and microphone type, we computed results separately for the head-mounted and lapel microphones. Results were also computed by speaker, since as shown earlier in Table 1, speakers varied in word error rates, total words, and words by microphone type. Note that speakers M 009 and F 002 have data from both conditions. As shown, our measure of overlap (albeit crude), clearly shows that overlapping speech is a major problem for the recognition of speech from meetings. If overlap regions are removed, the recognition accuracy overall is actually better than that for Switchboard. It is premature to make absolute comparisons here, but the fact that the same pattern is observed for all speakers and across microphone 1Given the limitations of these pilot experiments (e.g., no on-task training material and general pronunciation models), recognition on nonnative speakers is essentially not working at present. In the case of one nonnative speaker, we achieved a 200% word error rate, surpassing a previous ICSI record. Word error results presented here are based on meeting transcripts as of March 7, 2000, and are subject to small changes as a result of ongoing transcription error checking.  Table 2: Word error rates broken down by whether or not segment is in a region of overlapping speech.  Speaker M 004 M 001 F 001 M 009 F 002 M 002 M 007 M 008 Overall  No overlap  Headset Lapel  41.0  -  34.2  -  40.5  -  30.7 41.0  37.7 29.8  -  48.6  52.2  -  50.9  -  39.9 38.5  With overlap  Headset Lapel  50.3  -  47.6  -  45.8  -  40.7 117.8  50.5 56.3  -  71.3  81.3  -  69.9  48.7 85.2  conditions suggests that it is not the inherent speech properties of participants that makes meetings difﬁcult to recognize, but rather the presence of overlapping speech. Furthermore, one can note from Table 2 that there is a large interaction between microphone type and the effect of overlap. Overlap is certainly a problem even for the close-talking head-mounted microphones. However, the degradation due to overlap is far greater for the lapel microphone, which picks up a greater degree of background speech. As demonstrated by speaker F 002, it is possible to have a comparatively good word error rate (29.8%) on the lapel microphone in regions of no overlap (in this case 964/2480 words were in nonoverlapping segments). Nevertheless, since the rate of overlaps is so high in the data overall, we are avoiding the use of the lapel microphone where possible in the future, preferring head-mounted microphones for obtaining ground truth for research purposes. We further note that for tests of acoustic robustness for distant microphones, we tend to prefer microphones mounted on the meeting table (or on a mock PDA frame), since they provide a more realistic representation of the ultimate target application that is a central interest to us - recognition via portable devices. In other words, we are ﬁnding lapel mics to be too “bad” for near-ﬁeld microphone tests, and too “good” for far-ﬁeld tests. Error rates by error type. The effect of overlapping speech on error rates is due almost entirely to insertion errors, as shown in Figure 1. Rates of other error types are nearly identical to those observed for Switchboard (modulo a a slight increase in substitutions associated with the lapel condition). This result is not surprising, since background speech obviously adds false words in the hypothesis. However, it is interesting that there is little increase in the other error types, suggesting that a closer segmentation based on individual channel data (as noted earlier) could greatly improve recognition accuracy (by removing the surrounding background speech). Error rates by meeting type. Different types of meetings should give rise to differences in speaking style and social interaction, and we may be interested in whether such effects are realized as differences in word error rates. The best way to measure such effects is within speaker. The collection of regular, ongoing meetings at ICSI offers the possibility of such within-speaker comparisons, since multiple speakers participate in more than one type of regular meeting. Of the speakers shown in the data set used for this study, speaker M 004 is a good case in point, since he has data from three “Meeting Recorder” meetings and two “Robustness” meetings. These two meeting types differ in social interaction; in the ﬁrst, there is a fairly open exchange between many of the partici-  50  Switchboard  Head−Mic, Overlap  40  Head−Mic, Nonoverlap  Lapel−Mic, Overlap 
In this paper, we discuss experiments applying machine learning techniques to the task of confusion set disambiguation, using three orders of magnitude more training data than has previously been used for any disambiguation-in-string-context problem. In an attempt to determine when current learning methods will cease to benefit from additional training data, we analyze residual errors made by learners when issues of sparse data have been significantly mitigated. Finally, in the context of our results, we discuss possible directions for the empirical natural language research community. Keywords Learning curves, data scaling, very large corpora, natural language disambiguation. 1. INTRODUCTION A significant amount of work in empirical natural language processing involves developing and refining machine learning techniques to automatically extract linguistic knowledge from online text corpora. While the number of learning variants for various problems has been increasing, the size of training sets such learning algorithms use has remained essentially unchanged. For instance, for the much-studied problems of part of speech tagging, base noun phrase labeling and parsing, the Penn Treebank, first released in 1992, remains the de facto training corpus. The average training corpus size reported in papers published in the ACL-sponsored Workshop on Very Large Corpora was essentially unchanged from the 1995 proceedings to the 2000 proceedings. While the amount of available on-line text has been growing at an amazing rate over the last five years (by some estimations, there are currently over 500 billion readily accessible words on the web), the size of training corpora used by  our field has remained static. Confusable word set disambiguation, the problem of choosing the correct use of a word given a set of words with which it is commonly confused, (e.g. {to, too, two}, {your, you’re}), is a prototypical problem in NLP. At some level, this task is identical to many other natural language problems, including word sense disambiguation, determining lexical features such as pronoun case and determiner number for machine translation, part of speech tagging, named entity labeling, spelling correction, and some formulations of skeletal parsing. All of these problems involve disambiguating from a relatively small set of tokens based upon a string context. Of these disambiguation problems, lexical confusables possess the fortunate property that supervised training data is free, since the differences between members of a confusion set are surface-apparent within a set of well-written text. To date, all of the papers published on the topic of confusion set disambiguation have used training sets for supervised learning of less than one million words. The same is true for most if not all of the other disambiguation-in-string-context problems. In this paper we explore what happens when significantly larger training corpora are used. Our results suggest that it may make sense for the field to concentrate considerably more effort into enlarging our training corpora and addressing scalability issues, rather than continuing to explore different learning methods applied to the relatively small extant training corpora. 2. PREVIOUS WORK 2.1 Confusion Set Disambiguation Several methods have been presented for confusion set disambiguation. The more recent set of techniques includes multiplicative weight-update algorithms [4], latent semantic analysis [7], transformation-based learning [8], differential grammars [10], decision lists [12], and a variety of Bayesian classifiers [2,3,5]. In all of these papers, the problem is formulated as follows: Given a specific confusion set (e.g. {to, two, too}), all occurrences of confusion set members in the test set are replaced by some marker. Then everywhere the system sees this marker, it must decide which member of the confusion set to choose. Most learners that have been applied to this problem use as features the words and part of speech tags  appearing within a fixed window, as well as collocations surrounding the ambiguity site; these are essentially the same features as those used for the other disambiguation-in-stringcontext problems. 2.2 Learning Curves for NLP A number of learning curve studies have been carried out for different natural language tasks. Ratnaparkhi [12] shows a learning curve for maximum-entropy parsing, for up to roughly one million words of training data; performance appears to be asymptoting when most of the training set is used. Henderson [6] showed similar results across a collection of parsers. Figure 1 shows a learning curve we generated for our task of word-confusable disambiguation, in which we plot test classification accuracy as a function of training corpus size using a version of winnow, the best-performing learner reported to date for this well-studied task [4]. This curve was generated by training on successive portions of the 1-million word Brown corpus and then testing on 1-million words of Wall Street Journal text for performance averaged over 10 confusion sets. The curve might lead one to believe that only minor gains are to be had by increasing the size of training corpora past 1 million words. While all of these studies indicate that there is likely some (but perhaps limited) performance benefit to be obtained from increasing training set size, they have been carried out only on relatively small training corpora. The potential impact to be felt by increasing the amount of training data by any signifcant order has yet to be studied.  Test Accuracy  0.82 0.80 0.78 0.76 0.74 0.72 0.70 100,000  400,000  700,000  Training Corpus Size (words)  1,000,000  Figure 1: An Initial Learning Curve for Confusable Disambiguation  3. EXPERIMENTS This work attempts to address two questions – at what point will learners cease to benefit from additional data, and what is the nature of the errors which remain at that point. The first question impacts how best to devote resources in order to improve natural language technology. If there is still much to be gained from additional data, we should think hard about ways to effectively increase the available training data for problems of interest. The second question allows us to study failures due to inherent weaknesses in learning methods and features rather than failures due to insufficient data.  Since annotated training data is essentially free for the problem of confusion set disambiguation, we decided to explore learning curves for this problem for various machine learning algorithms, and then analyze residual errors when the learners are trained on all available data. The learners we used were memory-based learning, winnow, perceptron,1 transformation-based learning, and decision trees. All learners used identical features2 and were used out-of-the-box, with no parameter tuning. Since our point is not to compare learners we have refrained from identifying the learners in the results below.  We collected a 1-billion-word training corpus from a variety of English texts, including news articles, scientific abstracts, government transcripts, literature and other varied forms of prose. Using this collection, which is three orders of magnitude greater than the largest training corpus previously used for this task, we trained the five learners and tested on a set of 1 million words of Wall Street Journal text.3  In Figure 2 we show learning curves for each learner, for up to one billion words of training data.4 Each point in the graph reflects the average performance of a learner over ten different confusion sets which are listed in Table 1. Interestingly, even out to a billion words, the curves appear to be log-linear. Note that the worst learner trained on approximately 20 million words outperforms the best learner trained on 1 million words. We see that for the problem of confusable disambiguation, none of our learners is close to asymptoting in performance when trained on the one million word training corpus commonly employed within the field.  Table 1: Confusion Sets  {accept, except}  {principal, principle}  {affect, effect}  {then, than}  {among, between}  {their, there}  {its, it’s}  {weather, whether}  {peace, piece}  {your, you’re}  The graph in Figure 2 demonstrates that for word confusables, we can build a system that considerably outperforms the current best results using an incredibly simplistic learner with just slightly more training data. In the graph, Learner 1 corresponds to a trivial memory-based learner. This learner simply keeps track of all <wi-1, wi+1>, < wi-1> and <wi+1> counts for all occurrences of the confusables in the training set. Given a test set instance, the learner will first check if it has seen <wi-1,wi+1> in the training set. If so, it chooses the confusable word most frequently observed with this tuple. Otherwise, the learner backs off to check for the frequency of <wi-1>; if this also was not seen then it will back off to <wi+1>, and lastly, to the most frequently observed confusion- 
We present and evaluate the initial version of RIPTIDES, a system that combines information extraction, extraction-based summarization, and natural language generation to support userdirected multidocument summarization. 1. INTRODUCTION Although recent years has seen increased and successful research efforts in the areas of single-document summarization, multidocument summarization, and information extraction, very few investigations have explored the potential of merging summarization and information extraction techniques. This paper presents and evaluates the initial version of RIPTIDES, a system that combines information extraction (IE), extraction-based summarization, and natural language generation to support userdirected multidocument summarization. (RIPTIDES stands for RapIdly Portable Translingual Information extraction and interactive multiDocumEnt Summarization.) Following [10], we hypothesize that IE-supported summarization will enable the generation of more accurate and targeted summaries in specific domains than is possible with current domain-independent techniques. In the sections below, we describe the initial implementation and evaluation of the RIPTIDES IE-supported summarization system. We conclude with a brief discussion of related and ongoing work. 2. SYSTEM DESIGN Figure 1 depicts the IE-supported summarization system. The system first requires that the user select (1) a set of documents in which to search for information, and (2) one or more scenario templates (extraction domains) to activate. The user optionally provides filters and preferences on the scenario template slots, specifying what information s/he wants to be reported in the summary. RIPTIDES next applies its Information Extraction subsystem to generate a database of extracted events for the selected domain and then invokes the Summarizer to generate a natural language summary of the extracted information subject to the user’s constraints. In the subsections below, we describe the  IE system and the Summarizer in turn. 2.1 IE System The domain for the initial IE-supported summarization system and its evaluation is natural disasters. Very briefly, a top-level natural disasters scenario template contains: document-level information (e.g. docno, date-time); zero or more agent elements denoting each person, group, and organization in the text; and zero or more disaster elements. Agent elements encode standard information for named entities (e.g. name, position, geo-political unit). For the most part, disaster elements also contain standard event-related fields (e.g. type, number, date, time, location, damage sub-elements). The final product of the RIPTIDES system, however, is not a set of scenario templates, but a user-directed multidocument summary. This difference in goals influences a number of template design issues. First, disaster elements must distinguish different reports or views of the same event from multiple sources. As a result, the system creates a separate disaster event for each such account. Disaster elements should also include the reporting agent, date, time, and location whenever possible. In addition, damage elements (i.e. human and physical effects) are best grouped according to the reporting event. Finally, a slight broadening of the IE task was necessary in that extracted text was not constrained to noun phrases. In particular, adjectival and adverbial phrases that encode reporter confidence, and sentences and clauses denoting relief effort progress appear beneficial for creating informed summaries. Figure 2 shows the scenario template for one of 25 texts tracking the 1998 earthquake in Afghanistan (TDT2 Topic 89). The texts were also manually annotated for noun phrase coreference; any phrase involved in a coreference relation appears underlined in the running text. The RIPTIDES system for the most part employs a traditional IE architecture [4]. In addition, we use an in-house implementation of the TIPSTER architecture [8] to manage all linguistic annotations. A preprocessor first finds sentences and tokens. For syntactic analysis, we currently use the Charniak [5] parser, which creates Penn Treebank-style parses [9] rather than the partial parses used in most IE systems. Output from the parser is converted automatically into TIPSTER parse and part-of-speech annotations, which are added to the set of linguistic annotations for the document. The extraction phase of the system identifies domain-specific relations among relevant entities in the text. It relies on Autoslog-XML, an XSLT implementation of the Autoslog-TS system [12], to acquire extraction patterns. Autoslog-XML is a weakly supervised learning system that requires two sets of texts for training — one set comprises texts relevant to the domain of interest and the other, texts not relevant  user information need  text collection  IE System  slot filler slot sfliollterfiller ssslllooottt..ss.ffflliiioolllllltteeessss..rrrllll.ooooffiittttllllssss.ee.llll.ffffrrooooiiiillllttttllll.eeee..rrrrffffiiiilllllllleeeerrrr scenario templates  A powerful earthquake struck Afghanistan on May 30 at 11:25… Damage VOA (06/02/1998) estimated that 5,000 were killed by the earthquake, whereas AP (APW, 06/02/1998) instead reported … Relief Status CNN (06/02/1998): Food, water, medicine and other supplies have started to arrive. […] summary  Summarizer multi-document template merging event-oriented structure content selection event-oriented structure with slot importance scores NLG of summary  Figure 1. RIPTIDES System Design  to the domain. Based on these and a small set of extraction pattern templates, the system finds a ranked list of possible extraction patterns, which a user then annotates with the appropriate extraction label (e.g. victim). Once acquired, the patterns are applied to new documents to extract slot fillers for the domain. Selectional restrictions on allowable slot fillers are implemented using WordNet [6] and BBN’s Identifinder [3] named entity component. In the current version of the system, no coreference resolution is attempted; instead, we rely on a very simple set of heuristics to guide the creation of output templates. The disaster scenario templates extracted for each text are provided as input to the summarization component along with all linguistic annotations accrued in the IE phase. No relief slots are included in the output at present, since there was insufficient annotated data to train a reliable sentence categorizer. 2.2 The Summarizer In order to include relief and other potentially relevant information not currently found in the scenario templates, the Summarizer extracts selected sentences from the input articles and adds them to the summaries generated from the scenario templates. The extracted sentences are listed under the heading  Selected News Excerpts, as shown in the two sample summaries appearing in Figures 3 and 4, and discussed further in Section 2.2.5 below. 2.2.1 Summarization Stages The Summarizer produces each summary in three main stages. In the first stage, the output templates are merged into an eventoriented structure, while keeping track of source information. The merge operation currently relies on simple heuristics to group extracted facts that are comparable; for example, during this phase damage reports are grouped according to whether they pertain to the event as a whole, or instead to damage in the same particular location. Heuristics are also used in this stage to determine the most relevant damage reports, taking into account specificity, recency and news source. Towards the same objective but using a more surface-oriented means, simple word-overlap clustering is used to group sentences from different documents into clusters that are likely to report similar content. In the second stage, a base importance score is first assigned to each slot/sentence based on a combination of document position, document recency and group/cluster membership. The base importance scores are then adjusted according to user-specified preferences and matching  Document no.: ABC19980530.1830.0342 Date/time: 05/30/1998 18:35:42.49 Disaster Type: earthquake •location: Afghanistan •date: today •magnitude: 6.9 •magnitude-confidence: high •epicenter: a remote part of the country •damage: human-effect: victim: Thousands of people number: Thousands outcome: dead confidence: medium confidence-marker: feared physical-effect: object: entire villages outcome: damaged confidence: medium confidence-marker: Details now hard to come by / reports say  PAKISTAN MAY BE PREPARING FOR ANOTHER TEST Thousands of people are feared dead following... (voiceover) ...a powerful earthquake that hit Afghanistan today. The quake registered 6.9 on the Richter scale, centered in a remote part of the country. (on camera) Details now hard to come by, but reports say entire villages were buried by the quake.  Figure 2. Example scenario template for the natural disasters domain  criteria. The adjusted scores are used to select the most important slots/sentences to include in the summary, subject to the userspecified word limit. In the third and final stage, the summary is generated from the resulting content pool using a combination of top-down, schema-like text building rules and surface-oriented revisions. The extracted sentences are simply listed in document order, grouped into blocks of adjacent sentences. 2.2.2 Specificity of Numeric Estimates In order to intelligently merge and summarize scenario templates, we found it necessary to explicitly handle numeric estimates of varying specificity. While we did find specific numbers (such as 3,000) in some damage estimates, we also found cases with no number phrase at all (e.g. entire villages). In between these extremes, we found vague estimates (thousands) and ranges of numbers (anywhere from 2,000 to 5,000). We also found phrases that cannot be easily compared (more than half the region’s residents). To merge related damage information, we first calculate the numeric specificity of the estimate as one of the values NONE, VAGUE, RANGE, SPECIFIC, or INCOMPARABLE, based on the presence of a small set of trigger words and phrases (e.g. several, as many as, from … to). Next, we identify the most specific current estimates by news source, where a later estimate is considered to update an earlier estimate if it is at least as specific. Finally, we determine two types of derived information units, namely (1) the minimum and maximum estimates across the news sources, and  (2) any intermediate estimates that are lower than the maximum estimate.1 In the content determination stage, scores are assigned to the derived information units based on the maximum score of the underlying units. In the summary generation stage, a handful of text planning rules are used to organize the text for these derived units, highlighting agreement and disagreement across sources. 2.2.3 Improving the Coherence of Extracted Sentences In our initial attempt to include extracted sentences, we simply chose the top ranking sentences that would fit within the word limit, subject to the constraint that no more than one sentence per cluster could be chosen, in order to help avoid redundancy. We found that this approach often yielded summaries with very poor coherence, as many of the included sentences were difficult to make sense of in isolation. To improve the coherence of the extracted sentences, we have experimented with trying to boost coherence by favoring sentences in the context of the highest-ranking sentences over those with lower ranking scores, following the hypothesis that it is better to cover fewer topics in more depth than to change topics excessively. In particular, we assign a score to a set of sentences by summing the base scores plus increasing coherence boosts for adjacent sentences, sentences that precede ones with an initial 
Recent advances in Automatic Speech Recognition technology have put the goal of naturally sounding dialog systems within reach. However, the improved speech recognition has brought to light a new problem: as dialog systems understand more of what the user tells them, they need to be more sophisticated at responding to the user. The issue of system response to users has been extensively studied by the natural language generation community, though rarely in the context of dialog systems. We show how research in generation can be adapted to dialog systems, and how the high cost of hand-crafting knowledge-based generation systems can be overcome by employing machine learning techniques. 1. DIALOG SYSTEMS AND GENERATION Recent advances in Automatic Speech Recognition (ASR) technology have put the goal of naturally sounding dialog systems within reach.1 However, the improved ASR has brought to light a new problem: as dialog systems understand more of what the user tells them, they need to be more sophisticated at responding to the user. If ASR is limited in quality, dialog systems typically employ a system-initiative dialog strategy in which the dialog system prompts the user for speciﬁc information and then presents some information to the user. In this paradigm, the range of user input at any time is limited (thus facilitating ASR), and the range of system output at any time is also limited. However, such interactions are not very natural. In a more natural interaction, the user can supply more and different information at any time in the dialog. The dialog system must then support a mixed-initiative dialog strategy. While this strategy places greater requirements on ASR, it also increases the range of system responses and the requirements on their quality in terms of informativeness and of adaptation to the context. For a long time, the issue of system response to users has been studied by the Natural Language Generation (NLG) community, though rarely in the context of dialog systems. What have emerged from this work are a “consensus architecture” [17] which modularizes the large number of tasks performed during NLG in a par- The work reported in this paper was partially funded by DARPA contract MDA972-99-3-0003. .  ticular way, and a range of linguistic representations which can be used in accomplishing these tasks. Many systems have been built using NLG technology, including report generators [8, 7], system description generators [10], and systems that attempt to convince the user of a particular view through argumentation [20, 4]. In this paper, we claim that the work in NLG is relevant to dialog systems as well. We show how the results can be incorporated, and report on some initial work in adapting NLG approaches to dialog systems and their special needs. The dialog system we use is the AT&T Communicator travel planning system.We use machine learning and stochastic approaches where hand-crafting appears to be too complex an option, but we also use insight gained during previous work on NLG in order to develop models of what should be learned. In this respect, the work reported in this paper differs from other recent work on generation in the context of dialog systems [12, 16], which does not modularize the generation process and proposes a single stochastic model for the entire process. We start out by reviewing the generation architecture (Section 2). In Section 3, we discuss the issue of text planning for Communicator. In Section 4, we summarize some initial work in using machine learning for sentence planning [19]. Finally, in Section 5 we summarize work using stochastic tree models in generation [2]. 2. TEXT GENERATION ARCHITECTURE . NLG is conceptualized as a process leading from a high-level communicative goal to a sequence of communicative acts which accomplish this communicative goal. A communicative goal is a goal to affect the user’s cognitive state, e.g., his or her beliefs about the world, desires with respect to the world, or intentions about his or her actions in the world. Following (at least) [13], it has been customary to divide the generation process into three phases, the ﬁrst two of which are planning phases. Reiter [17] calls this architecture a “consensus architecture” in NLG. ¡ During text planning, a high-level communicative goal is broken down into a structured representation of atomic communicative goals, i.e., goals that can be attained with a single communicative act (in language, by uttering a single clause). The atomic communicative goals may be linked by rhetorical relations which show how attaining the atomic goals contributes to attaining the high-level goal. ¡ During sentence planning, abstract linguistic resources are chosen to achieve the atomic communicative goals. This includes choosing meaning-bearing lexemes, and how the meaning-bearing lexemes are connected through abstract grammatical constructions (basically, lexical predicate-argument  Dialog Manager  Natural Language Generation  Text Planner  Sentence Planner  Realizer  Prosody Assigner  Natural Language Understanding  System Utterance TTS User Utterance ASR  Figure 1: Architecture of a dialog system with natural language generation  structure and modiﬁcation). As a side-effect, sentence planning also determines sentence boundaries: there need not be a one-to-one relation between elementary communicative goals and sentences in the ﬁnal text. ¡ During realization, the abstract linguistic resources chosen during sentence planning are transformed into a surface linguistic utterance by adding function words (such as auxiliaries and determiners), inﬂecting words, and determining word order. This phase is not a planning phase in that it only executes decisions made previously, by using grammatical information about the target language. (Prosody assignment can be treated as a separate module which follows realization and which draws on all previous levels of representation. We do not discuss prosody further in this paper.) Note that sentence planning and realization use resources speciﬁc to the target-language, while text planning is language-independent (though presumably it is culture-dependent). In integrating this approach into a dialog system, we see that the dialog manager (DM) no longer determines surface strings to send to the TTS system, as is often the case in current dialog systems. Instead, the DM determines high-level communicative goals which are sent to the NLG component. Figure 1 shows a complete architecture. An advantage of such an architecture is the possibility for extended plug-and-play: not only can the entire NLG system be replaced, but also modules within the NLG system, thus allowing researchers to optimize the system incrementally. The main objection to the use of NLG techniques in dialog systems is that they require extensive hand-tuning of existing systems and approaches for new domains. Furthermore, because of the relative sophistication of NLG techniques as compared to simpler techniques such as templates, the hand-tuning requires specialized knowledge of linguistic representations; hand-tuning templates only requires software engineering skills. An approach based on machine learning can provide a solution to this problem: it draws on previous research in NLG and uses the same sophisticated linguistic representations, but it learns the domain-speciﬁc rules that use these representation automatically from data. It is the goal of our research to show that for dialog systems, approaches based on machine learning can do as well as or outperform handcrafted approaches (be they NLG- or template-based), while requiring far less time for tuning. In the following sections, we summarize the current state of our research on an NLG system for the Communicator dialog system.  3. TEXT PLANNER Based on observations from the travel domain of the Communicator system, we have categorized system responses into two types. The ﬁrst type occurs during the initial phase when the system is gathering information from the user. During this phase, the highlevel communicative goals that the system is trying to achieve are fairly complex: the goals include getting the hearer to supply information, and to explicitly or implicitly conﬁrm information that the hearer has just supplied. (These latter goals are often motivated by the still not perfect quality of ASR.) The second type occurs when the system has obtained information that matches the user’s requirements and the options (ﬂights, hotel, or car rentals) need to be presented to the user. Here, the communicative goal is mainly to make the hearer believe a certain set of facts (perhaps in conjunction with a request for a choice among these options). In the past, NLG systems typically have generated reports or summaries, for which the high-level communicative goal is of the type “make the hearer/reader believe a given set of facts”, as it is in the second type of system response discussed above. We believe that NLG work in text planning can be successfully adapted to better plan these system responses, taking into account not only the information to be conveyed but also the dialog context and knowledge about user preferences. We leave this to ongoing work. In the ﬁrst type of system response, the high-level communicative goal typically is an unordered list of high-level goals, all of which need to be achieved with the next turn of the system. An example is shown in Figure 2. NLG work in text planning has not addressed such complex communicative goals in the past. However, we have found that for the Communicator domain, no text planning is needed, and that the sentence planner can act directly on a representation of the type shown in Figure 2, because the number of goals is limited (to ﬁve, in our studies). We expect that further work in other dialog domains will require an extension of existing work in text planning to account better for communicative goals other than those that simply aim to affect the user’s (hearer’s) beliefs. implicit-conﬁrm(orig-city:NEWARK) implicit-conﬁrm(dest-city:DALLAS) implicit-conﬁrm(month:9) implicit-conﬁrm(day-number:1) request(depart-time) Figure 2: Sample text plan (communicative goals)  Realization What time would you like to travel on September the 1st to Dallas from Newark? Leaving on September the 1st. What time would you like to travel from Newark to Dallas? Leaving in September. Leaving on the 1st. What time would you, traveling from Newark to Dallas, like to leave?  Score 5 4.5 2  Figure 3: Sample alternate realizations of the set of communicative goals shown in Figure 2 suggested by our sentence planner, with human scores  Dialog System  5 Sentence Planner 5    `  SPG SPR  ... 
For languages without word boundary delimiters, dictionaries are needed for segmenting running texts. This figure makes segmentation accuracy depend significantly on the quality of the dictionary used for analysis. If the dictionary is not sufficiently good, it will lead to a great number of unknown or unrecognized words. These unrecognized words certainly reduce segmentation accuracy. To solve such problem, we propose a method based on decision tree models. Without use of a dictionary, specific information, called syntactic attribute, is applied to identify the structure of Thai words. C4.5 is used as a tool for this purpose. Using a Thai corpus, experiment results show that our method outperforms some well-known dictionary-dependent techniques, maximum and longest matching methods, in case of no dictionary. Keywords Decision trees, Word segmentation without a dictionary 1. INTRODUCTION Word segmentation is a crucial topic in analysis of languages without word boundary markers. Many researchers have been trying to develop and implement in order to gain higher accuracy. Unlike in English, word segmentation in Thai, as well as in many other Asian languages, is more complex because the language does not have any explicit word boundary delimiters, such as a space, to separate between each word. It is even more complicated to precisely segment and identify the word boundary in Thai language because there are several levels and several roles in Thai characters that may lead to ambiguity in segmenting the words. In the past, most researchers had implemented Thai word segmentation systems based on using a dictionary ([2], [3], [4], [6], [7]). When using a dictionary, word segmentation has to cope with an unknown word problem. Up to present, it is clear that  most researches on Thai word segmentation with a dictionary suffer from this problem and then introduce some particular process to handle such problem. In our preliminary experiment, we extracted words from a pre-segmented corpus to form a dictionary, randomly deleted some words from the dictionary and used the modified dictionary in segmentation process based two well-known techniques; Maximum and Longest Matching methods. The result is shown in Figure 1. The percentages of accuracy with different percentages of unknown words are explored. We found out that in case of no unknown words, the accuracy is around 97% in both maximum matching and longest matching but the accuracy drops to 54% and 48% respectively, in case that 50% of words are unknown words. As the percentage of unknown words rises, the percentage of accuracy drops continuously. This result reflects seriousness of unknown word problem in word segmentation. 1  Unknown word (%) 0 5 10 15 20 25 30 35 40 45 50  Accuracy (%)  Maximum Matching Longest Matching  97.24 95.92 93.12 89.99 86.21 78.40 68.07 69.23 61.53 57.33 54.01  97.03 95.63 92.23 87.97 82.60 74.41 64.52 62.21 57.21 54.84 48.67  Figure 1. The accuracy of two dictionary-based systems vs. percentage of unknown words  In this paper, to take care of both known and unknown words, we propose the implementation of a non-dictionary-based system with the knowledge based on the decision tree model ([5]). This model attempts to identify word boundaries of a Thai text. To do  
In this paper, we address the problem of combining several language models (LMs). We ﬁnd that simple interpolation methods, like log-linear and linear interpolation, improve the performance but fall short of the performance of an oracle. The oracle knows the reference word string and selects the word string with the best performance (typically, word or semantic error rate) from a list of word strings, where each word string has been obtained by using a different LM. Actually, the oracle acts like a dynamic combiner with hard decisions using the reference. We provide experimental results that clearly show the need for a dynamic language model combination to improve the performance further. We suggest a method that mimics the behavior of the oracle using a neural network or a decision tree. The method amounts to tagging LMs with conﬁdence measures and picking the best hypothesis corresponding to the LM with the best conﬁdence. 1. INTRODUCTION Statistical language models (LMs) are essential in speech recognition and understanding systems for high word and semantic accuracy, not to mention robustness and portability. Several language models have been proposed and studied during the past two decades [8]. Although it has turned out to be a rather difﬁcult task to beat the (almost) standard class/word n-grams (typically Ò ¾ or ¿), there has been a great deal of interest in grammar based language models [1]. A promising approach for limited domain applications is the use of semantically motivated phrase level stochastic context free grammars (SCFGs) to parse a sentence into a sequence of se- mantic tags which are further modeled using Ò-grams [2, 9, 10, 3]. The main motivation behind the grammar based LMs is the inabil- ity of Ò-grams to model longer-distance constraints in a language. With the advent of fairly fast computers and efﬁcient parsing and search schemes several researchers have focused on incorporating relatively complex language models into speech recognition and understanding systems at different levels. For example, in [3], we £The work is supported by DARPA through SPAWAR under grant #N66001-00-2-8906.  report a signiﬁcant perplexity improvement with a moderate in- crease in word/semantic accuracy, at Æ -best list (rescoring) level, using a dialog-context dependent, semantically motivated grammar based language model. Statistical language modeling is a ”learning from data” problem. The generic steps to be followed for language modeling are ¯ preparation of training data ¯ selection of a model type ¯ speciﬁcation of the model structure ¯ estimation of model parameters The training data should consist of large amounts of text, which is hardly satisﬁed in new applications. In those cases, complex models ﬁt to the training data. On the other hand, simple models can not capture the actual structure. In the Bayes’ (sequence) decision framework of speech recognition/understanding we heavily constrain the model structure to come up with a tractable and prac- tical LM. For instance, in a class/word Ò-gram LM the dependency of a word is often restricted to the class that it belongs and the de- pendency of a class is limited to Ò-1 previous classes. The estima- tion of the model parameters, which are commonly the probabilities, is another important issue in language modeling. Besides data sparseness, the estimation algorithms (e.g. EM algorithm) might be responsible for the estimated probabilities to be far from optimal. The aforementioned problems of learning have different effects on different LM types. Therefore, it is wise to design LMs based on different paradigms and combine them in some optimal sense. The simplest combination method is the so called linear interpolation [4]. Recently, the linear interpolation in the logarithmic domain has been investigated in [6]. Perplexity results on a couple of tasks have shown that the log-linear interpolation is better than the linear interpolation. Theoretically, a far more powerful method for LM combination is the maximum entropy approach [7]. However, it has not been widely used in practice, since it is computationally demanding. In this research, we consider two LMs: ¯ class-based 3-gram LM (baseline). ¯ dialog dependent semantic grammar based 3-gram LM [3]. After N-best list rescoring experiments with linear and log-linear interpolation, we realized that the performance in terms of word and semantic accuracies fall considerably short of the performance of an oracle. We explain the set-up for the oracle experiment and point out that the oracle is a dynamic LM combiner. To ﬁll the performance gap, we suggest a method that can mimic the oracle.  dialog goal  dialog context 
Although there has been regular improvement in speech recognition technology over the past decade, speech recognition is far from being a solved problem. Most recognition systems are tuned to a particular task and porting the system to a new task (or language) still requires substantial investment of time and money, as well as expertise. Todays state-of-the-art systems rely on the availability of large amounts of manually transcribed data for acoustic model training and large normalized text corpora for language model training. Obtaining such data is both time-consuming and expensive, requiring trained human annotators with substantial amounts of supervision. In this paper we address issues in speech recognizer portability and activities aimed at developing generic core speech recognition technology, in order to reduce the manual effort required for system development. Three main axes are pursued: assessing the genericity of wide domain models by evaluating performance under several tasks; investigating techniques for lightly supervised acoustic model training; and exploring transparent methods for adapting generic models to a speciﬁc task so as to achieve a higher degree of genericity. 1. INTRODUCTION The last decade has seen impressive advances in the capability and performance of speech recognizers. Todays state-of-the-art systems are able to transcribe unrestricted continuous speech from broadcast data with acceptable performance. The advances arise from the increased accuracy and complexity of the models, which are closely related to the availability of large spoken and text corpora for training, and the wide availability of faster and cheaper computational means which have enabled the development and implementation of better training and decoding algorithms. Despite the extent of progress over the recent years, recognition accuracy is still extremely sensitive to the environmental conditions and speaking style: channel quality, speaker characteristics, and background This work was partially ﬁnanced by the European Commission under the IST-1999 Human Language Technologies project 11876 Coretex. .  noise have an important impact on the acoustic component of the speech recognizer, whereas the speaking style and the discourse domain have a large impact on the linguistic component. In the context of the EC IST-1999 11876 project CORETEX we are investigating methods for fast system development, as well as development of systems with high genericity and adaptability. By fast system development we refer to: language support, i.e., the capability of porting technology to different languages at a reasonable cost; and task portability, i.e. the capability to easily adapt a technology to a new task by exploiting limited amounts of domainspeciﬁc knowledge. Genericity and adaptability refer to the capacity of the technology to work properly on a wide range of tasks and to dynamically keep models up to date using contemporary data. The more robust the initial generic system is, the less there is a need for adaptation. Concerning the acoustic modeling component, genericity implies that it is robust to the type and bandwidth of the channel, the acoustic environment, the speaker type and the speaking style. Unsupervised normalization and adaptation techniques evidently should be used to enhance performance further when the system is exposed to data of a particular type. With today’s technology, the adaptation of a recognition system to a new task or new language requires the availability of sufﬁcient amount of transcribed training data. When changing to new domains, usually no exact transcriptions of acoustic data are available, and the generation of such transcribed data is an expensive process in terms of manpower and time. On the other hand, there often exist incomplete information such as approximate transcriptions, summaries or at least key words, which can be used to provide supervision in what can be referred to as “informed speech recognition”. Depending on the level of completeness, this information can be used to develop conﬁdence measures with adapted or trigger language models or by approximate alignments to automatic transcriptions. Another approach is to use existing recognizer components (developed for other tasks or languages) to automatically transcribe task-speciﬁc training data. Although in the beginning the error rate on new data is likely to be rather high, this speech data can be used to re-train a recognition system. If carried out in an iterative manner, the speech data base for the new domain can be cumulatively extended over time without direct manual transcription. The overall objective of the work presented here is to reduce the speech recognition development cost. One aspect is to develop “generic” core speech recognition technology, where by “generic” we mean a transcription engine that will work reasonably well on a wide range of speech transcription tasks, ranging from digit recognition to large vocabulary conversational telephony speech, without the need for costly task-speciﬁc training data. To start with we assess the genericity of wide domain models under cross-task con-  Table 1: Brief descriptions and best reported error rates for the corpora used in this work.  Corpus Test Year  Task  Train (#spkr) Test (#spkr) Textual Resources  Best WER  BN  98 TV & Radio News  200h  3h Closed-captions, commercial transcripts, 13.5  manual transcripts of audio data  TI-digits 93 Small Vocabulary 3.5h (112) 4h (113) -  0.2 
This paper describes a system for rapidly retargetable interactive translingual retrieval. Basic functionality can be achieved for a new document language in a single day, and further improvements require only a relatively modest additional investment. We applied the techniques ﬁrst to search Chinese collections using English queries, and have successfully added French, German, and Italian document collections. We achieve this capability through separation of languagedependent and language-independent components and through the application of asymmetric techniques that leverage an extensive English retrieval infrastructure. Keywords Cross-language information retrieval 1. INTRODUCTION Our goal is to produce systems that allow interactive users to present English queries and retrieve documents in languages that they cannot read. In this paper we focus on what we call “rapid retargetability”: extending interactive translingual retrieval functionality for a new document language rapidly with few language-speciﬁcresources. Our current system can be retargeted to a new language in one day with only one language-dependent resource: a bilingual term list.1 Our language-independent architecture consists of two main components: 1. Document translation and indexing 2. Interactive retrieval We describe each of these components, demonstrate their effectiveness for information retrieval tasks, and then conclude by describing our experience with adding French, German and Italian document collections to a system that was originally developed for Chinese. 1For Asian languages we also use a language-speciﬁc segmentation system. .  2. DOCUMENT TRANSLATION AND INDEXING We have adopted a document translation architecture for two reasons. First, we support a single query language (English) but multiple document languages, so indexing English terms simpliﬁes query processing (where interactive response time can be a concern). Second, a document translation architecture simpliﬁes the display of translated documents by decoupling the translation and display processes. Gigabyte collections require machine translation that is orders of magnitude faster than present commercial systems. We accomplish this using term-by-term translation, in which the basic data structure is a simple hash table lookup. Any translation requires some source of translation knowledge—we use a bilingual term list containing English translation(s) for each foreign language term. We typically construct these term lists by harvesting Internet-available translation resources, so the foreign language terms for which translations are known are typically an eclectic mix of root and inﬂected forms. We accommodate this limitation using a four-stage backoff statistical stemming approach to enhance translation coverage. 2.1 Preprocessing. Differences in use of diacritic-s, case, and punctuation can inhibit matching between term list entries and document terms, so normalization is important. In order to maximize the probability of matching document words with term list entries, we normalize the bilingual term list and the documents by: converting characters in Western languages to lowercase, removing all accents and diacritics, and segmentation, which for Western languages merely involves separating punctuation from other text by the addition of white space. Our preprocessing also includes conversion of the bilingual term list and the document collection into standard formats. The preprocessing typically requires about half a day of programmer time. 2.2 Four-Stage Backoff Translation. Bilingual term lists found on the Web often contain an eclectic mix of root forms and morphological variants. We thus developed a four-stage backoff strategy to maximize coverage while limiting spurious translations: 1. Match the surface form of a document term to surface forms of source language terms in the bilingual term list. 2. Match the stem of a document term to surface forms of source language terms in the bilingual term list.  3. Match the surface form of a document term to stems of source language terms in the bilingual term list.  4. Match the stem of a document term to stems of source language terms in the bilingual term list.  The process terminates as soon as a match is found at any stage, and the known translations for that match are generated. Although this may produce an inappropriate morphological variant for a correct English translation, use of English stemming at indexing time minimizes the effect of that factor on retrieval effectiveness. Because we are ultimately interested in processing documents in any language, we may not have a hand-crafted stemmer available for the document language. We have thus explored the application of rule induction to learn stemming rules in an unsupervised fashion from the collection that is being indexed [2].  2.3 Balanced Top-2 Translation.  We produce exactly two English terms for each foreign-language  term. For terms with no known translation, the untranslated term is  generated twice (often appropriate for proper names in the Latin-  
 Aligned stories  Sentence retrieval  Speech Sources  Speech Recognition  Topic specific acoustic and language models  Query Ranked Answers  Basic Models: acoustic lexical language  Figure 1: Information Flow in Alignment and Extraction  We propose to align collections of stories, much like the example above, from multiple text and speech sources and then develop methods that exploit the resulting parallelism both as a tool to improve recognition accuracy and to enable the development of systems that can reliably extract information from parallel sources. Our goal is to develop systems that align text sources and recognize parallel speech streams simultaneously in several languages by making use of all related text and speech. The initial systems we intend to develop will process each language independently. However, our ultimate and most ambitious objective is to align text sources and recognize speech using a single, integrated multilingual ASR system. Of course, if sufﬁciently accurate automatic machine translation (MT) techniques ([1]) were available, we could address multilingual processing and single language systems in the same way. However MT techniques are not yet reliable enough that we expect all words and phrases recognized within languages to contribute to recognition across languages. We intend to develop methods that identify the particular words and phrases that both can be translated reliably and also used to improve story recognition. As MT technology improves it can be incorporated more extensively within the processing paradigm we propose. We consider this proposal a framework within which successful MT techniques can eventually be used for multilingual acoustic processing. 2. PROJECT OBJECTIVES The ﬁrst objective is to enhance multi-lingual information systems by exploiting the processing capabilities for resource-rich languages to enhance the capabilities for resource-impoverished language. The second objective is to advance information retrieval and knowledge information systems by providing them with considerably improved multi-lingual speech recognition capabilities. Our research plan proceeds in several steps to (i) collect and (ii) align multi-lingual parallel speech and text sources, (iii) exploit parallelism for improving ASR within a language, and to (iv) exploit  parallelism for improving ASR across languages. The main information ﬂows involved in aligning and exploiting parallel sources are illustrated in Figure 1. We will initially focus on German, English and Czech language sources. This section summarizes the major components of our project. 2.1 Parallel Speech and Text Sources The monolingual speech and text collections that we will use to develop techniques to exploit parallelism for improving ASR within a language are readily available. For instance, the North American News Text corpus of parallel news streams from 16 US newspapers and newswire is available from LDC. A 3-year period yields over 350 million words of multi-source news text. In addition to data developed within the TIDES and other HLT programs, we are in the process of identifying and creating our own multilingual parallel speech and text sources. FBIS TIDES Multilingual Newstext Collection For the purposes of developing multilingual alignment techniques, we intend to use the 240 day, contemporaneous, multilingual news text collection made available for use to TIDES projects by FBIS. This corpus contains news in our initial target languages of English, German, and Czech. The collections are highly parallel, in that much of the stories are direct translations. Radio Prague Multilingual Speech and Text Corpus Speech and news text from Radio Prague was collected under the direction of J. Psutka with the consent of Radio Prague. The collection contains speech and text in 5 languages: Czech, English, German, French, and Spanish. The collection began June 1, 2000 and continued for approximately 3 months. The text collection contains the news scripts used for the broadcast; the broadcasts more or less follow the scripts. The speech is about 3 minutes per day in each language, which should yield a total of about 5 hours of speech per language. Our initial analysis of the Radio Prague corpus suggest that only approximately 5% of the stories coincide in topic, and that there is little, if any, direct translation of stories. We anticipate that this sparseness will make this corpus signiﬁcantly hard to analyze than another, highly-parallel corpus. However, we expect this is the sort of difﬁculty that will likely be encountered in processing ‘realworld’ multilingual news sources. 2.2 Story-level Alignment Once we have the multiple streams of information we must be able to align them according to story. A story is the description of one or more events that happened in a single day and that are reported in a single article by a daily news source the next day. We expect that we will use the same techniques used in the Topic Detection (TDT) ﬁeld ([5]). Independently of the speciﬁc details of the alignment procedure, there is now substantial evidence that related stories from parallel streams can be identiﬁed using standard statistical Information Retrieval (IR) techniques. Sentence Alignment As part of the infrastructure needed to incorporate cross-lingual information into language models, we are employing statistical MT systems to generate English/German and English/Czech alignments of sentences in the FBIS Newstext Collection. For the English/German sentence and single-word based alignments, we plan to use statistical models ([4]) [3] which generate both sentence and word alignments. For English/Czech sentence alignment, we will employ the statistical models trained as part of the Czech-English MT system developed during the 1999 Johns Hopkins Summer Workshop ([2]).  2.3 Multi-Source Automatic Speech Recognition The scenario we propose is extraction of information from parallel text followed by repeated recognition of parallel broadcasts, resulting in a gradual lowering the WER. The ﬁrst pass is performed in order to ﬁnd the likely topics discussed in the story and to identify the topics relevant to the query. In this process, the acoustic model will be improved by deriving pronunciation speciﬁcations for out-of-vocabulary words and ﬁxed phrases extracted from the parallel stories. The language model will be improved by extending the coverage of the underlying word and phrase vocabulary, and by specializing the model’s statistics to the narrow topic at hand. As long as a round of recognition yields new information, the corresponding improvement is incorporated into the recognizer modules and bootstrapping of the system continues. 
This paper gives an overview of our work on statistical machine translation of spoken dialogues, in particular in the framework of the Verbmobil project. The goal of the Verbmobil project is the translation of spoken dialogues in the domains of appointment scheduling and travel planning. Starting with the Bayes decision rule as in speech recognition, we show how the required probability distributions can be structured into three parts: the language model, the alignment model and the lexicon model. We describe the components of the system and report results on the Verbmobil task. The experience obtained in the Verbmobil project, in particular a large-scale end-to-end evaluation, showed that the statistical approach resulted in signiﬁcantly lower error rates than three competing translation approaches: the sentence error rate was 29% in comparison with 52% to 62% for the other translation approaches. 1. INTRODUCTION In comparison with written language, speech and especially spontaneous speech poses additional diﬃculties for the task of automatic translation. Typically, these diﬃculties are caused by errors of the recognition process, which is carried out before the translation process. As a result, the sentence to be translated is not necessarily well-formed from a syntactic point-of-view. Even without recognition errors, speech translation has to cope with a lack of conventional syntactic structures because the structures of spontaneous speech diﬀer from that of written language. The statistical approach shows the potential to tackle these problems for the following reasons. First, the statistical approach is able to avoid hard decisions at any level of the translation process. Second, for any source sentence, a translated sentence in the target language is guaranteed to be generated. In most cases, this will be hopefully a syntactically perfect sentence in the target language; but even if this is not the case, in most cases, the translated sentence will convey the meaning of the spoken sentence. .  Whereas statistical modelling is widely used in speech recognition, there are so far only a few research groups that apply statistical modelling to language translation. The presentation here is based on work carried out in the framework of the EuTrans project [8] and the Verbmobil project [25]. 2. STATISTICAL DECISION THEORY AND LINGUISTICS 2.1 The Statistical Approach The use of statistics in computational linguistics has been extremely controversial for more than three decades. The controversy is very well summarized by the statement of Chomsky in 1969 [6]: “It must be recognized that the notion of a ‘probability of a sentence’ is an entirely useless one, under any interpretation of this term”. This statement was considered to be true by the majority of experts from artiﬁcial intelligence and computational linguistics, and the concept of statistics was banned from computational linguistics for many years. What is overlooked in this statement is the fact that, in an automatic system for speech recognition or text translation, we are faced with the problem of taking decisions. It is exactly here where statistical decision theory comes in. In speech recognition, the success of the statistical approach is based on the equation: Speech Recognition = Acoustic–Linguistic Modelling + Statistical Decision Theory Similarly, for machine translation, the statistical approach is expressed by the equation: Machine Translation = Linguistic Modelling + Statistical Decision Theory For the ‘low-level’ description of speech and image signals, it is widely accepted that the statistical framework allows an eﬃcient coupling between the observations and the models, which is often described by the buzz word ‘subsymbolic processing’. But there is another advantage in using probability distributions in that they oﬀer an explicit formalism for expressing and combining hypothesis scores: • The probabilities are directly used as scores: These scores are normalized, which is a desirable property: when increasing the score for a certain element in the  set of all hypotheses, there must be one or several other elements whose scores are reduced at the same time.  • It is straightforward to combine scores: depending on the task, the probabilities are either multiplied or added.  • Weak and vague dependencies can be modelled easily. Especially in spoken and written natural language, there are nuances and shades that require ‘grey levels’ between 0 and 1.  2.2 Bayes Decision Rule and System Architecture In machine translation, the goal is the translation of a text given in a source language into a target language. We are given a source string f1J = f1...fj...fJ , which is to be translated into a target string eI1 = e1...ei...eI . In this article, the term word always refers to a full-form word. Among all possible target strings, we will choose the string with the highest probability which is given by Bayes decision rule [5]:  eˆI1  =  arg  max eI1  {P  r(eI1 |f1J  )}  =  arg  
This paper describes the scalability and portability of a Belief Network (BN)-based mixed initiative dialog model across application domains. The Belief Networks (BNs) are used to automatically govern the transitions between a system-initiative and a user-initiative dialog model, in order to produce mixedinitiative interactions. We have migrated our dialog model from a simpler domain of foreign exchange to a more complex domain of air travel information service. The adapted processes include: (i) automatic selection of specified concepts in the user’s query, for the purpose of informational goal inference; (ii) automatic detection of missing / spurious concepts based on backward inference using the BN. We have also enhanced our dialog model with the capability of discourse context inheritance. To ease portability across domains, which often implies the lack of training data for the new domain, we have developed a set of principles for hand-assigning BN probabilities, based on the “degree of belief” in the relationships between concepts and goals. Application of our model to the ATIS data gave promising results. 1. INTRODUCTION Spoken dialog systems demonstrate a high degree of usability in many restricted domains, and dialog modeling in such systems plays an important role in assisting users to achieve their goals. The system-initiative dialog model assumes complete control in guiding the user through an interaction towards task completion. This model often attains high task completion rates, but the user is bound by many constraints throughout the interaction. Conversely, the user-initiative model offers maximum flexibility to the user in determining the preferred course of interaction. However this model often has lower task completion rates relative to the system-initiative model, especially when the user’s request falls beyond the system's competence level. To strike a balance between these two models, the mixed-initiative dialog model allows both the user and the system to influence the course of interaction. It is possible to handcraft a sophisticated mixed-initiative dialog flow, but the task is expensive, and may become intractable for complex application domains.  We strive to reduce handcrafting in the design of mixedinitiative dialogs. We propose to use Belief Networks (BN) to automatically govern the transitions between a system-initiative and a user-initiative dialog model, in order to produce mixedinitiative interactions. Previous work includes the use of semantic interpretation rules for natural language understanding, where the rules are learnt by decision trees known as Semantic Classification Trees (SCTs) [6]. Moreover, there is also previous effort that explores the use of machine learning techniques to automatically determine the optimal dialog strategy. A dialog system can be described as a sequential decision process that has states and actions. An optimal strategy can be obtained by reinforcement learning [7, 8]. While the system is interacting with users, it can explore the state space and thus learn different actions. Our BN framework was previously used for natural language understanding [1,2]. We have extended this model for dialog modeling, and demonstrated feasibility in the CU FOREX (foreign exchange) [3,4] system, whose domain has low complexity. This work explores the scalability and portability of our BN-based dialog model to a more complex application. We have chosen the ATIS (Air Travel Information Service) domain due to data availability.1 2. BELIEF NETWORKS FOR MIXEDINITIATIVE DIALOG MODELING − THE CU FOREX DOAMIN We have devised an approach that utilizes BNs for mixedinitiative dialog modeling, and demonstrated its feasibility in the CU FOREX domain. Details can be found in [4]. We provide a brief description here for the sake of continuity. CU FOREX is a bilingual (English and Cantonese) conversational hotline that supports inquiries regarding foreign exchange. The domain is relatively simple, and can be characterized by two query types (or informational goals – Exchange Rate or Interest Rate); and five domain-specific concepts (a CURRENCY pair, TIME DURATION, EXCHANGE RATE and INTEREST RATE). Our approach involves two processes: 2.1 Informational Goal Inference A BN is trained for each informational goal. Each BN receives as input the concepts that are related to its corresponding goal. In CU FOREX, there are two BNs, each with five input concepts. The pre-defined BN topology shown in Figure 1 (without dotted arrow) incorporates the simplifying assumption that all concepts are dependent only on the goal, but are independent of one another. This topology can be enhanced by 
This paper describes SCANMail, a system that allows users to browse and search their voicemail messages by content through a GUI. Content based navigation is realized by use of automatic speech recognition, information retrieval, information extraction and human computer interaction technology. In addition to the browsing and querying functionalities, acoustics-based caller ID technology is used to proposes caller names from existing caller acoustic models trained from user feedback. The GUI browser also provides a note-taking capability. Comparing SCANMail to a regular voicemail interface in a user study, SCANMail performed better both in terms of objective (time to and quality of solutions) as well as subjective objectives. 1. INTRODUCTION Increasing amounts of public, corporate, and private audio present a major challenge to speech, information retrieval, and humancomputer interaction research: how can we help people to take advantage of these resources when current techniques for navigating them fall far short of text-based search methods? In this paper, we describe SCANMail, a system that employs automatic speech recognition (ASR), information retrieval (IR), information extraction (IE), and human computer interaction (HCI) technology to permit users to browse and search their voicemail messages by content through a GUI interface. A CallerId server also proposes caller names from existing caller acoustic models and is trained from user feedback. An Email server sends the original message plus its ASR transcription to a mailing address speciﬁed in the user’s proﬁle. The SCANMail GUI also provides note-taking capabilities as well as browsing and querying features. Access to messages and information about them is presented to the user via a Java applet running under Netscape. Figure 1 shows the SCANMail GUI. .  2. SYSTEM DESCRIPTION In SCANMail, messages are ﬁrst retrieved from a voicemail server, then processed by the ASR server that provides a transcription. The message audio and/or transcription are then passed to the IE, IR, Email, and CallerId servers. The acoustic and language model of the recognizer, and the IE and IR servers are trained on 60 hours of a 100 hour voicemail corpus, transcribed and hand labeled for telephone numbers, caller names, times, dates, greetings and closings. The corpus includes approximately 10,000 messages from approximately 2500 speakers. About 90% of the messages were recorded from regular handsets, the rest from cellular and speaker-phones. The corpus is approximately gender balanced and approximately 12% of the messages were from non-native speakers. The mean duration of the messages was 36.4 seconds; the median was 30.0 seconds. 2.1 Automatic Speech Recognition 
We describe the treatment of questions (Question-Answer Typology, question parsing, and results) in the Weblcopedia question answering system. 1. INTRODUCTION Several research projects have recently investigated the problem of automatically answering simple questions that have brief phrasal answers (‘factoids’), by identifying and extracting the answer from a large collection of text. The systems built in these projects exhibit a fairly standard structure: they create a query from the user’s question, perform IR with the query to locate (segments of) documents likely t o contain an answer, and then pinpoint the most likely answer passage within the candidate documents. The most common difference lies in the pinpointing. Many projects employ a window-based word scoring method that rewards desirable words in the window. They move the window across the candidate answers texts/segments and return the window at the position giving the highest total score. A word is desirable if it is a content word and it is either contained in the question, or is a variant of a word contained in the question, or if it matches the words of the expected answer. Many variations of this method are possible—of the scores, of the treatment of multiword phrases and gaps between desirable words, of the range of variations allowed, and of the computation of the expected answer words. Although it works to some degree (giving results of up to 30% in independent evaluations), the window-based method has several quite serious limitations: • it cannot pinpoint answer boundaries precisely (e.g., an exact name or noun phrase),  • it relies solely on information at the word level, and hence cannot recognize information of the desired type (such as Person or Location), • it cannot locate and compose parts of answers that are distributed over areas wider than the window. Window-based pinpointing is therefore not satisfactory in the long run, even for factoid QA. In this paper we describe work in our Webclopedia project on semantics-based answer pinpointing. Initially, though, recognizing the simplicity and power of the window-based technique for getting started, we implemented a version of it as a fallback method. We then implemented two more sophisticated methods: syntacticsemantic question analysis and QA pattern matching. This involves classification of QA types to facilitate recognition of desired answer types, a robust syntactic-semantic parser t o analyze the question and candidate answers, and a matcher that combines word- and parse-tree-level information to identify answer passages more precisely. We expect that the two methods will really show their power when more complex nonfactoid answers are sought. In this paper we describe how well the three methods did relative to each other. Section 2 outlines the Webclopedia system. Sections 3, 4, and 5 describe the semantics-based components: a QA Typology, question and answer parsing, and matching. Finally, we outline current work on automatically learning QA patterns using the Noisy Channel Model. 2. WEBCLOPEDIA Webclopedia’s architecture (Figure 1) follows the pattern outlined above: Question parsing: Using BBN’s IdentiFinder [1], our parser CONTEX (Section 4) produces a syntactic-semantic analysis of the question and determines the QA type (Section 3). Query formation: Single- and multi-word units (content words) are extracted from the analysis, and WordNet synsets are used for query expansion. A Boolean query is formed. See [9]. IR: The IR engine MG [12] returns the top-ranked 1000 documents.  IR • Steps: create query from question (WordNet-expand) retrieve top 1000 documents • Engines: MG (Sydney)—(Lin) AT&T (TREC)—(Lin)  Input question Parse question  Segmentation • Steps:segment each document into topical segments • Engines: fixed-length (not used) TexTiling (Hearst 94)—(Lin) C99 (Choi 00)—(Lin) MAXNET (Lin 00, not used) Ranking • Steps: score each sentence in each segment, using WordNet expansion rank segments • Engines: FastFinder (Junk)  Create query Retrieve documents Segment documents Rank segments Parse top segments  Question parsing • Steps: parse question find desired semantic type • Engines: IdentiFinder (BBN) CONTEX (Hermjakob) Segment Parsing • Steps: parse segment sentences • Engines: CONTEX (Hermjakob)  Matching  Match segments against question  • Steps: match general constraint patterns against parse trees match desired semantic type against parse tree elements  Rank and prepare answers  match desired words against words in sentences • Engines: matcher (Junk)  QA typology  • Categorize QA types in taxonomy (Gerber)  Ranking and answer extraction • Steps: rank candidate answers  Output answers  Constraint patterns  extract and format them • Engines: part of matcher (Junk)  • Identify likely answers in relation to other parts of the sentence (Gerber)  Figure 1. Webclopedia architecture.  S e g m e n t a t i o n : To decrease the amount of text to be processed, the documents are broken into semantically coherent segments. Two text segmenter—TexTiling [5] and C99 [2]—were tried; the first is used; see [9]. Ranking s e g m e n t s : For each segment, each sentence i s scored using a formula that rewards word and phrase overlap with the question and its expanded query words. Segments are ranked. See [9] Parsing s e g m e n t s : CONTEX parses each sentence of the top-ranked 100 segments (Section 4). Pinpointing: For each sentence, three steps of matching are performed (Section 5); two compare the analyses of the question and the sentence; the third uses the window method t o compute a goodness score. Ranking of answers: The candidate answers’ scores are compared and the winner(s) are output. 3. THE QA TYPOLOGY In order to perform pinpointing deeper than the word level, the system has to produce a representation of what the user i s asking. Some previous work in automated question answering has categorized questions by question word or by a mixture of question word and the semantic class of the answer [11, 10]. To ensure full coverage of all forms of simple question and answer, and to be able to factor in deviations and special requirements, we are developing a QA Typology.  We motivate the Typology (a taxonomy of QA types) as follows. There are many ways to ask the same thing: What is the age o f the Queen of Holland? How old is the Netherlands’ queen? How long has the ruler of Holland been alive? Likewise, there are many ways of delivering the same answer: about 60; 63 years old; since January 1938. Such variations form a sort of semantic equivalence class of both questions and answers. Since the user may employ any version of his or her question, and the source documents may contain any version(s) of the answer, an efficient system should group together equivalent question types and answer types. Any specific question can then be indexed into its type, from which all equivalent forms of the answer can be ascertained. These QA equivalence types can help with both query expansion and answer pinpointing. However, the equivalence is fuzzy; even slight variations introduce exceptions: who invented the gas laser? can be answered by both Ali Javan and a scientist at MIT, while what is the name of the person who invented the gas laser? requires the former only. This inexactness suggests that the QA types be organized in an inheritance hierarchy, allowing the answer requirements satisfying more general questions to be overridden by more specific ones ‘lower down’. These considerations help structure the Webclopedia QA Typology. Instead of focusing on question word or semantic type of the answer, our classes attempt to represent the user’s intention, including for example the classes Why-Famous (for Who was Christopher Columbus? but not Who discovered  America?, which is the QA type Proper-Person) and Abbreviation-Expansion (for What does HLT stand for?). In addition, the QA Typology becomes increasingly specific as one moves from the root downward.  To create the QA Typology, we analyzed 17,384 questions and their answers (downloaded from answers.com); see (Gerber, i n prep.). The Typology (Figure 2) contains 72 nodes, whose leaf nodes capture QA variations that can in many cases be further differentiated.  ERACITY NTIT Y NARRATIVE  YES :NO  TRUE :FALSE  A GENT  NAME  LAST-NAME  FIRST-NAME  ORGANIZATION  GROUP-OF-PEOPLE  A NIMAL  PERSON  OCCUPATION-PERSON  GEOGRAPHICA L-PERSON  PROPER-NAMED-ENTITY  PROPER-PERSON  PROPER-ORGANIZATION  PROPER-PLACE CITY  COUNTRY  STATE-DISTRICT  QUANTITY  NU MERICAL-QUANTITY  MONETARY- QUANTITY  TEMPORAL-QUANTITY  MASS-QUANTI TY  SPATIAL-QUANTITY  DISTANCE-QUANTITY  AREA-QUANTITY  VOLUME-QUANTI TY  TEMP-LOC  DATE  LOCATOR  DATE-RANGE ADDRESS EMAIL-ADDRESS PHONE-NUMBER  URL TANGIBLE-OBJECT TITLED-WORK ABSTRACT  HU MAN-FOOD SUBS TANCE BODY-PART I NSTRUMENT GARMENT SHAPE  LIQUID  ADJECTIVE  COLOR  DISEASE  TEXT  GENERAL-INFO DEFINITION  USE  EXPRESSION-ORIGIN  HISTORY  WHY-FAMOUS BIO  ANTECE DENT  INFLUENCE CAUSE-EFFECT METHOD-MEANS CIRCUMSTANCE-MEANS EVALUATION PRO-CON CONTRAST RATING COUNSEL-ADVICE  CONSEQUENT REASON  Figure 2. Portion of Webclopedia QA Typology.  Each Typology node has been annotated with examples and typical patterns of expression of both Question and Answer, using a simple template notation that expressed configurations of words and parse tree annotations (Figure 3). Question pattern information (specifically, the semantic type of the answer required, which we call a Qtarget) is produced by the CONTEX parser (Section 4) when analyzing the question, enabling it to output its guess(s) for the QA type. Answer  pattern information is used by the Matcher (Section 5) t o pinpoint likely answer(s) in the parse trees of candidate answer sentences. Question examples and question templates Who was Johnny Mathis' high school track coach? Who was Lincoln's Secretary of State? who be <entity>'s <role> Who was President of Turkmenistan in 1994? Who is the composer of Eugene Onegin? Who is the chairman of GE? who be <role> of <entity> Answer templates and actual answers <person>, <role> of <entity> Lou Vasquez, track coach of…and Johnny Mathis <person> <role-title*> of <entity> Signed Saparmurad Turkmenbachy [Niyazov], president of Turkmenistan <entity>’s <role> <person> ...Turkmenistan’s President Saparmurad Niyazov <person>'s <entity> ...in Tchaikovsky's Eugene Onegin... <role-title> <person> ... <entity> <role> Mr. Jack Welch, GE chairman... <subject>|<psv object> of related role-verb ...Chairman John Welch said ...GE's Figure 3. Some QA Typology node annotations for Proper-Person. At the time of the TREC-9 Q&A evaluation, we had produced approx. 500 patterns by simply cross-combining approx. 2 0 Question patterns with approx. 25 Answer patterns. To our disappointment (Section 6), these patterns were both too specific and too few to identify answers frequently—when they applied, they were quite accurate, but they applied too seldom. We therefore started work on automatically learning QA patterns in parse trees (Section 7). On the other hand, the semantic class of the answer (the Qtarget) is used to good effect (Sections 4 and 6). 4. PARSING CONTEX is a deterministic machine-learning based grammar learner/parser that was originally built for MT [6]. For English, parses of unseen sentences measured 87.6% labeled precision and 88.4% labeled recall, trained on 2048 sentences from the Penn Treebank. Over the past few years it has been extended to Japanese and Korean [7]. 4.1 Parsing Questions Accuracy is particularly important for question parsing, because for only one question there may be several answers in a large document collection. In particular, it is important t o identify as specific a Qtarget as possible. But grammar rules  for declarative sentences do not apply well to questions, which although typically shorter than declaratives, exhibit markedly different word order, preposition stranding (“What university was Woodrow Wilson President of?”), etc. Unfortunately for CONTEX, questions to train on were not initially easily available; the Wall Street Journal sentences contain a few questions, often from quotes, but not enough and not representative enough to result in an acceptable level of question parse accuracy. By collecting and treebanking, however, we increased the number of questions in the training data from 250 (for our TREC-9 evaluation version of Webclopedia) to 400 on Oct 16 to 975 on Dec 9. The effect i s shown in Table 1. In the first test run (“[trained] without [additional questions]”), CONTEX was trained mostly o n declarative sentences (2000 Wall Street Journal sentences, namely the enriched Penn Treebank, plus a few other nonquestion sentences such as imperatives and short phrases). In later runs (“[trained] with [add. questions]”), the system was trained on the same examples plus a subset of the 1153 questions we have treebanked at ISI (38 questions from the preTREC-8 test set, all 200 from TREC-8 and 693 TREC-9, and 222 others). The TREC-8 and TREC-9 questions were divided into 5 subsets, used in a five-fold cross validation test in which the system was trained on all but the test questions, and then evaluated on the test questions. Reasons for the improvement include (1) significantly more training data; (2) a few additional features, some more treebank cleaning, a bit more background knowledge etc.; and (3) the 251 test questions on Oct. 16 were probably a little bit harder on average, because a few of the TREC-9 questions initially treebanked (and included in the October figures) were selected for early treebanking because they represented particular challenges, hurting subsequent Qtarget processing. 4.2 Parsing Potential Answers The semantic type ontology in CONTEX was extended t o include 115 Qtarget types, plus some combined types; more details in [8]. Beside the Qtargets that refer to concepts i n CONTEX’s concept ontology (see first example below), Qtargets can also refer to part of speech labels (first example), to constituent roles or slots of parse trees (second and third examples), and to more abstract nodes in the QA Typology (later examples). For questions with the Qtargets Q-WHYFAMOUS, Q-WHY-FAMOUS-PERSON, Q-SYNONYM, and others, the parser also provides Qargs—information helpful for matching (final examples).  Semantic ontology types (I-EN-CITY) and part of speech labels (S-PROPER-NAME): What is the capital of Uganda? QTARGET: (((I-EN-CITY S-PROPER-NAME)) ((EQ I-EN-PROPER-PLACE))) Parse tree roles: Why can't ostriches fly? QTARGET: (((ROLE REASON))) Name a film in which Jude Law acted. QTARGET: (((SLOT TITLE-P TRUE))) QA Typology nodes: What are the Black Hills known for? Q-WHY-FAMOUS What is Occam's Razor? Q-DEFINITION What is another name for nearsightedness? Q-SYNONYM Should you exercise when you're sick? Q-YES-NO-QUESTION Qargs for additional information: Who was Betsy Ross? QTARGET: (((Q-WHY-FAMOUS-PERSON))) QARGS: (("Betsy Ross")) How is "Pacific Bell" abbreviated? QTARGET: (((Q-ABBREVIATION))) QARGS: (("Pacific Bell")) What are geckos? QTARGET: (((Q-DEFINITION))) QARGS: (("geckos" "gecko") ("animal")) These Qtargets are determined during parsing using 276 handwritten rules. Still, for approx. 10% of the TREC-8&9 questions there is no easily determinable Qtarget (“What does the Peugeot company manufacture?”; “What is caliente i n English?”). Strategies for dealing with this are under investigation. More details appear in (Hermjakob, 2001). The current accuracy of the parser on questions and resulting Qtargets sentences is shown in Table 2. 5. ANSWER MATCHING The Matcher performs three independent matches, in order: • match QA patterns in the parse tree, • match Qtargets and Qwords in the parse tree, • match over the answer text using a word window. Details appear in [9].  Table 1. Improvement in parsing of questions.  Without, Oct 16 With, Oct 16 With, Dec 9  Precision 90.74%  Recall 9 0.72%  Lab eled Precision 8 4.62%  L abeled Recall 83.48%  T agging Accuracy 9 4.95%  94.19% 97.33%  9 4.86% 9 7.13%  9 1.63% 9 5.40%  91.91% 95.13%  9 8.00% 9 8.64%  Table 1. Improvement in parsing of questions.  Cro ssing Brackets 0.6 0.48 0.19  Table 2. Question parse tree and Qtarget accuracies.  # Penn T reeb ank sen ten ces 2 000 3 000 2 000 3 000 2 000  # Question sentences added 0 0 38 38 975  Lab eled Precision 83.47% 84.74% 91.20% 91.52% 95.71%  Labele d Recall 8 2.49% 8 4.16% 8 9.37% 9 0.09% 9 5.45%  Tag ging Accuracy 9 4.65% 9 4.51% 9 7.63% 9 7.29% 9 8.83%  Crossing b ra ckets (/ sent) 0 .34 0 .35 0 .26 0 .26 0 .17  Q target accuracy (strict) 63.00% 65.30% 85.90% 86.40% 96.10%  Q target accuracy (len ient) 6 5.50% 6 7.40% 8 7.20% 8 7.80% 9 7.30%  D ate 2 -Ju l 8 -Ju l 1 3-Jul 3-A ug  Table 3. Relative performance of Webclopedia modules on training corpus.  Numb er  IR  Qs  hits  52  1.00  38  0.89  52  1.00  55  n /a  Ranker hit s 0 .61 0 .40 0 .61 n/a  QA pattern 0.12 0.28 0.04 0.04  Qtgt m atch 0.49 0.40 0.48 0.32  Qword fallback 0 .15 0 .12 0 .15 0 .15  Window fallback 0 .19 n/a 0 .22 0 .19  Total 0 .62 0 .53 0 .53 0 .41  6. RESULTS We entered the TREC-9 short form QA track, and received an overall Mean Reciprocal Rank score of 0.318, which put Webclopedia in essentially tied second place with two others. (The best system far outperformed those in second place.) In order to determine the relative performance of the modules, we counted how many correct answers their output contained, working on our training corpus. Table 3 shows the evolution of the system over a sample one-month period, reflecting the amount of work put into different modules. The modules QA pattern, Qtarget, Qword, and Window were all run in parallel from the same Ranker output. The same pattern, albeit with lower scores, occurred in the TREC test (Table 4). The QA patterns made only a small contribution, the Qtarget made by far the largest contribution, and, interestingly, the word-level window match lay somewhere in between.  Table 4. TREC-9 test: correct answers attributable to each module.  IR hits QA pattern Qtarget Window Total  78.1  5.5  26.2  10.4  30.3  We are pleased with the performance of the Qtarget match. This shows that CONTEX is able to identify to some degree the semantic type of the desired answer, and able to pinpoint these types also in candidate answers. The fact that it outperforms the window match indicates the desirability of looking deeper than the surface level. As discussed in Section 4, we are strengthening the parser’s ability to identify Qtargets.  We are disappointed in the performance of the 500 QA patterns. Analysis suggests that we had too few patterns, and the ones we  had were too specific. When patterns matched, they were rather accurate, both in finding correct answers and more precisely pinpointing the boundaries of answers. However, they were too sensitive to variations in phrasing. Furthermore, it was difficult to construct robust and accurate question and answer phraseology patterns manually, for several reasons. First, manual construction relies on the inventiveness of the pattern builder to foresee variations of phrasing, for both question and answer. It is however nearly impossible to think of all possible variations when building patterns. Second, it is not always clear at what level of representation t o formulate the pattern: when should one specify using words? Parts of speech? Other parse tree nodes? Semantic classes? The patterns in Figure 3 include only a few of these alternatives. Specifying the wrong elements can result in non-optimal coverage. Third, the work is simply tedious. We therefore decided to try to learn QA patterns automatically. 7. TOWARD LEARNING QA PATTERNS AUTOMATICALLY To learn corresponding question and answer expressions, we pair up the parse trees of a question and (each one of) its answer(s). We then apply a set of matching criteria to identify potential corresponding portions of the trees. We then use the EM algorithm to learn the strengths of correspondence combinations at various levels of representation. This work i s still in progress. In order to learn this information we observe the truism that there are many more answers than questions. This holds for the two QA corpora we have access to—TREC and an FAQ website (since discontinued). We therefore use the familiar version of the Noisy Channel Model and Bayes’ Rule. For each basic QA type (Location, Why-Famous, etc.):  P(A|Q) = argmax P(Q|A) . P(A) Σ P(A) = all trees (# nodes that may express a true A) / (number of nodes in tree) Σ P(Q|A) = all QA tree pairs (number of covarying nodes in Q and A trees) / (number of nodes in A tree) As usual, many variations are possible, including how t o determine likelihood of expressing a true answer; whether t o consider all nodes or just certain major syntactic ones (N, NP, VP, etc.); which information within each node to consider (syntactic? semantic? lexical?); how to define ‘covarying information’—node identity? individual slot value equality?; what to do about the actual answer node in the A trees; if (and how) to represent the relationships among A nodes that have been found to be important; etc. Figure 4 provides an answer parse tree that indicates likely Location nodes, determined b y appropriate syntactic class, semantic type, and syntactic role in the sentence. Our initial model focuses on bags of corresponding QA parse tree nodes, and will help to indicate for a given question what type of node(s) will contain the answer. We plan to extend this model to capture structured configurations of nodes that, when matched to a question, will help indicate where in the parse tree of a potential answer sentence the answer actually lies. Such bags or structures of nodes correspond, at the surface level, t o important phrases or words. However, by using CONTEX output we abstract away from the surface level, and learn t o include whatever syntactic and/or semantic information is best suited for predicting likely answers. 8. REFERENCES [1] Bikel, D., R. Schwartz, and R. Weischedel. 1999. An Algorithm that Learns What s in a Name. Machine Learning Special Issue on NL Learning, 34, 1—3. [2] Choi, F.Y.Y. 2000. Advances in independent linear text segmentation. Proceedings of the 1st Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-00), 26—33.  [3] Fellbaum, Ch. (ed). 1998. WordNet: An Electronic Lexical Database. Cambridge: MIT Press. [4] Gerber, L. 2001. A QA Typology for Webclopedia. In prep. [5] Hearst, M.A. 1994. Multi-Paragraph Segmentation of Expository Text. Proceedings of the Annual Conference of the Association for Computational Linguistics (ACL94). [6] Hermjakob, U. 1997. Learning Parse and Translation Decisions from Examples with Rich Context. Ph.D. dissertation, University of Texas at Austin. file://ftp.cs.utexas.edu/pub/ mooney/papers/hermjakobdissertation-97.ps.gz. [7] Hermjakob, U. 2000. Rapid Parser Development: A Machine Learning Approach for Korean. Proceedings of the 1st Conference of the North American Chapter of the Association for Computational Linguistics (ANLPNAACL-2000). http://www.isi.edu/~ulf/papers/kor_naacl00.ps.gz. [8] Hermjakob, U. 2001. Parsing and Question Classification for Question Answering. In prep. [9] Hovy, E.H., L. Gerber, U. Hermjakob, M. Junk, and C.-Y. Lin. 2000. Question Answering in Webclopedia. Proceedings of the TREC-9 Conference. NIST. Gaithersburg, MD. [10] Moldovan, D., S. Harabagiu, M. Pasca, R. Mihalcea,, R. Girju, R. Goodrum, and V. Rus. 2000. The Structure and Performance of an Open-Domain Question Answering System. Proceedings of the Conference of the Association for Computational Linguistics (ACL-2000), 563—570. [11] Srihari, R. and W. Li. 2000. A Question Answering System Supported by Information Extraction. In Proceedings of the 1st Conference of the North American Chapter of the Association for Computational Linguistics (ANLPNAACL-00), 166—172. [12] Witten, I.H., A. Moffat, and T.C. Bell. 1994. Managing Gigabytes: Compressing and Indexing Documents and Images. New York: Van Nostrand Reinhold.  SURF Luxor is famed for its Valley of the Kings Pharaonic necropolis and the Karnak temple complex. CAT S-SNT CLASS I-EV-BE CLASSES (I-EV-BE) LEX be SCORE 0  SURF Luxor CAT S-NP CLASS I-EN-LUXOR CLASSES (I-EN-LUXOR I-EN-CITY I-EN-PLACE I-EN-AGENT I-EN-PROPER-NAMED-ENTITY) LEX Luxor ROLES (SUBJ) SCORE 4  SURF is CAT S-AUX CLASS I-EV-BE CLASSES (I-EV-BE) LEX be ROLES (PRED) SCORE 1  SURF famed CAT S-ADJP CLASS I-EADJ-FAMED CLASSES (I-EADJ-FAMED) LEX famed ROLES (COMPL) GRADE UNGRADED SCORE 0  SURF for its Valley of the Kings Pharaonic necropolis and the Karnak temple complex CAT S-PP CLASS I-EN-NECROPOLIS CLASSES (I-EN-NECROPOLIS) 
This paper proposes a practical approach employing n-gram models and error-correction rules for Thai key prediction and Thai-English language identification. The paper also proposes rule-reduction algorithm applying mutual information to reduce the error-correction rules. Our algorithm reported more than 99% accuracy in both language identification and key prediction. 
Signs are everywhere in our lives. They make our lives easier when we are familiar with them. But sometimes they also pose problems. For example, a tourist might not be able to understand signs in a foreign country. In this paper, we present our efforts towards automatic sign translation. We discuss methods for automatic sign detection. We describe sign translation using example based machine translation technology. We use a usercentered approach in developing an automatic sign translation system. The approach takes advantage of human intelligence in selecting an area of interest and domain for translation if needed. A user can determine which sign is to be translated if multiple signs have been detected within the image. The selected part of the image is then processed, recognized, and translated. We have developed a prototype system that can recognize Chinese signs input from a video camera which is a common gadget for a tourist, and translate them into English text or voice stream. 
Chunk parsing has focused on the recognition of partial constituent structures at the level of individual chunks. Little attention has been paid to the question of how such partial analyses can be combined into larger structures for complete utterances. The Tu¨SBL parser extends current chunk parsing techniques by a tree-construction component that extends partial chunk parses to complete tree structures including recursive phrase structure as well as function-argument structure. Tu¨SBL’s tree construction algorithm relies on techniques from memory-based learning that allow similarity-based classiﬁcation of a given input structure relative to a pre-stored set of tree instances from a fully annotated treebank. A quantitative evaluation of Tu¨SBL has been conducted using a semi-automatically constructed treebank of German that consists of appr. 67,000 fully annotated sentences. The basic PARSEVAL measures were used although they were developed for parsers that have as their main goal a complete analysis that spans the entire input. This runs counter to the basic philosophy underlying Tu¨SBL, which has as its main goal robustness of partially analyzed structures. Keywords robust parsing, chunk parsing, similarity-based learning 1. INTRODUCTION Current research on natural language parsing tends to gravitate toward one of two extremes: robust, partial parsing with the goal of broad data coverage versus more traditional parsers that aim at complete analysis for a narrowly deﬁned set of data. Chunk parsing [1, 2] offers a particularly promising and by now widely used example of the former kind. The main insight that underlies the chunk parsing strategy is to isolate the (ﬁnite-state) analysis of nonrecursive, syntactic structure, i.e. chunks, from larger, recursive structures. This results in a highly-efﬁcient parsing architecture that is realized as a cascade of ﬁnite-state transducers and that pur- .  sues a longest-match, right-most pattern-matching strategy at each level of analysis. Despite the popularity of the chunk parsing approach, there seem to be two apparent gaps in current research: 1. Chunk parsing research has focused on the recognition of partial constituent structures at the level of individual chunks. By comparison, little or no attention has been paid to the question of how such partial analyses can be combined into larger structures for complete utterances. 2. Relatively little has been reported on quantitative evaluations of chunk parsers that measure the correctness of the output structures obtained by a chunk parser. The main goal of the present paper is help close those two research gaps. 2. THE TU¨ SBL ARCHITECTURE In order to ensure a robust and efﬁcient architecture, Tu¨SBL, a similarity-based chunk parser, is organized in a three-level architecture, with the output of each level serving as input for the next higher level. The ﬁrst level is part-of-speech (POS) tagging of the input string with the help of the bigram tagger LIKELY [10].1 The parts of speech serve as pre-terminal elements for the next step, i.e. the chunk analysis. Chunk parsing is carried out by an adapted version of Abney’s [2] scol parser, which is realized as a cascade of ﬁnite-state transducers. The chunks, which extend if possible to the simplex clause level, are then remodeled into complete trees in the tree construction level. The tree construction is similar to the DOP approach [3, 4] in that it uses complete tree structures instead of rules. Contrary to Bod, we do not make use of probabilities and do not allow tree cuts, instead we only use the complete trees and minimal tree modiﬁcations. Thus the number of possible combinations of partial trees is strictly controlled. The resulting parser is highly efﬁcient (3770 English sentences took 106.5 seconds to parse on an Ultra Sparc 10). 3. CHUNK PARSING AND TREE CONSTRUCTION The division of labor between the chunking and tree construction modules can best be illustrated by an example. ½The inventory of POS tags is based on the Stuttgart-Tu¨bingen Tagset (STTS) [11].  −  −  SIMPX 517  −  −  −  NF 516 OA  NX 515  −  −  VF 509 MOD ADVX 505 HD  LK 506 HD VXFIN 502 HD  ON NX 501 HD  MF 511 MOD ADVX 504 HD  MOD ADVX 510 HD  VC 503 OV VXINF 500 HD  NX 508  HD  APP  NX 507  −  HD  dann 0 ADV  w"urde 1 VAFIN  ich 2 PPER  vielleicht 3 ADV  noch 4 ADV  vorschlagen Donnerstag  5  6  VVINF  NN  den 7 ART  elften 8 NN  und 9 KON  −  NX 514  HD  APP  NX 513  −  −  HD  ADJX 512 HD  Freitag 10 NN  den 11 ART  zw"olften August  12  13  ADJA  NN  Figure 2: Sample tree construction output  Input: dann w”urde ich vielleicht noch vorschlagen Donnerstag den elften und Freitag den zw”olften August (then I would suggest maybe Thursday eleventh and Friday twelfth of August)  Chunk parser output:  [simpx [advx [vxfin [nx2 [advx [advx [vvinf  [adv dann]] [vafin w"urde]] [pper ich]] [adv vielleicht]] [advmd noch]] vorschlagen]]  [nx3  [day [art [adja  Donnerstag] den] elften]]  [kon und]  [nx3  [day [art [adja [month  Freitag] den] zw"olften] August]]  Figure 1: Chunk parser output  For complex sentences such as the German input dann w”urde ich vielleicht noch vorschlagen Donnerstag den elften und Freitag den zw”olften August (then I would suggest maybe Thursday eleventh and Friday twelfth of August), the chunker produces a structure in which some constituents remain unattached or partially annotated in keeping with the chunk-parsing strategy to factor out recursion and to resolve only unambigous attachments, as shown in Fig. 1. In the case at hand, the subconstituents of the extraposed coordinated noun phrase are not attached to the simplex clause that ends with the non-ﬁnite verb that is typically in clause-ﬁnal position in declarative main clauses of German. Moreover, each conjunct of the coordinated noun phrase forms a completely ﬂat structure. Tu¨SBL’s tree construction module enriches the chunk output  as shown in Fig. 22. Here the internally recursive NP conjuncts have been coordinated and integrated correctly into the clause as a whole. In addition, function labels such as mod (for: modiﬁer), hd (for: head), on (for: subject), oa (for: direct object), and ov (for: verbal object) have been added that encode the function-argument structure of the sentence. 4. SIMILARITY-BASED TREE CONSTRUCTION The tree construction algorithm is based on the machine learning paradigm of memory-based learning [12].3 Memory-based learning assumes that the classiﬁcation of a given input should be based on the similarity to previously seen instances of the same type that have been stored in memory. This paradigm is an instance of lazy learning in the sense that these previously encountered instances are stored “as is” and are crucially not abstracted over, as is typically the case in rule-based systems or other learning approaches. Past applications of memory-based learning to NLP tasks consist of classiﬁcation problems in which the set of classes to be learnt is simple in the sense that the class items do not have any internal structure and the number of distinct items is small. The use of a memory-based approach for parsing implies that parsing needs to be redeﬁned as a classiﬁcation task. There are two fundamentally different, possible approaches: the one is to split parsing up into different subtasks, that is, one needs separate classiﬁers for each functional category and for each level in a recursive structure. Since the classiﬁers for the functional categories as well as the individual decisions of the classiﬁers are independent, multiple or no candidates for a speciﬁc grammatical function or constituents with several possible functions may be found so that an additional classiﬁer is needed for selecting the most appropriate assignment (cf. [6]). The second approach, which we have chosen, is to regard the complete parse trees as classes so that the task is deﬁned as the selection of the most similar tree from the instance base. Since in ¾All trees in this contribution follow the data format for trees deﬁned by the NEGRA project of the Sonderforschungsbereich 378 at the University of the Saarland, Saarbru¨cken. They were printed by the NEGRA annotation tool [5]. ¿Memory-based learning has recently been applied to a variety of NLP classiﬁcation tasks, including part-of-speech tagging, noun phrase chunking, grapheme-phoneme conversion, word sense disambiguation, and pp attachment (see [9], [14], [15] for details).  construct tree(chunk list, treebank): while (chunk list is not empty) do remove ﬁrst chunk from chunk list process chunk(chunk, treebank)  Figure 3: Pseudo-code for tree construction, main routine.  process chunk(chunk, treebank): words := string yield(chunk) tree := complete match(words, treebank) if (tree is not empty) then output(tree) else tree := partial match(words, treebank) if (tree is not empty) then if (tree = postﬁx of chunk) then tree1 := attach next chunk(tree, treebank) if (tree is not empty) then tree := tree1 if ((chunk - tree) is not empty) then tree := extend tree(chunk - tree, tree, treebank) output(tree) if ((chunk - tree) is not empty) then process chunk(chunk - tree, treebank) else pos := pos yield(chunk) tree := complete match(pos, treebank) if (tree is not empty) then output(tree) else while (chunk is not empty) do remove ﬁrst subchunk c1 from chunk process chunk(c1, treebank)  direct hit, i.e. complete chunk found in treebank if attach next chunk succeeded chunk might consist of both chunks chunk might consist of both chunks (s.a.) i.e. process remaining chunk back off to POS sequence back off to subchunks  Figure 4: Pseudo-code for tree construction, subroutine process chunk.  this case, the internal structure of the item to be classiﬁed (i.e. the input sentence) and of the class item (i.e. the most similar tree in the instance base) need to be considered, the classiﬁcation task is much more complex, and the standard memory-based approach needs to be adapted to the requirements of the parsing task. The features Tu¨SBL uses for classiﬁcation are the sequence of words in the input sentence, their respective POS tags and (to a lesser degree) the labels in the chunk parse. Rather than choosing a bag-of-words approach, since word order is important for choosing the most similar tree, the algorithm needed to be modiﬁed in order to rely more on sequential information. Another modiﬁcation was necessitated by the need to generalize from the limited number of trees in the instance base. The classiﬁcation is simple only in those cases where a direct hit is found, i.e. where a complete match of the input with a stored instance exists. In all other cases, the most similar tree from the instance base needs to be modiﬁed to match the chunked input. If these strategies for matching complete trees fail, Tu¨SBL attempts to match smaller subchunks in order to preserve the quality of the annotations rather than attempt to pursue only complete parses. The algorithm used for tree construction is presented in a slightly simpliﬁed form in Figs. 3-6. For readability’s sake, we assume here that chunks and complete trees share the same data structure  so that subroutines like string yield can operate on both of them indiscriminately. The main routine construct tree in Fig. 3 separates the list of input chunks and passes each one to the subroutine process chunk in Fig. 4 where the chunk is then turned into one or more (partial) trees. process chunk ﬁrst checks if a complete match with an instance from the instance base is possible.4 If this is not the case, a partial match on the lexical level is attempted. If a partial tree is found, attach next chunk in Fig. 5 and extend tree in Fig. 6 are used to extend the tree by either attaching one more chunk or by resorting to a comparison of the missing parts of the chunk with tree extensions on the POS level. attach next chunk is necessary to ensure that the best possible tree is found even in the rare case that the original segmentation into chunks contains mistakes. If no partial tree is found, the tree construction backs off to ﬁnding a complete match in the POS level or to starting the subroutine for processing a chunk recursively with all the subchunks of the present chunk. The application of memory-based techniques is implemented in the two subroutines complete match and partial match. The presentation of the two cases as two separate subroutines is for expository purposes only. In the actual implementation, the search is carried out only once. The two subroutines exist because of string yield returns the sequence of words included in the input structure, pos yield the sequence of POS tags.  attach next chunk(tree, treebank): take ﬁrst chunk chunk2 from chunk list words2 := string yield(tree, chunk2) tree2 := complete match(words2, treebank) if (tree2 is not empty) then remove chunk2 from chunk list return tree2 else return empty  attempts to attach the next chunk to the tree  Figure 5: Pseudo-code for tree construction, subroutine attach next chunk.  extend tree(rest chunk, tree, treebank): words := string yield(tree) rest pos := pos yield(rest chunk) tree2 := partial match(words + rest pos, treebank) if ((tree2 is not empty) and (subtree(tree, tree2))) then return tree2 else return empty  extends the tree on basis of POS comparison  Figure 6: Pseudo-code for tree construction, subroutine extend tree.  the postprocessing of the chosen tree which is necessary for partial matches and which also deviates from standard memory-based applications. Postprocessing mainly consists of shortening the tree from the instance base so that it covers only those parts of the chunk that could be matched. However, if the match is done on the lexical level, a correction of tagging errors is possible if there is enough evidence in the instance base. Tu¨SBL currently uses an overlap metric, the most basic metric for instances with symbolic features, as its similarity metric. This overlap metric is based on either lexical or POS features. Instead of applying a more sophisticated metric like the weighted overlap metric, Tu¨SBL uses a backing-off approach that heavily favors similarity of the input with pre-stored instances on the basis of substring identity. Splitting up the classiﬁcation and adaptation process into different stages allows Tu¨SBL to prefer analyses with a higher likelihood of being correct. This strategy enables corrections of tagging and segmentation errors that may occur in the chunked input. 4.1 Example Input: dann w”urde ich sagen ist das vereinbart  (then I would say this is arranged) Chunk parser output:  [simpx  [advx [vxfin [nx2 [vvinf  [adv dann]] [vafin w"urde]] [pper ich]] sagen]]  [simpx  [vafin [nx2 [vvpp  ist] [pds das]] vereinbart]]  Figure 7: Chunk parser output For the input sentence dann w”urde ich sagen ist das vereinbart (then I would say this is arranged), the chunked output is shown in Fig. 7. The chunk parser correctly splits the input into two clauses  Table 1: Quantitative evaluation  minimum maximum average  precision  76.82% 77.87% 77.23%  recall  66.90% 67.65% 67.28%  crossing accuracy 93.44% 93.95% 93.70%  dann w”urde ich sagen and ist das vereinbart. A look-up in the instance base ﬁnds a direct hit for the ﬁrst clause. Therefore, the correct tree can be output directly. For the second clause, only a partial match on the level of words can be found. The system ﬁnds the tree for the subsequence of words ist das, as shown in Fig. 8. By backing off to a comparison on the POS level, it ﬁnds a tree for the sentence hatten die gesagt (they had said) with the same POS sequence and the same structure for the ﬁrst two words. Thus the original tree that covers only two words is extended via the newly found tree. Tu¨SBL’s output for the complete sentence is shown in Fig. 9. 5. QUANTITATIVE EVALUATION A quantitative evaluation of Tu¨SBL has been conducted using a semi-automatically constructed treebank of German that consists of appr. 67,000 fully annotated sentences or sentence fragments.5 The evaluation consisted of a ten-fold cross-validation test, where the training data provide an instance base of already seen cases for Tu¨SBL’s tree construction module. The evaluation focused on three PARSEVAL measures: labeled precision, labeled recall and crossing accuracy, with the results shown in Table 1. While these results do not reach the performance reported for other parsers (cf. [7], [8]), it is important to note that the task carried out here is more difﬁcult in a number of respects: 1. The set of labels does not only include phrasal categories, but also functional labels marking grammatical relations such as subject, direct object, indirect object and modiﬁer. Thus, the evaluation carried out here is not subject to the justiﬁed criticism levelled against the gold standards that are typically See [13] for further details.  SIMPX 504  −  −  LK 502 HD  MF 503 ON  VXFIN 500 HD  NX 501 HD  ist 0 VAFIN  das 
This paper presents recent improvements in the development of the University of Colorado “CU Communicator” and “CUMove” spoken dialog systems. First, we describe the CU Communicator system that integrates speech recognition, synthesis and natural language understanding technologies using the DARPA Hub Architecture. Users are able to converse with an automated travel agent over the phone to retrieve up-to-date travel information such as flight schedules, pricing, along with hotel and rental car availability. The CU Communicator has been under development since April of 1999 and represents our test-bed system for developing robust human-computer interactions where reusability and dialogue system portability serve as two main goals of our work. Next, we describe our more recent work on the CU Move dialog system for in-vehicle route planning and guidance. This work is in joint collaboration with HRL and is sponsored as part of the DARPA Communicator program. Specifically, we will provide an overview of the task, describe the data collection environment for in-vehicle systems development, and describe our initial dialog system constructed for route planning. 1. CU COMMUNICATOR 1.1 Overview The Travel Planning Task The CU Communicator system [1,2] is a Hub compliant implementation of the DARPA Communicator task [3]. The system combines continuous speech recognition, natural language understanding and flexible dialogue control to enable natural conversational interaction by telephone callers to access information from the Internet pertaining to airline flights, hotels and rental cars. Specifically, users can describe a desired airline flight itinerary to the Communicator and use natural dialog to negotiate a flight plan. Users can also inquire about hotel availability and pricing as well as obtain rental car reservation  information. System Overview The dialog system is composed of a Hub and several servers as shown in Fig. 1. The Hub is used as a centralized message router through which servers can communicate with one another [4]. Frames containing keys and values are emitted by each server, routed by the hub, and received by a secondary server based on rules defined in a “Hub script”.  www  DDaattaaBBaassee// BBaacckkeenndd CCoonnffidideennccee SSeerrvveerr  LLaanngguuaaggee GGeenneerraattoorr Hub  SSppeeeecchh RReeccooggnniizzeerr SSppeeeecchh SSyynntthheessiizzeerr  AAuuddioioSSeerrvveerr  DDiaialologguuee MMaannaaggeerr  SSeemmaanntticic PPaarrsseerr  Figure 1. Block diagram of the functional components that comprise the CU Communicator system1.  1.2 Audio Server The audio server is responsible for answering the incoming call, playing prompts and recording user input. Currently, our system uses the MIT/MITRE audio server that was provided to DARPA Communicator program participants. The telephony hardware consists of an external serial modem device that connects to the microphone input and speaker output terminals on the host computer. The record process is pipelined to the speech recognition server and the play process is pipelined the text-tospeech server. This audio server does not support barge-in. Recently we have developed a new audio server that supports barge-in using the Dialogic hardware platform. The new audio server implements a Fast Normalized Least-Mean-Square (LMS) algorithm for software-based echo cancellation. During operation, the echo from the system speech is actively cancelled from the recorded audio to allow the user to cut through while  
This paper presents a novel language-independent question/answering (Q/A) system based on natural language processing techniques, shallow query understanding, dynamic sliding window techniques, and statistical proximity distribution matching techniques. The performance of the proposed system using the latest Text REtrieval Conference (TREC-8) data was comparable to results reported by the top TREC-8 contenders. Keywords Question/Answer, Natural Language Processing, Query Understanding, Dynamic Sliding Window, Proximity Distribution 1. INTRODUCTION Over the past decade, the TREC community has invested its efforts on and advanced technologies of automatic information retrieval systems. Recently, the same community decided to divide the traditional information retrieval task to several so called tracks: the cross-language information retrieval track, the ﬁltering track, the interactive track, the question and answering track, the query track, the spoken document retrieval track, and the web track[6]. The decision is mainly due to the mature technologies in the traditional information retrieval ﬁeld and the desire to expand the technologies to additional areas of interest. The goal of the question and answering track is the development of systems that generate concise answers to user queries. This goal is similar in nature to the goal of a traditional information retrieval system where relevant This work was funded by DARPA under Air Force Contract F19628-00-c-0002. Opinions, interpretations, conclusions, and recommendations are those of the authors and do not necessarily represent the views of the agency or the US Air Force. ¡ Daniel Pack is an associate professor of Electrical Engineering from the Air Force Academy on his sabbatical leave. .  documents are extracted for user queries; users are then required to read through the selected documents to ﬁnd answers. In a question answering system, it is the system's responsibility to ﬁnd the answers to queries.  Queries  Data  Q/A System  Translation Module  A Query Processing  Preprocessing Stemming POS Tagging Concept Tagging Query Identification Proximity Computation  B  Preprocessing 
BravoBrava! is expanding the repertoire of commercial user interfaces by incorporating multimodal techniques combining traditional point and click interfaces with speech recognition, speech synthesis, and gesture recognition. One of these applications is software to help children read. While the child reads aloud, the computer keeps the child on track and offers feedback when the child has difficulty. The feedback can be as subtle as changing the text color for a well articulated phrase or as friendly as a cartoon character that talks. The computer is infinitely patient and can keep detailed records of the child's progress. The reading software is being commercialized by BravoBrava!’s spinoff company, SUP Inc. Keywords: Reading, pedagogy Figure 1. Here is an example from a sample story. The system tracks the reading and intervenes when help is needed. This screen shows a sample intervention if the child stumbles on the word ‘butterflies’.  1. INTRODUCTION Our vision is to use technology to provide a high-quality, low cost reading coach that delivers voice-activated reading instruction, practice, and assessment over electronic media. Reading is fundamental; it can also be fun. However, about 40% of mainstream 4th graders cannot read at the basic level. One of our country’s critical needs is to improve reading performance for all children since their future, both individually and together, depends on literacy. Reading level predicts economic performance for both individuals and societies. Beyond the basics, as reviewed in the recent report of the National Reading Panel (2000), engaging children in supported oral reading is the most valuable means toward building their reading proficiency. At present, however, the only means of giving children such practice is by finding a human adult who will sit with them and help them. However, technology can provide an automated reading coach to break through this bottleneck, so that every child in every school can get the support that she or he needs. This technology will help reduce the digital divide, and provide an unprecedented level of tracking data to leverage teachers’ instruction and assessment efforts and to build the next generation of intervention techniques. 2. SUP’S APPROACH SUP targets the stage at which children have learned the letter to sound rules but are still struggling to gain vocabulary and fluency. This stage has sometimes been called the transition from ‘learning to read’ to ‘reading to learn’ and comes just after the stage requiring explicit pronunciation and vocabulary tutoring, provided by Mostow’s Project Listen at CMU and other reading software. At this stage in reading development, language skills are usually too poor to make traditional dictionaries much of a help. Dictionaries disrupt the child’s focus on the text being read, offer too many definitions, and usually provide definitions that are harder for the child to read than the original text. Instead, SUP provides a generalized ‘dictionary’, the Reading Resource, to give immediate context specific help when needed, much the way a human reading coach might intervene. “SUPplementing” text with the Reading Resource, which includes word definitions, sample sentences, graphics and multimedia can create an engaging environment for learning. However, the immediacy and contextspecificity of this resource aims to maintain the child’s focus on the material being read. Becoming engaged in the text itself is the  real goal of reading. SUPplementation can make texts accessible to children that would otherwise be above their reading level, so that the content of the text can draw them into the desire to read. Vocabulary and grammatical knowledge grow with experience of more words in more contexts. Figure 2. The screen shot above a sample after the child has asked for a display of how well the selection was read. SUP has developed a modular architecture that allows for rapid reconfiguration: • The current demonstration uses Microsoft speech recognition software. However, the architecture supports the use of other recognizers, and we have experimented with others for use on other platforms. • The audio outputs can be from recorded waveforms or, for maximum flexibility, can use a text to speech synthesis system. • Any text can flow through the system for reading practice.  Although explicit measures, such as a quiz to assess comprehension or vocabulary, can be included, several automatic measures are important by-products of use of the tool: • Total number of words read by session, and which were fluent or not • Words per minute as a function of time • Level of the material read as a function of time • Number of times intervention of the Reading Resource was used • Number of times the child had the story read to him/her • The actual recordings of what was read 3. FUTURE DIRECTIONS Of course, such software will not help unless it gets into the hands of the children who need it. Therefore, an important strategy of the company is to support as many different platforms as possible to enable this goal. The modular architecture of the system facilitates transition to various platforms. Similarly, to be of maximal interest to the most children, we need as much appealing content as possible. In the area of human language technology, we hope such technology will evolve to automatically provide: 
Quality assurance in the technical documentation and translation process is a means of assuring both quality as a process and quality documents as a result of any quality processes implemented. The willingness to improve quality is the common desire of commerce partners and a project team co-operating in the production of a quality assurance manual, which is the topic to be discussed in this paper. A concentration of mutual efforts on the road to higher quality in technical documentation is the focus of this quality assurance manual to be created in accordance with the partners' requirements. Furthermore, the easy implementation of necessary changes to this manual and the introduction of new approaches to process handling combine to make the quality assurance manual versatile and lasting. Preconditions to this approach are manifested in the quality awareness required of the participants, as well as in an openness towards innovative quality measures. As a consequence of these preconditions, a modular approach is taken to organise and structure the documentation workflow, and to incorporate quality aspects and measures into modules. Modularity is a key factor for converting these project requirements into reality, thus ensuring that quality criteria are assigned to individual tasks and steps within the documentation production workflow. Modules are specified and designed to constitute the centre of the quality assurance manual which will be presented as a flexible and editable on-line document. Introduction The present contribution poses the question whether working according to ISO and other national/international standards is enough to realise highest quality in technical documentation, and whether high-quality multilingual documentation is work ensured, if only deadlines and budgets are kept. While today work process-oriented quality assurance has become common practice in most production-oriented sectors, the discipline of writing and translating technical documentation has its own rules and regulations. This paper reports on the QUATRE approach which aims at instrumentalising these rules, thus attempting to further quality assurance in the technical documentation and translation process. In addition, the discussion printed here also provides insight into those perspectives offered when the QUATRE approach is successfully carried out.  The QUATRE project and its partners The QUATRE research project1 is carried out in the Department of Technical Translation at the Flensburg University of Applied Sciences (Germany). It focuses on the development of an on-line quality assurance manual for, initially, the QUATRE partners. The commercial partners involved are representatives of the documentation departments of the Drägerwerk AG, Lübeck, the Volkswagen AG, Wolfsburg, and the MobilCom AG, Büdelsdorf, as well as the former Great Plains Deutschland GmbH, Hamburg, the translation agency Lass Fachübersetzungen und Dokumentation, Flensburg, and the communication service provider Zindel Technische Dokumentation und Multimedia, Hamburg. All companies are located in northern Germany and, through participation in periodic workshops and a web forum, they contribute to the project their expertise in the translation and technical documentation workflow. The partners work in close co-operation with the QUATRE team2 of the Flensburg University of Applied Sciences. Along with the partners' hands-on experience, the team also draws on current research literature, e.g. on project management, quality control, languages for specific purposes and process optimisation, to form the QUATRE quality assurance manual. In order to meet the quality requirements of the partners the QUATRE team aims at developing a manual which takes the partners' individual wishes, preferences and needs into account. The challenge for the QUATRE team lies, on the one hand, in the significant differences between the unlike industrial and service sector companies and, on the other hand, in the partners' differing requirements. The companies vary greatly in size, in the form of organisation, in the kind of service and/or documentation they provide and in the way in which they currently integrate quality aspects or measures of quality management into their workflow. For the partners, the documentation process is a technical translation process in itself or includes technical translation or software localisation to a great extent. The kind of documentation produced ranges from internal and external print to on-line documentation. Partners produce full software documentation, translations of technical text, respectively internal standards and the like, and product briefs along with workplace descriptions, operator and service manuals, maintenance manuals and manuals for international markets. While some partners already work within the structures of company-wide quality management systems, others have not yet incorporated defined quality rules and standards-based approaches to quality assurance into their day-to-day business. QUATRE brings together the common interests of the partners in customer-oriented, high-quality documentation. In addition to customer satisfaction, there is a common necessity for the manual to focus on interaction and communication within the individual company structures. Partners require guidelines, ideas, and/or suggestions for handling procedures at the initial stage of a documentation assignment, for initial customer meetings and for interviews, preferably to be provided in the form of checklists. In addition to the service area, there is also a considerable need for quality milestone in the course of 
The IATE project was launched in early 2000 for the creation of a single central terminology database for all the institutions, agencies and other bodies of the European Union. By mid-2001, it had reached the prototype phase. It is evident that the attempt of uniting the terminology that has been created in different institutions, with different approaches to terminology and different working cultures, was not an easy task. Although the implementation of the system has, from a technical point of view, already reached a rather advanced stage it is predictable that user feedback during the prototype and pilot phases will still lead to a number of changes. The biggest challenge of the project, however, lies in its introduction in the terminology and translation workflow of the participating bodies. This is illustrated in the second part of the paper by the example of the European Parliament's Translation Service. PART ONE: CURRENT STATUS OF THE IATE PROJECT INTRODUCTION Since 1995 high level representatives of the translation services of the European Union's Institutions and agencies have met in the Interinstitutional Committee for Translation. This committee was set up to formalise contacts and cooperation between these partners that had already existed on an informal level previously. The mandate that governs its activities clearly stresses two aspects: that the partners involved share to a large extent the same problems and face similar challenges; they should thus thrive to find common solutions; whilst underlining, on the other hand, that one translation service is not like another; that due to the role each of the bodies of the Union plays in the life of the Community the practicalities of translation work may differ strongly from one service to another. Still, one of the fields where close cooperation was not only considered in the interest of complying with ever tighter budget lines, but also as offering substantial advantages for the linguistic staff, was terminology.  The availability of terminological resources in the translation services of the Union as such was (and is) far from what one would call unsatisfactory. The "big three", Commission, European Parliament and Council, have each built up powerful terminology databases: the Commission's Eurodicautom, as the oldest and biggest of the institutional databases contains about 1.4 million multilingual concepts. It offers, as do the Council's TIS and the EP's EUTERPE, web-based search interfaces and thus gives access to a vast store of linguistic information to a wide public inside and outside the institutions. The picture looks less bright for the smaller institutions and agencies who in some cases use internal databases (usually in MultiTerm 95) or make do with glossaries in word processor formats. Cooperation on terminological questions and the sharing of information is far from evident even for bodies that need to work closely together like, e.g. the decentralised agencies and the Translation Centre. There are yet more drawbacks to this situation. The absence of a single point of access to all terminological data makes the lives of translators and other people searching for terminological information difficult. In order to access all available information from the big three databases you would have to learn and use three different interfaces. Attempts to import data from TIS and Euterpe into Eurodicautom to overcome this difficulty have given only unsatisfactory results. Not only do the technical difficulties of the process make regular updates impossible. The difference in the data structures, expression of different terminological cultures and working methods, also lead to a loss of data in the import process. This fact points to another, more general problem that goes beyond pure convenience for the end-user. The existence of parallel, independent approaches for the creation and maintenance of terminology have made cooperation between institutions and agencies difficult if not impossible. There is no easy way of standardising the usage of terminology between institutions. Problems of inconsistency, redundancy in the data and duplication of work result from this "balkanisation" of the terminology in the European Union. A study carried out by the IT consultancy company ATOS in 1998 clearly analysed the shortcomings of this situation and concluded that the best remedy was the creation of a single interinstitutional terminology database. After the definition of a common data format all data collected by the different institutions should be merged into this database. But the recommendations of the report went yet further: they stressed the need for wider interinstitutional cooperation in the field of terminology, the reorganisation of terminology activity, reinforcement of staffing where necessary and the build-up of an infrastructure that would allow for cooperative data management. Acting on the recommendations of the ATOS study, the Translation Centre launched the "IATE" ("Inter-Agency Terminology Exchange") project in 1999; its initial objective was to create an infrastructure for the management of terminology for the Centre and the decentralised agencies of the Union. The other European Institutions later joined this initiative and gave the project its truly interinstitutional status.  The implementation of the IATE project started in January 2000. A consortium of the Greek IT company Quality&Reliability and the Danish government research institute Center for Sprogteknologi (CST) developed – together with institutional participants – the technical and functional specifications of the European Union's terminology database. In summer 2001 the tests of the prototype of this system were performed. Concepts that have been developed by the participants of various work groups in the phase of system analysis and design have become usable features of the prototype: interactive on-line data entry, a flexible validation system, tools for monitoring, reporting and auditing, advanced user management and modules for large scale data management are operational. However, when we speak of a prototype we should be aware that there is still some way to go until this database will be accessible for institutional users and a wider public. The first version of the EU term base made it possible for a group of test users to carry out functional tests, i.e. to check whether the underlining concepts have been implemented correctly and whether they were correct in the first place. A number of aspects of the system, especially the design of the user interfaces, will be subject to considerable modifications in the near future. The screen shots reproduced in this paper are taken from the prototype and should thus be seen as what they are: a glimpse of work in progress and not as a final product. Another two pilot test phases, scheduled for the first two quarters of 2002, will reflect the experience gathered during the prototype test phase and brings us much closer to a system that hopefully combines functionality and user-friendliness. It is beyond the scope of this paper to give a detailed account of all the different features and modules that have been implemented so far. I will concentrated on following three major aspects of this development: • One common database for all institutions and agencies containing all legacy data; • Interactivity, i.e. the possibility for user to carry out modifications, to add entries directly on the central database and to allow thus their colleague to profit from this work immediately; • In-build validation procedures to ensure quality. Other features, that can be discussed only briefly, include: • management tools, e.g. for user and data administration; • reporting tools; • messaging systems as communication mechanisms between the actors in the terminology workflow. LEGACY DATA Merging the terminology of the existing institutional databases into one single database was a major challenge of the first phase of the project. So far the following databases have been imported into the EU term base: Eurodicautom (Commission), TIS (Council), Euterpe (EP), Euroterms (Translation Centre) and CDCTERM (Court of Auditors). Data from the Court of Justice and the European Investment Bank will be added during a  second phase of data loading scheduled for the beginning of 2002. The resources of other European bodies can be added at a later stage as the need arises. The first achievement of the IATE project is that the legacy data has been physically merged into one relational database1 without serious loss or corruption of data.  Concepts Language specific entries Definitions Explanations  1,433,252 7,793,060 815,560 51,320  Breakdown by entry type  Terms  7,111,480  Abbreviations  493,603  Phrases  187,872  Table 1: The prototype of the EU term base in figures  This task was challenging not only because of the tremendous amount of data that had to be treated (Eurodicautom alone contains about 1.2 million multilingual concepts); a bigger problem than the actual number of entries was the content of the different databases and the ways in which it is structured: different philosophies of terminology and different historical backgrounds that are expressed in the data stored had to be reconciled. This process involved, in a first step, the definition of mapping rules between the data structures of the existing databases and the new format of the interinstitutional database. This data structure takes into consideration the evolving standards in the field (SALT/MARTIF, GENETER). It adopted a concept-oriented approach; the mono- and multilingual information on each aspect of a concept can be expressed on four interrelated levels of the data structure of the terminological entries:  
MLT (MultiLingual Technology) Department  SAP AG, Walldorf, Germany  E-Mail:  Jonathan.Nigel. Wells@sap.com  Tel:  +49(0)6227745886  Fax.:  +49(0)62277821174  Synopsis  This paper describes a practical example of MT and TM being used to facilitate the initial and subsequent translation of several hundred documents daily in a corporate environment. These documents make up a knowledge database, which customers can use to solve problems, find answers to their questions, install new features and generally gain information and knowledge about the SAP System. The successful implementation of MT/TM and the flexibility and scalability of both the software and its users have saved many millions of Deutschmarks over a period of 8 years. This system has been successfully developed and enhanced due to the active feedback and close cooperation of all parties involved - the developers, SAP and the translation agencies that work with it.  Introduction & Background  SAP is the world's leading provider of collaborative e-business solutions. With 36,000 installations serving 10 million users at 13,500 organizations in 120 countries across the globe, SAP ranks as the world's third-largest independent software provider. SAP has been in the business of e-business for 29 years and began trading publicly in 1988. Founded in 1972 by five former IBM systems engineers, SAP currently employs more than 23,700 people in more than 50 countries. The company is headquartered in Walldorf, Germany.  Almost half of SAP's employees are actively involved in the process of software development. As software is developed, tested and implemented at customer sites, various questions, problems and other issues occur, which must be documented and processed. To this end, SAP has implemented a workflow system in which any SAP user can send queries to SAP for immediate processing, assigning them to various categories, depending on the issue being raised. These reports, officially known as SAP Customer Messages, are then processed by the developers and returned, hopefully with a solution, to the user. The solution, officially referred to as an 'SAP Note' can often, of course, be reused and must also be stored somewhere in the system for future reference. The Notes are also indexed (full text and by keyword) to facilitate future lookup and reference.  Figure 1 shows a very simplified representation of the procedure for handling the Customer Messages and the relevance of Notes within this procedure. Notes can contain a variety of different information and can also be created as a source of information without first receiving a Customer Message. Notes can contain: • Bug fixes • Additional documentation • Process descriptions • Workaround descriptions • Recommendations • Collective information about other Notes • Other general information The structure of a Note is as follows: • Short Text One line description of the content of the Note • Symptom The issue that is addressed in the Note • Keywords Keywords used for quick searching • Cause and Preconditions Information required for reproducing the error or information relating to the scope of relevance of the Note • Solution Main text of the Note  In the worldwide market, 60% of SAP's revenues come from the English-speaking world. However, the vast majority of SAP Notes are written by in German by developers based in Walldorf. This seemingly simple translation problem is severely compounded by a number of factors, which turn it into a fairly major undertaking: • The Volume Each Note contains, on average, around 105 words, though they can be as long as 100 pages. Currently, around 200 new Notes are created every day and most of these are released for translation. This gives a total of 21,000 new words to be translated every day, and this volume is steadily increasing - it is approximately 25% higher than the same period 12 months ago. The system contains around 450,000 Notes at the moment. • Repetitivity Approximately 350 existing Notes are currently modified each day (information updated or added, errors corrected, etc.) and released for retranslation. This gives a total of 36,750 words, part of which has already been translated. • Turnaround Speed Required As many of the Notes are urgently required by companies around the world, it has been agreed that Notes should generally be translated within 24 hours of their creation, some, however, within 4 hours. These timeframes are primarily defined by SAP's Support Level Agreements with our customers and simply passed on to the agencies. In 1993, the MLT (MultiLingual Technology) department at SAP recognized that this area appeared to be an excellent candidate for the combination of MT and TM and, in conjunction with Siemens, began to implement the METAL system to assist in the translation of SAP Notes. Over the next few years, METAL progressed and developed, first becoming T1 and then Comprendium, moving from the UNIX operating system to Microsoft Windows and from Siemens to SNI, GMS, L&H and then Sail Labs in the process. During this period, the MLT group at SAP provided a complete translation service with full post-editing for Notes, as well as using the MT system for various other large-scale translation jobs within SAP. In 1996, however, the department began to shift its focus away from translation and at the same time realized that the Notes translation was getting too big to handle in such a small group (5 people at the time). A translation agency (S&D, Rendsburg (now part of Ll0nbridge)) was selected to begin taking over this translation job and, by April 1997, the transfer was complete and the Notes translation was completely outsourced. In 2000, for various reasons (including Y2K compatibility and a lack of scalability), METAL was officially retired at SAP, along with the Unix-based workflow tools and T1 was implemented. At the same time, a completely new workflow solution (The Notes Translation Solution, or NTS) was developed by SAP to handle the Windowsbased procedures. To assist us in this, Sail Labs provided various components to simplify the transition and implementation processes.  As of January 2001, some of the Notes translation has been transferred to a second translation agency (SimulTrans, Dublin). We estimate that the implementation of an MT/TM-based solution in this case allows us to save 40-60% of the translation costs (depending on how the figures are calculated). Regardless of whether you look at the best case or worse case figures, the saving over the operating period has been in the region of several million Deutschmarks. The Current Workflow Tool The Notes are initially created, modified and stored in a central SAP System, called CSN. A special program has been developed for the MT translation, so that a userdefined selection of Notes can be downloaded from the CSN system to various directories on a local (Walldorf) file server, depending on the agency that will be translating the Notes. Two files are created - a parameter file containing various information about the Note (author, SAP-specific component, version number, ...) and the Note file (in RTF format) itself. For each translation agency, a daemon program called the Notes Transfer Program (NTP) scans this download directory (plus several others) for files that require transfer to the agency system for translation. When a Note is downloaded, the NTP first makes backup copies of the files, in case there are any problems during later stages of the procedure. The backups are saved to a dedicated file server, the Central File Store (CFS). Once the backup has been saved, the NTP checks to see if this is a brand new Note, or whether a previous version exists and has already been translated. If so, translation memory files should already exist on the CFS. The source and parameter files are copied to the agency site (normally to a temporary storage location at the agency site) along with the memory files, if these exist. From here, another program, the NTS Server 'collects' the Note, runs various technical checks on it and inserts information about it into a central database (the DayDB). This central database is used to store various information about the Note, which can then be used either to assist in the processing of the Note or to enable various statistical analyses for the Notes translation at a later stage. 
Translation Memory (TM) programs facilitate the exploitation of previous translations, which in repetitive domains such as technical documentation are viewed as a valuable resource. Recycling 'old' translations not only saves companies both time and money but also relieves translators of repetitive work freeing up their time for other important tasks. Translation Memory programs crucially use alignment2 programs to enable parallel corpora (previous source language and translated texts) to be loaded into the "memory". The most time consuming and painstaking step of the alignment process for TM creation is checking the proposed alignments for mismatches and correcting them. In this paper the problems which arise in the automatic alignment of parallel texts for the creation of translation memories are addressed. It is the aim of this paper to give suggestions as to how the work of the translator or technical support staff correcting automatic alignments could be lessened. This paper looks more closely at the causes of misalignments and goes some way to proposing methods for reducing these misalignments. Such methods are an analysis of factors which contribute to the poor alignments as well as a proposal for a possible tool which would find and highlight misalignments for the checker, thus substantially decreasing the time and concentration that the checking process entails. A Definition of Alignment Alignment involves matching the sections of two texts with the same content in one or more languages. Alignment is defined as: "Sentence alignment is the problem of making explicit the relations that exist between the sentences of two texts that are known to be mutual translations." (Simard & Plamondon, 1998:59) "The problem of aligning parallel text is characterized more precisely as follows: INPUT. A bitext (C,D). Assume that C contains C passages and C is the set {1,2......C} and similarly for D. OUTPUT. A set A of pairs (i,j), where (i,j) is a coupling designating a passage Ci in one text that corresponds to a passage Dj in the other, and 1  i  C and 1  j  D. (In the general case A can indicate a many-to-many map from C to D.)" (Wu in Dale et al. (Eds), 2000:416; emphasis and typography original) The text segments considered to be mutual alignments are called beads. Alignments in which beads preserve the original structure of text are known as monotonic alignments, that is 
Globalisation is bringing translation and multilingual information processing to areas where it was previously unknown, relatively unimportant, or technically just not feasible. Today, Natural Language Understanding techniques are important for reaching global audiences, and are therefore becoming an indispensable component inside many systems and workflows. The main subjects of discussion continue to be the linguistic approaches used by such tools and applications. Equally important, however, are new types of software that support linguistic systems and that provide the information infrastructure to offer linguistic services to the user. NLU technology often means large and memory-intensive applications, unsuitable for installation and use on smaller computers and mobile clients. DTS is a delivery system ideally suited to providing multilingual and translation services to users within a distributed environment, in networks and on thin clients like mobile phones, PDAs and notebooks. With its modular and scalable architecture and automatic load balancing it provides a flexible basis for delivering linguistic capabilities and other services over the Internet or on an Intranet. Internet translation portals, SMS services, or networked multilingual information systems need a distribution architecture like that provided by DTS. 1. Introduction Even the most sophisticated translation functionality is useless without a means of delivering it to the consumer. In an increasingly networked world, companies like Sail Labs are keenly interested in not only providing applications and services but also in building the systems by which people can make use of these services in a distributed environment. This is where a system like DTS comes in. DTS (Distributed Tasks and Services) is a distributed client-server system offering high-level services to customers in different locations. The system is generic and will be used to make Sail Labs' multilingual communication and information solutions available both on the Internet and in corporate Intranets. The design of DTS is inspired by Sun Microsystems' JavaSpaces™ technology, which allows communication and sharing of objects in a distributed application: JavaSpaces technology acts as a virtual space between providers and requesters of network resources. An additional intelligent event mechanism in Comprendium DTS results in an automatic load balancing capacity. There is no supervisor component which controls all activities in Comprendium DTS and which could easily become a bottleneck for the whole system. Instead, components (engines) ask for more jobs as soon as they are out of work, thereby minimizing administrative overhead and communication costs.  A state-of-the-art delivery system like DTS must offer the following features: Modularity: Functionality should be encapsulated into pluggable components. A core configuration should contains vital functionality only. Any additional utilities should be built on top of the core configuration. Scalability: It must be feasible to extend the system in order to get a better performance (throughput of requests). Especially the administration and communication overhead must be minimized. It must be possible to offer a number of configurations depending on the customer requirements - starting from all system components on a single machine right up to large machine parks Portability: The system has to run on UNIX and Windows NT/2000 environments and must be capable of being used by a wide variety of clients. Security: Secure communication must be guaranteed, and at the same time, conventional communication protocols must be supported. Service independence: The software should be generic, i.e. it should be able to work with any type of service without containing service specific functions. This guarantees a simple integration of new services without software modifications. Figure 1: Service-Request Distribution System 2. DTS Scenarios Before we delve any deeper into the details of the DTS system, let's take a look at some of the scenarios in which we envisage that a system of this kind could be effectively employed to provide multilingual communication and information services. Scenario 1: Translation Portals on the Internet or in a Corporate Intranet First of all, let's take a look at a classical multilingual service: an Internet translation portal. It typically serves the purpose of gist translating foreign language information sources. A business analyst in a bank, for example, regularly screens the Internet to find information on European and Asian investments. Rapid access to this type of information represents a real competitive advantage. Occasionally, these sources are not available in English or any language she is able to understand. A translation portal can help her to get a first impression of a document and to assess, whether it might merit the effort and cost of human translation. 
The evaluation framework I propose is based on adopting a user-oriented perspective. This essential principle translates into defining, first, the context of use of the particular tool, and, then, into checking whether the system conforms to these specifications. The present paper rests on the preceding assumptions and is structured along the following lines: o Scenario test outline. This is broken down into (1) definition of stakeholders' profile as a set of characteristics covering such aspects as text type, translation environment, leveraging needs, team description and terminology requirements; and (2) definition of system performance criteria, using ISO 9126 as the starting point. o Criteria weighting. This implies assigning weighted values to the characteristics previously defined so as to decide their relative importance in the evaluation of the system performance. o Metrics and Measurement. A three element rating is defined for allocating scores to the system under evaluation. Introduction Current trends in the language industry indicate that the new methods implemented in information management are pervading the translation world. In this respect, the field of information management is already directing its procedures towards managing interdependencies, since the complexity of producing documentation impels us "to abandon the silo perspective of product development, marketing communication, technical writing, translation, and product support" (Hofmann and Mehnert, 2000: 60). A translation project's life-cycle must comply, then, with the demands of a wider production environment which considers multilingual information management in the form of information objects (IO). An IO is a collection of information identified as a unit, and defined by its communicative purpose, the specific user it is addressed to, the business entity it represents (a product line or a corporate function), the information it provides (in a specific format and for a target audience) and some publishing restrictions (61).  One of the immediate consequences of this tendency is the high demand imposed on translation project management. This demand is higher than ever and introduces a challenge as new stakeholders enter the translation arena: clients, global teams of translators, cost, time, quality and resources, project's life-cycle, technology, and budget, among others. At the core of the whole process, and providing a means to face this challenge, we find Computer Assisted Translation (CAT) tools, a key resource with a constant presence in the translation workflow. From groundwork, term extraction and glossaries management, to text alignment and leverage of materials extracted from translation databases; from machine translation to post-editing; the operational environment of the translator is directly affected by technology. In this context, the need for a coherent, reproducible model for translation tools evaluation emerges as a key element in the overall strategy for project management. This model should integrate the translation process into the evaluation procedure, considering at the same time stakeholders' needs and requirements. This should be a user-oriented framework for evaluation which could account for different translation scenarios: the industry, public administrations, agencies and freelancers. Up to the present, many evaluations on translation tools have been conducted, specially in the case of translation databases (or translation memories) (see Benis, 1999; Assenat-Falcone, 2000) but, to my knowledge, these are not easily reproducible since they do not supply a set of parameters which could serve as a comprehensible model. In a sense, while these are certainly useful and valuable instances of evaluation, they concentrate on particular products used in a particular setting and under a specific environment which, in turn, restrains the system assessment to a predefined and specific set of needs. Acknowledging the benefits of such an evaluation, I believe that the current trends in the translation industry, as stated above, call for a broader model, one which can be easily reproduced and implemented according to different translation settings. This paper is an attempt at presenting such a model. The point of departure is EAGLES1 document EAG-EWG-PR.2 on Evaluation of Natural Language Processing Systems (1996) which outlines a comprehensive framework for evaluation and, in turn, can be adapted to particular, case-specific tools. In fact, this document advances a set of features for evaluation of translator's aids, concentrating on the description of user profiles, types of translator's aids and the functionality of translation memories. The evaluation framework I propose is based on adopting a user-oriented perspective. This essential principle translates into defining, first, the context of use of the particular tool, and, then, into checking whether the system conforms to these specifications. This golden rule of evaluation implies the following: (a) designing an scenario test according to stakeholders' needs; (b) deciding how each feature of the scenario contributes to the final assessment of the system; and (c) executing the evaluation and assessing system performance. There is no such thing as a best system, but a best system for a particular situation. 
While a multilingual translation project is going on, participants usually fix terminology issues by sending emails to the project manager who will maintain an history database of questions and answers. The idea is to replace this traditional terminology management process with an interactive shared terminology list over the Internet. The list itself implements an intuitive question and answer workflow process and intrinsically acts as an history database reference, allowing participants to share all questions and answers in a simple way. This Internet terminology assistant tool is not intended to replace existing terminology database software but has been designed as a companion tool to help people ask terminology questions and get answers throughout the project. Overview eTermino Q&A will achieve the following major objectives: Collaborative "Questions & Answers" eSharing Using the potential of the Internet, the tool provides a shared Q&A content between the participants. Customer reviewers, project managers, translators and proofreaders will get the same up to date information. Multilingual Editing eService The list fields are all customisable to accommodate customer needs. Multilingual editing capabilities allow the list to be modified by participants from different countries using a standard browser: the only required tool is Internet Explorer 5 with the proper language packs. The list can be easily shared by teams working across time zones. Intrinsically built upon the new Internet enabling technologies such as xml, the shared content can virtually accommodate any Unicode languages. All participants benefit from the built-in sorting and filtering capabilities, and can copy/paste the list into Excel at any time, enabling the creation or update of local multilingual glossaries. On line/Off line Operation The list also features distinct access modes for participants permanently connected to the Internet as well as remote users over a dialled up connection, who would prefer working in disconnected mode. In addition, the shared content can be modified on line for teams permanently connected to the Internet but also off-line for remote translators over a dialled up connection.  How it works The application enables collaborative editing through the Internet between clients and a server. Internet Information Server 5 with Active Server Pages (ASP) is used on the server side. Clients are running Internet Explorer 5 (or later). 
This paper gives an overview of what resources, including software tools, reference material and course material, are currently available on the web for teaching machine translation, and discusses where to find these resources. It makes some suggestions as to how these resources and access to them can be enhanced in the future.
The paper describes the process of designing a new MT course for final-year undergraduates. It explains the skills to be acquired as part of the module. The course will include a practical and a theoretical component, and in addition to subject-specific knowledge the course should enable students to gain competence in analysis of language and appreciation of the nature of communication. It is hoped that some of these skills will be transferable from the specific context of MT to wider areas of application. Discrete profiling and evaluation is not envisaged. The paper also defines areas where an MT course can provide opportunities not necessarily offered on conventional translation courses.
An interactive hypertextual environment for MT training is described. It combines the ARIANE MT system with an hypertextual control interface implemented on the learner{'}s personal computer, and communicating with the ARIANE server through e-mail.
Machine Translation is increasingly being taught within non scientific subject areas at French universities, which involves instructors solving educational and scientific problems caused by the lack of training of these students in computer science. Most of these students are being taught MT within the framework of language and linguistic courses. As MT instructors in both Departments of Foreign Language and Linguistics at Orléans, we will report on our experience of teaching. Besides setting up the technological environment, we also had to consider the courses from two different angles. First of all, we can state that MT tools enable future users to enhance their skills in Machine-Assisted Translation, and secondly they introduce potential future system designers to computational linguistics issues.
This paper reports upon a survey carried out among thirty-eight trainee translators who took courses on machine translation. The survey was conducted asking the sample of students to fill out a questionnaire both at the beginning and at the end of the MT course. The questions aimed at assessing the degree of knowledge about MT of the respondents and the opinions and impressions that they accordingly had on it. The results of the questionnaire were elaborated so as to investigate the relationship between the increase in the knowledge about MT after the conclusion of the course, and the corresponding change in the students{'} attitude towards the discipline, which became much less biased and in general fairly positive, thanks to a very successful and rewarding learning process. The paper suggests that the more the trainee translators became familiar with MT, realising its reasonable potential and current limitations, the less afraid they were of it. These findings encourage the increasing integration and introduction of technology into translation curricula, since the impact of computer technology on language translation directly affects professional human translators. As a result, exposing trainee translators to machine translation seems to raise the profile of their training.
The Machine Translation course at Dublin City University is taught to undergraduate students in Applied Computational Linguistics, while Computer-Assisted Translation is taught on two translator-training programmes, one undergraduate and one postgraduate. Given the differing backgrounds of these sets of students, the course material, methods of teaching and assessment all differ. We report here on our experiences of teaching these courses over a number of years, which we hope will be of interest to lecturers of similar existing courses, as well as providing a reference point for others who may be considering the introduction of such material.
It is a common mispreconception to say that machine translation programs translate word-for-word, but real systems follow strategies which are much more complex. This paper proposes a laboratory assignment to study the way in which some commercial machine translation programs translate whole sentences and how the translation differs from a word-for-word translation. Students are expected to infer some of these extra strategies by observing the outcome of real systems when translating a set of sentences designed on purpose. The assignment also makes students aware of the difficulty of constructing such programs while bringing some technological light into the apparent {``}magic{''} of machine translation.
This paper considers the role of translation software, especially Machine Translation (MT), in curricula for students of computational linguistics, for trainee translators and for language learners. These three sets of students have differing needs and interests, although there is some overlap between them. A brief historical view of MT in the classroom is given, including comments on the author{'}s 25 years of experience in the field. This is followed by discussion and examples of strategies for teaching about MT and related aspects of Language Engineering and Information Technology for the three types of student.
This paper tackles the issue of how to teach Machine Translation (MT) to future translators enrolled in a university translation-training course. Teaching MT to trainee translators usually entails two main difficulties: first, a misunderstanding of what MT is really useful for, which normally leads to the misconception that MT output{'}s quality always equals zero; second, a widespread fear that machines are to replace human translators, consequently leaving them out of work. In order to fight these generalised prejudices on MT among (future) translators, translation instruction should be primarily practical and realistic, as well as learner-centred. It thus ought to highlight the fact that: 1) MT systems and applications are essential components of today{'}s global multilingual documentation production; 2) the way in which MT is employed in large multilingual organisations and international companies opens up new work avenues for translators. This will be illustrated by two activities, one using commercial MT systems for quick translations, whose process outcome is improved through the trainees{'} interaction with the system; the other focusing on MT output comprehensibility by speakers of target language only. MT is thus a mainstream component of a translation-training framework delineated in Yuste (2000) that, by placing the trainee in workplace-like situations, also echoes Kiraly (1999).
Despite considerable investment over the past 50 years, only a small number of language pairs is covered by MT systems designed for information access, and even fewer are capable of quality translation or speech translation. To open the door toward MT of adequate quality for all languages (at least in principle), we propose four keys. On the technical side, we should (1) dramatically increase the use of learning techniques which have demonstrated their potential at the research level, and (2) use pivot architectures, the most universally usable pivot being UNL. On the organizational side, the keys are (3) the cooperative development of open source linguistic resources on the Web, and (4) the construction of systems where quality can be improved ``on demand'' by users, either a priori through interactive disambiguation, or a posteriori by correcting the pivot representation through any language, thereby unifying MT, computer-aided authoring, and multilingual generation.
We propose a program of research which has as its goal establishing a framework and methodology for investigating the pragmatic aspects of the translation process and implementing a computational platform for carrying out systematic experiments on the pragmatics of translation. The program has four components. First, on the basis of a comparative study of multiple translations of the same document into a single target language, a pragmatics-based computational model is to be developed in which reasoning about the beliefs of the participants in the translation task and about the content of a text are central. Second, existing Natural Language Processing technologies are to be appraised as potential components of a computational platform that supports investigations into the effects of pragmatics on translation. Third, the platform is to be assembled and prototype translation systems implemented which conform to the pragmatics-based computational model of translation. Finally, a novel evaluation methodology is to be developed and evaluations of the systems carried out.
User feedback has often been proposed as a method for improving the accuracy of machine translation systems, but useful feedback can also serve a number of secondary benefits, including increasing user confidence in the MT technology and expanding the potential audience of users. Amikai, Inc. has produced a number of communication tools which embed translation technology and which attempt to improve the user experience by maximizing useful user interaction and feedback. As MT continues to develop, further attention needs to be paid to developing the overall user experience, which can improve the utility of translation tools even when translation quality itself plateaus.
In this short paper, I explore ways in which the MT community might formulate goals that will expand on known successes, build on existing strengths, and identify long term research goals.
Our focus is on high-quality (HQ) translation, the worldwide demand for which continues to increase exponentially and now far exceeds the capacity of the translation profession to satisfy it. To what extent is MT currently being used to satisfy this growing demand for HQ translation? Quite obviously, very little. Although MT is being used today by more people than ever before, very few of these users are professional translators. This represents a major change, for a mere ten years ago, translators were still the principal target market for most MT vendors. What happened to bring about this change? For that matter, what happened to most of those MT vendors? The view we present is that the most promising strategy for HQ MT is to embed MT systems in translation environments where the translator retains full control over their output. In our opinion, this new type of interactive MT will achieve better acceptance levels among translators and significantly improve the prospects of MT{'}s commercial success in the translation industry.
The performance of machine translation technology after 50 years of development leaves much to be desired. There is a high demand for well performing and cheap MT systems for many language pairs and domains, which automatically adapt to rapidly changing terminology. We argue that for successful MT systems it will be crucial to apply data-driven methods, especially statistical machine translation. In addition, it will be very important to establish common test environments. This includes the availability of large parallel training corpora, well defined test corpora and standardized evaluation criteria. Thereby research results can be compared and this will open the possibility for more competition in MT research.
NICE is a machine translation project for low-density languages. We are building a tool that will elicit a controlled corpus from a bilingual speaker who is not an expert in linguistics. The corpus is intended to cover major typological phenomena, as it is designed to work for any language. Using implicational universals, we strive to minimize the number of sentences that each informant has to translate. From the elicited sentences, we learn transfer rules with a version space algorithm. Our vision for MT in the future is one in which systems can be quickly trained for new languages by native speakers, so that speakers of minor languages can participate in education, health care, government, and internet without having to give up their languages.
In this paper, organized in essay style, I first assess the situation of Machine Translation, which is characterized, on the one hand, by unsatisfied user expectations, and, on the other hand, by an ever increasing need for translation technology to fulfil the promises of the global knowledge society, which is promoted by almost all governments and industries worldwide. The assessment is followed by an outline of the design of a blueprint that describes possible steps of an MT evolution regarding short term, mid term and long term developments. Although some user communities might aim at an MT revolution, the evolutionary implementation of the different aspects of the blueprint fit seamless with the foundation that we are faced with in the assessment part. With the blueprint the thesis of this MT evolution essay is established, and the stage is opened for the antithesis in which I develop the points for an MT revolution. Finally, in the synthesis part I develop a combined view which then completes the discussion and the establishment of a blueprint for MT evolution.
This paper discusses the challenges which Chinese-English machine translation (MT) systems face in translating personal names. We show that the translation of names between Chinese and English is complicated by different factors, including orthographic, phonetic, geographic and social ones. Four existing systems were tested for their capability in translating personal names from Chinese to English. Test data embodying geographic and sociolinguistic differences were obtained from a synchronous Chinese corpus of news media texts. It is obvious that systems vary considerably in their ability to identify personal names in the source language and render them properly in the target language. Given the criticality of personal name translation to the overall intelligibility of a translated text, the coverage of personal names should be one of the important criteria in the evaluation of MT performance. Moreover, name translation, which calls for a hybrid approach, would remain a central issue to the future development of MT systems, especially for online and real-time applications.
This paper presents an overview of the broad-coverage, application-independent natural language generation component of the NLP system being developed at Microsoft Research. It demonstrates how this component functions within a multilingual Machine Translation system (MSR-MT), using the languages that we are currently working on (English, Spanish, Japanese, and Chinese). Section 1 provides a system description of MSR-MT. Section 2 focuses on the generation component and its set of core rules. Section 3 describes an additional layer of generation rules with examples that address issues specific to MT. Section 4 presents evaluation results in the context of MSR-MT. Section 5 addresses generation issues outside of MT.
This paper addresses the challenging problem of automatically evaluating output from machine translation (MT) systems in order to support the developers of these systems. Conventional approaches to the problem include methods that automatically assign a rank such as A, B, C, or D to MT output according to a single edit distance between this output and a correct translation example. The single edit distance can be differently designed, but changing its design makes assigning a certain rank more accurate, but another rank less accurate. This inhibits improving accuracy of rank assignment. To overcome this obstacle, this paper proposes an automatic ranking method that, by using multiple edit distances, encodes machine-translated sentences with a rank assigned by humans into multi-dimensional vectors from which a classifier of ranks is learned in the form of a decision tree (DT). The proposed method assigns a rank to MT output through the learned DT. The proposed method is evaluated using transcribed texts of real conversations in the travel arrangement domain. Experimental results show that the proposed method is more accurate than the single-edit-distance-based ranking methods, in both closed and open tests. Moreover, the proposed method could estimate MT quality within 3{\%} error in some cases.
The morphology of inflectional languages poses specific problems in the processing of morphological alternations. Regular alternations at morpheme boundaries can be elegantly captured by the use of rule formalisms based on the two-level morphology model. Stem alternations and completely irregular alternations at morpheme boundaries, however, need to be captured in some way in the lexicon. This paper presents four possible solutions to the problem and makes a claim in favor of one of them. The proposed approach makes use of feature bundles that contain the necessary linguistic information to uniquely identify allomorphic variations of stems in the lexicon. The proposal is an improvement in that it simplifies the representation of allomorphic variations in the lexicon by avoiding duplication of stem allomorphs to capture cross-combination of several morphosyntactic features in stem+flex sequences.
This paper describes a system for finding phrasal translation correspondences from parallel parsed corpus that are collections paired English and Japanese sentences. First, the system finds phrasal correspondences by Japanese-English translation dictionary consultation. Then, the system finds correspondences in remaining phrases by using sentences dependency structures and the balance of all correspondences. The method is based on an assumption that in parallel corpus most fragments in a source sentence have corresponding fragments in a target sentence.
The translation of Spanish Noun + preposition + Noun (NPN) constructions into English Noun-Noun (NN) compounds in many cases produces output with a higher level of fluency than if the NPN ordering is preserved. However, overgeneration of NN compounds can be dangerous because it may introduce ambiguity in the translation. This paper presents the strategy implemented in SPANAM to address this issue. The strategy involves dictionary coding of key words and expressions that allow or prohibit NN formation as well as an algorithm that generates NN compounds automatically when no dictionary coding is present. Certain conditions specified in the algorithm may also override the dictionary coding. The strategy makes use of syntactic and lexical information. No semantic coding is required. The last step in the strategy involves post-editing macros that allow the posteditor to quickly create or undo NN compounds if SPANAM did not generate the desired result.
We present an RST-based discourse annotation proposal used in the construction of a trial multilingual XML-tagged corpus of teaching material in Basque, English and Spanish. The corpus feeds an experimental multilingual document generation system for the web. The main contributions of this paper are an implementation of RST through XML metadata and the adoption of gross-grained RST to avoid non-isomorphism in multilingual corpora.
Globalisation is bringing translation and multilingual information processing to areas where it was previously unknown or relatively unimportant. Today, translation is not only important for reaching global audiences, it is becoming an indispensable component inside other systems and workflows. MALT (Modular Architecture for Linguistic Tools) represents a fresh approach to a relatively new problem; how to provide translation capabilities plus any other vital linguistic tools and components inside a common framework, possibly together with other external applications. MALT{'}s modular structure and multi-tier architecture simplify integration into complex workflow scenarios, and the functional separation in the MALT interface permits new components to be added extremely quickly. The applications and components running under MALT can be accessed locally, in a network environment or as engines of a distributed client-server system such as DTS.
This paper describes the experiences of SAP and PROMT specialists with applying the PROMT English-Russian machine translation system, the PROMT Terminology Manager Tool for automatic terminology extraction, and the TRADOS TWB translation memory system to the automated process of translation of SAP content from English into Russian.
We present a method for combining two bilingual dictionaries to make a third, using one language as a pivot. In this case we combine a Japanese-English dictionary with a Malay-English dictionary, to produce a Japanese-Malay dictionary suitable for use in a machine translation system. Our method differs from previous methods in its use of semantic classes to rank translation equivalents: word pairs with compatible semantic classes are preferred to those with dissimilar classes. We also experiment with the use of two pivot languages. We have made a prototype dictionary of over 75,000 pairs.
The continuous trend towards globalization means that even the most modern of industries must constantly re-evaluate its strategies and adapt to new technologies. This not only involves living up to the demands set by the product life cycles but also to find solutions satisfying additional internal needs. As a long-time supporter of MT and TM technology, SAP has shown that it can make productive use of competitive, commercial NLP products. As a first step, an integrated solution using TM together with MT was targeted. Having implemented different solutions for two types of documentation, the focus is now on not merely to integrate other technologies (e.g. terminology mining or controlled language) but to provide a uniform solution for processing any type of text. This involves not only supporting the needs of technical writers and translators but of all employees in their multilingual working environment.
This paper describes a program that automatically selects the best translation from a set of translations produced by multiple commercial machine translation engines. The program is simplified by assuming that the most fluent item in the set is the best translation. Fluency is determined using a trigram language model. Results are provided illustrating how well the program performs for human ranked data as compared to each of its constituent engines.
The ISLE project is a continuation of the long standing EAGLES initiative, carried out under the Human Language Technology (HLT) programme in collaboration between American and European groups in the framework of the EU-US International Research Co-operation, supported by NSF and EC. In this paper we concentrate on the current position of the ISLE Computational Lexicon Working Group (CLWG), whose activities aim at defining a general schema for a multilingual lexical entry (MILE), as the basis for a standard framework for multilingual computational lexicons. The needs and features of existing Machine Translation systems provide the main reference points for the process of consensual definition of the MILE. The overall structure of the MILE will be illustrated with particular attention to some of the issues raised for multilingual lexicons by the need of expressing complex transfer conditions among translation equivalents
This paper describes interNOSTRUM, a Spanish3Catalan machine translation system currently under development that achieves great speed through the use of finite-state technologies (so that it may be integrated with internet browsing) and a reasonable accuracy using an advanced morphological transfer strategy (to produce fast translation drafts ready for light postedition).
This paper describes a small-scale but organized attempt to evaluate output quality of several Japanese MT systems. The project also served as the first experiment of the implementation of the in-house MT evaluation guidelines created in 2000. Since time was limited and the budget was not infinite, it was launched with the following compact components: Five people; 300 source sentences per language pair; and 160 hours per evaluator. The quantitative results showed noteworthy phenomena. Although the test materials had been presented in a way that evaluators could not identify the performance of any particular system, the results were quite consistent. The scoring ratio that the two E-to-J evaluators employed was almost identical, while that of the J-to-E evaluators was similar. This indicates that high-quality output has universal appeal. Additionally, the evaluators noted that stronger systems, regardless of language pair, tended to be superior in source sentence analysis, target sentence arrangement, word choice, and lexicon entries whereas weaker systems tended to be inferior in these areas. As for language-pair comparison, the results indicate that English-to-Japanese systems may require more improvement than their counterparts, judging from the scores given and the number of unfound words recorded.
An important part of the development of any machine translation system is the creation of lexical resources. We describe an analysis of the dictionary development workflow and supporting tools currently in use and under development at Logos. This workflow identifies the component processes of: setting goals, locating and acquiring lexical resources, transforming the resources to a common format, classifying and routing entries for special processing, importing entries, and verifying their adequacy in translation. Our approach has been to emphasize the tools necessary to support increased automation and use of resources available in electronic formats, in the context of a systematic workflow design.
Our society is coming through a lot of changes that are connected, basically, with information. Maybe those languages that are present in this challenge will survive, and languages that will not meet those changes will dissapear. The Linguistics section of the Centro Ramón Piñeiro para a Investigación en Humanidades (CRP) is devoted to the development of basic language resources for Galician for trying to solve the gap existing in computational resources and to made it possible for Galician to be present in the new information society. The aim of this paper is to explain how we have developed a Spanish-Galician Machine Translation system, what tools we have made use of, which difficulties we have found in our task and what are the final results of the project.
Nowadays, there is a growing need for dissemination of documents in several languages. Machine translation is usually regarded as a possible solution for this, but so far it cannot provide acceptable translations of unedited texts. Several methods which involve human participation in computerized processes of translation have been proposed, but none has given really satisfactory results (except in some restricted contexts). In the UTL (Universal Translation Language) project, which we present here, we propose a new approach to multilingualization, based on the usage of an artificial unambiguous human language in which the human translator writes the source text, and then gives it to the machine to translate into other languages. The nature of this constructed language, which is optimized for this role, ensures the high quality of the results rendered by the computer.
This paper describes an evaluation experiment designed to determine groups of subjects who prefer reading MT outputs to reading the original text. Our approach can be applied to any language pairs, but we will explain the methodology by taking English to Japanese translation as an example. In the case of E-J MT, it can be assumed that main users are Japanese and that most of them have some knowledge of English. It is often the case, in the case of E-J MT systems, that those people who are comfortable with reading English do not find E-J MT outputs useful, and in many cases, they would rather prefer reading the original English text. On the other hand, E- J MT outputs prove to be useful to those who find it hard to read the original English texts. We have used the reading comprehension part of the Test Of English for International Communication (TOEIC) to determine the threshold English ability level, dividing these two user groups.
We present an automated, system-internal evaluation technique for linguistic representations in a large-scale, multilingual MT system. We use machine-learned classifiers to recognize the differences between linguistic representations generated from transfer in an MT context from representations that are produced by ``native'' analysis of the target language. In the MT scenario, convergence of the two is the desired result. Holding the feature set and the learning algorithm constant, the accuracy of the classifiers provides a measure of the overall difference between the two sets of linguistic representations: classifiers with higher accuracy correspond to more pronounced differences between representations. More importantly, the classifiers yield the basis for error-analysis by providing a ranking of the importance of linguistic features. The more salient a linguistic criterion is in discriminating transferred representations from ``native'' representations, the more work will be needed in order to get closer to the goal of producing native-like MT. We present results from using this approach on the Microsoft MT system and discuss its advantages and possible extensions.
The increasing interest in the statistical approach to Machine Translation is due to the development of effective algorithms for training the probabilistic models proposed so far. However, one of the open problems with statistical machine translation is the design of efficient algorithms for translating a given input string. For some interesting models, only (good) approximate solutions can be found. Recently, a dynamic programming-like algorithm for the IBM-Model 2 has been proposed which is based on an iterative process of refinement solutions. A new dynamic programming-like algorithm is proposed here to deal with more complex IBM models (models 3 to 5). The computational cost of the algorithm is reduced by using an alignment-based pruning technique. Experimental results with the so-called {``}Tourist Task{''} are also presented.
The semantics of verbs implies, as is known, a great number of difficulties, when it is to be represented in a computational lexicon. The Slavic languages are especially challenging in respect of this task because of the huge complexity of verbs, where the stems are combined with prefixes indicating aspect and Aktionsart. The current paper describes an approach to build PolVerbNet, a database for Polish verbs, considering the internal structure of the aspect-Aktionsart system. PolVerbNet is thus implemented in a larger English-Polish MT-system, which incorporates WordNet. We report our translation procedure and the system{'}s performance is evaluated and discussed.
Machine Translation (MT) systems that process unrestricted text should be able to deal with words that are not found in the MT lexicon. Without some kind of recognition, the parse may be incomplete, there is no transfer for the unfound word, and tests for transfers for surrounding words will often fail, resulting in poor translation. Interestingly, not much has been published on unfound- word guessing in the context of MT although such work has been going on for other applications. In our work on the IBM MT system, we implemented a far-reaching strategy for recognizing unfound words based on rules of word formation and for generating transfers. What distinguishes our approach from others is the use of semantic and syntactic features for both analysis and transfer, a scoring system to assign levels of confidence to possible word structures, and the creation of transfers in the transformation component. We also successfully applied rules of derivational morphological analysis to non-derived unfound words.
In this paper we present a methodology for automating the evaluation of the grammatical coverage of machine translation (MT) systems. The methodology is based on the importance of unfolded grammatical structures, which represent the most basic syntactic pattern for a sentence in a given language. A database of unfolded grammatical structures is built to evaluate the parser of any NLP or MT system. The evaluation results in an overall measure called the grammatical coverage. The results of implementing the above approach on three English-to-Arabic commercial MT systems are presented.
This paper presents a multilingual Natural Language Generation system that produces technical instruction texts in Bulgarian, Czech and Russian. It generates several types of texts, common for software manuals, in two styles. We illustrate the system{'}s functionality with examples of its input and output behaviour. We discuss the criteria and procedures adopted for evaluating the system and summarise their results. The system embodies novel approaches to providing multilingual documentation, ranging from the re-use of a large-scale, broad coverage grammar of English in order to develop the lexico-grammatical resources necessary for the generation in the three target languages, through to the adoption of a {`}knowledge editing{'} approach to specifying the desired content of the texts to be generated independently of the target languages in which those texts finally appear.
In Japanese constructions of the form [N1 no Adj N2], the adjective Adj modifies either N1 or N2. Determing the semantic dependencies of adjective in such phrase is an important task for machine translation. This paper describes a method for determining the adjective dependency in such constructions using decision lists, and inducing decision lists from training contexts with correct semantic dependencies and without. Based on evaluation, our method is able to determine adjective dependency with an precision of about 94{\%}. We further analyze rules in the induced decision lists and examine effective features to determine the semantic dependencies of adjectives.
This paper describes a prototype Japanese-to-Chinese automatic language translation system. ALT-J/C (Automatic Language Translator - Japanese-to-Chinese) is a semantic transfer based system, which is based on ALT-J/E (a Japanese-to-English system), but written to cope with Unicode. It is also designed to cope with constructions specific to Chinese. This system has the potential to become a framework for multilingual translation systems.
Patent summaries are machine-translated using bilingual term entries extracted from parallel texts for evaluation. The result shows that bilingual term entries extracted from 2,000 pairs of parallel texts which share a specific domain with the input texts introduce more improvements than a technical term dictionary with 38,000 entries which covers a broader domain. The result also shows that only 10 pairs of parallel texts found by similar document retrieval have comparable effects to the technical term dictionary, suggesting that parallel texts to be used do not need to be classified into fields prior to term extraction.
In this paper we present an overview of an approach developed at Microsoft Research to generate strings for named entities such as places and dates. This approach uses abstract representations as input. We first provide an overview of our system to identify named entities in text. Next we present our approach to generate these entities from abstract representations, known as {``}logical forms{''} in our system. We then focus on the generation of place names in Spanish. We discuss our technique to generate Spanish place names from a logical form where language-specific features, such as word order, or capitalization conventions do not exist. We finally present the details of a study that we carried out to help us make sound linguistic decisions in the generation of place names in Spanish.
This paper describes a method for disambiguating word senses by using semi-automatically constructed ontology. The ontology stores rich semantic constraints among 1,110 concepts, and enables a natural language processing system to resolve semantic ambiguities by making inferences with the concept network of the ontology. In order to acquire a reasonably practical ontology in limited time and with less manpower, we extend the existing Kadokawa thesaurus by inserting additional semantic relations into its hierarchy, which are classified as case relations and other semantic relations. The former can be obtained by converting valency information and case frames from previously-built electronic dictionaries used in machine translation. The latter can be acquired from concept co-occurrence information, which is extracted automatically from large corpora. In our practical machine translation system, our word sense disambiguation method achieved a 9.2{\%} improvement over methods which do not use an ontology for Korean translation.
Most MT lexicography is devoted to developing rules of the kind, {``}in context C, translate source-language word S as target-language word T{''}. Very many such rules are required, producing them is laborious, and MT companies standardly spend large sums on it. We present the WASP-Bench, a lexicographer's workstation for the rapid and semi-automatic development of such rule-sets. The WASP-Bench makes use of a large source-language corpus and state-of-the-art techniques for Word Sense Disambiguation. We show that the WSD accuracy is on a par with the best results published to date, with the advantage that the WASP-Bench, unlike other high- performance systems, does not require a sense-disambiguated training corpus as input. The WASP-Bench is designed to fit readily with MT companies' working practices, as it may be used for as many or as few source language words as present disambiguation problems for a given target.
This paper describes KORTERM{'}s test suite and their practicability. The test-sets have been being constructed on the basis of fine-grained classification of linguistic phenomena to evaluate the technical status of English-to-Korean MT systems systematically. They consist of about 5000 test-sets and are growing. Each test-set contains an English sentence, a model Korean translation, a linguistic phenomenon category, and a yes/no question about the linguistic phenomenon. Two commercial systems were evaluated with a yes/no test of prepared questions. Total accuracy rates of the two systems were different (50{\%} vs. 66{\%}). In addition, a comprehension test was carried out. We found that one system was more comprehensible than the other system. These results seem to show that our test suite is practicable.
In this paper, we present a way to integrate bilingual lexicons into an operational probabilistic translation assistant (TransType). These lexicons could be any resource available to the translator (e.g. terminological lexicons) or any resource statistically derived from training material. We describe a bilingual lexicon acquisition process that we developped and we evaluate from a theoretical point of view its benefits to a translation completion task.
The Pan American Health Organization (PAHO) is proud to present the latest release of its fully automatic Spanish-to-English and English-to-Spanish machine translation systems. SPANAM and ENGSPAN have been ported to the 32-bit Windows platform. The bilingual graphical user interface provides easy access to all the features of the system. The translation engine can be accessed in three different ways: file translation from the desktop or word processing application, sentence translation from within the dictionary update module, or cut-and-paste translation using an ActiveX component. Any user can view all of PAHO's dictionary entries (words, expressions, and rules), and dictionary coders can add new entries of every type and modify all but a small number of protected records. The system is designed to be used by translation professionals in an institutional setting. Administrative utilities include job accounting, dictionary update log, terminology import and export, and dictionary merge. Users can view and print side-by-side listings of source and target texts, lists of not-found words, and the parse of any sentence.
This paper takes a practical look at ways of combining language engineering tools to produce more accurate, {``}more human{''} automatic translations. Whilst specific products are discussed, the author believes that the methodology could be successfully implemented with different sets of tools.
This paper summarizes the current status of version 2 of the Open Lexicon Interchange Format (OLIF). As a natural extension of the OLIF prototype (OLIF version 1), version 2 has been modified with respect to content and formalization (e.g., it is now XML-compliant). These enhancements now make it possible to use OLIF in a variety of Natural Language Processing applications and general language technology environments (e.g., terminology management systems). At the time of writing, several industrial partners of the OLIF Consortium had already started work on implementing OLIF support. Details on OLIF can be found on www.olif.net.
Japanese and Uighur languages are agglutinative languages and they have many syntactical and morphological similarities. And roughly speaking, we can translate Japanese into Uighur sequentially by replacing Japanese words with corresponding Uighur ones after morphological analysis. However, we should translate agglutinated suffixes carefully to make correct translation, because they play important roles on both languages. In this paper, we pay attention to them and propose a Japanese-Uighur machine translation utilizing the agglutinative features of both languages. To deal with the agglutinative features, we use the derivational grammar, which makes the similarities clearer between both languages. This makes our system proposed here simple and systematical. We have implemented the machine translation system and evaluated how effectively our system works.
This paper describes the evaluation of Machine Translation (MT) System for use in a large company. To take into account the specific requirements of such an environment, a pragmatic approach for the evaluation was developed. It consists of five steps ranging from a specification of the evaluation process to the integration of the chosen MT system in a given infrastructure. The process includes a specification of MT evaluation criteria relevant to systems which have to be employed for a large customer base. The paper also shows the results of such an evaluation study which was recently carried out at CLS Corporate Language Services AG, where COMPRENDIUM is in the meantime being employed as corporate MT system.
The DARPA MT evaluations of the early 1990s, along with subsequent work on the MT Scale, and the International Standards for Language Engineering (ISLE) MT Evaluation framework represent two of the principal efforts in Machine Translation Evaluation (MTE) over the past decade. We describe a research program that builds on both of these efforts. This paper focuses on the selection of MT output features suggested in the ISLE framework, as well as the development of metrics for the features to be used in the study. We define each metric and describe the rationale for its development. We also discuss several of the finer points of the evaluation measures that arose as a result of verification of the measures against sample output texts from three machine translation systems.
We describe the automatic resolution of pronominal anaphora using KANT Controlled English (KCE) and the KANTOO English-to-Spanish MT system. Our algorithm is based on a robust, syntax-based approach that applies a set of restrictions and preferences to select the correct antecedent. We report a success rate of 89.6{\%} on a training corpus with 289 anaphors, and 87.5{\%} on held-out data containing 145 anaphors. Resolution of anaphors is important in translation, due to gender mismatches among languages; our approach translates anaphors to Spanish with 97.2{\%} accuracy.
Some Japanese clauses contain more than one argument ellipsis, and yet this fact has not adequately been accounted for in the study of ellipsis resolution in the current literature, which predominantly focus resolving one ellipsis per sentence. This paper proposes a method using a ``salient referent list'', which identifies the referents of such multiple argument ellipses as well as offers ellipsis resolution as a whole by considering contextual information.
In the framework of statistical machine translation (SMT), correspondences between the words in the source and the target language are learned from bilingual corpora on the basis of so-called alignment models. Among other things these are meant to capture the differences in word order in different languages. In this paper we show that SMT can take advantage of the explicit introduction of some linguistic knowledge about the sentence structure in the languages under consideration. In contrast to previous publications dealing with the incorporation of morphological and syntactic information into SMT, we focus on two aspects of reordering for the language pair German and English, namely question inversion and detachable German verb prefixes. The results of systematic experiments are reported and demonstrate the applicability of the approach to both translation directions on a German-English corpus.
We describe methods for translating a text given in multiple source languages into a single target language. The goal is to improve translation quality in applications where the ultimate goal is to translate the same document into many languages. We describe a statistical approach and two specific statistical models to deal with this problem. Our method is generally applicable as it is independent of specific models, languages or application domains. We evaluate the approach on a multilingual corpus covering all eleven official European Union languages that was collected automatically from the Internet. In various tests we show that these methods can significantly improve translation quality. As a side effect, we also compare the quality of statistical machine translation systems for many European languages in the same domain.
An increasing interest in multi-lingual translation systems demands a reconsideration of the development costs of machine translation engines for language pairs. This paper proposes an approach that reuses the existing translation knowledge resources of high-quality translation engines for translation into different, but related languages. The lexical information of the target representation is utilized to generate the corresponding translation in the related language by using a transfer dictionary for the mapping of words and a set of heuristic rules for the mapping of structural information. Experiments using a Japanese-English translation engine for the generation of German translations show a minor decrease of up to 5{\%} in the acceptability of the German output compared with the English translation of unseen Japanese input.
This paper presents a snapshot of how the Commission's MT system (EC SYSTRAN) is used today and a glimpse of how that picture will change tomorrow. It looks in turn at: the origins of the system; how it is accessed; who requests MT and why; how users can influence the quality of output; the Rapid Post-editing Service; and the latest usage statistics, which augur well for the future. The paper closes with a look at that future, touching on the move to a new computer platform and plans for new language pairs, concluding that after twenty-five years of development, MT has become an integral part of the Commission's working environment.
Past research has shown that the ideal MT system should be modular and devoid of language pair specific information in its design. We describe here the assembly of TAMTAM (Traduction Automatique Microsoft), the French-English research MT system under development at Microsoft, which was constructed from a combination of pre-existing rule-based components and automatically created components. At this stage, the system has not been adapted either computationally or linguistically to the French-English context and yet it performs only slightly below the French-English Systran system in independent blind human evaluations
For a professional user of MT, quality, performance and cost efficiency are critical. It is therefore surprising that only little attention {--} both in theory and in practice - has been given to the task of post-editing machine translated texts. This paper will focus on this important user aspect and demonstrate that substantial savings in time and effort can be achieved by implementing intelligent automatic tools. Our point of departure is the PaTrans MT-system, developed by CST and used by the Danish translation company Lingtech. An intelligent post-editing facility, Ape, has been developed and added to the system. We will outline and discuss this mechanism and its positive effects on the output. The underlying idea of the intelligent post-editing facility is to exploit the lexical and grammatical knowledge already present in the MT-system{'}s linguistic components. Conceptually, our approach is general, although its implementation remains system specific. Surveys of post-editor satisfaction and cost-efficiency improvements, as well as a quantitative, benchmark-based evaluation of the effect of Ape demonstrate the success of the approach and encourage further development.
Some authors (Simard et al.; Melamed; Danielsson {\&} Mühlenbock) have suggested measures of similarity of words in different languages so as to find extra clues for alignment of parallel texts. Cognate words, like {`}Parliament{'} and {`}Parlement{'}, in English and French respectively, provide extra anchors that help to improve the quality of the alignment. In this paper, we will extend an alignment algorithm proposed by Ribeiro et al. using typical contiguous and non-contiguous sequences of characters extracted using a statistically sound method (Dias et al.). With these typical sequences, we are able to find more reliable correspondence points and improve the alignment quality without recurring to heuristics to identify cognates.
We describe MSR-MT, a large-scale example-based machine translation system under development for several language pairs. Trained on aligned English-Spanish technical prose, a blind evaluation shows that MSR-MT{'}s integration of rule-based parsers, example based processing, and statistical techniques produces translations whose quality in this domain exceeds that of uncustomized commercial MT systems.
Using an invented case study, the paper describes how multilingual translation projects can be managed efficiently with an enterprise resource management tool called {``}LTC Organiser{''}, which was developed specifically for the particular requirements of the language industry. The talk will describe the most important aspects of the integrated solution, such as client and supplier management, project and finance management, managing tools used in the translation process, reporting facilities, security and user management, directory management, sort and search facilities as well as web functionality available at several levels.
A finite-state, rule-based morphological analyser is presented here, within the framework of machine translation system TAVAL. This morphological analyser introduces specific features which are particularly useful for translation, such as the detection and morphological tagging of word groups that act as a single lexical unit for translation purposes. The case where words in one such group are not strictly contiguous is also covered. A brief description of the Spanish-to-Catalan and Catalan-to-Spanish translation system TAVAL is given in the paper.
In this paper, we present the design of the new generation Systran translation systems, currently utilized in the development of English-Hungarian, English-Polish, English-Arabic, French-Arabic, Hungarian-French and Polish-French language pairs. The new design, based on the traditional Systran machine translation expertise and the existing linguistic resources, addresses the following aspects: efficiency, modularity, declarativity, reusability, and maintainability. Technically, the new systems rely on intensive use of state-of-the-art finite automaton and formal grammar implementation. The finite automata provide the essential lookup facilities and the natural capacity of factorizing intuitive linguistic sets. Linguistically, we have introduced a full monolingual description of linguistic information and the concept of implicit transfer. Finally, we present some by-products that are directly derived from the new architecture: intuitive coding tools, spell checker and syntactic tagger.
In this article we present the concept of {``}implicit transfer{''} rules. We will show that they represent a valid compromise between huge direct transfer terminology lists and large sets of transfer rules, which are very complex to maintain. We present a concrete, real-life application of this concept in a customization project (TOLEDO project) concerning the automatic translation of Autodesk (ADSK) support pages. In this application, the alignment is moreover combined with a graph representation substituting linear dictionaries. We show how the concept could be extended to increase coverage of traditional translation dictionaries as well as to extract terminology from large existing multilingual corpora. We also introduce the concept of ``alignment dictionary'' which seems promising in its ability to extend the pragmatic limits of multilingual dictionary management.
This paper describes a comprehensive translation environment build on the Internet. This environment is designed not only to translate web pages but also to support translation work on the web. We first introduce a basic idea and implementation of this environment and then compare it to conventional machine translation (MT) systems available on the web and translation memories.
Translation memory systems (TMS) are a family of computer tools whose purpose is to facilitate and encourage the re-use of existing translations. By searching a database of past translations, these systems can retrieve the translation of whole segments of text and propose them to the translator for re-use. However, the usefulness of existing TMS{'}s is limited by the nature of the text segments that that they are able to put in correspondence, generally whole sentences. This article examines the potential of a type of system that is able to recuperate the translation of sub-sentential sequences of words.
This paper describes how information technology is used by the Translation Department of PricewaterhouseCoopers in Madrid to optimise translation processes. It commences by describing a mechanism for handling workflow via the corporate network, designed to maximise speed and efficiency in translation requests and also to function as an automated record for administration purposes. This is followed by an appraisal of the CAT system used in the Translation Department, namely the Trados Workbench and related applications. Finally, an ongoing project for making MT (Systran) available to PwC employees around the world over the Firm's intranet is outlined.
The main goal of the present paper is to propose new schemes for the overall evaluation of a speech translation system. These schemes are expected to support and improve the design of the target application system, and precisely determine its performance. Experiments are conducted on the Japanese-to-English speech translation system ATR-MATRIX, which was developed at ATR Interpreting Telecommunications Research Laboratories. In the proposed schemes, the system{'}s translations are compared with those of a native Japanese taking the Test of English for International Communication (TOEIC), which is used as a measure of one{'}s speech translation capability. Subjective and automatic comparisons are made and the results are compared. A regression analysis on the subjective results shows that the speech translation capability of ATR-MATRIX matches a Japanese person scoring around 500 on the TOEIC. The automatic comparisons also show promising results.
In this paper, we would like to present an approach to construct a huge Bilingual Knowledge Bank (BKB) from an English Malay bilingual dictionary based on the idea of synchronous Structured String-Tree Correspondence (SSTC). The SSTC is a general structure that can associate an arbitrary tree structure to string in a language as desired by the annotator to be the interpretation structure of the string, and more importantly is the facility to specify the correspondence between the string and the associated tree which can be non-projective. With this structure, we are able to match linguistic units at different inter levels of the structure (i.e. define the correspondence between substrings in the sentence, nodes in the tree, subtrees in the tree and sub-correspondences in the SSTC). This flexibility makes synchronous SSTC very well suited for the construction of a Bilingual Knowledge Bank we need for the English-Malay MT application.
A new system for statistical natural language translation for languages with similar grammar is introduced. Specifically, it can be used with Romanic Languages, such as French, Spanish or Catalan. The statistical translation uses two sources of information: a language model and a translation model. The language model used is a standard trigram model. A new approach is defined in the translation model. The two main properties of the translation model are: the translation probabilities are computed between groups of words and the alignment between those groups is monotone. That is, the order between the word groups in the source sentence is conserved in the target sentence. Once, the translation model has been defined, we present an algorithm to infer its parameters from training samples. The translation process is carried out with an efficient algorithm based on stack-decoding. Finally, we present some translation results from Catalan to Spanish and compare our model with other conventional models.
This paper describes a tool designed to assess the machine translatability of English source texts by assigning a translatability index to both individual sentences and the text as a whole. The tool is designed to be both stand-alone and integratable into a suite of other tools which together help to improve the quality of professional translation in the preparatory phase of the translation workflow. Assessing translatability is an important element in ensuring the most efficient and cost effective use of current translation technology, and the tool must be able to quickly determine the translatability of a text without itself using too many resources. It is therefore based on rather simple tagging and pattern matching technologies which bring with them a certain level of indeterminacy. This potential disadvantage can, however, be offset by the fact that an annotated version of the text is simultaneously produced to allow the user to interpret the results of the checker.
The reliable detection of sentence boundaries in running text is one of the first important steps in preparing an input document for translation. Although this is often neglected, it is necessary to obtain a translation with a high degree of quality. In this paper, we present a comparison of different paradigms for the detection of sentence boundaries in written text. We compare three different approaches: Directly encoding the knowledge in a program, a rule-based system relying on regular expressions to describe boundaries, and a statistical maximum-entropy learning algorithm to obtain knowledge about boundaries. Using the statistical system, we obtain a recall of 98.14{\%}, classifying boundaries of six types, and using a training corpus of under 10,000 sentences.
An automatic translation quality evaluation method is proposed. In the proposed method, a parallel corpus is used to query translation answer candidates. The translation output is evaluated by measuring the similarity between the translation output and translation answer candidates with DP matching. This method evaluates a language translation subsystem of the Japanese-to-English ATR-MATRIX speech translation system developed at ATR Interpreting Telecommunications Research Laboratories. Discriminant analysis is then carried out to examine the evaluation performance of the proposed method. Experimental results show the effectiveness of the proposed method. The discriminant ratio is 83.5{\%} for 2-class discrimination between absolutely correct and less appropriate translations classified subjectively. Also discussed are issues of the proposed method when it is applied to speech translation systems which inevitably make recognition errors.
Evaluation of machine translation is one of the most important issues in this field. We have already proposed a quantitative evaluation of machine translation system. The method was roughly that an example sentence in Japanese is machine translated into English, and then into Japanese using several systems, and that the comparison of output Japanese sentences with the original Japanese sentence is done for the word identification, the correctness of the modification, the syntactic dependency, and the parataxis. By calculating the score, we could quantitatively evaluate the English machine translation. However, the extraction of word identification etc. was done by human, and the fact affects the correctness of evaluation. In order to solve this problem, we developed an automatic evaluation system. We report the detail of the system in this paper..
Pre-processing of bilingual corpora plays an important role in Example-Based Machine Translation (EBMT) and Statistical-Based Machine Translation (SBMT). For our Mandarin-English EBMT system, pre-processing includes segmentation for Mandarin, bracketing for English and building a statistical dictionary from the corpora. We used the Mandarin segmenter from the Linguistic Data Consortium (LDC). It uses dynamic programming with a frequency dictionary to segment the text. Although the frequency dictionary is large, it does not completely cover the corpora. In this paper, we describe the work we have done to improve the segmentation for Mandarin and the bracketing process for English to increase the length of English phrases. A statistical dictionary is built from the aligned bilingual corpus. It is used as feedback to segmentation and bracketing to re-segment / re-bracket the corpus. The process iterates several times to achieve better results. The final results of the corpus pre-processing are a segmented/bracketed aligned bilingual corpus and a statistical dictionary. We achieved positive results by increasing the average length of Chinese terms about 60{\%} and 10{\%} for English. The statistical dictionary gained about a 30{\%} increase in coverage.
Following the guidelines for MT evaluation proposed in the ISLE taxonomy, this paper presents considerations and procedures for evaluating the integration of machine-translated segments into a larger translation workflow with Translation Memory (TM) systems. The scenario here focuses on the software localisation industry, which already uses TM systems and looks to further streamline the overall translation process by integrating Machine Translation (MT). The main agents involved in this evaluation scenario are localisation managers and translators; the primary aspects of evaluation are speed, quality, and user acceptance. Using the penalty feature of Translation Memory systems, the authors also outline a possible method for finding the {``}right place{''} for MT produced segments among TM matches with different degrees of fuzziness.
This paper reports the first results of an on-going research on evaluation of Machine Translation quality. The starting point for this work was the framework of ISLE (the International Standards for Language Engineering), which provides a classification for evaluation of Machine Translation. In order to make a quantitative evaluation of translation quality, we pursue a more consistent, fine-grained and comprehensive classification of possible translation errors and we propose metrics for sentence level errors, specifically lexical and syntactic errors.
It is often assumed that knowledge of both the source and target languages is necessary in order to evaluate the output of a machine translation (MT) system. This paper reports on an experimental evaluation of Chinese-English MT and Spanish-English MT from output specifically designed for evaluators who do not read or speak Chinese or Spanish. An outline of the characteristics measured and evaluation follows.
In this paper some of the problems encountered in designing an evaluation for an MT system will be examined. The source text, in French, provided by INRA (Institut National pour la Recherche Agronomique i.e. National Institute for Agronomic Research) deals with biotechnology and animal reproduction. It has been translated into English. The output of the system (i.e. the result of the assembling of several components), as opposed to its individual modules or specific components (i.e. analysis, generation, grammar, lexicon, core, etc.), will be evaluated. Moreover, the evaluation will concentrate on translation quality and its fidelity to the source text. The evaluation is not comparative, which means that we tested a specific MT system, not necessarily representative of other MT systems that can be found on the market.
Evaluation guidelines for a given domain or task must be rooted in a general model for software evaluation. In this paper, we consider as a starting point the ISO/EAGLES guidelines for natural language processing software evaluation, which we rst summarize. From these considerations, we derive several principles for a taxonomy aimed at the evaluation of machine translation (MT) systems. Then, we compare two editions of such a taxonomy, arguing in particular for a dichotomy relating user needs to system characteristics. We also outline the software infrastructure underlying the electronic publication and updating of the taxonomy, and conclude with a brief overview of the workshops that were staged to test, modify and disseminate the taxonomy. 
The main goal of the work presented in this paper is to find an inexpensive and automatable way of predicting rankings of MT systems compatible with human evaluations of these systems expressed in the form of Fluency, Adequacy or Informativeness scores. Our approach is to establish whether there is a correlation between rankings derived from such scores and the ones that can be built on the basis of automatically computable attributes of syntactic or semantic nature. We present promising results obtained on the DARPA94 MT evaluation corpus.
This paper reports on research which aims to test the efficacy of applying automated evaluation techniques, originally designed for human second language learners, to machine translation (MT) system evaluation. We believe that such evaluation techniques will provide insight into MT evaluation, MT development, the human translation process and the human language learning process. The experiment described here looks only at the intelligibility of MT output. The evaluation technique is derived from a second language acquisition experiment that showed that assessors can differentiate native from non-native language essays in less than 100 words. Particularly illuminating for our purposes is the set of factor on which the assessors made their decisions. We duplicated this experiment to see if similar criteria could be elicited from duplicating the test using both human and machine translation outputs in the decision set. The encouraging results of this experiment, along with an analysis of language factors contributing to the successful outcomes, is presented here.
This paper reports the results of an experiment in machine translation (MT) evaluation, designed to determine whether easily/rapidly collected metrics can predict the human generated quality parameters of MT output. In this experiment we evaluated a system{'}s ability to translate named entities, and compared this measure with previous evaluation scores of fidelity and intelligibility. There are two significant benefits potentially associated with a correlation between traditional MT measures and named entity scores: the ability to automate named entity scoring and thus MT scoring; and insights into the linguistic aspects of task-based uses of MT, as captured in previous studies.
Work on comparing a set of linguistic test scores for MT output to a set of the same tests{'} scores for naturally-occurring target language text (Jones and Rusk 2000) broke new ground in automating MT Evaluation. However, the tests used were selected on an ad hoc basis. In this paper, we report on work to extend our understanding, through refinement and validation, of suitable linguistic tests in the context of our novel approach to MTE. This approach was introduced in Miller and Vanni (2001a) and employs standard, rather than randomly-chosen, tests of MT output quality selected from the ISLE framework as well as a scoring system for predicting the type of information processing task performable with the output. Since the intent is to automate the scoring system, this work can also be viewed as the preliminary steps of algorithm design.
Attempts to formulate methods of automatically evaluating machine translation (MT) have generally looked at some attrinbute of translation and then tried, explicitly or implicitly, to extrapolate the measurement to cover a broader class of attributes. In particular, some studies have focused on measuring fidelity of translation, and inferring intelligibility from that, and others have taken the opposite approach. In this paper we examine the more fundamental question of whether, and to what extent, the one attribute can be predicted by the other. As a starting point we use the 1994 DARPA MT corpus, which has measures for both attributes, and perform a simple comparison of the behavior of each. Two hypotheses about a predictable inference between fidelity and intelligibility are compared with the comparative behavior across all language pairs and all documents in the corpus.
Approaches to the automation of machine translation (MT) evaluation have attempted, or presumed, to connect some rapidly measurable phenomenon with general attributes of the MT output and/or system. In particular, measurements of the fluency of output are often asserted to be predictive of the usefulness of MT output in information-intensive, downstream tasks. The connections between the fluency ({``}intelligibility{''}) of translation and its informational adequacy ({``}fidelity{''}) are not actually straightforward. This paper discussed a small experiment in isolating a particular contrastive linguistic phenomena common to both French-English and Spanish-English pairs, and attempts to associate that behavior in machine and human translations with known fidelity properties of those translations. Our results show a definite correlative trend.
This paper describes a Machine Translation (MT) evaluation experiment where emphasis is placed on the quality of output and the extent to which it is geared to different users' needs. Adopting a very specific scenario, that of a multilingual international organisation, a clear distinction is made between two user classes: translators and administrators. Whereas the first group requires MT output to be accurate and of good post-editable quality in order to produce a polished translation, the second group primarily needs informative data for carrying out other, non-linguistic tasks, and therefore uses MT more as an information-gathering and gisting tool. During the experiment, MT output of three different systems is compared in order to establish which MT system best serves the organisation's multilingual communication and information needs. This is a comparative usability- and adequacy-oriented evaluation in that it attempts to help such organisations decide which system produces the most adequate output for certain well-defined user types. To perform the experiment, criteria relating to both users and MT output are examined with reference to the ISLE taxonomy. The experiment comprises two evaluation phases, the first at sentence level, the second at overall text level. In both phases, evaluators make use of a 1-5 rating scale. Weighted results provide some insight into the systems' usability and adequacy for the purposes described above. As a conclusion, it is suggested that further research should be devoted to the most critical aspect of this exercise, namely defining meaningful and useful criteria for evaluating the post-editability and informativeness of MT output.
Previous work has shown that grammars and similar structure can be induced from unlabeled text (both monolingually and bilingually), and that the performance of an example-based machine translation (EBMT) system can be substantially enhanced by using clustering techniques to determine equivalence classes of individual words which can be used interchangeably, thus converting translation examples into templates. This paper describes the combination of these two approaches to further increase the coverage (or conversely, decrease the required training text) of an EBMT system. Preliminary results show that a reduction in required training text by a factor of twelve is possible for translation from French into English. 
This paper presents an algorithm for generating and ltering an invertible and structural analogous translation grammar from bilingual aligned and linguistically bracketed texts. The algorithm is discussed in general terms and applied to German-English alignments. It is shown that the induction of structural analogous translation grammars can lead to disambiguation of meaning and correction of bracketing errors. 
An approach to Example-Based Machine Translation is presented which operates by extracting translation patterns from a bilingual corpus aligned at the level of the sentence. This is carried out using a language-neutral recursive machine-learning algorithm based on the principle of similar distributions of strings. The translation patterns extracted represent generalisations of sentences that are translations of each other and, to some extent, resemble transfer rules but with fewer constraints. The strings and variables, of which translations patterns are composed, are aligned in order to provide a more refined bilingual knowledge source, necessary for the recombination phase. A non-structural approach based on surface forms is error prone and liable to produce translation patterns that are false translations. Such errors are highlighted and solutions are proposed by the addition of external linguistic resources, namely morphological analysis and part-of-speech tagging. The amount of linguistic resources added has consequences for computational complexity and portability.
Translation systems that automatically extract transfer mappings (rules or examples) from bilingual corpora have been hampered by the difficulty of achieving accurate alignment and acquiring high quality mappings. We describe an algorithm that uses a best-first strategy and a small alignment grammar to significantly improve the quality of the mappings extracted. For each mapping, frequencies are computed and sufficient context is retained to distinguish competing mappings during translation. Variants of the algorithm are run against a corpus containing 200K sentence pairs and evaluated based on the quality of resulting translations.
One key to the success of EBMT is the removal of the boundaries limiting the potential of translation memories. To bring EBMT to fruition, researchers and developers have to go beyond the self-imposed limitations of what is now traditional, in computing terms almost old fashioned, TM technology. Experiments have shown that the probability of finding exact matches at phrase level is higher than the probability of finding exact matches at the current TM segment level. We outline our implementation of a linguistically enhanced translation memory system (or Phrasal Lexicon) implementing phrasal matching. This system takes advantage of the huge and underused resources available in existing translation memories and develops a traditional TM into a sophisticated example-based machine translation engine which when integrated into a hybrid MT solution can yield significant improvements in translation quality.
This paper looks at EBMT from the perspective of the Case-based Reasoning (CBR) paradigm. We attempt to describe the task of machine translation (MT) seen as a potential application of CBR, and attempt to describe MT in standard CBR terms. The aim is to see if other applications of CBR can suggest better ways to approach EBMT.
We maintain that the essential feature that characterizes a Machine Translation approach and sets it apart from other approaches is the kind of knowledge it uses. From this perspective, we argue that Example-Based Machine Translation is sometimes characterized in terms of inessential features. We show that Example-Based Machine Translation, as long as it is linguistically principled, significantly overlaps with other linguistically principled approaches to Machine Translation. We make a proposal for translation knowledge bases that make such an overlap explicit.
The basics of classical formal language theory are introduced, as well as a wide coverage is given of some new nonstandard devices motivated in molecular biology, which are challenging traditional conceptions, are making the theory revived and could have some linguistic relevance. Only deﬁnitions and a few results are presented, without including any proof. The chapter can be proﬁtably read without any special previous mathematical background. A long list of references completes the chapter, which intends to give a ﬂavour of the ﬁeld and to encourage young researchers to go deeper into it. 
Abstracts constituted from extracted sentences contain unneeded information that may be deleted, or compressed into simpler units. By comparing full text sentences used in abstracting with correspond-ing sentences in abstract, the study found such units to include metadiscourse phrases, parenthetical texts, redundant units inserted for emphasis, or are repetitions. Apposed texts and units such as modifiers and relative clauses which provide details and precision in the full text, but are out of place in an abstract, are also deleted.
We apply word sense disambiguation to the definitions in a Spanish explanatory dictionary. To calculate the scores of word senses basing on the context (which in our case is the dictionary definition), we use a modification of Lesk{'}s algorithm. The algorithm relies on a comparison between two words. In the original Lesk{'}s algorithm, the comparison is trivial: two words are either the same lexeme or not; our modification consists in fuzzy (weighted) comparison using a large synonym dictionary and a simple derivational morphology system. Application of disambiguation to dictionary definitions (in contrast to usual texts) allows for some simplifications of the algorithm, e.g., we do not have to care of context window size.
In text generation, studies on aggregation often focus on the use of connectives to combine short made-up sentences. But connectives restrict the number of units that may be combined at any one time. So, how does information get condensed into fewer units without excessive use of connectives? From a comparison of document and abstract, this reconnaissance study reports on some preferred patterns in aggregation when authors write abstracts for journal articles on biology. The paper also discusses some prerequisites and difficulties anticipated for abstracting systems. More sentences were aggregated without than with the use of an explicit sign, such as a connective or a (semi-)colon.
The paper presents a system for querying (in natural language) a set of text documents from a limited domain. The domain knowledge, represented in description logics (DL), is used for filtering the documents returned as answer and it is extended dynamically (when new concepts are identified in the texts), as result of DL inference mechanisms. The conceptual hierarchy is built semi-automatically from the texts. Concept instances are identified using shallow natural language parsing techniques.
We present a system for extraction of temporal expressions from French texts. The identification of the temporal expressions is based on a context-scanning strategy (CSS) which is carried out by two complementary techniques: search for regular expressios and left-to-right and right-to-left local chartparsing. A number of semantic and distant-dependency constraints have been integrated to the chartparsing procedure in order to improve the precision of the system.
Weighted automata and transducers are used in a variety of applications ranging from automatic speech recognition and synthesis to computational biology. They give a unifying framework for the representation of the components of complex systems. This provides opportunities for the application of general optimization algorithms such as determinization, epsilon-removal and minimization of weighted transducers. We give a brief survey of recent advances in language processing with weighted automata and transducers, including an overview of speech recognition with weighted transducers and recent algorithmic results in that field. We also present new results related to the approximation of weighted context-free grammars and language recognition with weighted automata.
