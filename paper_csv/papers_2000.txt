We set out to empirically identify the range and frequency of basic verb alternation types in Japanese, through analysis of the Goi-Taikei Japanese pattern-based valency dictionary. This is achieved through comparison of the selectional preference annotation on corresponding case slots, based on the assumption that selectional preferences are preserved under alternation. Three separate extraction methods are considered, founded around: (1) simple match of selectional restrictions; (2) selectional restriction matching, with recourse to penalised backing-off; and (3) semantic density, again with recourse to backing-off. 
The Phonetic and the Homophone Error problem in a language have been characterized as a symbol substitution problem. Phonetically equivalent symbols or symbol combinations in the language are grouped together. Each group or a number of related groups give(s) rise to a dictionary or a number of dictionaries. A new design methodology for Orthographic Dictionaries in alphabetic languages has been described. The dictionaries include the root words. The meanings are stored only in case of Homophone words. Words are sorted on the basis of a Phonetic Ordering Scheme. The dictionaries are being used to detect and correct the Phonetic Error and the Homophone Error in isolated words of Bengali. The methodologies described in this paper can be used in developing spell-checkers not only for other Indian languages which have evolved from the ancient Brahmi script and have a common phonetic structure but also for other alphabetic languages with suitable modifications. 
This paper proposes a novel approach in clustering texts automatically into coherent segments. A set of mutual linguistic constraints that largely determines the similarity of meaning among lexical items is used and a weight function is devised to incorporate the diversity of linguistic bonds among the text. A computational method of extracting the gist from a higher order structure representing the tremendous diversity of interrelationship among items is presented. Topic boundaries between segments in a text are identified. Our text segmentation is regarded as a process of identifying the shifts from one segment cluster to another. The experimental results show that the combination of these constraints is capable to address the topic shifts of texts. 1. INTRODUCTION Recent research in textual information science has increasingly turned to text processing beyond sentence level, partly because text analysis is manifestly necessary, and partly through implicit or explicit endorsement that negotiation of meaning in verbal transactions is achieved within the framework of text. Text has a rich structure in which sentences are grouped and related to one another in a variety of ways. A text is usually taken to be a piece of connected and meaningful sentences, rather than just a collection or sequence of sentences, connected only by contiguity. It is clear that text cannot simply be defined as language above the sentence (Winograd, 1972:65). Nor can we assume that stretches of language use which are isomorphic with sentences are the sole business of the grammarian, for these too are realizations of the text process. To understand a text, one must understand the relations between its parts and determine how the various pieces fit together. Clearly, these parts are not just the individual sentences: rather, sentences are joined together to form larger units, which in their turn may be the building blocks of yet larger units. In information science, there is much ongoing research in finding textual regularities on the basis of empirical investigations. Analyzing text structure often calls for more than understanding individual sentences; it usually involves a process of making connections between sentences and sustaining that process as the text progresses (Grosz & Sidner, 1986). Current work also shows that centrality of text should no longer be defined in terms of any simplistic linguistic rules, but rather in terms of linguistic ties which exist among text segments (Stoddard, 1991). Among other kinds of interrelationship which a text may exhibit, cohesion has a prominent role in the understanding of its structure. As advocated by Halliday and Hasan (1976), cohesion can be a unity-creating device in texts. Lexical cohesion, which is a special kind of cohesion, investigates the repetitive sequences of lexically linked (co-articulated) items and their relations to the core sentences. Lexical cohesion has been identified as an important feature which underlines the structure of a wholesome text, distinguishing it from a non-text. Lexical cohesion, including lexical repetition as well as rhetorical continuity in conjunction with other related overt and covert linguistic markers, contributes to textual coherence by creating cohesive ties within the text. If the lexical items in a text can be related to the preceding or to following items, obviously, the text is seen more closely knit together than a text where such relationships do not exist. It has been ascertained that sentences with the greatest degree of lexical cohesion could be used to reflect the textual structure (Morris & Hirst, 1991).  Moreover, it is commonly believed that the recurrence of semantically related lexical items across the sentences could be used as an aid to identifying the core sentences which are characterized by their centrality and their expressive power. At the same time, while it is important for readers to be able to trace continuities in the entities under discussion, it is equally important to locate and understand the breaks in continuity. However, little research has demonstrated the functions of lexical cohesion in text segmentation and no computational theory or objective measure has as yet emerged in analyzing text structure on this basis. Given the increasing recognition of text structure in the fields of information retrieval in unpartitioned text, lexical cohesion also reveals the textual segmentability which means how texts are seen not as a continuous whole but as a complex grouping of larger pieces. There is a mounting demand for the indepth study of an implementable quantitative model on lexical cohesion in text segmentation. This research is to devise a quantitative model of text segmentation based on the study of lexical cohesion as well as other related linguistic features. What distinguishes it from previous studies is that attention is not primarily focused on itemizing cohesive features across a text but on investigating how they combine with other linguistic features to organize text into a coherent whole. We propose a novel approach in textual information science. The approach will identify discoursally salient text segment boundaries. The main objectives of this research are (i) to investigate patterns of cohesion in expository texts in order to test hypothesis about the textual continuity; (ii) to devise a measure in order to analyze the interrelations between each segment; (iii) to formulate a computational model and an objective measure in analyzing textual continuity; (iv) to propose and implement a method for the segmentation of texts into thematically coherent units. Demonstrations will be focused on Chinese expository and argumentative texts which usually consist of long sentences with little punctuation and textual demarcation. Section 2 describes the bonding analysis among the text. Our algorithm in textual segmentation identification is described in Section 3. The experimental results are presented in Section 4, followed by a conclusion. 2. BONDING ANALYSIS A text is composed of a number of paragraphs, each of which is made up of a number of segments. Given that our intention is to explore the means by which various linguistic factors link segments, it is necessary to have a formalism for representing the links that will accurately reflect the non-linear complexity of a discourse and, at the same time, permit us to handle and interpret them conveniently. In our consideration of how discourse structure is expressed, we have already established a discourse network that is employed to represent the inter-sentential relationships existing among the sentences. [DEFINITION 1] A discourse network D is defined by a set of discourse segments, which stands in functional relations to each sentence in the discourse. The discourse network is represented as a graph characterized by a 5-tuple (Chan & Franklin, 1996). D = (G, T, A, E, W) where • G is a finite set of the discourse segments composing the discourse. • T is a finite set of lexical items (hereafter, called token) composing the discourse segments. • A is a set of arcs representing the inter-sentential relations amongst the discourse segments. • E is a set of weights of the arcs, lies between [0,1]. • w is a function W: A ---> E which assigns lateral weights to arcs. In our discourse network, the lateral weights between the arcs among the discourse segments are defined by linguistic clues. Let g„ g, E G be two discourse segments in the discourse network D, each representing a different segment. If both of these segments are interrelated, the connection between them, i.e., is assigned a large positive weight. On the other hand, it is reasonable to assume that syntactic function words do not denote new topics, whereas new semantic content words (nouns, main verbs, adjectives, and some adverbs) do. Given this assumption in our segmentation, a segment could be generated for a document simply by removing all function words from those tokens. Our bonding analysis for text segmentation is  shown in the algorithm as follows. Partition the text to elemental)) segments. A segment is a sentence with all the function words removed. While more that one segment left Do Identify the possible links between every discourse segments g, Assign the lateral weights among the segments in the discourse network D under the three principles of lexical cohesion as shown below. End While Compute text boundary using cluster identification technique.  One aspect of world knowledge essential to constructing the network is to identify when two lexical items in the segments are related. Several major types of relationships provide a text with cohesiveness by relating its lexical items to one another: (i) identical lexical items MIA(currency) & vo.(currency)]; (ii) synonym mc {(currency) & Kin(exchange rate)]; (iii) association [M{ (currency) ffirli(export)]; (iv) antonymy [Et -(surge) & Ra(slump)]; (v) superordinate or subordinate rAt(financial sector) &Efd-(bank)]. In addition to the above lexical reiteration, we also adopt the term saliency factor which takes into consideration the frequency of occurrence of the processing token in the database (Salton, 1989). One of the objectives of indexing an item is to discriminate the semantics of that item from other items in the database. It can be seen as an associate meaning relationship between regularly co-occurring lexical items in the text. In the following sections, we will describe how these can be utilized in building up the discourse network D.  2.1 Lexical Repetition  Constant repetition of lexical items would make easier for any reader to match strings in a sequence of sentences, construct the appropriate entities, relate the individual segments, and make inferences from them easier than they would be from a text with pronouns and lexical replacement. Word repetitions in lexical cohesion are identified by same word matches and matches on inflections derived from the same stem. [DEFINITION 2]  The connection weights in the discourse network, due to the principle of lexical repetition, are assigned between word pairs, ar(w, E [0, 1] where w E g„ 1V 'E gp i j, and s( . ) is a length function  s(w n w) 0r (w,w')= max(s(w), s(w'))  (Eqn. 1)  The following Chinese examples suggest the feature of similarity a,. of the lexical item iptitri V (Heng Seng Index):  Gr(fitlitiM, tfhro = Low  ar(tH tAR, VT-T) = 0.727  (5,.(EtTizIR, iha-WM) = 0.155 Another sense of lexical repetition is collocation. Collocation is a technique that originates from a particular distributional feature of lexical cohesion, namely that the number of links shared by segment pairs tends to increase as the distance between segments decrease. In other words, lexical items that occur together regularly are said to be collocated. This can happen in closely associated pairs like ,h-fi_fri R (Heng Seng Index) and _ til+(surge). In fact, this habitual association is largely independent of grammatical and semantic structures, being a relationship between lexical items and not between classes of words. In our approach, the frequency of co-occurrences for each pair of lexical items is collected and the collocation measure between them is calculated.  [Definition 3]  Let w and w' be two lexical items, the collocation measure a(w, w') is defined by  (w, w')  =  f(0+  cf f (  ( w, w')  w') - cf (w,w")  Eqn. (2)  where f(w) and f(w ') are the numbers of occurrence of lexical items w and w' respectively while cf(w,w ') is the number of co-occurrence of both lexical items in a predefined size of window. This collocation measure gives each pair of lexical items with a range [0,1].  2.2 Semantic Overlapping  Lexical preference is crucial in solving many natural language processing tasks. Whittemore and his colleagues (1990) find lexical preferences to be the key to resolve ambiguity. They echo Taraban and McClelland (1988) who have shown that the structural models of language analysis are not in fact good predictors of human behavior in semantic interpretation. Within the domain defined by this approach to devising semantic overlapping in text segmentation, the choice of thesaurus as one type of knowledge structure to better understand is indicated by certain properties which thesauri have as sources for the formation of computerized knowledge representations. Thesauri have the merit of being already semiformalized. They also, by implication at least, embrace substantial subsets of any given natural language as a whole, inasmuch as they have the interesting property of serving an infinite multiplicity of functions as knowledge bases. Faceted thesauri are a special kind of semantic network with is-a and are being used in indexing and retrieving documents. Each occurrence of the same token under different categories of the thesaurus represents different senses of that word, i.e., the categories correspond roughly to word senses. A set of words in the same category is semantically related. In our approach, a Chinese thesaurus which defines a is-a hierarchy is employed. We make use the shortest path in the is-a hierarchy to measure the conceptual similarity between the tokens in the discourse segments (Rada et al., 1989). That is, given tokens w and w' in the is-a hierarchy, the distance between the tokens is as follows:  [DEFINITION 4] distance(w , w') = minimal number of is-a relationships between w and w' An is-a hierarchy in the Chinese thesaurus, as in other thesaurus such as WordNet (Miller, 1985), can be viewed as a directed acyclic graph with single root. Figure 1 shows the structure of an is-a hierarchy.  A(Economy)  (Banking) (Bank) (Bank Money)  gc .elt (Finance)  1;41. (Bonds)  (Stocks) I  (Foreign Trade)  RIPC'TfA (International Trade)  M-M114 (Foreign Trade Organization)  C, (share)  Ja+ (shareholder) pool capital  Figure 1: is-a hierarchy of a Chinese thesaurus In order to compute the conceptual distances between each segment, all pairwise combination between tokens in one segment g, and tokens in every other segment g2 are generated. For each pairwise combination, the following definition is used to create a metric over the segments in the discourse network.  [DEFINITION 5]  0 Let g1 = twx l, w x2,  and g2 = wt y i, wy2s wy„} be the two segments, the similarity a() and  dissimilarity 6Ds components due to the semantic overlapping are defined by:  
(1) ameyrikha taylyuk-ul palkyenha-n salam-un WAVO.1 ni ►ka)? America continent-Acc discover-Adn man-Top who-be-QE `Who is the person who discovered America?' (2) hankwuk cencayng ttay mikwun-i mayncheum sanglyukha-n hangkwu-nun Korean war during US;force-Nom the;first land-Adn port-Top (eti-i-pnikka)? (where-be-QE) `(Which place is) the port that the US force first landed during the Korean War?' Such WH-less WH-Questions (let us call them WLWQs) are very often used as questions in written tests or uttered by quizmasters in quizzes, though not often in daily conversations. 2 I assume that, despite their restriction in the range of registers, WLWQs are also constrained by the knowledge of human language. To my knowledge, there has been no previous research on WLWQs in the field. This paper discusses some syntactic issues as to WLWQs. In the course of discussions, I try to argue among others: (i) WLWQs differ in several syntactic aspects from other reduced expressions like VP ellipsis, gapping, pseudogapping, and other shortened wh-questions (section 2); (ii) they also differ from the cleft construction despite apparent similarities in interpreation (section 3); the overtly realized parts in WLWQs are topics (section 4); they read as subjects but not as other grammatical functions like objects or adjuncts due to the principle of deletion up to recoverability, which applies to elliptical structures in general (section 5); they do not have a non-interrogative reading due to a general principle constraining information structures (section 6); and nominative (as opposed to topic) WLWQs do not exist because of the restriction on phases (Chomsky 1998) (section 6). In the appendix, I discuss a potential conceptual problem that WLWQs face with respect to the feature copying mechanism and suggest possible solutions. [21 WLWQs and Other Reduced Expressions In this section, WLWQs and other reduced expressions will be compared. Among the reduced expressions are VP ellipsis constructions (Sag 1976, Wasow 1972, Williams 1977, Chomsky and Lasnik 1993, etc.), gapping constructions (Ross 1970, Larson 1990, Johnson 1994, An earlier version of this paper was presented at the fall conference of the Modern Grammar Circle of Korea, Taegu University, November 13, 1999. I thank the participants for their comments and suggestions. I also thank an anonymous Paclic paper reviewer for her valuable comments, suggestions, and corrections. I regret that I could not incorporate all of them into the paper, though. Of course, all shortcomings are mine. WLWQs are not entirely excluded in spoken languages, as the following example is acceptable: (i) kewul-a, kewul-a, seysang-eyse kacang yeppwun yeca-un (rmuk mirro-Voc mirror-Voc, world-in most beautiful woman-Top who-be-QE `Mirror, mirror, who is the most beautiful woman in the world?'  etc.), and pseudogapping constructions (Levin 1979/1986, Lasnik 1995, 1997, Jayaseelan 1990, Kim 1997, etc.). Some of the examples are given below:  (3) VP-ellipsis a. John loves Mary and Tom does  j, too.  b. A: I like apples.  B: I do  too.  (4) Gapping  John eats apples, and Mary bananas.  (5) Pseudogapping John eats apples, and Mary does eat bananas.  WLWQs syntactically differ from such constructions in several important points. First, the elided parts in these constructions require linguistically expressed antecedents in the sentence or in the discourse, while no such requirement applies to WLWQs. 3 A second difference lies in the size of copies. Phrasal or word level categories (VP or V) need to be copied for the LF interpretation of the elided parts in sentences in (3), (4), and (5).4 In contrast, just some subpart of words (probably phi-features and some basic semantic features) is copied for WLWQs, as will be clear later on. For example, to get WHO in (1), it only has to copy some subpart of the head of the topic, i.e., salam. 5 A third and more fundamental difference lies in the modes of copying. In case of the copying process in WLWQs, a c-command relation holds between the two copies: the wh-phrases in (1) and (2) are c-commanded by the topics, the antecedents which their interpretation depends on. In contrast, no c-command relation holds between the copies in the other constructions in question: neither the overlty realized parts nor the elided parts ccommand the other in (3), (4), and (5).6 WLWQs can not be equated with shortened wh-questions for similar reasons. Compare the following with the WLWQs in (1) and (2):  (6) A: John-i Mary-eykey kkoch-ul cwu-ess-ta. J.-Nom M.-Dat flower-Acc give-Pst-DE `John gave Mary flowers.' B: Sue(-eykey)-nun? S.-Dat-Top `What about Sue?'  3 This observation has some theoretical repercussion as to the treatment of ellipsis. It sides with the LF interpretive approach (Wasow 1972, Williams 1977, etc.) rather than with the PF deletion approach (Sag 1976, Lasnik 1997, etc.). I will follow the practice of the former approach for this reason in this paper, unless otherwise specified. 4 Lasnik (1995, 1997) analyzes English pseudogapping constructions as a sort of VP ellipsis constructions. For example, (5) results from VP ellipsis, with the remnant (banana) having moved out of the VP. Similarly Kim (1997, ch 4) analyzes English gapping also as a kind of phrasal ellipsis (TP ellipsis in his analysis), with the remnants having (rightward) moved out of the ellipsis site via focus movement. 5 Of course, a [+wh] feature somehow has to be added to the copy. For the time being, I am ignorant exactly how this is accomplished. 6 Rather, an anti-c-command relation seems to hold between the copies. Note that ellipsis is possible across sentences as in (3b) or across conjuncts. VP ellipsis may also take place in subordinate clauses.  (i) Almost 35 percent of the doctors said it was OK to say a suspicious breast lump existed when it didn't exist in  order to get coverage. (The LA Times, November, 1999)  (ii) Dulles suspected everyone Angleton did su  (May 1985)  The ellipsis site in (i) is not c-commanded by the trigger if the temporal adjunct clause is adjoined to position higher than the trigger. The so-called antecedent contained deletion structure as in (ii) does not seem to satisfy the anti-ccommand condition. If, however, the ellipsis site is c-commanded by the trigger, the infinite regress problem arises. Various solutions have been proposed in the literature: May's (1985) QR analysis, Baltin's (1987) extraposition analysis, Lasnik's (1993, 1997) and Hornstein's (1994) object-raising-to-[SPEC, Agro] analysis, among others. A common feature of all these analyses is that they assume an operation that abolishes the apparent c-command relation.  (R1) 'Who gave Sue flowers?' (R2) 'What did John give to Sue?' (R3) 'What did John do with Sue?'  The shortened question in (6B) conveys various readings depending on the contexts, as shown in the English translations. They are all [+wh] readings. This is possible because the elided part can be recovered from the previously uttered sentence. In contrast, WLWQs in (1) and (2) can be uttered without there being any previous linguistic antecedent for the elided parts.' Shortened wh-questions and WLWQs also differ in the size of the copies and in the copying modes.'  [3] Evidence against the Cleft Analysis  WLWQs look very similar to cleft constructions in their semantic interpretation. Compare the cleft construction in (7) and the WLWQ in (8):  (7) Songi-ka mek-un kes-un mwues-i-ni? S.-Nom eat-Adn KES-Top what-be-QE `What is it that Songi ate?' (8) Songi-ka mek-un kes-un mvvuesS.-Nom eat-Adn thing-Top what-be-QE `What is the thing that Songi ate?'  It seems that the WLWQ in (8) equals the cleft construction in (7) except that the shaded part is unrealized in (8). There are, however, several pieces of evidence against the cleft analysis of WLWQs. First, kes in (7) and that in (8) convey different semantic weights: kes in the cleft construction bears no (or little if any) substantial meaning, while that in WLWQs does bear some substantial meaning. Compare the following two sentences:  (9) [Songi-ka manna-n kes-un] citokyoswu-i-ess-ta. S.-Nom meet-Adn KES-Top advisor-be-Pst-DE `It was her advisor that Songi met.' (10) [Songi-ka manna-n kes-un]? S.-Nom meet-Adn KES-Top `What is the thing that Songi met?'  7 It is interesting to observe that the Korean sentence in (6B) contains no wh-phrase, while its English counterpart does. An anonymous paper reviewer suggests an interesting analysis of WLWQs in which structures like (1) and (2) are simply instances of sentences only the first half of which are produced by one speaker, who expects the addressee to complete them, parallel to the following English expressions:  (i)  a. Your name/age/nationality is  b. You left the party at  c. You saw him in  d. NGO here means  e. Mary left the party because  The reviewer continues to say that the rising intonation of the speaker signals the request. If this proves to be true, we will have a picture that is very different from the one to be presented in this paper. Although it is yet to be further examined whether the two sets of structures can be accounted for in a uniform way, (and I regret not being able to due to the paper submission deadline,) they appear to be different at least at the surface level, as will be clear later on. First, the elided parts in Korean are (nominal) complements incorporated into a copula, whereas those in English could be complements not only of a copula, but also of a transitive verb, of a preposition, and even of a complementizer. Second, the remnants function as subjects in Korean, whereas they do not even form a constituent in English. Third, the remnants in Korean are topics, whereas those in English cannot function as such since they do not form a constituent. Unless such differences are independently explained, it would be difficult to treat the two sets of data alike.  —59—  As in (9), kes in the cleft construction is compatible with a human being in the cleft position. In contrast, kes in the WLWQs is not. For example, (10) cannot be answered with (11):  ( 1 1) Citokyoswu advisor `It is my advisor.'  For a similar reason, sentences like (12) are not acceptable:  (12) a. *[e, yenge-lul kaluchi-si-nun] kes,-unr  English-Acc teach-Hon-Adn KES-Top  `Who is the person that is teaching English?'  b. *[Songi-ka konghang-eyse cip-ulo e, mosi-n]  kes,-un]?  S.-Nom airport-from home-to e take(HON)-Adn KES-Top  `Who is the person that Songi took from the airport to her place?'  Such sentences are unacceptable due to the honorificity conflict. In (a), the subject honorific morpheme si requires an honorable subject, which cannot fare with kes. In (b), the predicate mosi takes an honorable object, which also cannot fare with kes. Secondly, WLWQs need not end with kes-un. Content nouns are perfectly fine, as in (2), repeated below.  (2) Hankwuk cencayng ttay mikwun-i mayncheum sanglyukha-n hangkwu-nun  
Recognizing Mandarin ba sentences as the Ba Resultative Construction, this study implements a rigid test of Ding's (1993) treatment of ba as a matrix verb in the periphrastic resultative construction. The computational experiment is facilitated with a formal rendition of the analysis of ba in Head-driven Phrase Structure Grammar. Moreover, the small parsing experiment also highlights the important role of semantics in natural language processing, including common-sense knowledge. 1. INTRODUCTION Probably no other lexeme in Mandarin evokes as much controversy as ba in regard to the lexical category. The category of ba in Modem Mandarin has been claimed to be various types of verbs (including auxiliary verb) as well as a `deverbalized' preposition (cf. Wang 1947, Chao 1968, Hashimoto 1971, Huang 1974, Li & Thompson 1981, Sun 1996, and Sybesma 1999, among others). What counts towards the so-called `ba-construction' is also controversial (for details, see Ding 1993). Mangione (1982) presents an investigation of ba sentences most comparable to the one undertaken here in terms of the types of ba sentences and the explicit formal approach. The largest divergence lies in his treatment of ba as a preposition, heading a sort of 'adverbial prepositional phrase' (Mangione 1982: 158). Since Montague Grammar allows prepositions to head predicates semantically (cf. Dowty et al 1981: 243), the crux of the ba problem is dissolved in this framework. Taking advantage of this conflation of verb and preposition, Mangione is able to offer a formal analysis of the predicate headed by ba while arguing for treating ba as a preposition. This convenience, however, is intrinsically built upon the premises of Montague Grammar. The perennial debate about the syntactic category of ba endures such a lengthy period partly due to neglect of its resultative meaning, and in part because of the discrete-category classification that hinges on a clear-cut distinction. This study aims at demonstrating the advantage of treating ba as a resultative verb, proposed in Ding (1993), through a rigid computational implementation of the analysis of the Ba Resultative Construction within the framework of Head-driven Phrase Structure Grammar (HPSG). It also includes a small parsing experiment for processing the Ba Resultative Construction, showing the crucial interplay between syntax and semantics. The major parts of the paper are organized as follows: A working definition of ba sentences is proffered in §2. Discussing the option of considering ba as a resultative verb, §3 analyzes the general syntactic structure of the resultative construction. Following the HPSG analysis presented in §4, §5 is devoted to the parsing experiment that implements the proposed analysis. 2. THE BA RESULTATIVE CONSTRUCTION The following is a working definition for the Ba Resultative Construction (BRC, henceforth), advanced in Ding (1993): A ba sentence belongs to the Ba Resultative Construction if, and only if, the object of ba holds a proper semantic relationship with the successive clause that denotes a resultative  state. The semantic relation between the object of ba and the clausal complement can be PATIENT-and-resultant or EXPERIENCER-and-stative.  Following Nedjalkov & Jaxontov (1988: 6), the notion of `resultative' is extended to encompass the stative (a state without implication of its origin) in a broader sense. `Resultative' in its narrow sense is on par with Wang's (1947) 'disposal' inasmuch as both action and state are taken into account. While the term `resultative' is adapted in its broad sense in this study, 'resultant' will be used to refer to the narrow sense of the resultative, thus `resultative' = 'resultant' + stative'. According to the definition above, all the sentences below belong to the Ba Resultative Construction. Besides the typical ba sentence in (1), the resultative construction also covers those with a 'retained' object, as in (2), and those with a causative meaning, e.g. (3).  (1) The regular type of BRC  da huo ba fangzi gei shao le.  big fire RV  house  PS  burn AP  `The big fire has burned the house (down).'  (2) The object-retained type of BRC to ba san ge taozi chi le s/he RV three CL peach eat AP S/He has eaten two out of the three peaches.'  Jiang ge. two CL  (3) The causative type of BRC  ni ba women ji de budeliao.  you RV  we anxious ET excessively  `You have made us excessively distressing.'  On a closer look at the examples (1)-(3), each of the unique characteristics of these sentences is discernible. In the regular type, ba shares its two NP arguments with the lexical verb, i.e. the subject of ba in (1) is understood as the subject of 'to bum', and the object of ba is interpreted to be identical to the object of the lexical verb, as well. While ba still shares its subject with the lexical verb in the object-retained type, the object is no longer shared. In other words, each of the verbs in (2) has its own object (the object of ba is san ge taozi 'three peaches', while the object of chi 'to eat' is Jiang ge `two'). Even though the objects of the verbs are different, some kind of connection holds between them. Semantically, the second object must be associated to the first object in such a way that a resultant state can be set up or construed. The argument sharing is quite different in the causative type. The non-parallel coreference between the object of ba and the subject of the lexical verb in (3) is not found in the preceding examples. The dual grammatical role on the NP argument, where the object of ba is realized as the subject of the small clause, engenders a causative meaning in this type of ba sentence. The resultative state is crucial to the grammaticality of a ba sentence. A well-formed ba sentence becomes unacceptable when the resultative meaning is eliminated, as exemplified in (4). Notice that the only difference between (4)a and (4)b is the presence/absence of the perfective aspect marker le. Its use in (4)a signifies the completion of an action, and thus entails a resultative state. This resultative meaning vitally affects acceptability of the sentence.  (4)a shushu ba fangzi mai le.  uncle RV  house sell  PF  `Uncle has sold the house.'  b *shushu ba uncle RV  fangzi mai. house sell  3. THE CATEGORY OF BA IN MODERN MANDARIN Recognizing the subtle resultative meaning, Ding (forthcoming) propounds a resultative path hypothesis in addition to the relatively well-establlished serial verb path for the grammaticalization of  ba, as portrayed in Figure 1. Ba  Instrumental  Serial Verbs  Resultative Construction Resultative  Preposition  Verb  Figure 1: Bifurcation in the grammaticalization of ba The two-pathed hypothesis accounts for the instrumental use of ba in Old Mandarin and the development of the resultative function of ba in Modern Mandarin. On the left is the pathway involving serial verbs. It leads to the grammaticalization of ba as a `deverbalized' preposition; the semantic content of the verb has been reinterpreted as instrumental. On the other hand, the path for resultative construction fundamentally represents a semantic change, and not a categorial change. It explains the outcome of the grammaticalization in terms of semantic shift of the domain: from the concrete domain of 'to hold (something)' to the abstract domain of resultative. Whether the change subsequently triggers a categorial shift is determined by other linguistic factors such as further weakening of the semantic content to vacuity. This potential future development is not predicted with the semantic change concerned in this path of grammaticalization. Therefore, ba may be treated as a resultative verb in the spectrum from verb to other related categories. That resultative is a functional meaning does not necessarily ensue that the category of ba would logically be functional. Consider the following sketch of verbhood scale in Mandarin:  Lexical verb  Locative/ Existential  Causative/ Resultative  Auxiliary/ Copula  Figure 2: Grammaticalization continuum from verbs to prepositions in Mandarin  Preposition  The scale is arranged in accordance of semantic contents from concrete to more abstract domains, and thus Lexical verb and Preposition each occupy one end of the scale. These two categories represent the word classes of Verb and Preposition, respectively. Category member-ship of those between the two is rather fuzzy. General diagnostic tests are of little help to them. For instance, the aspect-taking test only identifies lexical verbs in Mandarin. To cope with these problematic categories, a plausible resolution is to extend the membership of the word class of Verb to include locative/existential verbs, and causative/resultative verbs by virtue of their distinguishable (functional) meanings, as indicated by the box. Auxiliary verbs and Copula can form a group either as a subclass under the expanded Verb class or simply as a new word class. Since resultative is a highly abstract meaning, one could still insist on treating ba as a preposition. There is little problem in advocating such a functional category for ba in the regular type of BRC. Since there are exactly two NP arguments, the object of ba can be regarded as a fronted object of the lexical verb, and the subject simply the agent of the verb (cf. Chao 1968). The inadequacy of this treatment becomes apparent when the other types of BRC are brought into picture. In the objectretained type, the existence of three NP arguments is clearly beyond the subcategorization capacity of the lexical verb, cf. (2) above. Although a counterpart of the ba sentence with a postverbal object is sometimes possible for the causative type, this is feasible only when the lexical verb is a resultative verb compound. For instance, the ba sentence of the causative type in (5)a cannot be changed to ordinary sentences by rendering the object of ba as the object of the lexical verb, as attempted in (5)b.1  
In this paper, I present a version of Categorial Grammar reinforced with subcategorizing and operational features. Employing the features allows the further specification of combinatory restrictions in natural languages. I show also that by assigning higher-order categories to words, such irregular expressions as "ago" and "last" and controversial constructions such as the formal subject "it," "tom" constniction and subject raising can be analyzed only through the rule of functional application. Flat categories are assigned to Japanese verbs for their "scrambling' nature. These are demonstrated on a parser working on Web pages.  1. Classical Categorial Grammar  In classical categorial grammar, categories are defined as follows.  (1) a. Any primitive category is a category. b. IfA and B are categories, then AB and B\A are categories.  The rule (1b) is recursively applied. In categorial grammar, two or three primitive categories are usually employed. I show a list of categories using the three primitive categories, N, NP, and S.  category NP\S (=IV) NP/N (NP\S) /NP N/N (=A) IV/IV A/A IV\IV (=ADP)  traditional name Intransitive Verb Determiner rfransitive Verb Adjective Auxiliary Verb Adverb Adverb  expression John walks the dog John loyea Mary hig dog gan walk very big walk slowly  ADP/ADP S/S ADP/NP TV/NP (SAS)/S S/S  Adverb Adverb Preposition Preposition Conjunction Conjunction  very slowly probably he walks a letter frail John run in the park Mary walks if John walks Mary knows if John walks  Two expressions are concatenated by the functional application rule.  (2) Functional Application : a. If X is an expression of the category A/B, and Y is an expression of the category B, then XY is an expression of the category A. b. If X is an expression of the category B\A, and Y is an expression of the category B, then YX is an expression of the category A.  All the expressions in the grammar are generated by the rule of functional application.  2. Categorial Grammar reinforced with features  Here, I present a version of Categorial Grammar reinforced with two kinds of features including features subcategorizing nouns and noun phrases, features subcategorizing sentences, directional features of "f---" and "--p" and other operational features for negation, copying, adding and deleting. These features will be shown and explained along with extended categories. Now, the basic categories are defined as sets of subcategorizing features, and there are two groups of basic categories, noun phrase categories and sentential categories.  2.1 nouns and noun phrases  Unlike many versions of Categorial Grammar, the categories N and NP are basically the same and merely differentiated by the presence of the features "p" and "d". All nouns and noun phrases has the feature "N." ("N' hereafter refers to a feature, not a category.) The features are noted as follows.  feature N p d sg  expressions having the feature all nouns and noun phrases all nouns and noun phrases that can be a subject or an object all nouns and noun phrases with determiners, pronouns and proper nouns (third person) singular nouns  The following are examples of noun phrase categories and expressions.  (N sg)  dog  (N p)  dogs  (N p sg) (N p d) (N p d sg)  water, air they, the dogs, his dogs a dog , the dog, he  2.2 sentences Sentential categories are sets of the following features and features of tense and aspect. All sentential categories have the feature "S" as one of its members.  S  all sentences  q  question  The following are expressions of categories of sentence. No lexical items have a sentential category.  (S present) (S q present)  He walks, They love dogs whether he walks, who she loves  2.3 verbs All categories but noun phrases and sentences are derived from basic categories and they are categories of functional type. They take an expression of some category and give an expression of another category. They are functions from a set of categories to another set of categories. Derived categories can contain operational features. Intransitive verbs or verb phrases take a noun phrase on its left side and give a category of sentence. Therefore, they are functions from categories of noun phrase to categories of sentence. The intransitive verb runs has a category that combines with a singular noun phrase to give a category of sentence with present tense.  runs  N p sg)(S present)  The first parenthesized part in the category, "(<---- N sg p)" expresses the condition of the category runs takes as its argument. It requires that the argument come on the left side, and that it have the features "N," "p," and "sg." Note that more than one category can satisfy the condition. The category "(N sg p d)" as well as the category "(N sg p)" satisfies the requirement. When the condition is satisfied, runs gives the sentential category "(S present)".  he (N sg p d)  runs (<— N p sg)(S present)  (S present  Contrastively, walk has a slightly different category. The operational feature "—" expresses the negative requirement that the input category should not have the specified subcategorizing feature which follows "—." The rule of functional application is extended as categories are. It consists of two processes, the checking or matching process for argument or input categories, and the yielding process for output categories. This operational feature operates in the checking process. "(<---- N p —sg)" can be satisfied by "(N p)" and "(N p d)" as its input category.  walk  N p —sg)(S present)  
Disjunctive expressions in Japanese coordinated with "ka" get either "or"-reading or "and"-reading depending on the situations in which the utterance takes place. This is comparable to some extent to the English counterparts with "or". In this paper, we will examine some cases where the same string of words gets different readings depending on the situations involved. We argue that the differences in interpretation emerge from the differences in accessibility among situations associated with the utterance. Pragmatically, our observation that a given disjunctive expression gets either "or"-reading or "and"-reading in a given situation reflects that only one of those readings can make the expression under consideration informative enough in that particular utterance situation.  1. INTRODUCTION The long-term goal behind our current study is to give an integrated account of how variables in formal languages and indeterminate expressions in natural languages should be treated in formal pragmatics. These expressions are typically used not in connection with particular entity or objects but to express quantification, query or co-variation among entities or objects within the domain of discourse. In Japanese, we have indeterminate expressions such as "dare" (who), "dore" (which one), "doko" (where) and "dono" (which)'. Combined with appropriate particles such as "mo" and "ka", these indeterminate expressions are used not only for question but also for quantification. It is interesting to note that these two particles are used also in coordination: "mo" for conjunctive coordination and "ka" for disjunctive coordination.' For disjunction with "ka", we observe that so-called "or"-reading and "and"-reading are available depending on the situations involved. Further, this disjunctive particle "ka" is also used as 'question marker'. Reflections upon these linguistic facts in Japanese will suggest how disjunction relates to existential quantification, question, and "and"-reading of conjunction. In this paper, we will mainly focus our attention on interpretation of disjunction with "ka", discussing how the differences between "or"-reading and "and"-reading emerge, touching upon existential and universal quantification and question. 
In this paper, we set forth a theory of lexical knowledge. we propose two types of modules: event structure modules and role modules, as well as two attributes: event-internal attributes and role-internal attributes which are linked to the event structure module and role module respectively. These module-attribute semantic representations have associated grammatical consequences. Our data is drawn from a comprehensive corpus-based study of Mandarin Chinese verbal semantics. 1. BACKGROUND Generative theories have long assumed that lexical semantics are encoded on each and every lexical entry, and hence represent idiosyncracies of each lexical item. The assumption, however, goes back much farther than generative theories. For example, Levin [1993] pointed out that Bloomfield wrote in 1933: 'The lexicon is really an appendix of the language, a list of basic irregularities" (p. 274). As a consequence of this assumption, lexical semantics was not intensively studied within the generative framework because it was not expected to offer any interesting generalizations. The notable exceptions, other than the short period of intensity of the generative semantics paradigm, were Jackendoff [1983] and Wierzbicka [1985]. However, as grammatical theories become more and more lexicon-driven, more in-depth theoretical and empirical studies of the lexicon have been carried out, and the above assumption is no longer valid. Levin [1993] in particular sounded the call for in-depth work on a theory of lexical knowledge. She writes that a theory of lexical knowledge: ...must provide linguistically motivated lexical entries for verbs which incorporate a representation of verb meaning and which allow the meanings of verbs to be properly associated with the syntactic expressions of their arguments (p.1). This goal of a theory of lexical knowledge has not yet been attained, for reasons we will discuss in section 2 below. It is, however, a worthy goal, and is in fact, the goal of this paper - to provide a theory of lexical knowledge based on lexical semantic features that are associated with a verb and predict their associated syntactic expressions. In what follows, we first look at why Levin's [1993] proposal of using diathesis alternations to ferret out meaning has fallen short of its goals, and propose a different way of looking for relevant syntactic behavior (Section 2). We next prescnz two underlying assumptions of our theory of lexical knowledge (Section 3), and then present the theory - (Section 4). We summarize our paper in Section 5. 2. VERBAL SEMANTICS Levin [1993] assumes that:  ....the behavior of a verb, particularly with respect to the expression and interpretation of its arguments, is to a large extent, determined by its meaning. Thus, verb behavior can be used effectively to probe for linguistically relevant pertinent aspects of verb meaning' (p.1). We agree with this assumption. But as we will discuss below we look at different aspects of verb behavior from Levin [1993]. Levin [1993] concentrates on the range of possible synactic alternations of a single verb (or a single verb class), and extracts semantic information from syntactic behavior. For example, she points out that break verbs (verbs such as break, crack, rip, shatter, snap,...) all can appear in the middle alternation, but cannot appear in the conative or body-part ascension alternation, while cut verbs (verbs such as cut, hack, saw, scratch, slash,...) can appear in all three alternations [1993: 7]. After comparing these two verb groups with two others, touch and hit, (and their respective alternations) she concludes that break is a pure change of state verb, and cut is verb of causing a change of state by moving something into contact with the entitty that changes state' (p. 8). The syntactic differences they display, she argues, are a direct result of their semantic differences. However, there are two reasons we have not followed Levin in examining the relationship between a verb alternation and its associated semantics. First, although the work done by Levin [1993] in this area is impressive (determining 50 different types of alternations and over 125 different semantics classes of verbs), the sheer number of possible permutations of alternations makes analysis difficult. In addition, when comparing verbs of very different meanings, as in the cut and break example above, it becomes hard to determine the relevant area of semantic difference. For example, in order to reach the generalization concerning cut and break, Levin had to look at two other verbs (touch and hit) and their respective diathesis alternations, as well as look at other verbs that could fit into those alternations, in order to determine the relevant semantics for cut and break for that one particular alternation [cf. 1993, pp. 5-8]. If she had picked different verbs instead of touch and hit or different diathesis alternations from the three that she did, she might not have been able to come up with a generalization at all. These factors may be contributing to the fact that there is currently no unified theory of lexical knowledge based on verbs alternations because scope of the undertaking is so vast. Second, we, along with other scholars in our research group [Liu 1997] tried a pure-alternation based approach and found that it is not adequate for defining Mandarin verb classes. There are several possible reasons for this. The first is that diathesis alternations have not been extensively studied in Mandarin, unlike English, where as Levin notes, there were several important studies done on the verbs cut, hit, break and touch prior to her own work. The second reason has to do with the vastness of the enterprise as we mentioned above. How do you decide which verbs to compare? How do you decide which alternations are relevant? The third possibility is that Mandarin differs from English in such a way as to make alternations a non-viable option for prying into a verb's relevant semantics. Liu [1997] argues that that verb aRernations are not suitable for extracting semantic generalizations from syntactic behavior in Mandarin Chinese because argument placement is relatively flexible. If we agree,then, that syntactic behavior can shed light on the relevant semantics of a verb, and that (at least for Mandarin, if not for other languages as well) diathesis alternations, while originally promising, are not taking us where we want to go -- that is, towards a unified theory of lexical knowledge, what other type of behavior is available? We concentrate on delimiting the lexical semantic distinctions between near-synonym pairs that differ slightly in both their syntactic behavior and in their semantics. Sometimes a semantic difference is apparent at first glance as in the case offang4 (put) and bai3 (set), and sometimes it is not clear and only becomes apparent after we compare the syntactic differences, as in the case of /made and gaoxing 'happy'. (We will discuss both examples further in Section 5). However, even in the cases where there is a difference in meaning, what we are looking for is the relevant difference in both syntax and semantics -- that is, along what semantic lines do these two words differ, and how is this difference related to their synactic behavior (and vice versa)? 
1. INTRODUCTION 1.1 Previous Works Bilingual corpus provides more information than monolingual one (Dagan, 1991). In recent years many works have been done on word alignment after the research on section, paragraph, sentence, and phrase level alignment. Word alignment works not only for th automatic construction of bilingual dictionaries and other useful resourc s, but also for the various applications such as machine translation (MT) and word sense disambiguation. Statistical approach has been used as main technique in most alignment systems (Gale, 1991; Brown, 1993; Dagan, Church, Gale, 1993). Correlation information of bilingual lexicon is mainly employed in statistical approach, and other information, such as character information or position information is used additionally (Brown, 1993; Dagan, 1993). In the alignment of some language pairs that do not belong to the same language family, word list (Wu, 1994) or functional word information (Shin and Choi, 1996) was employed additionally. In contrast to the systems that mainly rely on statistical approach, the English-Chinese alignment system developed by Ker (1997) used a class-based algorithm in its word alignment, and the syste did not use any statistical technique. The result show ed that this new approach could overcome the lower coverage of the statistical approach whil gaining high precision, even if his system was only based on small test set. Ker's work clews us the feasibility of pure linguistic approach in the resol utio of the alignment problem. Phrase level alignment is one of the most difficult problems in word alignment. Some statistical approaches have been used to resolve the problem, but most of them get hold of not linguistic phrases but statistical ones (Wu, 1994; Shin & Choi, 1996). Using parser maybe a solution to this problem (Turcato, 1998), but it can cause other problems such as data sparseness because of the syntax diversity between th two languages. For the improvement of correction, some research restrained the alignment object to som special phrase such as compound noun (Kupiec, 1993; Kumano, 1994). The research described in this paper is sponsored by Advanced Information Technology Research Center (AITrc) and Korean Science and Engineering Foundation (KOSEF)  Phrase level alignment is especially much more difficult when the language pairs do not belong to th same language family, becaus the syntax structure and the lexical formation are much more different in this case there are more cases out of many-to-many one to one correspondenc e relation between these languages. Because the information employed in conventional alignment system is not enough in such languages, it causes the decline in both of coverage and precision (Shin & Choi, 1996). 1.2 Our Proposal In most of previous research es, the definition of alignment was represented as "align word (text, phrase section etc) to its translation" (Gale, 1991; Shin & Choi, 1996). It seems like the concept of alignment is so obvious that no one have concer ed for the problem "what is the translation of an original word". In this paper, we would like to clarify the definition as follows: "Alignment" is a work on finding the translation of the source language. 'Word alignment" should find out the object that shares the highest semantic similarity with the source word When there is more than one candidate, system should find the object that shares the highest syntactic similarity among th candidates. If word level alignment is impossible because of the linguistic peculiarity, alignment object can be expand ed to phrase, which contains the least words and shares maximum semantic similarity with source word or phrase. From the above concrete definition, we can easily find that alignment problem is essentially the problem of bilingual word similarity calculation. The traditional statistical approach has been testified to be effective in the resolution of alignment problem, it shows that statistical information reflects word similarity in some stage. But this approach get good result mainly betwee the languages that belong to the same language family (Brown, 1993; Dagan, 1993) and shows limitation in coverage even after training with extremely large bilingual corpus (Ker, 1997). To the languages that do not belong to the sam language family, the statistical approaches have shown limited coverage and low correctness, even after the employment of additional information (Shin and Choi, 1996; Turcato, 1998). This result is not surprising becaus statistical approach is just an indirect way to get the word similarity, and the information employed in the previous syste s, such as statistical information, word position, functional word or rule, is indirect and vague knowledge in reflecting word similarity. Then, what is the more direct information in getting bilingual word similarity? In monolingual processing, some resources such a s dictionary, thesaurus and WordNet have been customar ily used. Alignment needs bilingual information, so we attempt to use bilingual dictionary instead of monolingual dictionary . Thesaurus and WordNet have almost never been used in bilingual alignment excepts Ker (1997) because they normally contain only monolingual information, but Ker shows us a sound approach to make use of monolingual thesaurus in bilingual alignment. Besides this, we believe that there are character similarity, lexicon similarity, syntactic similarity between some languages (e.g., Chinese and Korean). At least, above linguistic knowledge is more close to bilingual word similarity than something lik statistical information or word position information or so. We will discuss about it in next section. 2. LINGUISTIC COMPARISON BETWEEN CHINESE AND KOREAN We will compare the linguistic property of Chinese and Korean from the three viewpoints - character, lexicon, and syntax. At the end of the comparison, we will discuss bilingual word similarity and relativ information. 2.1 Character Comparison In theory, there is always at least one Korean character corresponding to any Chinese characters. Practically, there are some rarely used Chinese characters have no corresponding Korean characters. Most of the Chinese characters have exactly one corresponding Korean character (Ex, "[C]' [Ming]' t* [K]' [myeong]"', "[C]'iA[Kuai]' b [K]"f [kwae]"'), and seldom of them have 2 or more ones (Ex: "[C]'if[Bian]' b [K]'/e [byeo V[(pyeon]").  We constructed a Chinese-Korean Character Transfer Table (CKCT Table) to reflect the correspondence relatio . There are totally 436 different Korean characters corresponding to 6763 Chinese characters that are listed in GB2313-80 Chinese code table.  2.2 Lexicon Comparison In history, Korean language was influenced by Chinese languag especially in the formativ process of lexicon. We will look at th lexical similarity betwee the two languages from three viewpoints - lexical formatio , part of speech (POS) and lexical intra-structure.  (1) Lexical formation 60% of Korean words are derived from Chinese words. If these Korean lexicons are transformed into corresponding Chinese characters or vice verse, they will have similar form and meaning (Ex, "[C] 1=1*[Heping]<*[K][pyeonghwa](*fri[Pinghe])(<7>[E]peace)","[C]4J, [Bangongshi]<4>[K][samusin( *[Shiwushi]) (4::=>[E]office)". (Li and Choi, 1997) Similarity of lexical formatio is on the prolongation of character similarity. Similarity of lexical formatio exists between other language pairs too and this property has already been used in some previous works (Church, 1993).  (2) POS POS similarity indicates the regularity betwee the POS of the source word and its translation, for example, if a word is a pronoun in source language, then it is highly probable that the translati of the word is also a pronoun. POS similarity has been gotten attention by computational linguistic researchers long before and has been made use in several previous alignment systems (Dagan, Church, Gale, 1994; Shin and Choi, 1996).  (3) Lexical intra-structure Different from other language words, Chinese words have intra-structure. For example, the verb "T ffi [Xiayu](rain)" is composed of two words, one is verb "T[Xia](fall,drop...)" and the other one is noun "ffi [Yu](rain)", we'd like to say that intra-structure of word "T ffi [Xiayu] (rain)" is "verb+noun". We found that in most cases of one-to-many (1:n) correspondence, Chinese words have some specific POS and intra-structure. And the Korean phrases will hold similar syntactic structur s whil the source Chinese words have the same intra-structure. We name it "lexical intra-structure similarity" in our paper.  For example, in next two transfer examples, the Korean phrases have the same syntactic structure while the corresponding Chinese words have the same intra-structure: "[C][v4v+n)C*11(1[Vpi+n+aux v+termination]"  fall rain  (b rain)  "[C] -F + [Xiayu]"  11(1 1,11 +71- SL+ tl-[biga oda]":  rain come ( t* rain)  return home ( b go home)  "[M] + *"[Huijia]"  "[K}+°11  gada]" :  home go ( q5 go home)  In above example, "[C][v4v+n]" is Chinese word intra-structure, "[K.][ Vp4n+aux v+termination]" is syntactic structure of Korean phrase "[C] -F ffi[Xiayu]([E]rain)" and "[C] a [Huijia] ([E]go home)" ar Chinese words, "[K][biga oda] ([E]rain)" and "[jibe gada] ([E]go home)" ar corresponding Korean phrase. The lexical structure transfer rules ( e.g., "[C]verb4verb+noun[K]Vp4noun+aux verb +termination") can be constructed semi-automatically Chinese word intra-structure can be partly gotten from the Grammatical Knowledge-base of Contemporary Chinese (Yu, 1998) and Chinese dictionary.  (4) Lexical sense  Even if two languages do not belong to the same language family, their lexicon has semantic similarity because the objects they want to describe are the same world. One of the best examples about semantic similarity between two languages is bilingual dictionary , almost all of the source word s have their translation in target language.  Concept similarity is closely related to semantic similarity. Formatio , POS and intra-structure of lexicon all r flect semantic similarity as we have discussed above. Besides them, the syntactic similarity (that we will discuss below) and statistical information that has been used in most alignment system all reflect semantic similarity to some extent.  2.3 Syntactic Comparison  Word position (e.g.,between English and French), functional word (e.g., between Korean and English) and POS information all reflect syntactic information, and they have been used with statistical approach in previous works (Gale, 1991; Brown 1993; Shin & Choi, 1996). But in the alignment of Chinese and Korean, word position in sentence is not correct enough because their word order s are quite different. Chinese is SVO type languag whil Korean is SOV one, and both of their word orders ar quite flexible . Additionally, Chinese word order is reflected by semantic element more than by syntactic on (Li,1981).  But it does not mean that there is no syntactic similarity between Chinese and Korean. Syntactic similarity indicates that there is syntactic regularity in syntactic structure transformation. This property can be described in simple transfer patter s that contain no embedded structure (right hand side of next examples).  new book "[C]ff/adj 4S /noun[Xin Shur "[K]l/adj fill/noun[sae caeg]" new book  ([C]adj noun) ([K]adj noun)  this person  "[C]il/pron A/noun[Zhe Ren]"  ([C]pron noun)  "[K]°1 /pronoun A I- q/noun[i saram]" ([K]pron noun)  this  person  discuss discuss(*Let' have a debate) ([C]v1 v1) "[C] it-Ve`,/v ij i/v [Taolun Taolun]"  "[K] M Ni f/noun+4 01  [yinonha'yeo boja]"  discussion do let's (*Let's have a debate)  ([K]noun+tt 01 J-4)  Using simple transfer pattern will be more correct than only using word position in sentence or POS information. And it will be benefit to over come the data sparseness problem than using parser or sentenc transfer pattern.  3.CHINESE-KOREAN WORD ALIGNMENT 3.1 Alignment Object Alignment objects are restricted to some substantive (content words) in both of Chinese and Korean. The standard of exclusion is that, if most words of one POS have one to zero (1:0) correspondence relation in alignment, it will be excluded. As the result, all of th xpletives and quantifier of Chinese, and all of th function words of Korean are excluded. 3.2 Resource an Information used in the System Figure 1 shows the linguistic resources we used in our system and information they can provide.  Resource CKCT Table Bilingual Dictionar Bilingual Class-Net Grammatical Dictionary Simple Transfer Pattern Bilingual Corpus  Information Similarity of lexical formation Semantic similarit Conceptual Similarit Similarity of lexical intra-structure Syntactic Similarit Correlation information of bilingual words  Figure 1. Resource and information used in Chinese-Korean alignment system  Bilingual dictionary provides us the target words that have highest semantic similarity with the source words. Bilingual Class-Net is constructed with Korean and Chinese m onolingual thesaurus , and it provides us the conceptual similarity between Chinese and Korean words (Huang & Choi, 1999). CKCT Table helps us to get similarity of lexical formatio . And grammatical dictionary provides us some grammatical information of source lexicons and it is helpful in getting the lexical structure similarity Simple transfer pattern makes use of simple pattern information in getting syntactic similarity. Finally, bilingual corpus provides us correlation information of bilingual words as is quite well known.  3.3 Alignment Algorithm We use Chinese and Korean POS tagger as preprocessor of our system. The Chinese-Korean alignment system architecture is shown in Figure2.  Chine Sentence  Korean Sentence  cpOS tagger  CI Table Bilingual Dic. rillirTgualCroptOs Class-Net Simp e transfer Grammatical Dic.  Similarity of Sense & formation Correlation information Conceptual Similarity Syntactic Similarity Similarity o Lexical intra-structu re  Dictionary based alignment Statistic based alignment Class based alignment Filtering Pattern based alignment Expanding to phrases (when needed) Aligned bilingual. sentences  Figure2. Chinese-Korean Word Alignment System  Figure3 is an example that will be used for our alignment algorithm.  Example: "Going out in a rainy day is inconvenient".  rain [Xiayu [C]"T /v  day go out Tian Cliumen _Lair  not convenient Bu Fangbianj 7/d hitt  [K]" /ncn 141F-1 /pvg+t--:-/  51/' /ricpai-t1-471/ 4qt/ncps+til+t1-/+./"  [bi naeri+neun nal+'eun oecul+ha+gi  bulpyeon+ha+da.]  rain fall(drop..)  da  go out  inconvenient  Figure 3. Chinese-Korean alignment example  —125—  Stagel: Dictionary based alignment  We use CKCT and bilingual dictionary in this stage, and employed dice coefficient equation (Dice, 1945) to measure the similarity of lexical formatio . Using CKCT table, and transfer the given Chinese word to its corresponding Korean word by transferring characters one by one, and then add it to empty set kci.  1.Search the Korean translation words of the Chinese word c i from bilingual dictionary. Add them to the set ka . We consider one Korean phrase that contains no space as one word, and delete postfix of Korean word if the word is verb, adverb or adjective.  2. Calculate similarity of ci and k; - WordSim(ci ,k;) with equation (1). If WorSim(ci ,k;)> t1 ( t1 is threshold), then save the result to set Similarit  where  2x1k nkl  WordSim (c, k) = d x m kciaexk,I k  (1)  c = Chinese word of given sentence k, = Korean lexical set correspondent to C though CKCT and bilingual dictionary. kci = element i of set k, k = Korean morpheme of given sentence I k I = totel number of the characters in k I kci I = totel number of the characters in ith element of set kc d<1.00, if kci and k have different POS tagger, or IcI=1 and Ik1=1, d = 1.00, otherwise For the example of figure 3, two word pairs M M [Chumen] (go out)" and "M' ffl [oecul] (go out)", "[Tian] (day)" and " 1-a[nal] (day)") will be aligned correctly in stage 1.  1. From CKCT Table: il"} "as n [Chumen] (go out)"=" i r9 [culmun] 2" Set kc = {"  2. From Bilingual dictionary,  "ffi n [Chumen] (go out)" =  out), vrt  1-47t ri[bagg 'e  'eck i  ddeonadalileave home), 1-t} r-1-[oeculhadaligo  nagada] (go out), ffi tl- rf [culgahada] (leave home),  Al ,17}-ri-[sijibgadc] (take a husband), Mt5t1-tl[gyeolhonhada](marry)"  Set kc. = {"ffi r9 [chulmunr, t ql .-Uib i eulr , "al 1-1-[ddeonar, "37F ffl[oeculr, " 14 1[bagg' el" , "1+74nagar, *[culga] ", -]71-[sijibga]", "Mtkgyeolhaon]")3  3. Calculate WordSim(" n [ChuMen](go out)", "371' L [oecul](go out)") as follows, and add the result to set WordSim,;: Set WordSimi; = {1.0}  WordSim(ffin[Chumen] (go out), $7F [oecul] (go out))4  2"5Z-t = 1 .0 x max 1*-FH+  -'&1 2 xl--11*(-) -271E4-+79+* 2x  9-1 t 25zAtelin-ii* 2 x n  Fd11+19-1*1  41+0* 19.1*1+19.1*1 nt°111+191*1 114711+1-9-1*  2x1 2x0 2x0 2x2 2x0 2x0 2x1 2x0 2x0 =1 .0xmax 2+22+22+22-1-224-224-22-1-23+2'2+21  =1 .0xmax{ 0.5,0,0,1,0,0,0.5,0,0}  
One of the thorniest problems of large vocabulary continuous speech recognition systems is the large number of out-of-vocabulary (OOV) words. This is especially the case for the languages like Japanese, which has many inflections, compound words and loanwords. The OOV words vary with the application domains. It's not realistic to have a big general-purpose lexicon including any possible 00V words. Furthermore, embedded speech recognition systems become more and more popular recently. They strongly demand an economical and effective exploitation of lexicon space. In this paper, we introduce a lexicon development model dealing with different kinds of OOV words with the help of linguistic morphological knowledge. It provides unsupervised, fast vocabulary adaptation among different application domains. In the experiments that adapt a lexicon of typical vocabulary, we were able to reduce the OOV rate by 40% and improve the word segmentation error rate by 27%. And the smaller the lexicon is, the more we benefit from the vocabulary adaptation. 
The relationship between lexicon and pragmatics has been one of the most controversial issues in recent studies in linguistics. Lexicon in the conservative definition encodes only the information responsible for the regular syntactic mapping of lexical items. Any information that is not directly reflected in the syntactic structure is attributed to pragmatics. A more recent trend, in contrast, considers that lexicon should cover even apparently non-linguistic information if it leads to certain syntactic and/or semantic regularities. Generative Lexicon (GL) by Pustejovsky is a representative of such a trend. This paper applies GL to analyze the Japanese internally-headed relative clause (henceforth, IHRC) with implicit target, which has presented a serious challenge to formal approaches. The IHRC whose target is neither a syntactic argument nor an adjunct has strongly motivated a pragmatic approach to the identification of the target. This paper argues that IHRCs with implicit target can be formally accommodated without drawing on the poorly-defined pragmatics, if the lexicon is sufficiently elaborated. 1. INTRODUCTION Japanese IHRCs present several puzzling behaviors that challenge most syntactic approaches to them. One of such behaviors concerns the identification of the antecedent (henceforth, the (semantic) target) of the IHRC. Namely, the target is not limited to an argument of the head verb of an IHRC; in some cases it can be a set of arguments, while in others it can even be some entity merely implicit in the sentence. In order to accommodate such cases, most analyses posit an empty category as the relative head, whose antecedent is determined by some pragmatic considerations ([1], [2], [6]). The recourse to contextual information is shared by other approaches including the ones in Cognitive Grammar ([3], [4]). The analyses in Cognitive Grammar state that the target is selected from among salient participants of the situation, where the saliency is related to but is independent of syntactic argumenthood One question that emerges is what the "pragmatic" considerations licensing the implicit target exactly are. Obviously, only a certain set of implicit arguments can stand as the target. Without due restrictions, the theory would become too powerful, overgenerating the IHRCs with illicit implicit targets. Besides, the complete recourse to pragmatics predicts that the availability of the implicit target is totally independent of the lexical information; it would be sensitive only to the contextual information. However, as we shall see below, the distribution of the IHRC with an implicit target is actually very limited, and the restrictions reveal regularities related to lexical information. If a syntactic analysis assumes a lexicon which accommodates only what is structurally realized,  then the IHRC with an implicit target falls outside the scope of analysis. But if one assumes a lexiconbased framework which includes in its lexicon even the non-linguistic information of some kind, the implicit target could be dealt with in a formally restricted way. As such framework, I will employ the model of GL ([5]), and explore at the same time what kind of revision it needs to fully accommodate the data. My goal is to demonstrate that the availability of the implicit target of the IHRC is fairly constrained, and that the data can be accommodated without calling for the poorly-defined "pragmatics." The organization of this paper is as follows : Section 2 presents data of IHRC including the ones with implicit target, indicating the direction of approach this paper takes. The data of the IHRC with implicit target are put to detailed examination in Section 3. It shows that the availability of the implicit target is more restricted than previous analyses tacitly assume, giving the first approximate delineation of the implicit targethood. Section 4 introduces the model of GL by Pustejovsky, and explores how it can accommodate the primary data. Section 5 discusses how the model of GL should be modified to fully account for the data of IHRC. Section 6 concludes the discussion. 2. APPROACHES TO INTERNALLY-HEADED RELATIVE CLAUSES Following are some typical examples of Japanese IHRCs:1 (1) a. [[Oba -kara ringo -ga okutteki -ta] no] o tabe-ta aunt from apple nom send-come-past nmlzr acc eat-past `My aunt sent me apples and I ate one.' b. [[Koppu -ga ware-ta] no o katazuke-ta glass nom break-past nmlzr acc put-away-past `A glass broke and I put it away.' In (1), the bracketed part is the IHRC, and the target is the subject argument of the clause ringo 'apple,' which serves as the direct object of the matrix verb tabeta ate.' The fundamental problem of the IHRC is the form-meaning incongruity; that is, the semantic head (target) of the IHRC is not the syntactic head of the clause. The surface form of the IHRC is, in fact, identical with that of the sentential complement. Any analysis of the IHRC has to consider the apparent incongruity. Putting aside the details of the analyses, most of the syntactic approaches fall into either of the following two types (and their variations): (1) those which device a structure to directly access the semantic head within the IHRC and (2) those which turn to pragmatics to identify the referent of the IHRC. The latter typically posit an empty category as an argument of the matrix verb, which could be coreferential with an argument within the IHRC. Thus the semantic head of the IHRC is linked to the matrix verb only indirectly. The recourse to pragmatics is becoming prevalent, as more challenging data are found. The IHRC whose semantic target is kept implicit constitute the most challenging data. The following are some examples of the IHRC with an implicit target (data from [3], [4]): (2) a. [[Nikai -no huroba -no yokusoo -ga ahureta -no] -gal sita made moretekita 2nd floor gen bathroom gen bathtub nom overflow nmlzr nom downstairs to leak `The bathtub in the bathroom upstairs overflowed and (the water) leaked to downstairs.' 
The so-called multiple nominative constructions (MNC) have been one of the puzzling phenomena in topic-prominent languages like Korean, Japanese, and Chinese. This paper first investigates the basic syntactic properties of three MNCs and then provides a constraint-based analysis within the theory of HPSG (Head-driven Phrase Structure Grammar). The analysis presented here is lead-driven' and 'constraint-based' in the sense that properties of lexical heads and tight interactions among declarative constraints play crucial role in the formation of puzzling MNCs. Some appealing consequence of this analysis is a straightforward account of long standing problems, such as getting the semantics right and generating multiple nominative constructions without resorting to a mechanism that allows unlimited valence increase.  
This paper discusses deletion and movement operations in Korean from the perspective of information packaging, and claims that each operation is a systematic process of information packaging to generate the most optimized information structure. This paper argues that there is neither free word order nor free deletion in Korean. This paper explicitly illustrates that old arguments cannot stand still in information structure, but must undergo some kind of structural changes or deletion. This paper proposes a constraint on information packaging that covers both operations and gives an explanation of underlying motivation for them. It is also suggested to decompose each component of information structure into a feature complex.  
This paper is trying to show that the concept of the prototypical transitive sentence is very useful in the study of transitivity, but is as such not so unproblematic as has often been assumed. This is achieved by presenting examples from different languages that show the weaknesses of the "traditional" prototype. The prototype proposed here remains, however, a tentative one, because there are so many properties that cannot be described typologically because of their language-specific nature. 1. INTRODUCTION 
Since [5], 4-valued logic is known to be a useful tool to capture the human reasoning: it is paraconsistent, can treat incompleteness and inconsistency of information etc. In this paper, I propose a 4-valued reasoning system with stratified bilattices of [12]. It inherits desirable formal properties of 4-valued logic, and further realizes a certain kind of default reasoning and truth maintenance system with a simple, lucid LK-style calculus without esoteric, exotic 4-valued operations in [6, 7, 2, 3] etc. 
The purpose of the paper is twofold. First, we revise the well-known Centering Theory of anaphora resolution and propose the Controlled Information Packaging Theory (CIPT, for short). Second, we suggest a solution to the resolution of the antecedents of pronouns within the framework of CIPT. For this purpose, we select a dialogue of hotel reservation as a domain-restricted discourse, and discuss the characteristics of the distribution of pronouns. We suggest that we need to place the Slot-Link element on the top of the forward centering list. We claim that we need to establish a constraint on conceptual compatibility. As for the pronouns in the main dialogue, we propose a constraint of discourse command (d-command).  1. INTRODUCTION  In Korean, the zero anaphora is very common in a domain restricted dialogue such as the one found in the situation of hotel reservation as follows':  (1) U 1 :  iss e-yo?  exist  (Is there a room free?)  U2:  nalcca encey-sip-nikka?  date when  (For what date are you going to make a reservation?)  U3:  onul cenyek-ey.  today night  (I'd like to make a reservation for tonight)  [U = Utterance, = zero pro-form]  In the above example, a long discussed issue is how to establish the antecedent of the zero anaphors. In this study we propose a reasonable and reliable solution to the problem. The following five types of information structures are assumed in CIPT:  (2) a. Link - Tail - Focus structure (L-T-F structure) b. Link- Focus structure (L-F structure) c. Tail - Focus structure (T-F structure) d. Focus structure (F structure) e. Slot Link - Focus structure (SL-F structure)  The SL-F structure is the one defined by Lee & Lee(1998), in addition to the original information packaging theory of Vallduvi (1994). We adopt the concept of the frame theory devised in the Artificial Intelligence community.  We thank Professor Jungyun Seo of Sogang University and Professor Hyunho Lee of Tongyang Technical College for allowing us to use the corpus they constructed for the Soft Science Project.  In this paper we claim that the sentences with zero anaphors tend to exhibit the SL-F structure, on the basis of empirical evidence from actual dialogue corpora found in situations such as hotel reservation, theater talk, etc. As a next step we propose a revised ranking of the forward-looking centers in the sense of centering theory. It is claimed that the componential status of the information structure of the relevant utterance is revealed in the form of a hierarchy as follows:  (3) SL-component > Speaker, Hearer > Subject > Indirect Object > Direct Object > Others  With this hierarchy, we can calculate the reference of zero anaphora in any form of domain restricted dialogues. As for the overt anaphor, H. Lee(1998) postulates a constraint for the recovery of its antecedent at the moment when a sentence is uttered after returning from a sub-dialogue. He observes that an overt pronoun must have its antecedent in the sub-dialogue when it appears in the first utterance immediately after the sub-dialogue. Look at the example in (4).  (4) Ul: Seoul ollawa-se-nun meyn cheum-ey  came-after at first  ince ku Naksan kkokteyki-ey ku acu ku chenmakchon  well, the  top  at well the tent  kathun tey Inca.  like place well  (When I arrived in Seoul, I (went) to the top of the  Naksan mountain, well, to the poor village )  <Sub-dialogue>  U2(S 1): naksan-imyen ce Tongtaymunccok ?  direction  (Do do you mean the Nagsan mountain near East  Gate? )  U3(S2): yey Tongtaymun-ye i-ss-upni-ta.  yes  at exist  (Yes, it is. It is located near Tongtaymun.)  U4(S3): yey, yey.  yes, yes  (I see. I see.)  </Sub-dialogue>  U5: kuri kass-ess-nun-tey,  there went...  (I went there, ... )  In H. Lee's(1998) analysis, the overt anaphor kuri 'there' in the utterance U5 has its antecedent Naksan in the previous sub-dialogue(namely, U2(S1)). We, however, claim that the proposed analysis is not convincing because the same antecedent can also be found in the utterance Ul, which is in the main dialogue. In this paper we show that H. Lee's hypothesis is not correct and we propose a general constraint on the interpretation of the overt anaphor, on the basis of the analysis of the realistic corpus. The constraint is stated as follows:  (5) The overt anaphor has its antecedent in the discourse segment of the same or higher level.  2. INFORMATION PACKAGING THEORY In (2) above we mentioned five types of dialogue structures. We now discuss the ideas using Vallduvi's(1994) examples. Let us first examine the Link-Tail-Focus structure depicted in (2a). Examine the dialogue in (6).  (6) a. A: In the Netherlands I got the president a big Delft china tray that matches the set he has in the living room. Was that a good idea? b. B: No. [L The president] [ F HATES] [T the Delft china set]) Abbreviations: L = Link; F = Focus; T Tail. In a dialogue such as (6), when the sentence "The president hates the Delft china set" is uttered, only the verb 'hates' becomes the focus. The phrase 'the president' is a link component and 'the Delft china set' a tail component. Accordingly, the cognitive processing will go on as in (7). (7) a. Look up the information card of 'the president'. b. Replace any previous information concerning the relation between the president and the Delft china set with the new information 'HATES'. (Information updating) If the same sentence is uttered in a different context, the information structure will be different as shown in (8). (8) a. A: I'm arranging things for the president's dinner. Anything I should know? b. B: Yes. [L The president] [F hates the Delft CHINA SET]. In this case, 'the president' is a link component and 'hates the Delft china set' becomes the focus component. Here, in the cognitive process, the first step is to look up the information card of the noun phrase 'the president'. Then we are supposed to add the information 'hates the Delft china set' to the card. In the example in (9) we see that no explicit link component appears. (9) a. A: In the Netherlands I got the president a big Delft china tray that matches the set he has in the living room. Was that a good idea? b. B: No. [F (He) HATES] [T the Delft china set])2. Here only 'hates' becomes the focus component, and the noun phrase 'the Delft china set' functions as the tail component. We do not have the link component 'the president'. In this case, we assume that the information card for 'the president' has been activated and continues to be in the activated state. In the card we replace any previous information related to the relation between the president and the Delft china set with 'hates'. Let us now examine a situation where the example (8) is uttered in a different context as in (10). (10) a. A: I'm arranging things for the president's dinner. Anything I should know? b. B: Yes. The president always uses plastic dishes. [ F (He) hates the Delft CHINA SET]. Here the whole verb phrase 'hates the Delft china set' is the focus component. This information is added to the activated card of 'the president'. 3. CONTROLLED INFORMATION PACKAGING THEORY(CIPT) In this section, we discuss the two characteristics of the Controlled Information Packaging Theory(CIPT, for short). The CIPT is distinguished from Vallduvi's Information Packaging Theory in two respects. First, in our CIPT we postulate the fifth SL-F structure. Vallduvi(1994: 16) discusses dialogues like 2 The pronoun 'he' is not overtly pronounced. This is just to show the place where 'the president' is assumed to appear.  the one given in (11).  (11) a. A: Why don't you go to the theater more often? b. B: TICKETS are expensive.  He notes that the sentence in (1 lb) is not about any particular referent. He observes that in this case no particular focus of update is designated. He suggests that a salient general temporary situation file card be used to record the new information. This sentence is sometimes termed to be reporting a situation. If we look at the situation closely, however, we can clearly see that the noun phrase 'tickets' in (11 b) is referentially related to the noun phrase 'the theater' in (11a). If we use the notion of frame suggested by Minsky(1975) to represent our cognitive knowledge of the actual world, we can naturally relate 'tickets' to 'the theater'. Minsky assumes that our knowledge about the world is represented in terms of frames, each of which in turn consists of many slots. The theater provides us a frame of world knowledge and the noun phrase 'tickets' fills in one of the slots. The idea can be represented as in (12).  (12) Structure of the 'Frame and Slots'  F(frame) [Ex. THEATER]  +  + +  +  +  +  +  S(slot) 1  S(slot) 2 S(slot) 3  • •  [Ex. TICKETS]  In this frame and slot analysis, we can say that when (11a) is uttered, the information card of 'the theater' is activated in the cognitive structure of the hearer, and the noun phrase 'tickets' can be triggered by this activation, which is exemplified in [ ] in (12) By introducing this idea of frame and slot representation, we extend Vallduvi's theory and postulate the fifth information structure, namely Slot Link-Focus structure. We now analyze (11 b) as in (13).  (13) [SL TICKETS] [F are expensive]. As shown in (13) we treat the noun phrase in (1 lb) as a kind of link component. We now introduce a new notion of Hyper-link. The new information 'is expensive' is not directly linked to the noun phrase 'the theater' in (11a). We assume there to be a hyper-link between 'the theater' and 'tickets' by making an additional information card. The information conveyed by the verb phrase 'is expensive' is indirectly linked to the theater through this hyper-linking card. The new Slot Link-Focus device can naturally explain the so-called bridging phenomena discussed by I.-H. Lee(1994). Look at the examples in (14).  (14) a. John entered a large dining room. b. The chandelier hung by an imported gold chain.  The noun phrase 'a large dining room' in (14a) need to be related in some way to the noun phrase 'the chandelier' in (14b). This referential relation can be properly captured by the hyper-link structure, which may be represented by the sentence in (15).  (15) The large dining room had a chandelier. The sentence in (15) bridges (14a) to (14b). We see that Vallduvi's original information packaging theory cannot appropriately handle examples like (11) and (14). We see that our extended information packaging theory, including the Slot-Link Focus structure, can provide a proper account of the data in question. Second, our CIPT assumes a center controlling file card that includes the informations about the  discourse structure and ordinary file cards. A center controlling card is assumed to have the structure depicted in (16). (16) A Center Controlling Card (CCC) Card Number The set of discourse referents on the same level Forward-looking center list of the immediately previous utterance Hyper link with the center controlling card of the immediately higher level Hyper link with the center controlling card of the immediately lower level An example of the center controlling card is shown in (17). (17) An example of CCC 3 7 9 10 14 15 [ 14 15 ] 
Combinatory Categorial Grammar is a lexicalised formalism which is mildly context-sensitive. Recently Set Combinatory Categorial Grammar, a direct descendent of Combinatory Categorial Grammar was proposed to treat local scrambling adequately. In this paper, we briefly sketch Set Combinatory Categorial Grammar analyses of various Korean syntactic phenomena including coordination, extraction and multiple nominative construction. 1. INTRODUCTION We briefly introduce CCG (Combinatory Categorial Grammar) and Set-CCG (Set Combinatory Categorial Grammar), a direct descendent of the former. 1.1 Combinatory Categorial Grammar CCG, introduced by [1] as a generalisation of the Categorial Grammars (CG) of [2] and [6] is a lexicalised formalism which is mildly context-sensitive [21]. The mildly context-sensitive generative power of CCG enables it to provide adequate description of syntactic phenomena including crossing dependencies in some languages that are known to be beyond the coverage of context-free grammars [16]. For syntactic derivation, CCG utilises a system of rules which is based on the combinators of [9]. These combinators are composition (B), type-raising (T), and substitution (S). CCG's view of surface structure is quite different from that of conventional grammar formalisms. Surface structures in CCG do not exhibit traditional notions of constituency or dominance and command. All the fragments that arise under coordination and related constructions are considered as surface constituents. This flexible view of surface structure provides lucid descriptions not only for bounded dependencies but also for various sorts of unbounded dependencies that occur when the complements are displaced under coordination, relativisation, etc. Thus, it is as a theory of multiple unbounded dependency and coordinate structure that CCG has most to offer as a theory of grammar [20]. Extensive introductions and discussions of the linguistic properties and motivations of the CCG formalism together with analyses of many constructions in English, German and Dutch are presented in [18, 19, 20]. *I would like to express many thanks to Jason Baldridge for helpful suggestions and discussions. All remaining mistakes and errors are mine.  1.2 Local Scrambling and Set-CCG CG and CCG were originally developed for configurational languages in which the order of constituents is mostly fixed and functions of constituents are realised according to their position in sentences. Nonconfigurational languages including Korean exhibit local scrambling as shown in (1).1 (1) a. Hwanho-ka wuyu-lul masi-nta. Hwanho-NOM milk-ACC drink-DECL Ilwanho drinks milk.' b. wuyu-lul Hwanho-ka masi-nta. milk-ACC Hwanho-NOM drink-DECL Ilwanho drinks milk.' Typical solutions for local scrambling in CCG are extensive use of type-raising and crossing composition rules, and assignment of additional lexical categories for predicates which allow scrambling. The defects of these two approaches were presented in [3]. [10] proposed Multiset-CCG which is an extension of CCG. Sets are incorporated into the rules and categories of CCG in order to allow flexibility for handling scrambled word orders directly with single lexical categories. However Multiset-CCG achieves higher power than CCG and other mildly context-sensitive formalisms. [5] proposed Set-CCG, a direct descendent of CCG. Set-CCG also accepts scrambled orders with single lexical categories by incorporating sets into the categories and rules of CCG. However, the directional specifications of lexical categories and rules are retained. This allows Set-CCG to maintain the same generative capacity of CCG. The strong equivalence of CCG and Set-CCG are proved in [5]. We adopt Set-CCG for analysis of Korean throughout this study.2 2. PREVIOUS WORK [17] applied an extended version of pure Categorial Grammar to analysis of Korean. The notion of sets and unspecified directionality similar to those of [10] was introduced to handle the relative free word order. Some principles on rule application were also suggested. This work, however, cannot provide descriptions of coordinate constructions and other complex constructions involving the unconventional constituents. [15] and [7] incorporated sets into lexical categories, but directionality was retained. However, these studies lack format account for this extension. [7] used type-raising to treat case marking in Korean. [8] and [13] adopted Multiset-CCG for analysis of Korean, and coordination and quantifier floating were described. [14] adopted Set-CCG for description of Korean and showed analyses on various syntactic phenomena including coordination, relativisation, and clefting. 3. CASE MARKING AND PREDICATE CONJUGATION We will first look at the treatments of case marking and predicate conjugation in Korean within the framework of CCG. 
Both Hausser [1] and Lee [2][3] proposed Database Semantics as a computational model for natural language semantics that makes use of a database management system, DBMS. As an extension of these efforts, this paper aims at dealing with ways of representing linguistic descriptions in table forms because all the data in a relational model of DBMS is conceived of being stored in table forms. It is claimed here that, if an algorithm can be developed for converting linguistic representations like trees, logical formulas, and attribute-value matrices into table forms, many available tools for natural language processing can be efficiently utilized as part of interface or application programs for a relational database management system, RDBMS. 1. INTRODUCTION For business transactions or academic administration, a commercial database management system (DBMS) like DB2, Oracle or Informix is widely used. Database Semantics is an attempt to adopt such a system for doing semantics for ordinary language. Since an interpretation model or background is necessary for processing linguistic information, Database Semantics can use as its model a database in a DBMS that provides both lexical meaning and world knowledge information. Furthermore, a DBMS constantly updates its database with new data, as fragments of a natural language like Korean or English are processed through a linguistic processing system LIPS. Hence, Database Semantics can consistently process even a larger fragment of discourse in natural language. (1) A Model of Database Semantics But, before developing Database Semantics as an application program into an RDBMS, the issue of representation must be resolved at least in the logical or conceptual level. Unlike ordinary semantics, Database Semantics as a computational model ultimately aims at a computational implementation. Hence, it should be able to build a successful interface between the two different levels of linguistic description and computational processing. As a result, representation comes to play an important role of bridging the * This work was partially supported by the 1999 research grant from Korea Research Foundation. Here I would like to thank anonymous referees for the PACLIC14 and all my colleagues who helped me to complete this paper, especially, Suk-Jin Chang, Roland Hausser, Jae-Woong Choe, Masatoshi Kawamori and my graduate assistants Jungha Hong and Seungchul Choe. —231---  and computational processing. As a result, representation comes to play an important role of bridging the gap between human and computer interactions. Linguistic descriptions are often represented in trees, logical forms, or attribute-value matrix (AVM) structures.' On the other hand, a computer system like RDBMS, which only knows a relational language like SQL, can conceptually recognize data stored in a table form only. This paper will thus focus on ways of representing linguistic information in a table form so that it can be recognized by an RDBMS. The main task of this paper is then to discuss ways of converting linguistic representations like trees, logical forms, and matrices into table forms. If an algorithm is developed for these conversions, then many of the exiting parsers or interpreters can be directly incorporated into an RDBMS. Head-driven Phrase Structure, Lexical-Functional, and Left-Associative grammars, for instance, produce the results of their analysis represented in a sequence of Trie or AVM structures. At the present, however, the presentation of such an algorithm is beyond the scope of this paper, for the version of Database Semantics proposed in the framework of RDBMS is still in the nascent state of being designed as a computational model. It is thus only hoped that fragmentary but concrete illustrations will be given to show how linguistic analyses can be represented in table forms. 2. WHY A RELATIONAL MODEL? Hausser [1] was the first to develop Database Semantics. By adopting a network model for DBMSs, he proposed to construct a Word Bank, a lexical database consisting of word types and their tokens. By navigating this Word Bank, propositional content can then be processed linearly or left-associatively by a step-by-step manner. In a similar vein, Lee [2] also showed how a network model could be used to represent various structural relations in natural language analysis, while assuming that it could be a minimally sufficient model for processing natural language. Lee [3], however, argued for some advantages of adopting a relational model possibly with object extensions. First, the relational model has given a conceptual basis for implementing most of the currently running commercial DBMSs like Oracle8i or Informix7.0. Secondly, the relational model uses standardized SQL, a structured query language, for describing ways of building database structures and managing them, thus making it possible to apply it to the construction of a query system for natural language semantics. Finally, its basic representational scheme at the logical or conceptual level is of a table form consisting of attributes and their values like an attribute-value matrix for linguistic representation. In the relational model, a database is a set of tables representing various relations among objects in a domain. This paper thus proposes to adopt a relational model RDBMS for developing Database Semantics. 3. WHY DATABASE SEMANTICS? There are at least two reasons for adopting a DBMS as a basis for doing semantics. First, natural language generates a large amount of linguistic information with great complexity. Even a tiny fragment of text contains a lot of data. Especially when it contains indexical or context-dependent expressions, even a short sentence consisting of three or four words can theoretically create almost an infinite number of interpretations. Consider the following sentence: (2) I am happy today. The original version of this paper discussed the tripartite representation of quantified sentences in AVM structures and converting these matrices into table forms. But, due to the editorial constraint on the limitation of space, the discussion of AVM structures has to be postponed for another occasion.  This string of words in English constitutes a well-formed sentence of English, meaning that, whoever the speaker is, she or he is happy on the day of her or his making the statement. Hence, it can have an indefinitely large number of interpretations depending on who the speaker is and also when or what day it is spoken. Let's consider another example. Suppose two lovers say to each other: (3) I love you. This sentence consists of only three simple words. But, depending on the context of its use, it can produce infinitely many different interpretations. Here, neither I nor you refers to a fixed person. Even in a situation where two persons, say Mia and Bill, say to each other I love you, each of the pronouns has two different referents. The pronoun I once refers to Mia, while the pronoun you refers to Bill. And then the pronoun I refers to Bill, whereas the pronoun you refers to Mia. Hence, a model in which their referents are fixed cannot interpret sentences like (3). By allowing a DBMS to keep updating its database, Database Semantics as a model-theoretic semantics should be able to use the constantly updated set of data supplied from the database to interpret contextually dependent statements or utterances. By appropriately partitioning the database into smaller parts through DBMS, Database Semantics can get the effect of subdividing its interpretation model into smaller sub-models so that the indexical expressions like I or you or even anaphoric pronouns like she or he may have varying referents within a larger model. Event statements have the same problem of varying reference. Indexical expressions like today or yesterday have different referents as discourse situations vary. Then there are events that consist of smaller events, some of which may overlap each other. While statements are being made continuously, their interpretation model should be able to keep track of their sequential as well as overlapping relations,. This task, Database Semantics is expected to perform efficiently by making a DBMS to control and manage its database. While using a database as its interpretation model, Database Semantics installs in it various lexical and grammatical modules for processing natural language. The lexicon or word bank should be part of the database. This lexicon may be of various types. It can, for instance, be a multi-lingual dictionary or may contain a virtual dictionary temporarily built to deal with a given fragment of discourse. The entire grammar consisting of both syntactic and semantic rules should also be installed in the same database. There may be sets of constraints or principles governing the generation of well-formed sentences. DBMS then manages and upgrades all these modules with new lexical items as well as newly required grammatical rules and constraints, as a larger linguistic corpus is introduced into the system. By allocating all these managing functions to DBMS, Database Semantics can make its module LIPS for linguistic information processing simply function as an application interface between its users and DBMS. 4. TABLES A relational DBMS is normally identified with tables.' A table is a very simple and rigid form of representation, a triplet <Name, Attributes, Values> consisting of a name, a set of attributes or fields and their corresponding values. Nevertheless, it can carry a lot of information with varying types. The following is an example containing some personal information on professors: 2 More detailed discussions of the notion of table and its properties in an RDBMS can be found any ordinary books on database management systems like [4], [5], [6], and [7].  (4 PROF_NO 01 02 03 04  NAME Lee Choe Kang Kang  INITIAL K. J. B. M.  OFFICE_PHONE 3290-2171 3290-2172 3290-2173 3290-2174  The name of this table is PHONE BOOK. The table has four attributes: PROF NO, NAME, INITIAL, and OFFICE_PHONE. Under each attribute, 4 different values are given with each of the four rows forming a record. The first row, for instance, gives information on Lee's office phone number.  Tables in an RDBMS are characterized by their data and structural independence. The above table (4), for instance, can easily be expanded by adding new data without destroying its basic structure.  (5) PROF_NO 01 _ 02 03 04 05 06  NAME Lee Choe Kang Kang Kim You  INITIAL K. J. B. M. S. S.  OFFICE_PHONE 3290-2171 3290-2172 3290-2173 3290-2174 3290-2175 3290-2176  This new table can be considered as being obtained by joining table (4) with table (6) containing information on Kim and You.  (6) I PROF_NO 05 06  NAME Kim You  INITIAL S. S.  OFFICE_PHONE 3290-2175 3290-2176  Here, the attribute PROF_NO on each of the two tables (4), (5) and (6) uniquely identifies each row, thus playing a role as a primary key for linking one table to the other or joining them into a new expanded table.  The notion of table is very simple as sketched here. It is purely a conceptual construct. This paper will, however, show the expressive power of tables for linguistic description. For this purpose, the paper aims at showing how seemingly complex-looking linguistic objects like Phrase Structure trees and logical formulas can be converted into table forms.'  5. FROM TREES TO TABLES  One possible way of representing the constituent structure of a sentence is to use a tree form. The following sentence (7), for example, may be analyzed into a phrase structure tree (8):  (7) Mia loves Kim.  (8)  NP:O1  VP:02  N:011 V:021 NP:022  N:0221  Mia  loves  Kim  3 The conversion of AVM structures into table forms is briefly discussed in [3].  This tree is understood as representing how sentence (8) is formed: it consists of a noun Mia, a verb loves, and another noun Kim.  The same structural information can be represented in the following table form where the terminal nodes Mia, loves, and Kim are marked with a dummy symbol t:  (9) Table name: TREE8  NODE_CODE MOTHER  0  S  01  NP  02  VP  011  N  021  V  022  NP  0221  N  DAUGHTER1 NP N V t t N t  DAUGHTER2 VP NP  Here, a unique code number is assigned to each node from which one can obtain information on the dominance and precedence relations among the nodes. The NODE_CODE 0, for instance, stands for the root node labeled S, and the NODE CODEs 01 and 02 are its two daughters, NP and VP respectively. It is again understood here that 0 dominates 01 and 02 and that 01 precedes 02. This is so because the number of digits in each NODE_CODE is understood as representing its level in the tree and also the sequence of numbers except for the one in the last digit in each NODE_CODE as representing the code of its mother.  In Phrase Structural Grammar, the terminal nodes Mia, loves, and Kim in a tree like (8) are first marked with a dummy symbol like t, as in (9). It is then replaced by an appropriate lexical item selected from a lexicon by the process of lexical insertion. This fact can also be easily captured in a sequence of tables linked to each other. For this, we first construct a lexicon table like the following:  (10) LEX_CODE n_l n2 v_l v2  CATEGORY N N V V  WORD FORM Kim Mia hates loves  Secondly, the TREE table can be linked to the LEXICON table through the following bridging table:  NODE_CODE 011 021 0221  LEX_CODE n_2 v2 n_l  This is a very rigid table, for it only allows the selection of a single word for each pre-terminal node. But it makes it possible to convey the exact amount of information as represented by tree (8).  For generation, we need a more flexible way of building tables and linking them to each other. From the pre-terminal tree table (9), it should be possible to construct a table frame like the following:  (12) Table name: PARSING8 S_CODE I N I V I N  Here, we first expand the bridging table (11) by adding a few more possible links to it.  (13) Table Name: BRIDGING FOR GENERATION  NODE_CODE LEX_CODE  011  n_l  011  n_2  021  v_i  021  v2  0221  n_l  0221  n_2  On the basis of this bridging table, we can now complete the table frame (12) and obtain the following:  (14)  S_CODE N  V  N  
In Japanese potential and tough constructions, arguments standing in various semantic relations to a base verb can be marked with nominative case. Such subjects also may show up with genitive case in nominalizations irrespective of their semantic relations. This paper, addressing a variety of case alternations which have not hetherto attracted much attention from theoretical linguistics, proposes within the categorial framework that case alternations can be accounted for in terms of the applicability of a type shift rule introducing gaps into predicatephrases and the composition rule which combines base verbs with higher stative predicates passing the information about gaps up to the result categories.  0. INTRODUCTION  In this paper, we propose a completely new analysis concerning nominative and genitive marking in Japanese stative clauses in the categorial framework. Its aim is twofold: to examine a wider range of data than has done in the past, and to provide a unified account of the phenomena called case alternations  in Japanese. Let us begin with an overview of data.' I add the semantic roles of the arguments showing  case alternations to each of the examples:  (1) a. Tanaka-ga sakana-ga/-o tabe-rare-nai-koto  (theme)  Tanaka-NOM fish-NOM/-ACC eat-can-Neg-fact  'the fact that Tanaka cannot eat fish.  b. kono boorupen-ga/-de  hagaki-o kaki-nikui-koto  (instrumental)  this boll-point-pen-NOM/with postcard-ACC write-difficult-fact  'the fact that it is difficult to write a postcard with this boll-point pen'  c. Yamada-no ie-gat-kart  daigaku-ni tuugaku-si-yasui-koto (starting point)  Yamada-GEN house-NOM/-from college-to go-easy-fact  'the fact that it is easy to go to college from Yamada's house'  d. tihoo-gakkai-ga/-notameni jugyoo-ga yasum-e-nai-koto  (reason)  local meeting-NOM/because-of class-NOM skip-can-Neg-fact  'the fact that we cannot skip a class because of meetings of the local society'  e. kono jitensya-gai-no taia-ga kookan-si-yasui-koto (possessor of theme)  this bicycle-NOM/GEN tire-NOM change-easy-fact  add the formal noun koto 'fact' to example sentences throughout this paper. When the stative predicate expresses the property of an individual denoted by the subject that are permanent or relatively stable, the subject should be marked with the topic marker -wa, as in kono boorupen-wa kaki-nikui 'it is difficult to write with this ball-point pen' and if it is marked with nominative case, it receives the exhaustive-listing reading (see Shirai 1985). When it is embedded in the nominalization context, as in (1), the interpretation becomes ambiguous between the exhaustive- listing and neutral-description (Kuno 1973).  'the fact that it is easy to change tires of this bicycle'  f. kono biru-ga/-no  2-kai-de  syokuji-ga dekiru-koto (possessor of locative phrase)  this building-NOM/GEN second-floor-on meal-NOM do-can-fact  `the fact that you can have a meal on the second floor of this building'  The nominative NPs in (1) may show up with genitive case (Ga-No conversion) in nominalized  expressions headed by koto or no 'fact' or in relative clauses.2  (2) a. Tanaka-no sakana-no tabe-rare-nai-koto  (theme)  Tanaka-GEN fish-GEN eat-can-Neg-fact  b. kono boorupen-no  hagaki-no kaki-nikui-koto  (instrumental)  this boll-point-pen-GEN postcard-GEN write-difficult-fact  c. Yamada-no ie-no  daigaku-ni tuugaku-si-yasui-koto  (starting point)  Yamada-GEN house-GEN college-to go-easy-fact  d. tihoo-gakkai-no  jugyoo-no yasum-e-nai-koto  (reason)  local meeting-GEN class-Nom skip-can-Neg-fact  We find another kind of case alternation, Ni-Ga (dative-nominative) alternation with the experiencer  argument in potential constructions, as in simplex sentences including some of the psych predicates, and  they may be marked with genitive case in nominalizations.  (3) a. Taroo-ni/-ga/-no  eigo-ga  hanas-e-nai-koto  Taroo-DAT/-NOM/-GEN English-NOM speak-can-Neg-fact  'fact that Taroo cannot speak English'  b. Taroo-ni/-ga/-no  eigo-ga  wakara-nai-koto  Taroo-DAT/-NOM/-GEN English-NOM understand-Neg-fact  'the fact that Taroo does not understand English'  We attempt to propose a unified account of all these case alternation phenomena within the  categorial framework. In Section 1, I will sketch some background assumptions in the categorial  grammar. Section 2 deals with nominative marking of various arguments of base verbs as observed in  (1) and argue that it is made possible via a rule introducing gaps into predicate phrases and concatenation  of base verbs and the matrix suffix -(rar)e or the adjectives such as -yasui 'easy' or -nikui 'difficult' by  function composition. Section 3, following the property theory proposed by Chierchia (1984, 1985),  proposes a mechanism by which nominative NPs with various semantic roles can be marked with genitive  case. In section 4, we will discuss some of the consequences of the analysis and extend our approach to  case alternations observed in simplex stative clauses.  1. BACKGROUND ASSUMPTION Though categorial grammars have a long tradition in theoretical linguistics, they have been rapidly made progress in recent years, and there is still no framework which all grammarians share in detail. Adopting as a descriptive framework Combinatory Categorial Grammar, CCG, proposed by Steedman (1985, 1987, 1991, 1996), I briefly outline some principles and concatenative rules relevant to the present concerns. CCG analyzes only surface strings of natural language, avoiding descriptive devices such as movement or deletion rules, abstract levels of representation, and empty categories. Combinatory rules may only apply to entities which are linguistically realized and adjacent, building up expressions from words to larger expressions. The modes of combination of expressions are entirely determined by lexical syntactic types, which specify semantic valency and canonical constituent order, and nothing else. For example, the verb eat in English has the category assignment in (1): (4) eats := (S/LNP)/NP The diagonal 'slash' operators encode restrictions on word order. The notation adopted here observes the convention that the argument symbol is always to the right of the slash and the result symbol is to the left, no matter which order function and argument combine in. A forwards slash / indicates that the function takes an argument on the right, and a backslash, notated as `1,' in this paper, indicates the function looking for an argument on its left. The category of eats indicates that it is a function that seeks an NP argument  2In this paper I concentrate on Ga-No conversion in nominalizations with koto 'fact' to maintain the parallelism between nominative and genitive marking in the same context.  (i.e., the object) on its right to form a verb phrase of category S/LNP, and then seeks an NP argument (i.e.,  the subject) on its left to form an S. I introduce only three combinatory rules in (5) relevant for the  present discussions.  (5) a. XIV' Y:a  X fa  Y:a X/LYif  Xfa  b. X/Y:g Y/Z :f B X/Z :gf  YU./ X/LY:g B X/LZ:gf  c. X:a T T/L(T/X)  or  T/(T/LX): Alfa  (5a) is the rule of function application. A function of category X/Y combines with an adjacent argument  of category Y to yield a result of category X and interpretation fa, the result of applying fto a. This rule,  for example, combines a transitive verb with an object to yield the verb phrase, and then, combines the  verb phrase with a subject to produce the sentence. The rule of function composition (5b) allows the  main function of category X/Y to combine with the subordinate function of category Y/Z to yield a  function of category X/Z. (5c) is the rule of type-raising, which for present purposes is confined to  subject NPs. This operation converts a subject NP, which would normally be an argument to a verb  phrase of category NP/LS, into a function looking forward for a verb phrase to produce a sentence,  S/(S/LNP). In order to see how the rules in (5b) and (5c) interact, consider the case of topicalization  here, as in (6):  (6) Mary, John  loves  NPObj NPs„bi T (S/LNP)/NP  S/(S/LNP)  <B  SiNPob;  In (6), loves of category (S/LNP)/NP cannot combine with the object because the object Mary is preposed. Thus, it has to combine with the subject John by function composition, which is type-raised into the  function taking the verb phrase as argument. The resulting expression John loves of category S/LNPobi  finally combines with the topic Mary. In what follows, we use the generalized composition (Steedman 1996:35) in order to deal with combinations of higher functions:  (7) X/Yif (Y/Z)/$: Xz.gz  (X/Z)/$:  The $ notation stands for the remainder. X/$ is thus a variable ranging over the set which includes X and  all functions into X.  Observing a close parallel traditionally maintained between syntax and semantics, we spell out  semantic interpretations using italics following a colon, as in eat (S/LNP)/NP: r.Xy.eat'(x)(y). Here I will briefly explain the category symbols used below. The symbol S stands for clauses, to which I add the features such as fin (finite) or stat (stative) indicated by the subscripts since these features influence the nominative marking of arguments. Since all noun , phrases, whether arguments or adjuncts, must be  followed by one of postpositions in Japanese, I use `TP' (term phrase) as a cover term for all  postpositional phrases, with the subscripts indicating semantic roles and with the superscripts indicating  cases, as in TP'11,1„ (a instrumental argument marked with oblique case). The category VP is assigned to  tenseless clauses (infinitives). I introduce other assumptions as this paper proceeds.  2. NOMINATIVE MARKING AND CASE ALTERNATIONS 2. 1. Nominative Marking of Arguments, Adjuncts and Possessors in Embedded Clauses It has been widely assumed that the notion of tense is crucially involved in nominative marking in Japanese (see Takezawa 1987, Fukui and Nishigauchi 1992, Morikawa 1993, among others). Recasting it under the present framework with no rule directly involving discontinuous constituents, a possible way to license a nominative noun phrase is by it being adjacent to a predicate phrase headed by a finite verb/suffix/adjective. Besides, since it is well known that there may be multiple nominative NPs in stative clauses, we state the licensing condition for nominative marking as in (8) using the $ notation: (8) Multiple subjects may be allowed to occur in stative clauses, if they are adjacent to constituents of category (S fin,t/LTP)/L$ It should be noted here that the licensing condition (8) does not state that all TPs must be marked with nominative if they are adjacent to expressions of (Sfin.stat/LIT)/S• Any argument adjacent to them may show up with inherent case markers, as shown in the examples in (1).  Let us begin with the nominative object in potential constructions. In the generative literature, it  has been accounted for by assuming that restructuring optionally applies to a base verb and the potential  suffix -(rar)e to absorb the case-assigning feature of the verb, and thus, the nominative object should move up to the position where its case can be licensed (the targets of this movement varies among  authors)(see Tada 1992, Inoue 1988, Koizumi 1994, Ura 1999, among others).  (9) Tanaka-ga sakana-gai [  ] tabei-rare-nai-koto  (theme)  I I  I optional verb raising and case absorption.  Such accounts, limiting themselves to the nominative object, cannot explain a wide variety of case  alternation phenomena we observed in (1) and (2) because inherent cases (contra structural cases) in  principle cannot be absorbed and adjuncts cannot directly move up to the subject position. Moreover, the  examples in (10) show that the external argument of base verbs are not suppressed in potential  constructions, which also casts doubt on the case absorption approach. In (10a) involving the subject  honorific verbal form, sensei 'teacher' is always the person whom the speaker considers to be most worthy of respect, irrespective of whether it is marked with nominative or dative, as opposed to the passive case in  (11), where the agent of the base verb is suppressed and the derived external argument okusama 'wife' becomes the person for whom the speaker has a respect. As in (10c), the subject oriented anaphor zibun  'self may freely occur in potential constructions, irrespective of what semantic role the subject has, always  bound by the agent (in (13b), possibly some customers) of base verbs.  (10) a. Sensei-ga okusama-o o-sikari-ni-nar-e-nai teacher-NOM wife-ACC HON-scold-CAN-NEG  'Teacher cannot scold his wife.'  b. Sensei-ni okusama-ga o-sikari-ni-nar-e-nai teacher-DAT wife-NOM HON-scold-CAN-NEG  c. Kono hootyoo-wa ookina-sakana-o go-j ibun-de o-sabaki-ni-nar-e-masu. this kitchen-knife-TOP big-fish-ACC HON-SELF-by HON-cook-CAN-PRES  'You can cook even a big fish with this kitchen knife.'  (11) a. Sensei-ga okusama-o o-sikar-ni-natta.  teacher-NOM wife-ACC HON-scold-PAST  'The teacher scolded his wife.' b. Sensei-ni okusama-ga o-sikar-are-ni-natta.  teacher-BY wife-NOM HON-scold-PASS-PAST Assuming that case absorption tightly correlates with the suppression of an external argument, we cannot  explain the nominative object in (10) in the same way as in the passive case of (11). This fact, together  with the inability to account for a variety of case alternations in (1), shows that the case absorption  approach is completely untenable. Though the structures of lower clauses are different, the potential and tough constructions share the  p roperty that they allow any argument or adjunct of an embedded clause to be the matrix subject. 3 Thus, as far as the nominative marking of a variety of arguments, they should involve the analogous process of  derivations. In this paper, we assume that both constructions have the null operator structure in the  terminology of generative grammar. The structure of (lb) can thus be illustrated as in (12b), where the  trace ti occupies an adjunct position of a lower clause):  (12) a. kono boorupen-ga hagaki-o kaki-nikui-koto b. [[kono boorupen-ga][Ap[cp Op; [11) [vp PRO [v. ti [v, hagaki-o kaki] nikui]]] Within the framework of CCG, unbounded dependencies as in wh-movement or null operator structures  are treated by function composition. The information about a missing argument, which was not  introduced in the normal way, is passed up from a subordinate function to the composed function.  Before presenting concrete derivations, apart from wh-movement, we have to distinguish at least the  two kinds of unbounded dependency constructions in Japanese, as shown in (13):  (13) a. kono hootyou-de  syosinsya-ni-mo  yasai-ga  kir-eru-koto  this kitchen-knife-with beginners-DAT-EVEN vegetables-NOM cut-CAN-fact  3The potential suffix subcategorizes only transitive and unergative base verbs, while tough adjectives take any verbal forms including passives (Ano bokusaa-ga ut-are-nikui. 'That boxer is difficult to hit'). Besides, the postposition nitotte is preferred to indicate the experiencer of tough adjectives (or the agent of base verbs) in tough constructions.  `Even beginners can cut vegetables with this kitchen knife.'  b. kono hootyou-ga  syosinsya-ni-mo  yasai-ga  kir-eru-koto  this kitchen-knife-NOM beginners-DAT-EVEN vegetables-NOM cut-CAN-fact  In (13a) scrambling simply moves the instrumental argument to the sentence initial position, but the  oblique case marker de 'with' shows that the syntactic relation between the instrumental argument and the  base verb is maintained and, it has the same meaning as the corresponding expression without scrambling.  In (13b), on the other hand, the instrumental argument is the matrix subject which fails to indicate any  morpho-syntactic relation with the base verb. Semantically, (13b) predicates the property of the entity  denoted by the subject kono hootyoo. As will be shown in Section 5, these two constructions show the  difference in scope interpretation. Though the notion of syntactic conectivity, which has long been the  focus of debate in the literature (see Jacobson 1992; Bayer 1990; Hukari and Levine 1991; Steedman 1996  among others), is crucially involved in the two kinds of dependencies in (13), I do not have the space to go  into such vexed questions in the present discussion aiming at a descriptive generalization concerning case  alternations in Japanese. For now let us assume that the scrambled adjunct and its gap in (13a) exhibit  syntactic connectivity, whereas the nominative adjunct and its gap in (13b), displaying case mismatch,  lack such connectivity. To distinguish the two types of gaps in (13), I notate the extraction site in  scrambling as the usual slash, and the missing argument in a null operator structure as 'ITP', which I  borrow from Jacobson's (1999) notation. 4 Again I indicate the semantic role of the missing argument/adjunct by subscripts, as in 1TPinstru1nent, but it bears no specification of case. Because the information about a gap is transmitted from a lower clause to a matrix sentence through derivations via  composition, the matrix predicate phrase would be of category S finITP a . The subject in these constructions is taken to be of category SOITP), which indicates that it should be associated with a gap in a predicate phrase. The association of the subject and its gap is not carried out syntactically, but semantically or pragmatically (Jacobson (1992) proposes a meaning postulate for the association between the filler and gap in tough constructions). As supporting evidence for lack of syntactic connectivity, consider (14): (14) Yuugata-ni-wa reddo-heddo-ga yoku tur-eru. evening-in-TOP read-head-NOM well catch-fish-CAN-PRES Those who do not enjoy fishing will take reddo-head, the lure with its head painted red, to be the theme of the base verb tur, but anglers using lures can easily associate it with the gap in a instrumental adjunct position. In a passive clause of the form X-ga V-rare-ru, X must be the theme of V even if we do not know what X stands for, whereas any argument can be the subject of potential and tough constructions in Japanese, which suggests that the relationship between the subject and the gap is not morpho-syntactic but  semantic/pragmatic.6 Note that having a gap (ITP) in an embedded clause and thus, in a derived matrix VP is a lexical property of the potential suffix or tough-type adjectives. Non-stative sentences and some of apparently stative sentences (such as control sentences) do not allow adjuncts or possessors to become the matrix subjects. Also note that, since multiple subjects show up in the potential/tough constructions, the matrix VP may contain multiple gaps corresponding to the subjects. So the category of such predicates will be  of SITP al ...ITP an. Though each association of a subject and a gap will refer to a version of thematic hierarchy, it is essentially carried out semantically and/or pragmatically. From now on, let us consider some concrete examples. First, take the example (la), repeated  4To deal with unbounded dependency in quantification, Jacobson's (1999) proposes the type shift rule shifting an expression of category (A/B)/C... to (AIC)/B ..., and as to long-distance extraction, an item of category (S/...)/S can shift to ((SIA)/...)/(SIA), where the extraction gap feature is passed from S node to result S node. If we adapt this kind of type shift to potential and tough predicates, their category may be of (SITP a )/TPExp .../(VPITP a )...(where a indicates a semantic role which the gap bears), and we can attribute case alternation phenomena to the type shifting of the suffix/adjective, but we present derivations involving function composition for expository convenience. 5Nevertheless, it should be noted that we somehow need to introduce a gap/gaps into the predicate phrases in these constructions. If all arguments of a predicate are saturated, the resit must be a proposition, not a propositional function which can be predicated over the property of the entity that the subject denotes. We assign the semantic type <e,t> to an item of category SITP a , functions from individuals to propositions, as in the standard verb phrases.  below, which shows the nominative/accusative alternation of the object.  (1 5) a. Tanaka-ga sakana-o  tabe  rareru  TPExp TP Th  VP/LITPA"Th (SfidTPExp)/LVP  VP:tabe'(sakana')  S fin/LTPExp : Ax.tabe' ° rareru'(sakanal)(x),  S fin: tabe' ° rareru'(sakana')(Tanaka')  b. Tanaka-ga sakana-ga tabe  rareru  TPExp S/(SITP) VPITP Th  (S finaPap)/LVP <B  (SfintTPTh)/LTPExp: Xx.Xy.tabe' ° rarerut(x)(y)  S finkTPExp : Xy.tabe' ° rareru'(sakana')(y)<  S fin tabe' ° rareru'(sakana')(Tanaka')  Functional composition of two functions f and g is commonly written as f ° g' (-- -1(g(x))), and we reverse the order of main and subordinate functions to reflect the stem-suffix order of Japanese complex predicates. We leave the case marking of the experiencer to the next section, concentrating on the case alternation of the theme. In (15a), the base verb tabe 'eat' and the object sakana-o 'fish' are concatenated via usual application to produce the embedded VP, which is combined with rare via the same process. In (15b), on the other hand, the base verb first composes with the potential suffix by (7), encoding the information about the missing object on the output, i.e., the complex verb tabe-rare 'can eat', which takes the nominative object as argument. From now on, we will omit the derivations in which the arguments, adjuncts or possessors show up with their original case assigned by base verbs, because they are canceled in the normal way (by application), and present only the derivation in which they are marked with nominative case. We also omit the experiencer arguments from the derivations below to discuss them in the next section, and  introduce one' into the interpretations to indicate an experiencer with an arbitrary interpretation. 6 Next,  observe the derivation of (lb) with the adjunct marked with nominative case, which will be illustrated as  in 16)  (16) kono boons-pen-ga hagaki-o  kaki  nikui  S/(SITPins)  TPAcen (VPITPInst)/LTPTh_< SfinIVP  VP ITP hist : Xy.k-aku'(hagak-i l)(one')(with-y) <B  S fin n ITPI st : Xy.kaki' ° nikui l (hagakiXoneNwith-y) <  S fin : kaki' ° nikui'(hagaki')(one')(with-boorupen')  We assume, following Steedman (1996:41, 77), that adjuncts are also subcategorized by verbs in some sense and that they are the most oblique (and optional) arguments of verbs. In (16b), the missing instrumental argument of the base verb becomes a feature encoded on the category of the infinitive, as in VPITPE„,, which is passed up to the final result predicate phrase through the derivation, and some semantic/pragmatic predication rule will link the subject kono boorupen-ga and the gap in the predicate phrase. Note here that the matrix VP in (16b) is of category SITP a and of type <e,t>, as in normal VPs (intuitively in (16), denoting the set of individuals with which it is difficult to write a card). Let us turn to the nominative marking of the possessor of an embedded argument, as in (1e) and GO. The derivation of (1e) with the nominative possessor can be shown as in (17):  6When an argument is deleted by 'Argument Drop' (Chierchia 1984, Jacobson 1992), the position of the deleted  (17) kono jitensya-ga S/(SITPpos)  taia-o  kookan-si  yasui  TPThiTPposVP/LTP'ecThSfin/LVP  VPITPpos : Xx.kookan-si'(poss(taid)(x))(one i) <B  S finITPpos : Xx. kookan-si' ° yasui'(poss(taia')(x))(one')  San : kookan-si g yasuf(poss(kurumat)(banpaa))(one') Space precludes a discussion of the internal structure of noun phrases, but notice here that the cateogry TP/LTP (a function from individuals to individuals) is assigned to nouns referring to a part of an entity, not a whole entity, such as taia 'tire'. When the possessor is marked with genitive case, it combines with the possesee TP to yield kono jitensya-no taia 'the tire of this bicycle'. In (17b), on the other hand, the information of the missing possessor as a feature ITPp os is passed up via the iterative application of composition from the object TP to the final result category of the matrix predicate, which, intuitively, refers to the entities the tires of which are easy to change. In this section, we propose that the predicate phrases of potential and tough constructions are assembled by function composition, carrying over the information about gaps in embedded clauses. This view uniformly accounts for all the case alternation phenomena observed in (1) with no new tricks invoked. Because the subcategorization for an infinitive including a gap is a lexical property of the potential suffix and tough-type adjectives, the function composition analysis can not be generalized to other non-stative verbs and some of stative predicates which does not allow to arguments other than the object to be marked with nominative (i.e., which have no gaps in their predicate-argument structure).  2. 2. Dative-Nominative Case Alternation Let us turn to the alternation between dative and nominative case in potential constructions. This case alternation shows the properties slightly different from those discussed so far. First, consider whether the dative marked argument is the experiencer of the suffix (rar)e or the agent of base verbs. If it is the agent of a base verb (as in the causative or indirect passive constructions), the analysis proposed in 2. 1 also holds for the ni-ga alternations. It has been argued in the literature, however, that the dative argument occupies a higher position than other arguments. Consider: (18) Yamada,-ni [jibuni-no kimoti]-ga syoojiki-ni hanas-e-nai-koto Yamada-DAT self-GEN feeling-NOM frankly talk-CAN-NEG-fact 'the fact that Yamada cannot talk about his feelings frankly' It has been argued that the fact that the dative NP can antecede the subject-oriented anaphor in the nominative object indicates that it really has the status of subject and that its position is higher than that of nominative NPs (see Ura 1999, Takezawa and Whitman 1998 for relevant discussions). Assuming, though, that the phenomena such as reflexivization and honorification are sensitive to the argument structures of predicates, not to the surface case marking or configurational hierarchy, the antecedentanaphor relation in (18) at most suggests that the dative NP is an external argument of the higher suffix and/or a controller of the agent of the base verb. Nevertheless, the fact that some simplex psych-predicates show similar case arrays seems to lead us to acknowledge that the suffix (rar)e in fact subcategorizes for this dative argument. (19) Yamadai-ni [jibuni-no kimoti]-ga yoku wakaru. Yamada-DAT self-GEN feeling-NOM well understand 'Yamada understands his feeling well.' As will be touched on Section 4, the noun phrases marked with original cases and those marked with nominative case display different behaviors with respect to scope interpretation. The ni-ga alternation, however, does not affect scope interpretation, which appears to indicate that the dative NP is an argument of the matrix predicate (rar)e. (20) a. hahaoya-dake-ni kodomo-ga sodate-rareru-koto mother-only-DAT child-NOM grow.up-CAN-fact 'It is only mothers that can bring up children.' argument have to be existentially quantified over, but I simplify the derivations for brevity.  b. hahaoya-dake-ga kodomo-ga sodate-rareru-koto (the same meaning as (18))  cf. hahaoya-dake-de kodomo-ga sodate-rareru-koto  'Children can be brought up by mothers alone.'  We assume that to maintain the parallel between the suffix (rar)e and simplex psych predicates with  respect to the ni-ga alternation, the dative NP in (18) is the argument (experiencer) of the suffix -rare,  which controls the agent of base verbs (this control relation is dealt with under the lexical entailment  theory of control, which I can not discuss for reasons of space (See Chierchia 1984 and Dowty 1985).  We can then explain the ni-ga alternation observed in potential and other dative subject constructions  in a unified manner. Suppose again that (rar)e project the predicate phrase containing at least one gap,  and that the position of the experiencer in the matrix clause can be a gap. We adopt a type shift rule  suggested in Jacobson (1999) shifting an expression of category (AJB)/.../C into ((AIC)/B... Then, the  suffix rare of category ONP (s/TpDatEx .... will have an analogue of (SITP ExONP ..., which contains a  missing experiencer argument. The latter item is of tyep <e,t>, which qualify as a predicate.  (21) Yamada-ga jibun-no kimoti-ga hanas-  e-ru  S/(SITP)  TpNora  TP NP VP/TPTh (S I Exp)  <B  (SITPE„p)1TPTh: Xx.Xy.hanas' ° eru'(x)(y)<  SITPExp : Xy.hanas' ° eru'(kimoti)(y)<  S:hanas' ° eru'(kimoti')(Yamada')  Assuming that the matrix VP which rare or other simplex psych predicates project must contain at least one gap, we can account for the ungrammaticality seen in (22). (22) a. *Yamada-ni oyog-e-nai-koto Yamada-DAT swin-can-NEG-fact 'the fact that Yamada cannot swin' b. *Yamada-ni jibun-no kimoti-o hanas-se-nai-kot (= (18)) c. *Yamada-ni sono jijoo-o wakaru-kot Yamada-DAT the situation-ACC understand-fact 'Yamada can understand the situation.' (cf. Yamada-ni sono jijoo-ga wakaru-koto, Yamada-ga sono jijoo-o wakaru-koto7) Under the assumption that the VPs projected by the potential suffix and psych predicates allowing for the ni-ga alternation are category of S1TP a , all arguments of base and matrix verbs in (22) are cancelled via application and the resulting expressions contain no gap, which is not compatible with the category specification of such predicates. We can account for the ni/ga alternation of the experiencer by the simple type shift rule introducing gaps into the (matrix) predicates, and the applicability of this rule should be encoded on the lexical entries of the predicates which show the case arrays discussed above.  3. NOMINALIZATION AND GA-NO CONVERSION  As observed in (2), the term phrases marked with nominative case can freely show up with genitive case  irrespective of their semantic roles in the appropriate contexts. In this paper, we will concentrate on ga-  no conversion in the nominalizations headed by the formal noun koto, to maintain the parallelism between  the contexts of nominative and genitive assignments. First, observe the examples in (23):  (23) a. kono hootyoo-ga  yasai-ga  yoku kir-eru-koto  this kitchen-knife-NOM vegetables-NOM well cat-CAN-fact  'the fact that we can cut vegetables smoothly with this kitchen knife.'  b. kono hootyoo-no  yasai-no  yoku kir-eru-koto  this kitchen-knife-GEN vegetables-GEN well cat-CAN-fact  The instrumental and theme arguments are marked with nominative case in (23a) and with genitive case in  (23b). It has been argued in the generative literature that multiple occurrences of nominative NPs, as in  (23a), are due to the property of 141 bearing some feature specification of stativity which can enter into  71 find that this sentence is unacceptable, but younger speakers tend to find it grammatical. See Ura (1999).  multiple feature-checking relations with arguments (for example, see Ura (1999)). In parallel with the  assumption that the notion of finiteness is responsible for the nominative assignment, suppose that the  presence of head noun is responsible for the genitive case assignment. If the relevance of Infl to the  nominative case assignment is correct, the multiple genitive phrases in (23b) should be licensed by the  head noun koto specified for stativity because, if the potential suffix indicating stativity is removed from  (23b), the instrumental and theme arguments cannot be marked with genitive case. This line of reasoning  seems quite curious. There is no mechanism proposed in the literature which enables oblique arguments  to be marked with genitive case, to begin with.  If we extend the function composition analysis of stative predicates and the mechanism transmitting  the informations about gaps to the final result category, the genitive marking observed in (23b) and (2) can  be straightforwardly accounted for. First, we need to state the adjacency requiment on the genitive case  assignment to handle long-distance dependency between the (multiple) genitive phrases and the head noun  koto, as in (24):  (24) The genitive phrases must be string adjacent to the constituents of category (NPITP „)/$.  Another assumption we need is concerning nominalizations. Chierchia (1984, 1985) argues that  properties, the meanings of VPs, have two forms or modes of being: propositional functions and  individual correlates of propositional functions. That is, the expression denoting a property can be  realized either as a finite predicate phrase or as its nominalization. According to his suggestion, not only  Ss (propositions) but VPs (propositional functions) can be nominalized. For example, in Uso-o tukukoto-wa yoku-nai 'Telling lies is not good', the nominalized VP shows up as the subject and refers to as an  individual. Following Chierchia, we notate the nominalized property with the operator '', as in  (telli (lies'). Since the nominalized expression as such cannot take the subject as argument, it has to be  predicativized (i.e., denominalized) so as to be saturated by the subject. Let us assume that the case  marker no acts as the predicativizer, which is indicated by the operator ' c'. Namely, it applies to a  nominalized property, and returns the original predicate phrase, which is to be nominalized again after it  combines with the subject. No is thus translated as in (25):  (25) no := IxAdyn[uy(x)] Keeping these assumptions in mind, the derivation of (23b) can be illustrated as in (26), where the  operation of the predicativizer no is omitted.  (26) kono hootyoo-no yasai-no  kir  eru  koto  TP Gen  TPGen (VPITPast)ITPTh S fll-AVP NP/L S fin  (S fin ITPirlSt)ITPTh Xx.Xy.kir' ° eru'(x)(one)(with-y) <B  (NP ITPinst)ITPTh: nkx .Ay.kir' ° eru'(x)(one')(with-y)< NPITPInst: (Xy.kirt ° eruVasaii)(one)(win-.0,  NP: 'kir' ° eru'(yasai)(one)(with-hootyoo')  In (26), the base verb composes with the suffix, and the resulting complex verb again composes with the formal noun koto 'fact' of category NP/LS fin. The information about the missing theme and instrumental argument is passed up from the base verb to the nominalized predicate phrase kir-eru-koto. Since case conflict (suggesting the lack of connectivity) arises between the TPs marked with genitive case and the gaps in the predicate phrase, we have to invoke some semantic predication rule which allows us to associate the fillers and gaps. After the genitive phrases are interpreted in an appropriate way, the whole expression be shifted back to the individual by the type shifter no. According to Chierchia (1984, 1985), if we assume that predicate phrases as well as clauses can be nominalized and that the genitive marker act as a type shifter shifting nominalized predicates (individuals) to predicates (properties), we can extend our function composition analysis of potential and tough predicates to their nominalized expressions with koto, derive all the case alternations found in these constructions, as in Kono hootyoo-de/hootyoo-ga/hootyoo-no kir-eru-koto, and account for the fact that nominative phrases can be converted to genitive phrases without exception irrespective of their semantic roles. On the other hand, the case absorption approach to the nominative/accusative alternation cannot provide a principled solution to a wide range of case alternation phenomena observed in 2. 1 and 2. 2., including the nominalization phenomena. Note also that the interpretations given at each step in  derivations correctly reflect our intuitions about the meanings of expressions.  4. EMPIRICAL CONSEQUENCES OF THE PROPOSED ANALYSIS  Let us consider some consequences of our approach to the case alternation phenomena here. The first is  the interaction between scope interpretation and case marking. Observe the following examples:  (27) a. John-ga migime-dake-o  tumur-e-ru-koto  John-NOM right-eye-only-ACC close-can-fact  'John can close only his right eye.'  (can > only, only > ?*can)  b. John-ga migime-dake-ga tumur-e-r ►-koto  John-NOM right-eye-only-NOM close-can-fact  (only > can, *can > only)  c. John-no migime-dake-no tumur-e-ru-koto  John-GEN right-eye-only-GEN close-can-fact  (28) a.- boorupen-dake-de  rippana tegami-o kak-(ar)e-ru-koto  ball-point-pen-only-by good letter-ACC write-can-fact  'the fact that you can write a good letter only by a ball pen. (can > only, only > can)  b. boorupen-dake-ga  rippana tegami-o kak-(ar)e-ru-koto  ball-point-pen-only-NOM good letter-ACC write-can-koto (only > can, *can > only)  c. boonipen-dake-no  rippana tegami-no kak-(ar)e-ru-koto  ball-point-pen-only-GEN good letter-GEN write-CAN-PRES-fact  The difference in scope in (27a) and (27b) have repeatedly been picked up in the literature (see Tada 1992,  Koizumi 1994, Ura 1999, among others). They observes that, when the object is marked with accusative  case in (27a), it has the narrow scope reading with respect to the potential suffix -(rar)e, whereas the  nominative object in (27b) has scope over the potential verb, intuitively saying that John cannot close his  left eye. The point is that the nominative object in (27b) cannot take narrow scope with respect to -(rar)e.  Koizumi (1994) and Ura (1999), assuming Tada's observation, argue that the nominative object moves up  to a position higher than that of the suffix to get the wide scope reading.  This account, however, cannot explain the fact that the nominative-oblique case alternation as in  (28), as expected, shows the same behaviors concerning interpretations of quantified NPs. Boorupen  'ball-point pen' marked with oblique case in (28a) is ambiguous between the narrow scope and wide scope  readings with respect to the potential suffix, whereas, when it is marked with nominative case in (28b), it  must take the wide scope reading. Note also that genitive case parallels nominative case with regard to  the scope interactions of quantified NPs and the potential verb, as seen in (27c) and (28c). We can  account for such interactions between case marking and scope interpretation without additional  assumption. The embedded clauses and the suffix (or tough adjectives) are combined by composition  retaining the information about missing arguments encoded on the category of predicate phrases, which  denote the properties of missing arguments and adjuncts. The predicates phrases in (28) and (29) denote  singleton sets, only one member of which is the entity denoted by migime 'right eye' in (28) and the entity  denoted by boorupen 'ball-point pen' in (29). Therefore, the nominative and genitive phrases cannot  take narrow scope readings with respect to the suffix.  Another consequence of our analysis is that it could give an interesting account for (at least some  of) case alternations in simple stative sentences. Consider (29), which have long been the topic of  investigation in Japanese linguistics.  (29) a. kono inu-no/-ga atama-ga siroi-koto  this dog-GEN/NOM head-NOM white-fact  'the fact that this dog's head is white'  b. Tokyo-ni/Tookyo-gaiTokyo-no  nezumi-ga ooi-koto  Tookyo-LOC/Tookyo-NOM/Tokyo-GEN rat-NOM numerous-fact  'The fact that there are a lot of rats in Tokyo'  Kuno (1973) proposes the rules to change genitive or locative case on the sentence initial NPs to  nominative case. It has recently been assumed in the generatlive literature that the subject is multiply  licenced by the appropriate functional category concerning finiteness in stative sentences (e.g, see Ura  1999). It seems to be insufficient to discuss only the conversion of surface case or multiple occurrences  of the subject, however. What is noteworthy about sentences as in (29) is that, when marked with  nominative case, a sentence initial NP receive the reading in which the entity referred to by the subject has  the property which the residue of the sentence (the derived predicate phrase comprising another  nominative NP and the predicates) denotes.  We can derive the multiple subject constructions with appropriate interpretations in the same way as  the tough and potential constructions, using the gap introduction device and composition rule in (7).  Thus, (29a) with the nominative possessor is derived as in (30):  (30) Kono inu-ga  atama-ga  siroi  TpNo ►  TDNorn irrp Thl Pos  SITPTh <B  S ITPpos : Ax.siroP(poss(head)(x))  In kono inu-no atama, the category TP/LTPpos is assigned to atama 'head' because it is not an independent entity but a part of it (a function taking possessors to return their heads). In (30), the possessor is missing and the category of the subject atama shifts to TPITPpos. The information about the possessor is finally encoded on the output category S, which shifts to the predicate of type <e,t> and can denote the property of another argument (i.e., the missing possessor), roughly, the set of possessors whose heads are white. The derivation of (29b) with the locative phrase marked with nominative is quite simple. Assume that predicates optionally subcategorize for adjunct phrases. In (29b), the locative phrase is missing, and the category of ooi shifts from SkTPTh/TPLoc to (SITPL.,)/TPTh. After the predicate combines with the theme to yiled the derived predicate nezumi-ga ooi of category SITPLoc and of type <e,t>, denoting a property of Tokyo, yielding the reading in which Tokyo is such that there is a lot of rats there. We can easily derive more complex expressions such as Tokyo-ga tosibu-ni nezumi-ga ooi 'In the downtown of Tokyo, there are a lot of rats' exactly using both the type shift rule and composition. Under the approach pursued here, we can explain the fact that the genitive phrase in (31) is once subjectived, and then, marked with genitive case again because the constituent like [kono inu-no tokuni a tama](the sequence of NP-Adverb-NP) are impossible in Japanese. (31) kono inu-no tokuni atama-ga siroi-koto this dog-GEN especially head-GEN white-fact In (31), the derived paredicate phrase atama-ga siroi as such can concatenate with the adverb tokuni 'especially' and then nominalized. The genitive phrase kono inu-no will finally be associated with the gap of the posssessor in the nominalized predicate in the appropriate way.  5. CONCLUSION We proposes a unified account to a wide range of case alternation phenomena in stative sentences under the categorial framework. Some of the stative predicates in Japanese, including the potential suffix and tough predicates, have the property of encoding the information about a gap (or gaps) on the category of resulting predicate phrases. The subjects of category S/(SITP ...T P ) are multiply licensed by being 
 concrete  Figure 2: Distribution of Abstract and Concrete Nouns which collocate with 'big'  3.3 Distribution of Abstract Noun Collocates To further study the abstract collocates of big, it is desirable to find out what these  abstract nouns are and how the Taiwanese learners use them differently from native speakers. Three kinds of data are concerned here: (1) abstract nouns (N) which collocate with big more than once in TLCE, (2) freq(big, N)–the frequency of co-occurrences of big and N, and (3) the ratio of freq(big, N) to freq(N). Table 2 shows the comparisons of these figures in both TLCE and BNC. As some collocations of big in TLCE do not occur in the subset of BNC, a complete BNC of 100 million words is consulted for comparison.  Abstract Noun (N) problem trouble surprise deal burden pressure joke turn  TLCE  0.287 million  freq(big, N) ratio(%)  10  3.8  4  7.5  4  12  3  16.7  3  16.7  2  2.0  2  6.9  2  5.0  BNC  One million  100 millions  freq(big, N) ratio(%) freq(big, N) ratio(%)  3  0.6  128  0.4  
The present study investigates a type of metaphor involving socio-cultural values in their mapping and interpretation. The linguistic data are Japanese metaphorical expressions that conceptualize women as plants or animals. First, the typology of metaphor based on previous research is discussed, focusing on conceptual, correlation, and resemblance metaphors, followed by our proposal to distinguish "sociocultural metaphors" within resemblance metaphors. The main part analyzes the data to illustrate various characteristics of socio-cultural metaphors, which is divided into the following sections: 1) Women as Animals or Plants, 2) Some Mapping Gaps and Asymmetry, 3) Properties Involved in the Mapping, 4) Socio-cultural Codes, and 5) Social Structures and Interpretation of Woman Metaphors. The result of the questionnaire survey reported in 4) seems to suggest that interpretations and usage of some metaphorical expressions have undergone certain changes over the years. The study concludes by suggesting further research in socio-cultural metaphor in Japanese and other languages. 1. INTRODUCTION Since Lakoff and Johnson [1], the study of metaphor has made great progress in cognitive linguistics. It has been well established that there are inter-domain mappings of concepts in two different domains, and that the set of correspondences between them are called "conceptual metaphors." Research has also shown that in the most basic conceptual metaphors, the image-schematic structure of the source domain is preserved in the target domain. Up till now, the focus has been on the conceptual metaphors based on the image-schematic correlation. While such metaphors are important for the research in cognitive semantics, other kinds of conventional metaphors deserve more attention. Recent development in this field of study, however, has seen other conventional metaphors identified and classified in a more elegant way. One such example is the work of Grady [2] who has distinguished motivations for two kinds of metaphors, "correlation metaphor" and "resemblance metaphor." The present study makes a further distinction within resemblance metaphors, and proposes what we may call "socio-cultural metaphor." This is exemplified by Japanese metaphors that conceptualize women in terms of animals or plants. We present a model of socio-cultural metaphors illustrating how they are formed as a result of socio-cultural interpretations of the source and target concepts.  2. PREVIOUS STUDIES  2.1. Conceptual Metaphor Theory From the time of Aristotle, metaphor had been treated in the western tradition as a linguistic device used in rhetoric and literature. It was considered that all metaphors had literal meanings and were used by specialists for purposes such as to persuade people or to express an imaginary world. On the contrary, Lakoff and Johnson [1] claimed that metaphor is not just a matter of language, but that it governs our ordinary conceptual system. According to their view, "human thought processes are largely metaphorical" and the "human conceptual system is metaphorically structured and defined" (6). Since then, research on metaphor has headed towards a new direction, and the basic ideas of Lakoff and Johnson [1 have been developed as the Conceptual Metaphor Theory. In this theory, metaphor is defined as "a cross-domain mapping in the conceptual system" (Lakoff [3]: 203). Thus, "metaphorical expression" is used to refer to an individual linguistic expression or "a surface realization of such a cross-domain mapping" (203). The metaphor involves two domains, namely, a source domain and a target domain. The latter is understood in terms of the former, so the convention is to call each mapping as "TARGET-DOMAIN IS SOURCE-DOMAIN" or "TARGET-DOMAIN AS SOURCE-DOMAIN." For example, statements like "Look, how far we've come." and "Our relationship has hit a dead-end street." are based on the metaphor LOVE IS A JOURNEY where the love relationship is regarded as traveling through space.  2.2. Primary Metaphor  Grady [4] elaborated on the Conceptual Metaphor Theory by distinguishing between complex and primary metaphors. He reanalyzed the data presented in Lakoff & Johnson [1] as THEORIES ARE BUILDINGS and proposed that this is a complex metaphor which consists of two primary metaphors: ORGANIZATION IS PHYSICAL STRUCTURE and PERSISTING IS REMAINING ERECT. Here are some metaphorical expressions for each metaphor (Grady [4]):  (1) ORGANIZATION IS PHYSICAL STRUCTURE  a.  They tore the theory to shreds. (272)  b. The theory has completely unraveled. (275)  (2) PERSISTING IS REMAINING ERECT  a.  Your facts are solid, but your argumentation is shaky. (269)  b. All the arguments are solid, but they can't stand up without a factual basis. (269)  The metaphor THEORIES ARE BUILDINGS does not have an experiential basis, which calls into question whether this is a conceptual metaphor. Conceptual metaphor requires a physical motivation. The above analysis makes it possible to explain the motivation of mappings in terms of physical experiences.  2.3. Correlation Metaphor and Resemblance Metaphor The typology of metaphor is an issue that needs further investigation, since there are many metaphors that are not explained by co-occurrence of certain phenomena. Grady suggests a "Resemblance Hypothesis" which distinguishes between conceptual metaphor (e.g., MORE IS UP) and resemblance metaphor (e.g., "Achilles is a lion.") clarifying how the nature and the process of mapping differ in these two types [2].  According to Grady's model, primary metaphors could be characterized as links between distinct concepts, perhaps based on numerous experiences where the concepts are tightly correlated and therefore simultaneously activated ([2]: 8). In the case of MORE IS UP, the two phenomena in different domains – quantity increase and vertical elevation – often co-occur so that they are cognitively correlated. On the other hand, the mapping of resemblance metaphors does not involve such correlation, but rather, shared features of two different schemata are activated. As for the example, "Achilles is a lion," lions and people have separate sets of features in the conceptual schema, but they both have "bravery" which motivates the metaphor.' 2.4. Variation among Resemblance Metaphors It seems that there is some variation among resemblance metaphors. Image metaphor is an example where the metaphor is motivated by physical similarity between the source and target concepts. An expression such as tori-hada-ga tatu (bird-skin-NOM stand: skin turning into goose-flesh) would be an example. On the other hand, there are resemblance metaphors that are not based on literal similarity. Let's re-examine the above example, "Achilles is a lion." Lions are perceived to be brave because of the fact that they are carnivores whose nature is to hunt other animals. In human society, hunters who risk their lives by challenging stronger animals are considered brave. It seems that lions are brave only in the sense of killing other animals, but those animals may not be harmful to the lions. Although many other animals hunt (e.g., cats hunt mice), they are not considered brave. The idea that lion is a prototype of brave animals is symbolic and arbitrary. It seems that resemblance metaphors require further distinction. Then, the question to ask is: How can we account for resemblance metaphors whose source and target concepts do not have an apparent commonality but are associated under a socio-cultural concept? 3. SOCIO-CULTURAL METAPHOR In order to answer the above question, we propose a variety of metaphor that we call "socio-cultural metaphor." This is a kind of metaphor in which socio-cultural interpretations of the source and target concepts play a crucial role in the mapping. Again, let's take "Achilles is a lion." as an example. This mapping requires several steps. First, the source and target concepts are associated by socially defined properties: bravery is socially defined for human beings, and the lion is determined as a proto-typical category that possesses bravery. Then, the source concept "bravery of lion" is mapped onto the target concept "bravery of human." Putting this process into a general model, we may get the following (see 4.4 for further discussion): (i) The property of the target concept is defined socially. (ii) The property of the source concept is defined socially. (iii) The source concept is mapped onto the target concept because the properties defined in (i) and (ii) are alike. The motivation for this mapping is that the source and target concepts share a similar property that is socially defined. We will call this model the "socio-cultural metaphor model." This kind of metaphor is I Grady [2] does not discuss the motivation for this metaphor in detail, but he does mention: "My proposal does not imply that there is any literal similarity whatsoever between brave people and lions." (7)  included in resemblance metaphors as defined by Grady [2], but it is distinguished from other types of resemblance metaphors in that it involves socio-cultural interpretations of the source and target concepts. This distinction is crucial in explaining the motivation of metaphors that describe human beings in terms of other concepts such as animals, plants, objects, etc. For example, HUMAN BEINGS ARE MACHINES (e.g., "I ran out of gas.") does not mean that human beings require the same kind of energy as machines. There are also metaphors that involve gender differences. Men and women are not only genetically different, but they are also given separate roles, and thus perceived differently in some societies including Japan. This is reflected in metaphorical expressions that are specifically used for either male or female. In what follows, such metaphorical expressions in Japanese will be examined, applying the socio-cultural metaphor model.  4. WOMAN METAPHOR IN JAPANESE  4.1. Women as Animals or Plants  In this section, we will examine metaphors in Japanese that conceptualize women as something else ("woman metaphors" henceforth), by which we illustrate our model of socio-cultural metaphor, and discuss how well it can explain the mapping involved in Japanese woman metaphors. The pioneering work on Japanese woman metaphors is done by Hiraga [5], but the present study is more concerned with the typological issue, and the motivation and the mapping process of socio-cultural metaphor. In Japanese there are varieties of conventionalized metaphorical expressions referring to women. Among them, we focus on metaphors with animals and plants as the source concepts.' Consider examples in (3).  ( 3 ) a. b. c. d. e. f.  ofisu-no hana : office-GEN flower (flower in the office) kabe-no hana : wall-GEN flower (flower on the wall) yamato-nadesiko : Japanese-pink flower (a kind of Japanese pink flower) yoru-no choo : night-GEN butterfly (night butterfly) uguisu-joo: nightingale-girl kago-no tori : cage-GEN bird (a bird in a cage)  All of these examples refer to women. The expression (3a) refers to a female worker in an office, (3b) a woman who cannot join the conversation at a party and just stands close to the wall, (3c) an ideal Japanese lady who has tender, graceful and well-disciplined behavior, (3d) a woman working at pubs serving for men, (3e) a female announcer whose voice is beautiful, (30 a woman kept in a house or a room by her husband or employer, not allowed to go out freely. In these examples, women are conceptualized as flowers, butterflies, or birds. There are many other examples of metaphors in Japanese that conceptualize women as some kind of animals or plants (see 6. Appendix).  4.2. Some Mapping Gaps and Asymmetry What should we call this kind of metaphors? If we generalize the above expressions into formulae in accordance with Lakoff and Johnson's convention, we may get something like WOMEN ARE ANIMALS or WOMEN ARE PLANTS. However, these names are insufficient; they do not grasp the details of mapping precisely enough. Actually, some mapping gaps are observed in these metaphors. Not all kinds of animals and not all kinds of plants are used for women; only some of the animals and plants function as source concepts for woman metaphors. For example, women are very often conceptualized as  2 We have found other important metaphorical mappings such as WOMEM ARE GOODS.  flowers, butterflies, or pet animals, as seen in the examples in (3), while big trees, parts of plants like leaves or branches, or wild and/or large animals like wolves, bears and others are not mapped onto women. If we call a woman a pine tree, a leaf, or a branch, it makes no sense. If we call a person a wolf or a bear, we may think that the person is a man, but we never imagine a woman. This is one of the mapping gaps involved in woman metaphors that take animals or plants as the source concept. Even within the animals and plants used for women, not all properties are actually mapped. For example, having four legs, tails, beaks, wings, and so forth are some salient properties of animals, but they are not mapped onto the concept of woman. Similarly, photosynthesis is a salient property of plants, and pollen, that of flowers, but they are not involved in the mapping either. This is the second type of mapping gap. Furthermore, these metaphors referring to men and women have asymmetrical mapping systems. For example, we do not mean a male person by kabe-no hana, nor do we have expressions such as *kabe-no musi (bug on the wall) for male. On the other hand, we have an equally rich set of man metaphors using animals or plants: e.g., furu-danuki (old raccoon dog), nora-inu (homeless dog), okuri ookami (escort wolf), deku-no boo (wooden doll), nure-otiba (wet fallen leaves), etc. These expressions are specific to men (see 6. Appendix). In the following section, we discuss what properties of animals and plants are actually mapped onto the concept of woman, so that these mapping gaps can be explained.  4.3. Properties Involved in the Mapping  By analyzing various kinds of animals and plants in terms of their properties that are mapped or unmapped, we have found a set of properties that are crucial in woman metaphors: SIZE, PET-LIKE FUNCTION and PASSIVE FUNCTION. Let's examine them one by one.  (A) SIZE: Animals or plants that are relatively small and easy to handle are mapped onto women.  Ontological Correspondence: < Source Domain > Size of plants Size of animals  < Target Domain > Size of human beings Size of human beings  These correspondences seem to have some physical motivation, since average women are smaller than average men. However, wild dogs and raccoon dogs are not mapped onto women even if they are "relatively small" in the set of all animals. This shows that what is crucial here is not just physical smallness but also "social smallness," that is, the property of being cute and easy to handle.  (B) PET-LIKE FUNCTION: The properties of animals or plants that are perceptually enjoyable for the owner or the observer are mapped onto women.  Ontological Correspondence: < Source Domain > Perceptually enjoyable property of plants  < Target Domain > Perceptually enjoyable property of women  In this mapping, what is crucial is the property of being perceptually enjoyable for men, such as beauty of appearance or voice, softness of touch, and cute and lovable behaviors or motion. What counts as beautiful or lovable, however, depends on how men evaluate these properties in women. Thus, this is also a kind of "social value" interpreted especially by men.  (C) PASSIVE FUNCTION: The function as prey or as objects of being physically affected is mapped onto women.  Ontological Correspondence: < Source Domain > Passiveness of prey or objects  < Target Domain > Passiveness of women  Again, this mapping involves social interpretation. It is not at all clear whether women are physiologically passive or not, but they are socially interpreted as passive beings, at least in Japan. In sexual relationships, men are usually seen as active, hunter-like beings, whereas women are usually seen as passive, prey-like beings. The relationship between certain animals such as wolves and their prey is mapped onto this socially interpreted relationship between men and women. As shown by the above three properties involved in the mappings, what is mapped onto women seems to be a passive, patient-like function, while men are conceptualized as active agents who interact with the patient.  4.4. Socio-cultural Codes  The properties in woman metaphors are fundamentally based on the socio-cultural codes. The general model presented in 4.2 can be applied to woman metaphors. The structure of mappings seems to presuppose the following components:  (i) There is a socio-cultural code that registers certain animals or plants as pets or ornaments that give us enjoyment. (ii) There is a socio-cultural code that registers women as passive, patient-like beings that are enjoyable for men. (iii) The similarity between (i) and (ii) serves as the motivation for the metaphorical mapping from the domain of animals or plants to that of women.  The components in (i) and (ii) exist independently in our society and function as the basis for woman metaphors. Animals and plants are concrete, ordinary things we encounter in our daily life or in folk tales, whose images are conventionalized in our own culture. This seems to be the basis for using animals and plants as source concepts for woman metaphors. Note that (i) and (ii) do not indicate any serial order such as historical change or psychological processing of these metaphors. Both must exist as the basis of this mapping, but at present, we cannot determine which of (i) and (ii) comes first, historically or psychologically. Below are metaphors that reflect gender-differentiating socio-cultural codes.  (4) a.  furu-danuki : old-raccoon dog (an experienced and sly tactician)  b. furu-gitune : old-fox (an experienced and sly woman)  c.  nora-inu : homeless-dog (a single man wandering without fixed job or home)  d.  neko : cat (geisha)  e.  uguisu-joo : nightingale-girl (a female announcer)  Notice that (4a) and (4b) are similar in meaning and yet, they are counterparts; the former refers to male and the latter female. In Japanese culture, raccoon dogs are commonly associated with men, while foxes are associated with women. There is another pair of animals that are assigned particular gender: dogs and  cats? The examples (4c) and (4d) seem to reflect the socio-cultural code that registers dogs as men and cats as women, since there is no apparent reason otherwise. There are some physical characteristics of these animals that suggest the motivation of mapping such as the size of cats being smaller, and their body shape being round. However, the socio-cultural codes are mainly based on socio-cultural characteristics. Some dogs are trained to serve as a watchdog, guide dog, life-saver dog, hunting dog, etc., but cats are seldom trained. Another interesting aspect is that dogs are usually considered faithful, whereas cats are considered whimsical. According to the conceptualization in Japanese, human male is considered a work force, whereas human female is considered untrained. Thus, dogs are perceived as faithful, working animals which are related to men, while cats are perceived as whimsical, untrained animals which are related to women. The last example, (4e) shows that the biological sex of the source concept (animals) is not an issue for woman metaphors, since nightingales that sing beautifully are male. It is the socio-cultural codes that make fox, cat, and nightingale feminine in Japanese. Considering the socio-cultural codes, the majority of woman metaphors seem to be based on the perspective of men. Men seek in women certain properties (i.e., passiveness, patienthood, etc. [6]) which are similar to what hunters seek in prey and what people seek in pets. This relationship can be summarized as below: (A) Women are to men what prey is to hunters. (B) Women are to men what pets are to their owners. (C) Women are to men what the patient is to the agent of an action. The process of mapping here is complex compared to other types of metaphor such as primary metaphor and image metaphor discussed earlier. We suggest that what gets mapped in the woman metaphors are not the properties of animals and plants themselves, but rather, the relationships between hunters and prey, owners and pets, and agent and patient and so forth. Accordingly, socio-cultural metaphor involves the mapping of relationships, so the study of this kind requires the understanding of social and cultural backgrounds to discover various relationships and social codes that motivate the metaphors. 4 4.5. Social Structures and Interpretation of Woman Metaphors As discussed above, certain metaphors require that the social and cultural contexts be examined. This comes as no surprise, given the nature of language that "verbal interaction is a social process in which utterances are selected in accordance with socially recognized norms and expectations" (Gumperz [7]). In the area of sociolinguistics, researchers have been engaged in the task of examining the interaction between language and shared norms of behavior, as proposed by Fishman [8]. When we consider metaphor from the position expressed by Gumperz and developed by Fishman, metaphors, as an essential part of our language, are naturally expected to reflect our social norms and expectations. If certain changes occur in the social norms and expectations, the use and interpretation of the language – including metaphors – which are in accordance with the social/cultural environment should also see changes. Japanese metaphors which are used to describe women and men might well have undergone some changes in their interpretations and usage as the time has seen changes in the social norms and the alternation of generations. Hiraga [5] notes that some of the expressions she finds 3 We do not consider the words cat and dog that serve as a type of prefix to modify other nouns or verbs. Here are some examples: neko-nade-goe (cat-pet-voice: a coaxing voice), inu-jini (dog-die: die uselessly). It seems to be a type of grammaticalization, and thus gender differentiation is not involved. 4 We suppose that this is not unique to Japanese. The extent to which this kind of motivation is universal would be a topic for further study.  inadequate (e.g., expressions regarding men as food) have become widely accepted by university students. Actually we do find such expressions as shun no otoko (the man in season) in magazines and kanojo-wa wakai otoko-ni ueteiru (she is hungry for young men) in daily conversations. In order to confirm possible occurrences of such changes, a small-scale questionnaire survey was carried out among 112 native Japanese speakers ranging in age from 18 to 43, female and male, representing a number of different professions. Twelve sentences, each including a metaphorical expression normally used to describe a physical or psychological characteristic of a person, were given. The respondents were to consider the use of each metaphorical expression and choose whether it referred to a male, female, either, or was indeterminate. The questionnaire served as a test to identify some metaphorical expressions that have been neutralized or even changed in terms of gender, due to recent changes in gender roles in Japan. The results show the dynamic nature of socio-cultural metaphor (see Tables 1, 2, and 3). While there are expressions which were strongly associated with either female (e.g. ofisu-no hana) or male (e.g. ookami), there were also a number of expressions for which the traditional interpretation now appears to have been neutralized or moved into an indeterminate category. The items with inconsistent results were these expressions: furu-danuki (old raccoon dog), dooki-no sakura (cherry blossom of the same year), and udo-no taiboku (large tree-lookalike weed). More than a third of the respondents answered that these expressions refer to both male and female, whereas ookami (wolf) and nora-inu (homeless dog) were judged by the majority to be predominantly male. Yet, it should be mentioned that at least 20 respondents were not familiar with these expressions: furu-danuki and dooki-no sakura (Table 1). It was found in our survey that older respondents are more likely to have a traditional or conservative interpretation. We compared the results between professionals (ages 23-43) (N=75) and college undergraduates (ages 18-22) (N=37) (see Tables 2 and 3). A drastic change from older (Table 2) to younger (Table 3) generation is seen in the results for dooki- no sakura (cherry blossom of the same year). Whereas more respondents in the former group judged it to refer to male, this choice was the least preferred among those in the latter group. The expression dooki-no sakura originally comes from a military academy song in which the trainees and soldiers used it to call other comrades who entered the military in the same year they did. It is difficult to imagine that women were originally counted as comrades. On the other hand, the case is the opposite for udo-no taiboku (large tree-lookalike weed). The dominant interpretation seems to shift from neutral to male. Udo itself means a kind of plant that grows as high as two meters which is used to be eaten fairly commonly. The expression, udo-no taiboku, is used to refer to someone – either male or female – who is tall and big in physique and yet practically useless and unreliable, because udo, although it might look like a solid tree, is in fact not so much as a tree as a weed. The younger generation, who are not familiar with the plant udo, seems to have interpreted udo-no taiboku simply as a large tree, because they could make out taiboku – large tree – but not udo. As stated earlier, the relative size of the plant or the animal that appears in the metaphor corresponds to the physical, relative size of either women or men. A large tree seems to have inspired an image of a man, not of a woman, among the young people. The judgment given to furu-danuki (old raccoon dog) was distributed among male, female and both. Traditionally, raccoon dogs have been associated with men in Japanese folk tales, joking and swearing. Furu-danuki invokes the image of an old, wise, uncanny man with power. How can we account for such changes in the interpretation of these socio-cultural metaphors? The diversity in use and interpretation of the above metaphorical expressions, we suggest, are attributed to changes in Japanese society. Changes in Japanese social structure could have led to those linguistic changes, and the fact that there are more and more Japanese of the younger generation who are ignorant about the origin of a given expression can be the cause of generating or accelerating the changes in use and interpretation of the metaphor.  Table 1. Total (N-112)  judgment female male both don't know  expressions  original use  ofisu-no hana female  111  
The resumption of sovereignty over Hong Kong by China and the implementation of legal bilingualism there have given rise to an urgent need for producing verbatim court records of proceedings conducted in Cantonese, the predominant Chinese dialect spoken by the majority of the population. This has created a challenge to build up the jurilinguistic infrastructure vital for the full implementation of bilingualism and the retention of the Common Law system in Hong Kong. While there are Computer-Aided Transcription (CAT) systems for processing English and Mandarin (Putonghua), none exists for processing Cantonese. This paper discusses the design of a Cantonese CAT system based on the special features of Cantonese speech sounds. The CAT system works on the conventional English-based keyboard to process Cantonese and meets the bilingual requirements of the Hong Kong courts. By utilizing primarily statistical techniques, the system is highly successful in handling the ambiguity resolution of homophonous Chinese characters, a tantalizing problem in the conversion from phonetic to textual representation of Chinese. Additional linguistic analysis and related processing are discussed which could further improve the performance of the system from about 92% to over 94% accuracy. 1. INTRODUCTION With the implementation of legal bilingualism in Hong Kong, Cantonese Chinese is increasingly used in the legal domain, particularly in court proceedings. Previously, when court proceedings were conducted exclusively in English, verbatim records were kept by court stenographers using the Pitman method and, more recently, the shorthand machine. The shorthand codes recorded by the machine were transcribed into English words via a Computer Aided Transcription (CAT) system. However, it is incapable of processing Cantonese, the dialect spoken by the predominant majority of Chinese litigants in Hong Kong. In the absence of an efficient and reliable device, the Judiciary of Hong Kong is confronted with the urgent problem of finding a way to maintain legally tenable records of court proceedings conducted in Cantonese Chinese. [1, 2, 3] Currently, the Judiciary has to resort to a primitive solution, i.e., transcribing the audio records of court proceedings into Chinese characters by means of audio-typing. This process is not only time-consuming but also error-prone.  To remedy the situation, two supporting facilities are required, namely, a computer-compatible Cantonese shorthand method that allows court stenographers to make verbatim records of Cantonese speech, and a Cantonese CAT system that facilitates the transcription of Cantonese shorthand codes into Chinese characters. Both English- and Mandarin-based CAT systems have been available for some years. [4, 5] However, neither a computer-compatible Cantonese shorthand method nor a Cantonese CAT is currently available. This paper discusses (1) a phonetically-based Cantonese shorthand method for use on stenograph machines; and (2) the design and initial implementation of a Cantonese CAT system capable of converting the phonetically-based shorthand codes into Chinese characters. The rest of this paper is organized as follows. The next section briefly reviews the conversion from phonetic to textual representation in CAT. Section 3 gives a detailed account of the design of our Cantonese shorthand method and Cantonese CAT. Section 4 discusses the statistical techniques employed. Section 5 reports the evaluation results of the system. Section 6 discusses further linguistic analysis and enhancement features. Finally, Section 7 provides a summary and considers ways in which the system can be refined and expanded. 2. COMPUTER AIDED TRANSCRIPTION SYSTEM (CAT) 2.1 Overview of CAT The main function of CAT is to transcribe shorthand codes into the words of a target language. Functionally, a CAT system can be divided into three major components: (a) a stenograph machine with a shorthand scheme, (b) an automatic transcription system (ATS), and (c) a supporting module for postediting. (Figure 1)  (a) Stenograph Machine  (CAT codes)  (b) Automated Transcription System  (preliminary words in the target language)  W ',,...,W'm (correct words in the target language)  Figure 1: Schematic diagram of a typical CAT system. The stenograph machine is a typewriter-like electronic device specifically designed for fast shorthand recording. To facilitate rapid key striking, it is built with a special key arrangement and has far fewer keys than the conventional typewriter keyboard. The machine is designed in such a way that several keys can be pressed simultaneously in one stroke. Thus there are many combinations of keystrokes within a single strike. The stenographer encodes speech as a sequence of CAT codes' simultaneous to the litigant speaking. The CAT codes are stored electronically for subsequent offline automatic transcription. The automatic transcription system (ATS) is a computer program that converts CAT codes into humanreadable words of the target language (e.g. English words or Chinese characters). Finally, a CAT system normally comes with a supporting post-editing module. It enables stenographers to correct typos and mis-transcribed items and do further refinement to the output text. Each CAT system comes with a shorthand method designed specifically for a particular language. This is referred to as machine-compatible shorthand method, in contrast to traditional hand-written shorthand schemes. Two existing machine-compatible shorthand systems were studied, namely, the ComputerCompatible Stenograph Theory (CCST) [4] for English and the Ya Wei Chinese Stenography (YWS) [5] for Mandarin. Both CCST and YWS are phonetically-based. The set of CAT codes represents the set of syllable inventory in the target languages. Each input scheme has its own keyboard. Basically CCST operates on a one-stroke-one-syllable basis whereas YWS works on a one-stroke-two-syllable basis. Phonetically-based input inherently leads to ambiguous CAT codes. Each of these codes represents a  In this paper we shall use "CAT codes" and "shorthand codes" interchangeably. —314—  whole class of homonyms, words of identical pronunciation but different morphemes, resulting in code ambiguity. In both CCST and YWS, additional rules on using CAT codes have been enforced to eliminate code ambiguities. The rules in CCST generally appeal to the spelling of the intended English word, forcing the stenographer to supply extra "spelling hints" when typing a potentially ambiguous CAT code. In YWS, some 2,000 predefined special CAT codes are provided, each of which will enable the stenographer to uniquely identify the intended Chinese word. In short, both CCST and YWS reduce CAT code ambiguity through the provision of rules with which the stenographer can get unambiguous CAT codes. 2.2 The Homocode Problem – Preliminary Analysis In a purely phonetically-based CAT code scheme, there is an isomorphic mapping from syllables of the target language to the CAT codes. However, because of the presence of homonyms in the target language, the mapping from phonetic representation to textual representation is one-to-many. We call this the homocode problem in the conversion from phonetic to textual representation. Figure 2 gives a more abstract view of CAT in view of this problem.  pl,••• ,Pk (Phonetic Representation)  Disambiguation Procedure to single out a t, within the homonym class C represented by pi.  ,tk (Textual Representation)  Figure 2: A more abstract view of CAT. To circumvent the homocode problem, both CCST and YWS have not adopted a purely phoneticallybased representation in the design of their CAT codes. As mentioned in the previous section, CCST provides heuristics for encoding extra information into their CAT codes to reduce ambiguity whereas YWS appeals to a large group of pre-defined codes to uniquely identify thousands of commonly used Chinese words. 2 Our present focus is whether methods of disambiguation similar to these can be adopted to overcome the homocode problem in Hong Kong's Cantonese Chinese setting. Our observation is that it is difficult to adopt a scheme like that of YWS for use in Hong Kong's court environment. Even if a shorthand scheme similar to YWS could be devised, where Cantonese syllable types were represented on a one-keystroke-many-syllable basis on a YWS-like keyboard, the scheme would be very unintuitive for the court stenographers in Hong Kong because the stenographers are proficient in the English CCST tradition. Further, the significant linguistic gap between Mandarin and Cantonese also makes this not viable. 3 Neither can we adopt a simple CCST-like code conversion process by requiring the stenographer to key in "spelling hints" because Chinese characters are not based on spelling. A new synthesis is needed. It is noteworthy that the use of a purely phonetically-based shorthand scheme involves cognitive functions quite different from processing "spelling hints", or recalling ad hoc codes for a large list of salient items. In a purely phonetically-based input scheme, stenographers need to learn only how to phonetically represent syllables in CAT codes without worrying about ambiguity. Therefore, no ad hoc rules are needed. This makes Cantonese stenography far easier to learn and use, thus reducing the cost of training.  2 To be more precise, both CCST's and YWS's heuristics try to achieve a one-to-one relationship between CAT codes and words in their target languages. Homonyms with identical pronunciations but different morphemes are now encoded with different CAT codes, making the relationship between CAT codes and syllables to become many-to-one. 3 This gap can be as high as 40% in the lexical component. See [6]. —315—  3. CANTONESE CAT SYSTEM: SYSTEM DESIGN This section describe in detail the design of our Cantonese CAT system, specifically addressing the homocode problem. We shall consider two core components: the Cantonese shorthand method and the ATS engine for the CAT system. 3.1 Keyboard Layout Existing court stenographers have accumulated much experience in using CCST-based CAT system. It was decided that the CCST-based shorthand keyboard should be retained in view of two considerations. First, the stenographers are familiar with the CCST-based CAT system. It should be preserved as far as possible so as to capitalize on the existing equipment and the experience in the system. Second, English will continue to play a significant part in the legal domain. In the foreseeable future, the court proceedings are likely to be in a mixture of Cantonese and English. The Cantonese keyboard layout and the corresponding shorthand system must not just cater for Cantonese but should be capable of accommodating the bilingual transcription environment. Retaining the existing English CAT keyboard enables the stenographers to easily switch between the Cantonese and English environment, and minimizes the cost of such an extension. 3.2 Phonetic Representation of Cantonese--A Proposed Cantonese Shorthand Method The keyboard layout being determined, the next step is to introduce a set of Cantonese phonetic symbols fully representable by key combinations on the CCST-based keyboard. The Cantonese romanization system Jyutping [8] has been chosen as the foundation for representing Cantonese syllable types in this regard. Jyutping has 19 initial consonants, 9 vowels and 8 codas, and is able to represent Cantonese speech sound with accuracy and consistency by phonetic symbols close to Pinyin and the International Phonetic Alphabet. Furthermore, all the phonetic symbols of Jyutping are representable on the CCSTbased stenograph keyboard by a fairly natural extension. Instead of adapting a new keyboard and a set of novel key assignments, our Cantonese extension capitalizes on the existing CCST key combinations for English syllables. All original key combinations for English initial consonants, vowels and final consonants are preserved in the new scheme. Consonants and vowels unique to Cantonese are added to the scheme. The extension enables every Jyutping syllable type to be represented on a one-stroke-onesyllable basis, preserving the one-to-one correspondence between Jyutping syllables and CAT codes. The extended CCST scheme is called CVC (representing "Consonant", "Vowel" and "Coda" (i.e., the final consonant) of a Jyutping syllable). 3.3 Conversion from Phonetic to Textual Representation - The Problem of Homocodes The adoption of the CCST-based keyboard and the CVC scheme requires that the associated ATS module is to be redesigned for the following reasons. Recall that in both CCST and YWS, the stenographer may invoke various disambiguation rules to obtain alternative unambiguous CAT codes. The design of YWS's disambiguation rules is tied to the Mandarin-oriented keyboard and differs fundamentally from the CCST-based one. This makes it infeasible to utilize the existing ATS to process CAT codes under the CVC scheme. Being an ideographic and basically monosyllabic language, Cantonese has many homophonous characters sharing identical syllable types (and therefore shorthand codes), and yet it is impossible to differentiate among these homonyms using spelling cues during online transcription because Chinese characters are not based on spelling. We call this the Cantonese homocode problem in the conversion from phonetic to textual representation of Cantonese. 4 The total inventory of Cantonese syllable types is about 720, and there are at least 14,000 Chinese character types. We estimated this for the legal domain with reference to a 0.85-million character corpus comprising mostly of court proceedings. 565 distinct syllable types were found, representing 2,922 distinct character types. Of the 565 syllable types, 470 have 2 or more homophonous characters. These 470 syllables represent 2,810 character types (which account for 94.7% of the corpus' tokens) each of which has at least 1 homonym. The homocode problem thus is indeed very serious in the domain of local court —316—  We tackle the Cantonese homocode problem by equipping the Cantonese CAT system with statistical knowledge and an ambiguity resolution engine. In this way, the burden of homocode disambiguation is shifted from the stenographer (as in the cases of CCST- and YWS-based schemes) to the ATS. By adopting the bigram model commonly employed in speech recognition technology, the resolution engine is able to transcribe CAT codes into Chinese characters satisfactorily with over 90% accuracy. More precise evaluation figures will be presented in Section 5.  4. AMBIGUITY RESOLUTION OF HOMOPHONOUS CANTONESE CHARACTERS  We now turn to the statistical ambiguity resolution technique employed. Let us consider the homocode  problem in more detail. The goal is to transcribe a sequence of shorthand codes s h  ,sk into the  intended Chinese character sequence c l,  ,ck. In the phonetically-based shorthand system, a given  shorthand code, si represents the Cantonese syllable of the intended Chinese character c1 . The homocode  problem arises as the code si can be mapped onto any member of the homophonous character set C =  {cip ...,c,h } , where ci E C and every member of C shares the same shorthand code. 5 To resolve the  ambiguity, we resort to statistical frequencies obtained from large training corpora, and search for the most probable Chinese character in context, for each shorthand code in the input code sequence. We seek to maximize the conditional probability in (1).  (1)  P(ci, , ck S i , ,sk)  where cl , , ck stands for a sequence of k Chinese characters, and  S i , , sk stands for a sequence of k input shorthand codes.  To compute (1) directly from corpus statistics, however, is impractical, as huge amount of data is required to generate any reasonable estimates of (1). For this reason, we look for a more practical approximation. We first rewrite(1) into (2) using Bayes' rule.  (2)  Now the goal is to find a Chinese character sequence c1 ,..., ck to maximize (2). The denominator P(s 1 , , sk) can be ignored in the process, as its value remains the same for whatever character sequence chosen. The numerator in (2) can be approximated by (3) using two more assumptions'.  (3)  11 i=1,...,k (P( C il C i-1) * P ( S il C i))  The maximal value of (3) can now be evaluated more practically. 'I his is achieved by computing the cooccurrence frequencies obtained from a large training corpus (tagged with Cantonese syllable shorthand codes). The frequencies are to approximate the factors P(s il ci) (i.e., the pronunciation probability)' and also P(ci lci_i) (i.e., the bigram probability). The approximated maximal value of (3) is efficiently computed using the Viterbi algorithm [9] to determine the best sequence of Chinese characters c1,..., ck as the output. This ambiguity resolution method, originally developed mainly for speech recognition [10, 11], has been found to be useful and has been built into our system's ATS module.  5 There are 6 tone contours in Cantonese. However, tone contour is not encoded in transcription to reduce cognitive burden, thus making the homocode problem more acute.  6 Both are "Markov assumptions" about historical influence. Assumption 1: (Bigram model) The bigram model  assumes that for each Chinese character c i in the target Chinese character sequence c 1, ..., ck we seek to obtain, the  only historical factor of concern to its occurrence is the immediately previous character (When i = 1, this  n i=1,. .,k "historical factor" is stipulated as c i's being the beginning of the discourse in question.) Accordingly the expression  P(c„ , ck) in (2) is approximated by  P(e i lc i _,). Assumption 2: (Independence of Pronunciation) We  assume that the way ci is pronounced is independent of that for the preceding or succeeding members in c 1, . Accordingly the expression P(s,, , sklc,,..., ck) in (2) is approximated by n P(si I ci).  The value of P(sii c) need not always be 1 in Cantonese as Ci combines with different characters to form polysyllabic words.  —317—  5. EVALUATION We have built several prototypes of the Cantonese CAT system for evaluation. The most basic version is the one equipped only with the basic ambiguity resolution method as described in section 4. More sophisticated prototypes were built upon this basic version by the addition of enhancement features. In this section, we will first describe the reference corpus used in the evaluation tests, followed by the description of the basic prototype and test results. Other enhanced prototypes will be discussed in section 6.  5.1 Corpora Used in Evaluation We conducted experiments to evaluate the prototypes by setting up two data sets: a training set for training the ATS, and a testing set for evaluating the transcription accuracy. Both sets are derived from the corpus of authentic court proceedings (Chinese transcripts) obtained from the Hong Kong Judiciary.' Basic figures for the two sets of data are given in Table 1.  Data Set Training Testing Total  Chinese Numerals Punctuation English  characters  marks  Words  715,501  8,786  99,287  20,491  175,735  1,359  22,662  5,606  891,236  10,145  121,949  26,097  Table 1: The Training and Testing Data Sets  Total 844,065 205,362 1,049,427  The Training Set Recall that ATS requires pronunciation probability, P(s il ci ), and bigram probability, P(cil ci_ 1 ) in order to perform ambiguity resolution. To this end, we compiled a corpus of about 0.85 million Chinese characters, all tagged with the corresponding Jyutping syllables. The corpus was transformed into the training set by systematically assigning an appropriate CAT code for each such Jyutping syllables, and listing this code side by side with the original Chinese character. Supplied with this sequence of training data, the ATS estimated both the pronunciation and bigram probabilities for a given Chinese character, by computing the relative frequencies.9  The Testing Set The testing set was used for simulating the stenographer's actual input on the stenograph machine in order to test the system's accuracy. The corpus consists of about 0.21 million Chinese characters. The testing set was obtained by replacing each Chinese character with the appropriate CAT code. The trained ATS took only the CAT code sequence as input and transcribed them into Chinese characters.  5.2 Basic Results The same training and testing sets were used in conducting all the evaluation tests for every prototype.  The case types of these court proceedings are heterogenous: they comprise traffic, assault, robbery, among others. 9 Hence, based on this tagged corpus, an approximated value of P(silci ) can be computed as the ratio of ci 's being pronounced as the syllable denoted by si , to the observed total occurrences of c i'; similarly, P(cifci. ,) is approximated as the ratio of observed occurrences of ci after ci. , to the observed total occurrences of ci_1. —318---  CATvAl° CATVA is the most basic prototype. It was subject to training with successively more inclusive subsets of the same training set, each containing the immediately previous one until the whole set is exhaustively used. To get the baseline reference, we also built a control, CAT 0, which converts a CAT code s, into a Chinese character simply by selecting the member out of the homophonous set that has the highest occurrence frequency in the training set. The performance of the prototypes is summarized in Table 2. Prototypes CATo CATVA CATvA CATvA CATvA CATvA CATvA (training size (0.85) (0.2) (0.35) (0.5) (0.63) (0.73) (0.85) in million characters) Accuracy 78.0% 89.4% 91.2% 91.8% 92.1% 92.3% 92.4% Table 2: Summary of CAT VA 's performance. As shown in Table 2, CATo results in an accuracy of about 78.0% whereas CAT VA achieves at least 89.4%, yielding over 11% increase in accuracy with as meagre a training subset as 0.2-million characters. With the use of full training set, CATVA reaches 92.4% accuracy. At this level, there is a 14% gain in accuracy over CATo.  6. ADDITIONAL LINGUISTIC PROCESSING The above discussion indicates that the best results that probabilistic information retrieval means could produce are unlikely to go substantially beyond 92% accuracy. Further efforts to enlarge the training corpus led to diminishing returns, as Table 2 indicates. Our subsequent investigation has shown that the accuracy can be improved more profitably by equipping the basic prototype with extra heuristic features on top of the statistical resolution engine. They include shallow linguistic processing to deeper linguistic analysis. These features are discussed below.  6.1 Generic Treatment of Numerals In the original CCST-based stenograph keyboard, numeral tokens are input by the stenographer using the numeral row at the top of the keyboard. The shorthand codes for numeral types, e.g. 1998, 250,000, 2, 20, are thus represented by the numeral types themselves instead of being phonetically-based. This saves the stenographer from the tedious conversion of Arabic numerals into phonetically-based shorthand code during online input. However, it prevents the CAT system from capturing some potential regularities of numerals in the corpus. For example, the Chinese numeral is often followed by a member of a limited set of classifiers or units of measurement, e.g. sheet (for paper), dollar, metre. If each numeral type is represented by a different code, such regularities shared by all numerals can hardly be captured by the bigram probabilities between characters, as the frequency of individual numeral types is quite low even in a comparatively large corpus. 11 This is referred to as the under-representation problem of numerals. The negative impact of this problem during transcription is that numerals not encountered during training will always not be available in the statistical estimation and be treated as sparse data, affecting the accuracy. To overcome the problem, we represent all numeral types by using a generic category NUM. During the training phase, each time when a numeral token is encountered, NUM's total frequency and the relevant probability figures will be updated. In this way, the shorthand method remains unchanged from the perspective of the stenographer, while the system's ATS module is able to track the bigram  
In this paper, we propose a method for domain unconstrained language understanding based on the How-net knowledge base. The goal is to construct a system that reads in an article and answers some related questions. For each sentence in the article, word segmentation is first applied. Then, the major components such as agent, theme, event, time, and place are extracted to construct a semantic-slot table and a semantic network. Answers of the questions are derived using two approaches, which are based on the relational and hierarchical relation among the major components. Our method is applied to the understanding of the primary-school textbook, and it is able to answer questions in the exercise of the textbook. 1. Introduction Currently, most available applications of natural language processing (NLP) are domain specific. In this paper, we propose a method for domain unconstrained language understanding. The knowledge base we used is the How-net knowledge base, which is constructed by researchers in Beijing. The description and content of How-net can be found in the following URL, http://www.how-net.com. How-net describes the relationship of objects using both concepts and attributes. Based on How-net knowledge base, we have some methods to analyze the sentences of an article. First, word segmentation is performed to find  the corresponding word sequence. Then, major components (agent, event, time, place, and theme) conveyed in the sentence are extracted. Finally, the semantic table and semantic network are constructed for the understanding of the article. For the experiment, we try to answer the questions in the exercises of the primary-school textbook. Article of each lesson is used to construct the corresponding semantic table and semantic network. Answers of the questions are derived by measuring the likelihood of the major components. This paper is organized as follows. In Section 2, we introduce basic concept of the How-net knowledge base. In Section 3, the method to analyze sentences is described. In Section 4, we show how to answer the questions of the primary-school textbook. The conclusion is given in the final Section.  2. The Structure of How-net  How-net is a Chinese knowledge base which describes the objects using concepts and attributes. The basic units in the How-net are physical and spiritual objects such as components, attributes, time, space, events, attribute values, and so on.  Common and individual characteristics of concepts are both recorded in How-net. Consider the words "g-(doctor)" and "2ff(patient)" as example, the common characteristic of them is "people". This common characteristic is recorded in the How-net. On the other hand, "doctor" has the individual characteristic being the agent of "cure"; and "patient" has the individual characteristic being the experiencer of "suffer".  The relations among different concepts and attributes are also described by How-net. The relations are shown below:  (1) Upper-Down relation.  Ex. Father-Son, Father-Daughter, ..  (2) Synonymous relation.  Ex. Good-Well, Big-Large, ...  (3) Antonymous relation.  Ex. Good-Bad, Large-Small, ...  (4) Attribute-Host relation. (5) Component-Entire relation. (6) Material-Product relation. (7) Event-Agent relation.  Ex. Age-Person, Color-Flower, .. Ex. Leg-Body, Door-House, .. Ex. Rice-Wine, Sand-Glass, ... Ex. Cure-Doctor, Build-Worker, ..  Figure 1 is an example showing relations among some objects (in rectangle shape) and actions (in round-rectangle shape).  Figure 1. Example of relations among objects. 3. Analysis of the Sentences 3.1 Word Segmentation Unlike western language such as English, Chinese sentence is composed by characters. Since How-net is a word-based knowledge base, each sentence has to be segmented into word-sequence for further processing. Word segmentation can be done by several approaches. The simplest one is to use the greedy algorithm. This algorithm treats the input sentence as a large string, and then shrinks the string gradually to check whether the shrunk substring is a lexical term. The segmentation results may be different if the shrinking directions are different. For example, the sentence "7t_gp.A-istRibtt,4 (The life in college is very cheerful)" may be segmented into " 7t-A-(college student) ?S(live) avery) ilk,(cheerful)" if the  string is shrunk from right to left. However, it may also be segmented into " A•(college) (life) j,R(very) 'W(cheerful)", which is the correct segmentation, if shrunk from left to right.  In this paper, we employ the greedy algorithm for both shrinking directions. If the segmentation results are the same for both directions, then we are done. If not, the bigram scores of both word-sequences are calculated to determine the correct segmentation. This bigram information is trained using a large amount of news corpus. Since the bigram score calculation can be found in many lectures, it is abbreviated here.  Note that if the segmentation result contains dangling single-characters, these  single-characters may be further combined with other words/characters to form a compound  word using word-formation rules. For example, one of the rules is to deal with the Chinese  naming principle. We try to combine the characters sequence "  Rt into one word "  (Lee Kin-Nan)" since " (Lee)" is a Chinese surname. Another example is that the  word sequence 11 •02- - (thy)  M(neat)" is combined into the compound word  iP?P (clean)" since it satisfy the adjective formation rule.  3.2 Extracting Major Components of the Sentence Based on our previous study [2], we think that to understand a sentence is to know the answers of SW, i.e., who, what, when, where, and which. In this paper, we define these answers as the major components of sentence. The major components represent the agent, theme, time, place, and event conveyed in the sentence and they contain essential information to understand the sentence. For example, to analyze the sentence "..711p-fettiiajp (Lee Kin-Nan broke the window of the classroom)", the sentence is first segmented into the word sequence: " (Lee Kin-Nan) P(broke) ttV(classroom) 0(De) j (window)". Then, the major components are determined by the part-of-speech tags found in How-net. In this example, we  have the words: Lee Kin-Nan (agent: n. name of people); break (event: v. break); classroom (place: n. place to conduct a lesson); window (theme: n. the hole in the wall to illuminate the house). The major components in such a simple sentence can be easily determined. However, for some complex sentences, we may need extra information to properly catch the meaning of the sentences. So, the attributes of each component are attached to carry more information. Each component may have none or more than one attribute. The attribute of agent, for example, can be the height, weight, age, color, and so on. After extracting the major components of each sentence, the semantic table that consists of the major components and their attributes can be constructed sentence by sentence. Figure 2 shows an example text chosen from the primary-school textbook. The corresponding semantic table is shown in Table 1. Note that the attribute of an event can be another event or sentence.  MMV-_LT7  °  RifriMT*T- ,  °  KM_EziAllqq7A °  tzfrP_NA ° T*W AMVRA'-'1'n -  W,-",-;11MWTR r TVAin fiTZ afirki gla ' 4, 2a: ° '441 MAT fEfiUM °  MM,FiR r 4 R--VE/1.1.  (S i ) My brother is going to school, (5 2) He is very happy.  (S3) He is in the first grade, (5 4) I am in the second grade.  (S5) Mother says to me, (S 6) "today is the first day for your brother going to school, (5 7) you should take him to school carefully.  (S8)I go to school with my brother hand in hand.  (S9)There are many cars in the road: (S io) big cars and small cars.  (S 11 ) The cars run far and near, (5 12) my brother is afraid of the running cars.  (S 13) I told my brother, (5 14) "Don't be afraid. (S 15) Look, the red light is on, (S 16) don't go. (5 17) Wait until the green light is on, (S 18) then cross the road."  (5 19) My brother says, (5 20) "OK, I will remember that." Figure 2. The article chosen from the first grade textbook.  3.3 Constructing the Semantic Network In addition to the semantic table, semantic network [3,4] also records the relationship of  each sentence. The nodes of the semantic network are the major components and their attributes of the sentences. The edges are the relations of major components and attributes. The semantic network is constructed gradually while reading each sentence of the text. When the major components and attributes are extracted, if the component already exists in the semantic network, the attributes and relations are updated using current ones. Otherwise, new nodes and edges are added into the semantic network. Figure 3 – Figure 6 illustrate the construction of semantic network when input the sentences Sl–S4 in Figure 2. Note that new nodes and edges of the semantic network are painted in white color. H4 MI time N( feature C_EW go to school younger brother Figure 3. Semantic network construction, step 1: Input the sentence "firZ1-:4`.7*(My brother is going to school)" Figure 4. Semantic network construction, step 2: Input the sentence "faaiff./(He is very happy)"  Figure 5. Semantic network construction, step 3: Input the sentence " (My brother is a new student in first grade) Figure 6. Semantic network construction, step 4: Input the sentence "R_E_-:_g-r (I am in second grade)" 4. Answering the questions The questions we deal with are those in the exercises of primary-school textbook. Each question in the exercise provides several candidate answers for the reader to choose the right  one. Some examples of the questions are shown below.  Ql : It4.11_3- CUT / The classroom is the place to (study / ride).  Q2: ( gM. / Ma)  °  If the (red / green) light is on, you can cross the road.  Q3: AWIAT: iwin Tivn). Brave is to be (afraid / not afraid).  Q4: mm.2:—vAni,i  /  My brother is a new (student / classroom) in the first grade.  Q5: There are many (students / cars) in the school.  I am in the (first / second) grade.  To answer the questions, we have developed two methods. The first one tries to find answer directly from the input text. The other one calculates the relation score among major components extracted from the questions and the article to find the proper answer. 4.1 Answering the Question Directly from the Text Some questions of the primary-school textbook are easy to answer. The answer can be found by scanning the article to match the sentence that has the same components in question. For example, Q6 is a question of this kind. To answer question Q6, we first extract the components of the question, i.e., "R(I)". This component is used to active the corresponding subgraph in the semantic network. Then, the answers "—g(first grade), iiiiR(second grade)" are determined by choosing the one which matches the components best. In this case, the answer will be "ii it(second grade)". 4.2 Answering the Question According to the Similarity Measure If the method described in Section 4.1 does not work, we use another approach to find the right answer. This approach checks the similarities of the major components in the question. The similarities are measured by the relation derived from the How-net. Two kinds of similarities, i.e., relational and hierarchical similarity, are used to calculate the overall  similarity measure. 4.2.1 Measuring Relational Similarity In the How-net knowledge base, each word has its corresponding definition(s). These definitions are recorded in How-net as the field "DEF". For example, the words "fflt-" (school)", "(student))", and " nrnicar) have the DEFs as shown below. Note that a word may have several meanings, thus it has many DEFs. W_C[49479]=VSZ II word index G_C = N //part of speech tag DEF[0]=InstitutePlaceligffi // definition of the word's concept Feature of noun: *engagelft#affairsl.A Feature of verb: engagelfX${agent,content} DEF[1]=©teachig Feature of verb: teachig{agent,content,target}/{agent,patient,ResultEvent} DEF[2]=@studyl, Feature of verb: studyiNagent,content,source} DEF[3]=educationigW Feature of noun : educationlgt W_C[49451]=T G C = N DEF[0]=humanIA Feature of noun: N.1.1.1.1.1.1!namelt!wisdomitig!abilitylgjj!occupationl ffafractiffM DEF[1]=*studff Feature of verb : studyffiagent,content,source} DEF[2]=educationigft Feature of noun :education W_C[34290]=A G_C = N DEF[0]=LandVehiclel Feature of noun: N.1.1.1.2.2.7.3.1#1andil*WeVehicleGoIR Feature of verb: VehicleGoll.{agent,direction,Locationlni,LocationFin} The similarity of different words is measured by comparing their DEFs. Words with similar DEFs will result in higher similarity measure. For example, the relational similarity of the words " ffltk(school)" , " _W_(student))" , and " n(car)" are calculated as:  Relational Similarity (No.49479  & No.49451 Vt') =12  Relational Similarity (No.49479  & No.34290= 2  Relational Similarity (No.49451 ''fit' & No.34290  = 3  From above result, we can conclude that the word "VOschool)" is more similar to (student)" than "I-1, ift(car)".  4.2.2 Measuring Hierarchical Similarity How-net also specifies the hierarchical relation of entities like a tree structure as shown in Figure 7. In this paper, the hierarchical similarity of two objects is defined as the shortest-path distance between these objects. For example, the similarity measure of "human" and "animal" is two, as shown is the upper-right part of Figure 7.  N.1.1.1 phsical fi 'W  N.1.1 - thing 4b1  N .1 .1 .2 mental 3k  N.1.1.1.1  animate  ethi  N.1.1.1.2 inanimate -"-±_ 01- N.1.1.1.3 shape  N.1.1.1.1.1  
Object topicalization shares two functional properties with the passive: foregrounding of the patient and de-topicalization of the agent. This fact makes one wonder why the former occurs far less frequently in written text. The present paper is an attempt to illuminate some properties of object topicalization not shared by the passive. It is argued that while object topicalization may de-topicalize the agent, informational focus is brought upon it. It is then proposed that the information structure imposed by object topicalization has consequences different from the passive on the continuing discourse.  1. INTRODUCTION  Functionally, the passive is generally understood to be a syntactic construction that topicalizes the patient, de-topicalizes the agent and stativizes the event (Givon [1] cited by Forrest [2, p.149]). If one takes the view that constructions can be related by a function, as Givon [3] and Shibatani [4] do, object topicalization in a topic-prominent language like Japanese should be related to the passive because it can perform the first two functions listed above. Indeed, object topicalization appears occasionally in place of a passive in English-toJapanese translation. However, an examination of the properties of object topicalization reveals that the resemblance is only partial. This paper presents a study of object topicalization in relation to the passive in Japanese. First, I argue that object topicalization is semantically more restricted and information-structurally marked than the passive. Then, I propose that the difference in information structure presents different effects on the continuing discourse. The rest of the paper discusses the two constructions in written register only, because it is recognized (e.g. by Biber et al.[5] and Heo [6]) that generalization across registers/genres is difficult.  2. OBJECT TOPICALIZATION AND THE PASSIVE COMPARED  Compare (la) and (lb). (TOP=Topic, NOM=Nominative, PASS=Passive, PST=Past)  (1) a.  Tegami wa hisyo niyotte kakus-are-ta. letter TOP secretary by hide-PASS-PST 'The letter was hidden by the secretary.'  (1) b. Tegami wa hisyo ga kakusi-ta. letter TOP secretary NOM hide-PST 'The letter, the secretary hid (it).'  In both, the logical object "the letter" is topicalized, and the rest of the sentence is the rheme, or added information about the topic (Vallduvi and Vilkuna [7] among others). While object topicalization is quite marked and appears primarily in spoken register in English (Lambrecht [8]), it is not so marked in Japanese and is allowed more freely in written register. However, a small sample of Newsweek magazine and its published Japanese translation reveals that it is only occasionally that an English passive is translated into object topicalization. While this may be a reflection of the translator's conservatism in preserving the source text structures, it is not the case that for each "conservative" translation, object topicalization could have been possible. Yoshihara [9] observes similar uninterchangeability in monolingual newspaper texts. This suggests that the occasional appearance of object topicalization may be reflecting something systematic.  Literature on object topicalization is, somewhat surprisingly, scarce. Therefore, I start with some basic observations. First, compare in (2) the passive and object topicalization with an unexpressed agent. Like (2a), (2b) demotes the agent by not expressing it, as Yoshihara [9] rightly describes.  (2) a.  Tegami wa kakus-are-ta. letter TOP hide-PASS-PST 'The letter was hidden.'  b. Tegami wa kakusi-ta. letter TOP hide-PST 'The letter, (I) hid (it).'  However, while the agent in (2a) can be anyone, the one in (2b) is limited to the writer or the protagonist in this particular context. Therefore, I claim that these two constructions have a different propositional content.  How about when the agent is expressed? Consider (1) again. Notice that the agent in object topicalization (lb) receives emphasis, yielding the reading that THE SECRETARY hid the letter, nobody else. This is observed by Teramura [10, p.241]. The passive in this regard is neutral. The focused reading can be achieved in (la), but only with emphatic intonation on the agent phrase. (lb) requires no special intonation. This difference is significant in written register where no phonological information is available.  Object topicalization is restricted in two other ways. For one, it disallows an indefinite object from topicalizing. Compare (3a) and (3b).  (3) a.  Ittuu-no tegami #wa/ ga hisyo niyotte kakus-are-ta. one letter TOP/NOM secretary by hide-PASS-PST 'A letter was hidden by the secretary.'  (3) b.  Ittuu-no tegami #wa/*ga hisyo ga  kakusi-ta.  one letter TOP/NOM secretary NOM hide-PST  'A letter, the secretary hid (it).'  The topic position hosts given information only. The passive can accommodate an indefinite expression in the subject position marked by ga, but that position is taken by the agent in (3b). A new, unidentified referent in the topic position yields unacceptability. This point is also noted by Teramura [10].  The other restriction involves animacy. There are four possible combinations of animacy with the two event participants: (a) animate patient + animate agent, (b) animate patient + inanimate agent, (c) inanimate patient + animate agent, and (d) inanimate patient + inanimate agent. As (4) shows, the passive accommodates all the combinations.  (4) a.  Taroo wa Hanako ni uragir-are-ta. Taro TOP Hanako by betray-PASS-PST 'Taro was betrayed by Hanako.'  b.  Taroo wa huminsyoo ni nayamas-are-ta.  Taro TOP insomnia by trouble-PASS-PST  'Taro was troubled by insomnia.'  c.  Sono ginkoo wa munoona-keieisya niyotte hatan ni  oikom-are-ta.  that bank TOP incompetent president by bankruptcy to force-PASS-PST  'The bank was led to bankruptcy by its incompetent president.'  d. Sono ginkoo wa toosi-no-sippai niyotte hatan ni  oikom-are-ta.  that bank TOP investment failure by bankruptcy to force-PASS-PST  'The bank was led to bankruptcy by investment failure.'  However, object topicalization seems to dislike inanimate agents, as (5) shows.  (5) a.  Taroo wa Hanako ga uragit-ta. Taro TOP Hanako NOM betray-PST 'Taro, HANAKO betrayed (him).'  b. ??Taroo wa huminsyoo ga nayamase-ta. Taro TOP insomnia NOM trouble-PST 'Taro, INSOMNIA troubled (him).'  c. Sono ginkoo wa munoona keieisya ga  hatan ni oikon-da.  that bank TOP incompetant president NOM bankruptcy to force-PST  'The bank, ITS INCOMPETENT PRESIDENT caused (it) to bankrupt.'  d. ??Sono ginkoo wa toosi-no-sippai ga  hatan ni oikon-da.  that bank TOP investment failure NOM bankruptcy to force-PST  'The bank, INVESTMENT FAILURE caused (it) to bankrupt.'  This can be understood in terms of the proto-typical transitivity observed in Japanese, which favors an animate agent (Jacobsen [11], Hopper and Thompson [12]). Inanimate agents are tolerated in the passive because they are in a non-prominent oblique case (i.e. in the byphrase), but they are rejected in object topicalization which keeps them in the subject position and focused.  In summary, object topicalization is more restricted than the passive in terms of semanticopragmatic argument selection: it requires a definite object and an animate agent. In addition, it places focus on the agent.  3. OBJECT TOPICALIZATION AND INFORMATION STRUCTURE  The next question is how the focus difference between the two constructions is reflected in discourse. Specifically, under what sort of condition is object topicalization used in place of the passive? Literature on focus (most recently, Lambrecht [8], Rochemont [13], Roberts [14]) generally draws observations from spoken register, so I work out a hypothesis myself.  Consider the contrast in (1) once more.  (1) a.  Tegami wa hisyo niyotte kakus-are-ta. letter TOP secretary by hide-PASS-PST 'The letter was hidden by the secretary.'  b. Tegami wa hisyo ga kakusi-ta. letter TOP secretary NOM hide-PST 'The letter, THE SECRETARY hid (it).'  (I a) and (lb) both identify "the letter" as the topic, and treat the rest as the rheme. The difference is that (lb) emphasizes the agent. When do we want to place focus on the agent? I hypothesized the following as a first approximation: if (lb) is used, anaphoric reference to the agent will be observed in the sentences that follow; if (la) is used, no mention will be made thereafter. Insignificance of the agent in the following text would justify demoting it to the oblique using the passive. But in a pilot study which tested this hypothesis, the results were not clear-cut.  A deficiency of the pilot study was that it failed to consider cleft sentence with regard to agent focusing. Cleft structure, as in (6), also places focus on the agent in question.  (6) Tegami o kakusi-ta no wa hisyo da. letter ACC hide-PST one TOP secretary COP 'The person who hid the letter is the secretary.'  (ACC=Accusative, COP=Copula)  Indeed, Lambrecht [8, p.123] takes this structure to be synonymous with the "subjectaccented" sentence (of which object topicalization is an example). However, (6) and (lb) differ with regard to the information status of the logical object. "The letter" remains to be the topic in object topicalization (lb), but it is completely backgrounded in cleft (6).  Given this, I propose a revised hypothesis. The three constructions, the passive, object topicalization and subject cleft, have the following division of labor regarding topic shift. If  the topic remains the same in the next sentence, the passive will appear, demoting the agent. If the topic shifts to the agent completely in the next sentence, the subject cleft will appear, backgrounding the rest of the event. Object topicalization will show when the topic shift is not as "rough" as in the second case.  (7)-(9) exemplify the three-way distinction. The passive, object topicalization, and subject cleft are presented in (7a), (8a) and (9a) respectively, followed by another sentence (7b), (8b) and (9b). (a) and (b) constitute a continuous discourse and they are translated together at the end of each example.  (7) a. b.  Sono ginkoo wa munoona keikeisya niyotte hatan ni  oikom-are-ta.  that bank TOP incompetent president by bankruptcy to force-PASS-PST  Genzai sisan-no-syori ga okonaw-are-teiru.  now asset settlement NOM conduct-PST-STAT (STAT=Stative)  'The bank was forced to bankrupt by its incompetent president. Bankruptcy procedures are being taken at the moment.'  (8) a.  Sono ginkoo o hatan ni oikon-da no wa munoona-keieisya dearu.  that bank ACC bankruptcy to force-PST one TOP incompetent president COP  b.  Kono keieisya wa hito-no iken o  kikanakat -ta.  this president TOP others' opinion ACC not-listen-PST  'The person who caused the bank to bankrupt is its incompetent president. He didn't  listen to other people's opinions.'  (9) a.  Sono ginkoo wa munoo-na keieisya ga  hatan ni  oikon-da.  that bank TOP incompetent president NOM bankruptcy to force-PST  b. Baburu-no-sei Bake dewa-nai.  due-to-bubble only COP-not  'The bank, its incompetent president caused (it) to bankrupt. It's not only because of  the bursting of the bubble economy.'  Look at (b) in each example above. In (7b), the topic remains "the bank" (although not explicitly), and no mention is made of the agent of the previous event. In (8b), the topic shifts from the bank to the president. In (9b), the topic is still the bank but the focus shifts to the cause of bankruptcy. I propose that in these conditions the passive, subject cleft, and object topicalization are likely to occur respectively. Similar ideas are found in "Centering Theory" research discussed in Walker et al. eds. [15].  The proposed three-way distinction in focus on the agent is rather subtle, and I do not suggest under any of the conditions that no other structure would appear. But if the distinction exists, I expect to see statistically significant differences in the choice of the structures between these conditions proposed above. Data are being collected as I write this paper, and I hope to be able to report the results at the conference.  4. REFERENCES 
 The aim of this research was to develop a flexible, high quality articulatory speech synthesis tool. One feature of this research tool is the simulated annealing optimization procedure that is used to optimize the vocal tract parameters to match a specified set of formant characteristics. Another aspect of this study is the derivation of a new form of the acoustic equations. A transmission-line circuit model of the vocal system, which includes the vocal tract, the nasal tract with sinus cavities, the glottal impedance, the subglottal tract, the excitation source, and the turbulence noise source, was constructed. The acoustic equations of the vocal system were rederived for the proposed articulatory synthesizer. A digital time-domain approach was used to simulate the dynamic properties of the vocal system as well as to improve the quality of the synthesized speech.  1. INTRODUCTION Articulatory synthesis is the production of speech sounds using a model of the vocal tract, which directly or indirectly simulates the movements of the speech articulators. It provides a means for gaining an understanding of speech production and for studying phonetics. In such a model coarticulation effects arise naturally, and in principle it should be possible to deal correctly with glottal source properties, interaction between the vocal tract and the vocal folds, the contribution of the subglottal system, and the effects of the nasal tract and sinus cavities. Articulatory synthesis usually consists of two separate components. In the articulatory model, the vocal tract is divided into many small sections and the corresponding cross-sectional areas are used as parameters to represent the vocal tract characteristics. In the acoustic model, each cross-sectional area is approximated by an electrical analog transmission line. To simulate the movement of the vocal tract, the area functions must change with time. Each sound is designated in terms of a target configuration and the movement of the vocal tract is specified by a separate fast or slow motion of the articulators. A properly constructed articulatory synthesizer is capable of reproducing all the naturally relevant effects for the generation of fricatives and plosives, modeling coarticulation transitions as well as source-tract interaction in a manner that resembles the physical process that occurs in real speech production. Articulatory synthesizers will continue to be of great importance for research purposes, and to provide insights into various acoustic features of human speech. 2. REALIZATION OF THE ARTICULATORY MODEL Geometric data concerning the vocal tract is essential to our understanding of articulation, and is a key factor in speech production. According to the acoustic theory of speech production, the human vocal tract can be modeled as an acoustic tube with nonuniform and time-varying cross-sections. It modulates the excitation source to produce various linguistic sounds. The success of articulatory modeling depends to a  large extent on the accuracy with which the vocal tract cross-sectional area function can be specified for a particular utterance. Measurement of the vocal tract geometry is difficult. Several researchers have proposed analytical methods to derive the vocal tract cross-sectional area function from acoustic data.  Articulatory models can be classified into two major types: parametric area model and midsagittal distance model. The parametric area model describes the area function as a function of distance along the tract, subject to some constraints[1][2]. The area of the vocal tract is usually represented by a continuous function such as a hyperbola, a parabola, or a sinusoid. The midsagittal distance model describes the speech organ movements in a midsagittal plane and specifies the position of articulatory parameters to represent the vocal tract shape. Coker and Fujimura (1966) introduced an articulatory model with parameters assigned to the tongue body, tongue tip, and velum. Later this model was modified to control the movements of the articulators by rules[3].  Another articulatory model was designed by Mermelstein. His model can be adjusted to match the midsagittal X-ray tracings accurately. Our articulatory model is a modified version of Mermelstein's model[4]. A set of variables is used to specify the inferior outline of the vocal tract in the midsagittal plane (Figure 1). These variables, called articulatory parameters, are the tongue body center, the tongue tip, jaw, lips, hyoid, and velum. A modification of the lower part of the pharynx and tongue-tip-to-jaw region is also provided and included in our model.  Once the articulatory positions have been specified, the cross-sectional areas are calculated by superimposing a grid structure on the vocal tract outline. These grid lines vary with the positions of the articulators (they are fixed in Mermelstein's model). A total of 60 sections, 59 sections for the vocal tract plus one section (fixed length and area) for the outlet of the glottis, are used in our model. The sagittal distance, gi of section j, is defined as the grid line segment length between posterior-superior and anteriorinferior outlines. The center line of the vocal tract is formed by connecting the center points of the adjacent grid lines. The length of the center line is considered equivalent to the length of the vocal tract. The sagittal distances are eventually converted to cross-sectional areas by empiric formulas.  Figure 1: Articulatory model's parameters and midsagittal grids.  The calculation of formant frequencies from a given vocal tract cross-sectional area function has been well established in the acoustic theory of speech production. By computing the acoustic transfer function of a given vocal tract  configuration, we can decompose the formant frequencies from the denominator of the acoustic transfer function. One of the functions of the articulatory model is to compute the articulatory information (in particular, the vocal tract cross-sectional area) from the acoustic information (the first four formant frequencies in our study) that are obtained from the speech signal. Here we attempt a new solution using the simulated annealing algorithm, which is a "constrained multidimensional nonlinear optimization problem." The coordinates of the jaw, tongue body, tongue tip, lips, velum, and hyoid compose the multidimensional articulatory vector[5]. A comparison between the model-derived and the target-frame first four formant frequencies forms the cost function. There are two constraints: (1) the articulatory-to-acoustic transformation function, and (2) the boundary conditions for the articulatory parameters. The optimum articulatory vector is obtained by finding the minimum cost function. Once the optimum articulatory vector is determined, the articulatory model determines the vocal tract cross-sectional area function which in turn is used by the articulatory speech synthesizer[6][7][8].  3. REALIZATION OF THE ACOUSTIC MODEL  Basically, the acoustic model of the human vocal system embodies several submodels. Both the vocal tract and nasal tract models simulate the sound propagation in these tracts. The excitation source model represents and generates the voiced excitation waveforms for the vocal tract. The turbulent air flow at a constriction for fricatives and plosives is generated by the noise source model. The radiation model simulates the acoustic energy radiating from the lips and the nostrils. A transmission-line circuit model of the vocal system, which includes the vocal tract, the nasal tract with sinus cavities, the glottal impedance, the subglottal tract, the excitation source, and the turbulence noise source, was constructed. The acoustic model of each subsystem of the vocal system was analyzed.  Acoustical parameter p — Pressure u — Volume velocity — Air mass inertia (acoustic inductance Ai(Qc2) Air compressibility (acoustic capacitance Viscous loss Heat conduction loss Yielding wall  parameter Voltage Current L Inductance Capacitance Series resistance Shunt resistance zw— Shunt impedance  Acoustical I electrical analogues.  Figure 2: the equivalent circuit representation.  Transmission-line analogs of the vocal tract (or equivalent electrical circuit model) is based on the similarity between the acoustic wave propagation in a cylindrical tube and the propagation of an electrical wave along a transmission line. The derivation from the basic equations of acoustic wave propagation to an equivalent electrical quadripole representation is well known [9][10]. The analogs are summarized in Table 1. Figure 2 is an equivalent circuit representation of a soft-wall, lossy cylindrical tube. The series resistor R is used to represent the acoustic loss due to viscous drag in which the energy loss is proportional to the square of the volume velocity. The shunt conductance G represents the loss due to heat conduction, which is proportional to pressure squared. The shunt impedance is the acoustic equivalent mechanical impedance of the yielding wall. This wall impedance, which represents a mass-compliance-viscosity loss of the soft tissue, has three components. Note  –347–  that both R and G are a function of frequency. The vocal tract was approximated by a non-uniform, lossy, soft wall, straight tube with 60 concatenated elemental sections (circular or elliptic). The transmission-line analogy approach was used to model the vocal tract as an equivalent circuit network. A series resistor represents the viscous loss and a shunt conductance represents the thermal loss. The yielding wall vibration loss was modeled by a shunt impedance. The effect of the sinus cavities on the nasal consonants and nasalized vowels was discussed. The sinus cavity was regarded as a Helmholtz resonator and was modeled as a shunt impedance. Flanagan's model (1972) was considered the most appropriate radiation model for the time-domain articulatory synthesis. For the non-interactive excitation source, we simplified the unified glottal excitation model[11] that includes the jitter model and shimmer model into the LF model. For the interactive excitation source, we proposed a new model, which consists of the unified glottal excitation model, the subglottal model, and the glottal area model. The subglottal system was modeled by three cascaded RLC Foster circuits (Ananthapadmanabha and Fant, 1982). The triangular, sine, and raised-cosine functions were used as options to model the time-varying glottal area function[12]. For the turbulence noise source model, the distributed and series pressure noise source model[13] and the downstream parallel flow source model[14][15] were discussed. The parallel flow source model was adopted for this study. The turbulence noise source can be located 1) at the center of, 2) immediately downstream from, 3) upstream from, and 4) spatially distributed along the constriction region. A practical articulatory synthesizer was proposed that included the vocal tract, the nasal tract with sinus cavities, the glottal impedance, the subglottal system, the excitation source, and the turbulence noise source. The acoustic equations of the vocal system were derived for the proposed articulatory synthesizer. The time-domain approach was used to simulate the dynamic properties of the vocal system as well as to improve the quality of the synthesized speech. The vocal tract cross-sectional area or the articulatory parameters were interpolated between two consecutive target frames using a linear or arctan function. 4. RESULTS AND CONCLUSION The vocal tract tube can be described by two coupled partial differential acoustic equations. These two acoustic equations are functions of both time and space. Approximating the vocal tract as a sequence of elemental sections corresponds to digitizing the vocal tract in space, i.e., spatial sampling. For each elemental section, the transmission-line analog approach is applied to form the equivalent circuit model s as seen in Figure 2. Connecting the equivalent circuit of each section together in combination with the equivalent circuit models of the other parts of the vocal system (subglottal system, glottis, and nasal sinus cavities), a lumped circuit network representation of the vocal system can be formed, as shown in Figure 3. For the time-domain approach, the Kirchoffs and Ohm's laws are applied to the circuit network to obtain sets of differential equations. These differential equations, which correspond to the equivalent acoustic equations that govern the generation and the propagation of acoustic waves inside the vocal system, are transformed into discrete-time representations. This appendix provides a detailed derivation of the discrete-time acoustic equations, i.e., the difference matrix equations. The discretization scheme is similar to the work of Maeda (1982a)[16]. Our model, however, provides more features, such as the subglottal system, nasal sinus cavities, and turbulence noise source. Figure 4 presents the articulatory characteristics for /I/ and hi/ vowels. The midsagittal vocal tract outline and the corresponding synthetic speech waveform are obtained from sustained vowel phonations by using the simulated annealing algorithm. The articulatory synthesis windows (see Figure 5) is divided into twelve subareas. During the synthesis of speech these subareas are used to display the following messages and waveforms: (l )the target- and excitation-frame messages, (2)the vocal tract cross-sectional area function, the acoustic transfer function, and the midsagittal vocal tract outline of the current target frame, (3)the  Subglottal system and glottal impedance (optional)  Phaty ►geal tract n••n••••n•• •••••••••n••:: 10••••n••••1 •••••••••••• •••••••n••••• ••••••••••Wa  ure 3: The lumped circuit network representation of the vocal system.  1-1 11FUl Shunt sinus element (optional)  Nasal tract and!  ...mum... Me•:  • •  C14 *-s CD • • H CD SID * 0 C) fy C) C; V; • ti▪ 0 VI  CD O• 2. CD f"-tV)  ,tr CD < O0 AD 0  •0 ;ID  t..J1 Z1,*  CD`10::,  CD  1•••• •  1••••  '  AD CD e--1- 0 *-1 `-< n-1 CD 0 reh-0.t•t-• CD n-cl C4 " CCD AD(/)  CD  CD Cip  •  <  CD .- C) vp CD *C$ CD COD"' 6-  0• CD 0- Vl Fr r:1- CD  igI'eMrlkz52 ?4,'Zv'.4t. Otkr.t°gC<i;,:•*,i',:Og,i,,.'''<f0f.5"*W•4,.1, 73PeM'1.4:,)1, 0.4N404.A164,U; g1t",i1.;V :.::„4, . . . .$a04f1$,. ' • s,$::P: P' •<;*-i4. , .'•k0s+'i4: •,e••;4,••3*:..•V•. .'.•.,:. .: '4'.0. ..4. >4't4. 'sitei..f,‹ s4ip4Ceidekv`.,.(v',.S,...o.. *.r"*5*:.>.4.:z.4..c.s<.$.04...!,..X.','.'........l....,iP:;g_':'k„,,4 4a,4,i,% t,,a,1 , i,• i.♦4.•.,.g•  -1u-.40-0za*, $'4'" .44". 444,RA: ' ' *:Ralg,gati',t4UVIAAgigA<  .  IIK`..,V.  $  0, 4' . IP'. y+ f . ... 4'' ..,  ' ry «ti"w. •a--f--e10+44M 4,4,*  Total target fro) no.. 26  . amt target :Model :  ICurrerit target frau Po.: 14 ' ri  Fray Starting tin: 0,8900 set: htlit thIliti011: , (1.00 3e0  r2  17015 1701S : 18$71$ i : 188 2539.31 : 539;24  F4 3571.70.1 .: : 1577;47  ,  Error: 0:804081  :,  Total excitation fratello.-: 13. .Current excitation fteat na:: 16 Frain starting tine: 0.9142 set  ;vt. nirvtaz‘,  •  ,  \..  . 4  :: ',:).'Sfer Yogit: i ,i ‘ ,  , V „ I,1,,? e i , , ‘? 1 : 
ABSTRCAT The essence of standard X-bar theory is that structure building is asymmetric in the sense that a complex structure inherits properties from only one of its constituents. There are some structures, however, that are best analyzed as reflecting the properties of all their constituents. This kind of symmetric projection should in principle be allowed within the minimalist program if the union of the features of all the constituents contains no incompatible features. This claim is supported by the fact that Japanese wh-phrases marked with ka can function as indefinites as well as interrogatives. Under the assumptions that a wh-phrase with ka has the same internal structure regardless of its interpretations and that ka has no category feature, merging a wh-phrase with ka is a case of symmetric projection. The properties of both ka and its sister wh-phrase interact with those of the predicate taking the ka-phrase as its argument or adjunct, which ensures that an appropriate interpretation will be picked up from the two possible interpretations of the ka-phrase.  1. INDEFINITES AND PHRASAL INTERROGATIVES IN JAPANESE  It is well-known that wh-phrases in Japanese, unlike those in English, can be used as indefinite as well as interrogative expressions. For example, each sentence below contains exactly the same strings of words; the first one is interpreted as an indefinite and the second, as an interrogative:  (1) John-ga [ dare ka nagutta rasii ga [ dare ka ] sitteiru hito wa i nai. NOM someone beat it-seems but who Q know person TOP exist not 'John seems to have beaten someone but nobody knows who.' (2) John-ga [ dare kara ka ] henna tegami-o moratta rasii ga, NOM someone from strange letter ACC received it-seems but boku-wa [ dare kara ka sir- anai. I TOP who from Q know not 'John seems to have received a strange letter from someone but I don't know from whom.' (3) John-wa [ naze ka naiteita ga [ naze ka ]-wa daremo siranai. TOP some reason was-crying but why Q TOP nobody know 'John was crying for some reason but nobody knows why.'  The same point can be illustrated with (4), which can be interpreted as (5a) or (5b) depending on its context:  (4) John-wa [ naze ka rikai  dekinakatta  TOP why understand couldn't  (5) a. (Mary didn't come and) John was not able to understand [ why she didn't come ] b. (Mary explained the situation to John but) he was not able to understand it [ for some unclear reason ]  A wh-phrase to be construed as an interrogative will be called 'phrasal interrogative' in contrast to 'clausal interrogative' exemplified by the bracketed embedded question in (5a). A standard approach to the indefinite and phrasal interrogative usage of wh-phrases is to assume an empty clause for the latter, while treating the former as categorially the same as the inside whphrase (see [15] among others). Under this view, (2) would be analyzed as follows:  (6) John-ga [PP [PP dare kara ] ka ] henna tegami-o moratta rasii go,  NOM  someone from strange letter ACC received it-seems but  boku-wa [CP [PP dare kara] [[IP e ][COMP ka ]]] sir- anai.  I TOP  who from  Q know not  The idea behind this analysis is that if the same strings of words have distinct meanings, they have distinct structures. This is certainly one of the possible approaches, but a question should be raised whether there is a simpler and more plausible alternative to it. The purpose of this paper is to pursue this possibility. For ease of exposition, I will adopt Stabler's parsing/generation model presented in [12], [13], and [14], where the basic ideas in the minimalist program are assumed. The analysis, however, can be translated into any theoretical framework as long as it treats lexical items as consisting of syntactic and other features and assumes structure-building operations and principles such as Merge and the Projection Principle in the minimalist program. 1 The main claims will be that (i) lexical categories have category features that will project up to the complex they form, but functional categories do not; (ii) a complex structure is constructed from two inputs by combining the features of both symmetrically if one of them lacks a category feature; (iii) the indefinite/interrogative distinction with Japanese wh-phrases can be made simply under assumptions (i) and (ii); and (iv) two of the major syntactic differences between Japanese and English are also deducible from (i), (ii), and one parameter as to functional categories.  2. SEMANTIC AMBIGUITY WITHOUT STRUCTURAL AMBIGUITY  As discussed at the beginning, it is not unreasonable to assign a clausal structure to a wh-phrase that is to be interpreted as an interrogative 'clause.' In fact, assuming the same internal structure for the indefinite and interrogative usage of wh-phrase, as I will in this paper, might appear to be counterintuitive at first. There are clear cases, however, in which a single phrase can express distinct meanings but can hardly be associated with two distinct syntactic structures. Made, which is one of the Japanese particles, can be construed as temporal ('until' in English) or spatial ('as far as') depending on the main verb:  (7) a. John-wa 3-zi made matta. TOP o'clock until waited 'John waited until three o'clock.  b. John-wa Osaka made itta. TOP as far as went 'John went as far as Osaka.'  The interpretive mechanism here is simple: made can be temporal or spatial; its sister constituent in (7a) denotes a specific time and the temporal interpretation is obtained, whereas its sister in (7b) expresses the name of a place and hence, its spatial interpretation. Replacing the made-phrase in (7a) with that in (7b), as in (8), produces some strangeness:  
Following an ending intonation, postposed adverbial clauses stand for independent units in their own right (Ford, 1993: 102). The Mandarin data suggest that adverbial clauses after recipients have provided some sign of disbelief, lack of understanding or other trouble; often arrise in the context of self-editing and the negotiation of understanding between conversationalists (Wang, 1996). This paper proceeds as follows. In section 1, I'll offer the overall distribution of Chinese temporal and conditional connectors in different positins as preposed and postposed according to the 15 texts in my corpus. I'll list all the adverbial connectors found in my data and argue that even representing the same adverbial category, these adverbials are not at equal status in "frequency". That is, take when-clause for example, although there're three possible expressions as dang ...(de shihou) vs. zai...de shihou vs. ...de shihou, their frequencies in Chinese spoken discourse are significantly different. After-adverbials as zhihou/yihou are much more frequently used in my corpus than before-adverbials as zhiqian/yiqian. Similar to the situation of when-clause, if-adverbials such as ruguo ... vs. jiaru... vs. ...dehua are very different in frequency. Sections 2&3 will give examples of adverbial connectors mentioned above and more detailed discussion on their discourse functions in preposed and postposed positions respectively. And in the final part of this paper, I'll draw a brief conclusion and offer a semantic study on these adverbial connectors to explain how they differ in the performance of distribution and discourse function.  1. Classifications and Distribution Consistent with what Wang (1996) had already observed that both temporal and  conditional tend to occur initially (91% & 87%), the figures in table 1 add a strong support to  such tendency (97% & 88%):  Table 1. Distribution of temporal and conditonal clauses by position in my data  position/type Initial Final Total  Temporal 110 (97%) 3 (3%) 113  Conditional 77 (88%) 10 (12%) 87  Total 187 (93%) 13 (7%) 200  Table 2& 3 show the distribution of all the temporal and conditional connectors found in my data so far. Note that for temporal ones, the expression (X)...de shihou (here X can be dang or zai) is almost the only possible one in when-clause except dan... without de shihou; after-adverbials (29%) are more frequently used than before-adverbials. For conditional ones, ruguo... (47%) is the most productive adverbial connector in if-clause and ...de hua is the second one (34%). Note that we have also combination as ruguo ...de hua (13%). This means that they are not in complementary distribution. So ruguo... & ...de hua are the strongest predictors in conditional clauses (47%+34%+13%=94%).  Table 2. Temporal connectors in initial and final positions  connectors/position  Initial  Final  s...(n14K)dang... de (shihou) 'when'  5  rt...rrji4R zai ...de shihou 'when'  5  2  ...n[4N ...de shihou 'when'  65 (59%)  
This paper deals with alternative questions in English, and proposes an analysis employing an alternative operator. I claim that the alternative question interpretation is not obtained by such syntactic processes as movement or gapping, but by scoping of an alternative operator that originates from the conjunction or. Accordingly, while the syntactic forms of alternative questions contain nothing more than a coordinate structure, their semantic component will be analyzed as including an alternative operator. Furthermore, using a 'multiple inheritance' type hierarchy of clauses, I will show that how the relationship between alternative questions and other types of questions can be represented. 1. INTRODUCTION Questions can be classified into three types according to the kind of reply they elicit, polar questions (or yes-no questions), wh-questions, and alternative questions. Compared to other types of questions, alternative questions have not received as much attention. It is perhaps because the relationship between alternative questions and the other two types of questions is often not very clear; furthermore, an analysis of alternative questions usually requires dealing with various syntactic and semantic issues, such as disjunction, coordination, syntax and semantics of interrogatives, and treatment of whether and if. In this paper, we deal with English alternative questions such as (1) within the framework of Head-Driven Phrase Structure Grammar. (1) Did Mary buy books or video tapes? There are two issues to focus on regarding the analysis of (1). First, it should be explained how the alternative question interpretation of (1) is obtained. Some previous semantic analyses assume that the interpretation of (1) involves a semantic version of conjunction reduction, with its denotation corresponding to a set of propositions (Karttunen [5], Groenendijk & Stokhof [4], Roberts [10]). On the other hand, there can be other semantic mechanisms that yield wide scope of disjunction in ( I ). In this paper, I will pursue an approach that posits a disjunction operator associated with or. Second, the syntactic representation of (1) should be determined in conjunction with its semantic interpretation. After examining two possible syntactic analyses, one involving movement of disjunction scope indicator, and the other employing syntactic reduction process, I will claim that (1) can be analyzed without assuming either movement or a reduction process. In the analysis to be proposed, (1) simply involves coordination of noun phrases. With the base-generated disjunction in syntax, I will present an analysis where alternative question interpretation is obtained by scoping of a disjunction operator originating from or.  In section 2, I will discuss some basic properties of alternative questions. In particular, it is pointed out that an analysis of alternative questions should explain the fact that questions like (1) may receive a polar question interpretation as well as an alternative question interpretation. Section 3 deals with the semantic and syntactic behaviors of disjoined phrases in alternative questions. Based on Rooth & Partee [11], I will show that the alternative question interpretation is related to the wide scope reading of or. Moreover, as mentioned above, I will argue that alternative questions like (1) can be simply analyzed as involving coordinated NPs. In section 4, I propose an analysis of alternative questions within HPSG. I will show that in the type hierarchy, alternative questions can be represented as a clause type, whose semantic representation is distinguished from that of other clause types by an alternative operator. The ambiguity of alternative questions noted above is accounted for by the optional nature of the disjunction operator. When the operator does not arise in or, the question at hand will receive a polar question interpretation. Section 5 concludes the paper. 2. ALTERNATIVE QUESTIONS AND POLAR QUESTIONS An alternative question presents two or more options for the reply. It can be used as an indirect question introduced by whether or if, manifesting the same syntactic behavior as a polar question: (2) a. Did Sandy want Coke or iced tea? b. Bill asked Sandy whether she wanted Coke or iced tea. (3) a. Did Sandy want coffee? b. Bill asked Sandy whether she wanted coffee. Before we proceed, a terminological comment is in order. Cases like (3) have traditionally been called 'yes-no questions'. In this paper, however, I retain the term 'polar questions' to focus on the semantic aspect of the questions, which inquires the truth condition on a polarity scale. When the syntactic aspect involving formal processes such as subject auxiliary inversion or use of complementizers whether or if is at issue, I will use the term 'yes-no interrogatives'. It is well known that yes-no interrogatives such as (2) are ambiguous between two interpretations, polar question interpretation and alternative question interpretation. When it is interpreted as a polar question, what Bill asked Sandy is whether or not she wanted one of the two drinks. In this case, Bill is indifferent as to which of the two drinks Sandy wants. When it is interpreted as an alternative question, what Bill asked is whether Sandy wanted Coke or Sandy wanted iced tea. In actual utterance, different intonations disambiguate the two readings. There is another type of alternative questions in (4): (4) a. Is Sandy still at home or did she already leave for the party? b. Bill asked me whether Sandy was still at home or she had already left for the party. Unlike (2), two sentences are coordinated in (4). Following Ginzburg [2], we will treat cases like (4) as disjoined polar questions. In other words, each disjunct in (4) is a polar question and is connected by or to form a coordinated sentence. In this case, the disjunction is construed as a choice between two questions)) Thus in the response, it is sufficient to reply to one disjunct. We will not focus on examples such as (4), since their syntactic analyses will involve nothing more than disjunction of polar questions. Karttunen [5] argues that polar questions such as (3a) can be considered as 'degenerate' alternative questions, thus claiming that the indirect polar question in (3b) is an alternative question, whether Sandy likes coffee or Sandy doesn't like coffee. Accordingly, the indirect 1) Following Ginzburg, I suggest that such interpretation of disjoined questions is caused by a conversational implicature carried by disjunction that exactly one of the disjunct holds, but not both.  question in (3b) is analyzed as denoting the set of the propositions in (5): (5) {Sandy likes coffee, Sandy does not like coffee} In Karttunen, this is analogous to the denotation of the indirect alternative question in (2b), namely the disjunctively specified set of propositions in (6): (6) {Sandy wants Coke, Sandy wants iced tea} While our analysis does not directly draw on Karttenen's semantic analysis of polar questions and alternative questions, it shares the intuition behind his analysis: that both of them involve choice between propositions (or entities). As we will see in section 4.1, we assume that there is a common mode 'choice', by which both types of questions are interpreted. 3. SCOPING OF OR AND THE SYNTAX OF ALTERNATIVE QUESTIONS 3.1. Or As we saw in (1) and (2), alternative questions contain the conjunction or, and the semantics of disjunction is closely related to the alternative question interpretation. In this section, we will briefly look at the properties of disjoined phrases that are relevant in the discussion of alternative questions. Rooth & Partee [11] observe interesting facts regarding the interpretation of disjunction in English and argues that or bears scopal properties. The properties of or as a scope-bearing element are shown in the following example (Rooth & Partee's (13); See also Larson [7] for the discussion of the example): (7) Mary is looking for a maid or a cook. The example (7) is three-way ambiguous. The first reading is a de dicto reading of the conjoined phrase, in which Mary is searching for a servant and would end the search if she finds x that meets the description, x is a maid or x is a cook. The second reading is a de re reading, where there is some particular individual x who is either a maid or a cook such that Mary is seeking x. The third reading, which is referred to as the 'wide scope or' reading in Rooth & Partee, involves 'disjunction reduction' interpretation, and can be paraphrased as 'either Mary is looking for an individual x meeting the description of being a maid or else she is looking for an individual x meeting the description of being a cook'. The wide scope reading of or also appears when or-disjunction is contained in the complement of control verbs such as want (cf. Schwarz [15]): (8) John wanted to eat rice or beans. The example (8) is ambiguous depending on the scope of or with respect to the embedding verb want. If or takes narrow scope in (8), John would be indifferent as to whether he would end up with eating rice or eating beans. On the other hand, in the wide scope reading of or, the sentence is true if John wants to eat rice or John wants to eat beans. In this case, it is typically indicated that the speaker of the sentence does not know which of the two food John wanted to eat. So far, we have seen that or bears scopal properties, and that wide scope of or results in an interpretation containing disjunction of two propositions. We presume that or of alternative questions have the same property, and that such scopal nature of or is responsible for the alternative question readings of (1) and (2). 3.2 Syntactic Representations of Alternative Questions  This section briefly reviews two possible analyses of the syntax of alternative questions that may account for wide scope reading of disjunction; a movement analysis and a 'gapping' analysis. Based on Schwarz's [15], I will show that neither of these approaches is appropriate, and then propose that alternative questions like (1) and (2) do not involve syntactic processes such as movement or gapping. Disjunction in (1) and (2) will be simply treated by the coordination of NPs in its syntactic representation. Larson [7] proposes a movement analysis of disjunction. He observes that wide scope reading of disjunction discussed in 3.1 can be obtained by the sentence initial either. For example, while (9a) is still three-way ambiguous, (9b) lacks de ditto reading. (9) a. Mary is looking for either a maid or a cook. b. Either Mary is looking for a maid or a cook. The same pattern holds for examples with control verbs. In contrast to (10a), which is ambiguous, the example (lOb) wherein either is placed in the sentence-initial position, the ambiguity disappears. Thus (lOb) has only the wide scope reading of or: (10) a. John wanted to eat either rice or beans. b. Either John wanted to eat rice or beans. Larson argues that the DS (deep structure) position of either in (9b) and (10b) is adjacent to the disjoined phrase, as in (9a) and (10a). When either is moved to the clause-initial position, as in (9b) and (10b), unbalanced either/or disjunction occurs. 2) Under this assumption, he argues: "When either occurs displaced from its associated or, then its overt surface syntactic position explicitly ' marks' the scope of disjunction. On the other hand, when it occurs undisplaced and adjacent to its disjunction in surface form, then its potential surface positions delimit the potential scope of or" (Larson [7, 224-225]). To implement this generalization, Larson proposes that either may undergo SS- or LF-movement, and the LF position of either marks the scope of disjunction. Larson further proposes that the surface syntax of alternative questions is very similar to that of unbalanced either/or disjunction in (9b) and (10b). He argues that this view is supported by the fact that historically, whether is developed as the wh-counterpart' of either, with the original meaning, which of either A or B. According to him, whether of the alternative questions is moved from its underlying position that is adjacent to the disjoined phrase to the clause-initial position (i.e. a COMP position in Larson's analysis). (11) a. Bill wonders Sandy likes whether [cookies or jelly] (DS) b. Bill wonders whether, Sandy likes t, [cookies or jelly] (SS) Since whether is now placed in the clause-initial position, it invokes the wide scope reading that gives rise to the alternative question interpretation. This movement analysis is criticized by Schwarz [15], who points out that it does not apply to the examples in (12) in the desired way. Larson's theory predicts that the sentences in (12) are derived from their sources (13) without any problem, since there is no island or finite clause that blocks either-movement. However, (12a-c) are degraded contrary to Larson's prediction. Since sources of (12a-c) are well-formed, this poses a problem with the movement account. (12) a.??Either this pissed Bill or Sue off. b.??Either she turned the test or the homework in. c.??Either they locked you or me up. 2) Larson uses the term ' unbalanced disjunction' to indicate cases where two constituents joined by the two-part conjunction either... or... are not parallel.  (13) a. This pissed either Bill or Sue off. b. She turned either the test or the homework in. c. They locked either you or me up. If either does not move, then Larson's approach capitalizing on the movement of whether loses its motivation. While Schwarz's criticism focuses on either movement, there are some further problems that we can find with whether movement analysis. Consider the following sentence:  (14) a. Susan asked them whether they wanted meat or fish. b. Susan asked them if they wanted meat or fish.  Larson argues that the element if in (14b), which introduces an alternative question, is not plausibly analyzed as a scopal indicator on a par with either or whether, since if has no morphological or historical connection with disjunction. He claims that if in (14b) is simply a base-generated complementizer, and that the wide scope or reading is obtained not by the movement of if, but by the movement of the null (scopal) indicator 0. Thus the S complements of (14) are represented in (15):  (15) a. [-s- [comp whether, [+WH]] [s they like [ Np t, meat or fish]]]  b. [-s- [comp 0,  if ] [s they like [NP t, meat or fish]]]  The null indicator 0 is also posited in matrix alternative questions such as ' Do they like meat or fish?' in order to account for the wide scope of or. The use of 0, however, is not fully motivated. In particular, it is not clear why disjunction in English always needs a syntactic scopal indicator, while other scopal elements such as quantifiers and negation do not. If we have a way to account for the wide scope reading of or, without treating whether as a scope indicator, we would not need to posit a null element 0. Even when we follow Larson's employment of 0, other difficulties arise. As mentioned earlier, yes-no interrogatives are ambiguous between the polar question reading and the alternative question reading, depending on whether disjunction takes narrow or wide scope. When disjunction takes narrow scope, a problem may arise with the view that whether is a scopal indicator, since this view will only yield wide scope reading for or. In order to explain this, Larson assumes that there is an alternative way to derive examples like (14a). In addition to (16a) which involves whether arising in the disjoined phrase, he proposes another structure (16b) where whether originates from a hidden or not disjunction:  (16) a. [-s- [comp whether;] [s they like [ NP t, meat or fish]]] b. [ s-- [comp whether,' [s [coNJ t, or not] [s [s they like [ NP t, meat or fish]]]  In (16b), whether is moved from the S-initial conjunctive element whether or not, where the latter phrase or not may not be realized in surface form. Moreover, the overt or within the NP has its own scope indicator, which adjoins to S. In the configuration (16b), the overt or takes only narrow scope. When we consider the structures (15b) and (16b), we find another problem regarding the nature of null 0 movement. The nature of 0 movement is not clear at all; while (15b) contains movement to COMP, (16b) contains adjunction to S. Furthermore, it is questionable that one of the underlying structures of (14a) should contain the hidden element or not. Positing this hidden element is more problematic in the explanation of narrow scope reading of (14b), namely (17), because unlike whether, if is not directly followed by or not in the surface form.  (17)  [comp 0, if] [s [coNi t, or not] [s Oi [s they like [NP ti meat or fish]]]  (18) *He didn't asked if or not they like meat.  Based on the problems with either/whether movement that we have discussed so far, I conclude that it is worth looking for the account of alternative questions. Now we will briefly examine the second possible syntactic analysis of alternative questions, i.e. the 'reduction' account. Schwarz [15] proposes that either/or unbalanced disjunction (such as (10b)) is best analyzed as the result of a syntactic reduction process. Schwarz argues that the reduction process manifested in either/or disjunction can be identified with what Ross [12] calls 'gapping'. When gapping occurs, the 'gap' must include the finite verb in the second conjunct, as in (19a). In many cases, it contains other elements plus the verb, as in (19b,c).3) (19) a. Tom ate beans and others ate rice. b. Jack begged Mary to get married and Bill begged Lisa to get married. c. On Monday I bought a car and on Tuesday I bought a motorcycle. According to Schwarz, this general strategy of gapping is also operative in unbalanced disjunction. (20) a. Either John has seen Harry or Bill has–seen Sue. b. Either [this pissed Bill off] or [this–p-issed- Sue -aft Moreover, Schwarz shows that unbalanced either/or disjunction is subject to the same restrictions as gapping. For example, unbalanced disjunction observes a parallelism constraint on the coordinates for gapping. When gapping occurs, parallelism is required between the first and the second coordinates. Accordingly, the unacceptability of (2 1 b) is accounted for by lack of such parallelism. (21) a. [Some talked with you about politics] and [others talked with me about music]. b. *[Some talked about politics] and [others deck–with me about music]. Schwarz claims the degraded examples in (12) should be explained in the same way. (22) a.??Either [this pissed Bill] or Rh-is–pissed Sue off]. b.??Either [she turned the test] or [she turned the homework in]. c.??Either [they locked you] or [they locked me up]. Each sentence in (22) includes limping disjunction, which violates the parallelism constraint. So far we have examined Schwarz's proposal that unbalanced either/or disjunction involves syntactic reduction. Now, would the same account be applicable to disjunction in alternative questions? Schwarz suggests that the answer is negative. This is because alternative questions do not follow the same kind of restrictions as gapping. According to him, while sentences in (12) or (22) are degraded, the corresponding alternative questions in (23) are grammatical. (23) a. I wonder whether this pissed Bill or Sue off. b. I wonder whether they locked you or me up. c. I wonder whether she turned the test or the homework in. To summarize, Schwarz shows that alternative questions exhibit different syntactic properties from unbalanced either/or disjunction, and leaves it an open question how the interpretation of alternative questions is syntactically represented. Following Schwarz, I assume that what is responsible for the syntax of alternative questions in (2) is not gapping. I propose that the disjoined NPs in (2) are simply the result of NP coordination via the conjunction or. As will be shown in the next section, the wide scope of disjunction will be explained by a theory of scope that is based on Cooper's [1] quantifier storage technique. 3) The term 'gap' refers to the elided material in the second coordinate.  4. REPRESENTATION OF ALTERNATIVE QUESTIONS  In this section, we will show how alternative questions are represented within the framework of Head-Driven Phrase Structure Grammar (HPSG).  4.1. Representation of Questions  In order to capture the relationship between syntax and semantics, we will employ a type hierarchy of clauses. While clause types can be treated as one dimension of a phrasal type hierarchy (Sag [13]), we adopt in this work a more conservative version of HPSG, where the standard word/phrase distinction of sign is retained. In this version, clause types are subtypes of construction that are cross-classified with the word/phrase distinction (cf. Kathol [6]). A clause is classified in two dimensions, sentence-mood and rootedness. Three basic sentence moods, declarative, interrogative, and imperative constitute a partition of mood, and this partition is cross-classified with rootedness of a clause. Interrogative clauses are further partitioned into yes/no interrogative and wh-interrogative, and yes/no interrogative, in turn, is partitioned into inv(erted)- yes-no-interrogative) and subord(inate)-yes-no-int(errogative) . 4) The basic hierarchy that we will assume is shown in (24):  (24)  clause  I rootedness  mood  root subord  interrog  decl imp  yes/no-int wh-int  -  suhord-  yes/no-int yes/no-int  It is important to understand that each type (or subtype) is associated with type-specific constraints, and that for any sort in the hierarchy, constraints associated with that sort are inherited by all of its subsorts. For example, the sort yes-no-int will inherit all the constraints associated with interrog. Within HPSG, semantic content of a sign is represented as a value of the CONT(ENT) attribute. We assume that the CONT value of a sentence is of the sort, propositional)-obj , which includes information on the "mode" by which a sentence is interpreted (cf. Yoo [ 16]):  (25)  prop-obj  MODE mode  CONT  psoa  ISSUE QUANTS list quantifiers)  _NUCLEUS q(uantifier)f(ree)psoa _  The old psoa in Pollard & Sag [8] is now a value of the attribute ISSUE in the feature geometry in (25), and a new sort prop(ositional)-obj replaces the sort psoa in the partition of cont(ent):  (26)  content  nom-obj prop-obj quantifier  4) The type wh-int also has its subtypes cross-classified with the rootedness dimension, which we do not discuss in detail here.  In (25), the MODE value is of the sort, mode that is partitioned as in (27):  (27)  mode  assertion  question  command  choice  polar alt  In (27), the sorts polar and alt respectively represent the mode in which polar questions and alternative questions are interpreted. The sorts polar and alt are represented as subsorts of choice, in order to reflect the observation that both polar questions and alternative questions involve choice among a given set of answers. Polar questions concern whether the proposition at hand is true or false, whereas alternative questions provide an option in terms of the phrases conjoined. We analyze the content of wh-questions as a propositional-object whose MODE value is wh and which has a wh-operator in its QUANTS list. Thus the CONTENT of a wh-question 'Who sneezed?' can be represented as follows:  (28)  prop-obj  MODE wh  psoa  ISSUE QUANTS <11zwh-op>  NUC –sneeze  SNEEZER F21  For polar questions, the content is of the type, propositional-object whose MODE value is polar. The following illustrates the CONTENT value of 'Did he leave?':  (29)  prop-obj  MODE polar  – psoa  ISSUE QUANTS < >  NUC T sneeze  SNEEZER 2  On the other hand, the CONTENT of the alternative question 'Does he like cookies or jelly?' can be analyzed as in (30):  (30) – prop-obj MODE alt psoa ISSUE QUANTS <alt-op NUC – like LIKER LIKED 4  As will be shown in 4.2, we propose that there is an alternative operator for the disjoined phrase, which appears in the QUANTS list in (30). Since the MODE value can be wh if and only if the QUANTS contains a wh-operator, we need the following constraint to ensure this:  (31) {MODE wh] H [QUANTS <..wh-op..>]  Likewise the MODE value can be alt if and only if the QUANTS contains an alternative operator. The relevant constraint is as follows:  (32) [MODE all]  [QUANTS <..alt-op..>1  As we noted in section 2, matrix yes-no interrogatives in English, whether they are used as polar questions or alternative questions, involve subject-auxiliary inversion, while embedded yes-no interrogatives do not. This fact can be implemented via the INV(ERTED) value of the clause, which is identical to that of the head verb of the clause. This means we need to add the following constraints:  (33) inv-yes/no-int. --> [HEADIINV +] (34) subord-yes/no-int. ---> FHEADIINV LMARKING whetherV  In the case of subordinate yes/no interrogatives, we need an additional requirement to guarantee the introduction of whether or if via the MARKING value stated in (34).5)  4.2. Alternative Operator  It has been widely assumed that a wh-question interpretation is obtained by scoping of the interrogative operator associated with the wh-phrase. Based on the scoping property of disjunction that we saw in section 3, we assume that an alternative question interpretation in such examples as (35) is assigned by scoping of the alternative operator associated with the disjoined phrase: (35) Does Sandy like cookies or jelly?  In order to represent the operator scope, we will employ Pollard & Sag's [8] theory of quantification, which is based on Cooper's [1] quantifier storage technique.6) I propose that when NPs are conjoined by or, the conjunction optionally has an alternative operator in its QSTORE. The QSTORE value of the conjunction is inherited into the entire resulting NP, and further into successively larger constituents, and retrieved at an appropriate site in the structure (i.e. at the node whose CONT ISSUE value is of the sort, psoa). The logical form of the alternative question (35) can be represented as (36), where the bracketed part corresponds to an alternative operator:  (36) [alt (x=y V x=z) A cookies'(y) A jelly'(z)] like(j,x) The alternative operator originates in the conjunction or. The lexical entry of or that conjoins constituents whose CONT is of nom-object (normally NPs) can be represented as follows:7)  5) Here we take the words whether and if to be markers whose MARKING values are of the sorts, whether and if, respectively. Other examples of markers includes complementizers that and for, and the comparative words than and as. (Pollard & Sag [8]). See Ginzburg & Sag [3] for the treatment of whether as a complementizer which is a subtype of a verbal category. 6) Pollard & Sag's theory cannot explain scoping of wh-operators, and this is remedied in Yoo [16] and Pollard & Yoo [9], which provide revised and extended theory of operator scope. Since our discussion of alternative questions in this paper is not directly related with wh-operators, we will adopt Pollard & Sag's theory for the sake of simplicity. 7) We assume that there is another entry of or that conjoins two categories whose CONTIISSUE are of sort psoa (i.e. VPs or Ss in usual cases). We posit separate entries in order to account for different combinatorial semantics in each case. While the version of semantics presented in Sag & Wasow [14] enable them to state the combinatorial semantics of both cases in a single coordination rule, it remains to be worked out how operator scope is represented in such a  (37) or  HEAD conjunction  SPEC [IND RESTR  IND RESTR  }  CONT Ell IND  or  RESTR JUNTS equal  ARG1 0  ARG2  QSTORE( [ DET alt  )  
In this paper we propose a method to compositionally interpret tenses of Japanese complex sentences on the basis of Head-Driven Phrase Structure Grammar and Discourse Representation Theory. In this approach, each of the tense-bearing forms such as main verbs and the past auxiliary is given a single temporal meaning independent of its position in the syntactic structure. The `relative tense theory' according to which a tense in a subordinate clause is interpreted in relation to a syntactically higher tense lays a foundation for this formalization. 
Annne Abeille, IUF, TALaNa & LalTICe, Paris 7 Marie-Helene Candito, TALaNa & LaTIICe, Paris 7 and Lexiquest Alexandra Kinyon, TALaNa & LalTICe, Paris 7 and IRCS, UPenn abeille, kinyon, candito@linguist.jussieu.fr  Introduction We describe the current status and organization of a French Lexicalized Tree Adjoining Grammar (FfAG), developped over the last 10 years at TALaNa (Abeillt! 91, Candito 99). The new version grammar is generated semi-automatically, independently of any corpus or application domain..It is intended to m.odel. speaker competence, ~nd can be used both .for parsing and generat10n. As far as parsmg is concemed, we descnbe a general processmg module which can rank the different parses produced based on linguistic infonnation present in FfAG. 1. General linguistic choices Most of our linguistic analyses follow those of Abeille 91 (except that clitic arguments are substituted and not adjoined), complemented by Candito 99. We dispense with most empty categories, especially in the case of extraction.l Semantically void (or non autonomous) elements, such .as complementizers, argument marking prepositions or idiom chunks, are coanchors in the elementary tree of their governing predicates. 1.1 A minimal tagset We depart frorn traditional part of speech wherever the modern linguistic analyses have better to propose, especially in the generative tradition. We thus distinguish a special category for Clitics (weak pronouns) following Kayne 75, and for Complementizers. We collapse proper names, common nouns and pronouns into one category N, with features. We do not have a tag for subordinating conjunctions which are either Prepositions (followed by a complementizer: pendant que (during)) or (füll) Cornplementizers (si (if), comme (as).„). Sentential structures are 'flat' (no internal VP). We thus have the following tagset. Lexical categories: D (determiners), N (nouns, names, pronouns), V (verb), Cl {cJitic pronoun), Prep (preposition), A (adjective), Adv (adverb), Conj (Coordinating conjunction), C (complementizer, subordinating conjunction), Non lexical categories: SP (prepositional phrase), S (sentence). A and N are also used for nominal or adjectival phrases. 1. 2 A rieb set or grammatical functions Tree sketches of the French TAG are compiled out of the French metagrammar (Candito 99), which expresses subcategorization in tenns of grammatical fönctions. The functions used in the French MGfor verbs are the following: ~ubject, object, dat-object, obl-object, gen-objet, locative, source-locative, manner, goalmfinitive, perception-infinitive, interrogative clause, "predicative complement" All these functions can be both initial functions and final functions. An additional funciion "agt· object" is used as final function only, and is beared by a by-phrase in the case of passive. We use several "complement" functions for complements of adjectives, prepositions, nouns, adverbs. And these categories may bear the function "modifier" with respect to the element they modify. 1. 3. A parsimonious use of reatures Most of the syntactic properties handled by feature structures in unification based linguistic theories (LFG or HPSG) are directly captured by the topoJogy of the elementary trees in LTAG.  
.·. resen1es the equivalence with tree adjoining languages and we provide a tabulationframework to execute any automaton in polynomial time with respect to the length of the input string. 1. lntroduction Embedded Push-Down Automata (EPDA) were defined in (Vijay-Shanker, 1988) as an extension of Push-Down Automata that accept exactly the dass of Tree Adjoining Languages. They can also be seen as a Ievel-2 automata in a progression of linear iterated pushdowns involving nested stacks (Weir, 1994). An EPDA consists of a finite state control, an input tape and a stack made up of non-empty stacks containing stack symbols. A transition can consult the state, the input string and the top element of the top stack and then change the state, read a character of the input string and replace the top element by a finite sequence of stack elements to give a new top stack, and new stacks can be placed above and below the top stack. EPDA can describe parsing strategies for tree adjoining grarnmars in which adjunctions are recognized top-down. The same kind of strategies can be described in strongly-driven 2-stack automata (de la Clergerie & Alonso Pardo, 1998) and linear indexed automata (Nederhof, 1999), which has associated tabulation frameworks allowing those automata tobe executed in polynomial time with respect to the size of the input string. In this paper we propose a redefinition of EPDA in otder to provide a tabulation framework for this dass of automata. 2. EPDA without states Finite-state control is not a fundamental component of push-down automata, as the current state in a configuration can be stored in the top element of the stack of the automaton (Lang, 1991 ). a Finite-state control can also be eliminated from EPDA, obtaining a new definition that considers a EPDA as a tuple (VT, Vs, e, $0, $f) where VT is a finite Set of terminal symbols, Vs is finite eset of stack symbols, $0 E Vs is the initial stack symbol, $/ E Vs is the final stack symbol and is a finite set of six types of transition: SWAP: Transitions of the form C 8 F that replace the top element of the top stack while scanning a. The application of such a transition on a stack Y[aB retums the stack Y[n:C. • This research was partially supported by the FEDER of EU (Grant lFD97-0047-C04-02) and Xunta de Ga\icia (Grant PGIDT99Xll0502B).  20 Miguel A. Alonso, Brie de Ja Clergerie & Manuel Vilares  PUSH: Transitions of the form C 8 C F that push F onto C. The application of such a transition on a stack Y [aC returns the stack Y [aCF.  POP: Transitions of the form C F 8 G that replace C and F by G. Tue application of such a transition on Y [aCF returns the stack Y [a:G.  WRAP-A: Transitions wrap-above of the form C 8 C, [F that push a new stack [Fon the top of the automaton stack. The application of such a transition on a stack Y [aC returns the stack Y [a:C [F.  WRAP-ß: Transitions wrap-below of the form C 8 [C, F that store a new stack [C just below the top stack, and change from C to F the top elemenl of the top stack. The application of such a transition on a stack Y[aC returns the stack Y[C[aF.  UNWRAP: Transitions of the form C, [F 8 G that delete the top stack [F and replace the new top element by G. TI1e application of such a transition on a stack Y [aC [F returns the stack Y [aG.  where C, F, G E V„, Y E ([F,S)*, a E 118, a E Fr U {t} and [ (j. Vs is a new symbol used as stack Separator. lt can be proved that transitions of a EPDA with states can be emulated by e transitions in and vice versa. An instantaneous co11figuratio11 is a pair (Y, w ) , where Y represents the contents of the automa- ton stack and w is the part of the input string that is yet to be read. A configuration (Y, aw) derives a configuration (Y', w ), denoted (Y, aw) 1- (Y' , w), if and only if there exists a transition that applied to Y gives Y' and scans a from the input string. We use 1- • to denote the reflexive and transitive closure of 1-. An input string is accepted by an EPDA if ([$0 , w) 1- • ([$1, €). The Ianguage accepted by an EPDA is the set of w E Vf such that ( [$0 , w) 1- * ( [ $1, t).  3. Compiling TAG into EPDA  We consider each elementa.ry tree / of a TAG as forn1ed by a set of context-free produc- tions 'P('y): a node N'Y and its g child.ren Nl ... NJ are represented by a production N1 -4 Nl ... NJ . The elements of the productions are the nodes of the tree, except for the case of e!ements belonging to Vr u {e} in the right-hand side of production. Those elements may have  no children and can not be adjoined, so we identify such nodes labeled by a te1minal with that terminal. We use ß E adj(N'Y) to denote that a tree ß may be adjoined at node N1. lf ad-  junction is not mandatory at N1, then nil E adj (N1). We consider the additional productions  T 0 -4 R 0 : Tß -4 R ß and F ß -4 l. for each initial tree a E I and each auxiliary tree ß E A, where R °' is the root node of a and R ß and F ß are the root node and foot node of ß, respec-  tively. After disabling T 1 and ..Las adjunction nodes the generative capability of the grammar  remains intact. Figure l show_s the generic compilation schema from TAG to EPDA, where symbols 'il'/.,, have  been introduced to denote dotted productions. The meaning of each compilation rule is graph-  icaIJy shown in figure 2. This schema is parameterized by ~, the infonnation propagated  Jrr, top-down w.r.t. the node N 1 , and by  the infonnation propa~ed bottom-up. When the  schema is used to.implement a top-down strategy ~ = N1 and Ji.n = 0, where Dis a fresh  Jrr stack symbol. A bottom-u,P_strategy requires~ = O and = N'T. For a Earley-like parsing  = strategy, ~ = N'Y and Nt N'Y, where N1 and N1 are used to distinguish the top-down  prediction from the bottom-up propagation of a node.  We can observe in figure l that each stack stores pending adjunctions with respect to the node  placed on the top of the stack in a top-down treatment of adjunctions: when an adjunction node  21 A redefinition ofEmbedded Push-Down Automata  [INIT] [CALL] [SCALL] [SEL] [TAB] [RET] [SRET] [SCAN]  $0 f----t $0 [ 'Vg,o \i'l,s f----t 'Vl,s [ N:..+~ 'V~. f----t ['V~.•, N~ r,s+l  N-:-r::,:Oy+  f----t  'V'r r,O  ~ 'Vl,nr f----t  'Vl,.„ [ J\T;.+1 f----t \7~,s+1  'V~., [ Nf..+1 f----t 'V~,s+1 N!n N-=r,D7 f--a--t r,O  aEJ N:.s+l ~ spine(r), nil E adj(N:,.+1) Nf,s+1 E spine(ß), nil E adj(Nf.0 +1) N:.s+l ~ spine(7), nil E adj(N;.+1 ) N:,s+l E spine(ß), nil E adj (Nf..+1) NJ,0 --+ a  [ACALL-a] [ACALL-b] [ARET]  'Vl,s f----t [ 'Vl,s' t:,::.• ~ 6.l,s f----t f:>.l,s 'Vl,s> [T~ f----t \7'rr,s+l  adj(N:,.+1) # {nil} ß E adj(N:,s+1) ß E adj(N:,s+i)  [FCALL-a] 'Vj,o f----t [ 'Vj,o' ..l N:,.+i [FCALL-b] l:>.l,. ..l f----t  Nßf,O -- Fß  [FRET]  l\T;s+l 'Vj,o, [  f----t 'Yj,1 Nßf,O -- Fß ' ß E adj (NJ.,+1)  [FINAL] $0 [ 'Vg,l f----t [ $f  aEI  Figure 1: Genelic compilation schema from TAG to EPDA  'Y  Figure 2: Meaning of compilation rules  22 Miguel A. Alonso, Brie de Ja CJergerie & Manuel Vilares  Transition SWAP PUSH POP WRA.P-A WRA.P-B UNWRA.P WRA.P-B+PUSH WRA.P-B+POP  EPDA C HF C H CF CFHG CHC,[F C H [C, F c,[FH G CH [c, x F XC H [C, F  L-LIA C [oo] H F[oo] C[oo] H F [ooCJ F[ooCJ H G(oo] C(oo] H C [oo] F [] C [oo] H C( J F[oo] C[oo] F [ JH G[oo] C[oo] H C[ ] F[ooX] C[ooXJ H C[] F[oo]  Figure 3: Equivalence between EPDA and L-LIA is reached, the adjunction node is stored on the top of the stack ([ACALL-a]) and the traversal of the auxiliary tree is started ([ACALL-b]); the adjunction stack is propagated through the spine ([SCALL]) down to the foot node, where the traversal of the auxiliary tree is suspended to resume the traversal of the subtree rooted by the adjunction node ([FCALL-a]), which is eliminated of the stack ([FCALL-b]). To avoid confusion, we store D.7,s instead of \ll,s to indicate that an adjunction was started at node N:,s+i· A symbol D. can be seen as a symbol \l waiting an adjunction tobe completed. 4. EPDA and Left~oriented Linear Indexed Automata Left-oriented Linear Indexed Automata (L-LIA) is a class of automata defined by Nederhof (1999) that can be used to implement parsing strategies for TAG in which adjunctions are recognized in a top-down way. Given a EPDA, the equivalent L-LIA is obtained by means of a simple change in the notations: if we consider the top element of a stack as a stack symbol, and the rest of the stack as the indices !ist associated to them, we obtain the correspondence shown in figure 3. This change in notation is also useful to show that EPDA accept exactly the class of tree adjoining languages. That tree adjoining languages are accepted by EPDA is shown by the compilation schema defined previously. To prove that the languages accepted by EPDA are tree adjoined languages, we exhibit a procedure that, given an EPDA A = (VT, Vs, 8, $0, $1) , builds a linear indexed gramrnar (Gazdar, 1987) Q = (VT, VN, Vi ,S, P) that recognizes the language accepted by A. Non-terminals in VN are pairs (A, B), where A, B E Vs, and Vi = Vs. Productions in P e are obtained from transitions in as follows: • For each transition C H F and for each E E Vs, a production (C, E)[oo) -t a (F, E)[oo] is created. • For each transition C H CF and for each E E Vs, a production (C, E)[oo) -t a (F, E)[ooC) is created. • For each transition C F H G and for each E E Vs, a production (F, E) [ooC) -t a (G, E) [ooJ is created.  23 A redetinition ofEmbedded Push-Down Automata  • For each pair of transitions C ~ C, [F' and C, [F ~ G, and for each E E Vs. a  production  (C,  E)[oo]  ~  b  (F  
This paper describes the extension ofthe system DyALog to compile tabular parsersfrom Feature Tree Adjoining Grammars. The compilation process uses intermediary 2-stack automata to encode various parsing strategies and a dynamic programming interpretation to break automata derivations into tabulable fragments. Introduction This paper describes the extension of the system DyALog in order to produce tabular parsers for Tree Adjoining Grammars [TAGs] and focuses on some practical aspects encountered during the process. By tabulation, we mean that traces of (sub)computations, called items, are tabulated in order to provide computation sharing and loop detection (as done in Chart Parsers). The system DyALog1 handles logic programs and grammars (DCG). It has two main components, namely an abstract machine that implements a generic fix-point algorithm with subsumption checking on objects, and a bootstrapped compi!er. The compilation process first compiles a grammar into a Push-Down Automaton [PDA} that encodes the steps of a parsing strategy. PDAs are then evaluated using a Dynamic Programming [DP] interpretation that specifies how to break the PDA derivations into elementary tabulable fragments, how to represent, in an optimal way, these fragments by items, and how to combine items and transitions to retrieve all PDA derivations. Following this DP interpretation, the transitions of the PDAs are analyzed at compile time to emit application code as weil as to build code for the skeletons of items and transitions that may be needed at run-time. Recently, (Vi!lemonte de Ja Clergerie & Alonso Pardo, 1998) has presented a variant of2-stack automata [2SA] and presented a DP interpretation for them. These 2SAs allow the encoding of many parsing strategies fo r TAGs, ranging from pure bottom-up ones to valid-prefix top~down ones. For all strategies, the DP interpretation ensures worst-case complexities in ti me O(n6) and space O(n5), where n denotes the length ofthe input string. This theoretical work has been implemented in DyALog with minimum effort. Only a few flies have been added to the DyALog compiler and no modification was necessary in the DyALog machine. Several extensions and optimizations were added: handling of Feature TAGs, available at http: / / atoll. inria. fr / -clerger  28  Miguel A. Alonso, Djame Seddah, & Brie Villemonte de Ja CJergerie  use of more sophisticated parsing strategies, use of meta-transitions to compact sequences of transitions, use of more efficient items, and possibility to escape to logic predicates.  2. Tree Adjoining Grammars We assume the reader tobe familiar with TAGs (Joshi, 1987) and with the basic notions in Logic Programming (substitution, unification, subsumption, ... ). Let us just recall that Feature TAGs are TAGs where a pair of first-order arguments top T„ and bottom B„ may be attached to each node v Jabeled by a non-terminal. We have chosen a Prolog-like linear representation of trees. For instance, the grammar count (Fig. J) recognizes the language anbnecndn with n > 0 and returns the number n of performed adjunctions. lt corresponds (omitting the top and bottom arguments) to the trees on the right side. By default, the nodes are adjoinable, except when they are leaves or are prefixed with -. Obligatory Adjunction [OA] nodes are prefixed with ++ and foot nodes by *· Node arguments are introduced with the operators at, and, top, and bot and escapes to Prolog are enclosed with {} (as done in DCGs).  tree top=s(X) and bot=s(O) at ++s("e"). auxtree top =s ( XpI)  ++S l  at -s("a",  "e"  top=s(X) and bot=s ( Y)  at s("b", bot=s ( Y) at H. "c"),  {XpI is X+l},  "d").  -S /l~ a S "d" / l "-.. "b" *s "c"  Figure 1: Concrete representation of grammar count and corresponding trees  3. Compiling into 2SAs following a modulated Call/Return Strategy 2SAs (Becker, 1994) are extensions of PDAs working on a pair of stacks and having the power of a Turing Machines. We restrict them by considering asymmetric Stacks, one being the Master Stack MS where most of the work is done and the other being the Auxiliary Stack AS only used for restricted "bookkeeping" (Villemonte de Ja Clergerie & Alonso Pardo, 1998). When parsing TAGs, MS is used to save information about the different elementary tree tra\.;rsals that are under way while AS saves information about adjunctions, as suggested in figure 2.  Calls  Returns  n rq fI7'l l"V] ~ 1 ·v 1 1 1 transition ACALL  1fT·vv"lII~1~ Pl n transition ARET  r"fl  rvnJ 
Previous stochastic approaches to sentence realization da not include a tree-based representation ofsyntax. While this may be adequate or even advantageous for some applications, other applications profitfrom using as much syntactic knowledge as is available, leaving to a stochastic model only those issues that are not determined by the grammar. In this paper, we present three results in the context ofsurface realization: a stochastic tree model derivedfrom a parsed corpus outperforms a tree model derivedfrom unannotated corpus; exploiting a hand-crafted grammar in conjunction with a tree model outpe1fonns a tree model without a grammar; and exploiting a tree model in conjunction with a linear language model outperforms just the tree model. 1. Introduction Most sentence realizers (systems that take a fairly shallow semantic or lexico-syntactic representation and retum a surface string in the target Janguage) are entirely grammar-based, including quite a few based on TAG (starting with (McDonald and Pustejovsky1985)). Generators using hand-crafted grammars are useful for constrained applications, when strict control over the output is needed, and when a sufficiently !arge grammar is available. Recently, (Langkilde and Knight1998a) and (1998b) have used stochastic techniques in NLG, by mapping semantic primitives to a set of possible ordered sequences of tokens, and assembling theses into a Jattice. They then use a linear Janguage model to select the best path through the lattice. Stochastic generators are useful when a large grammar is not available, or when the range of generated utterances is Jarge. To date, generators are either fully hand-crafted or entirely syntax-free, and use a stochastic model only at the level of linear strings. In this paper we present FERGUS (Flexible Empiricist/Rationalist Generation Using Syntax). FERGUS follows Knight and Langkilde's seminal work in using an n-gram language model, but we augment it with a tree-based stochastk model and a TAG grammar. We argue that the combination of all three key modules of our approach - tree model, TAG grammar, linear model - is crucial and improves over models using only a subset of these modules. The structure of the paper is as follows. In Section 2, we start out by describing a modification to standard TAG that we have followed for the sake of generation. In Section 3, we describe the architecture of the system, and some of the modules. In Section 4 we discussthree experiments. We conclude with a summary of on-going and future work.  34 Bangalore et Rambow  2. 1-Trees We depart from standard TAG practice in our treatment of trees for adjuncts (such as adverbs or adjectives), and instead follow (McDonald and Pustejovskyl985) and (Rambow et al.1995). While in XTAG the elementary tree for an adjunct contains phrase structure that attaches the adjunct to a node in another tree with the specified label (say, VP) from the specified direction (say, from the left), in our system the trees for adjuncts simply express their own phrase and argument structure (active valency), but not how they connect to the lexical item they modify (passive valency). Information about passive valency is kept in the adjunction table which is associated with the grammar. We call trees that can adjoin to other trees (and have entries in the adjunction table) 'Y-trees, the other trees (which can only be substituted into other trees) are a-trees, while (J trees are now restricted to predicative auxiliary trees. Note that each 1 -tree corresponds to a set of predicative auxiliary trees in a traditional TAG grammar (which share common phrase structure but attach differently).  3. System Overview FERGUS is composed of three modules: the stochastic Tree Chooser, the grammar-based Unraveler, and the stochastic Linear Precedence (LP) Chooser. The input to the system is a dependency tree as shown in Figure 1 on the left. Note that the nodes are labeled only with lexemes, not with supertags. The Tree Chooser then uses a stochastic tree model to choose TAG trees for the nodes in the input structure. This step can be seen as analogous to supertagging (Bangalore and Joshil999), except that now supertags (i.e., names of trees) must be found for words in a tree rather than for words in a linear sequence. The Unraveler then uses the XTAG grammar (XTAG-Group1999) to produce a Jattice of all possible Jinearizations that are compatible with the supertagged tree and the XTAG grammar. The LP Chooser then chooses the most likely traversal of this Jattice, given a language model. We discuss the input representation and the three components in turn. For a more detailed overview over the system, see (Bangalore and Rambow2000b).  estimate ~ there was no cost for  estimate ~  there was no cost for  A_NXN G_Vvx G..Dnx G_Nn  
In this paper we present LEXIK. a /ool which allows lo mainlain and gather data on wide coverage grammars based on the Xf'AG format. We present the tool, show how it is used within the FTAG project (Abei/le & al. 2000a). and compare it lo similar work done on the Xtag grammar for English (Sarkar & Wintner 99). Jntroduction Over the past ten years, FTAG, a wide coverage LTAG has been developed at Talana, building up on the work of (Abeille, 91). Thanks to the MetaGrammar developed by (Candito "96,99), which allows to generate semi-automatically an LTAG, the number of trees has augmented drastically: from 650 trees for the manually written grammar, we have now reached 5000 elementary trees (cf Abeille & al 99,00). Although this has improved the coverage of the grammar, new maintenance issues have appeared : To remedy this problem, we have developed a tool which we call Lexik. The goal is twofold: Insuring consistency in the grammar ~ Easily extracting information on a !arge scale In the first part of this paper, we review the main characteristic of FTAG and present the oblems encountered for maintaining and updating the Grarnmar. In a second part, we present 'öur tool, as weil as its utility. Especially, we compare it to the work presented in (Sarkar & Wintner 99). Finally, we show how this tool is used in other projects. 1. Main characteristics of FTAG We assume some familiarity with the LTAG formalism. We recall that elementary units of an LTAG are lexicalized constituent trees, which encode all the surface constructions available for a given 'language. Within FTAG, elementary trees respect the following linguistic wellformedness principles: (Kroch and Joshi 1985, Abeille 1991, Frank 1992) : Strict Lexicalization : all elementary trees are anchored by at least one lexical element, the empty string cannot anchor a tree by itself, • Surfacism: .an elementary tree encodes all word order variations, all basic syntactic phenomena (passive, extraction.„) and crossing ofphenomena. Semantic Consistency : no elementary tree is semantically void (this ensures the compositionality ofthe syntactic analysis), Semantic Minimality : elementary trees correspond to no more than one semantic unit Predicate Argument Cooccurrence Principle : the elementary tree is the minimal syntactic structure that includes a leafnode for each realized semantic argument ofthe anchor(s).  42  N. Barrier, S. Barrier, A Kinyon  Semantic minimality and consistency imply that function words appear as co-anchors (cf. Figure 1, the relevant syntactic and semantic units are donner-a (give to) and penser-que (think that)). The elementary trees are combined by substitution or adjunction, and the features of nodes in contact must unify. They thus directly represent all the syntactic rules ofthe language.  s  s  /I~  /r--._  !"". NO 4 V NI ol SP <func>= 1 <~c>=  sujet  objet Prep  NO•l V <func>= sujct 1  {donner]  N2 ~  [penser]  ä <func>•a-01riet  SI' /- c s1• 
The HPSG-to-TAG compilation algorithm proposed in (Kasper et al., 1995) has been the basis of !arge scale experiments in VerbMobil, a speech-to-speech dialouge translation system in the scheduling and travel domain. The results here refer to the English HPSG grammar developed at CSLI. Several non-trivial theoretical problems have been discovered by the practical application of this algorithm. This paper presents these experiments, the main shortcomings of the initial algorithm and some of the solutions we have developed in order to use the resulting compiled LTAG grammar in a real world system. 1. Introduction The LTAG fonnalism is a mathematical tool that has proven to be attractive for the modeling of natural language syntax. In parallel to pure-LTAG grammar developments, some researches have addressed the relation between LTAG and existing fonnalisms both for theoretical and practical reasons. In particular, compiling a LTAG grammar from a HPSG grammar has been proposed by (Kasper, 1992). Such a compilation is interesting for several reasons: • Sharing of resources between the two formalisms, in particular the syntactic lexicon. For instance, since both fonnalisms are lexicalized, the syntactic Jexicon which gives all possible predicative frames for each lemma is very costly to write. • Speed efficiency: The precompilation process allows to identify substructures of the HPSG grammar that are not context-dependent. The extracted partial backbones can be tabulated (chart parsing, memoization) which results in more time efficient systems than a direct HPSG parser/generator. • Capturing dependencies: An LTAG elementary tree directly encodes a full syntactic context by the way of an extended domain of Jocality. Elementary trees are combined in order to realize dependency relations between the syntactic contexts they represent. Thus the construction of a sentence can be obtained very easily just with a dependency tree indicating the elementary trees that are involved and their mutual dependencies. This infonnation is represented only indirectly in an HPSG derivation. • Exploiting HPSG's expressivity as weil as utilizing existing HPSG grammars is interesting for the LTAG community. HPSG grammars usually include the syntax-semantics interface and a semantic level that is ignored in existing LTAG grammars. HPSG grammars also define explicitely all dependency relations (Pollard & Sag, 1994) while LTAG  48 T. Becker and P. Ldpez grammars are limited by a tree structure which is problematic for, e.g. coordination and equi-verbs. Finally, there is a !arge amount of linguistic research which done in the HPSG framework. Moreover, studying how such a compilation can be performed is an opportunity to identify the assets and the limits of the LTAG formalism. Which relations given in a HPSG grammar should be localized in the LTAG elementary trees in order to obtain a grammar that is either linguistically meaningful or computationally efficient? We first recapture the basic principles of the compilation algorithm as described in (Kasper et al., 1995). Then we present the various problems and limits of this initial algorithm and the adaptations that have been necessary for the practical HPSG-to-TAG compilation of a widecoverage grammar. 2. The initial compilation algorithm  OJ ]] S L 1C SHUEBAJD <>  [  [ COMPS 0<>  D [ HEAD-DTR [ S 1L J C [ : ; ; COMPS COMP-DTR ( S [Ij ]  ~, ll]  f HEAD OJ l SUtij l COMPS  0<> J  /  ~ , l "" HEAD SUBJ [ COMPS  [ 0]  Figure 1: HPSG Head-Subj Schema and its representation as a local tree. We assume that the rule schemata in the HPSG grammar only correspond to binary or unary rules. For instance, the Head-Subj-Schema given in figure 1 can be represented by a partial tree. The algorithm presented in (Kasper et al„ 1995) is based on the following mechanisms: • Selection/Reduction process: The features which constrain a possible argument are called Selection Features (SF). Given a binary schema S, if some SF are expressed in S, we say that the daughter which contains these features is the Selection Daughter (SO), the other one is the non-SO. The single daughter of unary rules is the SO. Given the SF of the SO, we say that a schema reduces the SF, if the value of at least one of the features that select the non-SO for this schema is not contained in the feature value of the mother node. In the example figure l, we see that the SF of attribute SUBJ is reduced. • Tree production iteration: The basic algorithm starts with the creation of a node for the lexical type. A root node n is first added to this initial node with a copy of all its features. Theo we instantiate each schema S which actually reduces at least one SF of n when n is unified as the SO of S. Finally, we add an additional root node dominating the instantiated schema. This step is repeated until the termination condition is met (see below). 
fn order to ease the process of engineering such a !arge grammar, we have made use of the exical knowledge representation language DATR (Evans & Gazdar, 1996) to compactly encode the elementary trees (Evans et al., 1995; Smets & Evans, 1998). In Section 5 we present some flgures that show how the size of the encoding of the grammar has increased during the gramm~ ~evelopment process as the number and complexity of elementary trees has grown. yve have addressed problems that result from trying to parse with such a !arge grammar by using ~ technique proposed by (Evans & Weir, 1997) and (Evans & Weir, 1998) in which all the trees that each word can anchor are compactly represented using a collection of finite state automata.  56 Carroll et al. In Section 6 we give some data that shows the extent to which this technique is successful in compacting the grammar. 2. Coverage of the LEXSYS Grammar The LEXSYS grammar has roughly the same coverage as the A1vey NL Tools grammar (Grover et al„ 1993), and adopts the same set of subcategorization frames as in the Alvey lexicon. There are at present 143 families in the grammar. Each family contains the base tree of the family, and definitions of lexical rules which derive trees from the base tree. There are currently 88 lexical rules. Possible rule combinations are determined automatically (see (Smets & Evans, 1998)). There are 7 noun and pronoun families. Tue noun families include trees for bare nouns, for small clauses headed by a noun, for noun-noun modifiers and for coordination. Coordination can be at the N, N or NP Jevels. There are 19 adjective families, distinguished according to the position of the adjective and its subcategorization frames. Trees derived by lexical rules include small clauses headed by an adjective, comparative constructions, trees with unbounded dependencies for adjectives which subcategorize for a complement (wh-questions, relative clauses, topicalization), a tree for tough-movement, and trees for coordination. Numerals also anchor adjective trees. Rules derive from the base tree uses of numerals as pronouns and nouns, and coordination of cardinal numbers (for example, hundred and ten). However, the grammar does not as yet have a complete account of complex numerals. For ordinals, there are rules to derive fractions with complement, fractions without complement, and the use of ordinals as degree specifiers. Adverbs are distinguished according to whether they are complements or modifiers. Modifier trees differ according to the modified category and the relative position of the adverb and its argument. Rules derive coordinated structures headed by adverbs, and also adverb distribution. Long distance dependencies possibly involving adverbs (for example, How did he behave) are handled in the PP modifier famil y.1 The grammar contains an account of constituent and sentential negation (but in the latter disregarding scope issues arising when an adverb comes in between rhe auxiliary and the negation). Specifier families include families for determiners, quantifying pre-determiners and genitive determiners. There is also a family for adjective and adverb specifiers. Prepositions followed by an NP are divided into two families: a family for case-marking prepositions and a family for predicative prepositions. These two types of prepositions differ in their semantic content, and syntactically also: case-marking prepositions do not head PP-modifiers: The case-marking preposition family includes trees for long-distance dependencies with preposition stranding (wh-questions, relative clause, tough-constructions) and trees for coordination. Tue family of predicative prepositions inherits these trees, and also contains trees for adjunc preposition phrases and long-distance dependencies involving adjunct PPs. There are also families for prepositions introducing ss, VPs, PPs and AP. There are two families for complemen~ tizers (introducing an s or a VP). The 94 verb· families constitute the bulk of the grammar. Verb families include trees2 for · gerunds (nominal and verbal), long-distance dependencies (topicalization, relative clause and wh-questions), VP complements, VP complements for tough-constructions, small clauses (headed by a present participle or a passive verb),for-to clauses, extraposition, imperative, passive with or without by, inversion (for auxiliaries and modals), VP-ellipsis (after auxiliaries and modals), dative alternation, movement of particles, and coordination (at V, VP and s). Finally, we have recently extended the grammar to include semantic features capturing predicate 1lt would be redundant also to have such a rule in the adverb family. 10f course, these constructions are not relevant for every single family.  Engineering Lexicalized Grammar  57  argument structures. We have not implemented quantification yet. The grammar adopts a semantic representation inspired by the Minimal Recursion Semantics (MRS) framework (Copestake et al., unpublished). MRS representations are ftat lists of elementary predications, with relations between predications being expressed through coindexation. 3. Localization of Syntactic Dependencies The LEXSYS grammar has been designed to localize syntactic dependencies, not only unbounded dependencies between filler and gap, but agreement relations, case and mode of the clause, etc. (Carroll et al., 1999). One immediate advantage is that there is no need for feature percolation during parsing: all syntactic features are grounded during anchoring. There are, however, a few cases where all syntactic features cannot be localized in the same tree. This happens when the values of syntactic features are determined by more than one consti tuent. This is the case, for example, in raising constructions: the subject raising verb agrees with its syntactic subject but the complement of the raising verb (adjective or verb) determines the category of the subject. In such cases, feature percolation is needed, unless one define trees for all the possible feature combinations. This is what we have done in the grammar, and 9 more trees are needed to that effect. In there-constructions, the NP following the verb (be) determines the agreement of the verb. This does not represent a problem if the dependency is local. However, if a subject rai sing verb comes in between there and the rest of the sentence, agreement cannot be determined locally anymore. We need one more tree to cover both possible instantiations of agreement features. Finally, PP phrases can involve a wh-NP or a rel-NP, t1'11~ !""'~~ !::;- ~p~~i~ec! :is such. Because the head of PPs does not set that feature, feature percolation would be needed between the NP and the root of the PP. In the grammar, we define three PP trees, one for each possible instantiation of that feature. Thus, two more trees are needed than if we had feature percolation. In all the above cases, the specification of all possible feature combinations does not involve the creation of many more trees. However, from a Iinguistic point of view, we do miss generalizations. With coordination, however, the problem is not the loss of linguistic generalizations, but the substantial increase in the number of trees. Indeed, coordination3 trees are anchored by the head of one of the coordinated constituents. The advantage of this is that constraints on the coordination phrase are defined at anchoring. But the disadvantage is that this doubles the number of trees in the grammar: every structure can occur in coordination. 4. Anchored Trees The previous two sections discussed the coverage of the grammar, and how some decisions have increased the number of unanchored trees. Another important property of the grammar is the number of trees that result from anchoring with lexical items. We find that some verbs induce a very !arge number of anchored trees: for example, get results in 2847 trees, put 3465, come 2656, and turn 1425. To illustrate why, consider get. First, get has 17 different subcategorization frames (it can be transitive, ditransitive, it can have a prepositional complement, be followed by one or more particles, etc.). lt therefore belongs to L7 different families, and each family contains a number of trees (for example, the v _pp family, >elected by get, has 33 trees, and the v ...NPJ>Pto family contains 146 trees). v.toreover, when a lexical item anchors a tree, features get grounded, and different feature in;tantiations characterize different trees. For example, get can be followed by one of 12 different )ffpositions which means that there are at least 12 x 33 trees for the single subcategorization 30nly samc constituent coordination has been implemented so far.  58 Carroll et al.  # trees in set 1-10 1 I-20 21-50 51-100 101-200 201-500 501-1000 1001-5000 Totals  # sets 112 83 69 47 68 56 23 16 474  # merged states (mean) 17.9 53.9 133 364 687 1815 3654 10912 927.7  #minimized states (mean) 6.9 13.I 18.1 28.l 33.0 42.8 48.9 60.1 23.5  ratio merged I minimized 2.6 4.1 7.4 13.0 20.8 42.4 74.7 181.5 39.4  Table l: Grammar compaction statistics  frame v _PP. Similarly, there are 16 different particles which can follow get, and this also multiplies the number of trees. Finally, there are other features that get instantiated and are responsible for the creation of new trees, such as agreement features of the anchor, verb form feature of the anchor and of its verbal complement. Thus the different instantiations of features together with the various, subcategorization frames that a word selects explain the very high number of trees anchored by some individual words. 5. Encoding for Grammar Development Following (Evans et al., 1995) and (Smets & Evans, 1998) the LEXSYS grammar is encoded using DATR, a non-monotonic knowledge representation language. In 1998, the grammar contained 620 trees organized into 44 tree families and produced using 35 rules. This grammar was encoded in 2200 DATR statements, giving an average of 3.55 DATR statements per tree. The grammar currently contains around 4000 trees in 143 families produceci with 88 rules. This grammar is encoded with around 53004 DATR statements, giving an average of 1.325 statements per tree. Thus, as the grammar has grown the number of DATR statements needed to encode it has grown, but not as rapidly. 6. Encoding for Parsing Following (Evans & Weir, 1997) and (Evans & Weir, 1998) each elementary tree is encodec as a finite state automaton that specifies an accepting traversal of the tree from anchor to root For each input word, the set of all the alternative trees that can anchor an input word can bc captured in just one such automaton, which can be minimized in the Standard way, and the1 used for parsing. In order to assess the extent to which this technique alleviates the problem of grammar sizt' we produced automata for the words appearing in the 1426 sentences (mean length 5.70 word! forming the Alvey NL Tools grammar development test suite. Bach sentence was processe by a morphological analyser, and the result was then used in conjunction with the Jexicon 1 determine for each word in the sentence the complete set of anchored trees, feature valuc being determined by the morphological analyser or lexicon as appropriate. 474 distinct sets 1 anchored trees ('tree sets') were produced in this way, ranging in size from 1 to 3465 tree The total number of anchored trees was 24198, with a mean of 175.5 trees in each tree sc 4We have excluded from this figure around 700 DATR statements that specify the semantics associated w elementary trees.  59 Engineering Lexicalized Grammar  # occurrences 
tUniversity of Pennsylvania Dept of Computer and Information Science Philade~i~i~ :~~;1~4 USA {dchiang, schuler}©linc. cis. upenn. edu  tUniversity of Pennsylvania Inst for Research in Cognitive Science Suite 400A, 3401 Walnut St Philadelphia PA 19104 USA madras©linc.cis.upenn.edu  .W, eb  stract explore  some  properties  of the  synchronous  formalism  introduced  in  Dras  (1999},  ~~owing ~hat.it handles a~ interaction, noted in Schuler (1999}, betwe~n brid9e and raisin9  '(/erbs whzch zs problematic for synchronous TAG. We also show that it has 9reater formal  power than synchronous TAG and discuss its computational complexity.  Introduction Synchronous TAG (S-TAG) , as defined by Sh1eber (l~!:l4), <leü.„es rP.!ations hetween lan- guages by assembling paired elementary structures into isomorphic derivations. This isofüorphism requirement is formally and computationally attractive, but for practical appli- <:;ations somewhat too strict. For this reason, Shieber suggests relaxing this requirement by treating bounded subderivations as elementary, but there are a few cases which remain problematic because t hey involve unbounded non-isomorphisms. One such case is described by Schuler (1999). If a predicate is analyzed as a VP-adju nct in one language but an S-adjunct in another, then an unbounded non-isomorphism will arise \Vhen this predicate interacts with other VP-adjuncts. Consider the following sentences from English and Portuguese:  (1) X is supposed to (be going to .. .) have to fiy. (2) E pressuposto que X (vai . ..) tem/ter que voar.  we might analyze these sentences with the trees in Figure 1, but the resulting derivations for (1) and (2) would be non-isomorphic (see Figure 2). Shieber (1994) describes this situation as "elimination of dominance"; in this case the non-isomorphism is potentially unbounded because the tree for supposed to adjoins into the lowest VP-adjunct on the derivation tree in English, but into the highest t ree (that is, the initial tree) in Portuguese. chuler (1999) describes a solution to this problem based on a compositional semantics for TAG (Joshi & Vijay-Shanker, 1999) which relies on a mapping of contiguous ranges of scope in source and target derivations, but because it does not map subderivations in the source to subderivations in the target, this solution can only be used on individual "This research is partially supported by ARO AASERT grant N00014-97-l-0603, ARO grant DAAG55971-0228, and NSF grant SBR-89-20230-15.  David Chiang, William Schuler, Marle Dra.S 62  VP  ~  V  VP  
Abstract We present a bottom-up bidirectional parserfor Tree Adjoi11i11g Grammars that is an extension bf the parser defmed by De Vreught and Honig for Context Free Grammars. Although this parser does not improve the comp/exity ofthe parsers de.fined in the literature, it presems several }!iaracteristics rhat can be of imerestfor practical parsing ofnatural languages. Introduction , Several algorithms have been proposed for parsing tree adioining grämmars (TAGs), most of them derived from context-free tabular parsers, ranging from simple bottom-up aigorithms, like CYK, to sophisticated extensions of Earley's algorithm (Alonso et al„ 1999). However, ,~pme of the bidirectional parsers proposed are not applicable in all the cases. Lavelli and Satta parser (1991) is restricted to elementary trees with only one anchor. Van Noord parser (1994) lJitroduces several improvements to Lavelli and Satta parser: the substitution operation, the f?ot-driven recognition of auxiliary trees and the notion of headed elementary trees in order to iake advantage of lexicalization. .1\ccording to Van Noord, a headed TAG is a TAG in which each elementary tree is a headed tree. for each intemal node in a headed tree, there must be a daughter which is the head of the subtree footed in that node. The reflexive and transitive closure of the head relation is called the head:omer relation. In order to establish the head-com er relation we must fulfill the following two §pnstraints: (i) the anchor of an initial tree must be a head-comer of the root node of the initial _tree and (ii) the foot node of an auxiliary tree must be head-comer of the root of the auxiliary ttee. Since there exists the notion of anchor in the context of lexicalized TAG, it seems that the notion of head, as defined by Van Noord, is redundant. Moreover, in the case of anchor siblings ~ the definition of head requires to select only one anchor as the head. In this paper we present a bidirectional bottom-up parser for TAG, called dVH, derived from the context-free parser defined by de Vreught and Honig (de Vreught & Honig, 1989;' Sikkel, J997), which presents several interesting characteristics: (i) the bidirectional strategy allows üs to implemen t the recognition of the adjoining operation in a simple way, (ii) the bottom- ilp behavior allows us to take advantage of Iexicalization, reducing the number of trees under consideration during the parsing process, (iii) in the case of ungrammatical input sentences, the :, parser is able to recover most of partial parsings according to lexical entries, and (iv) the parser 'can be applied to every kind of anchored elementary trees without introducing the notion of ~ head  68  Victor 1. Dfaz, Miguel A. Alonso & Vicente Canillo  1.1. Notation Let G = (Vr, VN, S, I, A) be a TAG where VT and VN are the terminal and non-tenninal alpha- v,.,. bets, S E the axiom symbol, and I and A the set of initial and auxiliary trees respectively. As usual , I U A consist of the set of elementary trees. Parsing algorithms for context-free grammars usually denote partial recognition of productions by dotted productions. We can extend this approach to the case of TAG by considering each elementary tree 'Y as fonned by a set of context-free productions 'P (r): a node N"I in 'Y and its g children Ni ... NJ are represented by a production N"I -+ Nl ... NJ. The elements of the productions are the nodes of the tree, except for the case of elements belonging to VTU{t:} in the right-hand side of productions. Those elements may not have children and are not candidates tobe adjunction nodes, so we identify such nodes labeled by a terminal or t: with the Jabel1. We use ß E aclj(N'Y) to denote that ß E A may be adjoined at node N"I. If adjunction is not mandatory at N'Y, then nil E adj(N'Y). With respect to substitution, we use a E sub(M'Y) to denote that a E I can be substituted at node M"I. To simplify the description of parsing algorithms we consider additional productions: T -t R 0 , T -t R ß and F ß -+ .l for each a E I and each ß E A, where R"' is the root node of o: and R ß and F ß are the root node and foot node of ß, respectively. After disabling T and .l as adjunction nodes the generative capability of the grammars remains intact. 2. The parser dVH The definition of the parser is based on deductive systems similar to Parsing Schemata (Sikkel, 1997). Given the input string w = a1 ... an with n ~ 0 and a TAG grammar, the formulas (called items in this context) in the deductive system will be of the form: [N"I -t II . 0. w, i, j, p , g]  where N "I -t vow E P(r) is a production decorated with two dots indicating the part of the  subtree dominated by N 'Y that has been recognized. When 11 and w are both empty, the whole  subtree has been recognized. The two indices i and j denote the substring of w spanned by ö.  If "I E A, p and g are two indices with respect to w indicating the substring spanned by the foot  = - , node of 'Y· In other case p = g  representing they are undefined.  With respect to deduction steps, we have that  ,,..,  _ '1"'1!ni U 'T'IE U 'T'llnc U 'T'>Conc U 'T'IFoot U ,,.., Adj U 'T'>Subs  vdVH - v dVH vdVH vdV H vdVH vdVH vdV H vdVH  The initializer steps deduce those items associated to productions whose right hand side includes a terminal that matches with an input symbol. The position of the terminal in the input string determines the values of the indices. Empty-productions are considered to match any position in the input string. Tue indices associated to the foot node in the consequent of both deduction steps are undefined since no foot has been recognized yet:  = vlni  a = aj  dVH [N'Y -t V. a . w , j - 1, j , -,-]  v~ - - - - -- - - dvH - [N'Y-+ ••, j,j, -, -]  Once the subtree dominated by a node Af'Y has been recognized completely, a include step in V~rz7H continues the bottom-up recognition of the supertree dominated by Af'Y when no adjoining  1Without lost of generality, we assume that if a node is labeled by E then it has no siblings.  Bidirectional parsing of TAG without heads 
Just as people make use ofinformationfrom punctuation to structure and understand text, NLP systems can use informationfrom punctuation in processing texts automatically. The aim ofthe research presented here was to explore the feasibility of treating a sizable core ofpunctuation phenomena at the level ofthe sentence gramma1: A /arge set ofpunctuation rules were manually derivedfrom naturally occurring data, and added to the XTAG English grammar. Our results confirm that punctuation can be used in ana/yzing sentences to increase the coverage of the grammm; reduce the ambiguity of certain word sequences and facilitate later processing of /arger text units, without either adverse/y impacting the existing grammar or deriving analyses which would be incompatible with later leve/s ofprocessing. 1. Motivation Punctuation helps us to stru cture, and thus to understand, texts. Many uses of punctuation straddle the line between syntax and discourse, because they serve to combine multiple propositions within a single orthographic sentence. They allow us to insert discourse-level relations at the level of a single sentence. Just as people make use of information from punctuation in processing what they read, natural language processing systems can use information from punctuation in processing texts automaticatly. Most current NLP systems fail to take punctuation into account at all, losing a valuable source of information about the text. Those which do mostly do so in a superficial way, again failing to fully exploit the infonnation conveyed by punctuation . To be able to make use of such information in a computational system, we must first characterize its uses and find a suitable representation for encoding them. Previous work on punctuation was mostly of the descriptive variety, of which Quirk et al. ( 1985) and Sampson (1995) are particularly good instances. Some linguistic work has been done by Chafe (1988), Schmidt (1995), Jones (1996b) and Meyer (1987). Nunberg (1990) offers the most comprehensive linguistic discussion of punctuation to date, with an extensive analysis of the interactions of different punctuation marks. He is primarily interested in characterizing punctu ation as a formal system, independent from syntax. Briscoe ( 1994) presents an treatment of punctuation within the Al vey Natural Language Tools grammar. He and Carroll ( 1995) show that this analysis considerably reduces ambiguity in parsing the SUSANNE corpus (a subset of the Brown corpus) and Jones shows similar results. The work discussed here differs from previous work in a number of ways. lt includes an analysis of the syntax of punctuation which has been implemented and integrated into a )arge Engl ish This work was done while the author was a graduale student at the University of Pennsylvania, and was parlially supported by NSF Grant SBR8920230 and ARO Grant DAAH0404-94-G-0426. Thanks to Aravind Joshi and Ted Briscoe for their helpful comments at all stages of this work, and lo the members of the XTAG Project.  74  C. Doran  grammar that is being used on an everyday basis. In addition, the analysis differs considerably from those of Jones and Briscoe in treating punctuation within a framework which allows for more concise characterization of the non-local aspects of certain uses of punctuation. Furthermore, neither of their implementations cover the range of punctuated constructions our treatment does. 2. Analysis Many parsers require that punctuation be stripped out of the input. Where punctuation is optional, as is often the case, this may have no effect. However, there are a number of constructions where punctuation is obligatory. Adding analyses of these to the grammar without the punctuation can lead to severe over-generation, possibly to the point where it is better to not add the constructions at all. The work here focuses on extending a lexicalized syntactic grammar to handle phenomena occurring within a si ngle sentence which have punctuation as an integral component. The main job of the sentence grammar, then, is to produce a structure that makes the appropriate units easily accessible to later levels of processing-not just basic grammatical elements like subject noun and verb group, but more complex relations like nominal apposition as well. Punctuation marks are treated as full-fledged lexical items in a Feature-Based Lexicalized Tree Adjoining Grammar (Joshi, 1985; Schabes, 1990; Vijay-Shanker & Joshi, 1991 ). The locali zation of both syntactic and semantic dependencies provides an e legant framework for encoding punctuation in the sentence grammar. The elementary units of LTAG are of a suitable size for stating most of the constraints we are interested in, and the derivation histories it produces contain information that Jater stages of processing will need about wh;„ h „1„rn„nt„„~ nnits have been used and how they have been combined. Each punctuation mark or pair of marks anchors Jl~ vv.„ w;;;"'~::~~ry trees and imposes constrai nts on the surrounding lexical items. The TAG adjunction operation is advantageous in handling paired punctuation marks, because it allows us to keep both pieces of the complex object, e.g. a pair of parentheses or commas, in the same elementary tree, regardless of the size of the constituent they endose. We have analyzed naturally-occurring data (pri marily from the Brown Corpus) representing a wide variety of constructions, and added treatments of them to the XTAG English grammar. The new trees are of two types. The first have the punctuation marks as anchors, reflecting the fact that they do not strongly constrain the lexical content of the constructions they participate in. For example, any NP except a pronoun can be an appositive, and this is reftected in the analysis by having the NP position as a substitution site in the NP appositive tree (Figure l). The seconä type of tree has the punctuation marks as substitution sites, for instance the tree for parenthetical adverbs, where the lexical material may vary, some punctuation mark is required, but any of several types of punctuation mark is permissible. This is illustrated by the tree for a quoting dause shown in Figure 2. There are a total of 47 trees containing punctuation marks in the current implementation. Doran (1998) discusses all of the trees in more detail. The full set of punctuation marks is divided into three dasses: balanced, structural (temi from (Meyer, 1987)) and terminal. The balanced punctuation marks are quotes and parentheses, structural are commas, dashes, semi-colons and colons, and terminal are periods, exclamation points and question marks. These three types of punctuation are essentiall y independent subsystems, and a given constituent will typically have only one of each type. Structural and terminal punctuation marks do not occur adjacent to other members of the same dass, but may occasionally occur adjacent to members of the other dass, e.g. a question mark on a clause which is separated by a dash from a second dause. Balanced punctuation marks are sometimes adjacent to one another, e.g. quotes immediately inside ofparentheses as in example  Punctuation in a Lexicalized Grammar  75  NPr*  Punct NPJ. [pron : -  J  case : nom/acc  Figure 1: The non-peripheral NP appositive tree, showing relevant features.  s,  ~  Punct!  s  / \ NA  NP.L  VP  vo  Figure 2: Tree for a quoting clause which follows the quote; the tree would be anchored by e.g. mutter in a sentence such as Liver again, Mal}' muttered.  (1 ). Features allow us to control these Jocal interactions. We also use these features to control non-Jocal phenomenon such as quote alternation, whereby single and double quotes altemate when embedded, and also to control the embedding of colons and semi-colons. (1) Each enjoys seeing the other hit home runs ("I hope Roger hits 80", Mantle says), and each enjoys even more seeing himself hit home runs ("and I hope I hit 81 "). [Brown:ca39] 2.1. How punctuation improves the grammar There are ·two primary ways in which adding punctuation improved the coverage and performance of the XTAG English grammar. First, it allowed us to add some syntactically "exotic" constructions which would have previously been considered too unconstrained in their unpunctuated forrns. Many such constructions occur with great frequency in naturally occurring texts. As an example, consider noun appositives, where an NP modifies another NP. Example (2) has two appositives. Without access to punctuation, the parser would derive every combinatorial possibility of NPs in noun sequences, which is obviously undesirable (especially since 'there is already unavoidable noun-noun compounding ambiguity). These phrases must be "bracketed" by punctuation, which provides precisely the sort of additional constraint we need to make the parsing task manageable. By adding a treatment of punctuation to the grammar, we can recognize and correctly analyze appositive constituents. Other similar such constructions include parenthetical elements, reported speech, compound sentences, comma coordination and vocatives. None ofthese constructions were handled by our English grammar before it was extended to treat punctuation.  C. Doran  (2) But Tony Robinson, the current slieriffofNo1tingham - a job that really exists -  rejected the theory, saying that "as far as we are concerned, Robin Hood was a  Nottinghamshire lad."  [cl ari.living.cele brities]  Second, punctuation provides additional constraints for parsing constructions already handled  by the grammar. In developing a !arge grammar for any language, one of the fundamental  concerns is the increase in ambiguity of derivations which invariably accompanies any increase 
This paper points out some comr"ntinnal i11<>(fi„;„„{'ies nf .~tandard TAG parsing algorithms when applied to LTAGs. We propose a novel algorifhm with an asymptotic tmj>tvven;c;;;;, Introduction Lexicalized Tree-Adjoining Grammars (LTAGs) were first introduced in (Schabes et a/„ 1988) as a variant ofTree-Adjoining Grammars (TAGs) (Joshi, 1987). In LTAGs each elementary tree i,s specialized for some individual lexical item. Following the original proposal, LTAGs have been used in several state-of-the-art, real-world parsers; see for instance (Abeille & Candito, ~000) and (Doran et al., 2000). Like link grammar (Sleator & Temperley, 1991) and lexicalized formalisms from the statistical parsing literature (Collins, 1997; Charniak, 1997; Alshawi, 1996; Eisner, 1996) LTAGs provide two main recognized advantages over more standard non-lexicalized formalisms: • subcategorization can be specified separately for each word; and • each word can restrict the anchors (head words) of its arguments and adjuncta, c1~-:--:-ding lexical preferences as weil as some effects of semantics and world knowledge. To give a simple example, consider the verb walk, which is usually intransitive but can take an object in some restricted cases. An LTAG can easily specify the acceptability of sentence Mary wa/ks the dog by associating walk with a transitive elementary tree that selects for an indirect object tree anchored at word dog (and some other words within a limited range). LTAGs are )arge because they include separate trees for each word in the vocabulary. However, garsing need consider only those trees of the grammar that are associated with the lexical syrn-  J. Eisner, G. Satta ~g. While this strategy reduces parsing time in all practical cases, since the t6h~h tends to be considerably smaller than the grammar size, it also introduces . 'öfiäff"actor in the runtime that depends on the input string length. This problem was e~pgrili~ early in (Schabes et al., 1988), but a precise computational analysis has not been jjfävided in the literature, up to the authors' knowledge. See (Eisner, 1997; Eisner, 2000) for reiated discussion on different lexicalized forrnalisms. In this work we reconsider LTAG parsing in the above perspective. Under standard assumptions, we show that existing LTAG parsing methods perform with 0(tg2 lwl8 ) worst case running time, where t and g are smallish constants depending on the grammar and w is the input string. As our main result we present an O(tg lwl6 max{g, lwl}) time algorithm, a considerable improvement over the standard LTAG parsing methods. 1. The problem We assume the reader is familiar with TAGs, LTAGs and related notions (Joshi, 1987; Joshi & Schabes, 1992). Each node n in an elementary tree is associated with a selectional constraint Adj(n) representing a (possibly empty) set of elementary trees that can be adjoined at n (we treat substitution as adjunction at a childless node). We define the size ofn as 1 + IAdj(n)I. The size of an LTAG G, written [GI, is the sum ofthe sizes of all nodes in the eleinentary trees ofG. Standard parsing algorithrns. for TAGs have running time O(IGI lwl5), where G is the input grammar and w is the input string. As already mentioned in the introduction, LTAGs allow more selective parsing strategies, resulting in O(j(G, w) jwj5 ) running time,~'.)!' :::::ne fünction J(G,w) that is independent ofthe size ofthe vocabulary treated by G (hence typically much less than IG!). In order to give an estimate ofthe factor f (G, w), !et us define t as the maximum number of nodes in an e!ementary tree (of G), and g as the maximum number of eleme~tary trees that are anchored in a common lexical item. We argue below that J(G, w) is O(tg2 lwl2) . We need to introduce some additional notation. We write w;,j to denote the substring ofw from position i to position j, 0 S i S j S lwl. (Position i is the boundary between the i -th and the (i + 1)-th symbols of w.) We write wi for w ;-i,i· In the grammar, assume some arbitrary ordering for the elementary trees with a given anchor and for the nodes ofeach elementary tree, with the root node always being the first. Then (h, k) denotes the k-th elementary tree anchored at Wh, (h, k, 1) denotes its root node, and (h, k, m } denotes its m-th node (for 1 S h S lwl, 
(3) Prefer Merge over Move By (3), we are forced to merge there into the specifier of the TP in (2), rather than moving a tcnicom: When we reach the matrix clause, however, the fact that no additional lexical items remain to merge forces us to employ the more costly move Operation. (Note that the presence or absence of there in these examples is, for Chomsky, detennined prior to the onset of the derivation. Further, on Chomsky's theory structures with distinct numerations are not compared for economy. See Chomsky (1995; 1998) for further discussion.) • Thanks to Colin Wilson, and Paul Hagstrom for helpful discussion, to two a.nonymous rcviewers for comments, and to thc National Scicnce Foundation for their monetary support in thc fonn of grant SBR-9710247.  86  Robert Frank  2. Eliminating the need for economy with TAG What becomes of the contrast in (1) in a TAG context? Under the assumptions of Frank (1992; to appear) conccming elementary trees, example (la) derives from the adjoining of the seems- headed auxiliary in (4a) to the T node of the tree in (4b).  (4) a. 'f  ~  T  VP  /""'-,.__ V T  
In Korean, a class of lexemes of Chinese origin exhibit both nominal and verbal behavior. Specifically, they can assign lexically idiosyncratic case, but require a semantically vacuous light verb in order to forma sentence and are themselves marked with accusative case. In this paper, we propose a TAG-based account of this behavior, and propose some generalizations towards a pure representation oflexical argument structure. 1. Linguistic Facts and Issues In this paper, we provide a syntactic analysis of Sino-Korean light verb constructions (LVC henceforth) that are composed of the light verb ha and an activity-denoting noun of Chinese origin.1 We will refer to this activity-denoting noun as the 'base' of the LVC. The argument structure of LVCs come from the base, and the light verb is sern„ntir-..11~, "'.'.::::::~::: ::::::! ,:!.:,.;.:; ::-::-~ assign any theta roles. This is shown by the fact that although the examples in (1) all contain ha, they have different argument structures. (1) a. John-i swuhak-ul yenkwu-lul ha-yess-ta. John-Nom math-Acc research-Acc HA-Past-Decl 'John researched math.' b. Kicha-ka Seoulyek-ey tochak-ul ha-yess-ta. train-Nom Seoul-station-at arrival-Acc HA-Past-Decl 'The train arrived at Seoul station.' c. Kicha-ka Seoulyek-eyse chwulpal-ul ha-yess-ta. train-Nom Seoul-station-from departure-Acc HA-Past-Decl 'The departed from Seoul station.' For instance, the arguments in (Ja) are agent and goal, those in (lb) are patient and goal, and those in (Je) are patient and source. 1f, however, the theta roles in LVCs are assigned by the base, it is puzzling why the argument NPs are syntactically realized outside of the base NP. The case postpositions such as Ace, -ey and -eyse on the argument NPs indicate that they are daughters of VP, and not the base NP. An 1Han has been partially funded by the Army Research Lab via a subcontract from CoGenTex , Inc., and by NSF Grant SBR 8920230. We would like to thank Aravind Joshi, Tony Kroch, Martha Palmer, and Anoop Sarkar for u sefu 1 discussions.  Chung-hye Han and Owen Rambow  .-„~1- .s„0•,;<'··n~•,•h.or NP requires genitive or null case postposition in Korean. We  o.the  as VERBAL CASE, and the second as NOMINAL CASE.  , ver/1s noted by (Grhnshaw & Mester, 1988), there are restrictions on argument realiza-  ti'Brt·wfiictl'can be clearly shown with ditransitive LVCs, as in (2). ~ <··-·  (2) a. John-i Mary-eykey inhyung-ul senmwul-ul ha-yess-ta. John-Nom Mary-to doll-Ace gift-Acc HA-Past-Decl  'John gave a gift of a doll to Mary.'  b. John-i Mary-eykey inhyung(-uy) senmwul-ul ha-yess-ta. John-Nom Mary-to doll(-Gen) gift-Acc HA-Past-Decl  c. * John-i inhyung-ul Mary-eykey-uy senmwul-ul ha-yess-ta. John-Nom doll-Ace Mary-to-Gen gift-Acc HA-Past-Decl  d. * John-i Mary-eykey-uy inhyung(-Gen) senmwul-ul ha-yess-ta. John-Nom Mary-to-Gen doll(-Gen) gift-Acc HA-Past-Decl  The base senmwul ('gift') assigns agent, goal and theme. In (2a), all the argument NPs are realized outside of the base NP. In (2b), the agent and goal arguments are realized outside of the base NP, but the theme argument is realized inside the base. However, it is not possible to realize theme argument outside of the base when the goal argument is realized inside the base/ as shown in (2c), and it is not possible to realize both theme and goal arguments inside the base, as shown in (2d). (Grimshaw & Mester, 1988) (G&M henceforth) summarize the restrictions on argument reaJization as follows: (i) the subject argument must always be outside the base NP; (ii) at Ieast one argument apart from the subject rnust be outside the base NP; and (iii) for nouns that take a theme and a goal, if the theme argument is realized outside the base NP, the goal must also be realized outside the base NP. In what follows, we first briefly discuss some previous analyses and their shortcomings, and present our own analysis using the framework of Feature Based Lexicalized Tree Adjoining Grammar. We discuss English data in comparison, and conclude with a discussion of noun phrases. 2. Previous Analyses According to G&M, a light verb such as ha has no argument structure on its own and it occurs with a noun which is 'theta-transparent.' Theta-transparent nouns can transfer some or all o their arguments to the argument structure of the light verb. This mechanism alJ ows the light verb to directly assign theta roles to the argument NPs in syntax and such argument NPs are realized outside the base NP. They further assume (following much previous work) that arguments have a hierarchy according to prominence. For instance, the agent is more prominent than the goal, which is more prominent than the theme. Based on this assumption, they propose that when a theta role is transfered (e.g„ the theme), any theta roles that are higher in prominence must transfer as well (i.e, the agent and goal). This explains the ungrammaticality of (2c). G&M also stipulate that the base noun must transfer at least one internal argument in order to be licensed. Otherwise, the theta-criterion is violated, since the base noun does not receive a theta role from anywhere. This is why (2d) is ungrammatical under G&M's system. G&M wrongly predict that intransitive LVCs do not exist, since there is no internal argument to participate in the transfer. But intransitive LVCs clearly do exist, as shown in (3) and (4). Note that while (4) may be ambiguous between a heavy and light verb reading of ha, (3) is not, since the subject is not an agent.  •[.,ight Verbs and Lexical Argument Structure  95  (3) John-i samang-ul ha-yess-ta. John-Nom death-Acc HA-Past-Decl 'John died.'  (4) John-i swuyuong-ul ha-yess-ta. John-Nom swimming-Acc HA-Past-Decl 'John was swimming.'  For this reason, (Yoon. 1991) rejects G&M's argument transfer theory and proposes 'argument sharing' mechanism. He argues that the light verb is thematically underspecified and so unsaturated. This forces the base noun which has theta structure and the light verb to undergo the operation of Theta Identification, allowing the argument structure of the base noun and that of the light verb to be shared. This sharing is viewed as the unification of the argument structure of the base noun into the underspecified argument strncture of the light verb. Yoon's theory predicts that when there are more than one internal arguments, they must all be realized outside of the base NP. But this is an incorrect prediction: in ditransitive LVCs, while the goal argument is realized outside the NP, the theme argument can be realized inside NP, as shown in (2b). The same problem persists in (Park, 1992). He argues that the categorial status of the base is not a noun, but a verb. Thus, it assigns theta-roles just as any other verbs. The light verb is simply an auxiliary verb that supports intlection. But if the base is simply a verb, then (2b) is wrongly predicted tobe ungrammatical.  3. TAG Analysis The key to our analysis is the assumption that the base is underspecified with respect to ward c1ass (verb or noun). We propose that this base is the anchor of an elementary tree with all its arguments and that it acquires a noun status only after the light verb adjoins into the elementary . tree. The assumption that the category of the base is unspecified is well-motivated for two reasons: (i) The base form ::::::;;::-M„~ from t'.hinese, in which the same form is used both as a noun and a verb, (ii) there is no consensus in the Iiterature as to v.l•..:.~ :!::: '.'.'.'.'"':"rYr.f th„ i.,„~„ is and positing that it is either a noun or a verb Ieads to difficulties, as discussed in §2. We represent this by using the labe) X for its category (which projects to XP). We also assume that each node in a tree is associated with a category feature CAT with values such as V(ERB) and N(OUN). The CAT feature of nodes Jabeled V, VP, or S is necessarily v for both the top and bottom feature structures, while nodes labeled N or NP necessarily have [CAT:N].2 But the CAT feature of the base of LVC is unspecified. In addition, we assume that nodes in a projection have a füll set ofmorpho-syntactic features. In this paper we use only the binary feature [TENSED: ]. We assume that the base is [TENSED:-] (since it carries no tense morphology), that the S node is marked [TENSED:+], and that the TENSED feature is shared among the nodes of a projection. We assum-e that when a lexeme (of any category) forms a syntactic predication structure it projects to a maximal verbal projection (VP) and we refer to this VP as the PREDJCATE. Furthermore, following (Heycock & Lee, 1989), we assume that in Korean, nominative case is assigned by the predicate, not by Intl. (Heycock & Lee, 1989) use as evidence the presence of multiple nominative constructions and the fact that infinitivals can have nominative case-marked subject. As a· result, all c1ausal structures need a VP node as a sister to the subject argument to license nominative case.3 We also assume that the lexeme projects all of its argument pösitions in canonical order according to theta hierarchy. That is, the most prominent argument attaches  2The node labels are not actually used in our analysis, and we could also label all nodes XP. We retain the traditional labels for clarity. 3This is compatible with the XTAG analysis of the predicative use of nouns and adjectives in English, the trees for which project from N (or A) to S via NP (AP) and VP (lhough perhaps the VP is less motivated in English than in Korean because the adjoined auxiliary providcs the nominative case in English, not the predication structurc itself).  96 Chung-hye Han and Owen Rambow  to the highest projection, and the least prominent attaches to the lowest projection.4 We assume that each lexeme idiosyncratically fixes a case grid for its arguments,5 which is only realized in appropriate syntactic contexts. (Thus, rather than speak of unified case assignment, we will henceforth speak of case assignment by the lexical head and subsequent case realization in a particular syntactic context.)  [c.'ill =V censcd • +]  ~ [ca1•V1<n<CJ•(~o S [cat„ Vtcru.cd• lSI]  ~ [ <>J<• norn] NPI  Ä[<At •V[llton...S•l•LJ  [  ca.«• •ykoyJ  / NP2  ~  ""' XP,  [['•""'•[•I1J211<~nn.«>«Jl••ll6711]]  ~ [ <&1<• •00] NP3 ~  [•:n =[211Cn>«l• l71] XIP2 ['u< • IJJtoru<J • (RI]  i [•„ •131"'"'<J = 1~1] [ <• 1• l4Jtm<J=-] senmwul  Figure 1: Sino-Korean base lexeme senmwul 'gift' projecting to a predicative structure  In Korean, a verbal case such as Nom, Ace, eykey is realized when the head has feature [CAT:v],  while if the head has feature [CAT:N], Nom and Ace are realized as Gen or null, while any other postpositional case is realized as that postposition followed by Gen.6 As an example, the  elementary tree for the base senmwul 'gift' is shown in Figure 1, which is a ditransitive structure. Assuming the argument hierarchy agent - goal - theme, as in G&M, agent (as indicated l:>y  [case:nom]) is attached to S, goal (as indicated by [case:eykey]) is attached to VP and theme  (as indicated by [case:acc]) is attached to XP1 • (The subscripts on nodes are used only for  distinguishing different nodes, they play no role in the analysis.)  ·  We now turn to the light verb. Its properties can be best explained in comparison to heavy ha;  Heavy ha (Figure 2, left) is a standard transitive verb: it has two arguments (i.e., theta-markecl dependents), to which it assigns nominative and accusative case, respectively. (Nominative cas~  is realized in a syntactic predication environment, while accusative case is realized whenever,  the Iexical head is verbal, which it is by assumption.) The light verb ha (Figure 2, right) diffed  from the heavy ha in that the light ha loses its ability to assign theta roles: it has no arguments of its own. Furthennore, it has lost its ability to create a predication structure. Thus it can nef  Jonger assign nominative case. lt therefore does not project to a VP after taking its complement,'  but only to an XP, with [CAT:V]. However, light ha retains its ability to assign accusative case  as weil as the feature [CAT:N] to its complement. Since there is only one substitution node left, and since both root and substitution node are labeled XP, the tree is optionally an auxiliary trge;  (as is the case for English predicative auxiliary trees).  4We do not deal with the issue of optional arguments in this paper. sAltcrnatively, we could assume each lexeme idiosyncratically chooscs a set of theta-roles and then devise :t  functi onal mapping that derives the cases of a lexeme from the set of theta roles. Such an approach is only:a.  notational variant of ours, and, as it has no additional content, we do not pursue it here.  :./  6In other Janguages, the mapping between verbal and nominal case may not be as straightforward and each mayf  be marked idiosyncratically from the head.  Light Verbsand Lexical Argument Structure  97  x-v]["'•v)  ·  Ä - XP [Cllc=Vtcnsed =+]  A ' [  ,NP 1 '"'""'mJ ''  VP [<••V) [<• • v]  [  ]  . cn.se,,,acc  XP/ *  cnt = N  f  
 only composition operation-and finite-state  linearizers that take caie of V\!ttic.:il rr:ove-  ment ('promotion') of phrases and of the lin-  ear order of branches of derived trees. s su~o·  
The relationship between strong and weak generative powers offonnal systems is explored, in particular,from the point ofview ofsqueezing more strong power out ofafornial system without increasing its weak generative power. We examine a whole range of old and new results from this perspective. Howeve1; the main goal of this paper· i:; ~e :'i'.'~.e1;3f.'.!" the stmng generative power ofLambek categorial grammars in the co1uext ofcrossing dependencies, in view ofthe recentworkofTiede (1998). Introduction Strang generative power (SGP) relates to the set of structural descriptions (such as derivation trees, dags, proof trees, etc.) assigned by a formal system to the strings that it specifies. Weak generative power (WGP) refers to the set of strings characterized by the formal system. SGP is clearly the primary object of interest from the linguistic point of view. WGP is often used to locate a formal system within one or another hierarchy of formal grammars 1• Clearly a study of the relationship between WGP and SGP is highly relevant, both formally and linguistically. Although there has been interest in the study of this relationship, almost from the beginning of the work in mathematical linguistics, the results are few, as this relationship is quite complex and not always easy to study mathematically (see Miller (1969) for a recent comprehensive discussion of SGP). Our main goals in this paper are (1) to Jook at some old and recent rcsult~ :md try to put them in a general framework, a framework that can best be described by the slogan-How to squeeze more strong generative power out of a grammatical system?- and (2) to present a new result concerning .Lambek categorial grammars. Our general discussion of the relationship of SGP and WGP will be in the context of context-free grammars, categorial grammars and Iexicalized tree-adjoining grammars. 1. Context~Free Grammar (CFG) McCawley(l 967) was the first person to point out that the use of context-sensitive rules by linguists was really for checking structural descriptions (thus related to SGP) and not for generating strings (i.e„ WGP), suggesting that this use of context-sensitive rules possibly does not 7his work was partially supported by NSF Grant SBR8920230 1SGP is also relevant in the context of annotated co.rpora. Tue annotations reßect aspects of SGP and not of the rules of a grammar and therefore WGP.  Aravind K. Joshi  .. nCFd's. Peters and Ritchie (1969) showed that this was indeed the case. i~'ffäfüy related to the notion of recognizable sets of trees (structural descrip- . aih~d below. ~q.G, the derivation trees of G correspond to the possible structural descriptions assignable yc. Jlis easily shown that there are tree sets whose yield language is context-free but the tree · are not ehe tree sets of any CFG. That is, we are able to squeeze more strong power out of CFG's indirectly. Here is a simple example.  Let T be the set of trees defined by trees such as /~ a/I l~b l'b  a  A~  
LaTiiCe - TALaNa  LexiQuest  sk@ccr.jussieu.fr marie-helene.candito@lexiquest.fr  LIMSI kercadio@limsi.fr  bstract /1e aim ofthe paper is to propose a 11ew description of e:x:traction in plain TAG. Contrary to Kroch 987's analysis, our description is based on the fact that the power of a relative clause to adjoin on a 10un can be attached to the wh-word rather than to a verb. This analysis solves some problems of the .revious analysis, notably by giving the right semantic dependency in case of pied-piping. .· e are thankful to our two reviewers for many valuable comments.  ntroduction  The only description of extractions in TAG we know has been developed by Kroch & Joshi (1986), Kroch (1987) and implemented in the developed grammars of English (XTAG 1995) and French (Abeille 1991, Candito 1999). This implementation solves the unboundedness of extractions with predicative adjoining, but the pied-piping is solved using a special feature. We think that this solution of pied-piping is not absolutely convenient, because some edge of the derivation tree cannot be interpreted as semantic dependency (Candito & Kahane 1998). Our assumption is based on the fact that a TAG derivation tree can be interpreted as a semantic graph, that is a predicate-argument structure. Moreover this implementation fails to describe some cases of extraction, such as some French dont-relatives. We propose a new description of extraction in TAG which solve most of these problems. Nevertheless, our study must rather be appreciated as an investigation of the limits of the TAG formalism, because we think that TAG is not the most appropriate framework for the implementation of our description of extractions. Tue same analysis i.is more suitably implemented in GAG/DTG (Candito & Kahane 1998). Semantic dependencies  The meaning of a sentence comes from the combination of the meaning of the lexical units of the sentence. A lexical meaning or semanteme can be considered as a semantic functor or predicate. For instance, consider: (1) Pete~· often saw black cats.  In (1), the meaning 'see' is a binary functor whose argument are 'Peter' and 'cat', whereas 'often'  and 'black' are unary functors with respectively 'see' and 'cat' as arguments. This predicate-  argument structure can be represented by a graph (Fig. 1), called a semantic graph (Zolkovski &  Mel'~uk 1967, Mel'yuk 1988). An edge of such a graph is called a semantic dependency . Tue  two extremities of a semantic dependency are called the semantic governor and the semantic  argument. A semantic graph can be converted into a logical fonnula by reification : for each  semanteme a variable is introduced as first argument of the predicate; this variable is used by other  predicates pointing on it in the semantic graph. The semantic graph of Fig. 1 is thus converted in  the fonnula:  ..  'Peter'(x) & 'cat'(y) & 'black'(p,y) & 'see'(e,.r,y) & 'often'(q,e)  Sylvain Kahane, .Marie-Helene C.andito, Yanflick de Kercadio 'see' / +"'-2 t . / 1 '-' cat' 'Pete r' ~ 'often' 1 
The paper presents a lexicaliz.ed depelldency grammar which solves some failures ofLexicaliz.ed TAGs, such as rhe co111bi11atorial explosion of rhe number of elementary trees and the non adequacy for the a11alysis ofsome constructions. lntroduction Wide coverage grammars for natural languages have been developed in Lexicalized TAG (cf. Abei!le 1991, Candito 1999 for French and Paroubek et al. 1992, XTAG 1995 for English). These implementations have brought to the fore some failures of the formalism for natural Janguage description which cannot be solved without adopting a descriptively more powerful formalism. These failures concem most of lexicalized grammars, including Categorial Grammars (CG). In this paper, we will present some of these failures and propose solutions in a lexicalized dependency grammar based on Nasr 1995, 1996. 1. Lexicalized grammars An LTAG is a particular case oflexicalized grammar (LG). A LG is a formal grammar that has the form of a lexicon: each lexical unit is associated to a set of elementary structures. The grammar has an operation of combination1 and each sentence (= a string of word) can be associated to set of structures obtained by combinations of elementary structures associated to the words of the sentence. Formally, a LG is a 5-uple G == < .L S. S~. c.p, c > where: .L is the Jexicon; S is the set of structures; it is an infinite set but it must be finitely defined; SF is the subset of S of final structures; q> is a many-to-many map from L to S; c is the operation of combination of structures; it is a many-to-many map from SxS to 5.2 Below c(a,ß) will be noted a.ß. The Operation c induces an operation c* from S* to S which associates to a sequence of structures of S all the structures of S obtained by combination of these structures. For instance, c*(a,ß,y) is all the structures obtained by the combinations (a.ß).y and a.(ß:y). The grammar G defines a = correspondence (= many-to-many map) q>* between L* and SF: a sentence u x 1x2„.x. in L* and s, = a structure S in SF are in correspondence if for each word x; there is a structure <p(x;) such that = c*(SpS2,.„,S.) S. 
Existing analyses of Ge nnan scrambling phenom ena within TAG-related formalisms all use 11011-local variants ofTAG. Howeve1; there are good reasons ro prefer local grammars, in particular with respecr ro tlze use of the derivatio11 structure for semantics. Therefore this paper proposes to use local TDGs, a TAG-variant generating tree descriprions tlzat shows a local derivation structure. However the construction of minimal trees for the derived tree descriptions is not subject to any locality constraint. This provides just the amoimt of non-locality neededfor all adequate analysis ofscrambling. To illustrate this a Iocal TDG for some Gennan scrambling data is presented.  1. Introduction  Scrambling in German poses a problem for most grammar formalisms. Neither Tree Adjoining Grammar (TAG, Joshi et al., 1975) nor even linear context-free rew1i ting systems (LCFRS, Weir, 1988) are powerful enough to deal with scrambling and the free word order in Ge1man (see Becker et al., 1992). (Becker et al. , 1991) propose a scrambling analysis with non-local multicomponent TAG (MCTAG, Weir, 1988), and (Rambow & Lee, 1994; Rambow, 1994)  propose the use of vector TAG (V-TAG). These formalisms are both non-local in the sense that  when adding a new element of the grammar in a derivation step, this element is not attacheri to  one single previously added element of the grammar.  There are however good reasons to prefer a local grammar. Firstly, locality often restricts the  parsing complexity, and local grammars often generate only semilinear Ianguages. (Though  some non-loc.al formalisms (lexicalized V-TAG for instance) also can be shown to be polynomi-  ally parsable.) Secondly, in a local grammar, the derivation structure might retlect a dependency  structure based on which semantic representations can be built (as for TAGs in Joshi & Vijay-  Shanker, 1999; Kallmeyer & Joshi, 1999). In a non-Jocal grammar, the deri vation structure  does not directly determine a suitable dependency structure. In some formalisms, it is possible  to identify parts of elementary structures that are relevant for the dependency structure (e.g. in  D-Tree Grammars, Rambow et al., 1995, the relevant part is the part of a d-tree that is substi-  tuted in a subsertion operation). But there is not one single structure that records the complete  derivation and that is a suitable dependency structure.  ·  As an alternative, I propose to use local Tree Description Grammars (local TDG, Kallmeyer,  1997; Kallmeyer, 1999). Local TDGs generate tree descriptions with a local derivation process.  They have a context-free derivation structure and generate onl y semilinear languages. The  descriptions generated by local TDGs allow an underspecification of the dominance relation,  and the construction of so-called minimal trees for these descriptions is not subject to locality  constraints. This limited amount of non-locality allows to deal with scrambling, as illustrated  by a local TDG for some German scrambling and extraposition data.  130  Laura Kallmeyer  2. Scrambling: The data The paper accounts for data like word order variations of (1 ), taken from (Rambow, 1994).  (1) Weil niemand das Fahrrad zu reparieren zu versuchen verspricht  because nobody theacc bikeacc to repair to try  promises  because nobody promises to try to repair the bike  Assuming that each NP precedes its verb, we get 30 word orders when combining scrambling with extraposition. According to Rambow, 6 of them are clearly not acceptable. The other 24 also show differences with respect to the judgment, but in principle it should be possible to generate them all. The word orders without extraposition and their judgments are shown in (2). Word orders that are ruled out occur with extraposition of reparieren as in (3).  (2) a. ok Weil niemand das Fahrrad zu reparieren zu versuchen verspricht b. ? Weil das Fahrrad niemand zu reparieren zu versuchen verspricht c. ok Weil.das Fahrrad zu reparieren niemand zu versuchen verspricht d. ? Weil das Fahnad zu reparieren zu versuchen niemand verspricht  (3) a. * Weil zu versuchen das Fahrrad niemand zu reparieren verspricht b. * Weil das Fahn-ad zu versuchen niemand zu reparieren versp1icht c. * Weil zu versuchen !"iPmand das F:ihm1d zu reoarieren versp1icht d. * Weil niemand zu versuchen das Fah1rnd verspricht zu repatiere11 e. * Weil zu versuchen niemand das FahJTad versp1icht zu reparieren f. * Weil zu versuchen das FahJTad niemand verspricht zu reparieren  I will also consider more than two levels of embedding as in (4).  weil das Fahrrad niemand glaubt zu repaiieren zu versuchen versprechen  because theacc bikeacc nobody thinks to repair to try  promise  (4) zu müssen  to need  because nobody thinks it necessary to promise to try to repair the bike  3. A local TDG for scrambling Local TDGs consist of tree desctiptions (elementary descriptio11s) and a starr description. The tree descriptions are negation and disjunction free formulas in a quantifier-free first order logic. The logic allows to express relations between node names k i, k2 such as immediate dominance k1 <l k2, dominance (reflexive transitive closure of <l) k1 <l• k2, linear precedence k1 -< k2 and equality k1 :::::: k2• Furthermore, nodes are supposed to be labelled by terminals or by atomic feature structures. 0 denotes the labeling function, o(k) ~ t signifies that k has a terminal labe! t, and a(o(k)) ~ v signifies that k is labelled by a feature structure containing the attribute value pair (a, v). Roughly, tree descriptions in a local TDG are fully specified (sub)tree descriptions that are connected by dominance relations. 1 In elementary descriptions, some node names are marked; this is important for the derivation. In the graphical representations, marked names are equipped with an asterisk. (5) shows a local TDG for some scrambling data with </>s = k1 <l k2 /\ k1 <l k3 /\ k2 -< k3 /\ k3 <l• k4 /\ • •• /\ cat(o(k1)) :::::: S /\ ... etc. (dotted edges represent dominance relations). Conjuncts as k3 <l• k4 in </>s not entailed by the rest of the formula are called strong dominance. 1Some of the conditions holding for descriptions in a local TDG are lcft aside herc. For a formal dcfinition of local TDGs sec (Kallmeyer, 1999, Chapter 4).  Scrambling in German and the non-locality oflocal TDGs  ------ <Ps Sk1  k2 C  "'.l ks  Vl k4  ..---1  (5)  k3  Vl Vl /~ ~  k; N  V,2 ks  ------Vl  N 
n rhi.\' pape1; 1i:e introduce a formalism called contextual tree adjoining grammar (CTAG). (::TAG.~ are a generalization ofmulti bracketed contextual reivriting gramnwrs (MBICR) which combine tree adjoini11g grammars (TAGs) and co11textual grammars. The generalization is to add a mechanism similar to obligatory adjoi11i11g in TAGs. Here, we present the definition o.f the model and some results co11cerni11g the ge11eratil'e capaciry and closure properries of rhe classes <!fla11g11ages generared by CTAGs. Introduction Contextual grammars are a formalization of the linguistic idea that more complex, weil tormea strings are obtained by inserting contexts into already we11 forlned strings. They were first introduced by Marcus in 1969; all models presented here are based on so-called internal contextual grammars which were introduced by Pllun and Nguyen. References and further details about contextual grammars can be found in the monograph (Pi\un, 1997); a survey is given in (Ehrenfeucht et a{., J997). Tree adjoining grammars (TAGs) and contextual grammars are linguistically we11111v,;, "~~~ ""'1 have been considered as a good model for the description of natural languages (c.f. (Marcus, 1997)). Although contextual grammars and tree adjoining grammars seem very different at first sight, a closer look reveals many similarities between both formalisms. Therefore, it seems natural to combine those formalisms in order to obtain a generalized class of grammars for the description of natural languages, which combines tbe mechanisms of various classes. A first step were so-called multi-bracketed contextual grammars (MBIC) and multi-bracketed contextual rewriting grammars (MBICR), c.f. (Kappes, l 999). These grammars operate on a tree structure induced by the grammar (the first approach aiming in this direction was introduced in (Martin-Vide & Päun, l 998)). However, the families oflanguages generated by MBIC and MBICR-grammars are cither strictly included in or incomparable to the family of languages generated by TAGs. This is the case since, in MBIC and MBICR-grammars, each yield of a de1ived tree is immediately a word in the language generated by the grammar. In other words, there.is no mechanism to distinguish between "finished" and "unfinished" trees like obligatory adjoining allows in TAGs. Here, by adding obligatory adjoining to MBICR-grammars, we obtain a generalized class whicli is also a proper extension of TAGs. Definition and Example Let I;• denote the set of all words over the finiLe alpbabet B and I;+ = i:• - {.A}, where ,\ denotes the empty word. We denote the Jength of a string :r: by !xi. In this paper, we use the term derived tree for a tree where the internal nodes are labelled by symbols from a nonter~ minal alphabet D.. and the leaves are labelled by symbols from a terminal alphabet I:. We use  136  Martin Kappes  .-\ /~  B  B  J. /1,~  II  " /;  ('  A ffi a B A ! t !A " ,.  B /l~  A B ! ,J,, (/  c t~ A H 1 J, ~  " "  Figure 1: Derived trees corresponding to the Dyck-covered words (from left to right) [..\ [n11]u [11/Jlw] n]..1. l.-111[B[.1cJ.--1]B[.--1 li]A).4 and [n [.40J.4[B/J)B [c[.4o].4[ut] nk]u.  a linenr representation of derived trees called Dyck-covered words. A Dyck-co\'ered ward is a string consisting of terminal symbols and opening and closing brad:ets indexed with nontermin al symbols. Formally. for the nonterminal alphabet ..::, \\'e define the brncket alpbabet B.:. = {]..i-).1 1.-! E ..::,}. Throughout the paper we always assume I: n B.:. = (~. The set of all Dyck-covered words DC.:. (I:) over I: \vith respect to the index alphabet ..::, is inductively defined by • For all 11· EI:"'" and .4 E ..::,, [..1ul1 is in DC.:. (2::). • Let 11 2_ 1 b.: ;, l"v„;,;·;.: :··~e~„,· lf .--1 E ..::, ;md o 1• n~ ..... n 11 are in DC.:,.(I:) U I:. then [. 1n 111:! ..·. n„]..\ is in DC..,;(2:: ). lt is not difAcult to see that each n E DC.:.(2:: ) can be interpreted as unique encoding for a derived tree. where ..::, is the labe! alphabet for the internal nodes and I: is the labe] alphabet for the leafnodes in the following way: Astring [.4nJ.4E DC.:. (I:) is identi fied with a tree where the root is labelled by .-!. and the subtrees of the root are determined by the unique decomposition = ofn n 111:.>.„n 11 suchthato; E DC.:. (I:)UI:, l Si S 11. ForexamplesseeFigure J. By DC~ (I:} we denote the set of all elements in DC.:. (I:) v.·here the root node is labelled by A. A comextual tree adjoining grammar (CTAG) is a tuple G """ (.S, ~. "[. n. I'). where ~ is a finite set of terminals, !l is a finite set of indices, T ~ ..::, is a set of permitted indices, n ~ DC_; (I:) u {>.} is a finite ser of axioms and p is a finite Set of productions. fatch production is of the form (5, C, !\, H), where 5 ~ I;+ is the selector language. ]\', H ~ ..::, are sets of nonterminals and Cis a finite subset of contexts where each context is of the form (/L, 11) such that 1111 E DC.:.(~) . The derivation process in a CTAG is illustrated in Figure 2: A context (11., 11) may be adjoined to an et = n-1[no·2]8 rra yielding a tree O-!f1foo:2]E110:3 if and only if there is an {5, C. f\-, H) E P such that the yield of n 2 is in 5 , (JL, 11) E C, [nn2]B E DC~ (:S), ß E /\' and E E H. The . string [..1 n~J 1 is called selector. In the above figure, we have <t E DC~ (E) , 1111 E DCf(B) and the yield of n·1• n :!. n·3• 11., 11 is w1. w2, w3 , ·u„ v respectively. The set of all sentential fo rms of G, S (G'), consists of all trees which can be derived in the above way starting from an ax.i om in n. The set of all trees derived by a CTAG G, T(G) , consists of all trees in S(G) where the internal nodes are only labelled by nonterminals in T. The weak generative caparity L(G} is the yield of all trees in T(G). Hence, internal nodes labelled by symbols from ..::, - T have to be rclabclled during the derivation process in order to obtain a tree in T(G).  Contextual Tree Adjoining Grammars  137  A A ~ u·„ ·'  [)  6  tl  1"  H'„  Figure 2: The derivation process in a CTAG L'p 10 some technical modificalions necessary to keep our formalism consislent 10 the usual model of contexrual grammars, we only added selector Janguages eo ehe productions of a TAG. These selector languages are used to control the derivation process as they do in contexrual grnrnmars. the adjunction of an auxilliary tree is only possible if ehe yield of the node where ehe adj unc1ion rnkes place is in the selector language. = We can classify CTAGs by their selector languages: A CTAG G (:=. ~. Y. ~l. !') is called \\'ith F-choice for a family of languages F. if 5' E F fo r all (5. C. J{. H} E !'. Consider for example the CTAG with ~+-selec1ion G ({o. b. r" t!. <-'}. {...I. B}. {...I}, { [..1a[Bbcfod}A }· {1.1. ;;:1}) where 1.! :::: (~+ . {([Au [JJb. f"]udj..i)}. {B} . {A} ) and 7i'°:.! = (::+. {([..1e. 1']..1)}. { B}. {A}). It is not difficult to see that using ..-1 i 1imes yields a derivation  In order 10 ol:itain a s1ring in T(G) we have to use production ;;2 exac1ly once 10 remove the pair of brackets indexed by B from the sentential form. After applying ;;'.! once. no J'u1 th,::· deriv'ltion steps are possible, hence L(G) = {011d1"<"1r·d11 l11 ~ 1}. GeneratiYe Capacity CTAOs are a generalization of MBlCR-grammars. For ~ = Y these models are equivalent (CTAOs could thus also be called multi-bracketed contextual grammars wich obligatory rewriting (MBICRO)). The obligatory adjoining fe ature increases ehe generative capacity. For instance. the language in the above example cannot be generated by any MBJCR-grammar. This is due to the fact that each Janguage L generated by an MBICR-grammar fulfills the so-called internal bounded step property (c.f. (Paun, 1997)): There is a constant 71 such that for each = = string .r E L, i:rJ > 7i there is a y E L such that :1: :c y 1'(L1:21i:1::i, :1:1:r'l:i::1 and 0 < 111.v\ S 11. CTAGs using only the selector language E+, i.e., in effect ignoring the selector languag~·mech­ anism. and TAGs are, up to some details. descriptions of the same model. lt is possible to construct a TAG equivalent to a given CTAG with E+-choice and vice versa. The technical detail is that all elementary trees of a TAG must be elements of DC6 (:S) if the foot nodes of the auxilliary trees are not taken inio account. Formally, the equivalence holds ifthe initial trees in a TAG are elements of DCc. (E) and each auxilliary tree i ofGis of the form CT; = 11;[.-1JA„11; such that11;11; E DC1'(~). Notice thatthepair(A„]A„ representsthefootnode ofO'.;. Theconstruction of an equivalent TAG for a given CTAG with r;+ -choice is a strnightforwurd generalization of  138  Martin Kappes  n similar construction for MBICR-grammars which can be found in (Kappes, 1999).  For the other direction. consider a TAG Gof the above form. Let .\ denote the selective (or .Y in case of an obligatory) adjoining constraint of an internal node in an elementary tree. X (or  _\) thus dereferences the subset of auxilliary trees which may be adjoined at this node. We can = construct an equivalent CTAG G' (~. S. Y'. rl'. P') with ~+ -choice as follows: The set of indices S and the set of permitted indices Y' of G' is given by  = { S  (.-! . .\·) !A E ~ and .\- is a (selective or obligatory) adjoining constraint}  J' = {(.-!. .\) 1 A E ~ and .\" is n selective adjoiningconsrraint} .  For each initial tree n of G we insert a tree n' into 0 '. where each node labelled by A E ~ \\·ith (selective or obligatory) adjoining constraint .\- is replaced by the index (.-L .\· ). We thus consider the adjoining consrraint of a node as part of its index. For each auxilliary tree i : n ; = t·{ 11;[..1,J.1y, we insert a production 7i; = (:~::::+. { (11; . 11;J}. {(.-!;..Y) 1i E .\: (.-1;. Z) }) into P' where 11'.1;j is obtained from 11;11; by the same procedure as above and Z is the (selective or obligmoryJ adjoining constnint of the foot node of n;. lt is possible to prove rhat both grammars are equiYalent.  lt cnn be shown that ench CTAG with finite selection generates a concext-free Janguage. This is the cnse since the Jength of ench string which may be used as selector in a derivation step can be bounded by some constant. Due to the bracket structure it is impossible to shift information through the semential form of a CTAG if the length of the selectors is finite. Therefore it is possible to conscruct a context-free grammar generating the s:ime Jangu:-we Alsn. for each context-free Janguage there is a CTAG \\'i th finite selection generating thnt language. So. CTAGs \\'ich finite selectors generate exactly the context-free Janguages.  CTAGs with regul<ir selectors can generate langunges which cannot be generated by TAGs = even ir \\'e do not take advamage of the oblig::uory adjoining feacure. The language L(G ) { o" /111 r.111 d''r" 1111 ?'. /1 ?'. 1} can be generated by an MBICR-grammar and hence by a CTAG with regu lar selector languages (c.f. (Kappes, 1999)) but not by any TAG because of the pumping-lemma for TAGs (cf. (Vijay-Shnnker. 1988)).  \Vith context-sensitve selector Janguages. CTAGs generate exactly the contexHensitive langunges: Let L ~ ~+ be n context-sensitive language. We consiruct lhe CTAG  G = (~.{A..B} ,{ .-1},0.{r.} U{ n,,. laE~}). where  rl {[A;rJ.., 1.1: E L.j:ri = 1} u {[Ba]11 ll7 E ~}  = r.  (~+,{([ßa. ]a)!aE ~}.{B},{.-1})and  = 1T11  ({.1:EI:+ l a:r EL},{ ([..ia.]..i)},{B}.{A}).  Since the family of context-sensitive language is closed under quotient with singleton sets. all = selector languages are context-sensitive, and it is not difficult to prove L(G) L.  Tbis result shows that the combined use of selector Janguages and obligatory adjoining Jeads = to a very powerful formalism. Whereas there are context-sensitive lm1guages (s uch as L {a"d111l'"<·rl11 l 11. ?'. 1}) which cannot be generated by any MB ICR-grammar regardlessly of the used selector languages, the above construction shows that for each fomil y of languages F closed under quotient with singleton sets and containing all finite Janguages each L E F cnn also be generated by a CTAG with F-choice.  Closure Properties The class of languages generated by CTAGs with F-choice is closed under union. concatenation = and Kleene-star for all families of languages F with L:+ E F . Let G 1 (~ 1 , ~ 1• Y 1, H1, I'1)  Contextual Tree Adjoining Grammars  139  = and (;" = (:~:::1. ~ 2 . 1 2 . 0:1. P,) be two CTAGs wi th F-choice for a family of languages F \\'ith ~ - E F. Without Joss of generality we may assume that ~ 1 n ~:! Vl. Therefore it is  n'.!. easv 10 see that for (; = ("S::1 u ~2 · ~ 1 U ~'.!· Y i U 1 2. 01 U P i U P1 ) we have L(G) =  n1 - {,\}} L(C;1) (~ 1 ·~  ...,; ~::  L (G1 ). For · ~ 1 !..J ~2 U  concatenntion {5} .1 1U 12  we take a new index U {5}. {[..,-n d]s io E  S t/:. r11 -  ~ 1 U ~2 and {>.}. J E  construct G' U {n  = E  !? 1 \\'e  f,\ "'= !l:: } construct  :.J {o E (;" = (  n~ ~1 •  j,\ ~1  E U  Oi}. { .5'}.  P1 11  U P2 ). U {S}  Clearly . {[sci]s  L(G') = 1n E 111  L(Gi) - P.}}  · L(G2 ). For Kleene-star U {>.}.P U {;;} ). where  ;-: = 1~-.{(~,,. 11].-·)Jrt E ni -P} }. {5} .{ S'} ) It is atechnicale>.:erciseto proveL(G"} =  L1(,'1J'. = For cach CTAG (,' and regular language R we can construct a CTAG G' such that L (G'} L((,') ;-; n. Furchermore. (,'' uses the same selector languages as (;. Hence. 1his construction directly proves that the class of langunges genernted by CTAGs with F -choice is closed under intersection wich regulnr Jnnguages for any family of Janguages F. For the rele\':lllce of closure under intersection with regular sets we refer the reader to (Lang. l 994).  = In the following. we will present n sketch ofthe proof. Let G (~. ~- Y. 0 . P) be an arbitrnry  CTAG and R a regular language. Without loss of generalicy we assume thnt c; is in anormal  form such that each internal nocte either has exnctly one leaf or only internal nodes as immediate  = ~uccessors: formnl ly for each n 1(1 111:1 E T( (,') such that (1 1 E DC.J. (:::) we eithcr lrnve n .2  = [.. [..1r1]..1 ror so me" E ::: and .-l E ~ or 11:!  1.11 .•• .1„]..1 for an...! E ~ and ./i E DC~(~).  l S i .:s; 11. Since R is regular. there exists a determiniscic fin ite automaton JJ = (C}. :::. c5. <Jo. F)  = with L( Jl) R (c.f. (Hopcroft & Ullnrnn. 1979) for notnti onal details). We conscruct a  grammar G' where the labe! of each internal node additionnlly carries two pnirs of states of .'1. -J formnlly lhe set of indices of G' is given by <I) = { (.-!. [11. g). [r. . l...1 E ~ . 11, <J. r. s E Q} .  Tntuiti\'ely. in the tree interpretation. if an internal node is labelled by (.-!. [11. <J]. [r. .~JJ then [11. 11] is a \'alue propagated from the immediate predecessor of the node stating thnt this node is supposed to generate a yield rr such that ci(11. w) = q. The pair [r. s) denotes that the immediate successors of the node are supposect to generate Cl yield 11: such that 15{r. II') = s .  (;' generates ns scntential forms exactly the sentential forms of G where a lnbel of an internnl node A in S'(G} is replnced by all labels (.-! . [J1.1JL [r. s]J. JI· •;. r s E (2, in 8(G') such chat for the resulti ng strings n E S(G'') the follov.·ing properties hold:  = = h·-,- ( l) For each partition 11 11 1n ~n:i such that n , E DC.J.CS) and rr2  
In this paper, we introduce the 11otio11 o.fHypertag, which allows to jactor the i11formatio11 co11tai11ed in severa/ Supertags illlo a single strucwre. We also discuss why this approach is usejul within frameworks other than LTAGs, and how it can be used jor a1111otati11g and searching corpora. Introduction Traditional part of speech tagging assigns very limited information (i.e. morphological and local) to lexical items, thus providing only limited help for parsing. To solve this problem, (Joshi & Srinivas 94, Srinivas 97) extend the notion of POS by introducing Supertags, within the framework of Lexicalized Tree Adjoining Grammars (LTAGs). Unfortunately, words are assigned on average a much higher number of Supertags than traditional POS : On average for English a word is associated with l.5 POS and with 9 supenag:. (;;;:,;,; 9':'). 0rie common solution to the problem is to only retain the "best" supertag for each word, or eventually the 3 best supertags for each word, which is what (Srinivas 97) does in a probabilistic manner. But then, early decision has an adverse effect on the quality of parsing if the wrong supertag(s) have been kept : one typically obtains between 75% and 92% accuracy when keeping onJy one supertag / item (depending on the type oftext being supertagged and on the technique used) (cf Srinivas 97, Chen & al. 99) which means that it may be the case that every word in 4 will have a wrong supertag, whereas typical POS taggers usually achieve an accuracy above 95%. Solutions for packing several supertags into a single srructure h~ve been prooosed in the past, for example by resorting to Jogical formulae (Kallmeyer 99) or linear types of trees (Halber 99). But as argued in (Kinyon OOa), these solutions are unsatisfactory because they re!y only on mathematical properties oftrees, and Jack a linguistic dimension. In this paper, we introduce the notion of Hypertag, which allows to factor the information contained in several Supertags, so that a single structure can be assigned to each word. In addition of being well-defined computational objects, hypertags should also be "readable" and also motivated from a linguistic point of view . In a first part, we explain the solution we have adopted, building up on the notion of MetaGrammar introduced by (Candito 96) & (Candito, '99). Finally, we discuss how this approach can be used in practice, and why it is interesting for frameworks other than LTAGs. We assume the reader is familiar with LTAGs and Supertags and refer respectively to (Joshi 87) & to (Srinivas 97) for an introduction. 1. Exploiting a MetaGrammar (Candito 96,99) has developed a tool to generate semi-automatically elementary trees ~he uses an additional layer oflinguistic description, called the metagrammar (MG), which imposes a general organization for syntactic infonnation in a 3 dimensional hierarchy : • Dimension 1: initial subcategorization  142  Alexandra Kinyon  • Dimension 2: redistribution of functions and transitivity altemations • Dimension 3: surface realization ofarguments, clause type and word order  Each terminal dass in dimension 1 describes a possible initial subcategorization (i.e. a tree family). Each terminal class in dimension 2 describes a !ist of ordered redistributions o functions (e.g. it allows to add an argument for causatives). Finally, each terminal class in dimension 3 represents the surface realization of a (final) function (e.g. cliticized, extracted ...). Each class in the hierarchy corresponds to the partial description of a tree (cf. Rogers & Vijay-Shanker 94). An elementary tree is generated by inheriting from one terminal class in dimension 1, from one tenninal class in dimension 2 and from n tenninal classes in dimension 3 (were n is the number of arguments of the elementary tree). 1 The hierarchy is partially handwritten. Then crossing oflinguistic phenomena (e.g. passive+ extraction), terminal classes and from there elementary trees are generated automatically off line2• This allows to obtain a grammar which can then be used to parse in real time. When the grammar is generated, it is straight forward to keep track of the terminal classes each elementary tree inherited from : Figure 1 shows seven elementary trees which can supertag "donne" (gives), as weil as the inheritance patterns3 associated to each ofthese supertags. All the examples below will refer to this figure. The key idea then is to represent a set of elementary trees by a disjunction for each dimension of the hierarchy. Therefore, a hypertag consists in three disjunctions (one for dimension 1, one for dimension 2 and one for dimension 3). The cross-product of the three · disjunctions can then be done automatically and from there, the set of elementary trees referred to by the hypertag will be automatically retrieved We will now illustrate this, first by showing how hypertags are built, and then by explaining how a set of trees (and thus of supenags) is retrieved from the information rnnt<iined in a hypertag.  1.1 ßuilding hypertags : a detailed examp/e  Let us start with a simple exemple were we want "donner" tobe assigned the supertags e1. l (J. donne une pomme alvf.) and e1.2 (J do1111e aJvl une pomme). On figure l, one notices that  these two trees inherited exactly from the same classes : the relative order of the two  complements is left unspecified in the hierarchy, thus one same description will yield both  trees. In this case, the hypertag will thus simply be identical to the inheritance pattem of these  two trees :  Dimension l : nOvnl (:in2)  ~  l Dimension 2 : no redistribution Dimef'6ion 3 lsubj :norri!nal-cano~cal  [  obJ : nommal-canorucal  a-obj: nominal-canonical  Let's now add tree a3 (J. do11ne 1111e pumme) to this hypertag.This tree had its second object declared empty in dimension 2 (thus it inherits only two terminal classes from dimension 3, since it has only two arguments realized). The hypenag now becomes4 :  
We present a class-based approach to building a verb lexicon that makes explicit the close relation between syntax and semanrics for Levin classes. We have used a Lexicalized Tree Adjoining Grammar to capture the syntax associated witfi P.ach verb class and have added semantic predicates to each tree, which allow for a compositional inte1pre1u::.::::. 1. Introduction We describe a computational verb lexicon called VerbNet which utilizes Levin verb classes (Levin, 1993) to systematically construct lexical entlies. We have used Lexicalized Tree Adjoining Grammar (LTAG) (Joshi, 1985; Schabes, 1990) to capture the svntax associated with each verb class, and have added semantic pred1cates. We also show how regular extensions of verb meaning can be achieved through the adjunction of particular syntactic phrases. We base these regular extensions on intersective Levin classes, a fine-grained variation on Levin classes, as a source of semantic components associated with specific adjuncts (Dang et al„ 1998). Whereas previous research on tying semantics to Levin classes (Dorr, 1997) has not explicitly implemented the close relation between syntax and semantics hypothesized by Levin, our lexical resource combines traditional lexical semantic inforrnation, such as thematic roles and semantic predicates, with syntactic frames and selectional restrictions. In order to increase the utility of VerbNet, we also include links to entries in WordNet, which is one of the most widely used online Jexical databases in Natural Language Processing applications. 2. Levin Classes and WordNet Two current approaches to English verb classifications are Word_Net and Levin classes. WordNet is an on-line lexical database ofEnglish that currently contains approximately 120,000 sets of noun, verb, adjective, and adverb synonyms, each representing a lexicalized concept. A synset (synonym set) contains, besides all the word forms that can refer to a given concept, a definitional gloss and- in most cases - an example sentence. Words and synsets are interrelated by means of lex.ical and semantic-conceptual links, respectively. Antonymy or semantic opposition links individual words, while the super-/subordinate relation links entire synsets. WordNet was designed principally as a semantic network, and contains little syntactic inforrnation. Even as a semantic resource, however, it is missing some of the information that has traditionally been required by NLP applications, including ex.plicit predicate-argument structures. WordNet senses are often too fine-grained as well, lacking an underlying notion of semantic components and a systematic extension of basic senses to produce these fine-grained senses.  148 Karin Kipper, Hoa Trang Dang, William Schuler, Martha Palmer The Levin verb classification, on the other hand, does explicitly state the syntax for each class, but still falls short of assigning semantic components to each class. The classes are based on the ability or inability of a verb to occur in pairs of syntactic frames that are in some sense meaning preserving (diathesis alternations) (Levin, 1993). The sets of syntactic frames associated with a particular Levin class are supposed to reftect underlying semantic components that constrain allowable arguments and adjuncts. For example, break verbs and cut verbs are similar in that they can all participate in the transitive and middle constructions. However, only break verbs can also occur in the simple intransitive, and cut verbs can occur in the conative, where break verbs cannot. The explanation given is that cut describes a series of actions directed at achieving the goal of separating some object into pieces. lt is possible for these actions to be performed without the end result being achieved, but where the cutting manner can still be recognized (i.e., "John cut at the loaf'). For break, the only thing specified is the resulting change of state where the object becomes separated into pieces. If the result is not achieved, no attempted breaking action can be recognized. 1. Transitive construction (a) John broke the window. (b) John cut the bread. 2. Middle construction (a) Glass breaks easi!y. (b) This loafcuts easily. 3. Intransitive construction (a) The window broke. (b) *The bread cut. 4. Conative construction (a) *John broke at the window. (b) John valiantly cut/hacked at the frozen loaf, but his knife was too dull to make a dent in it. The fundamental assumption is that the syntactic frames are a direct reftection of the underlying semantics. However, Levin classes exhibit inconsistencies that have hampered rese~chers' ability to reference them directly in applications. Many verbs are Iisted in multiple classes, some of which have confticting sets of syntactic frames. For instance, carry verbs are described as not taking the conative (*"The mother carried at the baby"), and yet many of the verbs in the carry class (push, pull, tug, shove, kick) are also listed in the push/pull class, which does take the conative. Dang et al. (1998) showed that multiple listings could in some cases be interpreted as regular sense extensions, and defined intersective Levin classes, which are a more syntactically and semantically coherent refinement of basic Levin classes. We implement these verb classes and their regular sense extensions in the Lexicalized Tree Adjoining Grammar formalism.  Building a class-based verb lexicon using TAGs 3. Verb lexicon VerbNet can be viewed in both a static and a dynamic way. The static aspect entries and how they are organized, providing the characteristic descriptions of averb siri~~-Ür. a verb class. The dynamic aspect of the lexicon constrains the entries to allow a composidöil~ interpretation in LTAG derivation trees, capturing extended verb meanings by incorporating adjuncts. 3.1. Static description Bach verb entry refers to a set of classes, corresponding to the different senses of the verb. For example, the manner of motion sense of "nin" is a member of the Manner of Motion class, whereas "run" as in "the street runs through the district" is a member of the Meallder class. For each verb sense there is a verb dass as weil as specific selectional restrictions (e.g., an in- strument of "kick" must be of type f oot) and semantic characteristics (e.g„ a particular manner of directed motion) that may not be captured by the class membership. In order to provide a mapping to other dictionaries, we also include links to WordNet synsets. Because WordNet has more fine-grained sense distinctions than Levin, each verb sense in VerbNet references the set of WordNet synsets (if any) that captures the meaning appropriate to the class. Verb classes allow us to capture genera!izations about verb behavior. This reduces not only the effo1t needed to construct the lexicon, but also the likelihood that errors are introduced when .,„,l; adding a new verb entry. E;;ch dass ast:. u1e themat1c roJes that the predicate-argument structure of its members allows, and provides descriptions of the syntactic frames corresponding to licensed constructions, with selectional restrictions defined for each argument in each frame. Each frame also indudes semantic predicates describing the participants at various stages of the event described by the frame. Figure l: Moens and Steedman's tripartite structure of events We decompose each event E into a tripartite structure in a manner similar to Moens and Steedman (1988), introducing a time function for each predicate to specify whether the predicate is true in the preparatory (during(E)), culmination (end(E)), or consequent (result(E)) stage of an event. The tripartite event structure (Figure 1) allows us to express the semantics of classes of verbs like change of state verbs whose adequate description requires reference to a complex event structure. In the case of a verb such as "break", it is important to make a distinction between the state of the object before the end of the action (during(E)), and the new state that results afterwards (result(E)). Verb classes are hierarchically organized, ensuring that each dass is coherent - that is, all its members have common semantic elements and share a common set of thematic roles arid basic syntactic frames. This requires some manual restructuring of the original Levin classes, which is facilitated by using intersective Levin classes. In addition, a particular verb may add more semantic information to the basic semantics ofits class. Figure 2 shows a partial entry for the Hit class. This class allows for three thematic roles: Agent, Patient and Instrument, with constraints that the Agent is generally animate; the Patient concrete; and the Instrument concrete and inanimate.1 These selectional restrictions refer to 1These constraints are more like preferences that generate a preferred reading of a sentence. They may be  150 Karin Kipper, Hoa Trang Dang, William Schuler, Martha  HITclass  ({MEMBERS ))  [(hit, 1), (kick , 1), (slap , 1), (tap, 1), . ..)  ((THEMATIC ROLES ))  Agent(A), Patient(P), Instrument(!)  ((SELECT RESTRICTIONS ))  Agent[+animate),  Pati en t[+concrete],  Instrument[+concrete,-animate]  ((FRAMES and PREDICATES ))  Basic Transitive AVP  manner(during(E),directedmotion,A) /\  manner(end(E),forceful,A) /\  contact(end(E),A,P)  Transitive with A VPwithI  manner(du1ing(E),directedmotion,I) /\  Instrument  manner(end(E),forceful,I) /\  contact(end(E),I,P)  Conative  A VatP  manner(during(E),directedmotion,A)  With/against A V I against/on P manner(during(E),directedmotion,I) /\  altemation  manner(end(E),forceful,I) /\  contact(end(E),I,P)  Transitive  IVP  manner(during(E),directedmotion,I) /\  ~"'!- • • - - • •  .• . . UT"'\ ~. • ;·--f„ 1 T)  Figure 2: Partial entry for the Hit class a feature hierarchy where animate subsumes animal and human, concrete subsumes both animate and inanimat~. ::!!'!~ " " forth. This representation does not suffer from some drawbacks of theta role analysis because our roles are not global primitives, but are only used to describe relationships within a class. The strength of our representation comes from the explicit relationship between syntax and semantics captured in each entry. Figure 2 shows some of the syntactic frames allowed for the Hit class and the semantic predicates for each frame. Thematic roles are used as descriptors which are mapped into arguments of semantic predicates as weil as the argument positions in a TAG elementary tree. The tripartite event structure also handles the conative construction, in which there is an intention of a goal during the event which is not achieved at the end of the event. The example shown in Figure 2 for the conative construction has the predicate mamter(du ring( E), directedmotion,A) but because the intended contact by sudden impact is not satisfied, the semantics does not include the predicates mmmer(end(E),forceful,A) /\ contact(end(E),A,P). 3.2. Compositional Semantics We use TAG elementary trees to describe syntactic frames and associate semantic predicates and selectional restrictions with each tree. Elementary trees capture the basic semantics of the verbs in each class. Each frame in the static aspect of the Jexicon maps onto a TAG elementary tree, in which the thematic roles correspond to substitution sites. Some auxiliary trees are class-based because they interact with the verbs in the class in peculiar ways and add seman- relaxed depending on the domain ofa particular application.  Building a class-based verb lexicon using TAGs  s ~ NPargo.!- VP ~ V NPargl.!- 
This paper presents the LTAG Workbench, a set ofgraphical tools and parsers freely available for LTAG. The system can be view as a modem alternative to the XTAG system. We presentfirst the outlines ofthe workbench including different graphical editors and two chart parsers. The encoding of resources and results is based on an XML application called TagML. We present thenfuture works dedicated to speed efficiency: Op:;;„;,_,ltion oasea u" s;1w in~ <e...:;..:.jwr:~ ;;::::! preprocessing offeatures. The whole system has been developed in Java which allows a strong portability and interesting reusability properties. 1. Introduction The success of a Jinguistic formalism can largely depend on the availability of dedicated tools. They are needed first for maintaining the consistency of a grammar and for checking its correctness, but also for proving the adequacy of a forrnalism for computational applications. Such tools raise several engineering problems that should not be neglected as portability, reusability, user-friendly graphical interface, easy installation procedure and recycling of existing grammars, see for instance (Erbach & Uszkoreit, 1990) for a overview of these problems. Focusing on these features, we present a set of freely available tools dedicated to the LTAG formalism (Joshi et al., 1975) which aims tobe an alternative to the XTAG systP-m (XTAG rcsr.arch group, 1998). The LTAG workbench is still an on-going work and we hope that it will appear enough promising to give rise to interests and possible contributions from the LTAG community. We present first the outlines of the current workbench including different graphical editors and two chart parsers. We introduce then our solution for resource management which is based on a XML application called TagML. The section 4 is dedicated to future optimizations for speed efficiency that emphasize precompilation techniques and sharing of computation on the basis of grammar redundancies. 2. The LTAG Workbench 2.1. Editors The workbench proposes general editors for the set of elementary trees schema and for the morphologic and syntactic lexicon. The graphical editors are based on a general tree editor developed at Thomson-CSF (France). lt includes a Jexicalizer function, similar to the one of the XTAG system, that allows to visualize an instancied elementary tree given a schema and lexical entry. These editors covers the functionality of the XTAG system and include browsers for Jexicons.  156  ~!~--J~~~~~~~l!:.~~~~i~i(LJ~~~'..;;':::,;~;!.:.~·,·, ;~.•.ci~ ~:.•,.:;_··-.  m l i (K;;i{~lU! j~,~~.!:~4~1!"1!~:'..,":~.-.,;-·~.;/f:1 ·„~··'i':~\ ; • )  ~  ,.„, lnptO: ~.!'!f!'l'"" ,...,... ~'!!!!!!..--.--___!  ~ mH  [)_ ein 1Afattllrll1~i~tAld• ,. U e COlllJll'fnd lati:tr<tlll qu• t& rH21t ul~ l-", _ la O t S/fH lt JOUHT d• fOCllld M!tllltlll\ P~• •vpetrif lolrt .,.htl llqlte M I• Hlftet  Pal \in};:  ~  Unlflc•t.100. ~ :::::• ..._'\ no tOF.l'bott„ ....,;f,  S1r11w9~·. \') r.ot'ka -. 1c f\ t• r •V.t  1-\1•sulfi.. ('J CIOllOlt \ t jll...Htc  • c::J• • CJ-l•U QrorNIU 0 C1,_.t;•1 e c::JI •OC'lllCl••t• D"""'"' D N.NIU DD""""•"l'l'l't•' O c:J p-i•I • Cl• [)«IO!PttU • C) ,..t••I DNiwlU D"'1N1u Q r u vlU D - NIU [ ) ro vlb  P. Lop  Figure 1: s~rp~n <hnt of tht>. TTAG oarsing workbench. 2.2. Parsers The workbench includes currently two parsers in a parsing test workbench (see screen shot on figure 1): • A bottom-up connection driven parser which can deliver extended partial results compared with other classical bottom-up a!gorithms and without time penalty (Lopez, 2000). • An implementation of the top-down Earley-like parser proposed in (Schabes, 1994). Note that both of them are complete chart parsers, including extraction of results from the shared parse forest and two-step feature based processing. The bottom-up parser gives complete and partial parses considering several parsing heuristics with or without unification of the feature structures used in Feature Based LTAG. lt is also possible to test and compare various parsing heuristics and strategies in tenn of speed efficiency. 2.3. Results The system can deliver and edit different kinds of results: complete parses (derivation and derived trees) or partial parses, with complete unification, with only the first unification steps or without any unification. These different kind of results aims: • To test a grammar by identifying the step involved in the failure of a parse during grammar debugging.  157 LTAG Workbench • To study out of grammar phenomena. The workbench is implemented in Java for portability and reusability reasons. The Java sources, classes and documentation of the editors and parsing test workbench will be freely available by the end of May 2000. We present now another facet of the technical choices conceming the workbench: all the involved data are encoded with the highly portable formalism XML. 3. TagML 3.1. Motivations A significant number of works are based on the TAG forma!ism. Still, for the moment, none has Jed to a common representation format of the grammars which would facilitate the exchange of TAG grammars and associated data or the development of normalised parsers and generic tools. A working group gathering people, mainly from TALaNa (University of Paris 7, France), ENST (Paris, France), INRIA (Rocquencourt, France), LORIA (Nancy, France) and DFKI (Saarbrücken, Germany) who are currently working on this formalism, made it necessary to define a shared and common representation with the aim of exchanging grammars and associated data, developing normalized parsers and specifying generic tools. Our proposal, TagML (Tree Adjoining Grammars Markup Language) is a general recommendation for the encoding and the exchange of the resources involved in LTAG. Anyone implementing a tool on the basis of this encoding can guarantee its interoperability with existing ones. The XTAG system (XTAG research group, 1998), developed in the early nineties, offers the first workbench dedicated to LTAG grammar design and an Earley-like parser. However, this integrated parser provides only a binary answer (accepted or rejected sentence) hardly compatible with the test of a !arge grammar. Partial results and diagnostics about errors are necessary to test a grammar and to identify the ~!e;- hwnlved in the failure of a parse during grammar debugging. Thus, designing a new parser is justified but integratmg ucw -:..:,;;:;~nf'nts to the XTAG system is technically very difficult for someone that has not been involved in the initial development of the system. More generally, this system has not been developed technically to be distributed since it is based on proper and non specified formats. lt requires a narrowly-specialised skill for its installation, its usage and its maintenance. TagML can be viewed as a standardization and an extension of the XTAG formats and more generally as an answer to these technical problems. We present in the following sections the broad outlines of TagML, for more details see (Bonhomme & Lopez, 2000). 3.2. Principles The definition of a generic tool for parsing and managing LTAG grammars supposes a common language specification, shared by the concerned community. The first step toward more generic and flexible tools undergoes the definition of an appropriate encoding for the management of large-size linguistic resources. This encoding should be able to structure possibly heterogeneous data and to give the possibility to represent the inevitable redundancies between lexical data. Given these expectations, we decided to define TagML as an application of the XML recommendation. A LTAG grammar is defined by a morphological Jexicon, a syntactic lexicon and a set of elementary tree schemas. The schema are ordered in tree families in order to capture the general aspects of the lexicalization process. This Jexicalization is obtained on the basis of information given in the syntactic lexicon. For the moment, a complete Document Type Definition (DTD) has been proposed for the schema. In an elementary tree schema, we can distinguish:  158 P. Lopcz • The structural part, i.e. a partial phrase structure or a partial parsing tree. • The set of feature equations constraining top and bottom feature structures. We keep from (Issac, 1998) most ofthe elements involved in the encoding of schema structures: • < t >: elementary tree, document that we specify in this part. • < n >: general node, the attribute cat gives the category of this node and the attribute type distinguishes foot node, substitution node and anchor. • < f s >: feature structure, of type bottom or top • < f >: typed feature (attribute-value) similarly to the TEI. For typed feature equation and their re-usability, we introduce the element linkGrp as specified in the TEI to group intemal or extemal links (element link) (Sperberg-McQueen & Bumard, 1994). 3.3. Structural comporient ofschema Similarly to (lssac, 1998) proposal, we represent straightforwardly the tree structure of a schema by an isomorphy with the XML tree structure (see figure 2).  [ 12.1  </n> </n> </t>  Figure 2: lsomorphy between the elementary tree schema and the XML tree structure  In practice in a broad-covering lexicalized grammar, the redundancy of common substructures is very important. For instance, the subtree dominated by a V category with a depth of 1 (the anchor and the pre-terminal category) is shared by most of the trees describing a verbal syntactical context (several hundred of trees for the English XTAG grammar, several thousand for the French LTAG grammar). This redundancy can be very useful to encode for linguistic or efficiency issues. In order to represent these redundancies, we propose to use the XML Link mecanism (DeRose et al., 1999) and to identify systematically every nodes. We use the principle of virtual resources systematically to obtain only one representation of the different nodes within the whole grammar. 3.4. Feature equatioris The TE! (Sperberg-McQueen & Bumard, 1994) proposes a recommendation for the encoding of feature structures that we propose to integrate to TagML. This standardization allows to type the features and to represent explicitly feature percolation. Note that the features used in the LTAG formalism have atomic values thanks to the extended domain of locality principle. The feature equations of an elementary tree schema can be view as a global term for a complete elementary tree, or as several terms distributed in the various nodes of an elementary tree sharing common variables. We propose to link directly the shared features in order to avoid the necessity to manage shared labels during the parsing of the features structures. These links are specified in JinkGrp.  LTAG Workbench We have the possibility to give a type to a linkGrp, i.e. for a feature subject-verb agreement, then by identifying this linkGrp to share the correspol1dirlg featur~ equation to several elementary tree schemas. If we still consider the example of subject-verb tr6e agreement feature equation, the corresponding JinkGrp will be shared by all elementary schemas that include this kind of agreement. The nodes corresponding to the features linked by percolation can be identified hy a special attribute which gives the function of each terminal node. The access to these specific nodes are obtained with the selection Janguage proposed both for XSL Tranformation Language (Clark, 1999) and for the XML pointers called XML Paths (Clark & DeRose, 1999). As we can see in figure 3, the percolated feature is linked to the JinkGrp corresponding to the feature equation, so it is straightforward to access with this link all the other features which share the same value, without dealing with any labels and tables of Jabels. <n cat•"P" 1ds"n0°> <fs type-=11top11 1d•ufs0"> <f nam.e=„num11 id•11fO"> <link xUnlt: type•"s imple" ><link:hreh"docltid(IO)"/ > </f> <f name•"det• id:=!;11f1 "><minus/>< /f> </f•> <fa type=·bottom" ids„fs 111> 1• „. */ </fe> </n> / * External documen t ~1 < linkGrp type=11accord0 > <link targe tac• id(nO) /fs [1] [@type, top] / f lll [@na.me,nu:n) id(n2) /f& [l] [@type,bottom] /f [l] [@name ,nUlll]" id~"IO" /> </linltGrp> 1• . . . * I Figure 3: Shared features and factorisation of common feature equation 3.5. Tree family In order to manage efficiently a set ofelementary trees that could be quite !arge, TagML provides a mechanism allowing to gather elementarv trP.e.~ sharine; the same sub-categorisation frame. A tree family is described (indicated by the tag < tfamily >) by defining a ser or iiuii.5 iv „ ;;.;~;;.... of elementary tree schemas. The figure 4 presents an example of tree family definition (in this example II _VTA_O and /2_VTD_J B refers to two elementary tree schemas for transitive verbs and I2_adjectif6 and ILadjectifl to two elementary tree schemas for adjective). The encoding of the syntactic lexicon which is much more complex will be the subject offurther research. The current system works with a very basic XML encoding of lexicon closed to the XTAG system flat representation. 3.6. Existing tools Our implementations are based on the Silfide XML toolkit1• The following tools are c~rrently available: • A XSL style sheet allowing the automatic generation of Latex documentation from the TagMLdata. 
/II rhis paper we extend rhe work by Michaelis (1999) which slwws how ro encode an arbirra1}' Minimalisr Grammar in the sense ofStab/er (1997) illto a weakly equivalem mulriple contextfree grammar (MCFG). By viewing MCFG rules as tenns in a free Lawvere the01y we can translate a give11 MCFG into a regular tree grammar. The latter is characteri:able by both a tree automaton and a correspondingfonnu/a in monadic second-order (MSO) logic. The trees of the res11/ti11g regular tree language are then unpacked into the intended "linguistic" trees witlz an MSO rra11sducrio11 based upon tree-walking automata. Tlzis rwo-step approacl1gives an operarional as well as a Jogica/ descriprinn nfthe tree sets involved. 1. Introduction Over the last couple of years, a rich class of mildly context-sensitive grammar formalisms has been proven ro be weakly equivalent. Among others, the following families of (string) Janguages are eguivalent: STR(HR) [languages generated by string generating hyperedge replacement grammars], OUT(DTlVT) [output languages of deterministic tree-wallcing tree-tostring transducers], yDT1c(REGT) [yields of images of regular tree languages under deterministic finite-copying top-down tree transductions), MCFL [Janguages generated by multiple context-free grammars], .HCTA.L [languages generated by multi-component tree adjoining grammars), LCFRL [languages generated by linear context-free rewr.iting systems), LUSCL [languages generated by local unordered scattered context grammars) (more on these eguivaJences can be found, e.g., in Engelfriet 1997, Rambow & Satta 1999, Weir 1992). The work by Michaelis (1999) shows how to encode an arbitrary minimalist grammar (MG) in the sense of Stabler {1997) into a weakly equivalent linear context-free rewriting system (LCFRS). The core idea is that for the set of trees appcnring as intermediate steps in converging derivations corresponding to a given MG one can define a finite partition. Thc eqtüv.alenc~ classes of this partition are formed by sets of trees where the features trigger.ing movement appear in identical structural positions. Each nonterminal in a corresponding LCFRS represents such an eguivalence class, i.e„ an infinite set of trees. We take the resulting LCFRSs as our starting point and present in this paper a translation from multiple context-free grammars (MCFGs)-which are a weakly eguivalent extension of LCFRSs-into regular tree grammars (RTGs)/monadic second-order (MSO) logic/tree automata. This is done via lifting by viewing MCFG rules as terms in a free Lawvere theory. Since this coding makes projection, tupling and composition explicit, the resulting trees contain these operations as labeled nodes. Therefore we use an MSO transduction-where the regular tree language constitutes the domain-to transform the Jifted trees into the intended ones. We think that our approach has decisive advantages. First, the operations of the relevant signature appear explicitly in the Jifted trees and are not hidden in node Jabels coding instances of rule application. Second, our path component is not dependent on the particular regular tree  164  J. Michaelis, U. Mönnich and F. Morawietz  family or the domain defined via the MSO fonnula. The instruction set of the tree-walking automaton and the corresponding definition of the MSO transduction are universal and only serve to reverse the lifting process. In that sense the instructions are nothing eise but a restatement of the unique homomorphism which exists between the free algebra and any other algebra of the same signature. Thus, the translation from MCFGs to RTGs constitutes a considerable simplification in comparison with other characterizations since it is not built upon derivation trees using productions of the original MCFG as node labels, but rather on the Operations of projection, tuple-fonnation and composition alone. In the following sections we Jimit ourselves to the special case of MCFG rules with only one nontenninal on the right hand side (RHS). This allows a significant simplification in the presentation since it requires only one level of tupling. The extension to ehe general case of using tuples of tuples is considerably more involved and, for Jack of space, cannot be described here.  2. Background and Basic Definitions  We first present some basic definitions before we proceed with the actual translation. Let S be a set of sorts. A ma11y-sorted sig11ature E (over S) is an indexed family (Eu,,s j u.; E S•, s E S) of disjoint sets. A symbol in Eu·,s is called an operator of type (tr, s), arity u:, sort s and rank jwj, where lwl denotes the Jength of w. Let X == {x 1, x2 , .r3 , . .. } be a countable set of variables, and for k EI'.\ define Xk as {x1, ...• „r.k} . Then, the set of k-ary trees T(E, Xk ) over Eis built up from xk using the Operators in the. usual way: If a E Et,S u xk for some s E s and € E s· = with !EI = 0 then a is a (trivial) k-ary tree of sort s. If, for some s E S and u: s1 · · ·Sn with Si E S, a E Ew ,s and t1, ... , tn are k-ary trees with t; of sort S; then a(t1, ... , tn) is a k-ary tree of sort s. Note that T(E, X1.J s:;: T(E. X1) for k 5 l. Let T (E, X)= LJkEI.' T(E, Xk )· The operator symbols induce operations on an algebra with the appropriate structure. ,A E- = algebra A consists of an S-indexed family of sets A (.45) sES and for each Operator a E Ew.„ = ap_: Aw -t A.5 is a function, where A.w .4' 1 x · · · x A.'" :!~~ c'.' - '" ::„ ·.-.;;.;, 8; t= ::i. The set T(E, X) can be made into a E-algebra 'f by specifying the operations as follows. For every = a E Eu.·,s• where s E Sand u.; s1 ···Sn with Si E 5, and every f1, ... , tn E T(E, X) with t; of sort s; we identify a,(ti. . .. ,tn) with a(ti, ... ,tn)· Our main notion is that of an a/gebraic (Lawvere) theo1y. Given a set of sorts S, an algebraic theory, as an a!gebra, is an S* x S*-sorted algebra 'f, whose carriers (T(u, V) j u, V E 5 *) consist of the morphisms of the theory and whose operations are of the following types, where n E N, u = u.1 · · · Un with U; E S for 1 $ i $ n and v, W E $•,  projection:  r.f E T(u, u;)  composition: C(u,v,w) E T (u, v) X T(v, w) -t T(u, w) targettupling: ( )(v,u.) E T(v,ui) x ···X T(v,un) -t T(v,u)  The projections and the operations of target tupling are required to satisfy the obvious identities for products. The composition operations must satisfy associativity. For S being a singleton and E a (many-sorted) signature over s• xs•, the power set p(T(L',X)) of T(E, X) constitutes the central example of i nterest for formal Janguage theory. The carriers (p(T(k, m)) 1k, m E IN) of the corresponding s·x s•-Lawvere algebra are constituted by the power sets of the sets T(k, m), where each T(k, m) is the set of all m-tuples of k-ary trees, i.e.T(k,m) = {(t1;.„ ,tm)l t; E T(E,Xk)} ..1 Compositionisdefinedassubstitutionofthe projection constants and target tupling is just tupling. For reasons of space, we cannot go into more details here. More on Lawvere theories in this context and their connection to linguistics can be found in Mönnich (1998).  s• 1Since S is a singleton, can be identified with IN, because up to length each w E S' is uniquely specified.  Derivarional Minimalism in Two Regular and Logical Steps A multiple context-free grammar (MCFG) is defined as a five-tup1e Q = (N, T, F, P, S) with s. T, F and P being a finite set of ranked nontenninals, tennina1s, .linear basic morphisms and productions, respectively. S E N is the start symbol. Each p E P has the form A - t f(.4a ... . , .411 _ 1) for A , .4a, ... , A11-1 E N and J E F a function from (T•)k to (T*)1 with ari ty k = Ef,;a1k; (k; the rank of .4;) and l thnank of .4. (cf. Seki et al. 1991). Recallthat the basic rnorphisms are those which use only variables, constants, concatenation, composition and tupling. = A regular tree grammar (RTG) is a 4-tuple 9 (E, Fa, S, P), where E is a many-sorted signa- ture of i11operatives and Fa a set of operatives of rank 0. S E F0 is the starting symbol and Pis a set of productions. Each p E P has the form F - t t, with F E F0, and t a tenn (tree) over Eu F0. An application of a rule F - t t "rewrites" F as the tree t. Since RTG rules always just substitute some tree for a leaf-node, it is easy to see that they generate recognizable sets of trees, i.e., context-free string languages (Mezei & Wright 1967).2 Afterthese algebraic notions, we briefly present those related to monadic second-order (MSO) logic. MSO Jogic is the extension of first-order predicate logic with monadic second-order variables and quantification over them. In particular, we are using MSO logic on trees such that individual variables x, y, ... stand for nodes in trees and monadic second-order ones X, Y, ... for sets of nodes (for more details see, e.g., Rogers 1998). Before we turn to purely logical notions, we introduce a concept which combines both automata theory and logic. We need a pa1ticular type of finite-state automaton: tree-walking automata witli MSO tests (Bloem & Engelfriet 1997). Intuitively, those automata make transitions from nodes in a tree to other nodes along its branches. A tree-walking automaton (with tests) over ·some ranked alphabet E is a finite automaton o: Q! = (Q, .J, ö, I, F) with states Q, directives .:.1, transitions Q x .:.J -t Q and the initial and final states J s;: Q and F s;: Q which traverses a tree along connected edges using three kinds ofdirectives: i;-"move up to the mother of the current node (if it has one and it is its i-th daughter)", ,J,;-"move to the i-th daughter of the current node (if it exists)", and <,0(x)-"ve1ify that ip holds at the current node". For any tree t E T(E), such a tree-walking autornaton 2! computes a node relation Rt(2! ) = {(x, y) j(x, q;) ,,;. (y, q1) for some q; E I and some q1 E F}, where for all states qk, q1 E Q and nodes x, y in t (x, qk) ==? (y, q1) iff 3d E .J : (qk, d, q1) E ö and y is reachable from x in t via d. Note that x is reachable from itself if the directive was a (successful) test. It is irnportant not to confuse this relation with the walking language recognized by the automaton, i.e„ the string of directives needed to move from the initial to the final node in a walk. Bloem and Engelfriet show that these automata characterize the MSO definable node relations, i.e„ every tree-walking autornaton we specify can be inductively transfonned into an equivalent MSO fonnula and vice versa. The following paragraphs go directly back to Courcelle (1997). Recall that the representation of objects within relational structures makes them available for the use of logical description languages. Let R be a finite set of relation symbols with the corresponding arity for each r E R n given by p(r). A relational structure = (Dn , (rn)ren.) consists of the domain Dn and the p(r)-ary relations rn. ~ D~r>, There does not seem tobe a convenient machine model for tree transfonnations. Fortunately, one can use logic directly to define the desired transduction. The classical technique of interpreting a relational structures within another one fonns the basis for MSO transductions. Intuitively, the output tree is interpreted on the input tree. E.g., suppose that we want to transduce the input tree t 1 into the output tree t 2 . The nodes of the output tree t2 will be a subset ofthe nodes from t 1 specified with a unary MSO relation ranging over the nodes of t1. The daughter relation will be specified with a binary MSO relation with free variables x 2Appropriate definitions for derivations and the tree languages generated can be found in Kalb et al. (2000).  166  J. Michaelis, U. Mönnich and F Morawietz  C(0,3.1)  ~  c (3.2.11 Aco,3) ~  S co.11 ---1  • r2. 11 ( l13.2J ~ r.f c3.11 crs.2.11 ~  ' (2.1) ( ) (3.2)  <'----3 r.2(3.1) r.3 (3.1)  --------C(Q.3,3)  ( )rs.3)  .4co.s)  ___-r--  Aco.3J--+  C(3.2.l )  C(J.2.1)  C(3.2.1)  ~  ~  ~ 
Tree adjoining grammars (TAG) represent a derivational formalism to construct trees from a given set of initial and auxiliary trees. We present a logical language that simultaneously describes the generated TAG-tree and the corresponding derivation tree. Based on this language we formulate constraints indicating whether a tree and a derivation tree mean a valid TAGgenerated tree. A method is presented that extracts the underlying TAG from an (imderspecified) TAG-tree and its derivation. This leads to an alternative approach of representing shared structures by means of TAGs. The result is a more general representation ofmovement which requires no indices since it basically makes use ofthe properties ofthe adjunction operation. 1. Introduction Recently, we find several approaches establishing a logical description of finite trees, e.g„ firs torder logic (Backofen et al„ 1995), dynamic logic (Kracht, 1995), temporal logic (Pahn, 1999), monadic second-order logic (Rogers, 1998). However, most of them lead to the class of recognizable sets of trees (Thatcher & Wright, 1968). Provided a finite labe! domain this applies to all logical formalisms that are equal or weaker than the (weak) monadic second-order logic (Rabin, 1969). However, TAGs do not belong to this class, since TAGs are properly stronger than context-free grammars. But a set of trees is recognizable if and only if it can be recognized by tree automaton, which can be also encoded as a context-free grammar. Nevertheless, there are logical formalisms to specify structures beyond context-free derivations. For instance, Rogers proposes in (1999) and previous works a logical description of TAGs that is based on a 3-dimensional view of trees. The important issue of his approach is to combine the derived TAG-tree and its derivation tree to a single 3-dimensional structure. Similarly, we propose a formal method to establish tree constraints outside the context-free paradigm that employs an additional tree structure that is linked with the tree in a particular manner. For TAGs we consider the corresponding TAG-derivation tree where each node of the derived TAG-tree is linked with the corresponding derivation node, e.g„ if we adjoin the aux- iliary tree ß to the auxiliary tree a then we reach a derivation tree with the root ma that has a single child mp. Correspondingly, we link each node of the underlying initial tree a with the ma node in the derivation tree and each node of the adjoined auxiliary tree ß with the mp node. Instead of labeling the nodes of the derivation tree with the name of the corresponding elementary tree and the tree address of the corresponding adjunction node, the former is sufficient due to these links. After adjoining a further ß tree to the forrner ßtree, the derivation tree includes a second mp node below the first one. In addition, the nodes of the second ß tree are linked with the second mp node of the derivation tree. Obviously, the dominance relation in the derivation tree expresses nested auxiliary trees in the derived TAG-tree. In contrast to Rogers' 3-dimensional trees, we keep the derived TAG-tree as a unit in order to  172 Adi Paim  a:S'  ß1: s·  ßz : s·  s 1  /1 a S  b/1s  
We sho1v that the co11struction ofproof11ets in the implicative fragment of intuirionistic linear logic reduces to the ge11eratio11 ofmodels in the shape of completely specifted a11d neutral trees from polarised tree descriptions. This provides 11s with a 11e111fram eworkfor revisiti11g grammatical formalisms and leads us to imrodr1ce l11teractio11 Grammars which aim to take advalltage of two main characterisrics ofthisframework: under-speciftcation and polarities. Introduction Apparently, Categorial Grammars (CG) and Tree Adjoining Grammars (TAG) are two very different approaches to the syntax of natural languages. CG are characteri sed as calculi of polarised syntactic types based on the idea that grammatical categories are consumable resources: some constituents behave as resource consumers whereas others behave as resource providers so that syntactic composition is viewed as a process in which consumers and providers try to cancel each other out; most often, CG are expressed in a logical framework that takes the Lambek Calculus as its nucleus, which combines resource sensitivity with order sensitivity. This intimate combination, which explains the central role of this logic as a framework for CG, is at the same time a cause of rigidity which limits its expressive power greatly. The search for an appropriate way of relaxing this framework constitutes an important research area of CG (Moo96). TAG do not manipulate syntactic types but syntactic trees with the adjunction operation as their comerstone. In this way, their expressivity goes beyond that of CG but their rigidity is also their weak point: like CG, they are lexicalised and all syntactic configurations in which a ward is used are stored in the lexicon in the fonn of elementary trees. As soon as a ward is used in a new syntactic configuration, a new elementary tree must be added to the lexicon directly or via a Jexical rule. In this way, lexicons quickly become colossal and very awkward to manage. Recent works have contributed to establish links between CG and TAG with the cornmon aim to ernbed TAG in a logical setting (AFV97; JK97). Our proposal aims to provide a common framework for comparing CG and TAG and for overcoming some of their specific limitations in a new foITnalism which we call lnteraction Grammars (IG). The common framework that we choose is that of tree descriptions. This notion is not new in the TAG community since it was introduced by (VS92) for making adjunction monotone and ernbedding TAG in a unification framework. The key idea behind this notion is to replace reasoning about syntactic trees as . completely defined objects with reasoning about properties which are used for characterising  178  GuyPerrier  these trees; in this way, syntactic trees are viewed as models of descriptions. This allows one to use the notion of under-specificarion in a fruitful manner for structuring TAG Jexicons (Can99) or for dealing with semantic ambiguity (MK; ENRX98) for instance. This also allows a new and promising con straint-based style of computing within linguistics (? ; DT99; Bla99). We propose to show that CG can be revisited in this framework with new developments which Jead us to IG. The starting point of this proposal is purely theoretic since it concerns proof theory in lntuitionistic Linear Logic (ILL). 1. Intuitionistic proof nets as polarised tree descriptions Resource sensitivity of linear logic entails a specific form of proof: proof nets (Gir87). In the general framework of classical linear logic, these proof nets are not directed so that each extremity of a proof net can be viewed as either an input or an output; in other words, each formul a that is attached to an extremity of a proof net can be considered either as an assumption (input) or as a conclusion (output) of the proof. In ILL, this symrnetry is broken and thi ngs freeze in a configuration where all formul as becorne polarised, one as the output (denoted +) and the others as the inputs (denoted -). F. Larnarche has devised a correctness criterion for these proof nets which takes their specificity into account (Larn96). Hence, he has sketched a rnore abstract representation of proof nets which is inspired by the games semancics for PCF introduced by (H093) and which only takes the induced order between atomic formulas into account. By using the notion of tree desc1i ption, we propose to perfect this representation for lmplicative Intuitionistic Linear Logic (IILL), which is the irnplicative fragment of ILL, built only from the linear implication (~); we choose this fragment because of its linguistic interest but our proposal can be easily extended to the whole multiplicative fragment. [.J. Syntactic descriptions ofIILLformulas Let 'P be a set of propositions. The set of IILL formulas built from 'P is defined by the grammar :F ::= 'P 1 :F ~ :F. By adding a polarity + or - to every IILL formula, we obtai n the set :F('P) of polarised IILL formulas. From the syntax of these formul as, we abstract particular tree descriptions, called IILL symactic descriprions. Definition 1.1 An IILL syntactic descriprio11 D is a set of polarised atomic fonnulas taken from :F('P) that is equipped wirh rwo binaJ)' relarions: dominance (denoted >") and immediate dominance (denored >). For every polarised IILL formula FP (p represents the polarity + or - and -p its opposite), we build its syntactic description, denoted D(FP) from the root, denoted Root(D(FP)), to the leaves recursively according to the following definition. Definition 1.2 D (F1' ) is an IILL syntactic descriptio11 such that: • ifFP is aromic, the11 D(F1') is reduced to the 1mique element FP, the two relatiOns >" and > are empty and Root(D(FP)) = FP; • iJ F'P = (F1 ~ F2 )P, then D(FP) is the disjoinr union ofD(F1-p) and D(Ff) where the relations >" a11d > are completed with a relation between Root(D(F1- p)) and Root(D (Ff) ) according to rite f ollowing rule: - ifp=+, then Root(D(F2+)) >" Root.(D(Fn) and Root(D(FP)) = Root(D(F/));  From lntuitionistic ProofNets to lnteraction Grammars  - ifp=-. then Root(D(F;)) > Root(D(Fn) and Root(D(FP)) =  According to the previous definition, an IILL syntactic description has a very particular shape it appears as a hierarchy of Ievels which altemate positive and negative fonnulas and, at the same time, dominance links and immediate dominance links between them.  1.2. Provability i11 IILL as validity ofsyntactic descriptions Syntactic descriptions are interpreted on trees according to the following definition:  Definition 1.3 A tree T is a model of a symactic description D if there is an interpretation I from D to T such that: • For eve01 11ode N of T, 1-1(N) is composed ofexactly two elements of D: F+ and F-. • For e1•e01 pair (Ff', Ff2 ) ofD, Ff' > Ff2 (Ff1 >" Ff2 ) emails that !(Ff' ) is the parent (an ancesror) of I(Ff2) in T. Ifa description D accepts a model, D is said tobe valid.  In others terms, a syntactic description is valid if one can merge its nodes by dual pairs while  respecting its dominance constraints. Equivalence between provability of IILL sequents in lin-  ear logic and validity of the corresponding syntactic descriptions is established by the following  theorem.  ·  Theorem 1.1 An l!LL sequent F 1 , ... , Fn 1-- G is provable in linear logic if and only if the syntactic description D((F1 --o · • • --o Fn --o G)+) is valid.  Sketch of proof 1.1 To slww rhat provabilityentails validity, we proceedby i11ductio11 an proofs of IIIL sequents in the linear sequem ca/culus. We consider the last inference I of any proof of such a sequent. By inductio11 hypothesis, we get models ofthe symactic descriprions ofthe l-premises and it is not ve1y difficult to co111bi11e rhese models ro bui!d a model ofthe syntactic description ofthe l -conclusion. Ta show that validity emails provability, we proceed by induction 011 the manber of nodes of syntacric descriptions. We consider any valid descriprion of an IILL f onnula F . We drop the root R + of the description and irs dual node R- which march in a model T; all partial descriptions D(Ft) which become u11co1111ecred in this way are /inked to the children of Rthat domi11ate tlzem in the model T. /11 this way, we obtain a set of valid syntactic descriptions to which we can apply the induction hypothesis; as a co11seque11ce, we obrain a set ofprovab/e sequemsfrom whic/1 we deduce J-- F.  Example 1.1 The transitivity of li11ear implication is expressed by the provability of rhe IILL sequent a --o b, b --o c J-- a ---<> c, whiclz amounts to the provability ofthe 011e-sided sequent 1-- (a --ob) --o (b --o c) --o (a --o c). From the left to the right, Figure I successively presents the proofner which esrablishes rhis provability, tlze correspo11di11g syntactic description d.nd the model 1 which guarantees rhe validity ofthis description. In the proofnet, positivefonnulas are represented by down arrays and negative f onnulas by up arrays; axioms links are represented by dotted edges.  Proof search, which, in IILL, takes the fonn of proof net construction, now reduces to the generation of models from syntactic descriptions; some details are forgotten while essentials 1The model is unique up to an isomorphism.  180  Guy Perrier  ... . . . . .C:•·„„.. . ..  ,_  c-  ...  
When people develop something intended as a !arge broad-coverage grammar, they usually have a more specific goal in mind. Sometimes this goal is covering a corpus; sometimes the developers have theoretical ideas they wish to investigate; most often, work is driven by a combination of these two main types of goal. What tends to happen after a while is that the community of people working with the grammar starts thinking of some phenomena as "central", and makes serious efforts to deal with them; other phenomena are labelled "marginal", and ignored. Before Jong, the clistinction between "central" and "marginal" becomes so ingrained that it is automatic, and people virtually stop thinking about the "marginal" phenomena. In practice, the only way to bring the marginal things back into focus is to look at what other people are doing and compare it with one's own work. In this paper, we will take two !arge grammars, XTAG and CLE, and examine each of them from the other's point of view. We will find in both cases not only that important things are missing, but that the perspective offered by the other grammar suggests simple and practical ways of filling in the holes. lt tums out that there is a pleasing symmetry to the picture. XTAG has a very good treatment of complement structure, which the CLE to some extent Jacks; conversely, the CLE offers a powerful and general account of adjuncts, which the XTAG grammar does not fully duplicate. If we examine the way in which each grammar does the thing it is good at, we find that the relevant methods are quite easy to port to the other framework, and in fact only involve generalization and systematization of existing mechanisms. The paper is structured as follows. Section 2 presents a very brief overview of the CLE and XTAG grammars. In Section 3, we describe the CLE grammar from the XTAG grammar's point of view, following which Section 4 describes the XTAG grammar from a CLE perspective. Section 5 concludes. 2. An Overview of the XTAG and CLE Grammars The CLE and XTAG grammars for English are extensively described elsewhere (Pulman, 1992; The XTAG-Group, 1995), and this section will only present the briefest possible summary. Both grammars make a serious attempt to cover all major syntactic phenomena of the langua,ge; the CLE grammar also associates each syntactic construction with a compositional scope-free semantics expressed in Quasi Logical Formnotation (van Eijck & Alshawi, 1992). In particular, both grammars provide good coverage of the following: NP structure: Pre- and post-nominal adjectival modification, postnominal modification by PPs, relative clauses, -ing and -ed VPs, comparative and superlative adjectives, possessives,  186  Manny Rayner, Beth Ann Hockey, Frankie James  complex detetminers, compound nominals, time, date and code expressions, numbers, "kind of" NPs, determiner and .t\1BAR ellipsis, sentential NPs, apposition, conjunction of NP. Clausal structure: A !arge variety of verb types, including intransitives, transitives, ditransitives, copula, auxiliaries, modals, verbs subcategorizing for PPs, particles, embedded clauses, raising and small clause constructions, and combinations of the above; VP modification by PPs, verbal ADVPs, -ing VP, "to" VP declaratives, imperative, WH-questions and Y-N questions; clefts; passives; sentential ADVPs; topicalization; negation; embedded questions; relative clauses; conjunction.  3. \Vhat XTAG Teils Us About the CLE Grammar Both grammars are explicitly lexicalized in a way that makes it easy to define a wide valiety of types of complement structure. The XTAG grammar detines complement structure through the very flexible and general mechanism of initial trees combined with the adjunction operation for introducing recursion. Very briefty, each initial tree defines one possible complement structure for its head. Complements can be specified as substitution nodes, with features constraining the possible constituents that can be substituted in; altemately, they can be specified as adjunction nodes, which allow auxiliary trees to be adjoined onto them. CLE grammar, in contrast, defines complement structure through rule schemas. For example, the VP rule schema is of the form  VP --t V:[subcat=COMPS] l COMPS  the right hand side of which can be glossed as "V whose <subcat> feature has value COMPS, followed by a Jist of constituents which unify with COMPS". From a TAG perspective, COMPS is more or less equivalent to a !ist of substitution nodes; there is nothing corresponding to adjunction nodes. The CLE grammar can get along without the adjunction operation, which is absolutely central to XTAG, because it has a powerful mechanism for handling long-range dependencies based on the idea of "gap-threading" (Pereira, 1981; Karttunen, 1986; Pulman, 1992). From the XTAG point of view, it is none the less hard to believe that substitution nodes on their own will be capable of modeling an equally broad range of complement structures. lt does indeed appear tobe the case that certain types of complements, particularly those related to idioms and light verbs, are difficult to capture in the CLE framework whereas there is an obvious way to treat these in XTAG. The most convincing example we have identified so far is the class of constructions, very common in English, involving a combination of a verb, a possessive, and a noun, for instance sliake 011e's head, c/ose one's eyes, slzrug one's shoulders, take one 's time, lzave one 's way. In all of these constructions, the NP's detenniner must be a possessive pronoun agreeing with the verb, and it is in general possible to modify the NP (shake his pretty head, shrug her powerful slzoulders, have lzis silly way). It is obvious that take one 's time and have one 's way should be treated as light verb constructions and there are good arguments for modeling the less obvious cases such as shake one's head, close one's eyes and slzrug one's slzoulders as idioms or light verbs as weil, rather than just viewing them as instances of the general transitive verbs shake, close or shrug. For instance, modeling them as idioms or light verbs would be an advantage in the context of a transfer-based ni"achine translation system. Few languages express these concepts in the same way as English1 and a straight forward compositional treatment will lead to serious complications in defining the associated transfer rules. 1for example, c/ose 011e's eyes isfermer /ex yeux in French (transitive verb +definite NP) and blu11da in Swedish (intransitive verb)  A comparision ofthe XTAG and CLE Grammars for English Coding the constraints needed to capture these constructions as idioms is unproblematic ii1 XTAG: for e·xample the initial tree for have one's way will be roughly of the form Sr[] (agr : <3>]  NPo.!. (agr : <1> [J]  VP (agr : <3> [J] (agr : <2>]  V ~gr : <2> (] []  NP 1 (] []  ~  have  Dt.!. [ref: [agr : <1>]] Ni[]  []  
 O(n) - bounded state TAGs e.g. the  We present an implementation ofa chart-based  = usual grarnmar G where L(G )  head-corner parsing algorithm fo r lexicalized  {an bn e cn dn 1n 2:: O} (see (Joshi et al.,  Tree Adjoining Grammars. We report on some  1975))  practical experiments where we parse 2250  sentences f rom the Wall Street Journal usin.g The grammar iacturs a1c = .:.-v:!.:,;·;:: S:~:!bes' this parser. In these experiments the parser Earley-style algorithm takes O(IAI IIUAINn6 ) is run without any statistical pruning; it pro- worst case time and O(IAU IINn4 ) worst case  duces all valid parses f or each sentence in space, where n is the length of the input, A is  the form of a shared derivation forest. The the set of auxiliary trees, I is the set of initial  parser uses a Zarge Treebank Grammar with trees and N is maximum number of nodes in  6789 tree templates with about 120, 000 lexi- an elementary tree.  calized trees. The results suggest that the ob- Given these worst case estimates we wish to  served complexity ofparsingfor LTAG is dom- explore what the observed times might be for a  inated byfactors other than sentence length. TAG parscr. It is not our goai here to compare  1. Motivation  different TAG parsing algorithms, rather it is to discover what kinds offactors can contribute to  The particular experiments that we report on parsing time complexity. Of course, a natural-  in this paper were chosen to discover certain language grarnmar that is !arge and complex  facts about LTAG parsing in a practical setting. enough to be used for parsing real-world text  Specifically, we wanted to discover the impor- is typically neither unambiguous nor bounded  tance of the worst-case results for LTAG pars- in state size. It is important to note that in this  ing in practice. Let us take Schabes' Earley- paper we are not concemed with parsing ac-  style TAG parsing algorithm (Schabes, 1994) curacy, rather we want to explore parsing effi-  which is the usual candidate for a practical ciency. This is why we do not pursue any prun-  LTAG parser. The parsing time complexity of ing while parsing using statistical methods. In-  this algorithm for various types of grammars stead we produce a shared derivation forest for  are as follows (for input of length n):  each sentence which stores, in compact form,  O(n6) - TAGs for inherently ambiguous lan- all derivations for each sentence. This helps  guages  us evaluate our TAG parser for time and space  efficiency. The experiments reported here are  O(n4) - unambiguous TAGs  also useful for statistical parsing using TAG  'I would like to thank Aravind Joshi, Carlos Prolo since discovering the source of grarnmar comand Fei Xia for their help and suggestions. This work plexity in parsing can help in finding the right was partially supported by NSF Grant SBR 8920230. figures-of-merit for effective pruning in a sta-  194  Anoop Sarkar  tistica1 parser.  '"  2. Treebank Grammar  "'  
In natural language generation, the use of a lexicalized grammarfonnalism and increme11tal syntactic and semantic processing places strong a11d specific constraints 011 the fonn and meaning of grammaticaf entries. These principles restrict which grammatica/ representations are possible and suggest examples an analyst cmi consult to decide among possibilities. We discuss and j11stify a number of such constraints, and describe how they i11fo1111 the design of lexical entries for motion verbs. Our entries allow a generator to match the lexical choices found in a target co1pus ofactio11 descriptions by assessing how th~ f::•n.:n..n~.-:~::::: ::j:; :·:;:·!; ;;, ;;;;::~n,„ contributes towards the hearer 's identification ofthe intended action. 1. Introduction This paper originates in a project of tailoring a natural language generation system called SPUD, for sentence planning using description (Stone & Doran, 1997), to generate instructions for action in a concrete domain. The desired behavior for the system is specified by a corpus of edited, naturally-occurring action instructions whose form and content the system must mirror. The input to the system consists of three components: a representation of the context in which instruction is to be issued; a set of communicative goals describing the content that the instruction should make available to the audience; and a database of facts describing the GENERALIZED INDIVIDUALS such as paths, places and eventualities involved in the action (Bach, 1989; Hobbs, 1985). The task is further complicated because the content and organization of this input database must suit a variety of other tasks, such as animation (Badler et al„ 1998). Such a generation task demands a detailed model ofhow the available input determines appropriate linguistic elements to arrange in output. The problem of LEXICAL CHOICE illustrates this. English offers a wide range of verbs to describe events in which an agent moves some object along a path; any motion instruction obliges the generator to choose just one. Uses of verbs differ syntactically in the kinds of optional elements that accompany them; they differ semantically both in the constraints they place on the motion event itself and in the links they establish between the event and the speaker and hearer's mutual knowledge ofthe environment. As we shall see, often many verbs, in many syntactic frames, can truly and appropriately describe • The bulk of 1his work was performed while thc authors were localed at and supported through IRCS, Penn (NSF-STC SBR 8920230). Thanks to Aravind Joshi, Alistair Knott and Bonnie Wehber.  200  Matthew Stone, Tonia Bleam, Christine Doran, Martha Palmer  each event. Nevertheless, we find a constrained and consistent pattern of Jexical choice across naturally-occurring instructions. In order to mirror lexical choice in SPUD. we must provide a computational account of Jexical items through which SPUD can exhibit the same consistency. SPUD is based on the widely-espoused view that sentence generation is goal-directed activity (Appelt. 1985; Dale, 1992; Moore, I 994; Moore & Paris, 1993); SPUD's repertoire of communicative action is determined by a declarative Jexicalized grammar. To plan a sentence, SPUD searches among the derivations admitted by the grammar for a true sentence whose interpretation achieves the system's communicative goals in the current context. Clearly, then, to mirror a specified corpus ofinstructions, the grammar provided to SPUD must characterize the words and constructions used in the corpus accurately and comprehensively. lt must describe forms syntactically. so that they are combined appropriately, but it must also describe them semantically and pragmatically. in order to support a useful assessment of interpretation. In this paper we articulate a methodology for constructing Jexicalized grammatical resources for generation systems such as SPUD, and show how this methodology allo ws us to ensure that SPUD deploys its lexical and syntactic options as observed in a corpus of desired output. Our methodology involves guidelines for the construction of syntactic structures, semantic representations and the interface between them, but the basic principle behind all of these guidelines is this: THE REPRESENTATJON OF A GRAMMATICAL ENTRY MUST MAKE IT AS EASY AS POSSIBLE FOR THE GENERATOR TO EXPLOIT ITS u;;..;,-i'.;;:::;:;0'-1 !~! c:";;."-·;~::::; -:- 1_i-r FlJRTHER PLANNING. This principle responds to two concerns. First, our research has revealed many characteristic uses of Janguage in which a single entry helps achieve multiple communicative goals (Stone & Webber. 1998). This is an important way in which a generator needs tobe able exploit the contribution of an entry it has already used , in line with our principle. Second, SPUD is currently constrained to greedy or incremental search for reasons of efficiency. At each step. SPUD picks the entry whose interpretation goes furthP~t rrnv<> r'!~ ~~!-;;.:·::;:; ::~ ~'"'" "u";"'a';. ~ goals. As the generator uses its grammar to build on these greedy choices, our principle facilitates the generator in arriving at a satisfactory overall utterance.  2. Syntax  We collected occurrences of the verbs slide, mtate, push, pull. lift, co1111ect, disco1111ect, remove, position and place in the maintenance manual for the fuel system of the American F 16 aircraft; in this manual, each operation is described consistently and precisely. Syntactic analysis of instructions in the corpus and the application of standard tests allowed us to duster the uses of these verbs into five syntactic classes; these classes are consistent with each verb's membership in a distinct Levin class (Levin, 1993). Differences among these classes include whether the verb lexicalizes a path of motion (rotate), an endpoint (position), or a change of state (disconnect); and whether a spatial complement is optional (as with the verbs just given) or obligatory.(place). The data in (1) illustrate these alternatives.  (1) a b c d e f g  Rotate valve one-fourth turn clockwise. [Path) Rotate halon tube to provide access. [No path] Position one fire extinguisher near aircraft servicing connection point. [Endpöint] Position drain tube. [No endpoint] Disconnect generator set cable from ground power receptacle. [Change of state] Disconnect coupling. [No source argument) Place grommet on test set vacuum adapter. [Endpoint, required]  We crafted syntactic entries for these verbs as trees in Lexicalized Tree- Adjoining Grammar, LTAG (Joshi et a/., 1975; Schabes, 1990). Our entries respecl three requirements that reflect the analvsis of the cornus and the generator's need to huild on the svntax of entries it ~Plt>:rl,.  Lexicalized Grammar and the Description of Motion Events  J. The grammar must associate each verb with its observed range of complements and ifiers, in the observed orders.  2. All optional e!ements, regardless of interpretation, must be represented in the syntax as modifiers, using the LTAG operation of adjunction. This allows the generator to select an optional element when it is needed to achieve communicative goals not otherwise satisfied . Recall that, in LTAG, a substitution site indicates a constituent that must be supplied syntactically to obtain a grammatical sentence; we call a constituent so provided a SYNTACTJC ARGUMENT. The alternative way of elaborating a sentence is to rewrite a node so as to include additional material (generally optional) specified by an auxföary tree; we call material so provided a SYNTACTIC ADJUNCT. If optional elements are represented as syntactic adjuncts, it is straightforward to select one whenever its potential benefit is recognized. With other representations-for example, using alternative syntactic entries some of which include a syntactic argumenc position (substitution site) for the "optional" constituent-the representation can result in artificial dependencies or even deadend paths in the search space in generation. To use this representation successfully, the generator would have eo anticipate how the sentence would be fleshed out later in order to select the right entry early on.  3. The appropriate order of complements a nd modifiers for a verb must be represented using hierarchies of nodes in the verb's elementary tree. In a fixed word-order language like English, the nodes we add reflect diffe rent semantic classes which tend tobe realized in a particular order: in a free word-order Janguage, we might instead introduce ordering nodes based on information-structure status. Introducing such nodes decouples the generator's search space of derivations from the overt output word-order. It a!Jows the generator toselect complements and modifiers in any search order, while still realizing the complements and modifiers with their correct surface order. Again, alternative designs-representing word-order in the derivation itself or in features that clash when elements appear in the wrong order- introduce dependencies into the search space for generation that make it more difficult for the generator to build on its earlier choices successfully.  The latter requirements induce certain differences between our trees and other LTAG grammars forEnglish, such as the XTAG grammar(Doran et al., 1994), even in cases when the XTAG trees do describe our corpus. For example, we associate slide with the tree in (2); the structure reflects the optionality of the patlr constituent and makes explicit the observed characteristic order of constituents specifying path (PTH), duration (DUR) and purpose (PRP).  s ~ NP VP(PRP)  
This paper presents a possibility to extend the fonnalism of linear indexed grammars. The extension is based on the use of tuples ofpushdowns instead ofone pushdown to store indices during a derivation. If a restriction on the accessibility of the pushdowns is used, it can be shown that the resulting fonnalisms give rise to a hierarchy of languages that is equivalent with a hierarchy defined by Weir. For this equivalence, that was already known for a slightly different fonnalism, this paper gives a new proof. Since all languages of Weir's hierarchy are known to be mildly context sensitive, the proposed extensions ofL!Gs become comparable with extensions oftree adjoining grammars and head grammars. 1. Introduction lt is weil known that tree adjoining grammars (TAGs), head grammars (HGs) and linear indexed grammars (LIGs) are weakly equivalent (Vijay-Shanker & Weir, 1994). Each of these formalisms was developed independently for the description of natural languages. For TAGs and HGs hierarchies of extensions were defined by increasing the number of auxiliary trees that are inserted in one step and by increasing the size of the tuples that are handled, resp. (cf. (Weir, 1988)). The extensions of TAGs, multi-component TAGs (MCTAGs) (Joshi, 1987), were argued tobe useful for the description of natural languages by Kroch ( 1987) and Kroch and Joshi (1987). For LIGs a Iinguistically motivated extension is defined by Rambow (l 994) that is however of a rather different nature than the extensions of HGs and TAGs and does not give rise to a hierarchy of formalisms and language classes. Weir (1988; 1992) defines a hierarchy of linear controlled grammars that are strongly related to LIGs. lt is however not immediately apparent what use these formalisms could have for linguistics. In (Wartena, 1998) recently extensions of LIGs, called context-free linear multi-pushdown grammars (CFL-MPD-Gs), were defined that use tuples of pushdowns to store indices instead of a single pushdown. The use of tuples was motivated by linguistic needs. These extensions form a hierarchy of formalisms with an increasing number of pushdowns. lf no pushdown is available the grammars are strongly equivalent to context-free grammars. If one pushdown is used we obtain LIGs. The nth element of the hierarchy can be shown to be a subclass of the nth class of Weir's hierarchy of controlled languages. CFL-MPD-Gs seem to fill up an apparent gap in the square formed by TAGs, HGs and LIGs on the first axis and their extensions on the other axis. In order to formally justify this square we have to show that CFLr-MPD-Gs and MCTAGs1 or the extensions of head grammars are equivalent. (The equivalence between the last two was shown by Weir (1988)). We will go 1There are two variants of MCTAGs, the first ofwhich allows only for simultaneous adjunction in one elemen-  208  Christian Wartena  the following way to show this equivalence. First we will prove the equivalence between the hierarchy of CFL-MPD-Gs and Weir's hierarchy of linear controlled grammars. Subsequently the equivalence between the latter hierarchy and MCTAGs has to be shown. In this paper we will do the first of the two steps. 2. Grammars with storage LIGs store their indices in pushdowns. For the description of non-Iocal dependencies in natural languages this organization can be argued to be too restrictive. Thus we might want to define formalisms similar to LIGs but with a more liberal Stack structure. We start defining abstract storages, that will form the base of the subsequent extensions. Definition 1 (storage) A storage is a 6--tuple S = (C, Co, CF, P, F, m), where C is a set of configurations, Co ~ C and CF ~ C the sets of initial and final configurations, respectively, P is a set of predicate symbols, F a set of instruction symbols. m is the meaning function, which associates every p E P with a mapping m(p) : C -+ { true, false} and every .f E F with a partial function m(.f) : C -+ C. Usually we are interested in properties of classes of storages rather than in properties of individual ones. Classes of storages are often called storage types.  Example 1 A trivial storage is defined as Striv = ({c},{c},{c} , 0, {id},m), where c is an = arbitrary object and m(id)( c) c. Tue class of all trivial storages is denoted 6tri\'·  Example 2 A pushdown over some finite alphabet r can be defined as a storage2 Sµd(f) = (r-, {€},{t:},P, F ,m) with P = {top(!)l1 Er}, F = {push(i) h Er} U {pop} U {id} and for every a E f and ß E r·,  m(top(i))(aß) = (a = 7) m(push(i))(ß) = 1ß  = m(pop)(aß) ß m(id)(ß) =ß  The class of all pushdowns is denoted 6pd·  On the base of this.notion of storages we can define context-free linear-S grammars (CF&-SGs) as a generalization of LIGs.  Definition 2 (CF linear S-gramrnar) If S = (C, C0 , CF, P, F, m) is a storage then a contextfree linear S-grammar is a five tuple G = (N, I:, R, Ain, c0 ), where N, I; denote the sets of nonterminal and terminal symbols, respectively. Ain E N is a distinguished symbol, called the start symbol, Co E Co is the initial configuration, and R is a set of production rules of the following two forms:  A -+ if 7r then ( 1 (B, .f)(2 A ---+ if 7r then w  where A, BEN, 7r E BE(P)) and (i.(2 E (N U I;)•, f E F, w EI:*. BE(P ) denotes the set  ofboolean expressions over P.  ·  tary tree, the second ooe of which allows for adjuoction of a tuple in a tuple of elementary trees as weil. The first variant is equivalcnt to (simple) TAGs, the second ooe gives rise to an hierarchy of languages. In this paper we will only consider these more powerful MCTAGs. 2Throughout the paper the following notational conventions are used. The empty string is denoted by c. For each sei V the notatioo V, is used as an abbreviation for V U {c}.  
 2. Extracting MC sets from  This paper presents a new methodology f or  the Theebank  examining cases of non -locality. The algo-  rithm presented here allows us to extract Extracting multi-component (MC) tree sets  from a Zarge annotated corpus sentences that from Treebanks is one of the tasks per-  appear to require non-local MCTAG. W e formed by a grammar development system  examine one such case, extraposition from named LexTract, whose structure is shown  NP, and o.rgue that the dependency involved in Figure 1, with the components relevant  is not syntactic and theref ore does not re- to the MC extraction task marked in hold.  quire non-local MCTAG.  There are three main steps in the MC ex-  traction procedure: füst, a bracketed struc-  1. Introduction  ture in a Treebank (ttree) is decomposed into a set of elementary trees (etrees); sec-  Mnch important work has been done to in- ond, a derivation tree is built to show how  vestigate the adequacy of local TAGs to ac- the etrees are combined; third, any pair of  count for various linguistic phenomena, see, etrees that coutain co-indexed components  e.g., (Heycock, 1987; Becker et al. , 1992; are placed in a trees set with the etrees that  Abeille, 1994; Bleam, 1994; Kulick, 1998; connect them in the derivation tree. If t he  Joshi et o.l„ 2000). This paper presents a size of the set is more than three, the re-  new methodology for doing this kind of re- lation between the co-indexed components  search. The algorithm presented here allows is not tree-local, assuming the correctness  us to extract from a !arge annotated cor- of Treebank annotations. For lack of space,  pus (the Penn Treebank) constructions that we will use an example to demonst rate these  seem to require non-local1 derivations. We main steps without going into the details of  propose that, in fact, these non-local depen- the algorithms (see (Xia, 1999) for details).  dencies should not be represented syntacti-  cally, and t herefore do not constitute a problem for maintaining tree-local MCTAG.  2.1.  The extracted grammar .  To ensure that the extracted etrees are com-  pact and linguistically sound, we require  •we would like to thank Aravind Joshi, J eff Lidz, Anoop Sarkar and the XTAG resea.rch group for t heir help and suggestions. This work was sup-  that each etree in the gramrnar fall into one of three types determined by the relations  ported by NSF Grant SBR 8920230.  between the anchor of the etree and other  1By non-local, we mean non-tree-local.  nodes in the tree, as shown in Figure 2:  216  Fei Xia, Tonia Bleam  ;-···~:~~~~·~;:;;~·································~  :  ! . - n l ac:J ~  : LTAGa  Tm:N.llU tttellri.·~lk~ u1ftln!l.aw. •  tTUl'PMttli<'•"°tfl tln'CtHflllU QnrJ c\TW flfQS •.,-..~~ d<"w-a. Trcc:hmk. - - - - - - - - - annotatlrwi clTilJli ;,,,... ..••.„ •....•....• ••••... ...• ••••••.••••••..•••....:  Figure 1: The structure of LexTract  x• \/'L'~ ...X,~I ).: t " - x(I zr1 
This paper addresses linguistic and implementation problems for a practical LTAG parser raised by rich morphology in Korean. We propose a way ofrepresenting the Korean inftectional system asfeature structures in lexicalized elementary trees, and describe our implemented modijicarions on the XTAG system for a more efficient grammar development for Korean. 1. Issues Korean is an agglutinative language with a very productive infiectional system. lnfiections include postpositions on nouns; tense morphemes and endings that indicate sentence types on verbs and adjectives; among others. Furthermore, these inftections can combine with each other to form compound infiections.  (1) Noun a. hakkyo-ka school-Nom b. hakkyo-eyse-ka school-from-Nom c. hakkyo-eyse-num school-from-only · d. hakkyo·eyse·man-un school-from-only-Topic  (2) Verb a. ka-ss-ta go-Past-Decl b. ka-si-ess-ta go-Honor-Past-Decl c. ka-ki-ka go-Nominalizer-Nom d. ka·si-ess-ki-ey-nun go-Honor-Past-Nominalizer-to-Topic  This implies that a word in Korean can have a very !arge number of morphological variants. For example, verbs can be followed by honorific and tense mo1phemes which can then be followed by endings indicating clause-type which then can be followed by case postpositions. Similarly, adverbial postpositions which correspond to English prepositions, can be followed by other case postpositions such as nominative or accusative case markers and auxiliary postpositions such as to ('also') and man ('only'), which then can be followed by a topic marker. Accordingly,  'We thank Martha Palmer, Aravind Joshi, Anoop Sarkar and the XTAG Group at Penn for their support and discussion. Wc also acknowledge the two anonymous rcviewers. This projcct has bcen partially funded by the Anny Research Lab via a subcontract from CoGenTex, Inc. and NSF Grant SBR 8920230. The third author's contribution to this work was made while she was n visiting researchcr at m.CS.  222  J. Yoon, C. Ran, N. Kim and M. Kim  the number of possible morphological variants of a word can in principle be in the tens of thousands. This property of Korean raises two issues within the context of developing and implementing a Feature Based Lexicalized Tree Adjoining Grammar (FB-LTAG) for Korean using the XTAG system (The XTAG-Group, 1998): (1) adequate linguistic description of the inflections and (2) efficient Jexicon development. From a linguistic point of view, describing a grammar of a Janguage is to construct rules that generate sentences in the language at a formal level. From an implementational point of view, the grammar should be described in a consistent and efficient way. The XTAG system helps US to pursue both these goals, but the complicated inflection system mentioned above leads to difficulties in building a grammar for Korean. In this paper, we provide our solution to the linguistic and implementational issues raised by these morphological properties of Korean. We first provide a way of handling the Korean inflectional system using feature structures in lexicalized elementary trees in section 2. We impose a hierarchy on various types of inflections in order to handle all possible ways of combining inflections, and we represent this by assigning different feature attributes to different types of inflections. In section 3, we then point out that the current XTAG system as it is forces us to construct a lexicon (i.e., syntactic database) that lists all possible morphological variants of words. A lexicon must contain all possible eojeols, where an eojeol is a terrn in Korean for denoting a spacing unit which consists of a content word and associated functional words. However, this is highly impractical and inefficient given the rich inflectional system in Korean. We would end up with a very !arge (even unbounded) Jexicon. Therefore, we found it necessary to develop an alternative method for constructing the lexicon in order to continue to use the XTAG parser for developing a Korean grammar . One possible solution to the problem is to incorporate morphological rules in the grammar that regulate the generation of eojeols with several morphemes combined. However, doing so will mix up morphological generative rules with syntactic rules, complicating the TAG grammar tremendously. Instead, we have chosen to pursue an approach in which morphological regularities are handled by a separate morphological component using a morphological analyzer (Yoon et al„ 1999). The output of this analysis then interacts with our Korean TAG grammar which handles syntactic regularities. As a way of implementing this approach, we modified the XTAG system by dividing up the syntactic database into elementary syntactic database.(ESDB) and local syntactic database (LSDB). ESDB is a general lexicon that contains stems with the elementary trees associated with them. LSDB is a partial lexicon dynamically generated for each input sentence using infonnation from ESDB and the output of a morphological analyzer. That is, it contains only entries for eojeols occurring in the input sentence. The morphological analyzer produces the morphological analysis of each eojeol in the input sentence identifying its stem and inflections. Then, the stem of each eojeol is associated with elementary trees or tree families by looking up the ESDB and stored in the LSDB. The inftections of each eojeol are converted into features and are also stored in the LSDB. This modification to the XTAG system allows us to build a Jexicon efficiently and develop a grammar for Korean that is compatible with the XTAG system. 2. Handling inflectional morphology In our current Korean grammar, the inflectional morphology on an eojeol that are relevant for syntactic analysis is represented as features on the tree node. For instance, a noun with a nominative case marker is associated with the feature <case:nom> and when this Jexical item is anchored by an NP tree, the feature <case:nom> is passed up to the NP node. In Korean, combining inftections is a highly productive process with some restrictions. For  Korean XTAG System  example, nominative, accusative and genitive CASE postpositions occur in a distribution, but ADVERBIAL postpositions (which correspond to English prepositfons)~lichäs -ey ('at'), eykey ('from'), -kkaci ('to'), etc. can be followed by nominative case or geniti~{dl.s~f Case and adverbial postpositions are assumed to be assigned by the predicate of the setitent~~:· Moreover, AUXILIARY postpositions which have semantic content such as -man ('only') and-t() ('even') can combine with an adverbial postposition and the topic marker -(n)un can combine with an adverbial postposition and/or an auxiliary postposition but not the case postposition. Moreover, predicates1 in Korean are infiected with several morphemes. They carry CLAUSETYPE morphemes that indicate whether the clause is a main, coordinate, subordinate, relative clause, or nominalized clause. If a clause is a main clause, the verb carries a MODE morpheme that indicates whether the clause is a declarative, imperative, interrogative, exclamation, or propositive, etc. Clause-type morphemes and mode morphemes occur at the end of the verb. In addition, verbs also carry TENSE infiections right before the clause-type and mode morphemes. Further, all these infiections can be expressed in many different ways. In order to handle all possible ways of combining inftections, we imposed a hierarchy among various types of inflections and represented this by assigning different types of infiections to different feature attributes. Table 1 summarize.s the !ist of infiectional feature attributes and the corresponding feature values currently being used by our grammar. The labe! 'pp' on <advpp> and <aux-pp> stand for postpositions. Note that verbal features include <ending> which allows us to store the string values of mode and clause-type morphemes in the tree node for later semantic interpretation. Examples of an NP tree that anchors a noun (hakkyo 'school') with compound inflections, and an S tree that anchors a verb (ka 'go') with some verbal infiections are given in Figure 1.  
(ia.) Somebody left. .(ib.) Nobody left.  (iiia.) Kirn rarely says anything at all. (iiib.) "Kirn says anything at all.  (iia.) Nobody left yet. (iib.) „Somebody Ieftyet.  (iva.) Nobody rarely says anything. (ivb.) Nobody says anything.  These data show that: although NPis require a negative licenser the converse is not the case (i,ii); the negative context created by a licenser can license more than one NPI within its scope (iii); and NPis can occur in sentences with more then one licenser (iv). Furthermore, NPis can occur in more complex structures as weil, as shown below:  (va.) Nobody thinks Peter did anything wrong. (vb.) *Somebody thinks Peter did anything wrong.  (va.) A doctor who knew anything about acupuncture was not available. (vb.) *Some doctor who knew anything about acupuncture was not found.  These example show that NPis can occur in an embedded sentence while licensed by an expression in the main sentence (v); and that they are felicitous when part of a relative construction which allows to escape the syntactic scope of the licenser, but still force them tobe interpreted in its semantic scope (vi). See (de Swart, 1998)1 where the last example has been proposed and discussed.  2. Polarity Items in MMCG Two well known facts regarding MMCG and Pis are that: MMCG belongs to the family of resource sensitive logic, where the resources are meant as linguistic signs; and Pis are linguistic expressions sensitive to the polarity of their context. We suggest to consider the polarity as a particular feature required by the NPI and produced by the licenser. This idea has been independently implemented in two different resource Iogics, namely MMCG (Bernardi, 1999), and Multiplicative Linear Logic (Fry, 1999). In the latter the e 'polarity feature' is represented as a proposition assigned to the linguistic categories, of the NPis and licensers, by means of the tensor operator ®· The proper function of this operator is to concatenate logical types, or in other words the linguistic resources the logic is reasoning about. When employing it to concatenate the polarity feature to a linguistic category the former is treated as a 'phantom resource'. The language of MMCG is expressive enough to avoid this improper use of the concatenation operator, and of the resource management. A detailed comparison of the two proposals is given in (Bernardi, 2000). In the following we briefly introduce MMCG system and then we show its application to NPI. Classical Categorial Grammar (CG), has its logical counterparts in the Lambek Calculus (Lambek, 1958). The formal language of this calculus is built on the binary opera-  Deriving polarity effects 231  tors, \, / and •, viz. the directed implication operators and the product one, and a finite set A of atomic formula, e.g. A= {np, s, n }. MMCG is obtained extending thiS language with unary operators o! and 0. We refrain from presenting the logical rules of the whole system which can be found in (Moortgat, 1997) and we comment the logical behavior of the unary operator on which the Pis account is based. Let r 1-- A stand for the assignment of the category A to the linguistic structure r,  Logical Rules  fj, 1-- OA f((A)] 1-- B (OE] fl--A ( )  r(6.] 1-- B  (r) 1-- OA Ol  
 informati on when needed . For the details of the  1. INTRODUCTION  implementations the reader should lock at [1 0).  We assume, together with [l ] that POS tagging is esscntially a syntactically-based phenomenon and that by cle,·erly coupling stochnstic and Jinguisric processing one should be able to remedv some if not all of the drawbacks usually associated with the two approaches, when used in isolation. However, as ·will be shown in delail in the following section, rnlher llion using FSA we use Elementary Trees organized in an RTN both for training and for parsing. As to the statistica! pan , we don'l use HMMs but only conditional probabil ities on the basis of trigrnm infoimation as discussed lx:low. Syntactic driven disambiguation is accompl ished by using an R1N made up of 1700 arcs and 22 nets, which we use in a non-recursive way, as expJained below. Data for the construction of the RTN were derived from thc manual annotation of 60,000 token corpus suite which is lhen used as tcst set. Frcquency of occurrence associated to each rcwrite rule is used as organizing criteria in lhe ordering of lhe arcs contained in cach node of each net. However, in the experiment, we Jet conditional probabilities al the level of major constituent, or net, do lhe choice for the best path. Rather lhan flallcning the Phrase Structure Grammar as [8] suggest in their shifl-reduce algotithm, we only check for reaehability in nontc1111inal symboJs. So, even though the foimal structure of RTN is recursive, the disambiguating algorithm does not use recursive calls and all computation is flattened doll'n to one leveJ, that of tags corTesponding to preterminals in th e RTN. The syntactic-slatistica l disambiguator (hence SSD) can be defined as a slightly augmented finite slate lransducer which works nt a single level of computation nnd has access to higher level  2 .STATJ STI C A L VS. SYN T A C TIC DI SAMBIGUA TION 
We show that clitics are not as problematicfor Synchronous TAG as has been Sllpposed, and give two  o·[soigne]  a (2)[t re ats]  ~ ---,  o{2}[1ur1  o (2 · l )(denu] a (2 · l)[1ee1h]  solutio11s; a11d, in doing so, demonstrate that 'unbounded relations ', such as it is argued clitics in-  
Abstract In nn uniform genemtion system nll knowledge bnses are specified in the sameformnlism and run t/re same processing component. The ndvantage ofthis behavior is timt any order ofapplying the know/edge bases, i.e. a negotintion on revisions between tlze individual components, can easily be imposed on the system. F11rthermore, the implementntion ofthe overnll system is simpler because only one algorithm must be developed nnd tested. In the project !NTEGENINE we specify all knowledge so11rces in theformnlism ofScl1emn-TAGs with Unificntion (SU-TAGs). A genera/ pamdigm of our work is to reuse existing knowledge bases, i.e. to trnnsform vnrious formnts into n SU-TAG. For the syntactic nnd lexicnl knowledge the existing XTAG system hns nlrendy a11tomaticnl/y been transfo1111ed. In tlzis paper we address the genernl question how to trnnsfonn plan-based knowledge-sources w/ticlr are frequently 11sed in tlze wlzat-to- say pnrt ofn generntion system. As an instnnce of the genernl transformntion model presented here, we show how to transform the knowledge sources ojtlte plan-bnsed system VOTE. TI1e transformation component we describe in the following appertain to a uniform ge11emtio11 system based on Schema- TAGs. Let us first briefly address this system in order to motivate the serviceableness of the transformation component in the general system. The idea of uniform or so called integmted generation was basically described in the system KAMP (Appelt, 1985). In this system a hierarchical action planner explores expressions based on the formalism of intensional modal logic. KAMP was not intended tobe a psycholinguistic model of human behavior, although is reflects some aspects of human language production such as increment11/ity. Thi.s behavior directly results from the integrated model. Any knowledge base is supposed to become active at any time, i.e. as early as possible. From this observation the question arises whether the uniform model can serve as a basis to remedy the generation gap (Meteer, 1990), i.e. the situation in which a sequential process (first what- to-say, then how-to-say) leads to dead end situations which cannot be solved by local modifications in the component in which the problem occurs. Our asswnption is to extend the - in a sense demon-li.ke - activation of knowledge bases towards a parametrised model which allows for recovery strategies to escape from Iocal dead ends by imposing revisions of parameter-defined components. This rneans that parameters trigger the activation of specific knowledge bases and hence initiate overall revisions. Our clairn is that this approach is able to build up any kind of communication model in a generation system. As underlying formalism of our integrated generation model we have chosen Schema-TAGs with Unification (SU-TAGsJ1 because TAGs provide the necessary complexity to express any kind ofconceptin the what-to-say and how-to-say component (cf., e.g., (Stone & Daran, 1997), (Webber & Joshi, 1998), (Becker et al., 1998), (Nicolov, 1998)). Schema-TAGs are especially 1In a schematic ele~zentan; tree, a regular expression (RX), is annotated at each inner node of an elementary tree. This means, that the elementary schemata enumerate a possibly infinite set of elementary trees. RXs are inductively defined. Let o ,ß and ß1, ••„ ß„ (n ;::_ 2) be Gorn addresses uniquely referring to daughters or arbitrarily complex RXs, then a.ß (concatenation ofbranches), (ß1 + „.+ß„) (enumeration of alternatives), a• (Kleene Star) and er+ (er• without the emfty repetition) are RXs. Finally, '.'-" allows to cut off a subtree at the end of a path. As an abbreviation we write er("'I~ which enumerates L:;;':0 er"'+' (n, m 2:. 0). Notlee,"." binds strenger than " +". Notice also, that here the feature specifications are attached to the regular expression because the branches are licensed by RXs (cf. (Harbusch & Woch, 2000)).  246  PJ Pr.  -  H (11j„„.[nJ)' ~  „,1p, „. „,1Pn  P?~  PCS~  II!' cond THZN Px ELSE Py  (1)  (2)  (3)  Figure 1: Transformation of sequences, alternatives, and repetitions into SU-TAGs  advantageous because they compress gram.mars in a manner that allows for the underspecified generation of substrµctures (cf. (Harbusch, 2000), (Harbusch & Woch, 2000)). In order to provide a flexible generation systern, the example dornain is not of particular interest but only a necessary prerequisite of a demonstration systern. On that account, we decide to reuse existing knowledge sources to circumvent the time-consumirig task of developing a knowledge base from scratch. Thus, transforrnation algorithms for the individual knowledge bases of a generation system must be provided. Any TAG can automatically be transformed into a SU-TAG (Harbusch et 111., 1998). This is already done for the syntactic knowledge base XTAG (Doran et 111., 1994). The knowledge sources of SPUD (Stone & Doran, 1997), those of anchored L-TAGs (cf. (Webber & Joshi, 1998)), as weil as the TAG transformed from an HPSG (Becker et al., 1998) w ill be rewritten as SU-TAG next. Doing so, the generation system is extended towards a generntion workbench which provides libraries with knowledge sources from which the user can select a personal generation sys tem with self-defined parameters. In this paper, we describe the transformation of plan-based knowledge bases into a SU-TAG. Here we only concentrate on the particular dass of plans which is widely applied in what-to-say components of generation systems (e.g. VOTE (Slad e, 1994)), i.e. the classical pla11-based plans (cf. (Yang, 1997)). As an illustration a concrete plan of the system VOTE is transformed and the decision making on the basis of VOTE's further knowledge bases is presented. Finally, the interaction of pla.ns with the system's knowledge about the domain- also specified as SU-TAG - is outlined in order to dernonstrate basically, how the uniform generation works. A p/11112 consists of n steps, any of them in turn rnay be an action or a plan again. Each step consists of pre- and postconditions, as weil as controlling elements of a programming language (e.g. IF- THEN- ELSE, WHILE). A plan can be applied iff the overall goal, i.e. the input specification, matches the preconditions of the first step. A plan step can be applied iff the current situatio11, i.e. the postconditions of the previous step, or the input specifications respectively, match the preconditions of the currently considered plan step. If a plan step is atomic, i.e. an action, it is performed by replacing the preconditions with the postconditions, resulting in thi:: new current situation. An overall plan can successfully be applied in the current situation iff the final postconditions can be computed according to the overall goal and the initial situation. Given that, the general idea of the transforrnation into a SU-TAG is as follows:  1. Each plan step in a sequence becomes an individ ual n ode of an elementary scheme under a common root n ode. 2. The chronological sequence of plan steps is rewritten via concatenation in the RX. 3. Pre- and postconditions at each node are wrapped up in feature specifications. 4. The conditions of concepts of the programming language are realized by unification too, whilst the branches and repetitions itself are transformed into RXs.  In the transformation of Fig. 1-1, the first three steps are illustrated. Each plan step P1, „., Pn is transformed into a daughter node. The regular expression at the root node enumerates the concatenation of all daughters from left to right and all pre- and p ostconditions are rewritten as feature specifications. Step 4 is illustrated by two example statements in Fig. 1-2 and 13. Basically, the conditions in the statements are checked by a. feature "cond". For instance, 2For an illustration of a plan, see the strategy for decision making in VOTE (abstracting from technical notations, cf. (Slade, 1994), p. 140) on the left s ide of Fig. 2.  Reuse of Plan-based Knowledge Sources  247  (1) WHILE ?no-<ledsion  (2) IF ?Unanimous (3) THENPlanp0 polar  =  (4) ELSE I F ?Consensus  (5) THEN Plancon••n•••  (6) ELSE IF ?Majority THEN PlanMojorH•  (7) ELSE Planoth•r-Strat<gy;  (8) IF ?no-<lecision THEN Planv••p•r-Analy•i•;  (9)00  Figure 2: Transformation of the VOTE-Plan  D  Figure 3: Part of VOTE's knowledge for MEMBER:gingrich and BILL:limit-clean-water in the IF- THEN-ELSE statement a positive value for "cond" activates the THEN part and a negative value the ELSE part. Tue behavior of the steps 1, 2 and 4 is exemplified in Fig. 2. This plan describes the decision making process in the system VOIE. Tue actual knowledge in the pre- and postconditions is suppressed here for reasons of simplicity. As outlined in step 4, IF-THEN-ELSE statements are rewritten as sums, (i.e. representing the choice of one of the branches according to the instantiation of the condition) and the WHILE construction is rewritten as Kleene Star which stops according to the instantiation of the respective condition represented as feature specification. At the root node the concatenation represents the sequence of the two IF-THEN-ELSEstatements in line (2) and (8) (step 1and3resultin 121.lll according to the order ofbranches). Here, the Kleene Star in the RX rewrites line (1). Now we explain how pre- and postconditions are specified and tested in order to apply plans in this particular example. For this reasqn we must describe VOTE's further knowledge bases in more detail: VOTE consists of ISSUES (e.g. gun control), STANCES (PRO, CON, normal case), GROUPS (e.g. ACLU), RELATIONSHIPS, MEMBERS, BILLS and STRATEGIES. Fig. 3 shows the structure of what VOTE .knows about a concrete BILL:limit-dean-water and the attitude of a concrete MEMBER:gingrich towards this bill. Let us presuppose here that the structures described in Fig. 3 can be produced by SU-TAG structures of the form outlined in Fig. 4. This is directly obvious because all mother--daughter relations in the instantiation are represented as elementary schemata. Furthermore, any scheme licenses the specification of any number of such relations by Kleene Star. In any plan of VOTE the pre- and postconditions are yet specified by unification about STANCEs of bills and members. For instance in Plan,,opul"r' PRE = {Unify  248  f11'.12t GAOUP~~"" /'\ ISSUE CREDO  j1f.12t'.f3f'MEMBEA [""""' ~  CREDO  RELATION  ill'CAEOO 
During the last decade we developed and continuously improved CDL-TAGs, an extension of TAGsfor incremental syntactic generation. This paperpresents the current state ofdevelopment and gives details ofthe definltion ofcontext dependent linearization rules. 1. Introduction This paper presents Tree Adjoining Grammars with Context-Dependent Disjunctive Linearization Rufes (CDL-TAG) that have been developed for incremental syntactic generation in the system WIP (WAF+93). CDL-TAGs were successfully used in the projects PERFECTION (Fin96), EFFENDI (PH 96), PRACMA (JI(N+94), and VERBMOBIL (Wah93). A fully incremental system is characterized by realizing interleaved input consumption, processing and output production, so that first output elements may even be produced before the input is complete. So, decisions based on the data at hand impose assumptions about the outstanding input, thereby reducing the set of input increments that can be consistently integrated into processing. For syntactic generation, two different processing levels can be distinguished. First, for each new element the hierarchical structure of the sentence under construction has to be expanded. Second, elements have to be positioned in the final utterance thereby constraining any further positioning. According to that, it is essential to choose a syntactic representation formalism that facilitates the dynamic construction of the hierarchical structure and the stepwise linearization and utterance production for its substructures. The grammar formalism must be flexible enough to preserve word order variations as long as possible during generation. Thereby, it should be easy to handle the prefix of the sentence already uttered as constraining the set of applicable linearization rules. Additionally, the grammar formalism should support linearization rules that describe situational factors (e.g., time or space restrictions). The separation of a grammar into Hierarchical and Positional constraints (in the following called HIP paradigm) fulfills these requirements. Such a grammar (e.g., LD/LP-TAGs (Jos87)) consists of two distinct sets of rules, one merely describing mother-daughter relations only hierarchically, while the other describes positional constraints by referring to elements of the hierarchical structures. This paper presents CDL-TAGs, that almost perfectly refiect the required different levels·of processing for incremental syntactic generation and thereby strongly facilitate the implementation of the incrementality effects on syntactic generation (FS92). 2. Definition of CDL-TAGs TAG with Context-Dependent Disjunctive Linearization Rules (CDL-TAG) is an extension of Tree Adjoining Grammar (JLT75) that helps to design an extremely compact grammar by avoiding redundant descriptions without extending the power of the fonnalism.  250  A. Kilger and P. Poller  2.1. The Standard TAG Formalism Standard TAG combines elementary (initial and auxiliary) trees by adjoining, an operation which makes the grammar mildly context-sensitive and adequate for the representation of natural language. The TAG formalism has been extended by a second combination operation called substitution which has only context-free power (SAJ88). In order to allow compact representations of complex syntactic dependencies, TAG has been extended furthermore by feature structures (TAGs with unification, (Ki192), (Kil94), or Feature Structure Based TAG, (VJ88)). The H/P-paradigm was app1ied to TAG by (Jos87). He defined Local Dominance!Linear Precedence-TAG (LD/LP-TAG) by "taking the elementary trees as domination structures over which linear precedences can be defined." The descriptive power of LP-rules in LD/LP-TAGs is not sufficient to describe all Jinearization alternatives of one hierarchical structure Jocally, i.e„ without duplicating the hierarchical structure (e.g., for German verbal phrases subjectverb-object, object-verb-subject, ... ). Furthermore, there is no means to associate different LP-rules with contextual (semantic and pragmatic) constraints. To get more flexible linearization, we developed a new extension of TAG on the basis of LD/LP-TAGs.  2.2. CD~TAG CDL-TAG is defined according to the HIP paradigm, i.e., domination structures are used as elementary structures instead of trees. The possible orderings of sister nodes are restricted by linearization rules which are associated with the mother node. They have the form: "(<" {"("context lin-rule* ")"}* ")". The rules are initiated by the key "<". Bach alternative starts with the name of a context in which the rule is valid. The value of context is matched with a feature lin-context of the feature structure associated with the respective node. Tue Jeft part of Figure 1 illustrates a VP-node whose subtree represents a German verbal phrase.  (lin·nij11it.lU: \'Crb-KC<Jnd)  (< (verb-flrs& . ..)  (< („y ...)  VP (verb-scconcl ... ) ~vcrb·fin•I ...))  ........N-P-r---C_shon •..))  V Sobj! ~ccobjl  Specitier.i N Modifict.l  Figure 1: Examples for German Linearization Rules Its linearization rules include statements about verb-first, verb-second and verb-final word order while 'verb-second' is the actual 'Jin-context' inside its feature structure. Other contexts (like 'any ' or 'short' at the NP-node in the right part of the figure) distinguish ward order rules that differ with respect to their suitability for specific situational - non-syntactic - factors which is useful for a generation system with globally set 'parameters'. E.g„ the value 'short' 1 is used for word orders that permit to save space and time in the final utterance. Each lin - rule is encoded as a !ist that contains linearization elements lin - el. The order of the !ist elements defines the order of the elements of the TAG tree they refer to. A symbol sym is a lin - el and refers to a daughter of the node the linearization rule is associated with. In order to describe constraints on sister nodes which include complements as weil as optional elements, we extended the formalism by a combination operation that allows to add sister nodes without introducing additional depth into the tree. The operation offurcation has been „defined by (DK88) as the unification of two root nodes of structures to one root node with two substructures. We adapted it to CDL-TAGs by defi.ning an new kind of elementary tree, namely a furcation auxiliary tree whose foot node is leftmost o r rightmost daughter of the root node, as a structure leaving away the foot node2• 1The key 'short' is meant in the sense of saving space and time in the final utterance when using this alternative. This may be meaningful under time pressure or when the space for the written text is restricted. 2This is comparable to the modijier au.xiliary tree in conuast to the predicative au.xiliary• tree inlloduced by  CDL-TAGs  Some symbols sym in lin - rule denote adjuncts. They have to be detailed enough to. all aspects that infiuence word positions. For English, e.g„ different adverb classes have'to be defined according to their different linearization constraints. Symbols referring to adjunct~· always appear inside disjunctions with 'regular-like' expressions. They permit to describe ex actly one occurrence of one element of a list by ((sym1 ...symn)1), one or zero occurrences by ((sym1.„symn) 11°), at least one occurrence of elements by ((sym1„.symn)+) or an arbitrary (or zero) number of elements by ((sym1 ...symn)°). The following expression is a possible linearization rule of the VP-node of Figure 1:  (verb-second (subj v „ . (advp)' „. accobj . „)  ((advp)1 v subj .„ (advp)" „.accobj ... )  " .)  It shows two alternatives to fill the first position of a verbal phrase in the Jinearization context  verb- second, namely a complement ('subj' refers to the subject), or exactly one optional ele-  ment ('advp' refers to an optional adverbial phrase). After the first element, the finite part of the  inftected verb (referred to by 'v' in the linearization rule) has to follow. The second expression  prescribes that the subject directly follows the verb in case of a topicalized adverbial phrase.  Furthermore, the selection of adequate linearization rules may be restricted by features of the  subtree to be linearized. CDL-TAGs use child-info that is inherited from the daughters of  the node the linearization rule is associated with. The resulting structure for LP- rules is "(<"  {"("context child-info lin-rule* ")"}* ")". The entry child-info realizes a specific test (identified  by the key ' test') for feature-value-combinations which have to hold for some of the daughters  of the actual node. The LP-rule  (short (test (mod (cat) name)) („ . mod „. (adjp)* „. n . „) .„)  might be associated witll the NP-node on the right in Figure 1. It describes a possible linearization of a Specifier-Noun- Modifier construction in German: Instead of "Die Werke Goethes" (the works of Goethe) it is also possible to say "Goethes Werke" (Goethe's works). The presupposition for choosing this 'brief linearization alternative is that the modifier is realized as a proper name which is tested by referring to the third daughter of NP (the Modifier,j, node, referred to in the test above by 'mod') and then checking the equality of feature-value of 'cat' and the atomic value 'name'. The generative power of CDL-TAGs is equivalent to Standard TAG (with constraints) because the only addition to standard TAG is the combination operation "furcation" which has only context-free power. So, CDL-TAGs are not sufficient to describe all linearization phenomena that include adjuncts. E.g„ there is no easy way to describe scrambling without mixing hierarchical and positional information. Nevertheless, we use it as a promising starting point, concentrating on its usefulness for (incremental) syntactic generation.  3. Conclusions and Future Work In this paper we presented CDL-TAGs, a highly compact grammar formalism, that is especially well-suited for the representation of grammar sources for (incremental) natural language generation. Furthermore, the lexicalization allows the grammar to consider a subset of word class specific elementary trees (tree families) for each Jexical entry. The TAG-GENgenerator (Kil94) makes use of the CDL-rules by preferring linearization alternatives that reflect the order of input elements so that the output can start as early as possible,  (SS92). In this sense, furcation au:lliliary trees are the CDL-TAG variant of sister adjunction in, e.g„ DTG (RVW95) and furcation in, e.g„ 1FG (Cav98).  252  A. Kilger and P. Poller  e.g„ by fronting elements which are given early in the input. lt also sorts linearization alternatives according to some generation parameters such as time pressure and style. Although the forrnalism has been successfully used in several different application systems, there is no grammar developing tool yet. So, the most important task for future work is the development and implementation of a CDL-TAG parser, e.g., as an extension of the work described in (Po194).  References M. Cavazza. An integrated parser for TFG wich Explicit Tree Typing In TAG+4'98, Institute for Research in Cognitive Science (IRCS), University of Pennsylvania, Philadelphia, PA, 1998. 
In a restricted domain and task, we propose that the elementary tree backbones represent statically the predicative Level and the possible distribution ofarguments while the syntactic categories and constraints would be only processed dynamically by the way offeatures. The resulting grammar can be viewed as an intermedi.ate Level between the surface syntax ofa sentence and its conceptual representation. In addition to possible speed efficiency and robustness relevance, an interesting property is that such a grammar could be tested in a straightforward way to integrate contraints provided by additional trees and to inject progressively semantic and pragmatic constraints during the analysis. 1. Introduction Considering applications such as spoken annotation of elements into a specific virtual environment, the most important task is first to identify referred objects or terms among several speech hypotheses. Given these expectations, how can be used the assets of a LTAG grammar with robustness? To address this question. we propose a Feature-Based LTAG grammar focusing on the semantic and predicative level while the pure syntactic processing is achieved by the twostep unification mechanism. Before introducing this preclicative LTAG grammar, we define the applicative framework. 2. From Terms extraction to spoken annotations The research project under consideration is based on a virtual platform (which represents an architecture of aeronautical components and a tenninological model obtained from technical documents (example: cautions toset on the manipulation of components). Let us clarify that first the virtual platfonn (i.e. a 3D scene) is used as an interface between the desing and assembly tasks. The aim of this interface is to Jet people easily move in a complex architecture, to display or mask related annotations, and to gather vocal synthetic annotations that overlap one or several elements of a scene (example : recommendations for people of a related trade). Secondly, the tenninology of the technical documents is ideally subjected to editorial constraints and is getting close to a controlled language. A tenns extraction and clusterization based on statistical criteria supply classes of elements. Then, an expert is efficient to grab the terms in a knowledge base containing ontological and conceptual relationships. Tools of the market are helpful for these tasks (Fig. 1, Fig. 3). The aim of this step is twofold: - Build a model used to check the cohesion from various technical documents or versions. - ldentify the stable tenns and build up various tenninological resources {authoring mem- ory, multilingual thesaurus needed for automatic Janguage processing). For example, the  254  P. Lopez and D. Roussel  knowledge base designed by the experts is used to categorize various technical documents within an Information Retrieval System. For the spoken annotation purpose, we derived constraints from the knowledge base in order to restrict the combination between technical properties (ex: ftoat valve, needle valve), functionalities (ex: drain valve, directional valve) and the system in which a unit is used (ex: water valve, bleed valve). By this way, terms like water drain valve, electrical drain valve are well recognized, but some other complex terms are rejected.  Figure J: Cluster of words computed for technical documentation extracts. Note that the word valve covers at least three notions expressed in French by the terms valve, soupape and vanne <C.2616> MAKE SURE YOU WILL NOT CAUSE UNWANTED CHANGES TO OTHER SYSTEMS BEFORE YOU PUSH THE ENG 1 (2, 3 OR 4). WHEN YOU PUSH THE ENG 1 (2, 3 OR 4) FIRE PUSHBUTTON SWITCH THESE VALVES CLOSE : THE LP FUEL VALVE . THE HYDRAULIC FIRE VALVE. THE BLEED AIR VALVES. THE ANTI-ICE VALVES . THE AIR· CONDITION!NG PACK VALVES. Figure 2: Example of caution integrated in a structured technical documentation. Taking advantage of lexical resources obtained from technical procedures called "wamings and cautions" (see Fig. 3), the MRTERESA project (Multilocutor speech Recognition, TERms Extraction and Spoken Annotation) consists in the customization of a speech recognizer for vocal annotations, a robust term analysis of speech recognition hypotheses and vocal annotations indexing with regard to components existing in a virtual scene. If necessary, the indexing has to be confirmed by the users. Tue robust terms analysis relies upon: - A mapping between Jexicalized elementary trees and technical terms. Some category labels in these trees are semantic types that belong to an ontology. - A representation of the terms variability in the spoken annotations thanks to the TAG substitution and adjonction operations. This variability results from spatial relations between the displayed objects and the spontaneity of the verbalizations. - Semantic labels compatibility constraints for modification and dependency relations - If necessary, syntactic constraints are applied to filter out speech recognition hypotheses. 3. Syntactic vs. semantic Dependencies The semantic head is the lexical unit that represents the semantic type of the interpretation of a given phrase structure. We consider that the syntactic head is the Jexical unit that constraints the  Predicative LTAG lPG Lubricont l.J.ltwieele Lud< Lull9000 \.umbogo Figure 3: Example of knowledge base from few va!ves achieved with a tool of the market morphosyntactic and mode features of the phrase structure it belongs to. The LTAG formalism is well suited to loca!ize semantic dependencies, but is limited to represent syntactic dependencies for very frequent phenomena as object extraction with auxiliary. When we use the term Jocalizing semanric dependencies for a LTAG grarnrnar, we suppose that the elemeiltary trees have been designed properly to capture this kind of dependencies, i.e. that the elementary trees respect the Predicate-Argument (PA) and Semantic Consistency (SC) principles introduced in (Abeille, 1991). These principles stipulate that a lexicalized elementary tree corresponds to an unique semantic unit (semanteme) and that we have a terminal node (substitution or foot node) per argument expected by the corresponding semanteme. In our approach we systemize the localization of semantic dependencies: we drop out from the elementary tree backbones aJI the aspects which traditionally refer to syntactic categories and replace them dynamically with semantic types. 4. A new definition for the elementary trees The first point is to capture in an elementary tree a particular word distribution and the corresponding predictive structure under the form of semantic dependencies. Closely to the solution proposed in (Abeille, 1992) for the representation of this level, we use the following predicative categories as node Jabels of elementary trees: - Formula (F) or proposition representing the association of a relation and its arguments. - Term (T) which corresponds to the non-relational semantic heads. - Relation (R). - Property (P). - Null (N): used for semantically empty nodes (in general preterminal nodes of co-anchors, semantically empty prepositions or auxiliaries). Top and bottom features are added on this backbone in order to check syntactical constraints at the end of the parsing. The figure 4 gives examples of Feature-Based predicative LTAG elementary trees. During the lexicalization process, semantic types are added to the LTAG tree backbone according to the semanteme that the elementary tree represents and an ontol-  256  P. Lopez and D. Roussel  Predicative LTAG schema  Ä ....  ~c\a+  N  R· llUJl;;•  
3. What Reliability is about  Reliability is an important evaluation criterion for NLP which until now has failed to obtain a formal definition as weil the attention it merits. The standard evaluation tests a parser on unleamed corpora, deterrnining its coverage in terms of recall and precision. The pendant of the coverage is the reliability, which we define as a system property, i.e. as pe1formance on trained corpora. Reliability is thus close the notion of tunability. However, the impact of reliability is more fare-reaching: A system which has a high reliability can always enlarge its coverage by leaming new items. A system with low reliability cannot improve its coverage by leaming new items: the system is quickly over-trained.  We define coverage (C) and reliability (R) as meta-scores which Figure 1: The hypoth-  elaborate the values of recall and precision. As R is neither com- esis of converging C  patible with low precision (false alarrn) nor with low recall (a silent and R for hypothetical  system), we define R and C as f-score with leamed respectively data. K is the estimated  unleamed test corpora. Wirh this definitions we formulate the hy- pothesis of com·erging C a11d n : 1) R is always higher than C. n 2) decreases with more training data (due to ambiguities which n arise). 3) C approaches with more training data (more items are  maximal coverage, .. .. . . l. ••••••:~~R,,,,,,,V. K. • • • t  known or similar to known items). 4) Before C and R converge C may decreases under the inftuence of decreasing R.  training data  While most experimental data available support an asymptotic rise of C, little is known about R. Given the above (hypothetic) distribution, the maximal covernge a system can achieve as weil as its current position are important data. We propose to estimated the maximal coverage K as C+ \~~;,-;;l'::§. With K > C further investment in more teaching is profitable, otherwise system properties have tobe changed in order to enforce n and with it future grow.  - . 4. Factors determining Reliability Figure 2: Reference Data. C and n for 500 to 1  • R•  e99- •  •  •  25.000 training sentences, evaluating the recog- ·  .7 .....  nition of semantic relations (agent, theme, goal, · K  .4  experiencer, time etc) between head and de- : pendents (2a) and the bracketing (2b) for the  0  .„ (2a)  semanCtic  r2o2tes5  (2b) bracketing 225  example-based parser on a test-corpus of 7.27  sentences.  Experiment 1 In order to establish the effect of the string and lexeme encoding in addition to the category encoding we removed the string and lexeme encoding as done in all eager learners.  259  • :: • • Figure 3:Parsing with categories only.  .l~ : ~ R • ~7 ~  :~f  ·~ K., . c .  •(Ja) semantic rol.es (3b) bracketing  225 1  225  n. c F.(3) shows a loss is  and K, compared to  F.(2). We assume that the drop in C has been n. produced by the drop in as unknown items  are treated only in reference to leamed items. lt  is the ambiguity in the leamed items (the inverse  of R) which causes the drop of C.  Experiment 2 In order to establish the effect of the context sensitivity we not only re-parsed  awkward subtrees (see our description of the parser above), but re-parsed (artificially) all sub-  trees, thus breaking the links between sisters.  We observe a small loss ofn compared to F.(2). Figure 4: Context-free parsing in (4a-b).  If we test a context-free version with category Idem wirhout string-encoding in (4c-d). encoding only (4c-d) and thus simulate standard 1. - • • R • .9 ~ ~ ~ ~ ~~ parsing approaches, we observe an additional : 7!:, • • • •7 .- • • • • drop of R compared to F.(3). Thus conrext sen- . .„ • • C • •4 sitivity is important for n but to a smaller ex- · °(4a) semantic roles (4b) bracketing  'Je · tend than the encoding of lexemes and strings. Without string encoding the context-free gram-  
 tures and semantic information which are  Grammars are core elements of many NLP rarely represented in the corpora. lt would applications. Grammars can be developed in be ideal if we could combine the strengths  two ways: built by hand or extracted from of both types of grammar. As a first step  corpora. In this paper, we compare a hand- towards addressing this issue, in this paper crajted grammar with a Treebank grammar. we compare a hand-crafted grammar with  We contend that recognizing substructures a Treebank grammar and propose a way of of the grammars' basic units is necessary integrating them to produce new grammars.  not only because it allows grammars to be  compared at a higher level, but also because 2 . Two grammars  it provides the building blocks f or consistent and efficient integration of the grammars. The two LTAGs that we compare are the XTAG English grammar (XTAG-Group,  1. Introduction  1995) and a grammar extracted from Penn  A Lexicalized Tree Adjoining Grammar (LTAG) is a core element of many NLP ap-  English Treebank. The XTAG grammar has 1004 tree templates.1 The Treebank  plications. lt often has hundreds of elemen- grammar that we use in this paper is ex-  tary trees (etrees), which can either be built tracted from the Penn English Treebank II  by hand (hand-crafted grammars), or ex- (Marcus et al., 1994) using the extraction  tracted from anriotated corpora (Treebank algorithm described in (Xia, 1999}. The ex-  grammars}. Hand-crafted grammars have tracted grammar has 3072 templates.  rich representations (such as feature struc- For lack of space, we will not describe the  tures), and tend tobe more precise, but they extraction algorithm, other than pointing  take a long time to build and their coverage out that by design all the etrees extracted  on naturally-occurring data is hard to de- from the Treebank fall into one of three  termine. In additiou, they lack statistical types according to the relations between the  information which is crucial for statistical anchor of the etree and other nodes in the  parsers. Treebank grammars, on the other tree, as shown in Figure 1. Figure 2 shows a  hand, require little hnman effort (Xia, 1999; bracketed sentence from the Penn Treebank.  Chen & Vijay-Shanker, 2000) to build, once From that sentence, five etrees are extracted  the Treebank has been created. They have by the algorithm, as shown in Figure 3.  rieb statistical information and will cover at  least the corpora from which the grammars  are extracted. However, Treebank gram- 1If we remove the anchor(s) from etrees, we get mars are noise-prone because of annotation tree templates. Each template indicates where the errors in the corpora and they also lack fea- anchor(s) of that etree will be instantiated.  266  Fei Xia, Martha Palmer  .. ,/"-..... Y'. X..•! x' . I"---.. x~· zt ' caJ spint-tucc 1prcdicatc-01riumcn1 rdation>  .~ ~ 'L"• a:::',,.-X-.„.. Y~f "·~1 ): 1 I"---.. xo ztt 
Project AMALGAM explored a range of Partof-Speech tagsets and phrase structure parsing schemes used in modern English corpus-based research. The PoS-tagging schemes and parsing schemes include some which have been used for hand annotation of corpora or manual postediting of automatic taggers or parsers; and others which are unedited output of a parsing program. Project deliverables include: a detailed description of each PoS-tagging scheme, and multi-tagged corpus; a “Corpus-neutral” tokenization scheme; a family of PoS-taggers, for 8 PoS-tagsets; a method for “PoS-tagset conversion”, a sample of texts parsed according to a range of parsing schemes: a MultiTreebank; an Internet service allowing researchers worldwide free access to the above resources, including a simple email-based method for PoS-tagging any English text with any or all PoS-tagset(s). We conclude that the range of tagging and parsing schemes in use is too varied to allow agreement on a standard; and that parserevaluation based on ‘bracket-matching’ is unfair to more sophisticated parsers. 1. Introduction The International Computer Archive of Modern and medieval English, ICAME, is an international research network focussing on English Corpus Linguistics, including the collation and linguistic annotation of English language corpora, and applications of these linguistically interpreted corpora. ICAME publishes an annual ICAME Journal (now in its  24th volume) and holds an annual ICAME conference (ICAME’2000, the 19th ICAME conference, was held in Sydney, Australia). Many English Corpus Linguistics projects reported in ICAME Journal and elsewhere involve grammatical analysis or tagging of English texts (eg Leech et al 1983, Atwell 1983, Booth 1985, Owen 1987, Souter 1989a, Benello et al 1989, O’Donoghue 1991, Belmore 1991, Kyto and Voutilainen 1995, Aarts 1996, Qiao and Huang 1998). Each new project reviewed existing tagging schemes, and chose which to adopt and/or adapt. The project AMALGAM (Automatic Mapping Among Lexico-Grammatical Annotation Models) has explored a range of Part-of-Speech tagsets and parsing schemes used in ICAME corpus-based research. The PoS-tagging schemes include: Brown (Greene and Rubin 1981), LOB (Atwell 1982, Johansson et al 1986), parts (man 1986), SEC (Taylor and Knowles 1988), POW (Souter 1989b), UPenn (Santorini 1990), LLC (Eeg-Olofsson 1991), ICE (Greenbaum 1993), and BNC (Garside 1996). The parsing schemes include some which have been used for hand annotation of corpora or manual post-editing of automatic parsers; and others which are unedited output of a parsing program. 2. Defining the PoS-tagging schemes ICAME researchers have used a range of different PoS-tag annotation schemes or models. Table 1 shows how an example sentence from the IPSM Corpus (Sutcliffe et al 1996), ‘Select the text you want to protect’, is tagged according to several alternative tagging schemes and vertically aligned.  
The paper reports on a multi-layered corpus of Italian, annotated at the syntactic and lexicosemantic levels, whose development is supported by a dedicated software augmented with an intelligent interface. The issue of evaluating this type of resource is also addressed. Introduction It is nowadays widely acknowledged that linguistically annotated corpora have a crucial theoretical as well as applicative role in Natural Language Processing. Italian still lacks such a resource. The paper describes a large scale effort to provide Italian with a multi-level annotated corpus, the Italian Syntactic-Semantic Treebank (henceforth referred to as ISST). Evaluation of ISST is foreseen in the framework of a machine translation application. Specifically developed software, including an intelligent interface, supports both annotation and evaluation activities. ISST - which represents one of the main actions of an ongoing Italian national project, SI-TAL1 is developed by a consortium of companies and computational linguistics sites in Italy (see author's affiliations above). 1], 4] and 5] are in charge of the annotation, 3] of the design and 
The paper asks how much structural detail it is reasonable to include in a detailed general-purpose grammar annotation scheme. I argue that there is no principled answer to that question; even grammatical distinctions which in general are clear and linguistically central will often be “distinctions without a difference” in particular examples. The discipline which offers the closest intellectual precedent for linguistic treebank-compilation activity, biological systematics, is disanalogous from our work in that respect.*  Detailed v. skeleton analytic schemes Any scheme for structural annotation of corpora must embody decisions about how much detail to include. Some groups explicitly aim at “skeleton parsing”, marking much less grammatical detail than linguists recognize a language as containing. In many circumstances, this will be a sensible strategy. If one’s chief goal is to have as large as possible a quantity of analysed material, from which reliable statistics can be derived, then skeleton parsing is more or less unavoidable. Automatic parsers may be able to deliver skeleton but not detailed analyses, and human analysts can produce skeleton analyses quickly. Furthermore, for some natural language processing applications skeleton analysis may be all that is needed. 
CP[std,−dep]  PERIOD  NP[std]  Cbar[fin] .  NPap V[v,fin] VP[v,−h,inf]  NAMES Vmorph[v,fin] NP[std]  Cat[name] sieht  NPap  H[name]  NAMES  NAMEbase  Cat[name]  Maria  H[name]  NAMEbase  Hans  
This paper describes a specific part of the Prague Dependency Treebank annotation, the step from the surface dependency structure towards the underlying representation of the sentence. The first section explains the theoretical basis of the project. In Section 2 all the procedure of conversion to the tectogrammatical structure is summarized and Section 3 presents in detail the present stage of the automated part of the conversion procedure. 
Temiar reduplication is a difﬁcult piece of prosodic morphology. This paper presents the ﬁrst computational analysis of Temiar reduplication, using the novel ﬁnite-state approach of One-Level Prosodic Morphology originally developed by Walther (1999b, 2000). After reviewing both the data and the basic tenets of One-level Prosodic Morphology, the analysis is laid out in some detail, using the notation of the FSA Utilities ﬁnite-state toolkit (van Noord 1997). One important discovery is that in this approach one can easily deﬁne a regular expression operator which ambiguously scans a string in the left- or rightward direction for a certain prosodic property. This yields an elegant account of base-length-dependent triggering of reduplication as found in Temiar.  
This paper describes the semantic annotations we are performing on the CallHome Japanese corpus of spontaneous, unscripted telephone conversations (LDC, 1996). Our annotations include (i) semantic classes for all nouns and verbs; (ii) verb senses for all main verbs; and (iii) relations between main verbs and their complements in the same utterance. Our semantic tagset is taken from NTT’s Goi-Taikei semantic lexicon and ontology (Ikehara et al., 1997). A pilot study demonstrates that the verb sense tagging can be e ciently performed by native Japanese speakers using computergenerated HTML forms, and that good interannotator reliability can be obtained in the right conditions. 
The most effective paradigm for word sense disambiguation, supervised learning, seems to be stuck because of the knowledge acquisition bottleneck. In this paper we take an in-depth study of the performance of decision lists on two publicly available corpora and an additional corpus automatically acquired from the Web, using the fine-grained highly polysemous senses in WordNet. Decision lists are shown a versatile state-of-the-art technique. The experiments reveal, among other facts, that SemCor can be an acceptable (0.7 precision for polysemous words) starting point for an all-words system. The results on the DSO corpus show that for some highly polysemous words 0.7 precision seems to be the current state-of-the-art limit. On the other hand, independently constructed hand-tagged corpora are not mutually useful, and a corpus automatically acquired from the Web is shown to fail. Introduction Recent trends in word sense disambiguation (Ide & Veronis, 1998) show that the most effective paradigm for word sense disambiguation is that of supervised learning. Nevertheless, current literature has not shown that supervised methods can scale up to disambiguate all words in a text into reference (possibly fine-grained) word senses. Possible causes of this failure are: 1. Problem is wrongly defined: tagging with word senses is hopeless. We will not tackle this issue here (see discussion in the Senseval e-mail list – senseval-discuss@sharp.co.uk). 2. Most tagging exercises use idiosyncratic word senses (e.g. ad-hoc built senses, translations, thesaurus, homographs, ...) instead of widely recognized semantic lexical resources (ontologies like Sensus, Cyc, EDR, WordNet, EuroWordNet,  etc., or machine-readable dictionaries like OALDC, Webster's, LDOCE, etc.) which usually have fine-grained sense differences. We chose to work with WordNet (Miller et al. 1990). 3. Unavailability of training data: current handtagged corpora seem not to be enough for state-ofthe-art systems. We test how far can we go with existing hand-tagged corpora like SemCor (Miller et al. 1993) and the DSO corpus (Ng and Lee, 1996), which have been tagged with word senses from WordNet. Besides we test an algorithm that automatically acquires training examples from the Web (Mihalcea & Moldovan, 1999). In this paper we focus on one of the most successful algorithms to date (Yarowsky 1994), as attested in the Senseval competition (Kilgarriff & Palmer, 2000). We will evaluate it on both SemCor and DSO corpora, and will try to test how far could we go with such big corpora. Besides, the usefulness of hand tagging using WordNet senses will be tested, training on one corpus and testing in the other. This will allow us to compare hand tagged data with automatically acquired data. If new ways out of the acquisition bottleneck are to be explored, previous questions about supervised algorithms should be answered: how much data is needed, how much noise can they accept, can they be ported from one corpus to another, can they deal with really fine sense distinctions, performance etc. There are few indepth analysis of algorithms, and precision figures are usually the only features available. We designed a series of experiments in order to shed light on the above questions. In short, we try to test how far can we go with current hand-tagged corpora, and explore whether other means can be devised to complement handtagged corpora. We first present decision lists and the features used, followed by the method to derive data from the Web and the design of the experiments. The experiments are organized in three sections: experiments on SemCor and DSO,  cross-corpora experiments, and tagging SemCor using the Web data for training. Finally some conclusions are drawn.  
Corpus annotation is now a key topic for all areas of natural language processing (NLP) and information extraction (IE) which employ supervised learning. With the explosion of results in molecular-biology there is an increased need for IE to extract knowledge to support database building and to search intelligently for information in online journal collections. To support this we are building a corpus of annotated abstracts taken from National Library of Medicine’s MEDLINE database. In this paper we report on this new corpus, its ontological basis, and our experience in designing the annotation scheme. Experimental results are shown for inter-annotator agreement and comments are made on methodological considerations. 
We are annotating a corpus with information relevant to discourse entity realization, and especially the information needed to decide which type of NP to use. The corpus is being used to study correlations between NP type and certain semantic or discourse features, to evaluate hand-coded algorithms, and to train statistical models. We report on the development of our annotation scheme, the problems we have encountered, and the results obtained so far. 
Various kinds of video recordings have discourse structures. Therefore, it is important to determine how video segments are combined and what kind of coherence relations they are connected with. In this paper, we propose a method for estimating the discourse structure of video news reports by analyzing the discourse structure of their transcripts. 1. INTRODUCTION A large number of studies have been made on video analysis, especially segmentation, feature extraction, indexing, and classiﬁcation. On the other hand, little attention has been given to the discourse structure (DS) of video data. Various kinds of video recordings, such as dramas, documentaries, news reports, and sports castings, have discourse structures. In other words, each video segment of these video recordings is related to previous ones by some kind of relation (coherence relation) which determines the role of the video segments in discourse. For this reason, it is important to determine how video segments are combined and what kind of coherence relations they are connected with. In addition, Nagao et.al proposed a method for multimedia data summarization using GDA tags [Nagao 00]. However, the cost of making GDA tagged data is great. Our method will be helpful in reducing the annotation cost. In this paper, we propose a method for estimating the discourse structure of video news reports. Coherence relations between video segments are estimated in the following way: 1. a video news article is segmented into shots by using DCT components, 2. consecutive shots are merged by using speech information, and  discourse structure discourse unit shots news video  discourse structure analysis ( using transcripts ) merge shots ( using speech information) segmentation ( using image information)  Figure 1: Procedure of discourse structure analysis for news video  3. coherence relations are estimated by using three kinds of clues in the transcript of the news video: • clue expressions indicating a certain relation, • occurrence of identical/synonymous words/ phrases in topic chaining or topic-dominant chaining relation, and • similarity between two sentences in list or contrast relation. Figure 1 shows the procedure of discourse structure analysis for news video. We applied our method to NHK1 News 2. This method is aimed to make the process of retrieval, summarization, and information extraction more e cient. 1Nippon Hoso Kyokai (Japan Broadcasting Corporation) 2NHK news reports do not have closed captions. Instead of closed captions, we used scripts which were read out by newscasters as transcripts.  shot (a)  shot (b)  shot (c)  (I) The US dollar inched up against the yen as the stock market continued the selling trend in Tokyo today.  (II) The US currency traded at 145.63–65 yen at 5 p.m. Tokyo.  Figure 2: An example of shots and their transcript in a news video (NHK evening news, August/3/1998)  2. DISCOURSE STRUCTURE AND VIDEO Little attention has been given to discourse structure of video data in image processing. This is because it is di cult to determine it only by analyzing image data. In contrast to this, discourse structure is the subject of a large number of studies in natural language processing. So several methods for estimating the discourse structure of a text have been explored[Sumita 92] [Kurohashi 94]. Therefore, these methods can be applied to language data of video data in order to determine discourse structure in video data. In addition, some researchers in natural language processing showed that the information of discourse structure is useful for extracting signiﬁcant sentences and summarizing a text [Miike 94] [Marcu 97]. It suggests that information of video discourse structure is utilized for extracting signiﬁcant video segments and skimming. It may be useful to look at video skimming and extraction of signiﬁcant segments before we discuss some points about discourse structure analysis because they are closely related to the discourse structure estimation. One of the simple ways to skim a video is by using the pair of the ﬁrst frame/image of the ﬁrst shot and the ﬁrst sentence in the transcript. However, this representative pair of image and language is often a poor topic explanation. To solve this problem, Zhang, et.al, proposed a method for keyframe selection by using several image features such as colors, textures, and temporal features including camera operations [Zhang 95]. Also, Smith and Kanade proposed video skimming by selecting video segments based on TFIDF, camera motion, human  face, captions on video, and so on [Smith 97]. These techniques are broadly applicable, however, still have problems. One is the semantic classiﬁcation of each segment. To solve this problem, Nakamura and Kanade proposed the spotting by association method which detects relevant video segments by associating image data and language data [Nakamura 97]. Also, Watanabe, et.al, proposed a method for analyzing telops (captions) in video news reports by using layout and language information [Watanabe 96]. However, these studies did not deal with coherence relations between video segments. In contrast to this, several works on discourse structure have been made by researchers in natural language processing. Pursuing these studies, we are confronted with two points of discourse structure analysis: • available knowledge for estimating discourse structure, and • deﬁnition for discourse units and coherence relations. First, we shall discuss the available knowledge for estimating discourse structure. Most studies on discourse structure have focused on such questions as what kind of knowledge should be employed, and how inference may be performed based on such knowledge (e.g., [Grosz 86], [Hobbs 85], [Zadrozny 91]). In contrast to this, Kurohashi and Nagao pointed out that a detailed knowledge base with broad coverage is unlikely to be constructed in the near future, and that we should analyze discourses using presently available knowledge. For these reasons, they proposed a method for estimating discourse structure by using surface information  in sentences [Kurohashi 94]. In video analysis, the same problems occurred. Therefore, we propose here a method for estimating the discourse structure in a news report by using surface information in the transcript. Next, we shall discuss the deﬁnition for discourse unit and coherence relation. As mentioned, discourses are composed of segments (discourse units), and these are connected to previous ones by coherence relations. However, there has been a variety of deﬁnitions for discourse unit and coherence relation. For example, a discourse unit can be a frame, a shot, or a group of several consecutive shots. In this study, we consider as a discourse unit, one or more shots which are associated with one part of announcer’s speech. For example, shot (a), (b), and (c) in Figure 2 represent consecutive shots in the news video “Both yen and stock were dropped”(Aug/3/1998), while sentence (I) and (II) are parts of the transcript. Sentence (I) was spoken in shot (a) and (b), and correspondingly, sentence (II) was spoken in shot (c). As a result, shot (a) and (b) were merged and the result was considered as one discourse unit. On the other hand, shot (c) alone constituted one discourse unit. We will explain how to extract the discourse units in Section 3.1. In contrast to this, coherence relations strongly depend on the genre of video data: dramas, documentaries, news reports, sports castings, and so on. From the number of coherence relations suggested so far, we selected the following relations for our target, news reports: List: Si and Sj involve the same or similar events or states, or the same or similar important constituents Contrast: Si and Sj have distinct events or states, or contrasting important constituents Topic chaining: Si and Sj have distinct predications about the same topic Topic-dominant chaining: A dominant constituent apart from a given topic in Si becomes a topic in Sj Elaboration: Sj gives details about a constituent introduced in Si Reason: Sj is the reason for Si Cause: Sj occurs as a result of Si Example: Sj is an example of Si where Si denotes the former segment and Sj the latter.  3. ESTIMATION OF DISCOURSE STRUCTURE Our determination of how video segments are combined and what kind of coherence relations are involved is made in the next way: 1. extract discourse units from a news report, 2. extract three kinds of clue information from transcripts, and then, transform them into reliable scores for some relations, and 3. choose the connected sentence and the relation having the maximum reliable score. If two or more connected sentences have the same maximum score, the chronological nearest segment is selected. 3.1. Extraction of Discourse Units A shot is generally regarded as a basic unit in video analysis. In this study, however, not only a shot but also more consecutive ones are considered a basic unit (discourse unit). This is because there are some cases where several consecutive shots correspond with one sentences in a transcript. In this case, these consecutive shots should be regarded as a discourse unit. In contrast to this, one shot should be regarded as a discourse unit when it correspond with one or more sentences in a transcript. In both cases, the start/end point of a discourse unit often lies in the pause because the announcer needs to take breath at the end of a sentence. As a result, discourse units are extracted in the next way: 1. detect scene cuts in a video by using DCT components [Iwanari 94], 2. detect speech pauses in the video, and 3. extract the start/end points of discourse units by detecting the cuts in the pause. For evaluating this method, we used 105 news reports of NHK News. The recall and precision of discourse unit detection were 71% and 97%, respectively, while those of scene change detection were 80% and 90%. We modiﬁed the extracted discourse units by hand and used them in the discourse structure analysis described in Section 3.2. In addition, each discourse unit was associated with the corresponding sentences in a transcript by hands. This is because NHK news reports do not have closed captions.  3.2. Detection of Coherence Relations  Rule-1  In order to extract discourse structure, we use three  range : 1  kinds of clue information in transcripts: • clue expressions indicating some relations, • occurrence of identical/synonymous words/phrases in topic chaining or topic-dominant chaining  relation of CS : * CS : * NS : nazenara ( because ) * relation : reason score : 20  relation, and  Rule-2  • similarity between two sentences in list or contrast relation.  range : *  relation of CS : *  CS :  NS : *  Then they are transformed into reliable scores for some relations. In other words, as a new sentence (NS) comes in, reliable scores for all possible connected sentences and relations are calculated by using above three types of clues. As a ﬁnal result, we choose the connected sentence (CS) and the rela-  *  X no  X *  ( of ) rei  *  ( example )  *  relation : exemplification-present  score : 30  tion having the maximum reliable score.  Figure 3: Examples of heuristic rules for clue ex-  pressions  3.2.1. Detection of Clue Expressions  In this study, we use 41 heuristic rules for ﬁnding clue expressions by pattern matching and relating them to proper relations with reliable scores. A rule consists of two parts: (1) conditions for rule application and (2) corresponding relation and reliable score. Conditions for rule application consist of four parts: • rule applicable range, • relation of CS to its previous DS, • dependency structure pattern for CS, and • dependency structure pattern for NS. Pattern for CS and NS are matched not for word sequences but for dependency structures of both sentences. We apply each rule for the pairs of a CS and NS. If the condition of the rule is satisﬁed, the speciﬁed reliable score is given to the corresponding relation between the CS and the NS. For example, Rule-1 in Figure 3 gives a score (20 points) to the reason relation between two adjoining sentences if the NS starts with the expression “nazenara (because)”. Rule-2 in Figure 3 is applied not only for the neighboring CS but also for farther CSs, by specifying the occurrence of identical words “X” in the condition. 3.2.2. Detection of Word/Phase Chain In general, a sentence can be divided into two parts: a topic part and a non-topic part. When two sentences are in a topic chaining relation, the same  topic is maintained through them. Therefore, the occurrence of identical/synonymous word/phrase (the word/phrase chain) in topic parts of two sentences supports this relation. On the other hand, in the case of topic-dominant chaining relation, a dominant constituent introduced in a non-topic part of a prior sentence becomes a topic in a succeeding sentence. As shown, the word/phrase chain from a non-topic part of a prior sentence to a topic part of a succeeding sentence supports this relation. For these reasons, we detect word/phrase chains and calculate reliable scores in the next way:  1. give scores to words/phrases in topic and nontopic parts according to the degree of their importance in sentences,  2. give scores to the matching of identical/synonymous words/phrases according to the degree of their agreement, and  3. give these relations the sum of the scores of two chained words/phrases and the score of their matching.  For example, by Rule-a and Rule-b in Figure 4,  words in a phrase whose head word is followed by  a topic marking postposition “ ” are given some wa scores as topic parts. Also, a word in a non-topic  part in the sentential style, “  (there is ...)” is  ga aru  given a large score (11 points) by Rule-c in Figure 4  because this word is an important new information  in this sentence and topic-dominant chaining rela-  tion involving it often occur. Matching of phrases  Topic part Rule-a pattern : * wa score : 10  Rule-b pattern : * score : 8  * * wa  Non-topic part Rule-c pattern : * ga  score : 11  aru ( there is )  Matching Rule-d pattern : X * score : 5 Rule-e pattern : X *  x * x *  Y * score : 8 Rule-f pattern : X Y * score : 6  y *  x  no niyoru  ( of | by ) y *  Figure 4: Examples of rules for topic/non-topic parts  like “A of B” is given a larger score (8 points) by Rule-e than that of word like “A” alone by Rule-d (5 points) in Figure 4. 3.2.3. Calculation of Similarity between Sen- tences in a Transcript When two sentences have list or contrast relation, they have a certain similarity. As a result, we measure such a similarity for ﬁnding list or contrast relation in the next way. First, the similarity value between two words are calculated according to exact matching, matching of their parts of speech, and their closeness in a thesaurus dictionary. Second, the similarity value between two word-strings is calculated roughly by combining the similarity values between words in the two word-strings with the dynamic programming method for analyzing conjunctive structures [Kurohashi 94]. Then, we give the normalized similarity score between a CS and an NS to their list and contrast relations as a reliable score. 4. EXPERIMENTS AND DISCUSSION For evaluating this method, we used 22 news reports of NHK News. Each report was a few minutes in length. The experimental results are shown in Table 1. As mentioned, news reports of NHK News do not have closed captions. For this reason, each video segment (discourse unit) was associated with the corresponding sentences in a transcript by hands.  Table 1: Analysis results  Relation  Success  List  2  Contrast  
Number of moras in a logical scene  Roughly aligned logical scenes Adjustment of logical scene boundaries Aligned logical scenes  Time of shot change in motion image  Feedback  Sentence alignment  Number of moras in a sentence  Roughly aligned sentences  Speech recognition Recognized words  A language model generated from speech lines  Results of alignment  Word alignment The sound track aligned to the speech lines  Sentences of the speech lines  b’mj B’  b’mj-1  (i-p+1,j) 1 (i-p,j-1)  p  
This paper proposes an easy and simple method for constructing a super-structure on the Web which provides current Web contents with new value and new means of use. The super-structure is based on external annotations to Web documents. We have developed a system for any user to annotate any element of any Web document with additional information. We have also developed a proxy that transcodes requested contents by considering annotations assigned to them. In this paper, we classify annotations into three categories. One is linguistic annotation which helps the transcoder understand the semantic structure of textual elements. The second is commentary annotation which helps the transcoder manipulate non-textual elements such as images and sounds. The third is multimedia annotation, which is a combination of the above two types. All types of annotation are described using XML, and correspondence between annotations and document elements is deﬁned using URLs and XPaths. We call the entire process “semantic transcoding” because we deal with the deep semantic content of documents with annotations. The current semantic transcoding process mainly handles text and video summarization, language translation, and speech synthesis of documents including images. 
Semantic Annotation is a basic technology for intelligent content and is beneﬁcial in a wide range of contentoriented intelligent applications. In this paper we present our work in ontology-based semantic annotation, which is embedded in a scenario of a knowledge portal application. Starting with seemingly good and bad manual semantic annotation, we describe our experiences made within the KA -initiative. The experiences gave us the starting point for developing an ergonomic and knowledge base-supported annotation tool. Furthermore, the annotation tool described are currently extended with mechanisms for semi-automatic information-extraction based annotation. Supporting the evolving nature of semantic content we additionally describe our idea of evolving ontologies supporting semantic annotation. 
Context free grammars parse faster than TFS grammars, but have disadvantages. On our test TFS grammar, precompilation into CFG results in a speedup of 16 times for parsing without taking into account additional mechanisms for increasing parsing efficiency. A formal overview is given of precompilation and parsing. Modifications to ALE rules permit a closure over the rules from the lexicon, and analysis leading to a fast treatment of semantic structure. The closure algorithm, and retrieval of full semantic structure are described. Introduction Head Driven Phrase Structure Grammar (HPSG), Pollard and Sag (1994) is expressed in Typed Feature Structures (TFSs). Context Free Grammar (CFG) without features supports much faster parsing, but a TFS grammar has many advantages. Fast parsing can be obtained by precompiling a CFG approximation, with TFSs converted into CF near-equivalents. CFG parsing eliminates impossible trees, and TFS unification over the remainder eliminates more, and instantiates path values. Our method treats slashes separately in a precompiled table, and careful allocation of categories to TFSs makes TFS unification unnecessary: instead skeleton semantic structures are formed in parsing, and full structures retrieved afterwards. A prototype precompiler and fast parser1 were built in Prolog, and tested with an HPSG grammar of English by Matheson (1996), 
In this paper, we identify syntactic lexical ambiguity and sentence complexity as factors that contribute to parsing complexity in fully lexicalized grammar formalisms such as Lexicalized Tree Adjoining Grammars. We also report on experiments that explore the effects of these factors on parsing complexity. We discuss how these constraints can be exploited in improving efﬁciency of parsers for such grammar formalisms. 
is an indian man yes? v DET ADJ N ADV  would like you this is what he does every day # and i  would like 11\tyou ## i  would like you to tell me.  Au>IV v  v Wh• Pron V ADJ N  v Conj Pron Au>IV  Pron  v Pron Au>IV  Pron  v Pron  is doing. v N = is an v DET  GATE List View  utterance  pos  ADV  utterance  pos  v  utterance pos utterance pos  excuse me i  doni understand.  v  Pron Pron Au>IV V  
The eXtensible Markup Language (XML) (Bray, et al., 1998) is the emerging standard for data representation and exchange on the World Wide Web. The XML Framework includes very powerful mechanisms for accessing and manipulating XML documents that are likely to significantly impact the development of tools for processing natural language and annotated corpora. Introduction All language processing applications, including machine translation, information retrieval and extraction, text summarization, user/machine dialogue systems, and speech understanding and synthesis, manipulate language data represented in some electronic format. Some applications (e.g., machine translation, summarization, speech understanding) process streams of data more or less sequentially, while others (e.g., retrieval and extraction) rely more heavily on search and access over large bodies of data. In either case, processing exploits the markup in the data to assist in the analysis. For example, in textual data, markup for logical structure (e.g., section, paragraph, and sentence boundaries, etc.) provides essential information for any language processing task. In addition, markup identifying terms, foreign words, names, dates, etc. can be exploited for tasks such as machine  translation and information retrieval, while identification of titles, footnotes, and other extra-textual matter can be used to limit the data to be searched. Because the data that will be analyzed by language processing applications in the future will consist largely of documents delivered over the World Wide Web, the markup format these applications process will be XML. The language processing community also creates text and speech data for training statistical language processing algorithms. The cost of creating annotated data can be very high, both in direct financial terms and in terms of the cost of allocating skilled labor. So funders, whether public or commercial, have come to expect that the cost of resource creation will be amortized over multiple research and development efforts. Such reusability demands the use of standardized, non-proprietary encoding formats for data interchange and to enable easy human-readable display and access to data. For the applications we are now beginning to develop, these formats must support multi-lingual, multi-media, and multimodal data, as well as linkage among them. As an international standard, the eXtensible Markup Language (XML) (Bray, et al., 1998) is the obvious basis for a standardized encoding format, and is or will be used by several language processing projects (e.g., LT XML1, ATLAS2, XCES3, ANC4). At its most basic level 
E-rater is an operational automated essay scoring application. The system combines several NLP tools that identify linguistic features in essays for the purpose of evaluating the quality of essay text. The application currently identifies a variety of syntactic, discourse, and topical analysis features. We have maintained two clear visions of e-rater’s development. First, new linguistically-based features would be added to strengthen connections between human scoring guide criteria and e-rater scores. Secondly, e-rater would be adapted to automatically provide explanatory feedback about writing quality. This paper provides two examples of the flexibility of e-rater’s modular architecture for continued application development toward these goals. Specifically, we discuss a) how additional features from rhetorical parse trees were integrated into e-rater, and b) how the salience of automatically generated discourse-based essay summaries was evaluated for use as instructional feedback through the re-use of erater’s topical analysis module. 
This paper presents a methodology that aims at building knowledge models from a natural language description of a domain. Our methodology is based on the establishment of a dialogue with the knowledge engineer of an application. This dialogue is motivated by the Semantic Differentiation Process, which solves problems related to acquisition and modelling. Moreover, the dialogue can be naturally formalised within a theory of communicating rational agents. We can thus consider a more complete automation of the process of modelling and show how to integrate our methodology into this type of theory. Introduction Knowledge Based Systems separate the semantic model - which handles the system knowledge - from the reasoning process which uses this knowledge. The main advantage of this approach is that only the semantic model has to be changed to handle a different application domain. However, the creation of a semantic model for a given application is a manual process, which is difficult to automate (Paris and Vander Linden (1996)). Tools (Heijst et al. (1997)) or workbenches ((Mikheev and Finch (1995), (Delisle (1996)) already exist that aim at building semantic representations at the domain level (using the vocabulary of KADS (Wielinga et al. (1992)). With these tools and workbenches, conceptual knowledge models (like ontologies) independent of the application domain are built. However, the knowledge engineer task remains fastidious. One of the difficulties in  completely automating the acquisition and modelling process comes from a lack of interaction with the knowledge engineer. In order to improve these interactions (and thus to facilitate modelling), we propose a methodology based on a natural language dialogue with the knowledge engineer. This methodology can be implemented into a rational agent. In this way, this agent is given capabilities of modelling by means of conceptual diagrams defined in our methodology. We show how to make this integration within the formal theory of communicating rational agents of Sadek (Sadek (1991), (Sadek et al. (1997))). Section 1 introduces the bases of the methodology. Section 2 explains how to integrate it into a theory of rational agents for its effective implementation. The last section presents the guidelines to implement our methodology into a rational agent. 
Certain generation applications may profit from the use of stochastic methods. In developing stochastic methods, it is crucial to be able to quickly assess the relative merits of different approaches or models. In this paper, we present several types of intrinsic (system internal) metrics which we have used for baseline quantitative assessment. This quantitative assessment should then be augmented to a fuller evaluation that examines qualitative aspects. To this end, we describe an experiment that tests correlation between the quantitative metrics and human qualitative judgment. The experiment confirms that intrinsic metrics cannot replace human evaluation, but some correlate significantly with human judgments of quality and understandability and can be used for evaluation during development. 
We present an evaluation framework in which the effectiveness of evaluative arguments can be measured with real users. The framework is based on the task-efficacy evaluation method. An evaluative argument is presented in the context of a decision task and measures related to its effectiveness are assessed. Within this framework, we are currently running a formal experiment to verify whether argument effectiveness can be increased by tailoring the argument to the user and by varying the degree of argument conciseness. Introduction Empirical methods are fundamental in any scientific endeavour to assess progress and to stimulate new research questions. As the field of NLG matures, we are witnessing a growing interest in studying empirical methods to evaluate computational models of discourse generation (Dale, Eugenio et al. 1998). However, with the exception of (Chu-Carroll and Carberry 1998), little attention as been paid to the evaluation of systems generating evaluative arguments, communicative acts that attempt to affect the addressee's attitudes (i.e. evaluative tendencies typically phrased in terms of like and dislike or favor and disfavor). The ability to generate evaluative arguments is critical in an increasing number of online systems that serve as .personal :assistants, advisors, or sales asslstants ~. For instance, a travel assistant may need to compare two vacation packages and argue that its current user should like one more than the other. i See for instance www.activebuyersguide.com  In this paper, we present an evaluation framework in which the effectiveness of evaluative arguments can be measured with real users. The measures of argument effectiveness used in our framework are based on principles developed in social psychology to study persuasion (Miller and Levine 1996). We are currently applying the framework to evaluate arguments generated by an argument generator we have developed (Carenini 2000). To facilitate the evaluation of specific aspects of the generation process, the argument generator has been designed so that its functional components can be easily turned-offor changed. In the remainder of the paper, we first describe our argument generator. Then, we summarize literature on persuasion from social psychology. Next, we discuss previous work on evaluating NLG models. Finally, we describe our evaluation framework and the design of an experiment we are currently running. 
We present discourse annotation work aimed at constructing a parallel corpus of Rhetorical Structure trees for a collection of Japanese texts and their corresponding English translations. We discuss implications of our empirical findings for the task of text planning in the context of implementing multilingual natural language generation systems. 
The use of XML-based authoring tools is swiftly becoming a standard in the world of technical documentation. An XML document is a mixture of structure (the tags) and surface (text between the tags). The structure reflects the choices made by the author during the top-down stepwise refinement of the document under control of a DTD grammar. These choices are typically choices of meaning which are independent of the language in which the document is rendered, and can be seen as a kind of interlingua for the class of documents which is modeled by the DTD. Based on this remark, we advocate a radicalization of XML authoring, where the semantic content of the document is accounted for exclusively in terms of choice structures, and where appropriate rendering/realization mechanisms are responsible for producing the surface, possibly in several languages simultaneously. In this view, XML authoring has strong connections to natural language generation and text authoring. We describe the IG (Interaction Grammar) formalism, an extension of DTD's which permits powerful linguistic manipulations, and show its application to the production of multilingual versions of a certain class of pharmaceutical documents. 
 made in the development of suitable markup  Extensively annotated bilingual parallel corpora can be exploited to feed editing tools that integrate the processes of document composition and translation. Here we discuss the architecture of an interactive editing tool that, on top of techniques common to most Translation Memory-based systems, applies the potential of SGML's DTDs to guide the process of bilingual document generation. Rather than employing just simple task-oriented mark-up, we selected a set of TEI's highly complex and versatile collection of tags to help disclose the underlying logical structure of documents in the test-corpus. DTDs were automatically induced and later integrated in the editing tool to provide the basic scheme for new documents.  options to encode a variety of textual types and functions -as clearly demonstrated by the Text Encoding Initiative, TEI; (Burnard & SpebergMacQueen, 1995). While the tag-sets employed by TMBS are simple and task-oriented, TEI has offered a highly complex and versatile collection of tags. The guiding hypothesis in our experiment has been the idea that it is possible to explore TEI/SGML markup in order to develop a system that carries the concept of Translation Memory one step further. One important leature of SGML is the DTD. DTDs determine the logical structure of documents and how to tag them accordingly. We have concentrated on the accurate description of documents by means of TEI conformant SGML markup. The markup will help disclose the underlying logical struc-  
We describe a mechanism which generates rebuttals to a user's rejoinders in the context of arguments generated from Bayesian networks. This mechanism is implemented in an interactive argumentation system. Given an argument generated by the system and an interpretation of a user's rejoinder, the generation of the rebuttal takes into account the intended effect of the user's rejoinder, determined on a model of the user's beliefs, and its actual effect, determined on a model of the system's beliefs. We consider three main rebuttal strategies: refute the user's rejoinder, strengthen the argument goal, and dismiss the user's line of reasoning. 
We propose an argumentation strategy for generating evaluative arguments that can be applied in systems serving as personal assistants or advisors. By following guidelines from argumentation theory and by employing a quantitative model of the user's preferences, the strategy generates arguments that are tailored to the user, properly arranged and concise. Our proposal extends the scope of previous approaches both in terms of types of arguments generated, and in terms of compliance with principles from argumentation theory. Introduction Arguing involves an intentional communicative act that attempts to create, change or reinforce the beliefs and attitudes of another person. Factual and causal arguments attempt to affect beliefs (i.e. assessments that something is or is not the case), whereas evaluative arguments attempt to affect attitudes (i.e., evaluative tendencies typically phrased in terms of like and dislike or favor and disfavor). With the ever growing use of the Web, an increasing number of systems that serve as personal assistants, advisors, or sales assistants are becoming available online ~. These systems frequently need to generate evaluative arguments for domain entities. For instance, a real-estate assistant may need to compare two houses, arguing that one would be a better choice than the other for its user. Argumentation theory (Mayberry and Golden 1996; Miller and Levine 1996; Corbett and Connors 1999) indicates that effective arguments should be constructed tbllowing three  general principles. First, arguments should be constructed considering the dispositions of the audience towards the information presented. Second, sub-arguments supporting or opposing the main argument claim should be carefully arranged by considering their strength of support or opposition. Third, effective arguments should be concise, presenting only pertinent and cogent information. In this paper, we propose an argumentation strategy for generating evaluative arguments that can be applied in systems serving as personal assistants or advisors. By following principles and guidelines from argumentation theory and by employing a quantitative model of the user's preference, our strategy generates evaluative arguments that are tailored to the user, properly arranged and concise. Although a preliminary version of our argumentative strategy was cursorily described in a previous short paper (Carenini and Moore 1999), this paper includes several additional contributions. First, we discuss how the strategy is grounded in the argumentation literature. Then, we provide details on the measures of argument strength and importance used in selecting and ordering argument support. Next, we generalize the argumentative strategy and correct some errors in its preliminary version. Finally, we discuss how our strategy extends the scope of previous approaches to generating evaluative arguments in terms of coverage (i.e., types of arguments), and in terms of compliance with principles from argumentation theory. Because of space limitations, we only discuss' previous work on generating evaluative arguments, rather than previous work on generating arguments in general.  See llbr instance www.activebuyersguide.com  47  
During argumentation, people persuade their audience using a variety of strategies, e.g., hypothetical reasoning, reasoning by cases and ordinary premise-to-goal arguments. In this paper, we of-  Premise to goal: Corrective lenses are required. "Being unable to see far objects is evidence for myopia, which indicates that corrective lenses are required." Reductio ad absurdum: There has always  fer an operational definition of the conditions for  been matter.  pursuing these strategies, and incorporate into a  "There could never have been a time when  Bayesian argument-generation system a mechanism  nothing existed, for, if there were [hypotheti-  for proposing applicable argumentation strategies,  cal assumption], then nothing would exist now,  generating specific arguments based on these strategies, and selecting a final argument.  since from nothing comes nothing." (from St. Thomas Aquinas) Inference to the best explanation: Patient  
Based on our experiences in VERBMOBIL, a large scale speech-to-speech translation system, we identify two types of problems that a generation component must address in a realistic implementation and present relevant examples. As an extension to the architecture of a translation system, we present a module for robustness preprocessing on the interface between translation and generation. 
 2. Which aspects of the RAGS repertoire would  :. . . . . . . . -.... -,.= ., ~,~,aemaltybe'requireti~ftrr~strch~a-~reinterpretation;  The RAGS project aims to define a reference ar-  which would be unnecessary and which addi-  chitecture for Natural Language Generation (NLG)  tions to the RAGS repertoire would be moti-  systems. Currently the major part of this archi-  vated.  tecture consists of a set of datatype definitions for  specifying the input and output formats for mod-  3. Whether studying the system would generate  ules within NLG systems. In this paper we describe  good ideas about possible reusable generation  our efforts to reinterpret an existing NLG system in  modules that could be developed.  terms of these definitions. The system chosen was the Caption Generation System.  In this exercise it was important to choose a system that had been developed by people outside the  
This paper describes an implemented system which uses centering theory for planning of coherent texts and choice of referring expressions. We argue that text and sentence planning need to be driven in part by the goal of maintaining referential continuity and thereby facilitating pronoun resolution: obtaining a favourable ordering of clauses, and of arguments within clauses, is likely to increase opportunities for non-ambiguous pronoun use. Centering theory provides the basis for such an integrated approach. Generating coherent texts according to centering theory is treated as a constraint satisfaction problem. 
In this paper we present a psycholinguistically motivated architecture and its prototypical implementation for an incremental conceptualizer, which monitors dynamic changes in the world and simultaneously generates warnings for (possibly) safety-critical developments. It does so by conceptualizing events and building up a hierarchical knowledge representation of the perceived states of affairs. If it detects a safety problem, it selects suitable elements from the representation for a warning, brings them into an appropriate order, and generates incremental preverbal messages (propositional structures) from them, which can be taken by a subsequent component to encode them linguistically. 
When a lexical item is selected in the language production process, it needs to be explained why none of its superordinates gets selected instead, since their applicability conditions are fulfilled all the same. This question has received much attention in cognitive modelling and not as much in other branches of NLG. This paper describes the various approaches taken, discusses the reasons why they are so different, and argues that production models using symbolic representations should make a distinction between conceptual and lexical hierarchies, which can be organized along fixed levels as studied in (some branches of) lexical semantics. 
In this paper, we describe how quantifiers can be generated in a text generation system. By taking advantage of discourse and ontological information, quantified expressions can replace entities in a text, making the text more fluent and concise. In addition to avoiding ambiguities between distributive and collective readings in universal quantification generation, we will also show how different scope orderings between universal and existential quantitiers will result in different quantified expressions in our algorithm. 
It is not a rare phenomenon for human written text to use non-restrictive NP modifiers to express essential pieces of information or support the situation presented in the main proposition containing the NP, for example, "Private Eye, which couldn't afford the libel payment, had been threatened with closure." (from Wall Street Journal) Yet no previous research in NLG investigates this in detail. This paper describes corpus analysis and a psycholinguistic experiment regarding the acceptability of using non-restrictive NP modifiers to express semantic relations that might normally be signalled by 'because' and 'then'. The experiment tests several relevant factors and enables us to accept or reject a number of hypotheses. The results are incorporated into an NLG system based on a Genetic Algorithm. 
A range of research has explored the problem of generating referring expressions that uniquely identify a single entity from the shared context. But what about expressions that identify sets of entities? In this paper, I adapt recent semantic research on plural descriptions--using covers to abstract collective and distributive readings and using sets of assignments to represent dependencies among references--to describe a search problem for set-identifying expressions that largely mirrors the search problem for singular referring expressions. By structuring the search space only in terms of the words that can be added to the description, the proposal defuses potential combinatorial explosions that might otherwise arise with reference to sets.  As (lc) records, these are the intersecting dotted segments of (1 a), and can be designated as such. Or again, we find distinguished in (2b) five elements of (2a), which might hold some independent interest. So we can and should identify these elements, and (2c), the squares clustered at the lower left, will do the trick.  []  []  0  ••  (2) a  0 [] [30 [] [] []  O0 00[ 3 [] []  
We present a new approach to paratactic content aggregation in the context of generating hypertext summaries of OLAP and data mining discoveries. Two key properties make this approach innovative and interesting: (1) it encapsulates aggregation inside the sentence planning component, and (2) it relies on a domain independent algorithm working on a data structure that abstracts from lexical and syntactic knowledge. 
 text. ILEX generates descriptions of database ob-  This paper outlines a text generation system suited to a large class of information sources, relational databases. We focus on one aspect of the problem: the additional information which needs to be specified to produce reasonable text quality when generating from relational databases. We outline how databases need to be prepared, and then describe various types of domain semantics which can be used to improve text qualify.  jects on the fly, taking into account the user's context of browsing. Figure 1 shows the ILEX web interface, as applied to a museum domain, in this case the Twentieth Century Jewellery exhibition at the the National Museum of Scotland. 2 The links to related database objects are also automatically generated. ILEX has been applied to other domains, including personnel (Nowson, 1999), and a sales catalogue for computer systems and peripherals (Anderson and Bradshaw, 1998).  
 veloped for CL users, the best known being con-  This paper argues for looking at Controlled Languages (CL) from a Natural Language Generation (NLG) perspective. We show that CLs are used in a normative environment in which different textual modules can be identified, each having its own set of rules constraining the text. These rules can be used as a basis for natural language generation. These ideas were tested in a proof of concept generator for the domain of aircraft maintenance manuals.  formity checkers/controllers such as AlethCL or SECC (CLA, 1996). A writer expects that the checking tool should not only detect errors but also propose a CL conformable expression. A. Nasr (Nasr, 1996), who worked on the problem of CL refornmlation, underlines the difficulties of this task. Reformulation cannot make any hypotheses about the conformity of the input sentences, and therefore must deal with a wider variety of lexico-syntactical constructions than those al-  
This paper describes a novel functionality of the VERBMOBIL system, a large scale translation system designed for spontaneously spoken multilingual negotiation dialogues. The task is the on-demand generation of dialogue scripts and result summaries of dialogues. We focus on summary generation and show how the relevant data are selected from the dialogue memory and how they are packed into an appropriate abstract representation. Finally, we demonstrate how the existing generation module of VERBMOBIL was extended to produce multilingual and result summaries from these representations. 
Word order and accent placement are the primary linguistic means to indicate focus/background structures in German. This paper presents a pipelined architecture for the generation of German monologues with contextually appropriate word order and accent placements for the realization of focus/background structures. Our emphasis is on the sentence planner that extends the respective propositional contents with discourse-relational features and decides which part will be focused. Such an enriched semantic input for an HPSG-based formulator allows word order variations and the placement of prenucleus and nucleus accents. Word order is realized by grammatical competition based on linear precedence (LP) rules which are based on the discourserelational features. Accent placement is realized by a syntax-driven focus principle that determines the focus exponent and possible bearers of prenucleus accents within the syntactically realized focus, the so-called focus domain. 
We present a new approach to enriching underspecified representations of content to be realized as text. Our approach uses an attribute grammar to propagate missing information where needed in a tree that represents the text to be realized. This declaratively-specified grammar mediates between application-produced output and the input to a generation system and, as a consequence, can easily augment an existing generation system. Endapplications that use this approach can produce high quality text without a fine-grained specification of the text to be realized, thereby reducing the burden to the application. Additionally, representations used by the generator are compact, because values that can be constructed from the constraints encoded by the grammar will be propagated where necessary. This approach is more flexible than defaulting or making a statistically good choice because it can deal with long-distance dependencies (such as gaps and reflexive pronouns). Our approach differs from other approaches that use attribute grammars in that we use the grammar to enrich the representations of the content to be realized, rather than to generate the text itself. We illustrate the approach with examples from our template-based textrealizer, YAG. 
 In this paper, we describe the generation of com-  We describe the generation of communicative actions in an implemented embodied conversational agent. Our agent plans each utterance so that multiple communicative goals may be realized opportunistically by a composite action including not only speech but also coverbal gesture that fits the context and the ongoing speech in ways representative of natural humanconversation. We accomplish this by reasoning from a grammar which describes gesture declaratively in terms of its discourse function, semantics and synchrony with speech.  municative actions in an implemented embodied conversational agent. Our generation framework adopts a goal-directed view of generation and casts knowledge about communicative action in the form of a grammar that specifies how forms combine, what interpretive effects they impart and in what contexts they are appropriate (Appelt, 1985; Moore, 1994; Dale, 1992; Stone and Doran, 1997). We expand this framework to take into account findings, by ourselves and others, on the relationship between spontaneous coverbal hand gestures and speech. In particular, our agent plans each utterance so that  
This paper deals with the generation of definite (i.e., uniquely referring) descriptions containing semantically vague expressions ('large', 'small', etc.). Firstly, the paper proposes a semantic analysis of vague descriptions that does justice to the contextdependent meaning of the vague expressions in them. Secondly, the paper shows how this semantic analysis can be implemented using a modification of the Dale and Reiter (1995) algorithm for the generation of referring expressions. A notable feature of the new algorithm is that, unlike Dale and Reiter (1995), it covers plural as well as singular NPs. This algorithm has been implemented in an experimental NLG program using ProFIT. The paper concludes by formulating some pragmatic constraints that could allow a generator to choose between different semantically correct descriptions. 
In natural language generation, different generation tasks often interact with each other in a complex way. We think that how to resolve the complex interactions inside and between tasks is more important to the generation of a coherent text than how to model each individual factor. This paper focuses on the interaction between aggregation and text planning, and tries to explore what preferences exist among the features considered by the two tasks. The preferences are implemented in two generation systems, namely ILEX-TS and a text planner using a Genetic Algorithm. The evaluation emphasises the second implementation and shows that capturing these preferences properly can lead to coherent text. 
Scott and Souza (1990) have posed the problem of how a rhetorical structure (in which propositions are linked by rhetorical relations, but not yet arranged in a linear order) can be realized by a text structure (in which propositions are ordered and linked up by appropriate discourse connectives). Almost all work on this problem assumes, implicitly or explicitly, that this mapping is governed by a constraint on compatibility of structure. We show how this constraint can be stated precisely, and present some counterexamples which seem acceptable even though they violate compatibility. The examples are based on a phenomenon we call extraposition, in which complex embedded constituents of a rhetorical structure are extracted and realized separately. 
In practical natural language generation systems it is often advantageous to have a separate component that deals purely with morphological processing. We present such a component: a fast and robust morphological generator for English based on finite-state techniques that generates a word form given a specification of the lemma, part-of-speech, and the type of inflection required. We describe how this morphological generator is used in a prototype system for automatic simplification of English newspaper text, and discuss practical morphological and orthographic issues we have encountered in generation of unrestricted text within this application. 
This paper presents the integration of a largescale, reusable lexicon for generation with the FUF/SURGE unification-based syntactic realizer. The lexicon was combined from multiple existing resources in a semi-automatic process. The integration is a multi-step unification process. This integration allows the reuse of lexical, syntactic, and semantic knowledge encoded in the lexicon in the development of lexical chooser module in a generation system. The lexicon also brings other benefits to a generation system: for example, the ability to generate many lexical and syntactic paraphrases and the ability to avoid non-grammatical output. 
We describe the knowledge acquisition (KA) techniques used to build the STOP system, especially sorting and think-aloud protocols. That is, we describe the ways in which we interacted with domain experts to determine appropriate user categories, schemas, detailed content rules, and so forth for STOP. Informal evaluations of these techniques suggest that they had some benefit, but perhaps were most successful as a source of insight and hypotheses, and should ideally have been supplemented by other techniques when deciding on the specific rules and knowledge incorporated into STOP. 
When generating utterances, humans may choose among a number of alternative sentence forms expressing the same propositional content. The context determines these decisions to a large extent. This paper presents a strategy to allow for such context-sensitive variation when generating text for a wearable, advice giving device. Several dimensions of context feed a model of the heater's attention space, which, in terms of Information Structure Theory, determines the form of the sentence to be generated. 
 2 Presentation of GTAG  " . . . . . . . . . . . . . We+will discuss the:somi-reeursiveaigorithm for .......-+-+,~he~GTAG.£ovmalismdescrihes.th~domain_  text generation, as defined for the GTAG  model used to specify the input of the generator,  formalism, and its implementation in the CLEF project. We will show how to use iexical choice constraints and properties of the LTAG grammar to minimize the backtracking of the semirecursive algorithm. 
In this paper we describe a neural networks approach to generation. The task is to generate sentences with hotel-information from a structured database. The system is inspired by Karen Kukich's ANA, but expands on it by adding generality in the form of language independence in representations and lexical look-up. Introduction In the growing field of intelligent communication (web-browsers, dialogue systems, etc.) the need for a flexible generator has become more important (e.g. Hovy & Lin, 1999). NLG is usually seen as a two-stage process where the planning component takes care of the inter-sentential content planning, while the surface realisation component transforms the content representation into a string of words. Interactions between the two components have called for the micro-planning stage to be postulated in the middle, but still the rule-based pipeline architecture has problems with sequential rules and their two-way relations. Statistical approaches have been developed, and seem to provide flexibility to generation tasks. The approach taken in this thesis, however, explores generation as .a .classification task whereby the representation that describes the intended meaning of the utterance is ultimately to be classified into an appropriate surface form. Although the task as such is a complex one, the approach allows its decomposition into a series of smaller classification tasks tbrmulated as input-output mappings rather than step-wise  rules. One of the goals of the thesis is to study the ways generation could .be broken down into suitable sub-classification tasks so as to enhance flexibility in the generation process in general. Artificial neural networks are a classification technique that is robust and resistant to noisy input, and learns to classify inputs on the basis of training examples, without specific rules that describe how the classification is to be done. There is not much research into using ANN's for generation, the main reason being long training times. Two notable exceptions are Kukich (1987) and Ward (1997), both argue in favour of NN's robustness, but at the same time point out problems with scalability. We believe that with improved computer facilities that shorten the training time, this new way of looking at generation as a classification task constitutes an interesting approach to generation. We have chosen Kukich's approach, as our application domain is to generate utterances from structured databases. This paper is structured as follows; we first discuss the general model. The second part briefly describes neural networks. We continue with describing a possible implementation of the model, and finally we draw some conclusions and point to future challenges. 
In this paper we report on several issues arising out of a first attempt to annotate task-oriented spoken dialog for rhetorical structure using Rhetorical Structure Theory. We discuss an annotation scheme we are developing to resolve the difficulties we have encountered. 
RSTTool is a graphical tool for annotating a text in terms of its rhetorical structure. The demonstration will show the various interfaces of the tool, focusing on its ease of use. 
We will demonstrate the ILEX system, a system which dynamically generates descriptions of database objects for the web, adapting the description to the discourse context and user type. Among other improvements in version 3, the system now generates from relational databases, and this demonstration will focus on this ability. We will also show how incremental extensions to the domain semantics improve the quality of the text produced. 
{ mcroy, songsak, syali} @cs. uwm. edu  Natural Language and Knowledge Representation Research Group ~.:..... a h t ~ p g / . / t i g g e r . c s , u w m a e d u / 7 n l k r r g Electrical Engineering and Computer Science Department University of Wisconsin-Milwaukee  
......... andcormider the difficult problem of generatiiig nat-  We present a system which uses lexical chains as an ural language texts from the representation.  intermediate representation for automatic text sum-  marization. This system builds on previous research  by implementing a lexical chain extraction algorithm 1.2 B a c k g r o u n d Research  in linear time. The system is reasonably domain in-  dependent and takes as input any text or HTML document. The system outputs a short summary based on the most salient concepts from the original document. The length of the extracted summary can be either controlled automatically, or manually  Much research has been conducted in the area of automatic text summarization. Specifically, research using lexical chains and related techniques has received much attention.  based on length or percentage of compression. While  Early methods using word frequency counts did  still under development, the system provides useful not consider the relations between similar words.  summaries which compare well in information con- Finding the aboutness of a document requires find-  tent to human generated summaries. Additionally, ing these relations. How these relations occur within  the system provides a robust test bed for future sum- a document is referred to as cohesion (Halliday  mary generation research.  and Hasan, 1976). First introduced by Morris and  Hirst (1991), lexical chains represent lexical cohe-  
 Each lexical semantic component projects a V-complement structure, the complement typically realizing the categorical type of the component, as shown:  V / \ V N I VM  V / \ V A I VS  V / \ V P I VL  M = manner/means/instrument S = state L = location  The full projections correspond roughly to the Vendler-Dowty aspectual classes:  process/ activity  change-of-state/ achievement  cause + change of state/ accomplishment  V I \ D V I /\ Jane V N I I 
A wide range of natural language problems can be viewed as disambiguating between a small set of alternatives based upon the string context surrounding the ambiguity site. In this paper we demonstrate that classification accuracy can be improved by invoking a more descriptive feature set than what is typically used. We present a technique that disambiguates by learning regular expressions describing the stnng contexts in which the ambiguity sites appear. 'C Introduction Many natural language tasks are essentially nway classification problems, where classification decisions are made from a small set of choices, based upon the linguistic context in which the ambiguity site occurs. Examples of such tasks include: confusable word set disambiguation; word sense disambiguation; determining such lexical features as pronoun case and determiner number for machine translation; part of speech tagging; named entity labeling; spelling correction; and some formulations of skeletal parsing. Very similar feature sets have been used across machine learning algorithms and across classification problems. For example, in confusable word set disambiguation, systems typically use as features the occurrence of a particular word within a window of +/- n words of the target, and collocations based on the words and part of speech tags of up to two words to the left and two words to the fight of the target. Below we present a machine learning algorithm that learns from a much richer feature set than that typically used for classification in  natural language. Our algorithm learns rule sequences for n-way classification, where the condition of a rule can be a restricted regular expression on the string context in which the ambiguity site appears. We demonstrate that using this more powerful feature space leads to an improvement in disambiguation performance on confusable words. 
We believe that identifying the structure of scientific argumentation in articles can help in tasks such as automatic summarization or the automated construction of citation indexes. One particularly important aspect of this structure is the question of who a given scientific statement is attributed to: other researchers, the field in general, or the authors themselves. We present the algorithm and a systematic evaluation of a system which can recognize the most salient textual properties that contribute to the global argumentative structure of a text. In this paper we concentrate on two particular features, namely the occurrences of prototypical agents and their actions in scientific text. 
This paper presents a method of Japanese dependency structure analysis based on Sup-port Vector Machines (SVMs). Conventional parsing techniques based on Machine Learning framework, such as Decision Trees and Maximum Entropy Models, have difficulty in selecting useful features as well as finding appropriate combination of selected features. On the other hand, it is well-known that SVMs achieve high generalization performance even with input data of very high dimensional feature space. Furthermore, by introducing the Kernel principle, SVMs can carry out the training in high-dimensional •spaces with a smaller computational cost independent of their dimensionality. We apply SVMs to Japanese dependency structure identification problem. Experimental results on Kyoto University corpus show that our system achieves the accuracy of 89.09% even with small training data (7958 sentences). 
Transformation-based learning has been successfully employed to solve many natural language processing problems. It has many positive features, but one drawback is that it does not provide estimates of class membership probabilities. In this paper, we present a novel method for obtaining class membership probabilities from a transformation-based rule list classifier. Three experiments are presented which measure the modeling accuracy and cross-entropy of the probabilistic classifier on unseen data and the degree to which the output probabilities from the classifier can be used to estimate confidences in its classification decisions. The results of these experiments show that, for the task of text chunking 1, the estimates produced by this technique are more informative than those generated by a state-of-the-art decision tree. 
We address the issue of 'topic analysis,' by which is determined a text's topic structure, which indicates what topics are included in a text, and how topics change within the text. We propose a novel approach to this issue, one based on statistical modeling and learning. We represent topics by means of word clusters, and employ a finite mixture model to represent a word distribution within a text. Our experimental results indicate that our method significantly outperforms a method that combines existing techniques. 
Corpus-based grz.mmar induction relies on using many hand-parsed sentences as training examples. However, the construction of a training corpus with detailed syntactic analysis for every sentence is a labor-intensive task. We propose to use sample selection methods to minimize the amount of annotation needed in the training data, thereby reducing the workload of the human annotators. This paper shows that the amount of annotated training data can be reduced by 36% without degrading the quality of the induced grammars. 
Grammars are core elements of many NLP applications. In this paper, we present a system that automatically extracts lexicalized grammars from annotated corpora. The data produced by this system have been used in several tasks, such as training NLP tools (such as Supertaggers) and estimating the coverage of harid-crafted grammars. We report experimental results on two of those tasks and compare our approaches with related work. 
This paper presents results for a maximumentropy-based part of speech tagger, which achieves superior performance principally by enriching the information sources used for tagging. In particular, we get improved results by incorporating these features: (i) more extensive treatment of capitalization for unknown words; (ii) features for the disambiguation of the tense forms of verbs; (iii) features for disambiguating particles from prepositions and adverbs. The best resulting accuracy for the tagger on the Penn Treebank is 96.86% overall, and 86.91% on previously unseen words. Introduction I There are now numerous systems for automatic assignment of parts of speech ("tagging"), employing many different machine learning methods. Among recent top performing methods are Hidden Markov Models (Brants 2000), maximum entropy approaches (Ratnaparkhi 1996), and transformation-based learning (Brill 1994). An overview of these and other approaches can be found in Manning and Schiitze (1999, ch. 10). However, all these methods use largely the same information sources for tagging, and often almost the same features as well, and as a consequence they also offer very similar levels of performance. This stands in contrast to the (manually-built) EngCG tagger, which achieves better performance by using lexical and contextual information sources and generalizations beyond those available to such statistical taggers, as Samuelsson and Voutilainen (1997) demonstrate. i We thank Dan Klein and Michael Saunders for useful discussions, and the anonymous reviewers for many helpful comments.  This paper explores the notion that automatically built tagger performance can be further improved by expanding the knowledge sources available to the tagger. We pay special attention to unknown words, because the markedly lower accuracy on unknown word tagging means that this is an area where significant performance gains seem possible. We adopt a maximum entropy approach because it allows the inclusion of diverse sources of information without causing fragmentation and without necessarily assuming independence between the predictors. A maximum entropy approach has been applied to partof-speech tagging before (Ratnaparkhi 1996), but the approach's ability to incorporate nonlocal and non-HMM-tagger-type evidence has not been fully explored. This paper describes the models that we developed and the experiments we performed to evaluate them.  
This paper proposes a new error-driven HMMbased text chunk tagger with context-dependent lexicon. Compared with standard HMM-based tagger, this tagger uses a new Hidden Markov Modelling approach which incorporates more contextual information into a lexical entry. Moreover, an error-driven learning approach is adopted to decrease the memory requirement by keeping only positive lexical entries and makes it possible to further incorporate more contextdependent lexical entries. Experiments show that this technique achieves overall precision and recall rates of 93.40% and 93.95% for all chunk types, 93.60% and 94.64% for noun phrases, and 94.64% and 94.75% for verb phrases when trained on PENN WSJ TreeBank section 00-19 and tested on section 20-24, while 25-fold validation experiments of PENN WSJ TreeBank show overall precision and recall rates of 96.40% and 96.47% for all chunk types, 96.49% and 96.99% for noun phrases, and 97.13% and 97.36% for verb phrases. Introduction Text chunking is to divide sentences into nonoverlapping segments on the basis of fairly superficial analysis. Abney(1991) proposed this as a useful and relatively tractable precursor to full parsing, since it provides a foundation for further levels of analysis, while still allowing more complex attachment decisions to be postponed to a later phase. Text chunking typically relies on fairly simple and efficient processing algorithms. Recently, many researchers have looked at text chunking in two different ways: Some  researchers have applied rule-based methods, combining lexical data with finite state or other rule constraints, while others have worked on inducing statistical models either directly from the words and/or from automatically assigned part-of-speech classes. On the statistics-based approaches, Skut and Brants(1998) proposed a HMM-based approach to recognise the syntactic structures of limited length. Buchholz, Veenstra and Daelemans(1999), and Veenstra(1999) explored memory-based learning method to fred labelled chunks. Ratnaparkhi(1998) used maximum entropy to recognise arbitrary chunk as part of a tagging task. On the rule-based approaches, Bourigaut(1992) used some heuristics and a grammar to extract "terminology noun phrases" from French text. Voutilainen(1993) used similar method to detect English noun phrases. Kupiec(1993) applied. finite state transducer in his noun phrases recogniser for both English and French. Ramshaw and Marcus(1995) used transformation-based learning, an error-driven learning technique introduced by Eric Bn11(1993), to locate chunks in the tagged corpus. Grefenstette(1996) applied finite state transducers to fred noun phrases and verb phrases. In this paper, we will focus on statisticsbased methods. The structure of this paper is as follows: In section 1, we will briefly describe the new error-driven HMM-based chunk tagger with context-dependent lexicon in principle. In section 2, a baseline system which only includes the current part-of-speech in the lexicon is given. In section 3, several extended systems with different context-dependent lexicons are described. In section 4, an error=driven learning method is used to decrease memory requirement of the lexicon by keeping only positive lexical  71  entries and make it possible to further improve the accuracy by merging different contextdependent lexicons into one after automatic analysis of the chunking errors. Finally, the conclusion is given. The data used for all our experiments is extracted from the PENN" WSJ Treebank (Marcus et al. 1993) by the program provided by Sabine Buchholz from Tilbug University. We use sections 00-19 as the training data and 20-24 as test data. Therefore, the performance is on large scale task instead of small scale task on CoNLL-2000 with the same evaluation program. For evaluation of our results, we use the precision and recall measures. Precision is the percentage of predicted chunks that are actually correct while the recall is the percentage of correct chunks that are actually found. For convenient comparisons of only one value, we also list the F~=I value(Rijsbergen 1979): (/32 + 1). precision, recall , with/3 = 1. /3 2. precision + recall 
This paper presents a novel nonlocal lmlguage model which utilizes contextual information. A reduced vector space model calculated from co-occurrences of word pairs provides word co-occurrence vectors. The sum of word cooccurrence vectors represents tile context of a document, and the cosine similarity between the context vector and the word co-occurrence vectors represents the ]ong-distmlce lexical dependencies. Experiments on the Mainichi Newspaper corpus show significant improvement in perplexity (5.070 overall and 27.2% on target vocabulary) 
The bigram language models are popular, in much language processing applications, in both Indo-European and Asian languages. However, when the language model for Chinese is applied in a novel domain, the accuracy is reduced significantly, from 96% to 78% in our evaluation. We apply pattern recognition techniques (i.e. Bayesian, decision tree and neural network classifiers) to discover language model errors. We have examined 2 general types of features: modelbased and language-specific features. In our evaluation, Bayesian classifiers produce the best recall performance of 80% but the precision is low (60%). Neural network produced good recall (75%) and precision (80%) but both Bayesian and Neural network have low skip ratio (65%). The decision tree classifier produced the best precision (81%) and skip ratio (76%) but its recall is the lowest (73%). Introduction Language models are important post-processing modules to improve recognition accuracy of a wide variety of input, namely speech recognition (Balh et al., 1983), handwritten recognition (Elliman and Lancaster, 1990) and printed character recognition (Sun, 1991), for many human languages. They can also be used for text correction (Ron et al., 1994) and part-of-speech tagging.  et al., 1993). For better probability estimation, the model was extended to work with (hidden) word classes (Brown et al., 1992, Ward and Issar, 1996). A more error-driven approach is the use of hybrid language models, in which some detection mechanism (e.g. perplexity measures [Keene and O'Kane, 1996] or topic detection [Mahajan et al., 1999]) selects or combines with a more appropriate language model. For Asian languages (e.g. Chinese, Japanese and Korean) represented by ideographic characters, language models are widely used in computer entry because these Asian languages have a large set of characters (in thousands) that the conventional keyboard is not designed for. Apart from using speech and handwriting recognition for computer entry, language models for Asian languages can be used for sentence-based keyboard input (e.g. Lochovsky and Chung, 1997), as well as detecting improper writing (e.g. dialectspecific words or expressions). Unlike Indo-European languages, words in these Asian languages are not delimited by space and conventional approximate string matching techniques (Wagner and Fisher, 1974; Oommen and Zhang, 1974) in handwriting recognition are seldom used in Asian language models. Instead, a widely used and reported Asian language model is the character-bigram language model (Jin et al., 1995; Xia et al., 1996) because it (1) achieved high recognition accuracy (around 90-96%) (2) is easy to estimate model parameters (3) can be processed quickly and (4) is relatively easy to implement.  For Indo-European languages, the word-bigram language model is used in speech recognition (Jelinek, 1989) and handwriting recognition (Nathan et al., 1995). Various ways to improve language models were reported. First, the model has been extended with longer dependencies (e.g. trigram) (Jelinek, 1991) and using non-contiguous dependencies, like trigger pairs (Rosenfeid, 1994) or long distance n-gram language models (Huang  Improvement of these language models for IndoEuropean languages can be applied for the Asian languages but words need to be identified. For Asian languages, the model was integrated with syntactic rules (Chien, Chen and Lee, 1993). Class based language model (Lee and Tung, 1995) was also examined but the classes are based on semantically related words. A-new approach (Yang et al., 1998) is reported using segments  87  expressed by prefix and suffix trees but the comparison is based on perplexity measures, which may not correlate well with recognition improvement (Iyer et al., 1997).  language model selection (Keene and O'Kane, 1996) can be applied to those area to find a more appropriate language model because usually topic-dependent words are those causing errors.  While attempts to improve the; (bigram) language models were (quite) successful, the high recognition accuracy (about 96%) is not adequate for professional data entry services, which typically require an error rate lower than 1 in 1,000. As part of the quality control exercises, these services estimate their error rate by sampling, and they identify and correct the errors manually to achieve the required quality. Faced with a large volume of text, the ability to automatically identify where the errors are is perhaps more important than automatically correcting errors, in post-editing because (1) manual correction is more reliable than automatic correction, (2) manual error sampling can be carried out and (3) more manual efforts are required in error identification than correction due to the large volume of text. For example, if the identification of errors is 97% and there are no errors in error correction, then the accuracy of the language model is improved from 96% to 99.9% after error correction. In typical applications, the accuracy of the bigram language model may not be as high as those reported in the literature because the data may be in a different genre than that of the training data. For evaluation, we tested a bigram language model with text from a novel domain and its accuracy dropped significantly from 96% to 78%, which is similar to English (Mahajan et al., 1999). Improvement in the robustness of the bigram language model across different genre is necessary and several approaches are available, based on detecting errors of the language model. One (adaptive) approach is to automatically identify the errors and manually correcting them. The information about the correction of errors is used to improve the bigram language model. For example, the bigram probabilities of the language model may be estimated and updated with the corrected data. In this way, future occurrences of these errors are reduced. Another (hybrid) approach uses another language model to correct the identified errors. This language model can be computationally more expensive than the bigram language model because it is applied only to the identified errors. Also, topic detection (Mahajan et al., 1999) and  Another (integrative) approach improves the language model accuracy using more sophisticated recognizers, instead of a complementary language model. The more sophisticated recognizer may give a set of different results that the bigram language model can re-apply on or this recognizer simply gives the recognized character. This integrates well with the coarse-fine recognition architecture proposed by Nagy (1988) back in the 1960s. Coarse recognition provides the candidates for the language model to select. Fine, expensive recognition is carried out only where the language models failed. Finally, it is possible to combine all the different approaches (i.e. adaptive, hybrid and integrative). Given the significance in detecting errors of language models, there is little work in this area. Perhaps, it was considered that these errors were random and therefore hard to detect. However, users can detect errors quickly. We suspect that some of these errors may be systematic due to the properties of the language model used or due to language specific properties. We adopt a pattern recognition app~'~zc, h to detecting errors of the bigram language rnoaei for the Chinese language. Each output is assigned to either the class of correct output or the class of errors. The assignment of a class to an output is based on a set of features. We explore a number of features to detect errors, which are classified into model-based features and language-specific features. The proposed approach can work with IndoEuropean languages at the word-bigram level. However, language-specific features have to be discovered for the particular language. In addition, this approach can be adopted for n-gram language models. In principal, the model-based features can be found or evaluated similar to the bigram language model. For example, if the trigram probability (instead of bigram probability) is low, then the likelihood of a language model error is high. This paper is organized as follows. Section 1 discusses various features and some preliminary evaluation of their suitability for error  88  identification. Section 2 describes 3 types of classifiers used. In section 3, our evaluation is reported. Finally, we conclude.  language properties. Figure 1 shows the likelihood of an error occurring against the percentage of the conditional probabilities that are zero.  1. Features We evaluate individual features for error detection because they are important to the success of detection. Articles from Yazhou Zhoukan (YZZK) magazine (4+ Mbytes)/PH corpus (Guo and Liu, 1992) (7+ Mbytes) are used for evaluation. We use the recall and precision measurements for evaluation. The recall is the number of errors identified by a particular feature divided by the total number of errors. The precision is the number of errors identified by a particular feature divided by the total number of times the feature indicate that there are errors. In the first subsection, we describe some model-based features. Next, we describe the language-based features. In the last subsection, we discuss the combined use of both types of features. 1.1 Model-based features The bigram language model selects the most likely path Pm~ out of a set S. The probability of a path s in S is simply the product of the conditional probabilities of one character c, after the other c~.l where s = Co.C+..c:s:, after making the Markov assumption. Formally, Pm~ = arg max {p(s)} s~S = arg~ax{p(Co)I~IP(cilC,_l)[coC,...c,,== s} The set s is generated by the set of candidate characters for each recognition output. The recognizer may supply the set of candidate characters. Alternatively, a coarse recogniser may simply identify the best-matched group or class of characters. Then, members of this class are the candidate characters. Formally, we use a function h(.), that maps the recognition position to a set of candidate characters, i.e. h(i) = {ciJ. We can also define the set of sentences in terms of h(.), i.e. S = {s I s = cocz...c,, ~ , c, ~ h(i)}. 1.1.1 Features based on zero probabilities (Fl,t) One feature to detect errors is to count the number o f conditional probabilities p(cilc~.l) that are zero, between 2 consecutive positions. Zero conditional probabilities may be due to insufficient training data or may be because they represent the  60%  50%  40% - -  30%  20%  10%  0% 50%  60%  70%  80%  90%  100%  110%  Figure 1: The language model output errors against percentages of zero conditional probabilities. 1.1.2 Features based on low probability (Fl,z) When there are insufficient data, the conditional probabilities that are small are not reliable. IfPm~ have selected some conditional probabilities that are low, then probably there are no other choices from the candidate sets. Hence, the insufficient data problem may occur in that particular Pm~. In Figure 2, we plot the likelihood of errors identified against the different logarithmic conditional probability values. When the recall increases, unfortunately, the precision drops.  120% [ 100% [ 80% l | 6°% [  : ~ precision  : ~ recall  .......  Accuracy  /  .~,~'-  ,,  ! - .... L  t .................... ! 20% .... -\ ................ I 0% I ~.  -  I  .........  -  ]  III [  .. ...  .... .  Figure 2: The precision, recall and accuracy (i.e. recall x precision) of detecting language model errors by examining the logarithm conditional probabilities on the maximum likelihood path.  1.2 Language-specific Features The language-specific features are based on applying the word segmentation algorithm (Kit et al., 1989) to the maximum likelihood path. The ROCLING (Chen and Huang, 1993) word list used for segmentation has 78,000+ entries.  89  1.2.1 Features based on word length (F2,O If the matched word in the maximum likelihood path is long, then we expect the likelihood of an error is low because long words are specific. Figure 3 shows the precision of detecting the matched word is correct and the recall of errors in multi-character words. In general, the longer the matched words, the more likely that they are correct and the likelihood of missing undetected long words is small.  120%  lOO%  00%  60%  40%  • precision ..  20%  o;  
This paper presents empirical results in cross-lingual information retrieval using English queries to access Chinese documents (TREC-5 and TREC-6) and Spanish documents (TREC-4). Since our interest is in languages where resources may be minimal, we use an integrated probabilistic model that requires only a bilingual dictionary as a resource. We explore how a combined probability model of term translation and retrieval can reduce the effect of translation ambiguity. In addition, we estimate an upper bound on performance, if translation ambiguity were a solved problem. We also measure performance as a function of bilingual dictionary size. 
This paper proposed a new query translation method based on the mutual information matrices of terms in the Chinese and English corpora. Instead of looking up a •bilingual phrase dictionary, the compositional phrase (the translation of phrase can be derived from the translation of its components) in the query can be indirectly translated via a general-purpose Chinese-English dictionary look-up procedure. A novel selection method for translations of query terms is also presented in detail. Our query translation method ultimately constructs an English query in which each query term has a weight. The evaluation results show that the retrieval performance achieved by our query translation method is about 73% of monolingual information retrieval and is about 28% higher than that of simple wordby-word translation way. Introduction With the rapid growth of electronic documents and the great development of network in China, there are more and more people touching the Intemet, on which, however, English is the most popular language being used. It is difficult for most people in China to use English fluently, so they would like to use Chinese to express their queries to retrieval the relevant English documents. This situation motivates research in Cross Language Information Retrieval (CLIR). There are two approaches to CLIR, one is query translation; the other is translating original language documents to destination This research was supported by the National Science Fund of China for Distinguished Young Scholars under contact 69983009.  language equivalents. Obviously, the latter is a very expensive task since there are so many documents in a collection and there is not yet a reliable machine translation system that can be used to process automatically. Most researchers are inclined to choose the query translation approach [Oard. (1996)]. Methods for query translation have focused on three areas: the employment of machine translation techniques, dictionary based translation [Hull & Grefenstette (1996); Ballesteros & Croft (1996)], parallel or comparable corpora for generating a translation model [Davis & Dunning (1995); Sheridan & Ballerini (1996); Nie, Jian-Yun et a1.(1999)]. Machine translation (MT) method has many obstacles to prevent its employment into CLIR such as deep syntactic and semantic analysis, user queries consisting of only one or two words, and an arduous task to build a MT system. Dictionary based query translation is the most popular method because of its easiness to perform. The main reasons leading to the great drops in CLIP,. effectiveness by this method are ambiguities caused by more than one translation of a query term and failures to translate phrases during query translation. Previous studies [Hull & Grefenstette (1996); Ballesteros & Croft (1996)] have shown that automatic word-byword (WBW) query translation via machine readable dictionary (MKD) results in a 40-60% loss in effectiveness below that of monolingual retrieval. With regard to the use of parallel corpora translation method, the critiques one often raises concern the availability of reliable parallel text corpora. An alternative way is that making use of the comparable corpora because they are easier to be obtained and there are more and more bilingual even multilingual documents on the Internet. From analyzing a document collection, an associated word list can be yielded and it is often used to expansion the query in monolingual information retrieval [Qiu & Frei(1993); Jing & Croft(1994)].  104  In this paper, a new query translation is presented by combination dictionary based method with the comparable corpora analyzing. Ambiguity problem and phrase information lost are attacked in dictionary based ChineseEnglish Cross-Language information Retrieval (CECLIR). The remainder of this paper is organized as follows: section 1 gives a method to calculate the mutual information matrices of Chinese-English comparable corpora. Section 2 develops a scheme to select the translations of the Chinese query terms and introduces how the compositional phrase can be kept in our method. Section 3 presents a set of preliminary experiment on comparable corpora to evaluate our query translation method and gives some explanations.  
In this paper, a method for the word alignment of English-Chinese corpus based on chunks is proposed. The chunks of English sentences are identified firstly. Then the chunk boundaries of Chinese sentences are predicted by the translations of English chunks and heuristic information. The ambiguities of Chinese chunk boundaries are resolved by the coterminous words in English chunks. With the chunk aligned bilingual corpus, a translation relation probability is proposed to align words. Finally, we evaluate our system by real corpus and present the experiment results. Key Words: Word Alignment, Chunk Alignment, Bilingual Corpus, Lexicon Extraction 
We propose an empirical method for estimating term weights directly from relevance judgements, avoiding various standard but potentially troublesome assumptions. It is common to assume, for example, that weights vary with term frequency (tf) and inverse document frequency (idf) in a particular way, e.g., tf. idf, but the fact that there are so many variants of this formula in the literature suggests that there remains considerable uncertainty about these assumptions. Our method is similar to the Berkeley regression method where labeled relevance judgements are fit as a linear combination of (transforms of) t f, idf, etc. Training methods not only improve performance, but also extend naturally to include additional factors such as burstiness and query expansion. The proposed histogram-based training method provides a simple way to model complicated interactions among factors such as t f, idf, burstiness and expansion frequency (a generalization of query expansion). The correct handling of expanded term is realized based on statistical information. Expansion frequency dramatically improves performance from a level comparable to BKJJBIDS, Berkeley's entry in the Japanese NACSIS NTCIR-1 evaluation for short queries, to the level of JCB1, the top system in the evaluation. JCB1 uses sophisticated (and proprietary) natural language processing techniques developed by Just System, a leader in the Japanese word-processing industry. We are encouraged that the proposed method, which is simple to understand and replicate, can reach this level of performance. 
In this paper, we report results on answering questions for the reading comprehension task, using a machine learning approach. We evaluated our approach on the Remedia data set, a common data set used in several recent papers on the reading comprehension task. Our learning approach achieves accuracy competitive to previous approaches that rely on handcrafted, deterministic rules and algorithms. To the best of our knowledge, this is the first work that reports that the use of a machine learning approach achieves competitive results on answering questions for reading comprehension tests. 
The development of natural language interfaces (NLI's) for databases has been a challenging problem in natural language processing (NLP) since the 1970's. The need for NLI's has become more pronounced due to the widespread access to complex databases now available through the Internet. A challenging problem for empirical NLP is the automated acquisition of NLI's from training examples. We present a method for integrating statistical and relational learning techniques for this task which exploits the strength of both approaches. Experimental results from three different domains suggest that such an approach is more robust than a previous purely logicbased approach. 
This paper presents the automatic construction of a Korean WordNet from pre-existing lexical resources. A set of automatic WSD techniques is described for linking Korean words collected from a bilingual MRD to English WordNet synsets. We will show how individual linking provided by each WSD method is then combined to produce a Korean WordNet for nouns. 
This paper introduces an intuitive search environment for casual and novice Chinese users over Internet. The system consists of four components, a concept network, a query reformulation model, a standard search engine, and an automatic summarizer. When the user enters one or more fairly general and vague terms, the search engine returns an initial answer set, and at the same time pipes the query to the concept network that connects thousands of conceptual nodes, each referring to a specific concept for a given domain and pointing to a number of associated conceptual terms. If the concept is located in the network, the related conceptual terms are displayed. The user has the option of using one or more of these specific terms to reformulate the next round of searches. Such search iterations continue until the user' s ultimate information seeking goal is reached. For each search iteration, auto summarizer presents the main theme of the document retrieved and an optional text-to-speech engine can read out the output summary if the user prefers. 1. Introduction Internet is changing the world, and at the same time changing people' s information seeking behaviour. Traditionally, information searchers are trained professionals working in libraries or other special technical or scientific fields. They have developed a variety of techniques and heuristics for addressing information seeking difficulties in the environment typically dominated by Boolean query formula. These Boolean information retrieval systems are normally commercial and non-interactive systems and the searches conducted in such settings are exact-match and set-based retrieval from databases of indexed citations and  
This paper describes a first attempt at a statistical model for simultaneous syntactic parsing and generalized word-sense disambignation. On a new data set we have constructed for the task, while we were disappointed not to find parsing improvement over a traditional parsing model, our model achieves a recall of 84.0% and a precision of 67.3% of exact synset matches on our test corpus, where the gold standard has a reported inter-annotator agreement of 78.6%. 
Long sentence analysis has been a critical problem because of high complexity. This paper addresses the reduction of parsing complexity by intra-sentence segmentation, and presents m a x i m u m entropy model for determining segmentation positions. The model features lexicalcontexts of segmentation positions, giving a probability to each potential position. Segmentation coverage and accuracy of the proposed method are 96% and 88% respectively.The parsing efficiencyisimproved by 77% in time and 71% in space. 
This paper describes a set of experiments carried out to explore the domain dependence of alternative supervised Word Sense Disambiguation algorithms. The aim of the work is threefold: studying the performance of these algorithms when tested on a different corpus from that they were trained on; exploring their ability to tune to new domains, and demonstrating empirically that the LazyBoosting algorithm outperforms state-of-theart supervised WSD algorithms in both previous situations. K e y w o r d s : Cross-corpus evaluation of Ni_P systems, Word Sense Disambiguation, Supervised Machine Learning 
Because of their constant renewal, it is necessary to acquire fresh named entities (NEs) from recent text sources. We present a tool for the acquisition and the typing of NEs from the Web that associates a harvester and three parallel shallow parsers dedicated to specific structures (lists, enumerations, and anchors). The parsers combine lexical indices such as discourse markers with formatting instructions (HTML tags) for analyzing enumerations and associated initializers. 
Abstract; This paper presents a query tool for syntacticaUy ~.nnotated corpora. The query tool is developed to search the Verbmobil treebanks annotated at the University of Tfibingen. However, in principle it also can be adapted to other corpora such as the Negra Corpus, the Penn Treebank or the French treebank developed in Paris. The tool uses a query language that allows to search for tokens, syntactic categories, grammatical functions and binary relations of (immediate) dominance and linear precedence between nodes. The overall idea is to extract in an initializing phase the relevant information from the corpus and store it in a relational database. An incoming query is then translated into a corresponding SQL query that is evaluated on the database. 
Research "into the automatic acquisition of subcategorization frames (SCFS) from corpora is starting to produce large-scale computational lexicons which include valuable frequency information. However, the accuracy of the resulting lexicons shows room for improvement. One significant source of error lies in the statistical filtering used by some researchers to remove noise from automatically acquired subcategorization frames. In this paper, we compare three different approaches to filtering out spurious hypotheses. Two hypothesis tests perform poorly, compared to filtering frames on the basis of relative frequency. We discuss reasons for this and consider directions for future research. 
This paper revisits the one sense per collocation hypothesis using fine-grained sense distinctions and two different corpora. We show that the hypothesis is weaker for fine-grained sense distinctions (70% vs. 99% reported earlier on 2-way ambiguities). We also show that one sense per collocation does hold across corpora, but that collocations vary from one corpus to the other, following genre and topic variations. This explains the low results when performing word sense disambiguation across corpora. In fact, we demonstrate that when two independent corpora share a related genre/topic, the word sense disambiguation results would be better. Future work on word sense disambiguation will have to take into account genre and topic as important parameters on their models. Introduction In the early nineties two famous papers claimed that the behavior of word senses in texts adhered to two principles: one sense per discourse (Gale et al., 1992) and one sense per collocation (Yarowsky, 1993). These hypotheses were shown to hold for some particular corpora (totaling 380 Mwords) on words with 2-way ambiguity. The word sense distinctions came from different sources (translations into French, homophones, homographs, pseudo-words, etc.), but no dictionary or lexical resource was linked to them. In the case of the one sense per collocation paper, several corpora were used, but nothing is said on whether the collocations hold across corpora. Since the papers were published, word sense disambiguation has moved to deal with fine-  grained sense distinctions from widely recognized semantic lexical resources; ontologies like Sensus, Cyc, EDR, WordNet, EuroWordNet, etc. or machine-readable dictionaries like OALDC, Webster's, LDOCE, etc. This is due, in part, to the availability of public hand-tagged material, e.g. SemCor (Miller et al., 1993) and the DSO collection (Ng & Lee, 1996). We think that the old hypotheses should be tested under the conditions of this newly available data. This paper focuses on the DSO collection, which was tagged with WordNet senses (Miller et al. 1990) and comprises sentences extracted from two different corpora: the balanced Brown Corpus and the Wall Street Journal corpus. Krovetz (1998) has shown that the one sense per discourse hypothesis does not hold for finegrained senses in SemCor and DSO. His results have been confirmed in our own experiments. We will therefore concentrate on the one sense per collocation hypothesis, considering these two questions: • Does the collocation hypothesis hold across corpora, that is, across genre and topic variations (compared to a single corpus, probably with little genre and topic variations)? • Does the collocation hypothesis hold for freegrained sense distinctions (compared to homograph level granularity)? The experimental tools to test the hypothesis will be decision lists based on various kinds of collocational information. We will compare the performance across several corpora (the Brown Corpus and Wall Street Journal parts of the DSO collection), and also across different sections of the Brown Corpus, selected according to the genre and topics covered. We will also perform a direct comparison, using agreement statistics, of the collocations used and of the results obtained.  207  This study has special significance at this point of word sense disambiguation research. A recent study (Agirre & Martinez, 2000) concludes that, for currently available handtagged data, the precision is limited to around 70% when tagging all words in a running text. In the course of extending available data, the efforts to use corpora tagged by independent teams of researchers have been shown to fail (Ng et al., 1999), as have failed some tuning experiments (Escudero et al., 2000), and an attempt to use examples automatically acquired from the Internet (Agirre & Martinez, 2000). All these studies obviated the fact that the examples come from different genre and topics. Future work that takes into account the conclusions drawn in this paper will perhaps be able to automatically extend the number of examples available and tackle the acquisition problem. The paper is organized as follows. The resources used and the experimental settings are presented first. Section 3 presents the collocations considered and Section 4 explains how decision lists have been adapted to n-way ambiguities. Sections 5 and 6 show the incorpus and cross-corpora experiments, respectively. Section 7 discusses the effect of drawing training and testing data from the same documents. Section 8 evaluates the impact of genre and topic variations, which is fiarther discussed in Section 9. Finally, Section 10 presents some conclusions. 
Research into the automatic acquisition of subcategorization frames from corpora is starting to produce large-scale computational lexicons which include valuable frequency information. However, the accuracy of the resulting lexicons shows room for improvement. One source of error lies in the lack of accurate back-off estimates for subcategorization frames, delimiting the performance of statistical techniques frequently employed in verbal acquisition. In this paper, we propose a method of obtaining more accurate, semantically motivated back-off estimates, demonstrate how these estimates can be used to improve the learning of subcategorization frames, and discuss using the method to benefit large-scale lexical acquisition. 
This paper presents the first-ever results of applying statistical parsing models to the newly-available Chinese Treebank. We have employed two models, one extracted and adapted from BBN's SIFT System (Miller et al., 1998) and a TAGbased parsing model, adapted from (Chiang, 2000). On sentences with <40 words, the former model performs at 69% precision, 75% recall, and the latter at 77% precision and 78% recall. 
Information Engineering National Taiwan University Taipei, TAIWAN cclin@nlg2.csie.ntu.edu.tw  Abstract Contextual information and the mapping from WordNet synsets to Cilin sense tags deal with word sense disambiguation. The average performance is 63.36% when small categories are used, and 1, 2 and 3 candidates are proposed for low, middle and high ambiguous words. The performance of tagging unknown words is 34.35%, which is much better than that of baseline mode. The sense tagger achieves the performance of 76.04%, when unambiguous, ambiguous, and unknown words are tagged. 
In this paper, a knowledge extraction process was proposed to extract the knowledge for identifying Chinese organization names. The knowledge extraction process utilizes the structure property, statistical property as well as partial linguistic knowledge of the organization names to extract new organizations from domain texts. The knowledge extraction processes were experimented on large amount of texts retrieved from WWW. With high standard of threshold values, new organization names can be identified with very high precision. Therefore the knowledge extraction processes can be carried out automatically to self improve the performance in the future. 1. INTRODUCTION The occurrences of unknown words cause difficultiesin natural language processing. The word set of a natural language is open-ended. There is no w a y of collecting every words of a language, since n e w words will be created for expressing new concepts, n e w inventions, newborn babies, new organizations. Therefore how to identify new words in a text will be the most challenging task for natural language processing. It is especially true for Chinese. Each Chinese morpheme (usually a single character) carries meanings and most arc polyscmous. N e w words are easily constructed by combining morphemes and their meanings are the semantic composition of morpheme components. However there are also semantically non-compositional compounds, such as proper names. In Chinese text, there is no blank to mark word boundaries and no inflectional markers nor capitalization markers to denote the syntactic or semantic types of new words. Hence the unknown word identification for Chinese became one of the most difficultand demanding research topic. There are many different types of unknown words and each has different morphsyntactic and morph-scmantic structures. In principle their syntactic and semantic categories  can be determined by their content and contextual information, but there arc many difficult problems have to be solved. First of all it is not possible to find a uniform representational schema and categorization algorithm to handle different types of unknown words due to their different morphsyntactic structures. Second, the clues for identifying different type of unknown words are also different. For instance, identification of names of Chinese people is very much relied on the surnames, which is a limited set of characters. The statistical methods are commonly used for identifyingproper names (Chen & Lee 1996, Chang ct al. 1994, Sun et al. 1994). The identification of general compounds is more relied on the morphemes and the semantic relations between morphemes (Chcn & Chcn 2000). The third difficulty is the problems of ambiguities, such as structure ambiguities, syntactic ambiguities and semantic ambiguities. For instances, usually a morpheme character/word has multiple meaning and syntactic categories and m a y play the roles of common words or proper names. Therefore the ambiguity resolution became one of the major tasks. In this paper w e focus our attention on the identificationof the organization names. It is considered to be a hard task to identify organization names in comparing with the 
Our partial parser for Chinese uses a learned classifier to guide a bottom-up parsing process. We describe improvements in performance obtained by expanding the information available to the classifier, from POS sequences only, to include measures of word association derived from co-occurrence statistics. We compare performance using different measures of association, and find that Yule's coefficient of colligation Y gives somewhat better results over other measures. Introduction In learning-based approaches to syntactic parsing, the earliest models developed generally ignored the individual identities of words, making decisions based only on their part-of-speech classes. On the othor hand, many later models see each word as a monolithic entity, with parameters estimated separately for each word type. In between have been models which auempt to generalize by considering similarity between words, where knowledge about similarity is deduced fi'om hand-written sources (e.g. thesauri), or induced from text. For example, The SPATTER parser (Magerman, 1995) makes use of the output o f a clustering algorithm based on co-occurrence information. Because this co-occurrence information can be derived from inexpensive data with a minimum of pre-processing, it can be very inclusive and informative about even relatively rare words, thus increasing the generalization capability of the parser trained on a much smaller fully annotated corpus.  The cunent work is in this spirit, making  complementary use of a relatively small  treebank for syntactic information and a  relatively large collection of flat text for  co-occurrence information. However, we do  not use any kind of clustering, instead using the  co-occurrence data directly. Our parser is a  bottom-up parser whose actions are guided by a  machine-learning-based  decision-making  module (we use the SNoW learner developed at  the University of Illinois, Urbana..Champaign  (Roth, 1998) for its strength with potentially  very large feature sets and for its ease of use).  The learner is able to directly use statistics  derived from the co-occu~euce data to guide its  decisions.  We collect a variety of statistical measures of  association based on bigram co-occurrence data  (specifically, mutual information, t-score, X2,  likelihood ratio and Yule's coefficient of  colligation Y), and make the statistics available  to the decision-making module. We use  labelled constituent precision and recall to  compare performance of different versions of  our parser on unseen test data. We observe a  marked improvement in some of the versions  using the co-occurrence data, with strongest  performance observed in the versions using  Yule's coefficient of colligation Y and mutual  information, and more modest improvements in  those using the other measures.  
This paper describes the design criteria and annotation guidelines of Sinica Treebank. The three design criteria are: Maximal Resource Sharing, Minimal Structural Complexity, and Optimal Semantic Information. One of the important design decisions following these criteria is the encoding of thematic role information. An on-line interface facilitating empiricalstudies of Chinese phrase structure is also described. 1. Introduction The Penn Treebank (Marcus et al. 1993) initiated a new paradigm in corpus-based research. The English. Penn Treebank has enabled and motivated corpus and computational linguistic research based on information extractable from structurally annotated corpora. Recently, the research has focused on the following two issues: first, when and how can a structurally annotated corpus of language X be built?  Second, what information should or can be annotated? A good sample of issues in these two directions can be found in the papers collected in Abeille (1999). The construction of the Sinica Treebank deals with both issues. First, it is one of the first structurally annotated corpora in Mandarin Chinese. Second, as a design feature, the Sinica Treebank annotation includes thematic role information in addition to syntactic categories. In this paper, we will discuss the design criteria and annotation guidelines of the Sinica Treebank. We will also give a preliminary research result based on the Sinica Treebank. 2. Design Criteria There are three important design criteria for the Sinica Treebank: maximal resource sharing, minimal structural complexity, and optimal semantic information. First, to achieve maximal resource sharing, the construction of the Sinica Treebank is bootstrapped from existing  29  Chinese computational linguistic resources. The textual material is extracted from the tagged Sinica Corpus (hRp:l/www.sinica.edu.tw/ftms-bin/ kiwi.sh, Chen et al. 1996). In other words, the tasks and issues involving tokenization / word segmentation and category assignment are previously resolved. It is worth noting that the segmentation and tagging of Sinica Corpus have undergone vigorous post-editing. Hence the precision of category-assignment is much higher than with an automatically tagged corpora. In addition, since the same research team carried out the tagging of Sinica Corpus and annotation of Sinica Treebank, consistency of the interpretation of texts and tags are ensured. For structure-assigument, an automatic parser (Chen 1996) is applied before human post-editing.  Second) the criterion of minimal  structural complexity is motivated to  ensure that the assigned structural  information can be shared regardless of  users' theoretical presupposition. It is  observed  that  theory-internal  motivations often require abstract  intermediate phrasal levels (such as in  various versions of the X-bar theory).  Other theories may also call for an  abstract covert phrasal category (such as  INFL in the GB theory for Chinese). In  either case, although the phrasal  categories are well-motivated within the  theory, their significance cannot be  maintained in the context of other  theoretical frameworks. Since a primary goal of annotated corpora is to serve as the empirical base of linguistic investigations, it is desirable to annotate structure divisions that are the most commonly shared among theories. We came to the conclusion that the minimal basic level structures are the ones that are shared by all theories. Thus our annotation is designed to achieve minimal structural complexity. All abstract phrasal levels are eliminated and only canonical phrasal categories are marked. Third) a critical issue involving Treebank construction as well as theories of NLP is how much semantic information, if any, should be incorporated. The original Penn Treebank took a fairly straightforward syntactic approach. A purely semantic approach, though tempting in terms of theoretical and practical considerations, has never been attempted yet. A third approach is to annotate partial semantic information, especially those pertaining to argument-relations. This is an approach shared by us and the Prague Dependency Treebank (e.g. Bohmova and Hajikova 1999). In this approach, the thematic relation between a predicate and an argument is marked in addition to grammatical category. Note that the predicate-argument relation is usually grammatically instantiated and generally considered to be the semantic relation that interacts most closely with syntactic behavior. This allows optimal semantic  30  information to be encoded without going too beyond the partially automatic process of argument identification. 3. Annotation Guidelines I: Category and Hierarchy The basic structure of a tree in a treebank is a hierarchy of nodes with categorical denotation. As in any standard phrase structure grammar, the lexieal (i.e. terrninal) symbols are defined.by the lexicon (CKIP 1992). And following the recent lexicon-driven and information=based trends in linguistic theory, linguistic information will be projected from encoded lexical information. Please refer to CKIP (1993) for the definition of lexieal categories that we followed. We will give below the inventory of the restricted set of phrasal categories used and their interpretation. This set defines the domain of expressed syntactic information (instead of projected or inherited information). Readers can also consult Chen et al.'s (2000) general description of how the Siniea Treebank is constructed for a more complete list of tags as well as explanation in Chinese. 3.1. Defining Phrasal Categories There are only 6 non-terminal phrasal categories annotated in the Sinica Treebank. (1) Phrasal Categories 1. S: An S is a complete tree headed by a predicate (i.e. S is the start symbol). 2.VP: A VP is a phrase headed by a  predicate. However, it lacks a subject and cannot function alone.  3. NP: An NP is beaded by an N.  4.GP: A GP is a phrase headed by  locational noun or locational adjunct. Since the thematic role is often 
Discourse markers are complex discontinuous linguistic expressions which are used to explicitly signal the discourse structure of a text. This paper describes efforts to improve an automatic tagging system which identifies and classifies discourse markers in Chinese texts by applying machine learning (ML) to the disambiguation of discourse markers, as an integral part of automatic text summarization via rhetorical structure. Encouraging results are reported. Keywords: discourse marker, Chinese corpus, rhetorical relation, automatic tagging, machine learning 
This paper presents a mechanism of new word identification in Chinese text where probabilities are used to filter candidate character strings and to assign POS to the selected strings in a ruled-based system. This mechanism avoids the sparse data problem of pure statistical approaches and the over-generation problem of rule-based approaches. It improves parser coverage and provides a tool for the lexical acquisition of new words. 
In this paper, we present a method for comparing Lexicalized Tree Adjoining Grammars extracted from annotated corpora for three languages: English, Chinese and Korean. This method makes it possible to do a quantitative comparison between the syntactic structures of each language, thereby providing a way of testing the Universal Grammar Hypothesis, the foundation of modern linguistic theories. 
Word sense disambiguafion (WSD) is a difficult problem in natural language processing. In this paper, a sememe co-occurrence frequency based WSD method was introduced. In this method, Hownet was used as our information source , and a co-occurrence frequency database of sememes was constructed and then used for WSD. The experimental result showed that this method is successful. 
Although substantial efforts have been made to parse Chinese, very few have been practically used due to incapability of handling unrestricted texts. This paper realizes a practical system for Chinese parsing by using a hybrid model of phrase structure partial parsing and dependency parsing. This system showed good performance and high robustness in parsing unrestricted texts and has been applied in a successful machine translation product. Introduction Substantial efforts have been made to parse western languages such as English, and many powerful computational models have been proposed (Gazdar, et al, 1987, Tomita, M, 1986). However, very limited work has been done with Chinese. This is mainly due to the fact that the structure of the Chinese language is quite different from English. Therefore the computational model in processing English may not be directly applied to the Chinese language. Lin-Shan Lee et al (1991) proposed a Chinese natural language processing system with special consideration of some typical phenomena of Chinese. Jinye Zhou et al (1986) presented a deterministic Chinese parsing methodology using formal semantics to combine syntactic and semantic analysis. However, most of the proposed approaches were realized on small-scale lexicon and rule base (usually thousands words and tens or hundreds rules). It  is still an open issue whether these models will work on real texts containing various ungrammatical phenomena. A parser capable of handling real text should have not only large lexicon and big rule base, but also high robustness in coping with different kinds of ungrammatical phenomena. Therefore, it is important to design a grammar scheme which not only is capable of representing the unique grammar structures which are different with English, but also qualified of handling unrestricted text. Phrase structure scheme is usually used in English parsing models to represent sentence structures, but it is not convenient and not strong enough to express Chinese sentence by phrase structure in some occasions. For examples: Sentence-1 ~ t ~ f f ] i ~ ~ . Fig. 1 phrase structure SOC Fig.2 dependency structure  I Thiswork was mainly done while the author visited Kodensha Ltd, Japan during 1996.-1999  78  Sentence-1 is a pivot sentence(~gd'f~3), i.e., "~1~" is not only the object of "i,W" butalso the subject of " Ik~" . But this phrase structure cannot indicate the relations clearly as shown in Fig.1. However, the grammar structure is clarified if it is represented in dependency structure (Fig. 2). Therefore, it is believed that dependency grammar sebeme is more suitable than phrase structure to represent Chinese structures (Zhou, Huang, 1994). However, traditional Dependency gammar realizes the dependency relations between any of two specific words, then numerous word based dependency knowledge should be constructed, this is a time-consuming task. Fortunately, knowledge for phrase structure parsing has been accumulated for Chinese for many years and it should be re-used to compensate the lack of knowledge of word-based dependency parsing. Therefore, to combine the advantages of phrase structure parsing and dependency parsing, we propose a new parsing strategy, called "block-based dependency parsing". A "block" means a basic component of sentence, for example, there are six blocks for sentence 1: ]  Another example: Sentence 2: ~:~_12q::~IJ~Af]i~ ~  ~ ~n~  Blocks:[If~.J2q:] [~t~'ff]] [ ~ ] [ ~ l ~ : ~ : ~ t ~ : ] ]  A block represents an information unit in communications. For example, in Chinese-Japanese machine translation, translations of the members within a block in a Chinese sentence usually are in a same blocks in the Japanese translation. Furthermore, it is clear to represent block with phrase structure, while it is rather complicated with dependency structure. This block-based dependency parsing process works like follows. For an input sentence, basic components of sentence, i.e., "blocks" are first identified by an ATN-like partial parsing procedure, which produces a clear skeleton of the sentence structure. In our phrase structure analysis, we don't try to deduce the whole sentence into root S, instead, we only try to get  the components, namely blocks. This partial parsing strategy guarantees high robustness. Then dependency parsing is applied in order to build dependency relations among blocks. The dependency parsing skips ungrammatical portions it encounters. This strategy confines ungrammatical portion and avoids errors to be • propagated globally. By partial parsing and skip strategy, this parser can handle long, complicated, or even faulty sentences. The experiments show that this parser is very robust and powerful. A parser constructed based on this approach has been developed, with 220,000 words, 5,000 part-of-speech tagging rules, over 1,000 block parsing rules and 300 dependency parsing rules. This parser has been applied in a Chinese-Japanese machine translation product (Zhou, 1999). To the author's knowledge, this parser is one of the largest scale Chinese parser ever implemented in the world. The outline of this paper is as follows. In section 1, we present our special solution to part-of-speech tagging which significantly affects the Chinese parsing. Section 2 describes in details the block-based dependency parsing approach. We then explain the dependency parsing algorithm in section 3. The experiment and its analysis are given in section 4. The conclusion is given in section 5. 
This paper reported our work on annotating Chinese texts with information structures derived from HowNet. An information structure consists of two components: HowNet definitions and dependency relations. It is the unit of representation of the meaning of texts. This work is part of a multi-sentential approach to Chinese text understanding. An overview of HowNet and information structure are described in this paper. 
This paper reports our evaluation of k Nearest Neighbor (kNN), Support Vector Machines (SVM), and Adaptive Resonance Associative Map (ARAM) on Chinese web page classification. Benchmark experiments based on a Chinese web corpus showed that their predictive performance were roughly comparable although ARAM and kNN slightly outperformed SVM in small categories. In addition, inserting rules into ARAM helped to improve performance, especially for small welldefined categories. 
In this paper, we propose a recursive graph based scheme for semantic annotation of Chinese phrases. Compared with others, this scheme can fully differentiate those Chinese phrases that comprise the same content words but hold different meanings due to their different word order or some involved function words, and capture the hierarchical conceptual structure of Chinese phrases, which underlies their main semantic information. We also give the guidelines for annotating various commonly used types of Chinese phrases. 
This paper describes text meaning representation for Chinese. Text meaning representation is composed of a set of ontological concept instances along with ontological links among them. It integrates lexical, textual and world knowledge into a single hierarchical framework. In NLP application it serves as an interlingual representation for various processing. The methodology and implementation of text meaning representation is discussed in detail. 
This study measures comparative lexical and syntactic closure rates in annotated Chinese newspaper corpora from the Academica Sinica Balanced Corpus and the University of Pennsylvania's Chinese Treebank. It then draws inferences as to how large such corpora need be to be representative models of subject-matterconstrained language domains within the same genre. Future large corpora should be built incrementally only by combining smaller representative sublanguage collections. 
This paper proposes a bidirctional hierarchical clustering algorithm for simultaneously clustering words of different parts of speech based on collocations. The algorithm is composed of cycles of two kinds of alternate clustering processes. We construct an objective function based on Minimum Description Length. To. partly solve the problem caused by sparse data two concepts of collocational degree and revisional distance are presented. 
This paper is to introduce a statistical method to extract Chinese compound words from a very large corpusL This method is based on mutual information and context dependency. Experimental results show that this method is efficient and robust compared with other approaches. We also examined the impact of different parameter settings, corpus size and heterogeneousness on the extraction results. We finally present results on information retrieval to show the usefulness of extracted compounds. 
Temporal information analysis is very important for Chinese Information Process. Comparing with English, Chinese is quite different in temporal information expression. Based on the feature of Chinese a phase-based method is proposed to deal with Chinese temporal information. To this end, an algorithm is put forward to classify verbs into different situation types automatically. About 2981 verbs were tested. The result has shown that the algorithm is effective. 1.*Introduction We are now launching a research project on Events Extraction from Chinese Financial News, which requires us to extract the related temporal information from news. Temporal expressions in Chinese form a complex system. We cannot fully understand the temporal information only by extracting the verbs, adverbs, auxiliary words and temporal phrases. Instead, more profound analysis is needed. In this paper, we first introduce the temporal system of Chinese, then we put forward a method in dealing with Chinese temporal information, in which situation types is very important. Therefore, an algorithm is rendered to classify verbs into several situation types.  1.1 Temporal System of Chinese Commonly, Chinese linguists [3][4] think that the temporal system of Chinese includes three parts: phase, tense and aspect. Each of these represents some profile of temporal expression (these definitions are a little different from linguistic theory of English). (1) Phase. A sentence may describe a static state or an action; an action may be durative or instantaneous; a durative action may indicate a terminal or not. All of these are the research fields of phase. So, static vs. dynamic, durative vs. instantaneous, telic vs. non-telic are three pairs of phase features. Phase depends fully on meaning. According to phase features, we can classify the verbs into different situation types. (2) Tense. Tense describes the relations between an event (E), reference time (R) and speaking time (S). First, taking S as the origin, we can get three reIations between R and S: if R is before S, the sentence describes past; if R is the same time as S, it describes present; if R is after S, it desci'ibes future. This is called primary tense. Secondly, we can get three relations between E and S: If E is before R, we call it anterior; if E is the same time as R, we call it simple; otherwise we call it posterior. This is called secondary tense. Therefore, there are nine tenses including anterior past, anterior future, simple future, posterior present, etc.  * Supported by National Natural Science Foundation of China (69975008) and 973 project (G 1998030507)  140  (3) Aspect . Aspect reflects the way we observe an event. For the same event, there are many perspectives. We can take the event as atomic and not consider its inner structure, and call it perfective. We can consider it being in process, and call it imperfective. For imperfective, we can observe it at a"position before it, at the beginning of it, in the middle of it, etc. Different perspectives lead to different expressions in the language. Phase, tense and aspect are not independent even though they are different conceptions; each of them can influence and restrict the others, ultimately building up the complex temporal system of Chinese. 1.2 Phase-based Chinese temporal information analysis Most languages express temporal information through phase, tense and aspect, however, for different languages, the relative importance of the three parts is different. A very important feature of English is that tense and aspect are expressed by variation of predicates. But for Chinese, predicates keep the same form no matter how the tense and aspect are different. Therefore, in English, temporal information analysis mainly considers tense and aspect, as well as temporal adjective and time words and phrases. But in Chinese, tense and aspect of a sentence are not very clear, verbs do not vary in form with the change of tense and aspect. So we suggest basing temporal information analysis on phase. We mainly perceive the situation type of a sentence, then roughly acquire tense from adverbs and auxiliary words. After considering the temporal phrases, we can understand the temporal information of single event fully. Finally, according to the absolute temporal information of single event, we can get the temporal relation between two events. Phase-based temporal information analysis has been used in our research on Event Extraction from Financial  News, in which the most important and fundamental problem is to acquire the situation types of a sentence.  1.3 Situation Classification of Chinese  Verbs  In the West, research on situation has a  long history. The earliest can be traced to  the times of Aristotle. In resent years,  Western researchers have published a large  volume of papers, which present many  points of view. The most important are  Vendler(1967), Bache(1982), and Smith  (1985) They approximately classify the  situation  as  four  types:  state, activity, accomplishment, and  achievement.  Chinese researchers have also done  considerable work, among which the most  typical research were done by Chen[3] and  Ma[5].  Ma[5] stated that the situation of a  sentence is fully determined by the situation  of the main verb of the sentence. He use  three phases: static, durative, telic to classify  verbs into four situational types  V1,V2,V3,V4.  Static  Durative  Telic  VI  +  +  +  V2  +  +  V3  +  V4  +  Table 1.1  Chen[3] stated that the situation of a sentence not only depends on the main verb of the sentence but also on other parts of the sentence. That is, although the main verb is the most important in determining a sentence situation, other parts such as adverbs also have effect. Cheri s classification is more detailed.  141  NO.  Verb types  Instances  (1)  Attribute  :E(be), ~(equal)  (2) Mental state  ;~:l~'~(believe), ~l~J~(re~'et)  (3)  Position  ~.~i(stand), ~(sit), J]~j  (lie)  (4)  Action and  gf~jump), ,~.(think),  Mental Activity  ~i=~q~uess)  (5)  Verb-object Structure i~t~(read (books)),  I1~(sing (songs))  (6)  Change  (become)  (7)  Directional Action  /EgE(run up), ~,_J2(climb on)  (8)  Instantaneous Change ~.(die), ~lie), IS(snap) ..  (9)  Instantaneous Action ~t~(sit), ~td/(stand)  00) Verb-verb or  ~J(push down),  Verb-adjective  ~,TJ~..(smas(hinto pieces))  Table 1.2  Situation types State Activity Accomplishment Simple change Complex change  Static Dura- Telic tive  +  +  +  +  +  Table 1.3  verb types (table above) (1) (2) (3) (3) (4) (5) (3) (4) (6) (7) (S) (9) (10)  From the tables above, we can find that some words(such as (3) and (4) in table 1.3) can belong to more than one category, so Chen use modifiers, auxiliary words and prepositions to eliminate the ambiguity.  State  Acti- Accom- Complex Simple  vity plishment change  change  Ell l~l I[  ~1~+V v~  ÷  (-1  ÷  ÷  +  ~v  (-)  (-) +  ÷  ÷  v+(y)  ÷  ÷  ÷  +TQP+  ~act)  V+(T)  + +  ÷  ÷  ÷  +TQP+ ~m,e)  TQP: Time Quantity Phrase, (-) : in most case, it is  Table 1.4  2. Our Classification Algorithm for Verbg Situation  2.1 Guiding Thoughts (1) Our algorithm is for information processing eMa[5] uses three pairs of phase features in classifying, but from which we can not get an automatic classification algorithm for computers; the classification can only be done manually. eln linguistics, telicity is a phase feature used in classifying. In table 1.1 the difference between category V2 and V3, in table 1.3, the difference between "activivJ' and "accomplishmenf', are attributed to telicity. But in information process, we need not distinguish whether an event is telic or not. For example,  Exp. 1 ~)~t]~'j~, '~.  (He is playing the flute) (He is playing a song '%iangzhu" )  Chen[3] thinks that in Exp. 1, the first sentence has the features: dynamicity, and durativity, and non-telicity; it belongs to "activi~' . The second sentence has the features dynamicity, durativity, and telicity, because in the second sentence, there is a default terminal.... when the song ~angzhd' is over, the action '~la~ is over, so the sentence belongs to 'hccomplishmenf instead of "activity. However we think such discrimination is useless for information extraction, because telicity is an ambiguous concept itself. What we need is to acquire the exact duration of the event. So if we knew the event is durative or not, and got the temporal phrases, we can know terminal time of the event. Besides, whether an event is telic or not can not be attributed to collocation and only can be done manually(as the exp 1 shows). For these reasons, we consider the two verbs in Exp. 1 belonging to the same situational type, that is, we do not use talicity as a phase feature to classifying verbs.  142  (2) Separate classification of the verb situation from classification of the sentence situation. Chen[3] points that some verbs belong to more than one category, and gives a method to distinguish these cases. To make the ideal more clear, we use two steps to complete the seritence situation recognition. In this paper, we render an algorithm to classify verbs into different categories, which is the basis of another research.... recognition of sentence situation, which will be discussed in future work.  'Men(MentalityJ' can follow '~1~ (very).  Verbs in the "AmlS' category can followed  by "~-~(preposition-objec0' structures, etc.  The following is the set of collocational  features.  Static verbs Amb Act  Ins  Att Men  Verb+T ~ll~+Verb Verb+~ :i-+Verb Verb+~  +  ÷  +  +  (-) +  (+)  (-) (-) +  ÷  Table 2.1  2.2 Classification Method We classify the verbs into five categories , Att(Attribute), Men(Mentality), Act (Activity) , Ins(Instantaneous) , Amb (Ambiguous).  Att: ~(be), ~'(equal), '~'(include), m~(accordwith) Men: ~.~J~(like), ~.,(belittle), ~(love), ~ff~ (be satisfied with) Act: ~(draw), ~l~l(gab), ~(drink), ~ ( r u n ) I n ; ~¢~(explore), ~l;~(extinguish), I~(snap), (discovery) Amb: ~.~(sit), ~i(stand), Jig(lie), ~(kneel), ~:(bring), ~(hang), ~(wear), ~-~(install) Amb(Ambiguous) include those words which describe different situations in different context. For example:  Exp. 2:  (they hung the picture on the wall. ) (Picture is hanging on the wall.)  In Exp. 2, the two sentences have the same predicate ' ~ (hang). In the first sentence, '~ descnbes an instantaneous action, but the second sentence describes a state. In English, forms of these two predicates are different; while in Chinese, they are the same. For this reason, we consider it ambiguous and indistinguishable without context. We have pointed out previously that phase depends only on meaning. However different situational types collocate with different words. So the essence of our algorithm is replace semantic judgement with collocational judgement. For example,  2.3 Implementation of the algorithm According to table 2.1, a classification algorithm was designed, and we use two resources to implement our algorithm: The Contemporary Chinese Cihai [11] (which we will refer to as the Cihai below) dictionary and the Machine Tractable Dictionary of Contemporary Chinese Predicate Verbs [12](which we will refer to as the predicate dictionary below). The Cihai dictionary includes 12,000 entries and 700,000 collocation instances, predicate dictionary includes about 3000 verbs with their semantic information, case relations and detailed collocation information. These two dictionaries both include some of the collocation information that the algorithm needs. Considenng the features of these two dictionaries, we adjust part of our algorithm: (1) In predicate dictionary, there is a slot named "verb typ£ , which includes 'transitive verlY , 'fntransifive verB' , ~ttribufive verlS", 'linking vertt' etc. So, at the beginning of the algorithm, we judge if the verb is a "linking verlS'( ~(be~', ~ ( e q u a l ~ ' ,etc) or a "possessive verlS' ('~-q~J' (have)). If it is, we directly classify the verb as "att(attribute~' without further processing. (2) The predicate dictionary provides the case relation of verbs, and their semantic ategodes. We restrict the agent of verbs in the "Men(mentality~' to belong to one of:  143  "{ .)kI"(people), "{ )l,.,~} "(human), "1  )l,~}  "(multitude)  , {~s:}  (collectivity)'; "{:~:-~¢~}(creatures)'; ,,~~,-i-i,~,~,jtr  belieff, "{gJJl~}(animal)" (3) Because  Cihai includes collocation instances instead  of collocation relations, we should consider  synonyms. To be exact, when we determine  whether a verb belongs to "Men(Mentality~'  or not, we judge if it can follow  (very) and  synonyms such as  'trY'P;  However, some seldom seen instances were  included. All these cause some errors.  The final algorithm is as follows:  if (a verb is labeled as" linking verb" or '~:~ossessivevertf in predicate dictionary )  then  the verb belongs to "Att(Attribute)"  else if (the verb can follow ":~[~"(very)and synonyms "~.~,I~",  "1-¢~"; ' ~ l " ~ '~71~2', " ~ " ) and (its agenf s  semantic belongs to setl*)  then the verb belongs to "Men(Mentality)"  else if (it can follow ";t~E")or (be followed by"~")  then if (it can be followed by "preposition-object"  structure)  then the verb belongs to "Amb(Ambiguous)"  else the verb belongs to "Act(Activity)"  else if (it can be followed by"T")  then the verb belongs to"Ins(Instantaneous)"  else the verb belongs tff unknown"  *setl={human, multitude, collectivity, creatures, belief, animal }  3. Results and Analysis  3.1 Results We use the algorithm above to classify the 2981 words in predicate dictionary, at the same time, we do the classification manually, Table 3.1 is the result:  Att Men Amb Ins Human 20 112 500 662 Algo- 20 111 537 691 Rithm *this 4 words are not verb. Table 3.1  Act Un-  Totel  known  1683 4* 1519 i 103 i  2981 2981  Table 2.2 shows the details:  by algo- Classifying b human  rithm  Att Men Amb Ins Act Non- Totel verb  Art  20  0  0  0  0 0  20  Men  0  99  
This paper describes a method for adapting a general purpose synonym database, like WordNet, to a specific domain, where only a subset of the synonymy relations defined in the general database hold. The method adopts an eliminative approach, based on incrementally pruning the original database. The method is based on a preliminary manual pruning phase and an algorithm for automatically pruning the database. This method has been implemented and used for an Information Retrieval system in the aviation domain. 
This paper describes an experiment aiming at evaluating the role of NLP based optimizations (i.e. morphological derivation and synonymy expansion) in web search strategies. Keywords and their expansions are composed in two different Boolean expressions (i.e. expansion insertion and Cartesian combination) and then compared with a keyword conjunctive composition, considered as the baseline. Results confirm the hypothesis that linguistic optirnizations significantly improve the search engine performances. Introduction The purpose of this work was to verify if, and in which measure, some linguistic optimizations on the input query can improve the performance of an existing search engine on the web 1. First of all we tried to determine a proper baseline to compare the optimized search strategies. Such a baseline should reflect as much as possible the average use of the search engine by typical users when querying the web. A query is usually composed of a limited number of keywords (i.e. two or three), in a lemmatized form, that the search engine composes by default in a conjunctive 1The results reported in this paper are part of a more extended project under development at ITC-irst, which involves a collaboration with Kataweb, an Italian web portal. We thank both Kataweb and Inktomi Corporation for kindly having placed the search engine for the experiments at our disposal.  expression. Starting from this level (we call it "basic level") we have designed two more sophisticated search strategies that introduce a number of linguistic optirnizations over the keywords and adopt two composition modalities allowed by the "advanced search" capabilities of the search engine. One modality (i.e. Keyword expansion Insertion Search - KIS) first expands each keyword of the base level with morphological derivations and synonyms, then it builds a Boolean expression where each expansion is added to the base keyword list. The second modality (i.e. Keyword Cartesian expansion Search KCS) adopts the same expansions of the previous one, but composes a Boolean expression where all the possible tuples among the base keywords and expansions are considered. The working hypothesis is that the introduction of lexical expansions should bring an improvement in the retrieval of relevant documents. To verify the hypothesis, a comparative evaluation has been carried out using the three search modalities described above over a set of factual questions. The results of the queries have been manually scored along a five value scale, with the aim of taking into account not only the presence in the document of the answer to the question, but also the degree of contextual information provided by the document itself with respect to the question. Both the presence of the answer and the contextual information have been estimated by two relevance functions, one that considers the document position, the other that does not. The experiment results confirm that the introduction of a limited number of lexical expansions (i.e. 2-3) improves the engine performance. In addition, the Cartesian  13  composition of the expansions behaves significantly better than the; search modality based on keyword insertion. Some of the problems that we faced with in this work have been already discussed in previous works in the literature. The use of query expansions for text retrieval is a debated topic. Voorhees (1998) argues that WordNet derived query expansions are effective for very short queries, while they do not bring any improvements for long queries. From a number of experiments (Mandala et al., 1998) conclude that WordNet query expansions can increase recall but degrade precision performances. Three reasons are suggested to explain this behavior: (i) the lack of relations among terms of different parts of speech in WordNet; (ii) many semantic relations are not present in WordNet; (iii) proper names are not included in WordNet. (Gonzalo et al., 1998) pointed out some more weaknesses of WordNet for Information Retrieval purposes, in particular the lack of domain information and the fact that sense distinctions are excessively fine-grained for the task. A related topic of query expansion is query I~anslation, which is performed in Cross-Language Information Retrieval (Verdejo et al. 2000). This work brings additional elements in favor of the thesis that using linguistic expansions can improve IR in a web search scenario. In addition we argue that, to be effective, query expansion has to be combined with proper search modalities. The evaluation experiment we carried out, even within the limitations due to time and budget constraints, was designed to take into account the indications that came out at the recent TREC workshop on Question Answering (Voorhees, 2000). The paper is structured as follows. Section 1 and 2 respectively present the modalities for the linguistic expansion and for the query composition. Section 3 reports the experimental setting for the comparative evaluation of the three search modalities. Section 4 describes and discusses the results obtained, while in the conclusions we propose some directions for future work.  
In several recent years, natural language processing (NLP) has brought some very interesting and promising outcomes. In the field of information retrieval (IR), however, these significant advances have not been applied in an optimal way yet. Author argues that traditional IR methods, i.e. methods based on dealing with individual terms without considering their relations, can be overcome using NLP procedures. The reason for this expectation is the fact that NLP methods are able to detect the relations among terms in sentences and that the information obtained can be stored and used for searching. Features of word senses and the significance of word contexts are analysed and possibility of searching based on word senses instead of mere words is examined. The core part of the paper focuses on analysing Czech sentences and extracting the context relations among words from them. In order to make use of lemmatisation and morphological and syntactic tagging of Czech texts, author proposes a method for construction of dependency word microcontexts fully automatically extracted from texts, and several ways how to exploit the microcontexts for the sake of increasing retrieval performance.  
We describe in this paper a boolean Information l~.etrieval system that adds word semantics to the classic word based indexing. Two of the main tasks of our system, namely the indexing and retrieval components, are using a combined wordbased and sense-based approach. The key to our system is a methodology for building semantic representations of open text, at word and collocation level. This new technique, called semantic indexing, shows improved effectiveness over the classic word based indexing techniques. 
In spite of long controversy, effectiveness of phrasal indexing is not yet clear. Recently, correlation between query length and effect of phrasal indexing is reported. In this paper, terms extracted from the topic set of the NACSIS test collection 1 are analyzed utilizing statistic tools in order to show distribution characteristics of single word/phrasal terms with regard to relevant/nonrelevant documents. Phrasal terms are found to be very good discriminators in general but not all of them are effective as supplemental phrasal terms. A distinction of informative / neutral / destructive phrasal terms is introduced. Retrieval effectiveness is examined utilizing query weight ratio of these three categories of phrasal terms. Introduction 
In this paper, we present a corpusbased learning method that can index diverse types of compound nouns using rules automatically extracted from a large tagged corpus. We develop an efficient way of extracting the compound noun indexing rules automatically and perform extensive experiments to evaluate our indexing rules. The automatic learning method shows about the same performance compared with the manual linguistic approach but is more portable and requires no human efforts. We also evaluate the seven different filtering methods based on both the effectiveness and the efficiency, and present a new method to solve the problems of compound noun over-generation and data sparseness in statistical compound noun processing. 
This paper argues that a finite-state language model with a ternary expression representation is currently the most practical and suitable bridge between natural language processing and information retrieval. Despite the theoretical computational inadequacies of finitestate grammars, they are very cost effective (in time and space requirements) and adequate for practical purposes. The ternary expressions that we use are not only linguistically-motivated, but also amenable to rapid large-scale indexing. REXTOR (Relations EXtracTOR) is an implementation of this model; in one uniform framework, the system provides two separate grammars for extracting arbitrary patterns of text and building ternary expressions from them. These content representational structures serve as the input to our ternary expressions indexer. This approach to natural language information retrieval promises to significantly raise the performance of current systems. 
We propose a new approach to text categorization based upon the ideas of summarization. It combines word-based frequency and position method to get categorization knowledge from the title field only. Experimental results indicate that summarization-based categorization can achieve acceptable performance on Reuters news corpus. Introduction With the current explosive growth of Interact usage, the demand for fast and useful access to online data is increasing. An efficient categorization system should provide accurate information quickly. There are many applications for text categorization, including information retrieval, text routing, text filtering and text understanding systems. The text categorization systems use predefmed categories to label new documents. Many different approaches have been applied to this task, including nearest neighbor classifiers (Masand, Linoff and Waltz, 1992; Yang, 1994; Lain and Ho, 1998; Yang, 1999), Bayesian independence classifiers (Lewis and Ringuette, 1994; Baker and McCallum, 1998; McCallum and Nigam, 1998), decision trees (Fuhr et al., 1991; Lewis and Ringuette, 1994; Apte et al., 1998), induction rule learning (Apte et al., 1994; Cohen and Singer, 1996; Mouilinier et al., 1996), neural networks (Wiener, Pedersen and Weigend, 1995; Ng, Gob and Low, 1997), and support vector machines (Joachims, 1998). These categorization algorithms have been applied to many different subject domains, usually news stories (Apte et al., 1994; Lewis and Ringuette, 1994; Wiener, Pedersen and Weigend, 1995; Yang, 1999), but also physics abstracts (Fuhr et  al., 1991), and medical texts (Yang and Chute, 1994). In this research to resolve the task of text categorization we apply a method of text summarization, that is, combining word-based frequency and position method to get categorization knowledge from the title field only. Experimental results indicate that summarization-based categorization can achieve acceptable performance on Reuters news corpus. Additionally, the computation time for the title field is very short. Thus, this system is appropriate for online document classifier. Following is a description of the organization of this paper. Section 2 describes the previous work of summarization. Summarization-based algorithms for text categorization are outlined in Section 3. The experiments we undertook to assess the performance of these algorithms are the topic of Section 4. Quantitative experimental results are also summarized. Finally, concluding remarks and recommendation for future work is made. 
This paper describes a system which enables users to create on-the-fly queries which involve not just keywords, but also sortal constraints and linguistic constraints. The user can specify how the results should be presented e.g. in terms of links to documents, or as table entries. The aim is to bridge the gap between keyword based Information Retrieval and pattern based Information Extraction. 
We present our work on open-domain multi-document summarization in the framework of Web search. Our system, SNS (pronounced "essence"), retrieves documents related to an unrestricted user query and summarizes a subset of them as selected by the user. We present a taskbased extrinsic evaluation of the quality of the produced multi-document summaries. The evaluation results show that summarization quality is relatively high and does help improve the reading speed and judge the relevance of the retrieved URLs. 
This paper describes a Japanese dialogue corpus annotated with multi-level information built by the Japanese Discourse Research Initiative, Japanese Society for Artificial Intelligence. The annotation information consists of speech, transcription delimited by slash units, prosodic, part of speech, dialogue acts and dialogue segmentation. In the project, we used the corpus for obtaining new findings by examining the relationship between linguistic information and dialogue acts, that between prosodic information and dialogue segment, and the characteristics of agreement/disagreement expressions and non-sentence elements. 
In this paper annotation modularity and use of annotation metaschemes are identified as basic requirements for achieving actual corpora reusability. We discuss these concepts and the way they are implemented in the architectural framework of the ADAM corpus, which is a corpus of 450 Italian spontaneous dialogues. The design of ADAM architecture is compatible with as many practices of dialogue annotation as possible, as well as approaches to annotation at different levels. 
Since early 1998, the European Telematics project MATE has worked towards facilitating re-use of annotated spoken language data, addressing theoretical issues and implementing practical solutions which could serve as standards in the field. The resulting MATE Workbench for corpus annotation is now available as licensed open source software. This paper describes the MATE markup framework which bridges between the theoretical and the practical activities of MATE and is proposed as a standard for the definition and representation of markup for spoken dialogue corpora. We also present early experience from use of the framework. 1. Introduction Spoken language engineering products proliferate in the market, commercial and research applications constantly increasing in variety and sophistication. These developments generate a growing need for tools and standards which can help improve the quality and efficiency of product development and evaluation. In the case of spoken language dialogue systems (SLDSs), for instance, the need is obvious for standards and standard-based tools for spoken dialogue corpus annotation and automatic infomaation extraction. Information extraction from annotated corpora is used in SLDSs engineering for many different purposes. For several years, annotated speech corpora have been used to train and test speech recognisers. More recently, corpus-based approaches are being applied regularly to other levels of processing, such as syntax and dialogue. For instance, annotated corpora can be used to construct lexicons and grammars or train a grammar to acquire preferences for frequently  used rules. Similarly, programs for dialogue act recognition and prediction tend to be based on annotated corpus data. Evaluation of usersystem interaction and dialogue success is also based on annotated corpus data. As SLDSs and other language products become more sophisticated, the demand will grow for corpora with multilevel and cross-level annotations, i.e. annotations which capture information in the raw data at several different conceptual levels or mark up phenomena which refer to more than one level. These developments will inevitably increase the demand for standard tools in support of the annotation process. The production (recording, transcription, annotation, evaluation) of corpus data fer spoken language applications continues to be timeconsuming and costly. So is the construction of tools which facilitate annotation and information extraction. It is therefore desirable that already available annotated corpora and tools be used whenever possible. Re-use of annotated data and tools, however, confronts systems developers with numerous problems which basically derive from the lack of common standards. So far, language engineering projects usually have either developed the needed resources from scratch using homegrown formalisms and tools, or painstakingly adapted resources from previous projects to novel purposes. In recent years, several projects have addressed annotation formats and tools in support of annotation and information extraction (for an overview, see http://www.ldc.uperm.edu/annotation/). Some projects have addressed the issue of markup standardisation from different perspectives. Examples are the Text Encoding Initiative (TEI) 0attp://www-tei.uic.edu/orgs/tei/ and http://etext.virginia.eduffEI.html), the Corpus Encoding Standard (CES) (http://www.cs.vassar.edu/CES/), and the European Advisory Group for Language Engineering Standards (EAGLES) (http://www.- 
Conversational grunts, such as uhhuh, un-hn, rnrn, and oh are ubiquitous in spoken English, but no satisfactory scheme for transcribing these items exists. This paper describes previous approaches, presents some facts about the phonetics of grunts, proposes a transcription scheme, and evaluates its accuracy.1 
This paper presents an empirical analysis of prosodic phenomena (intonation and timing) in 'common ground units' (Nakatani & Traurn 1999). The analysis is used to address questions of the role of prosody in dialogue while taking into account the complexities of multispeaker discourse. We address some methodological concerns of how best to carry out a study of this kind as well as our theoretical questions about the formal identification of dialogue structures at levels higher than the micro-level of 'dialogue act', or 'move'. Introduction This paper reports some of the results from our research into the relationship between prosodic structure and discourse structure in dialogue. One of our particular interests is how to identify and analyse relevant prosodic parameters in multi-speaker discourse. We have been examining the kinds of dialogue structure frameworks that best account for patterns of prosodic phenomena; and conversely, the types of dialogue structure that exhibit prosodic regularities. Our research domain is hurnam human natural dialogue Settings but our questions  are equally relevant to researchers working on systems for more naturalistic human-computer interfaces, as well as those developing better automated systems for annotating large speech corpora. The main methodological considerations associated with our current work are: a) how natural dialogues can be reliably annotated to allow independent comparisons and correlations of prosodic and structural features, b) the identification and classification of units of dialogue that reflect the 'joint action' feature of interactive discourse (ie. that both participants in a dialogue contribute to dialogue structure (Clark 1992, 1996), an aspect of dialogue that fundamentally differentiates it from monologic discourse). These issues will be addressed in this paper using data from a corpus of naturally produced spoken dialogue taken from the Australian Map Task corpus (Millar et al 1994). Here we have focussed on the process of grounding, the assignment of utterances to 'common ground units' (CGUs - Nakatani & Traurn 1999) and the internal structure of these units, as a means of illustrating some of the problems that both methodological issues raise. We show how  some of these problems might be overcome by focussing on sequences of initiating and responding (typically grounding) contributions within CGUs, as a site for prosodic analysis, rather than on the boundaries of the units as a whole (cf. Stiding et al 2000a). This approach thus preserves the notion that one can identify 'chunks' of dialogue in which particular types of information are acknowledged as being in the common ground of both participants, while remaining true to the dynamic nature of the grounding negotiation.  1. Background  1.1. Prosody and Discourse Structure  Most. empirical work examining prosody in discourse has focussed on its function in monologue (eg. Swerts 1997, Nakatani et al 1995, Hirchberg & Nakatani 1996). These studies have found that a range of acoustic parameters associated with prosody, such as final lengthening and type of boundary tone, are good indicators of the boundaries between different discourse units at micro and macro levels of discourse structure.  More recently, there has been an interest in  examining how prosody may be used in dialogue  to signal discourse structure in that domain.  Shriberg et al (1998) showed that various  prosodic cues (duration, F0, pause length and  speaking rate) were relevant for the automatic  classification of dialogue 'acts'. Stifling et al  (2000b)  similarly  showed  strong  correspondences between the boundaries of  dialogue acts and prosodic phenomena such as  pitch reset and intonational phrase boundaries  (represented as ToBI 'break indices'). But  dialogue acts are the 'parts' of dialogue most  akin with structural elements of monologic  discourse, since each 'act' can be analysed as  independent utterances by a single speaker.  Higher levels of dialogue structure necessarily  involve some interactive 'chunk' of the discourse  to which both participants in the dialogue contribute some speech. So while it is clear that prosody serves to delimit dialogue acts, and to some extent distinguish between them (eg. Shriberg et al 1998, Koiso et al 1998, Stifling et al 2000b), the question remains whether prosody is also a reliable indicator of dialogue structure at higher levels (analogous with the higher levels of monologic discourse structure described in Swerts (1997), Nakatani et al. (1995), and others) and to what uses are prosodic phenomena put in the context of higher levels of dialogue structure. 1.2 Grounding and Common Ground Units Grounding is the process by which information contributed by participants in interaction is taken to have entered the 'common ground', or mutual knowledge of the participants (Clark & Schaefer 1989, Clark 1996, Traum 1994). The process of grounding requires that one participant contributes something to the discourse (minimally, a dialogue act), and that the other participant make some indication that the contribution has been heard and accepted as a contribution (though not necessarily understood). This 'indication' may be a verbal acknowledgment (or some other kind of verbal response) or it may be some kind of non-verbal comrnlmicative act (like head nods, facial expression and other gestures). Traum (1998) and Nakatani & Traum (1999) have recently proposed taking grounding as the basic principle behind the structuring of dialogue at levels ligher than the dialogue act. Minimal units of acknowledged common ground have been considered as the building blocks of higher level dialogue structures based on intentional or informational content (eg. 'Common Ground Units', or 'CGUs' Nakatani & Traum (1999)). CGUs, which represent grounding at the 'illocutionary level' (Clark 1996), have been proposed as a meso-level dialogue structure roughly the same level that dialogue games (Carletta et al, 1997) or adjacency pairs (eg.  37  Sinclair & Coulthard 1975) occupy in their dialogue structure frameworks. The appeal of taking units basexl on grounding as the level of dialogue structure above the microlevel of 'act' (as argued in Nakatani & Traurn 1999) lies in its pfiofitization of mutual understanding as a central component of dialogue, regardless of the type of initiation and response. In the 'CGU' fiamework, some responses themselves get grounded so that the result is a complex configuration of overlapping and embedded units of information entered into the common ground of the participants. This approach thus acknowledges importance of the conlributions by both participants in the grounding process. It highlights the 'joint action' aspect of dialogic communication. Evaluation of the coding of CGUs in dialogue by the Discourse Resource Initiative (Core et al 1999) showed a low degree of intercoder reliability, especially for those coding the HCRC Map Task corpus (Anderson et al 1991). Some of the inconsistencies across coders were attributed to "iraonation and ~rfing" (p. 61), as well as to difficulties in coding different types of acknowledgments. Some proposals for the classification of acknowledgments were made (Core et al 1999) and Stirling et al (2000a) have noted some parameters along Which CGUs might be further classified. The ways that this classification has been refined and utilised for the cun,ent paper are described in section 2 (methods), where we also describe our system of annotation for both prosodic and CGU properties of the dialogues, and explain our method for utilising this annotation for the current analysis. In section 3 we present some results of our investigation of prosody and grounding in the light of the methodological issues listed above. These results will be used in section 4 to address the following theoretical questions, as well as to set the agenda for future research into prosodic correlates of dialogue structure: a) can prosody be used as a heuristic for identifying dialogue structure above the level of the 'dialogue act'?  b) does the process of 'grounding' have any formal basis, prosodic or otherwise, independent of the identification of dialogue acts?  2. Methods  2.1 Annotation Methods  Our corpus consists of 4 dialogues from the MAP TASK section of the Australian National Database of Spoken Language - ANDOSL (Millar et al, 1994). This corpus is closely modelled on the HCRC Map Task (Anderson et al 1991). Participants worked in pairs, each with a map in front of them that the other could not see. One participant (the 'instruction-giver' IG) had a route marked on their map and was required by the task to instruct the other (the 'instruction-follower' IF) in drawing the correct route onto their own map. The maps were similar, but differed in the presence, position and names of certain of the landmarks. Each pair of participants participated in two dialogues, swapping roles of instruction-giver and instruction-follower, and thus producing a first time and second time attempt at the task.  The four dialogues used here were from two  pairs of participants: the two dialogues from a  pair 'Known' to one another, differing in which  participant took the role of instruction-giver, and  the two dialogues from a pair 'Unknown' to one  another, also differing in which participant took  the role of instruction-giver. In each case, the  pairs Were mixed-gender. The dialogues were  chosen randomly and the speakers belonged to  the General Australian English dialectal grouping.  The pre-recorded dialogues were copied from  CD and digitised for analysis at 22 kHz using  Entropic's ESPS/ Waves + speech analysis  software running on a Sun workstation in the  Phonetics Laboratory of the UniversiW of  Melbourne.  A complete orthographic  transcription of the dialogues was carried out.  . ..._-  -  38  The prosodic featu_es of the dialogues were labelled on separate tiers using the ToBI (Tone and Break Indices) prosodic transcription conventions for Australian English, detailed in Fletcher & Harrington (1996), closely modelled on the criteria developed for American English intonation (Beckman & Ayers Elam 1994/1997). The dialogues were prosodically annotated for break indices (degree of juncture) and phrase and boundary tones according to ToBI conventions using the Xwaves label function. The dialogues were also aunota~ed on separate tiers in Xwaves for the timing features of pause location and duration (in ms), and overlap location and duration (in ms)i. These features were noted with respect to preceding and following talk. The features of break index, phrase and boundary tones and pause and overlap phenomena were selected to address the 'intonation and timing' problems that were raised by the Discourse Resource Initiative (Core et al 1999), and because related work in conversation analysis has demonstrated the importance of intonation (especially final contours) to being able to account for the meaning of acknowledgments in interaction (eg. Mffller 1996, Gardner 1998). Break Indices (BI) were labelled as follows: 4 (full intonation phrase boundary); 3 (intermediate intonation phrase boundary); 3p (disfluent intermediate intonation phrase boundary). Boundary tones were labelled as follows: H-H% for a high rising tone (1314); L-L% for a falling or low tone (BI 4); I.-H% for a low rising tone (BI 4); H-L% for a mid-level tone (1314); H- for a high intermediate phrase boundary (BI 3), I.for a low intermediate phrase boundary (BI 3). The pause and overlap labelling allowed us to extract information about the timing of each speaker contribution with respect to the previous and following contributions to the talk. With respect to the preceding talk, contributions were analysed as having a pause before (pb), partially overlapping with the preceding conlribution (olb), completely overlapping (ol), no pause or overlap  with the preceding unit (or 'latch' - lb), or 'continued" (if the previous contribution was by the same speaker without an interceding pause (c)). With respect to the following talk, contributions were analysed as having a pause after (pa), partially overlapping with the following contribution (ola), completely overlapping (ol), no pause or overlap with following contribution (or latch - la), or 'continued' (if the next contribution was by the same speaker without an interceding pause (c)). Table 1 provides a summary of the labels used for prosodic and timing phenomena.  Table 1: Prosodic and Timing Labels  Break Index  Phrase and Timing  Boundary (previous  Tones  ta )  Timing (following tan0  4  H-H%  pb  pa  3  L-L%  o]b  oh  3p  L-H%  ol  ol  H-L%  h  H-  C  L-  Since the major goal of our research was to investigate associations between prosodic phenomena and discourse structure, we coded discourse categories independently from dividing the speech signal into prosodic units and the coding of prosodic phenomena (these parameters were coded by different researchers). The dialogues were annotated for Common Ground Units, following Nakatani & Tram (1999). ii The coding was carried out by two researchers independently who then collaborated on a consensual version for each dialogue. The codes were entered on separate tiers in ESPS/ Waves +, separated according to which speaker gave which contribution. This meant that initiations and endpoints of CGUs were numbered and aligned with the speaker who began and ended each unit. Within each CGU we also coded the first 'response' by the other participant, which typically established either that the initial information was entered into the common ground, or that further negotiation was required in order to ground the information. In  39  this paper, we only considered CGUs that contained fully grounded infi3rrnafion at their close (ie. we did not include abandoned or discontinuous CGUs - see Nakatani & Traum 1999). In Stifling et al (2000a) we reported on some of the prosodic characteristics of the final unit in CGUs (break indices and turn boundaries only). Here we report on the prosodic characteristics of the initiation move of the CGU and the prosodic profile of this first responding contribution. 2.2. Classification of CGUs We go a step further than Nakatanl & Traum (1999) and Stirling et al (2000a) in further classifying CGUs in terms of the internal characteristics of the grounding process whether they consisted of a 'simple' exchange of initiating move and responding move, or whether their structure involved more contributions (by either speaker) before the CGU in question was considered to be complete (ie. the information was acknowledged as entered into the common ground). The former type we called 'Simple' CGUs, while the latter type we called 'Complex' CGUs. Complex CGUs were further classified into four types based on pragmatic and sequential criteria, as follows: a) Overlapping CGUs, where the grounding element of one unit was itself grounded with some verbal acknowledgment in the next CGU, as in (1) below. b) CGUs which consisted of further acknowledgment(s) by the other speaker subsequent to the first grounding element, as in (2) below. c) CGUs which contained more than one acknowledgment by the same speaker (as in (3) below) d) CGUs which negotiated information at different levels of communication lower than the 'illocutionary' level (eg. the level of 'presentation' or 'locution' (Clark 1996)), and were therefore considered part of a larger CGU which was gounded at the level ofillocution, as in (4) below.  (1) IF: am I to the left-hand side or the right- hand side of the d~ of your Galah Open-cut Mine IG: looking at it you're on the left (finishes one CGU and starts another) IF: okay (a) IG: oh you've got Whispering Pine have you IF: yes IG: right  (3) IG: that is uh as a point of looking at the Consumer Trade Affak [riOt] IF: [okay] yeh  (4)  IG: so you're] swee[ping east]  IF:  [so am I sweep]ing  fight around [am I going east]  IG:  [you're sweeping] east  IF: yeh okay well don't~ yeh okay allright  I'm going east  All four types of complex CGUs represent some kind of 'expansion' of the canonical CGU exchange. In overlapping CGUs, the second 'response" functions to ground the first response. In multiple acknowledgment CGUs, the second response is not analysed as 'grounding', but is nevertheless some kind of filrther acknowledgment (either by the same speaker or by the other speaker). In the final case of complex CGUs, the first response is a signal that perhaps some part of the initial contribution had not entered into the eomrnon gound and that further collaboration was required. '~  These categories are not claimed to represent the only way that CGUs can be classified (el. Core et al 1999). Nevertheless they provide us with a useful basis for addressing differences in the role of prosody in CGUs.  40  Altogether there were 419 CGUs in the corpus. Of these, 241 were 'simple' CGUs and 178 were 'complex'. Of the 178 complex CGUs, 47 (26%) were 'overlapping', 59 (33%) contained multiple acknowledgmentby both participants, 52 (29%) contained multiple acknowledgements by the same speaker, and 20 (1 I%) contained more complex negotiations of understanding until acknowledgment of mutual understanding of the initial dialogue act was reached (the level at which CGUs were delineated). We hypothesised that since, on our definition, complex CGUs differed from the simple type in terms of the structure of their response 'phase', rather than in terms of their initiation phase, there should be no difference between simple and complex CGUs with respect to the timing and prosodic profile of initiations. However, we predicted that there might be quantitatively recognisable properties of the first response in complex CGUs which motivated their expansion, and that these would be different from those found in simple CGUs. While we expected that these 'recognisable' properties would involve a combination of grammatical, pragmatic and prosodic properties, here we were only interested in the extent to which simple and complex CGUs could be differentiated on prosodic grounds. In order to check these hypotheses, we pooled the four dialogues and compared initiation and first response contributions in simple and complex CGUs for each of the four parameters identified above (BI, boundary tone, timing (before) and timing (after)). Chi square tests were carried out on each of these comparisons to determine which sets of parameters displayed significant variation between simple CGUs and complex CGUs. The results are presented in the following section.  3. Results Chi square tests on the combined totals of all CGUs showed highly significant differences (p<0.001) between initiations and responses for all four prosodic and timing parameters. This result is consistent with those reported in Stirling et al (2000b), where it was shown that initiation type dialogue acts could be prosodically differentiated from response type dialogue acts. Here we were more interested in the question of whether the complexity of the CGU was also reflected in initiation and response contributions. The results reported in the following subsections therefore refer to the percentage number of each prosodic and timing parameter as a proportion of the CGU type (percentages are in boldface) - the total number of simple CGUs or the total number of complex CGUs - for initiations and responses independently.  3.1 Correspondences with Break Indices  Table 2: Initiations (CGU type x BI)  BI4 BD BI3p No BI  Simple  227 7  5  2  CGUs  94.2 2.9 2.1  0.8  Complex CGUs  165 4  9  0  92.7 2.3 5.0 0  392 11 14  2  Total  93.6 2.6 3.3  0.5  Z2 (dr=3, N=419) = 2.81, NS  Total 241 100 178 100 419 100  Table 3: Responses (CGU type x BI)  BI4 BI3 BI3p No BI  Simple CGUs  161 53 2  25  66.8 22.0 0.8  10.4  Complex  145 18 2  13  CGUs  81.5 10.1 1.1  7.3  Total  306 71 4  38  73.0 16.9 1.0  9.1  Z2 (dr=3, N--419) = 12.69, p<0.01  Total 241 100 178 100 419 100  As expected, there was no significant difference between simple and complex CGUs with respect to their initiation contribution break indices. Examination of the first res~nse of these units did show a significant difference between simple  41  and complex CGUs with respect to break indices. In particular, the first responses for simple CGUs had a high proportion of BI3 (corresponding with an intermediate phrase boundary) compared with complex CGUs and a relatively low proportion of BI4 (corresponding with a full intonation phrase). That is, the first response contribution of a complex CGU was more likely to end with a full inttonafionalphrase boundary (BI4) than the first response contribution of a simple CGU.  boundary tones were not significantly different and there were proportionally few instances of other types of contours. Like initiation conlributions, the results for response contributions also show a higher proportion of low falling tones in complex CGUs than in simple CGUs (30.3% cf. 17.5%). The proportions of both high rising and low rising tones appears stable across the types of CGUs however.  3.2 Correspondences with Boundary Tones  Table 4: Initiations (CGU type x tones)  H- L- L- HH% L% H% L% H- L- No Tot  Simpl 82 71 72 4  4 3 5 241  e  43. 29. 29. 1.7 1.7 1.2 2.0 100  CGUs 0  5  9  C o m p 53 75 33 2  2 4 9  178  I  29. 42. 18. 1.1 1.1 2.3 5.1 100  CGUs 8  
In this paper 1 I describe the use of Danish pronouns and deictics in dialogues. Then I present an adaptation to Danish of Eckert and Strube's algorithm for resolving anaphora referring to individual NPs and abstract objects in English dialogues (Eckert and Strube, 1999b; Eckert and Strube, 1999a). The adapted algorithm is tested on four Danish dialogues from two dialogue collections and the results obtained are evaluated. .1 Introduction Many natural language processing applications involve the complex task of resolving anaphora. Different strategies for anaphora resolution have been proposed, some exclusively relying on the syntactic structure of discourse, some including semantic and pragmatic constraints, some based on statistical methods. One of the most popular approaches to anaphora resolution is centering (Grosz et al., 1995), henceforth GJW95, which accounts for the relation between the saliency of entities in discourse and the use of referring expressions, incorporating syntax, semantics and pragmatics. Centering fits into Grosz and Sidner's model of discourse structure (Grosz and Sidner, 1986). In this model a discourse is composed of segments which exhibit global coherence. A discourse 1This work has been carried out under Staging, an on-goingDanish project funded by the Danish Research Councils.  segment, on the other hand, is composed of a sequence of utterances which exhibit local coherence. This latter phenomenon is accounted for by centering theory. Centering predicts that there is a connection between the coherence of a referring expression and the inference load necessary to resolve it. Although Grosz, Joshi and Weinstein recognize that many factors determine the prominence of entities in an utterance, in GJW95 this prominence is established simply by the linear order of the entities in the utterance. Different centering algorithms have been presented, spelling out the strategy described in GJW95, extending the theory to more linguistic phenomena or specifying the concept of prominence of discourse entities. Strube and Hahn (Strube, 1998; Strube and H~.hn, 1999) in particular, calculate prominence considering the information structure of the utterances (functional centering).2 The prominence ranking they adopt does not exclusively rely on word order, which is language dependent. Moreover GJW95 only dealt with intersentential anaphora, while Strnbe and Hahn account for both intrasentential and intersentential, pronominal and nominal anaphora, a Centering-based algorithms have been tested on written texts. Recently they have also been applied to written dialogues. Byron and Stent (1998), in particular, test centering on multi-party dialogues. They conclude that centering seems to be a valid theory also in this case, but it must be extended to ac- 2In (Strube and Hah~ 1996) a functional-based prominenceranking has been proposed. SAn other extension of the centering framework to intrasentential anaphora has been proposed by Kameyama (1998).  56  count for dialogne-specific aspects such as the definition of utterance boundaries, the specification of a strategy for tackling partial utterances and including discourse participants in the list of relevant discourse entities. Eckert and Strube (1999a; 1999b), henceforth ES99, describe an algorithm for resolving anaphors referring to individual NPs and abstract objects in English dialogues. The algorithm is based on rules for discriminating among the two types of anaphor based on the predicative contexts in which the anaphors occur. The individual anaphors are then resolved by the functional centering algorithm described in (Strube, 1998), while abstract anaphors are resolved with a different algorithm. ES99 test the approach on selected dialogues and obtain a precision of 63,6% for discourse deictics and 66,2% for individual anaphors. They report that most errors are due to the inability to distinguish between discourse deictics and pronouns which vaguely refer to concepts in the preceding discourse (vague anaphors). Another cause of error is the lack of information about abstract nominals. I believe that the strategy followed by ES99 is a good starting point for investigating how far one can go in resolving individual and abstract anaphors in dialogues on the basis of the local contexts in which the anaphors occur. I have adapted the algorithm so it accounts for Danish data and have applied it to Danish dialogues.4 In section 2 I shortly present the original centering framework and functional centering as described in (Strube, 1998), $98. In section 3 Eckert and Strube's algorithm is introduced and in 4 the Danish personal and demonstrative prononn~ are described with focus on discourse deictics in dialogues. In section 5 1present my adaptation of the ES99algorithm to Danish data. Section 6 contalus an evaluation of the results obtained by manually testing the adapted ES99-algorithm on randomly selected dialogues from the collection "Samtale hos Leegen" (Conversation at the doctor's) (SL) and "ProjektIndvaudr- 4Centering-based algorithms have recently been tested on Danish discourse (Navarretta, 2000).  erdansk" (Project Immigrant Danish) (PID), collected by researchers at the Department of General and Applied Linguistics of the University of Copenhagen. In section 7 I outline future work for improving the results of the algorithm and make some concluding remarks. 2 Centering In GJW95 the entities which link an utterance Un to the others in the same discourse segment are the centers of that utterance. Each utterance is assigned a set of forward-looking centers, Cf, and, with the exception of the initial utterance of the segment, a backwardlooking center, Cb. The Cb of an utterance Un connects with one of the forward-looking centers of the preceding utterance Un-1 while the forward-looking centers only depend on the expressions in Un. The forward-looking centers are partially ordered to reflect relative prominence. GJW95 recognize three types of transition relation across pairs of utterances: continue, retain and shift (see table 1). Center movement and realization are constrained by two rules: R u l e I: If any element of CCf(U~-i) is realized by a pronoun in Un, then Cb(Un) must also be realized by a pronoun Rule 2: Center continuation is preferred to center retaining which is preferred to center shifting 2.1 Functional Centering In $98 the functions of the backward-looking center and the transitions in the centering theory are replaced by the order of elements in a list of salient discourse entities, the Slist. The ranking criteria for the elements in the S-list are based on (Prince, 1981), where discourse entities are classified into hearerold (OLD), mediated (MED) and hearer-new (NEW). The two tuples (x, Uttx, posx) and (y, utty, posy) in the S-list indicate that the entity x is evoked in utterance uttx at position posx and that y is evoked in utterance utty at position posy respectively. Given that Uttx and utty refer to Un or Un-1, the follow-  57  Table 1: Transition States  O R no Cb(U.-1)  CONTINUE RETAIN  SHIFT  ing ranking constraints on the S-list entities are valid (Strube, 1998)[p.1253]: s 1. i f x E O L D a n d y E M E D , t h e n x ~ y if x E OLD and y E NEW, then x --<y ifx E MED and y E NEW, then x ~ y 2. if x , y E OLD or x , y E MED or x , y E NEW, then if uttx > Utty then x ~ y if utt~ = u t t y and p o s z < posy then x -~ y The S98-algorithm Consists in testing a referring expression against the elements in the S-list from left to right until the test succeeds. The S-list is then updated so that new elements are inserted according to the S-list ranking criteria. When the analysis of an utterance is finished all the entities which were not realized in the utterance axe removed from the S-list. 3 Eckert and Strube's Algorithm ES99 propose a new algorithm for resolving anaphors with abstract object antecedents. Analyzing a collection of telephone conversations they distinguish the following anaphor types: individual anaphors, discourse deictics, inferrable-evoked anaphors6 and vague anaphors. Other types of pronoun are not taken into consideration. Predicates that are preferentially associated with abstract objects are marked as Iincompatible (*I) while predicates that are preferentially associated with individual objects are marked as A-incompatible (*A). 5I mark ranking precedence with ~. 61nferrable-evokedanaphors refe~to the use of the plttral pronoun they indirectly co-specifying with a singular NP which indicates a country or an institution.  ES99 define the following *I predicates (Eckert and Strube, 1999b)[p. 40]: Equating constructions where a pronominal referent is equated with an abstract object, e.g., x is making it easy, x is a suggestion. Copula constructions whose adjectives can only be applied to abstract entities, e.g., x is true, x ks false, x is correct, x is right, x isn't right. Arguments of verbs describing propositional attitude which take S'-complements, e.g., assume. Object of do. Predicate or anaphoric referent is a "reason", e.g., x is because I like her, x is why he's late. Predicates that are preferentially associated with individual objects are the following (Eckert and Strube, 1999b)[p. 40]: Equating constructions where a pronominal referent is equated with a concrete individual referent, e.g., x is a ear. Copula constructions with adjectives which can only be applied to concrete entities, e.g., x is expensive, x is tasty, x is loud. Arguments of verbs describing physical contact/stimulation,which cannot be used anaphorically, e.g., break x, smash z, eat x, drink x, smell x but NOT *see x Grounded acts are used as domain for the anaphor resolution algorithms in dialogues.  58  In particular two dialogue acts, Initiations (Is) and Acknowledgments (As) are recognized. Is have semantic content, while As are only used to ground the preceding I. Acknowledgments/Initiations (A/Is) are dialogue acts that have both the function of grounding the preceding I and that of establishing a new I. An I and the corresponding A, together with longer Is in the same turn-taking which do not need to be acknowledged, constitute a Synchronizing Unit (SU). Short Is which are not acknowledged are ignored by the resolution algorithms. ES99 follow i.a. (Webber, 1991) in assuming that anaphoric discourse deictic reference involves reference coercion and that only discourse sections adjacent to the anaphor or, using Webber's terminology, sections on the right frontier of the discourse structure tree, are available for discourse-deictic reference. Like (Asher, 1993) they assume that the type of abstract object is determined by the context in which the anaphor occurs. Anaphora referring to abstract objects are resolved using a list, the A-list. The A-list is only filled when discourse deictics occur and its elements remain for one I. The parts of the linguistic contexts are accessed in the following order: 1. the A-list; 2. in the same I the clause to the left of the clause which contains the anaphor; 3. within the previous I the rightmost main clause and subordinated clauses to its right; 4. within previous Is the rightmost complete sentence, if previous I is an incomplete sentence. The anaphora resolution algorithm for third person singular neuter personal pronouns is the following (Eckert and Strube, 1999a): case PRO is I-incompatible if resolveDiscourseDeictic(PRO) then classify as discourse deictic else classify as vague pronoun; case PRO is A-incompatible if resolveIndividual(PRO) then classify as individual pronoun else classify as vague pronoun; case PRO is ambiguous if resolveIndividual(PRO)  then classify as individual pronoun else if resolveDiscourseDeictic(PRO) then classify as discourse deictic else classify as vague pronoun; The same algorithm is used for demonstratives, with the exception that the last two if constructions in the algorithm for pronouns are reversed reflecting the preference for demonstratives to be discourse deictics (Webber, 1991). 4 Danish Data In this section I shortly describe Danish third person personal and possessive pronouns and demonstrative pronouns. The description focuses on the discourse deictic use of these pronouns based on occurrences in three Danish dialogue collections, Bysoc,7 SL and PID. My description is also based on (Allan et al., 1995). The third person singular personal and possessive pronouns can be found in table 2, while the third person plural personal and possessive pronouns can be found in table 3.s Den, det and de are also used as definite articles (the) and demonstrative determiners (this/that and these/those). In spoken language the demonstratives are always stressed. 9 Den, det, de are demonstratives if followed by the adverbials her and der in which case they correspond to the English this/these and that/those respectively. Furthermore, the demonstratives denne, dette (this) and disse (these) exist. Femjnlne and masculine pronouns generally co-refer 1° with persons, but can also refer to pets as in English. Common gender pronouns refer to common gender nouns which do not denote humans. Common gender nouns denoting humans are neutral as to the sex of the person they refer to. Thus the gender of 7The Bysoc corpus has been collected by researchers at Copenhagen University under "Projekt Bysociolingvistik" (Project Urban Sociolinguistics). SThe Danish reflexivepronouns are used differently than the English ones, see i.a. (Neville, 1998). 9Because I do not have access to phonetic information about the considered dialogues I cannot account for important phenomena such as intonation and prosody, see i.a. (Vallduv~and Engdahl, 1995). 1°From now on I will simply write "refer to".  59  Table 2: Third person singular pronouns  gender feminine masculine COmmOn neuter  subject hun she han he den it det it  object hende her ham him den it det it  reflexive sig herself sig himself sig itself sig itself  possessive hendes hers hans his dens its dets its  pos.refl. si-nJtJne hers si-nftfne his si-n/tfne its si-n/t/ne its  Table 3: Third person plural pronouns  LsubjectI°bJeer Lreexive IP°  I  de they dem them sig themselves deres their/theirs  the referring pronoun corresponds to the sex of the person the noun refers to. Neuter gender pronouns are used to refer to neuter nouns. They can also refer to a few common person nouns in neuter gender, such as barn (child) and menneske (person) if the sex of the person is unknown or irrelevant (syntactic agreement). In case the sex is known or relevant, the appropriate feminine or masculine pronouns are used (semantic agreement). The two cases are illustrated in the following examples: barnet var pd millimeter sd stort d e t skulle v,~re i l,~ngden og i hovedstcrrelsen og... (the child was precisely as high as it ought to be and its head was as big as it ought to and... ) sd ch... jeg kunne gd ud ]or jeg havde mit barnebarn reed pd tre et halvt dr sd..., kunne jeg jo bare holde hami hdnden (so oh... I could leave because I was together with my three and half year old grandchild so..., I could just hold his hand) Both den and det can refer to collective nouns. In this case the choice between the singular den or det and plural de depends on whether the speaker focuses on the collective meaning or on the individuals. Det and in few idiomatic expressions den are also used as expletives.  In Danish the most frequently used discourse deictic is det which corresponds to it, this or that. Other discourse deictics are det her (this) and det der (that). These two deictics can be used in most of the same contexts as det, although there seems to be a preference for using them to refer to several clauses. The neuter demonstrative dette (this) has also a discourse deictic use, but is mostly used in written language. I did not found any occurrences of it in the three dialogue collections. As discourse deictic det refers to an infinitive or a clause, as it is the case in the following examples: At ryge er ]arligt og det er ogsd dyrt (Smoking is dangerous and it is also expensive) A: Du skal rage en blodprcve (You have to take a blood test) B: Hvorffor det? (Why that.*) Det is also used as the subject complement of vmre (be) and blive (become) in answers. A: Blev du ff,~rdig reed opgaven? (Were you done with the task?) B: Ja, det blev jeg (lit. Yes, that was I) (Yes, I was) Det refers to a verb phrase when it is used as the object complement for the verb have (have), gCre (do) and modal verbs as in  60  Alle faldt, men det gjorde jeg ikke (lit. All fell, but that did I not) (All fell, but I did not)  Det refers to a clause in constructions with attitude verbs and other verbs which take clausal complements, such as synes (think), fro (believe) and vide (know), sige (say), hdbe (hope):  A: Det begynder snart at regne. (It will soon begin to rain) B: Det hdber jeg ikke (lit. That hope I not) (I hope not)  In the latter three cases the pronoun det is often topicalized, i.e. it appears before the main verb, in the place that usually is occupied by the subject 11. Det can also refer to more clauses, or to something that can vaguely be inferred from the discourse.  A: barnets .far chin ...  (the baby's father uhm ...)  B: ja  (yes)  A:  havde  alvorlig,  ch. . . spmdbcrnsgulsot da han  blev fcdt  (had serious, uh ... infant icterus  when he was born)  B: ja  (yes)  A: og fik sd ogs~ skirter sit blod ikke  ogs~  (and then he also got a blood  transfusion, didn't he)  B: mmh  A: det havde hans storebror ogs~i  (lit. that had his brother too)  (his brother had it too)  B: ja  (yes)  A: og er blevet hjerneskadet a f d e t  (and he got brain damage from it)  B: ja  11This position is called fundamentfelt (actualization field) by (Diderichsen, 1984 1946).  (yes) A: altsd jeg red ikke om deter noget jeg skal, om deL skal skrives nogen steder eller gCres noget red (so I don't know whether it is something I should do, whether it should be written somewhere or something should be done) In the above example the deictics in the last utterance do not refer to a single clause or predicate, but to the whole family history of icterus. To conclude, Danish deictics are used in more contexts than the English ones. Especially noticeable is the Danish use of discourse deictics in cases where elliptical constructions are normal in English. 12 5 The Adapted ES99-algorithm On the basis of the deictics in the two Danish dialogue corpora, SL and PID I have established the following *I predicates for Danish: constructions where a pronoun is equated with an abstract object, e.g., x er et forslag (x is a suggestion) copula constructions with adjectives which can only be applied to abstract entities, such as x er sandt (x is true), x er usandt (x is untrue), x er rigtigt (x is correct) arguments of verbs which take S'complements, e.g., fro (believe), antage (ass-me), mene (think), sige (say) anaphoric referent in constructions such as x er /ordi du er holdt op reed at ryge (x is because you have stopped smoking) x er pd grund af at duer gravid (x is because you are pregnant) • object of g#re (do) * subject complement with vmre (be) and blive (become) in answers 12I have not included in the descriptioncataphoric deictic pronouns.  61  • object of have (have) if"the verb was not used as a main verb in the previous clause • object of modal verbs The last four predicates are specific for Danish. I have assumed the following *A predicates, which are mainly' translations of the English ones: • constructions where a pronominal referent is equated with a concrete individual referent, such as x er en legemsdel (x is a body part), x er et barn (x is a baby) • copula constructions with adjectives which can only be applied to concrete entities, such as x er rcdt (x is red) • arguments of verbs describing physical contact/stimulation, which cannot be used anaphorically, e.g. spise x (eat x), drikke x (drink x) As Eckert and Strube notice for English, also in Danish there are cases where the contexts of an anaphor can allow both an individual NP and an abstract object. Some examples are copula constructions like x er godt/ddrligt (x is good/bad), and objects of verbs such as elske (love), hade (hate), foretraekke (prefer). To partially accomodate this, I have added the following condition to the algorithm: in the above cases the anaphor is classified as A* incompatible unless the previous clause contain.~ a raising adjective construction in which case it is considered I* incompatible. Consider the fi)llowing two examples: Peter boede iet r~dt hus. Det hadede han. (Peter lived in a red house. He hated it.) Deter dcdsygt at sidde pd et vaskeri. Det hader jeg. (It is boring to be in a laundry. I hate it) In the first example the algorithm chooses et r~dt hus (a red house) as the antecedent  of det, while in the second example the algorithm chooses at sidde pd et vaskeri (being in a laundry) instead of et vaskeri. There are cases, similar to the first example, where it is impossible, without a deeper analysis of the discourse to determine whether an anaphor refers to an individual NP or an abstract object. In the test I have taken into account the metaphorical uses of verbs encoded in a semantic lexicon, the Danish SIMPLE lexicon (Pedersen and Nimb, 2000). From the analysis of anaphors in the considered dialogue collections I found that many individual anaphors refer back to entities which have not been evoked in the immediately preceding utterances (SUs) and thus they would not be on the S-list (the entities which are not evoked in the current SU are removed from the list). Thus I have extended the scope of resolution for all individual anaphors except the neutral singular. If an antecedent to an individual NP cannot be resolved by looking at the actual S-list, the elements on the S-lists for the preceding SUs are considered. 13 6 Evaluation of the Algorithm I have applied the modified ES99-algorithm to three randomly selected SL dialogues (6,305 words) and to one of the dialogues between native Danes recorded in the PID collection (5,367 words). It must be noted that in my test only one annotator (the author) identiffed dialogue acts, classified the anaphors in the dialogues, marked NPs and anaphor antecedents. In (Eckert and Strube, 1999a) these tasks have been accomplished by two annotators. In dividing the three SL dialogues into discourse segments I have mainly used a partition made by two researchers at the University of Copenhagen in an independent project. The discrimination criteria were topic shift and a few linguistic clues. I have then ap- IsI have followed the cache model described in (Walker, 1998). In the present test it was necessary to go back maximally seven SUs to find an antecedent to an individual pronominal anaphor.  62  plied the same discrimination criteria to the dialogue from the PID collection. I have defined dialogue units syntactically following (Eckert and Strube, 1999a). 14 Because it is not always possible to distinguish between den, det, de used as personal or demonstrative pronouns without having access to stress information, I have classified them as personal pronouns unless they are topicalized, or occur in syntactic constructions where demonstratives are normally used. The manual classification of pronouns and demonstratives in the four dialogues can be found in table 4. The results of the individual anaphora resolution algorithm can be found in table 5, while the results of the discourse deictics resolution algorithm are given in table 6. The results obtained are better than those reposed in (Eckert and Strube, 1999a), but I have used more background information than ES99 and extended the scope of resolution for individual anaphors (without this extension the precision of the individual resolution algorithm was of 64.5). Furthermore the Danish deictic det occurs in more contexts than the English it, this and that, thus there are more I* predicates in the Danish version of the algorithm than in the original one. The fact that only one annotator divided the dialogues into SUs may also have influenced the results. The algorithm classifies anaphors and resolves some of them, thus there are two types of error, classification errors and resolution errors. Most of the instances of wrongly classified anaphors are due to the fact that the algorithm classifies vague anaphors as discourse deictics and then resolves the anaphor to a preceding predicate or clause. Few errors are due to the fact, already noticed by ES99, that the defined I* and A* predicates do not contain information about nominals referring to abstract objects. 15 These errors resulted in most cases in resolution errors. Some errors are due to the inability to find 14The dialogue collections have been tagged. 15The semantic lexicon I used did not contain the relevant nominals.  an individual NP antecedent to the pronoun det, when this refers generally to an NP of different gender 16 and to wrongly resolved plural pronouns with complex NP antecedents or with no antecedent. Correctly classified, but wrongly resolved discourse deictics are, i.a., due to the fact that I did not mark in any particular way parenthetical utterances. The latter kind of errors are chaining errors. In table 7 the occurrences of each type of error are reported. 7 Concluding Remarks The adapted ES99-algorithm has been tested on two kinds of dialogue, that have been classifted by one annotator. Although the types of dialogue in the Danish test is quite different from that used by ES99, the results reported in the previous section (6) indicate that the algorithm performs as well for Danish as for Enghsh. Because the use of Danish pronouns, especially those referring to abstract objects, is different from the English one, these results provide an interesting evaluation of the algorithm. As noticed by ES99, adding more lexical knowledge to the algorithm could improve its performance. I also beheve that the contexts of abstract anaphors should be studied in more dialogues, and that more attention should be given to the connection between discourse deictics and the relations that ]in~ pieces of discourse to each other (Webber, 1991; Fraurud, 1992; Asher, 1993; Kehler, 1997). Further work will thus consist in analyzing the occurrences of discourse deictics in both written texts and dialogues and paying additional attention to the relations linking pieces of discourse to each other (i.a. (Hobbs, 1979; Mann and Thompson, 1987; Polanyi, 1988) leThe use of especially generic plural pronouns in Swedish is discussed in (Fraurud, 1992).  63  Table 4: Classification of Pronouns and Demonstratives  A5 AA10 A A l l  Individual Pro  39 43  34  Discourse Deictics Pro 25 16  17  Vague Pro  4  6  0  Inferrable Evoked  
We introduce CST (cross-document slructure theory), a paradigm for multidocument analysis. CST takes into aceount the rhetorical structure of clusters of related textual documents. We present a taxonomy of cross-document relationships. We argue that CST can be the basis for multidocument summarization guided by user preferences for summary length, information provenance, cross-source agreement, and chronological ordering of facts. 
We propose a model where dialog obligations arise from the interplay of social goals and intentions of the participants: when an agent is addressed with a request, t:he agent's decision to commit to the requester's linguistic and domain goals is motivated by a trade-off between the preference for preventing a negative reaction of the requester and the cost of the actions needed to satisfy the goals. 
The learning and self-adaptive capability in dialog systems has become increasingly important with the advances in a wide range of applications. For any application, particularly the one dealing with a technical domain, the system should pay attention to not only the user experience level and dialog goals, but more importantly, the mechanism to adapt the system behavior to the evolving state of the user. This paper describes a methodology that first identifies the user experience level and utility metrics of the goal and sub-goals, then automatically adjusts those parameters based on discourse history and thus directs adaptive dialog management. Introduction A new generation of dialog systems should be viewed as learning systems rather than static models (Jokinen, 2000). Close-world and static approaches have tremendous limitations and often fail when the task becomes complex and the application environment and knowledge changes. Thus, the learning capability of a dialog system has become an important issue. It has been addressed in many different aspects including dynamic construction of mutual knowledge (Andersen et al, 1999), learning of speech acts (Stolcker et al, 1998), learning optimal strategies (Litman et al, 1998; Litman et al, 1999; Walker et al, 1998), collaborative agent in plan recognition (Lesh et al, 1999), etc. This paper addresses the dynamic user modeling and dialog-goal utility measurement to facilitate adaptive dialog behavior.  For any dialog system dealing with a technical domain, such as repair support (Weis, 1997), help-desk support, etc, it is crucial for the system not only to pay attention to the user knowledge and experience level and dialog goals, but more important, to have certain mechanisms that adapt the system behavior in terms of action planning, content selection, and content realization to user cognitive limitations. Dialog strategies and management should be adjusted to the evolving state of the user. Thus a better understanding and modeling of user cognitive process and human perception is desirable. In this paper, we propose a methodology that automatically learns user experience levels based on sub-goal utilities and characteristics observed during the interaction. Those user levels will further feedback to update utility metrics and direct different dialog strategies at each level of dialog management: action planning, content selection and content realization. The Help-Desk is our application domain. This is a work in progress. We have built a prototype system and are currently in the process of evaluation of our methodology and hypotheses. 
In the paper we describe an approach to dialogue management in the agreement negotiation where one of the central roles is attributed to the model of natural human reasoning. The reasoning model consists of the model of human motivational sphere, and of reasoning algorithms. The reasoning model is interacting with the model of communication process. "/'he latter is considered as rational activity where central role play the concepts of communicative strategies and tactics. Introduction Several researches have modelled the process of argument negotiation in cooperative dialogue where one participant makes a proposal to another participant and as the result of negotiation this is accepted or rejected. Chu-Carroll and Carberry (1998) present a cooperative response-generation model as a recursive cycle Propose-Evaluate-Modify. They concentrate on dialogues of information sharing and negotiation. An information sharing dialogue is started, when the agent recognised a turn of his/her partner as a proposal, but does not have enough information to decide whether to accept it or not. A negotiation dialogue is started, when the agent concludes that the proposal is in conflict with his/her beliefs and preferences, i.e. tends to reject it. Heeman and Hirst (1995) model cooperation by the cycle Present-Judge-Refashion. They use two levels of modelling - planning and cooperation. On the first level utterances are generated and interpreted, on the second level  the cooperation of agents is modelled, relating it to agent's mental states and planning processes. The Shared Plans cooperation model deals with planning processes in which participate multiple agents, see Lochbaurn (1998). The model concentrates on group tasks that can be divided into separate, but interacting subtasks, and the central problem is coordination of intentions and goals of partners. Di Eugenio et al. (2000) present a model BalanceProposeDispose: first, the relevant information concerning the task is considered and discussed, then a proposal is made and, lastly, the decision concerning the proposal is made - it is accepted or rejected. In our model we depart from the same type of situation. One agent, A, addresses another agent, B, with the intention that B will carry out an action D. After some negotiation, B agrees or rejects the proposal. In this paper we concentrate on the problems connected with modelling participants as conversation agents who are able to participate in negotiation in the form of natural dialogue dialogue that is carried out in natural language and according to the rules of human communication. Such a dialogue can be considered as rational behaviour which is based on beliefs, wants and intentions of agents, at the same time being restricted by their resources, see Jokinen (1995), Webber (2000). Conversation agent is a kind of intelligent agent - a computer program that is able to communicate with humans as another human being. As it is generally accepted, in a model of conversation agent it is necessary to represent its cognitive states as well as cognitive processes.  102  One of the most well-known models of this type is the BDI model, see Allen (1994). Our main point in this paper is that the general concepts of cognitive states and processes used in BDI-type models should be extended in order to include certain factors from human motivational sphere and certain social principles in order to guarantee naturalness of dialogues of the type we are concerned with. This is especially important in connection with the fact that interest in modelling cooperative dialogues where partners are pursuing a common goal has considerably increased in recent years. On the one hand, this is connected with rapid spreading of Internet-based services. On the other hand, the interest in models of full natural dialogue derives from the possibility of building speech interfaces with different knowledge and databases, see Dybkjaer (2000). Both of these developments broaden the concept of naturalness of dialogue considerably and present to it much stronger requirements concerning its empirical adequacy as it has been generally accepted thus far. 
We discuss ways to explore how instructional material needs to be structured to be presented with various degrees of interactivity. We use the TRINDI 1 information state approach to model three different degrees of interactivity and present IMDiS, a small experimental implementation based on the GoDiS dialogue system. 
Intelligent dialogue systems must be able to respond properly to a variety of requests involving knowledge of the dialogue, the task at hand, and the domain. This requires advanced knowledge reasoning performed by various processing modules. We argue that it is important to understand the nature of the various reasoning mechanisms involved and to separate not only, for instance, interpretation, generation, and dialogue management but also domain knowledge and task reasoning. This facilitates portability of the dialogue system to new domains and makes it easier to enhance its capabilities. In this paper we will focus on the dialogue and domain knowledge reasoning components and show how they can cooperate to achieve natural interaction. 
We present an application independent dialogue engine that reasons on application dependent knowledge sources to calculate predictions about how a dialogue might continue. Predictions are language independent and are translated into language dependent structures for recognition and synthesis. Further, we discuss how the predictions account for different kinds of dialogue, e.g., question-answer or mixed initiative. 
This paper describes a dialog helpsystem which advises users in using computer facilities and software applications provided by the Center for Information and Multimedia Studies, Kyoto University. The system employs a knowledge base written in natural language and retrieves a proper knowledge unit by flexible matching of user query with the knowledge base. The system is running since July 1999, received about 2,000 queries for the first seven months, and answered about 40~ of them satisfactory.  come popular, such as START (Katz, 1990), FAQ Finder (Cooper, 1996), and QA Track in TREC 8 (NIST and DARPA, 2000). These systems, however, basically produce one time response, and do not have a conversation with users. This paper proposes a dialogue helpsystem in which natural language knowledge base is not only used for one time response, but also for conducting a conversation. To put it concretely, the system can • ask the user back if there is an unknown word in the user utterance, • interpret the user utterance contextually, and  
This paper describes WI'I; a toolkit for building spoken dialogue systems. WIT features an incremental understanding mechanism that enables robust utterance understanding and realtime responses. WIT's ability to compile domain-dependent system specifications into internal knowledge sources makes building spoken dialogue systems much easier than :it is from scratch. 
The purpose of this paper is twofold. First, we describe some complexity aspects of spoken dialogue. It is shown that, given the internal setting of our dialogue system, it is impossible to test even a small percentage of the theoretically possible utterances in a reasonable amount of time. An even smaller part of possible dialogues can thus be tested. Second, an approach for early testing of the dialogue manager of a dialogue system, without the complete system being put together, is described. 'C 
This paper describes a method of comparing corpora which uses frequency profiling. The method can be used to discover key words in the corpora which differentiate one corpus from another. Using annotated corpora, it can be applied to discover key grammatical or word-sense categories. This can be used as a quick way in to find the differences between the corpora and is shown to have applications in the study of social differentiation in the use of English vocabulary, profiling of learner English and document analysis in the software engineering process. 
WordSmith Tools (Scott, 1998) offers a program for comparing corpora, known as KeyWords. KeyWords compares a word list extracted from what has been called 'the study corpus' (the corpus which the researcher is interested in describing) with a word list made from a reference corpus. The only requirement for a word list to be accepted as reference corpus by the software is that must be larger than the study corpus. one of the most pressing questions with respect to using KeyWords seems to be what would be the ideal size of a reference corpus. The aim of this paper is thus to propose answers to this question. Five English corpora were compared to reference corpora of various sizes (varying from two to 100 times larger than the study corpus). The results indicate that a reference corpus that is five times as large as the study corpus yielded a larger number of keywords than a smaller reference corpus. Corpora larger than five times the size of the study corpus yielded similar amounts of keywords. The implication is that a larger reference corpus is not always better than a smaller one, for WordSmith Tools Keywords analysis, while a reference corpus that is less than five times the size of the study corpus may not be reliable. There seems to be no need for using extremely large reference corpora, given that the number of keywords yielded do not seem to change by using corpora larger than five times the size of the study corpus. Introduction WordSmith Tools (Scott, 1998) offers a program for comparing corpora, known as  KeyWords. This tool has been used in several studies as a means for describing various lexicogrammatical characteristics of different genres (Barbara and Scott, 1999; Batista, 1998; Berber Sardinha, 1995, 1999a, b; Berber Sardinha and Shimazumi, 1998; Bonamin, 1999; Collins and Scott, 1996; Conde, 1999; Dutra, 1999; Freitas, 1997; Fuzetti, 1999; Granger and Tnbble, 1998; Lima-Lopes, 1999; Lopes, 2000; Ramos, 1997; Santos, 1999; Scott, 1997; Silva, 1999; Tribble, 1998). The keywords identified by the program are not necessarily the 'most important words' in the corpus (Scott, 1997), or those that correspond to readers' intuitions as to what the topics of the texts are. It is generally thought that a set of WordSmith Tools keywords indicate 'aboutness' (Phillips, 1989). KeyWords compares a word list extracted from what has been called 'the study corpus' (the corpus which the researcher is interested in describing) with a word list made from a reference corpus. The result is a list of keywords, or words whose frequencies are statistically higher in the study corpus than in the reference corpus. The software also identifies words whose frequencies are statistically lower in the study corpus, which are called 'negative keywords', in contrast to positive keywords, which have higher frequencies in the study corpus. Negative keywords, though, will not be discussed in the present paper. Hence, whenever keyword is mentioned in this paper, it will mean 'positive keyword'. The only requirement for a word list to be accepted as reference corpus by the software is that must be larger than the study corpus. Thus, the composition and length of KeyWord lists can vary according to at least six parameters: • The composition of the study corpus.  • The composition of the reference corpus. • The size of the study corpus;. • The size of the reference corpus. • The statistical test used in the comparison of frequencies (loglikelihood and chi-square are available). • The level of significance (p) used as the 'keyness' benchmark (the cut-off point). Since WordSmith Tools is Windows software, it has appealed to a large audience of applied linguists willing to do corpus-based research, to whom this platform is generally the only one that they know how to use. To them, one of the most pressing questions with respect to using KeyWords seems to be what would be the ideal size of a reference corpus. The aim of this paper is thus to propose answers to this question. 
In this paper we compare two types of corpus, focusing on the lexical mnbiguity of each of them. The first corpns consists mainly of newspaper articles and Hterature excerpts, while the second belc)ngs to the medical domain. To conduct the study, we have used two different disambiguation tools. However, first of all, we must verify the performance of each system in its respective application domain. We then use these systems in order to assess and compare both the general ambiguity rate and the particularities of each domain. (mantitative results show that medical documents are lexically less ambiguous than tmrestrieted documents. Our conclusions show the importance of the application area in the design o f NLP tools. Introduction and background Although some large-scale evaluations carried out on unrestricted texts (Hersh 1998a, SparkJones 1999), and even on medical documents (Hersh 1998b), conclude in a quite critical way about using NLP tools for information retrieval, we believe that such tools are likely to solve some lexical ambiguity issues. We also believe that some special settings -particular to the application area- must be taken into account while developing such NLP tools. Let us recall two major problems while retrieving documents with NLP engines (Salton, 1988): 1-Expansion: the user is generally as interested in retrieving documents with exactly the same words, as in retrieving documents with semanticallyrelated words (synonyms, generics,  specifics...). Thus, a query based on the word liver, should be able to retrieve documents containing words such as hepatic. This expansion process is usually thesaurus-based. The thesaurus can be built manually or automatically (as, for ex~ple, in Nazarenko, 1997). 2-Disambiguation: a search based on tokens may retrieve irrelevant documents since tokens are often lexically ambiguous. Thus, face can refer to a body part, as a noun, or an action, as a verb. Finally, this latter problem may be split into two sub problems. The disambiguafion task can be based on parts-of-speech (POS) or word-sense (WS) information,but the chronological relation is still a discussion within the community. Although, the target of our work (Ruch and al., 1999, Bouillon and al., 2000) is a free-grained semantic disambiguation of medical texts for IR purposes, we believe that the POS disambiguation is an important preliminary step. Therefore this paper focuses on POS tagging, and compares morpho-syntacfic lexical ambiguities (MSLA) in medical texts to MSLA in unrestricted corpora. Although the results of the study conform to preliminary naive expectations, the method is quite originalI. Most of the comparative studies, dedicated to corpora, have addressed the problem by applying metrics on words entities or word pieces (as in studies working with n- I We do not claim to be pioneer in the domain, as others authors (Biber 1998, Folch and al., 2000) axe exploring similar metrics. However, it is interesting to notice that for these authors the adaptationof the NLP tools has rarely been questioned in a technical point-ofview, and in order to feed back the designof NLP systems.  14  gram strings), or on special sets of words (the indexing terms, see Salton, 1988) as in the space-vector model (see Kilgariff, 1996, for a survey of these methods), whereas the present paper attempts to compare corpora at a morphosyntactic (MS) level 
We present two measures for comparing corpora based on infbrmationtheory statistics such as gain ratio as well as simple term-class ~equency counts. We tested the predictions made by these measures about corpus difficulty in two domains - - news and molecular biology - - using the result of two well-used paradigms for NE, decision trees and HMMs and found that gain ratio was the more reliable predictor. 
We explore the differences in verb subeategorization frequencies across several corpora in an effort to obtain stable cross corpus subcategonzation probabilities for use in norming psychological experiments. For the 64 single sense verbs we looked at, subeategorizatlon preferences were remarkably stable between British and American corpora, and between balanced corpora and financial news corpora. Of the verbs that did show differences, these differences were generally found between the balanced corpora and the financial news data. We show that all or nearly all of these shifts in subcategorization are realised via (often subtle) word sense differences. This is an interesting observation in itself, and also suggests that stable cross corpus subcategorization frequencies may be found when verb sense is adequately controlled. Introduction Verb subcategorizafion probabilities play an important role in both computational linguistic applications (e.g. Carroll, Minnen, and Briscoe 1998, Charniak 1997, Collins 1996/1997, Joshi and Srinivas 1994, Kim, Srinivas, and Tmeswell 1997, Stolcke et al. 1997) and psycholinguisfic models of language processing (e.g. Boland 1997, Clifton et al. 1984, Ferreira & McClure 1997, Fodor 1978, Garnsey et al. 1997, Jurafsky 1996, MacDonald 1994, Mitchell & Holmes 1985, Tanenhaus et al. 1990, Trueswell et al. 1993).  Previous research, however, has shown that subcategorization probabilities vary widely in different corpora. Studies such as Merlo (1994), Gibson et al. (1996), and Roland & Jurafsky (1997) have found subcategorization frequency differences between traditional corpus data and data from psychological experiments. Biber (1993) and Biber et al. (1998) have shown that that word frequency, word sense (as defined by collocates), the distribution of synonymous words and the use of syntactic structures varies with corpus genre. Roland & Jurafsky (1998, 2000 in press) showed that there were subcategorization frequency differences between various written and spoken corpora, and furthermore showed that that these subcategorization frequency differences are caused by variation in word sense as well as genre and discourse type differences among the corpora. While the subcategorization probabilities in a computational language model can be adjusted to match a particular corpus, cross corpus differences in such probabilities pose an important problem when using corpora for norming psychological experiments. If each corpus generates a separate set of probabilities, which probabilities are the correct ones to use as a model of human language processing? In an attempt to use corpora to provide norming data for 64 verbs for experimental purposes, we investigate in detail how verb frequencies and verb subcategorization frequencies differ among three corpora: the British National Corpus  28  (BNC), the Wall Street Journal corpus (WSJ), and the Brown Corpus (Brown). For the 64 verbs, we randomly selected a set of sentences from each corpus and hand-coded them for transitivity, passive versus active voice, and whether the selected usage was an instance of the most common sense of the verb. We then ask two questions: Do these verbs have the same subcategorizafion probabilities across corpora, and, when there are differences, what is the cause. If a set of factors causing the differences can be identified and controlled for, then a stable set of cross-corpus probabilities suitable for norming psychological experiments can be generated. While previous work has shown that differences between corpora do exist, and that word sense differences play a large role in realising these differences, much less is known about the effect of other factors on subcategorizafion variation across corpora. For example, are there gross subcategorization differences between British and American English? To what extent does the business-genre nature of the Wall Street Journal corpus affect subcategorization probabilities? Finally, while Roland and Jurafsky (2000 in press) suggested that sense differences played a major role in subcategorization biases, they were only able to test their hypothesis on a small number of verbs. Our eventual goal is an understanding of many levels of verb differences across corpora, including verb frequency, frequency of transitive versus intransitive uses, frequency of other subcategonzafion frames, and frequency of active versus passive use. This paper reports our preliminary results on the first two of these issues. Verb usage was surprisingly unaffected by differences between British and American English. Those differences that did occur seem mostly to be caused by differences in the distribution of verb senses across corpora. The business-genre nature of the Wall Street Journal corpus caused certain verbs to appear more often in particular senses that had a strong effect on its subcategorization frequencies. Even after controlfing for the broad sense of the verb, we found subcategorization differences caused by  the "micro-differences" in sense, including quite specific arguments to the verb.  
This article investigates (a) whether register discrimination can successfully exploit linguistic information reflecting the evolution of a language (such as the diglossia phenomenon of the Modern Greek language) and (b) what kind of linguistic information and which statistical techniques may be employed to distinguish among individual styles within one register. Using clustering techniques and features reflecting the diglossia phenomenon, we have successfully discriminated registers in Modem Greek. However, diglossia information has not been shown sufficient to distinguish among individual styles within one register. Instead, a large number of linguistic features need to be studied with methods such as discriminant analysis in order to obtain a high degree of discrimination accuracy.  
With an increasing number of languages making their way to our desktops everyday via the Internet, researchers have come to realize the lack of linguistic knowledge resources for scarcely represented/studied languages. In an attempt to bootstrap some of the required linguistic resources for some of those languages, this paper presents an unsupervised method for automatic multilingual word sense tagging using parallel corpora. The method is evaluated on the English Brown corpus and its translation into three different languages: French, German and Spanish. A preliminary evaluation of the proposed method yielded results of up to 79% accuracy rate for the English data on 81.8% of the SemCor manually tagged data. Keywords Unsupervised; multilingual; alignments; parallel corpora; word sense tagging 1. Introduction With the term "globalization" becoming the theme of cuxrent political and economic discourse, communications technology exemplified by the World Wide Web OVWW) has become a source of an abundance of languages. Language researchers are faced with an ever so present challenge and excitement of being able to study and process these languages and create the appropriate NLP applications for them. Yet, a major bottleneck for many NLP applications such as machine translation, cross language information retrieval, natural language understanding, etc, is word sense ambiguity. The problem escalates as we deal with languages that  are scarce in processing resources and knowledge bases. The availability of large scale, accurately, sense tagged data should help alleviate the problem. It has been acknowledged that best way to acquire sense tags for words in a corpus is manually, which has proven to be a very expensive and labor intensive endeavor. In an attempt to approximate the human effort, both supervised [Bruce & Weibe, 1994; Lin, 1999;etc.] and unsupervised methods [Resnik 1997; Yarowsky, 1992&1995; etc.] have been proposed to solve the problem automatically. On average supervised methods report higher accuracy rates, but they are faced with the problem of requiring large amounts of sense tagged data as training material. Most of the methods, to date, aim at solving the problem for one language, namely the language with the most available linguistic resources. Moreover, most of the proposed approaches report results on a handful of the data, rendering them solutions for a small scale of the data. Many researchers in the field have looked at language translations as a source for sense distinctions [Dagan & Itai, 1994; Dyvik, 1998; Ide, in press; Resnik & Yarowsky, 1999; etc.]. The idea is that polysemons words in one language can be translated as distinct words in a different language. The problem has always been the availability of large corpora in translation, i.e. parallel corpora. Resnik [1999] proposed a method for facilitating the acquisition of parallel corpora from the WWW. Potentially, we can have parallel corpora in a myriad of languages, yet the downside is the scarcity of linguistic knowledge resources and processing tools for less widely represented/studied languages. Consequently, we decided to bootstrap the process of word sense tagging for both languages in a parallel 
W e examine three differenttypes of sense clustering criteria with an Information Retrieval application in mind: methods based on the wordnet structure (such as generMization, cousins, sisters...);eooccurrence of senses obtained from Serecot; and equivalent translations of senses in other languages via the EuroWordNet InterLingual Index (ILI). W e conclude that a) different N L P applications demand not only different sense granularities but different (possibly overlapped) sense clusterings, b) co-occurrence of senses in Semcor provide strong evidence for Information Retrieval clusters, unlike methods based on wordnet structure and systematic polysemy, e) parallel polysemy in three or more languages via the ILI, besides providing sense clusters for MT and CLIR, is strongly correlated with co-occurring senses in Semcor, and thus can be useful for Information Retrieval as well.  distinguishing different word senses for retrieval, enhancing precision, and identifying synonymic or conceptually related terms, enhancing recall. But not all sense distinctions in a lexical database are meaningful for Information Retrieval. For instance, the following sense distinctions are superfluous in an information retrieval application, as the different senses do not lead to different topics or different kinds of documents: Behaviour 1. Manner of acting or conducting oneselJ 2. (psychology) the aggreaate of the responses or reaction or movements made by an organism in any situation 3. Behavioural attributes Bet 1. The act of gambling 2. The money risked on a gamble Band 8. Instrumentalists not including string players 9. A group of musicians playing popular music for dancing  
Yuji MATSUMOTO ComputationalLinguistic Laboratory Nam Institute of Science and Technology 8916--5, Takayama, Ikoma, Nara, 630-0101 Japan matsu @is.aist-nara.ac.jp  AbsU'act In this paper, we investigate cross language information retrieval (CLIR) for Chinese and Japanese texts utilizing the Han characters - common ideographs used in writing Chinese, Japanese and Korean (CJK) languages. The Unicode encoding scheme, which encodes the superset of Han characters, is used as a common encoding platform to deal with the mulfilingual collection in a uniform manner. We discuss the importance of Han character semantics in document indexing and retrieval of the ideographic languages. We also analyse the baseline results of the cross language information retrieval using the common Han characters appeared in both Chinese and Japanese texts. Keywords: Cross Language Information Retrieval, Multilingual Information Processing, Chinese, Japanese and Korean (CJK) Languages Introduction After the opening of the Cross Language Information Retrieval (CLIR) track in the TREC-6 conference (TREC-1998), several reports have been published on cross language information retrieval in European languages, and sometimes, European languages along with one of the Asian languages (e.g., Chinese, Japanese or Korean). However, no report is found in cross language IR thatfocuses on the Asian languages exclusively. In 1999, Pergamon published a special issue of the journal, Information Processing and Management focusing on Information Retrieval with Asian Languages  (Pergamon-1999). Among the eight papers included in that special issue, only one paper addressed CLIR (Kim et al., 1999). Kim et al. reported on nmltiple Asian language information retrieval (English, Japanese and Korean CLIR) using mulfilingual dictionaries and machine translation techniques (to translate both queries and documents). In TREC, intensive research efforts are made for the European languages, for example, English, Gerrn~, French, Spanish, etc. Historically, these languages share many similar linguistic properties. However, exclusive focus on Asian languages, for example, Chinese, Japanese and Korean (CJK) - which also share significantly similar linguistic properties, has not been given. Enormous amount of CJK information is currently on the Internet. The combined growth rate of the CJK electronic information is also predicted to be growing at a faster rate. Cross language IR focusing on these Asian languages is therefore inevitable. In this paper, we investigate the potential of indexing the semantically correlated Han characters appear in both Chinese and Japanese documents and queries to facilitate a cross language information retrieval. Using Han character oriented document and query vectors, within the framework of the vector space information retrieval, we then evaluate the effectiveness of the cross language IR with respect to their monolingual counterparts. We conclude with a discussion about further research possibilities and potentials of Han character oriented cross language information retrieval for the CJK languages.  19  
This paper describes some preliminary results about Word Domain Disambiguation, a variant of Word Sense Disambignation where words in a text are tagged with a domain label in place of a sense label. The English WoRDNET and its aligned Italian version, MULTIWORDNET, both augmented with domain labels, are used as the main information repositories. A baseline algorithm for Word Domain Disambiguation is presented and then compared with a mutual help disambignation strategy, which takes advantages of the shared senses of parallel bilingual texts. 
This article summarizes work on developing a learning theory account for the major learning and statistics based approaches used in natural language processing. It shows that these approaches can all be explained using a single distribution free inductive principle related to the pac model of learning. Furthermore, they all make predictions using the same simple knowledge representation - a linear representation over a common feature space. This is significant both to explaining the generalization and robustness properties of these methods and to understanding how these methods might be extended to learn from more structured, knowledge intensive examples, as part of a learning centered approach to higher level natural language inferences. 
Broad-coverage grammars tend to be highly ambiguous. When such grammars are used in a restricted domain, it may be desirable to specialize them, in effect trading some coverage for a reduction in ambiguity. Grammar specialization is here given a novel formulation as an optimization problem, in which the search is guided by a global measure combining coverage, ambiguity and grammar size. The method, applicable to any unification grammar with a phrasestructure backbone, is shown to be effective in specializing a broad-coverage LFG for French. 
The prevailing dual-route model of oral reading claims that a lexical route is used for the pronunciation of words and a non-lexical route processes nonwords. Neurological data from patients with acquired dyslexias have been highlighted to support this claim. Models using a lexicon alone are generally held to be incapable of explaining these data. However, by selectively impairing its component parts, it is easily possible to account for phonological and surface dyslexias using a single-route model based upon pronunciation by analogy. 
Morphosyntactic Disambiguation (Part of Speech tagging) is a useful benchmark problem for system comparison because it is typical for a large class of Natural Language Processing (NLP) problems that can be defined as disambiguation in local context. This paper adds to the literature on the systematic and objective evaluation of different methods to automatically learn this type of disambiguation problem. We systematically compare two inductive learning approaches to tagging: MXPOST (based on maximum entropy modeling) and MBT (based on memory-based learning). We investigate the effect of different sources of information on accuracy when comparing the two approaches under the same conditions. Results indicate that earlier observed differences in accuracy can be attributed largely to differences in information sources used, rather than to algorithm bias. 
This paper describes algorithms and software developed to characterise and detect generic intelligent language-like features in an input signal, using natural language learning techniques: looking for characteristic statistical "language-signatures" in test corpora. As a first step towards such species-independent language-detection, we present a suite of programs to analyse digital representations of a range of data, and use the results to extrapolate whether or not there are language-like structures which distinguish this data from other sources, such as music, images, and white noise. Outside our own immediate NLP sphere, generic communication techniques are of particular interest in the astronautical community, where two sessions are dedicated to SETI at their annual International conference with topics ranging from detecting ET technology to the ethics and logistics of message construction (E1liott and Atwell, 1999; Ollongren, 2000; Vakoch, 2000). 
This paper describes a set of comparative experiments, including cross-corpus evaluation, between five alternative algorithms for supervised Word Sense Disambiguation (WSD), namely Naive Bayes, Exemplar-based learning, SNOW, Decision Lists, and Boosting. Two main conclusions can be drawn: 1) The LazyBoosting algorithm outperforms the other four state-of-theart algorithms in terms of accuracy and ability to tune to new domains; 2) The domain dependence of WSD systems seems very strong and suggests that some kind of adaptation or tuning is required for cross-corpus application. 
I describe two methods for incorporating information about the relative positions of bilingual word pairs into a Maximum Entropy/Minimum Divergence translation model. The better of the two achieves over 40% lower test corpus perplexity than an equivalent combination of a trigram language model and the classical IBM translation model 2.  
Article choice can pose difficult problems in applications such as machine translation and automated summarization. In this paper, we investigate the use of corpus data to collect statistical generalizations about article use in English in order to be able to generate articles automatically to supplement a symbolic generator. We use data from the Penn Treebank as input to a memory-based learner (TiMBL 3.0; Daelemans et al., 2000) which predicts whether to generate an article with respect to an English base noun phrase. We discuss competitive results obtained using a variety of lexical, syntactic and semantic features that play an important role in automated article generation. 
We present a novel approach to the problem of overfitting in the training of stochastic models for selecting parses generated by attributevalued grammars. In this approach, statistical features are merged according to the frequency of linguistic elements within the features. The resulting models are more general than the original models, and contain fewer parameters. Empirical results from the task of parse selection suggest that the improvement in performance over repeated iterations of iterative scaling is more reliable with such generalized models than with ungeneralized models. 
Error-correcting output codes (ECOC) have emerged in machine learning as a successful implementation of the idea of distributed classes. Monadic class symbols are replaced by bit strings, which are learned by an ensemble of binary-valued classifiers (dichotomizers). In this study, the idea of ECOC is applied to memory-based language learning with local (knearest neighbor) classifiers. Regression analysis of the experimental results reveals that, in order for ECOC to be successful for language learning, the use of the Modified Value Difference Metric (MVDM) is an important factor, which is explained in terms of population density of the class hyperspace. 
A computational framework is presented which is used to model the process by which human language learners acquire the syntactic component of their native language. The focus is feasibility - - is acquisition possible within a reasonable amount of time and/or with a reasonable amount of work? The approach abstracts away from specific linguistic descriptions in order to make a 'broad-stroke' prediction of an acquisition model's behavior by formalizing factors that contribute to cross-linguistic ambiguity. Discussion centers around an application to Fodor's Structural Trigger's Learner (STL) (1998) 1 and concludes with the proposal that successful computational modeling requires a parallel psycholinguistic investigation of the distribution of ambiguity across the domain of human languages. 
Morphology induction is a subproblem of important tasks like automatic learning of machine-readable dictionaries and grammar induction. Previous morphology induction approaches have relied solely on statistics of hypothesized stems and affixes to choose which affixes to consider legitimate. Relying on stemand-affix statistics rather than semantic knowledge leads to a number of problems, such as the inappropriate use of valid affixes ("ally" stemming to "all"). We introduce a semantic-based algorithm for learning morphology which only proposes affixes when the stem and stem-plusaffix are sufficiently similar semantically. We implement our approach using Latent Semantic Analysis and show that our semantics-only approach provides morphology induction results that rival a current state-of-the-art system. 
An extension to memory-based learning is described in which automatically induced rules are used as binary features. These features have an "active" value when the left-hand side of the underlying rule applies to the instance. The RIPPER rule induction algorithm is adopted for the selection of the underlying rules. The similarity of a memory instance to a new instance is measured by taking the sum of the weights of the matching rules both instances share. We report on experiments that indicate that (i) the method works equally well or better than RIPPER on various language learning and other benchmark datasets; (ii) the method does not necessarily perform better than default memory-based learning, but (iii) when multivalued features are combined with the rulebased features, some slight to significant improvements are observed. 
The Maximum Entropy principle (ME) is an appropriate framework for combining information of a diverse nature from several sources into the same language model. In order to incorporate long-distance information into the ME framework in a language model, a Whole Sentence Maximum Entropy Language Model (WSME) could be used. Until now MonteCarlo Markov Chains (MCMC) sampling techniques has been used to estimate the paramenters of the WSME model. In this paper, we propose the application of another sampling technique: the Perfect Sampling (PS). The experiment has shown a reduction of 30% in the perplexity of the WSME model over the trigram model and a reduction of 2% over the WSME model trained with MCMC. 
In this paper are described experiments on unsupervised learning of the domain lexicon and relevant phrase fragments from a dialog corpus. Suggested approach is based on using domain independent words for chunking and using semantical predictional power of such words for clustering and automatic extraction phrase fragments relevant to dialog topics. 
We present ongoing work on prosody prediction for speech synthesis. This approach considers sentences as tree structures and infers the prosody from a corpus of such structures using machine learning techniques. The prediction is achieved from the prosody of the closest sentence of the corpus through tree similarity measurements, using either the nearest neighbour algorithm or an analogy-based approach. We introduce two different tree structure representations, the tree similarity metrics considered, and then we discuss the different prediction methods. Experiments are currently under process to qualify this approach. 
This paper addresses the issue of the automatic induction of syntactic categories from unannotared corpora. Previous techniques give good results, but fail to cope well with ambiguity or rare words. An algorithm, context distribution clustering (CDC), is presented which can be naturally extended to handle these problems. 
Spam filtering is a text categorization task that shows especial features that make it interesting and difficult. First, the task has been performed traditionally using heuristics from the domain. Second, a cost model is required to avoid misclassification of legitimate messages. We present a comparative evaluation of several machine learning algorithms applied to spam filtering, considering the text of the messages and a set of heuristics for the task. Cost-oriented biasing and evaluation is performed. 
We investigate the usefulness of evolutionary algorithms in three incarnations of the problem of feature relevance assignment in memory-based language processing (MBLP): feature weighting, feature ordering and feature selection. We use a simple genetic algorithm (GA) for this problem on two typical tasks in natural language processing: morphological synthesis and unknown word tagging. We find that GA feature selection always significantly outperforms the MBLP variant without selection and that feature ordering and weighting with CA significantly outperforms a situation where no weighting is used. However, GA selection does not significantly do better than simple iterative feature selection methods, and GA weighting and ordering reach only similar performance as current information-theoretic feature weighting methods. 
We study the problem of identifying phrase structure. We formalize it as the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints, and develop two general approaches for it. The first is a Markovian approach that extends standard HMMs to allow the use of a rich observations structure and of general classifiers to model state-observation dependencies. The second is an extension of constraint satisfaction formalisms. We also develop efficient algorithms under both models and study them experimentally in the context of shallow parsing. 1 
In this paper we describe the construction of a part-of-speech tagger both for medical document retrieval purposes and XP extraction. Therefore we have designed a double system: for retrieval purposes, we rely on a rule-based architecture, called minimal commitment, which is likely to be completed by a data-driven tool (HMM) when full disambiguation is necessary. 
Weighted Probability Distribution Voting (WPDV) is a newly designed machine learning algorithm, for which research is currently aimed at the determination of good weighting schemes. This paper describes a simple yet effective weight determination procedure, which leads to models that can produce competitive results for a number of NLP classification tasks.  
In this paper, we compare three different approaches to build a probabilistic context-free grammar for natural language parsing from a tree bank corpus: 1) a model that simply extracts the rules contained in the corpus and counts the number of occurrences of each rule 2) a model that also stores information about the parent node's category and, 3) a model that estimates the probabilities according to a generalized k-gram scheme with k -- 3. The last one allows for a faster parsing and decreases the perplexity of test samples. 
Treating shallow parsing as part-of-speech tagging yields results comparable with other, more elaborate approaches. Using the CoNLL 2000 training and testing material, our best model had an accuracy of 94.88%, with an overall FB1 score of 91.94%. The individual FB1 scores for NPs were 92.19%, VPs 92.70% and PPs 96.69%. 
This paper proposes an error-driven HMMbased text chunk tagger with context-dependent lexicon. Compared with standard HMM-based tagger, this tagger incorporates more contextual information into a lexical entry. Moreover, an error-driven learning approach is adopted to decrease the memory requirement by keeping only positive lexical entries and makes it possible to further incorporate more context-dependent lexical entries. Finally, memory-based learning is adopted to further improve the performance of the chunk tagger. 
This paper describes our actual and ongoing work in supporting semi-automatic ontology acquisition from a corporate intranet of an insurance company. A comprehensive architecture and a system for semi-automatic ontology acquisition supports processing semi-structured information (e.g. contained in dictionaries) and natural language documents and including existing core ontologies (e.g. GermaNet, WordNet). We present a method for acquiring a application-tailored domain ontology from given heterogeneous intranet sources. 
In this paper we study learning from a logical perspective. We show that there is a strong relationship between a learning strategy, its formal learning framework and its logical representational theory. This relationship enables one to translate learnability results from one theory to another. Moreover if we go from a classical logic theory to a substructural logic theory, we can transform learnability results of logical concepts to results for string languages. In this paper we will demonstrate such a translation by transforming the Valiant learnability result for boolean concepts to a learnability :result for a class of string pattern languages. 
We report work on effectively incorporating linguistic knowledge into grammar induction. We use a highly interactive bottom-up inductive logic programming (ILP) algorithm to learn 'missing' grammar rules from an :incomplete grammar. Using linguistic constraints on, for example, head features and gap threading, reduces the search space to such an extent that, in the small-scale experiments reported here, we can generate and store all candidate grammar rules together with information about their coverage and linguistic properties. This allows an appealingly simple and controlled method for generating linguistically plausible grammar rules. Starting from a base of highly specific rules, we apply least general generalisation and inverse resolution to generate more general rules. Induced rules are ordered, for example by coverage, for easy inspection by the user and at any point, the user can commit to a hypothesised rule and add it to the grammar. Related work in ILP and computational linguistics is discussed. 
In the context of language learning, we address a logical approach to information extraction. The system INTHELEX, used to carry out this task, requires a logic representation of sentences to run the learning algorithm. Hence, the need for parsers to produce structured representations from raw text. This led us to develop a prototypical Italian language parser, as a preprocessor in order to obtain the structured representation of sentences required for the symbolic learner to work. A preliminary experimentation proved that the logic approach to learning from language is able to capture the semantics underlying the kind of sentences that were processed, even if a comparison with classical methods as regards efficiency has still to be done. 
In this paper, we propose an Inductive Logic Programming learning method which aims at automatically extracting special Noun-Verb (NV) pairs from a corpus in order to build up semantic lexicons based on Pustejovsky's Generative Lexicon (GL) principles (Pustejovsky, 1995). In one of the components of this lex- ical model, called the qualia structure, words are described in terms of semantic roles. For example, the relic role indicates the purpose or function of an item (cut for knife), the agentive role its creation mode (build for house), etc. The qualia structure of a noun is mainly made up of verbal associations, encoding relational information. The Inductive Logic Programming learning method that we have developed enables us to automatically extract from a corpus N-V pairs whose elements axe linked by one of the semantic relations defined in the qualia structure in GL, and to distinguish them, in terms of surrounding categorial context from N-V pairs also present in sentences of the corpus but not relevant. This method has been theoretically and empirically validated, on a technical corpus. The N-V pairs that have been extracted will further be used in information retrieval applications for index expansion1. 1This works is funded by the Agence universitaire de la Francophonie (AUF) (Action de recherche partag4e "Acquisition automatique d'dldments du Lex-  Keywords: Lexicon learning, Generative Lexicon, Inductive Logic Programming, Information indexing. 
The purpose of this work is to investigate the process of grammatical acquisition from data. We are using a computational learning systern that is composed of a Universal Grammar with associated parameters, and a learning algorithm, following the Principles and Parameters Theory. The Universal Grammar is implemented as a Unification-Based Generalised Categorial Grammar, embedded in a default inheritance network of lexical types. The learning algorithm receives input from a corpus annotated with logical forms and sets the parameters based on this input. This framework is used as basis to investigate several aspects of language acquisition. In this paper we are concentrating on the acquisition of word order for different learners. The results obtained show the different learners having a similar performance and converging towards the target grammar given the input data available, regardless of their starting points. It also shows how the amount of noise present in the input data affects the speed of convergence of the learners towards the target. 
In Czech corpora compound verb groups are usually tagged in word-by-word manner. As a consequence, some of the morphological tags of particular components of the verb group lose their original meaning. We present a method for automatic recognition of compound verb groups in Czech. From an annotated corpus 126 definite clause grammar rules were constructed. These rules describe all compound verb groups that are frequent in Czech. Using those rules we can find compound verb groups in unannotated texts with the accuracy 93%. Tagging compound verb groups in an annotated corpus exploiting the verb rules is described. Keywords: compound verb groups, chunking, morphosyntactic tagging, inductive logic programming 
We present-some new results for the reading comprehension task described in [3] that improve on the best published results - from 36% in [3] to 41% (the best of the systems described herein). We discuss a variety of techniques that tend to give small improvements, ranging from the fairly simple (give verbs more weight in answer selection) to the fairly complex (use specific techniques for answering specific kinds of questions). 
Audio comprehension tests are designed to help evaluate a listener's understanding of a spoken passage and are frequently a key component of language competency exams. Just as reading comprehension exams are proving useful in evaluating text-based language processing technology, audio comprehension exams can be used to evaluate spoken language processing systems. In this paper we discuss some of the challenges of developing automated systems for taking audio comprehension exams. 
We have developed a rule-based system, Quarc, that can reada short story and find the sentence in the story that best answers a given question. Quarc uses heuristic rules that look for lexical and semantic clues in the question and the story. We have tested Quarc on reading comprehension tests typically given to children in grades 3-6. Overall, Quarc found the correct sentence 40% of the time, which is encouraging given the simplicity of its rules. 
We argue that reading comprehension tests are not particularly suited for the evaluation of NLP systems. Reading comprehension tests are specifically designed to evaluate human reading skills, and these require vast amounts of world knowledge and common-sense reasoning capabilities. Experience has shown that this kind of full-fledged question answering (QA) over texts from a wide range of domains is so difficult for machines as to be far beyond the present state of the art of NLP. To advance the field we propose a much more modest evaluation set:up, viz. Answer Extraction (AE) over texts from highly restricted domains. AE aims at retrieving those sentences from documents that contain the explicit answer to a user query. AE is less ambitious than full-fledged QA but has a number of important advantages over QA. It relies mainly on linguistic knowledge and needs only a very limited amount of world knowledge and few inference rules. However, it requires the solution of a number of key linguistic problems. This makes AE a suitable task to advance NLP techniques in a measurable way. Finally, there is a real demand for working AE systems in techni: cal domains. We outline how evaluation procedures for AE systems over real world domains might look like and discuss their feasibility. 
This paper describes the Question Answering System constructed during a one semester graduatelevel course on Natural Language Processing (NLP). We hypothesized that by using a combination of syntactic and semantic features and machine learning techniques, we could improve the accuracy of question answering on the test set of the Remedia corpus over the reported levels. The approach, although novel, was not entirely successful in the time frame of the course. 
This paper proposes an end-to-end process analysis template with replicable measures to evaluate the filtering performance of a Scan-OCR-MT system. Preliminary results1 across three language-specific FALCon2 systems show that, with one exception, the derived measures consistently yield the same performance ranking: Haitian Creole at the low end, Arabic in the middle, and Spanish at the high end. 
The importance of machine translation (MT) in the stream of text-handling processes has become readily apparent in many current production settings as well as in research programs such as the Translingual Information Detection, Extraction, and Summarization (TIDES) program. The MT Proficiency Scale project has developed a means of baselining the inherent "tolerance" that a text-handling task has for raw MT output, and thus how good the output must be in order to be of use to that task. This method allows for a prediction of how useful a particular system can be in a text-handling process stream, whether in integrated, MTembedded processes, or less integrated userintensive processes. 
A growing trend in Machine Translation (MT) is to view MT as an embedded part of an overall process instead of an end result itself. For the last four years, we have fielded (primarily) Commercial-Off-TheShelf (COTS) MT systems in an operational process. MT has been used to facilitate cross-language information retrieval (IR), topic detection and other, wide-scoped scenarios. These uses caused a fundamental shift in our views about MT - everything from user interface to system evaluation to the basic system structures. This paper presents our lessons learned in developing an MT service for a wide range of user needs. Introduction .-, The foreign language material to be handled by the government is increasingly diverse and problematic. Foreign language processing needs are increasing because of the changing conditions of the world. Traditionally, users could focus on just a few foreign languages and a limited number of sources of foreign language materials. As we begin the 21 ~'century, users of online materials are faced with having to process, utilise and exploit documents that may be in one of many languages or a combination of languages. It is not feasible to expect a given user to know all of the languages related to their topic of research. It is equally unrealistic to expect to have on-demand translators available in every language whenever they are needed. Because of the expanding need, tools are being developed to automate the use of foreign language materials.  Unlike previous views of tools, the current vision for machine translation (MT) is as a small part of a larger, mostly automated process. For many users, this does not mean yet another tool with yet another interface, but a nearly invisible companion that incorporates translation and necessary support technologies. One such system, the Army Research Lab (ARL) FALCON system, combines scanning, optical character recognition (OCR), translation and filtering into a single process. Another view of this is the DARPA Translingual Information Detection, Extraction and Summarisation effort (TIDES). TIDES represents the pinnacle of information access and is a real challenge for MT. MT supports the translingual aspects of the effort and can be viewed as an embedded tool which facilitates other technologies. Finally, the integration of MT into the process for intelligence analysis serves as the basis for the CyberTrans project. For this paper, we will discuss the CyberTrans project, the lessons learned and the supporting technologies necessary for the successful integration of MT into other systems. 
We describe a system which supports English text queries searching for Mandarin Chinese spoken documents. This is one of the first attempts to tightly couple speech recognition with machine translation technologies for cross-media and cross-language retrieval. The Mandarin Chinese news audio are indexed with word and subword units by speech recognition. Translation of these multiscale units can effect cross-language information retrieval. The integrated technologies will be evaluated based on the performance of translingnal speech retrieval. 1. Introduction Massive quantities of audio and multimedia programs are becoming available. For example, in mid-February 2000, www.real.com listed 1432 radio stations, 381 Internet-only broadcasters, and 86 television stations with Internet-accessible content, with 529 broadcasting in languages other than English. Monolingual speech retrieval is now practical, as evidenced by services such as SpeechBot (speechbot.research.compaq.com), and it is clear that there is a potential demand for translingual speech retrieval if effective techniques can be developed. The Mandarin-English Information (MEI) project represents one of the first efforts in that direction. MEI is one of the four projects selected for the Johns Hopkins University (JHU) Summer  
We report on a small study undertaken to demonstrate the feasibility of combining portable information extraction with MT in order to support translingual information access. After describing the proposed system's usage scenario and system design, we describe our investigation of transferring information extraction techniques developed for English to Korean. We conclude with a brief discussion of related MT issues we plan to investigate in future work. 
We describe an approach to Machine Translation of transcribed speech, as found in closed captions. We discuss how the colloquial nature and input format peculiarities of closed captions are dealt with in a pre-processing pipeline that prepares the input for effective processing by a core MT system. In particular, we describe components for proper name recognition and input segmentation. We evaluate the contribution of such modules to the system performance. The described methods have been implemented on an MT system for translating English closed captions to Spanish and Portuguese. 
This paper describes the embedding of a statistical translation system within a text editor to produce TRANSTYPE, a system that watches over the user as he or she types a translation and repeatedly suggests completions for the text already entered. This innovative Embedded Machine Translation system is thus a specialized means of helping produce high quality translations. 
Stochastic finite-state models are efficiently learnable from data, effective for decoding and are associated with a calculus for composing models which allows for tight integration of constraints frora various levels of language processing. In this paper, we present a method for stochastic finite-state machine translation that is trained automatically from pairs of source and target utterances. We use this method to develop models for English-Japanese and Japanese-English translation. We have embedded the Japanese-English translation system in a call routing task of unconstrained speech utterances. We evaluate the efficacy of the translation system :in the context of this application. 
We describe a method of text summarization that produces indicative-informative abstracts / for technical papers. The abstracts are generated by a process of conceptual identification, topic extraction and re-generation. We have carried out an evaluation to assess indicativeness and text acceptability relying on human judgment. The results so far indicate good performance in both tasks when compared with other summarization technologies. 
Discourse markers foreshadow the message thrust of texts and saliently guide their rhetorical structure which are important for content filtering and text abstraction. This paper reports on efforts to automatically identify and classify discourse markers in Chinese texts using heuristic-based and corpus-based data-mining methods, as an integral part of automatic text summarization via rhetorical structure and Discourse Markers. Encouraging results are reported. 
We present a multi-document summarizer, called MEAD, which generates summaries using cluster centroids produced by a topic detection and tracking system. We also describe two new techniques, based on sentence utility and subsumption, which we have applied to the evaluation of both single and multiple document summaries. Finally, we describe two user studies that test our models of multi-document summarization. 
This paper proposes a method for extracting key paragraph for multi-document summarization based on distinction between a topic and a~ event. A topic emd an event are identified using a simple criterion called domain dependency of words. The method was tested on the TDT1 corpus which has been developed by the TDT Pilot Study and the result can be regarded as promising the idea of domain dependency of words effectively employed. 
This paper discusses a text extraction approach to multidocument summarization that builds on single-document  parts. Summaries of the individual documents would help, but are likely to be very similar to each other, unless the summarization system takes into account other  I  summarization methods by using additional, available in-, formation about the document set as a whole and the relationships between the documents. Multi-document  summaries that have already been generated. Multidocument summarization - capable of summarizing either complete documents sets, or single documents in the  !  summarization differs from single in that the issues context of previously summarized ones - are likely to  of compression, speed, redundancy and passage selection are critical in the formation of useful summaries. Our approach addresses these issues by using domain-  be essential in such situations. Ideally, multi-document summaries should contain the key shared relevant information among all the documents only once, plus other  I  independent techniques based mainly on fast, statistical information unique to some of the individual documents  processing, a metric for reducing redundancy and maximizing diversity in the selected passages, and a modular  that are directly relevant to the user's query. Though many of the same techniques used in single-  i  framework to allow easy parameterization for different document summarization can also be used in multi-  genres, corpora characteristics and user requirements. 
commercial  summarization products have appeared in the  market place and developers continue to  explore new summarization areas, few papers  have been user-centered, examining  summarization technology in-use. In this  ~aaper, we show how applied work and the  knowledge gleaned about technology in-use  "can temper theoretical considerations and  •motivate as well as direct development likely  to result in higher return on investment.  2.0 Background The importance of understanding the function a summary serves for users is widely acknowledged, and seminal works defining summary types by functions (Paice, 1990; Sparck-Jones, 1993) are frequently cited by developers. Task orientation defines extrinsic technology assessments, and the research literature on how to assess performance for machine generated summaries in an experimental task scenario has grown ( Brandow et al., 1994; Morris et al., 1999;  Jing et al., 1998; Merlino and Maybury, 1999; Wasson, 1998; Tombros et al., 1998; Firmin and Chrzanowski, 1999; and Mani et al., 1999). An increasing number of research papers on summarization systems now also describe some type of extrinsic evaluative task (e.g. Salton et al., 1999; Strzalkowski et al., 1998). A number of factors (i.e. characteristics of summaries, documents, users, and tasks) have surfaced which have implications for technology use. More " research assessing technology (or any aspect of it) in-use on a user's own data even in a development mode along the lines of McKeown et al. (1998) is needed. While experimentation designs involving subjects performing short term controlled tasks may yield results of statistical •significance, generalizability to the user community is limited. In addition, the level of user support text. summarization systems should provide also continues to be speculative. More interest lies in new areas of inquiry like visualization and browsing techniques (e.g., Boguraev et al., 1998), multi-document summarization ( e.g., McKeown and Radev, 1995), multi-media summarization (e.g., Merlino and Maybury, 1999), summarization  49  I  of documents with graphics (e.g., Futrelle, user adjustment of compression rates) to  I  1998) and multi-lingual summarization (e.g., judge relevance of full documents to their  Cowie, 1998). But systematic user studies on information need. As an information analyst," interface support, applicability of proposed our typical user routinely scans summaries to  I  summarization features, or on the real-world stay current with fields of interest and  use of demonstration and prototype systems or enhance domain knowledge. This scanning even commercial systems have not task is one of many jobs an analyst performs  I  materialized.  to support report writing for customers in  3.0 Overview  other Government agencies. Our goal is to  generate summaries  that accelerate  I  This paper presents a user study of a eliminating or selecting documents without  summarization system and provides insights misleading or causing a user to access the on a number of technical issues relevant to the original text unnecessarily.  I  summarization R&D community that arise in,  The system in this user study is a  the context of use, concerning technology version of the SRA sentence extraction system  performance and user support. We describe described in Aone et al. (1997, 1998, 1999).  initial stages in the insertion of the SRA Users retrieve documents from a database of  summarizer in which (1) a large scale beta multiple text collections of reports and press.  test was conducted, and (2) analysis of tool Documents are generally written in a  usage data, user surveys and observations, and journalistic style and average 2,000 characters  user requirements is leading to system in length. The number of documents in a batch  enhancements and more effective may vary from a few to hundreds. Batches of  summarization technology insertion. In our retrieved texts may be routinely routed to our  user study, we begin with a brief description summary server or uploaded by the user. The  of the task and technology (3.1). We then system is web-.based and provides the  describe the beta test methodology (3.2) and capability to tailor summary output by  :analysis of tool usage data (3.3). We focus on creating multiple summary set-ups. User  what we learned in our user-centered options include:  number of sentences  . approach about how technology performance viewed, summary type applied and sorting,  in a task and user support affect user other information viewed (e.g. title, date),  • acceptance (3.4) and what significant and high frequency document terms and  technology-related modifications resulted named entities viewed. Users can save, print  and what studies are in progress to measure or view full text originals with summaries  tool efficacy, summarization effectiveness, appended. Viewed originals highlight  and the impact of training on tool use (3.5). extracted sentences.  Though work to enhance the text  All system use is voluntary. Our users  summarization system is underway, we focus are customers and, if dissatisfied, may elect to  in this paper on user-centered issues. Our scan data without our technology.  work is predicated on the belief that there is  no substitute for user generated data to guide 3.2 Beta Test Methodology  tool enhancement.  In the fall of 1998, 90+ users were  recruited primarily through an IR system news  3.1 Task and Technology  group and provided access to the SRA system  The task is indicative. Our users rely summarizer to replace their full text review  on machine generated summaries (single process of scanning concatenated files.  document, either generic or query-based, with Procedural (how-to) training was optional, but  50  approximately 70 users opted to receive a oneon-one hands-on demonstration (about fortyfive minutes in length) on texts that the new user had retrieved. The beta testing took place over a six month period. With no stipulation on the length of participation, many users simply tried out the system a limited number of times. Initial feedback gave us a clear picture of the likelihood of continued use. Our relatively low retention rate highlighted the fact that the experimental conditions in previous summary experiments may be misleading and masked factors that do not surface until users use a system in a daily work in a real-world setting.  3.3 Analysis of Tool Usage Data Usage data were collected for all system users and analyzed through web logs." These logs were a record of what users did on their actual work data. For each user, our logs provided a rich source of information: number of summary batches, number of documents in each, whether documents were viewed, and set up features--summary type, summary lines viewed, number of indicator (high frequency signature terms) lines viewed, number of entity (persons, places, organizations-) lines viewed, query terms). Table 1 below illustrates the type of representative data collected, questions of interest, and findings.  Table 1: Questions of Interest, Tool Usage Data, Findings  Questions  Data  Finding  Were documents number of sum- Users routinely accessed our system to read  summarized?  mary events  machine generated summaries.  Did users actually number of current Most users did not appear to fully exploit the flex-  tailor the system? set-ups  ibility of the system. The beta test population had  a median of only two set-up types active.  Did the users select type of summary generic or querybased summaries?  Usage data indicated that about half the population selected generic and the other half querybased summaries. (Note: The default set-up was the generic summarization.)  Is there a difference among summary t3~pes for the number of sentences viewed?  number of sentences viewed by summary types (generic, querybased, lead)  The hypothesis of equal median number of sentences available for viewing sentences was tested. The number of sentences viewed with generic summary type (3) is significantly different from either query-based (5) or lead (6).  Do users choose to use indicators and entities when tailoring browsing capability?  indicator/entity preferences for non-default set-ups (on or off)  Users tended to retain indicator and entity preferences when tailoring capabilities. (But users generallymodified a default set-up in which both preferences have a line viewed.)  II II 51 II  i  Table 1: Questions of Interest, Tool Usage Data, Findings  i  Questions  Data  Finding  Does training make training and tool A chi-squared test for independence between  I  a difference on sys- use data tem use or user profile type? Users  training and use reflected a significant relation-  ship (p value close to 0) i.e., training did impact  I  the user's decision to use the system. However,  were categorized (advanced, intermediate, novice) on  training did not make a difference across the three user profile types. A Fisher Exact test on a 3x2  I  contingency table revealed that the relative num-  the basis of usage features with Harti-  bers of trained and untrained users at the three user profile types were the same (p-value=  I  gan's K-Means  0.1916) i.e., training and type are independent.  clustering algo-  t  rithm.  I  As we began to analyze the data, we e.g., Firmin and Chrzanowski, 1999; Merlino realized that we had only a record of use, but and Maybury, 1999), most studies use  II  were not sure of what motivated the use summary system output as the metric for  patterns. Therefore, the team supplemented evaluation. The question routinely posed tool usage data with an or/-line survey and seems to be "Do summaries save the user  I  one-on-one observations to help us understand time without loss in accuracy?" However, we  and analyze the user behavior. These confirmed observations on the integration of additional data points motivated much of our summarization and retrieval technologies of  I  work described in 3.5. Throughout the six McKeown et al. (1998) and learned that  month cycle we also collected and categorized users are not likely to consider using  user requirements.  summaries as a time saver unless the  I  3.4 Insights on Text Summarization  summaries are efficiently accessed. For our users a tight coupling of retrieval and  I  •3.4.1 Technology Performance  summarization is pre-requisite. Batches  Insight 1: For user acceptance, technology automatically routed to the summary server  •performance must go beyond a good available for user review were preferred over  I  suthmary. It requires an understanding of the those requiring the user to upload files for  users" work practices.  summarization. Users pointed out that the  We learned that many factors in the uploading took more time then they were  l  task environment affect technology willing to spend.  performance and user acceptance.  User needs and their work practices  Underpinning much work in summarization is often constrain how technology is applied.  I  the view that summaries are time savers. Mani For example, McKeown et al. (1998) focused  et al. (1999) report that summaries at a low on the needs of physicians who want to compression rate reduced decision making examine only data for patients with similar  I  time by 40% (categorization) and 50% (ad- characteristics to their own patients, and  hoc) with relevance asessments almost as Wasson (1998) focused on the needs of news accurate as the full text. Although evaluators information customers who want to retrieve  I  acknowledge the role of data presentation ( documents likely to be on-topic. We too  I!  62  I  discovered that the user needs affect their The task of judging relevance with a summary  interest in summarization technology, but (even a machine generated one) instead of the  I  from a more general perspective. Text full text version does not require a user t o REtrieval Conferences (e.g., Harman, 1996) acquire a fundamentally different work  have baselined system performance in terms practice. Yet our system was not apparently  of two types of tasks--routing or ad-h.oc. In sufficiently supporting tool navigation. One of  our environment the ad-hoc users were less the reasons was that our on-line help was not  likely .to want a summary. They simply developed from a user perspective and was  I  wanted an answer to a question and did not rarely accessed. Another was that browse and want to review summaries. If too many view features did not maximize performance.  documents were retrieved, they would simply For example, the interface employed a scroll  I  craft a more effective query.  bar for viewing summaries rather than more  Measuring the efficiency gains with a effective Next Or Previous buttons. Users  real population was quite problematic for frequently asked the same questions, but we  I  technology in-use. We faced a number of were answering them individually. challenges. Note that in experimental Terminology clear to the technologists was  conditions, subjects perform, on full and not understood by users. We also noticed that  I  reduced versions. One challenge was to though there were requirements for baseline non-intrusively the current (non- improvement of summarization quality, many  summary) full text review process. A second requirements were associated with these user  I  was to measure both accuracy and efficiency support issues.  gains for users performing on the job. These  One of the more unexpected findings  challenges were further exacerbated by the was the under-utilization of tailoring features.  I  fact that users in an indicative task primarily The system offered the user many ways to  use a summary to eliminate most documents. tailor summaries to their individual needs, yet  I  They have developed effective skimming and most users simply relied on default set-ups. scanning techniques and are already quite Observations revealed little understanding of  efficient at this task.  the configurable features and how these  I  .  In short, our experience showed that features corresponded to user needs to say  technologists deploying single document nothing of how the algorithm worked. Some  summarization capability are likely be users did not understand the difference  I  constrained by the following factors: •• the ease of technology use  between the two summary types or sorting effects with query-based summary selection.  ° the type of user information need  Non-traditional summary types--indicators  I  • how effective the user performs the task and named entities--did not appear to help  without the technology.  render a relevance judgment. We came to  understand that just because technologists sees  I  3.4.2 User Support  the value to these features does not mean that  Insight 2: Users require more than just a good a user will or that the features, in fact, have  summary. They require the right level of utility.  I  technology support, Although the bulk of the research work  still continues to focus on summarization 3.5 Technology-related Modifications  I  algorithms, we now appreciate the importance 3.5.1 User-centered Changes to Technology of user support to text summarization use. Work Practices  The SRA software was quite robust and fast. I  53 I  I  On technology performance, we scanning time required over a similar time  I  learned that  frame. We are currently analyzing these data.  • seamless integration with an IR system  System in a work environment is  was preferred  considered a good indicator of tool utility, but  I  • users with static queries were more likely we wanted some gauge of summary quality  customers for a summary service  and also anticipated user concerns about an  • gains in efficiency are hard to measure for emerging technology like automatic text  I  a task already efficiently performed in a summarization. We compromised and  real-world situations.  selected a method to measure the  In response, we have established a summary effectiveness of our summaries that serves a  I  service in which retrieval results are directly dual purpose--our users gain confidence in the  routed to our summary server and await the utility of the summaries and we can collect user. We plan to integrate the summarization and measure the effectiveness of the generic  I  tool into the IR system. (Uploading batches summaries for some of our users on their data.  and then submission to the server is still an  We initially piloted and now have  option.) We also abandoned the naive idea incorporated a data collection procedure into  I  that data overload equates to summarization our software. In our on-line training, we  requirements and realized that the technology guide users to explore tool capabilities does not apply to all users. We have more through a series of experiments or tasks. In the  I  effectively selected users by profiling first of these tasks, a user is asked to submit a  characteristics of active ,users (e.g. daily batch for summarization, then for each of five document viewing work practice, document to seven user-selected summaries to record  I  volume, static query use, etc.) and have answers to the question:  prioritized deployment to that population  "Is this document likely to be relevant to  I  which could most benefit from it.  me?"(based on the summary)  In order to demonstrate tool  ~.yes  no  ~summarization efficiency, we needed to Then, the user was directed to open the  I  :baseline full-text review. We considered, but original documents for each of the summaries  . rejected a number of options--user self-report and record answers to the question:  and timing, observations, and even the  "Is the document relevant to me?"  I  • creation of a viewing tool to monitor and  (after reading the original text)  document full text review. Instead, we ba,selined full text scanning through  yes no In a prototype collection effort, we  I  information retrieval logs for a subgroup of asked users to review the first ten documents,  users by tracking per document viewing time but in follow-on interviews the users for a month period. These users submit the recommended review of fewer documents. We  I  same queries daily and view their documents understand the limits this places on  through the IR system browser. For the interpreting our data. Also, the on-line training heaviest system users, 75% of the documents is optional so we are not able to collect these  I  were viewed in under 20 seconds per data for all our users uniformly.  document, but note that users vary widely  Most of the users tested exhibited both  with a tendency to spend a much longer high recall and precision, with six users  I  browse time on a relatively small number of judging relevance correctly for all documents  documents. We then identified a subgroup of (in Table 2 below). The False Negative error these users and attempted to deploy the was high for only one user, while the majority  I  summarizer to this baseline group to compare of-the users exhibited no False Negative I  54 I  errors, a worse error to commit than wasting time viewing irrelevant data, False Positive. Across all the users, 79% of all relevant documents and 81% of the irrelevant documents were accurately categorized by examination of the summary.  Table 2: Relevance Classes by User  User  True False True False Positive Positive Negative Negative  I  5  0  0  0  2  5  0  0  0  3  4  0  0  
We have developed an improved task-based evaluation method of summarization, the accuracy of which is increased by specifying the details of the task including background stories, and by assigning ten subjects per summary sample. The method also serves precision/recall pairs for a variety of situations by introducing multiple levels of relevance assessment. The method is applied to prove phrase-represented summary is most effective to select relevant documents from information retrieval results. Introduction Summaries are often used to select relevant documents from information retrieval results. The goal of summarization for such "indicative" use is to serve fast and accurate judgement. We have developed the concept of the "at-a-glance" summary, and its realization in the Japanese language - "phrase-representation summarizatiola" - to achieve this goal (Ueda, et al. 2000). We have conducted an evaluation experiment to verify the effectiveness of this summarization method. There are two strategies for evaluating summarization systems: intrinsic and extrinsic (Jing, et al. 1998). Intrinsic methods measure a system's quality mainly by comparing the system's output with an "ideal" summary. Extrinsic methods measure a system's performance in a particular task. The aim of the phrase-representation summarization method is fast and accurate judgement in selecting documents in information retrieval. Thus, we adopted a task-based method to evaluate  whether the goal was achieved. Task-based evaluation has recently drawn the attention in thq summarization field, because the assumption that there is only one "ideal" summary is considered to be incorrect, and some experiments on information retrieval were reported (Jing, et al. 1998) (Mani, et al. 1998) (Mochizuki and Okunura 1999). However, there is no standard evaluation method, and we consider that there are some shortcomings in the existing methods. Thus, we have developed an improved evaluation method and carried out a relatively large experiment. In this paper, we first give an overview of the phrase-representation summarization method. We then consider the evaluation method and show the result of an experiment based on the improved method to demonstrate the effectiveness of phrase-representation summarization. 
Summary evaluation measures produce a ranking of all possible extract summaries of a document., Recall-based evaluation measures, which depend on costly human-generated ground truth summaries, produce uncorrelated rankings when ground truth is varied. This paper proposes using sentence-rankbased and content-based measures for evaluating extract summaries, and compares these with recallbased evaluation measures. Content-based measures increase the correlation of rankings induced by synonymous ground truths, and exhibit other desirable properties. 
This paper describes a framework for multidocument summarization which combines three premises: coherent themes can be identified reliably; highly representative themes, running across subsets of the document collection, can function as multi-document summary surrogates; and effective end-use of such themes should be facilitated by a visualization environment which clarifies the relationship between themes and documents. We present algorithms that formalize our framework, describe an implementation, and demonstrate a prototype system and interface. 
We describe a system which automatically generates multimedia briefings from highlevel outlines. The system uses summarization in content selection and creation, and in helping form a coherent narrative for the briefing. The approach does not require a domain knowledge base. 
This paper reports on the development of two spoken language collaborative interface agents built with the Collagen system. It presents sample dialogues with the agents working with email applications and meeting planning applications, and discusses how these applications were created. It also discusses limitations and benefits of this approach.  
This paper accompanies a demo of the GoDiS system. Work on~hi~ system was reported at IJCAI99 (Bohlin et-al.~ 1999). GoDiS is a prototype dialogue system for information-seeking dialogue, capable of accommodating questions and tasks to enable the user to present information in any desired order, without explicitly naming the dialogue task. GoDiS is implemented using the TRINDIKIT software package, which enables implementation of these behaviours in a compact and natural way.  
This paper describes the dialogue module of the Mercury systemewhich has been under development over the past year or two. Mercury provides telephone access to an on-line flight database, and allows users to plan and price itineraries between major airports worldwide. The main focus of this paper is the dialogue control strategy, which is based on a set of ordered rules as a mechanism to manage complex dialogue interactions. The paper also describes the interactions between the dialogue component and the other servers of the system, mediated via a central hub. We evaluated the system on 49 dialogues from users booking real flights, and report on a number of quantitative measures of the dialogue interaction. 
This paper describes NJFun, a real-time spoken dialogue systemthat-provides users with information about things to d~ in New Jersey. NJFun automatically optimizes its dialogue strategy over time, by using a methodology for applying reinforcement learning to a working dialogue system with human users. 
Note: The two flightsdifferonly in the second of three segments of the trip, Mthough the user would only discover this rather unusual coincidence upon asking for detMls about the individual flights. The difficulty in this case arises because the mechanism for generation for a very short list of flights has not yet been subsumed as part of the aggregation mechanism (described below) which would have automaticMly determined that the two flights should be grouped. Excerpt 3: i SYSTEM: I will book this leg. Do you want to continue with another leg? Please say yes or no. 
The two current approaches to language generation, Template-based and rule-based (linguistic) NLG, have limitations when applied to spoken dialogue systems, in part because they were developed for text generation. In this paper, we propose a new corpus-based approach to natural language generation, specifically designed for spoken dialogue systems. Introduction Several general-purpose rule-based generation systems have been developed, some of which are available publicly (cf. Elhadad, 1992). Unfortunately these systems, because of their generality, can be difficult to adapt to small, task-oriented applications. Bateman and Henschel (1999) have described a lower cost and more efficient generation system for a specific application using an automatically customized subgrammar. Busemann and Horacek (1998) describe a system that mixes templates and rulebased generation. This approach takes advantages of templates and rule-based generation as needed by specific sentences or utterances. Stent (1999) has proposed a similar approach for a spoken dialogue system. However, there is still the burden of writing and maintaining grammar rules, and processing time is probably too slow for sentences using grammar rules (only the average time for templates and rule-based sentences combined is reported in Busemann and Horacek, 1998), for use in spoken dialogue systems. Because comparatively less effort is needed, many current dialogue systems use templatebased generation. But there is one obvious  disadvantage: the quality of the output depends entirely on the set of templates. Even in a relatively simple domain, such as travel reservations, the number of templates necessary for reasonable quality can become quite large that maintenance becomes a serious problem. There is an unavoidfible trade-off between the amount of time and effort in creating and maintaining templates and the variety and quality of the output utterances. Given these shortcomings of the above approaches, we developed a corpus-based generation system, in which we model language spoken by domain experts performing the task of interest, and use that model to stochastically generate system utterances. We have applied this technique to sentence realization and content planning, and have incorporated the resulting generation component into a working natural dialogue system (see Figure 1). In this paper, we describe the technique and report the results of two evaluations. We used two corpora in the travel reservations domain to build n-gram language models. One corpus (henceforth, the CMU corpus) consists of 39 dialogues between a travel agent and clients (Eskenazi, et al. 1999).  Surface Realization  Content Planning  Sentence Planning Dialogue Manager  Generation Engine  Figure 1 : Overall Architecture  27  query_arrive_city query_arrive_time query_confirm query_depart_date query_depart_time query_pay_by_card query_preferred_airport query_returndate query_return_time hotel car info hotel_hotel_chain hotel_hotel_info  inform_airport inform_confirm_utterance inform_flight inform_flight_another inform_flight_earlier nform_flight_earliest inform_flight_later inform_flight_latest inform_not_avail inform_num_flights inform_price other  Figure 2 : utterance classes  airline arriveairport arriveSci:ty arrive_date arrive_time car company car price depart airport depart city  depart_date depart_time flight_hum hotel_city hotel_price name num_flights pm price  Figure 3 : word classes Another corpus (henceforth, the SRI corpus) consists of 68 dialogues between a travel agent and users in the SRI community (Kowtko and Price 1989). The utterances in the two corpora were tagged with utterance classes and word classes (see Figure 2 and Figure 3). The CMU corpus was manually tagged, and back-off trigram models built (using Clarkson and Rosenfeld, 1997). These language models were used to automatically tag the SRI corpus; the tags were manually checked. 
As a step toward conversational systems that allow for a more natural human-computer interaction, we rep6r~ on GSG, a system that, while providing a natural-l~nguage interface to a variety of applications, engages in clarification dialogues with the end user through which new semantic mappings are dynamically acquired. GsG exploits task- and language-dependent information but is fully taskand language-independent in its architecture and strategies. 
Dialog mantigement addresses two specific problems: (1) providing a coherent overall structure to interaction that extends beyond the single turn, (2) correctly managing mixedinitiative interaction. We propose a dialog management architecture based on the following elements: handlers that manage interaction focussed on tightly coupled sets of information, a product that reflects mutually agreed-upon information and an agenda that orders the topics relevant to task completion. 
We present an implemented concept-to-speech (CTS) syst@n'~J tl~at offers original proposals for certain couplings-oir dialogue computation with prosodic computation. Specifically, the semantic interpretation, task modeling and dialogue strategy modules in a working spoken dialogue system are used to generate prosodic features to better convey the meaning of system replies. The new CTS system embodies and extends theoretical work on intonational meaning in a more general, robust and rigorous way than earlier approaches, by reflecting compositional aspects of both dialogue and intonation interepretation in an original computational framework for prosodic generation. 
We describe an architecture for spoken dialogue interfaces to semi-autonomous systems that transforms speech~signals through successive representations of linguistic, dialogue, and domain knowledge. Each step produces an output, and a meta-output describing the transformation, with an executable program in a simple scripting language as the final result. The output/meta-output distinction permits perspicuous treatment of diverse tasks such as resolving pronouns, correcting user misconceptions, and optimizing scripts. 
We describe robustness techniques used in the CommandTalk system at: the recognition level, the parsing level, and th~ dia16gue level, and how these were influenced by the lack of domain data. We used interviews with subject matter experts (SME's) to develop a single grammar for recognition, understanding, and generation, thus eliminating the need for a robust parser. We broadened the coverage of the recognition grammar by allowing word insertions and deletions, and we implemented clarification and correction subdialogues to increase robustness at the dialogue level. We discuss the applicability of these techniques to other domains. 
In this paper we outline an interlingualbased procedure for resolving reference and suggest a practical approach to implementing it. We assume a two-stage language analysis system. First, a syntactic analysis of an input text results in a functional structure in which certain cases of pronominal reference are resolved. Second, the f-structure is mapped onto an interlingual representation. As part of this mapping, the reference of the various f-structure elements is resolved resulting in the addition of information to certain existing IL objects (coreference) or in the creation of new IL objects which are added to the domain of discourse (initial reference). For this effort, we adopt Text Meaning Representation for our IL and rely on the ONTOS ontology (Mahesh & Nirenburg, 1995) as a general knowledge base. Since the central barrier to developing such a system today is the incompleteness of the knowledge base, we outline a strategy starting with the implementation of a series of form-based resolution algorithms that are applied directly to the referring expressions of the input text. These are initially supplemented by a knowledge-based resolution procedure which, as the knowledge base grows and the adequacy of the f-structure and IL-representation increases, takes on more and more of the processing load.  We examine the operation of the formbased algorithms on a sample Spanish text and show their limitations. We then demonstrate how an IL-based approach can be used to resolve the problematic cases of reference. This research effort is part of the CREST project at the CRL funded by DARPA 1. 
 W(PeApRr)estehnattaprPoavriadmeseatecroiznecdepAtcutaiol nreRpreepsreensteanttiaotnioonf  di erent types of actions used to animate virtual hu-  man agents in a simulated 3D environment. These  a(kcitnioenmsaitnicv)olavnedcheaxnergteisonofosftfaotrec,ec(hdaynngaems oicf)l.oPcaAtiRons  are hierarchical, parameterized structures that fa-  cilitate both visual and verbal expressions. der to support the animation of the actions,  PInARors-  have to make explicit many details that are often un-  derspeci ed in the language. This detailed level of  representation also provides a suitable pivot repre-  sentation for generation in other natural languages,  i.e., a form of interlingua. We show examples of  how certain divergences in machine translation can  be solved by our approach focusing speci cally on  how verb-framed and satellite-framed languages can  use our representation.  
IF (Interchange Format), the interlingua used by the C-STAR consortium, is a speech-act based interlingua for task-oriented dialogue. IF was designed as a practical interlingua that could strike a balance between expressivity and simplicity. If it is too simple, components of meaning will be lost and coverage of unseen data will be low. On the other hand, if it is too complex, it cannot be used with a high degree of consistency by collaborators on different continents. In this paper, we suggest methods for evaluating the coverage of IF and the consistency with which it was used in the C-STAR consortium. Introduction IF (Interchange Format) is an interlingua used by the C-STAR consortium 1 for task-oriented dialogues. Because it is used in five different countries for six different languages, it had to achieve a careful balance between being expressive ehough and being simple enough to be used consistently. If it was not expressive enough, components of meaning would be lost and coverage of unseen data would be low. On the other hand, if was not simple enough, different system developers would use it inconsistently and the wrong meanings would be translated. IF is described in our previous papers ([PT98, LGLW98, LLW+]). For this paper, we have proposed methods for evaluating the coverage of IF and the degree to which it can be used consistently across C-STAR sites. Coverage was measured by having human IF specialists annotate unseen data. Consistency was measured by two means. The first was inter-coder agreement among IF specialists at Carnegie MellonUniversity and ITC-irst (Centre per la ricerca lhttp://www.c-star.org  scientifica e tecnologica). The second, less direct method, was a cross-site end-to-end evaluation of English-to-Italian translation where the Englishto-IF analysis grammars were written at CMU and IF-to-Italian generation was developed at IRST. If the English and Italian grammar writers did not agree on the meaning of the IF, wrong translations will be produced. In this way, the cross-site evaluation can be an indirect indicator of whether the CMU and IRST IF specialists agreed on the meaning of IF representations. For comparison, we also present within-site end-to-end evaluations of English-to-German, English-to-Japanese, and English-to-IF-to-English, where all of the analysis and generation grammars were written at CMU. The Interchange Format Because we are working with task-oriented dialogues, adequate rendering of the speech act in the target language often overshadows the need for literal translation of the words. IF is therefore based on domain actions (DAs), which consist of on speech acts plus domain-specific concepts. An ex- ample of a DA is give-information+price+room (giving information about the price of a room). DAs are composed from 45 general speech acts (e.g., acknowledge, give-information, accept) and about 96 domain-specific concepts (e.g, price, temporal, room, f l i g h t , a v a i l a b i l i t y ) . In addition to the DA, IF representations can contain arguments such as room-type, destination, and price. There are about 119 argument types. In the following example, the DA consists of a speaker tag (a: for agent), the speechact give-information, and two main concepts, + p r i c e and +room. The DA is followed by a list of arguments: room-type= and price=. The arguments have values that represent-information for the type of room double and the cost repre-  18  Percent Cumulatlve Coverage 15.7 19.8 28.3 26.0 28.0 30.1 31,9 33.7 35.5 37.2 38.8 40.3 41.7 43.2 44.5 45.8 46.9 48.0 49.1 50.1 NA*  Percent 15,7 4.1 3.4 2.7 2.0 2.0 1.9 1.8 1.8 1.7 1.6 l.S 1,4 1.4 1.3 1.3 1.2 1.1 1.1 1.0 ;:;  Count DA  652 172 143 113 85 85 78 75 73 70 66 64 60 60 56 52 48 46 44 42 244  acknowledge affirm thank introduce-self give-lnformation+prtce greeting give-lnformarion+ternporal give-lnformatlon+numeral give-information+ price+room request-in for matio n+ payment give-information+ payment give-inform+features+room give-inform -t-availability+ r o o m accept give-information+personal-data req-act +reserv+ feat ures+room req- verif-give-inforra+ n u m e r a l offer+help apologize request-inform+personal-data no-tag  Figure 1: Coverage of Top20 DAs and No-tag in development data  sented with the complex argument price= which has its own arguments quantity=, currency= and p e r - u n i t = . This IF representation is neutral between sentences that have different verbs, subjects, and objects such as A double room costs 150 dollars a night, The price of a double room is 150 dollars a night, and A double room is 150 dollars a night. ~ AGENT: ''a double room costs $150 a night.'' a:give-information+price+room (room-type=double, price=(quantity=lSO, currency=dollar, per-unit=night) Coverage and Distribution of Dialogue Acts In this section, we address the coverage of IF for task-oriented dialogues about travel planning. We want to know whether a very simple interlingua like IF can have good coverage. We are using a rather subjective measure of coverage: IF experts hand-tagged unseen data with IF representations and counted the percentage of utterances to which no IF could be assigned. (When they tagged the unseen data, they were not told that the IF was being tested for coverage. The tagging was done for system development purposes.) Our end-toend evaluation described in the following sections can be taken as a less subjective measure of cov- 2When we add anaphora resolution, we will need to know whether a verb (cost) or a noun (price) was used. This will be an issue our new project, NESPOLEI (http://nespole. itc.it/).  Percent Cumulative Coverage 30.1 45,8 57,7 62,7 87.6 71.7 75,1 77,9 80.2 82,4 84.4 85.7 66.8 87.8 88.5 89.2 89,8 90,2 90.6 91.0  Percent 80.1 15.7 11.9 5,0 4.9 4.1 3.4 2.7 2.4 2.1 2.0 1.3 I.I 1.0 0.8 0.6 0.6 0.5 0.4 0.4  Count 1250 655 498 209 203 172 143 113 98 89 85 55 44 41 32 27 25 19 15 15  Speech Act glve-lnformation acknowledge request-lnformation request-verification-give-inform request-actlon affirm thank introduce-self offer accept greeting suggest apologize closing negate.give-information delay-action introduce-topic please-wait reject request-suggestlon  Figure 2: Coverage of speech-acts in development data  erage. However, the score of an end-to-end evaluation encompasses grammar coverage problems as well as IF coverage problems. The development portion of the coverage experiment proceeded as follows. Over a period of two years, a database of travel planning dialogues was collected by C-STAR partners in the U.S., Italy, and Korea. The dialogues were role-playing dialogues between a person pretending to be a traveller and a person pretending to be a travel agent. For the English and Italian dialogues, the traveller and agent were talking face-to-face in the same language -- both speaking English or both speaking Italian. The Korean dialogues were also role playing dialogues, but one participant was speaking Korean and the other was speaking English. From these dialogues, only the Korean utterances are included in the database. Each utterance in the database is annotated with an English translation and an IF representation. Table 1 summarizes the amount of data in each language. The English, Italian, and Korean data was used for IF development. The development database contains over 4000 dialogue act units, which are covered by a total of about 542 distinct DAs (346 agent DAs and 278 client DAs). Figures 1 and 2 show the cumulative coverage of the top twenty DA's and speech acts in the development data. Figure 1 also shows the percentage of n o - t a g utterances (the ones we decided not to cover) in the development data. The first column shows the percent of the development data that is covered cumulatively by the DA's or speech acts from the top of the table to the current line. For example, acknowledge and a f f i r m together account for 19.8 percent of the data. The  19  Language(s) D'evelopment Data: English Italian Korean-English Test Data: Japanese-English  Type of Dialogue monolingual monolingual biiingual (only 'Korean utterances are included) bilingual (Japanese and English utterances are included)  Number of DA Units 2698 1142 6069  Table 1: The IF Database  Percent ' Cumulative Percent  Cover~,--= - " 15.6 20.2 23.7 27.0 29.7 32.3 34.6 36.3 38.0 39.5 41.1 42.5 43.8 45.0 46.0 47.0 48.0 48.9 49.8 50.7  4.6 15.6 4,6 3.5 3.4 2.7 2.6 2.3 1.7 1.7 1.6 1.5 1.4 1.3 I.I 1.0 1.0 l.O 1.0 0.9 0.9  Count 263 • - 885 260 200 191 153 147 128 98 95 89 88 82 75 65 59 55 55 55 50 49  DA no-tag acknowledge thank introduce-self affirm apologize greeting closing give-information+personal-data glve-information +t em poraI give-information + price please-wait give-inform+telephone-number give-information+features+room request-inform+personal-data give-infor m ÷ t e m p oral-.{-arrival accept give-inform +availability+ room give-information+price-broom verify request-in form +temporal+arrival  Figure 3: Coverage of Top 20 DAs and No-tag in test data  second column shows the percent of the development data covered by each DA or speech act. The third column shows the number of times each DA or speech act occurs in the development data. The evaluation portion of the coverage experiment was carried out on 124 dialogues (6069 dialogue act units) that were collected at ATR, Japan. One participant in each dialogue was speaking Japanese and the other was speaking English. Both Japanese and English utterances are included in the data. The 124 Japanese-English dialogues were not examined closely by system developers during IF development. After the IF design was finalized and frozen in Summer 1999, the Japanese-English data was tagged with IFs. No further IF development took place at this point except that values for arguments were added. For example, Miyakocould be added as a hotel name, but no new speech acts, concepts, or argument types could be added. Sentences were tagged as no-tag if the IF did not cover them. Figures 3 and 4 show the cumulative cover-  Percent Cumulative Coverage 25.6 41.7 53.6 58.2 62,0 65.5 68.8 72.0 74.8 77.5 80.1 82.4 84.4 86.3 87.9 89.5 90.6 91.5 92.0 92.5  Percent Count  25.6  1454  16.1  916  11.9  677  4.6  260  3.7  213  3.5  200  3.4  191  3.2  181  2.8  159  2.7  153  2.6  147  2.3  130  2.1  117  1.8  104  1.7  94  1.5  88  I.I  65  0.9  50  0.5  30  0.5 . 26  DA give-information acknowledge request-information thank request-verification-give-inform introduce-self affirm request-action accept apologize greeting closing suggest verlfy-give-information offer please-wait negate-glve-lnformation verify negate request-affirmatlon  Figure 4: Coverage of Top 20 SAs in test data  age of the top twenty DAs and speech acts in the Japanese-English data, including the percent of no-tag sentences. Notice that the percentage of no-tag was lower in our test data than in our development data. This is because the role playing instructions for the test data were more restrictive than the role playing instructions for the development data. Figures 1 and 3 show that slightly more of the test data is covered by slightly fewer DAs. Cross-Site Reliability of IF Representations In this section we attempt to measure how reliably IF is used by researchers at different sites. Recall that one of the design criteria of IF was consistency of use by researchers who are separated by oceans. This criterion limits the complexity of IF. Two measures of consistency are used - inter-coder agreement and a cross-site end-to-end evaluation. I n t e r - C o d e r Agreement: Inter-coder agreement is a direct measure of consistency among  20  Percent Agreement  Speech-act  82.14  Dialog-act  65.48  Concept lists 88.00  Argument lists I 85.79  Table 2: Inter-coder Agreement between CMU and IRST  C-STAR partners. We used 84 DA units from the Japanese-English data described above. The 84 DA units consisted of some coherent dialogue fragments and and some isolated sentences. The data was coded at CMU and at IRST. We counted agreement on ~he components of the IF separately. Table 2 shows agreement on speech acts, dialogue acts (speech act plus concepts), concepts, and arguments. The results are reported in Table 2 in terms of percent agreement. Further work might include some other calculation of agreement such as Kappa or precision and recall of the coders against each other. Figure 5 shows a fragment of a dialogue coded by CMU and IRST. The coders disagreed on the IF middle sentence, I'd like a twin room please. One coded it as an acceptance of a twin room, the other coded it as a preference for a twin room. Cross-Site Evaluation: As an approximate and indirect measure of consistency, we have compared intra-site end-to-end evaluation with cross-site end-to-end evaluation. An end-to-end evaluation includes an analyzer, which maps the source language input into IF and a generator, which maps IF into target language sentences. The intra-site evaluation was carried out on English-German, English-Japanese, and English-IF-English translation. The English analyzer and the German, Japanese, a n d English generators were all written at CMU by IF experts who worked closely with each other. The cross-site evaluation was carried out on English-Italian translation, involving an English analyzer written at CMU and an Italian generator written at IRST. The IF experts at CMU and IRST were in occasional contact with each other by email, and met in person two or three times between 1997 and 1999. A number of factors contribute to the success of an inter-site evaluation, just one of which is that the sites used IF consistently with each other. Another factor is that the two sites used similar development data and have approximately the same coverage. If the inter-site evaluation results are about as good as the intra-site results, we can con-  clude that all factors are handled acceptably, including consistency of IF usage. If the inter-site results are worse than the intra-site results, consistency of IF use or some other factor may be to blame. Before conducting this evaluation, we already knew that there was some degree of crosssite consistency in IF usage because we conducted successful inter-continental demos with speech translation and video conferencing in Summer 1999. (The demos and some of the press coverage are reported on the C-STAR web site.) The demos included dialogues in English-Italian, EnglishGerman, English-Japanese, English-Korean, and English-French. At a later date, an Italian-Korean demo was produced with no additional work, thus illustrating the well-cited advantage of an interlingual approach in a multi-lingual situation. The end-to-end evaluation reported here goes beyond the demo situation to include data that was unseen by system developers. Evaluation Data: The Summer 1999 intra-site evaluation was conducted on about 130 utterances from a CMU user study. The traveller was played by a second time user - - someone who had participated in one previous user study, but had no other experience with our MT system. The travel agent was played by a system developer. Both people were speaking English, but they were in different rooms, and their utterances were paraphrased using IF. The end-to-end procedure was that (1) an English utterance was spoken and decoded by the JANUS speech recognizer, (2) the output of the recognizer was parsed into an IF representation, and (3) a different English utterance (supposedly with the same meaning) was generated from the IF representation. The speakers had no other means of communication with each other. In order to evaluate English-German and English-Japanese translation, the IFs of the 130 test sentences were fed into German and Japanese generation components at CMU. The data used in the evaluation was unseen by system developers at the time of the evaluation. For English-Italian translation, the IF representations produced by the English analysis component were sent to IRST to be generated in Italian. Evaluation Scoring: In order to score the evaluation, input and output sentences were compared by bilingual people, or monolingual people in the case of English-IF-English evaluation. A score of ok is assigned if the target language utterance is comprehensible and no components of meaning are deleted, added, or" changed by the translation. A  21  We have singles, and t,ins and also Japanese rooms available on the eleventh. CMU a:give-information+availability+room (room-type=(single ~ twin ~ japanese_style), time=mdll) IRST a:give-in2ormation+availability+room (room-type=(single ~ twin & japanese_style), time=mdll) I'd like a twin room, please. CMU c:accept+features+room (room-typeffitwin) IBST c:give-information+preference+features+room (room-type=twin) A twin room is fourteen thousand yen. CMU a:give-information+price+room (room-type=twin, price=(currency=yen, quantity=f4000)) IRST a:give-in.formation+price+room (room-type=twin, price=(currency=yen, quantity=f4000))  Figure 5: Examples of IF coding from CMU and IRST  .o  Method  I OutPut Language II OK+Perfect Perfect Grader I No. of Graders  
In this paper, we describe the Universal Networking Language, an interlingua to be plugged in a Web environment aiming at allowing for many-to-many information exchange, 'many' here referring to many natural languages. The interlingua is embedded in a Knowledge-Base MT system whose languagedependent modules comprise an encoder, a decoder, and linguistic resources that have been developed by native speakers of each language involved in the project. Issues concerning both the interlingua formalism and its foundational issues are discussed.  1. Introduction The widespread use of the Web and the growing Intemet facilities have sparked enormous interest in improving the ways people use to communicate. In this context multilingual Machine Translation systems become prominent, for they allow for a huge information flow. To date, MT systems have been built under limited conditions, of which we highlight two: i) in general, they mirror one-to-many(languages) or many(languages)to-one approaches, often involving English at the "one" end; ii) communication is reduced to basic information exchange, ignoring richness and flexibility implied by human mind. The first limitation has been seldom overcome, since it requires a robust  environment and research teams that can cope with knowledge of several languages 1, to derive precise automatic language analyzers and synthesizers. The second limitation follows up the first: adding up communicative issues to linguistic processing/modeling makes still harder to overcome MT limitations. In this article, we elaborate on work using an interlingua conceived to overcome the first limitation, i.e., to allow for a many-to-many information exchange environment, which shall be plugged in a nontraditional Internet platform. The goal is to allow interlocutors to entangle communication even if they do not share the same mother tongue or the English Standing, most often, for natural language, or NL.  24  language, unlike MT systems that have just one language at one of their edges. As the main component of a Knowledge-Base MT system (hereafter, KBMT), the interlingua approach has been developed under the Universal Networking Language Project, or simply UNL Project. What makes the interlingua UNL special is its intended use: as an electronic language for networks, it has to allow for high quality2 conversation systems involving many languages. As the main component of a KBMT system, it has to be sufficiently robust to ground research and development (R&D) of the language-specific modules to be attached to the system. It is this latter perspective that is undertaken here: from the viewpoint of R&D, we discuss how broad, or language-independent, the interlingua UNL is, especially focusing on its syntax and coverage. In addition to being consistent and complete to represent meaning, we also consider its sharing by researchers all around the world, which is an important bottleneck of the UNL Project, since information exchange by researchers during R&D brings about the problems introduced by the interlingua UNL itself, concerning both its formalism and foundational issues. Before discussing this topic in Section 5, we present an overview of the UNL Project (Section 2) and describe the main features of the interlingua UNL (Section 3). In Section 4, we describe the UNL system architecture. Hereafter, 'interlingua UNL' will be simply referred to as UNL, the acronym for Universal Networking Language. Also, the viewpoint presented here is that of interlingua users who experience R&D for a given NL, and not of its authors. 2. The UNL Project  The UNL Project3 has been launched by the United Nations University to foster and ease international web communication by means of NLP systems. Its main strength lies on the development of the UNL, as a unique semantic (or meaning) representation that can be interchanged with the various languages to be integrated in the KBMT system. In the UNL Project, plug-in software to encode NL texts onto UNL ones (NL-UNL encoders) and to decode UNL into NL texts (UNL-NL decoders) have been developed by R&D groups in their own native languages. The modules to process Brazilian Portuguese4, for example, have been developed by a team of Portuguese native speakers that comprises linguists, computational linguists, and computer experts. Such packages will be made available in WWW servers and will be accessible by browsing through Internet, thus overcoming the need for people all around the world tO learn the language of their interlocutors. Several linguistic groups have signed to the. Project, namely: the IndoEuropean (Portuguese, Spanish, French, Italian, English, German, Russian, Latvian and Hindi), the Semitic (Arabic), the SinoTibetan (Chinese), the Ural-Altaic (Mongolian), the Malayan-Polynesian (Indonesian), and the Japanese. On the one hand, the main strength of the Project is that knowledgeable specialists address language-dependent issues of their mother tongue, most of which are related to R&D of the encoding and decoding modules and to the specification of the NL-UNL lexicon. On the other hand, this also represents a crucial problem faced by the project participants, for distinct groups may interpret the interlingua specification differently. There is thus the need for a consensus about the UNL formalism,  2 By 'high quality' we mean 'at least allowing for readability and understandability by any user'.  3A description of both, the Project and the UNL itself, can be found in http://www.unl.ias.unu.edu/. 4Hereafter referred to as Portuguese or by its acronym, BP.  25  bringing about an assessment of its coverage, completeness, and consistency, all features that will be discussed shortly.  3. The Universal Networking Language  The UNL is a formal language designed  for rendering automatic multilingual  information exchange. It is intended to be a  cross-linguistic semantic representation of  NL sentence meaning, being the core of the  UNL System, the KBMT system developed  by H. Uchida (1996) at the Institute of  Advanced- Studies, United Nations  University, Tokyo; Japan.  UNL subsumes a tridimensional theory of  (sentence) meaning, whose components are  defined according to one of the following sets  (Martins et al., 1998a): concepts (e.g., "cat",  "sit", "on", or "mat"), concept relations (e.g.,  "agent", "place", or "object"), and concept  predicates (e.g., "past" or "definite"). Such  components  are formally  and  correspondingly represented by three  different kinds of entities, namely: Universal  Words (UWs), Relation Labels (RLs), and  Attribute Labels (ALs). According to the  UNL syntax, information conveyed by each  sentence can be represented by a hypergraph  whose nodes represent UWs and whose arcs  represent RLs. To make symbol processing  simpler, hypergraphs are often reduced to  lists of ordered binary relations between  concepts, as it is shown in Figure 1 for the  sentence (1) The cat s a t on the mat. 5  'sit', 'cat', 'on' and 'mat' are UWs; 'agt' (agent), 'pie' (place) and 'obj' (object) are RLs; '@def, '@entry' and '@past' are ALs.  Figure la: UNL hypergraph representation of the English sentence "The cat sat on the mat" agt(sit. @entry.@past,cat.@def) plc(sit.@entry.@past,on) obj(on,mat. @def) Figure lb: UNL linear representation of the English sentence "The cat sat on the mat." UWs are labels for concept-like information, roughly corresponding to the lexical level in the sentence structure. They comprise an open large inventory, virtually capable of denoting every non-compositional meaning to be conveyed by any speaker of any language. For the sake of representation, these atomic semantic contents are associated to English words and expressions, which play the role of semantic labels. However, there is no one-to-one mapping between the English vocabulary and the UNL lexicon, for UNL, as a multilingual representation code, is larger than the English vocabulary. To avoid unnecessary proliferation of the UNL vocabulary and to certify that standards be observed by UNL teams, control over the specification of the UW set is centered at the UNL Center, in Japan. Several semantic relationships hold between UWs, namely synonymy, antonymy, hyponymy, hypemymy and meronymy, which compose the UNL Ontology. Steady semantic valencies (such as agent and object features) can also be represented, forming the UNL Knowledge-Base. Both Ontology and Knowledge-Base aim at constraining the scope of UW labels, whenever ambiguity is to be avoided. The. UNL representation of sentence (1), for example, can be ambiguous  26  in Romance languages, for the translation of 'cat' should make explicit the animal sex: if male, it would be "gato" (Portuguese and Spanish), "gatto" (Italian), "chat" (French), whereas different names would have to be used for the female cat. Instead of having a unique UW 'cat', it is thus quite feasible to have a whole structure in which 'cat' is only the hyper-ordinate option. For the English-UNL association not to undermine the intended universality of the UW inventory, its semantic-orthograpical correspondence has to be considered rather incidental, or even. approximated. It is not always the case that extensions 6 of a UW label and of its corresponding English word coincide. The extension of the English word "mat", for example, does not exactly coincide with the extension of any Portuguese word, although we can find many overlaps between "mat" and, e.g., "capacho" (Portuguese). Portuguese speakers, however, would not say "capacho" for the ornamental dishmat, as would not English speakers use the word "mat" for a fawner (still "capacho" in Portuguese). Since each language categorizes the world in a very idiosyncratic way, it would be misleading to impose a straightforward correspondence between lexical items of two different languages. In UNL, this problem has been overcome by proposing a rather analogic lexicon, instead of a digital one. Although discrete, UWs convey continuous entities, in the sense that semantic gaps between concepts are fulfilled by the UNL Knowledge-Base, as it is shown for the UW 'mat' in Figure 2. Granularity thus plays an important role in UNL lexical organization and brings flexibility into crosslinguistic lexical matching. Cf. (Frege, 1892), extension here is used to establish the relationship between a word and the world, opposed to intension, referring to the relationship between aword and its meaning.  icl Figure 2a: UNL hypergraph partial representation for the meaning denoted by the English word "mat" "mat" "mat(aoj>entity)" "mat(icl>event)" "mat(icl>frame)" "mat(icl>rug)" "mat(icl>state)" "mat(obi>entitv)" Figure 2b: UNL partial linear representation for the meaning denoted by the English word "mat" While lexical representation in UNL comprises a set of universal concepts signaled by UWs, the cross-lexical level involves a set of ordered binary relations between UWs, which are the Relation Labels (RLs). RLs specification are similar to Fillmore's semantic cases (1968), with RLs corresponding to semantic-value relations linking concept-like information. There are currently 44 RLs, but this set has been continuously modified by empirical evidence of lack, or redundancy, of relations. The inventory of RLs can be divided into three parts, according to the functional aspects of the related concepts: ontological, event-like and logical relations. Ontological relations are used as UW constraints in reducing lexical granularity or avoiding ambiguity as shown above, and they help positioning UWs in a UNL lexical structure. Five different labels are used to convey ontological relations: icl (hyponymy), equ (synonymy), ant (antonymy), pof (meronymy), and fld (semantic field).  2"7  UNL depicts sentence meaning as a fact composed by either a simple or a complex event, which is considered here the starting point of a UNL representation, i.e., its minimal complete semantic unit. Event-like relations are assigned by an event external or internal structure, or by both. An event external structure has to do nearly always with time and space boundaries. It can be referred to by a set of RLs signaling the event co-occurrent meanings, such as7 its environment (scn); starting place (pl0, finishing p!ace (pit), or, simply, place (plc); range (fmt); starting time (tmf), finishing time (tmt), or, simply, time (tim); and duration (dur). Action modifiers, such as manner (man) and method (met) can also qualify this structure. An event internal structure is associated to one of the following simple frames: action, activity, movement, state, and process, each expressing different RLs in the event itself, including its actors and circumstances. Event actors are any animate or inanimate character playing any role in events, which can be the main or the coadjutant actors. There can be up to eight actors, signaled by the following RLs: agent (agt), co-agent (cag), object (obj), co-object (cob), object place (opl), beneficiary (ben), partner (ptn) and instrument (ins). They can also be coordinated through the RLs conjunction (and) and disjunction (or), or subordinated to each other by possession (pos), content (cnt), naming (nam), comparison (bas), proportion (per), and modification (mod). They can still be quantified (qua) or qualified by the RLs "property attribution" (aoj) and co-attribution (cao). It is possible to refer to an "initial actor" (src), a "final actor" (gol), or an "intermediary actor" (via). Finally, spatial relationships can also hold between actors: current place (plc), origin (firm), destination (to), and path (via). Besides single events, there can still be complex cross-event 7 RLs names are bracketed.  relationships which express either paralleled events - co-occurrence (coo), conjunction (and), and disjunction (or) - or hierarchically posed events - purpose (pur), reason (rsn), condition (con), and sequence (seq). They can all be referred to as logical relations, since they are often isomorphic to first-order logic predicates. According to the UNL authors, it is possible to codify any sentence written in any NL into a corresponding UNL text expressing the sentence meaning through the use of the above RLs. This is still a claim to be verified, since cases of superposition and competition between different RLs have been observed, as it is discussed in Section 5. In addition to UWs and RLs, UNL makes use of predicate-like information, or Attribute Labels (ALs), which are names for event and concept "transformations", in a sense very close to that intended by Chomsky (1957, 1965). They are not explicitly represented in a UNL hypergraph, although they are used to modify its nodes. ALs can convey information about concept intensions and extensions. In the former case, ALs name information about utterers' intensions over either specific parts of a sentence (focus, topic, emphasis, theme) or the whole structure (exclamation, interrogation, invitation, recommendation, obligation, etc.). In the latter case, ALs refer to spatial (definite, indefinite, generic, plural) or temporal (past, present, future)information, or still, temporal external (begin-soon, beginjust, end-soon, end-just) or intemal (perfecfive, progressive, imperfective, iterative) structures. To differentiate ALs from UWs, ALs are attached to UWs by the symbol ".@". The cOncept expressed by the UW 'sit' in "sit. @entry. @past", for example, is taken as the starting point (. @entry) of the corresponding hypergraph and it is to be modified by temporal information (. @past).  28  4. The UNL System The UNL system architecture consists of two main processes, the encoder and decoder, and several linguistic resources,  each group of these corresponding to a NL embedded in the system, as depicted in Figure 3.  ~source I language I  ~U~qLeldaincgtiounaagrey-to-~  Encoder •r I  •s~t~CeNlaLgnragmuamgaer-tJo-~  
Machine translation between any two languages requires the generation of information that is implicit in the source language. In translating from Chinese to English, tense and other temporal information must be inferred from other grammatical and lexical cues. Moreover, Chinese multiple-clause sentences may contain inter-clausal relations (temporal or otherwise) that must be explicit in English (e.g., by means of a discourse marker). Perfective and imperfective grammatical aspect markers can provide cues to temporal structure, but such information is not present in every sentence. We report on a project to use the ]exical aspect features of (a)te]icity reflected in the Lexical Conceptual Structure of the input text to suggest tense and discourse structure in the English translation of a Chinese newspaper corpus. 
In this ~paper, we present the Interlingua system ISS to generate the pronominal anaphora into the Spanish and English languages. We also describe the main problems in the pronoun generation into both languages such as zero-subject constructions and number, gender and syntactic differences. Our system improves other proposals presented so far due to the fact that we are able to solve and generate intersentential anaphora, to detect coreference chains and to generate Spanish zero-pronouns into English, issues that are hardly considered by other systems. Finally, we provide outstanding results of our system on unrestricted corpora. Introduction One of the main problems of many commercial Machine Translation (MT) and experimental systems is that they do not carry out a correct pronominal anaphora generation. Solving the anaphora and extracting the antecedent are key issues in a correct generation into the target language. Unfortunately, the majority of MT systems do not deal with anaphora resolution and their successful operation usually does not go beyond the sentence level. In this paper, we present a complete approach that allows pronoun resolution and generation into the target language. Our approach works on unrestricted texts unlike other systems, like the KANT interlingua  system (Leavitt et al. (1994)), that are designed for well-defined domains. Although full parsing of these texts could be applied, we have used partial parsing of the texts due to the unavoidable incompleteness of the grammar. This is a main difference with the majority of the interlingua systems such as the DLT system based on a modification of Esperanto (Witkam (1983)), the Rosetta system which is experimenting with Montague semantics as the basis for an interlingua (Appelo and Landsbergen (1986)), the KANT system, etc. as they use full parsing of the text. After the parsing and solving pronominal anaphora, an interlingua representation of the whole text is obtained. In the interlingua representation no semantic information is used as input, unlike some approaches that have as input semantic information of the constituents (Miyoshi et al. (1997), Castell6n et al. (1998), the DLT system, etc). From this interlingua representation, the generation of anaphora (including intersentential anaphora), the detection of coreference chains of the whole text and the generation of Spanish zero-pronouns into English have been carried out, issues that are hardly considered by other systems. Furthermore, this approach can be used for other different applications, e.g. Information Retrieval, Summarization, etc. The paper is organized as follows: In section 1, the complete approach that includes Analysis, Interlingua and Generation modules will be described. These modules will be explained in detail in the next three sections. In section 5, the Generation module has been evaluated in order  ! This paper has been partly financed by the collaborativeresearch project between Spain and The United Kingdom number HB1998-0068.  42  to measure the efficiency of our proposal. To do so, two experiments have been accomplished: the generation of Spanish zero-pronouns into English (syntactic generation module) and the generation of English pronouns into Spanish ones (morphological generation module). Finally, the conclusions of this work will be presented. 
We propose a method for dealing with semantic complexities occurring in information retrieval systems on the basis of linguistic observations. Our method follows from an analysis indicating that long runs of content words appear in a stopped document cluster, and our observation that these long runs predominately originate from the prepositional phrase and subject complement positions and as such, may be useful predictors of semantic coherence. From this linguistic basis, we test three statistical hypotheses over a small collection of documents from different genre. By coordinating thesaurus semantic categories (SEMCATs) of the long run words to the semantic categories of paragraphs, we conclude that for paragraphs containing both long runs and short runs, the SEMCAT weight of long runs of content words is a strong predictor of the semantic coherence of the paragraph. Introduction One of the fundamental deficiencies of current information retrieval methods is that the words searchers use to construct terms often are not the same as those by which the searched information has been indexed. There are two components to this problem, synonymy and polysemy (Deerwester et. al., 1990). By definition of polysemy, a document containing the search terms or indexed with the search terms is not necessarily relevant. Polysemy contributes  heavily to poor precision. Attempts to deal with the synonymy problem have relied on intellectual or automatic term expansion, or the construction of a thesaurus. Also the ambiguity of natural language causes semantic complexities that result in poor precision. Since queries are mostly formulated as words or phrases in a language, and the expressions of a language are ambiguous in many cases, the system must have ways to disambiguate the query. In order to resolve semantic complexities in information retrieval systems, we designed a method to incorporate semantic information into current IR systems. Our method (1) adopts widely used Semantic Information or Categories, (2) calculates Semantic Weight based on probability, and (3) (for the purpose of verifying the method) performs partial text retrieval based upon Semantic Weight or Coherence to overcome cognitive overload of the human agent. We make two basic assumptions: 1. Matching search terms to semantic categories should improve retrieval precision. 2. Long runs of content words have a linguistic basis for Semantic Weight and can also be verified statistically. 
This paper presents an algorithm for finding systematic polysemous classes in WordNet and similar semantic databases, based on a definition in (Apresjan 1973). The introduction of systematic polysemous classes can reduce the amount of lexical semantic processing, because the number of disambiguation decisions can be restricted more clearly to those cases that involve real ambiguity (homonymy). In many applications, for instance in document categorization, information retrieval, and information extraction, it may be sufficient to know if a given word belongs to a certain class (underspecified sense) rather than to know which of its (related) senses exactly to pick. The approach for finding systematic polysemous classes is based on that of (Buitelaar 1998a, Buitelaar 1998b), while addressing some previous shortcomings. Introduction This paper presents an algorithm for finding systematic polysemous classes in WordNet (Miller et al 1990) and GermaNet (Hamp and Feldweg 1997) -- a semantic database for German similar to WordNet. The introduction of such classes can reduce the amount of lexical semantic processing, because the number of disambiguation decisions can be restricted more clearly to those cases that involve real ambiguity  (homonymy). Different than with homonyms, systematically polysemous words need not always be disambiguated, because such words have several related senses that are shared in a systematic way by a group of similar words. In many applications then, for instance in document categorization and other areas of information retrieval, it may be sufficient to know if a given word belongs to this group rather than to know which of its (related) senses exactly to pick. In other words, it will suffice to assign a more coarse grained sense that leaves several related senses underspecified, but which can be further specified on demand 1. The approach for finding systematic polysemous classes is based on that of (Buitelaar 1998a, Buitelaar 1998b), but takes into account some shortcomings as pointed out in (Krymolowski and Roth 1998) (Peters, Peters and Vossen 1998) (Tomuro 1998). Whereas the original approach identified a small set of top-level synsets for grouping together lexical items, i As pointed out in (Wilks 99), earlier work in AI on 'Polaroid Words' (Hirst 87) and 'Word Experts' (Small 81) advocated a similar, incremental approach to senserepresentationand interpretation.In line with this, the CoreLex approach discussed here provides a large scale inventory of systematicallypolysemous lexical items with underspecifiedrepresentationsthat can be incrementallyrefined.  14  the new approach compares lexical items according to all of their synsets on all hierarchy levels. In addition, the new approach is both more flexible and precise by using a clustering algorithm for comparing meaning distributions between lexical items. Whereas the original approach took into account only identical distributions (with additional human intervention to further group together sufficiently similar classes), the clustering approach allows for completely automatic comparisons, relative to certain thresholds, that identify partial overlaps in meaning distributions. 
This paper describes an automatic method for extracting systematic polysemy from a hierarchically organized semantic lexicon (WordNet). Systematic polysemy is a set of word senses that are related in systematic and predictable ways. Our method uses a modification of a tree generalization technique used in (Li and Abe, 1998), and generates a tree-cut, which is a list of clusters that partition a tree. We compare the systematic relations extracted by our automatic method to manually extracted WordNet cousins. 
Word Sense Disambiguation (WSD) is a central task in the area of Natural Language Processing. In the past few years several context-based probabilistic and machine learning methods for WSD have been presented in literature. However, an important area of research that has not been given the attention it deserves is a formal analysis of the parameters affecting the performance of the learning task faced by these systems. Usually performance is estimated by measuring precision and recall of a specific algorithm for specific test sets and environmental conditions. Therefore, a comparison among different learning systems and an objective estimation of the difficulty of the learning task is extremely difficult. In this paper we propose, in the framework of Computational Learning theory, a formal analysis of the relations between accuracy of a context-based WSD system, the complexity of the context representation scheme, and the environmental conditions (e.g. the complexity of language domain and concept inventory) . 
This paper will describe a way to organize the salient objects, their attributes, and relationships between the objects in a given domain. This organization allows us to assign an information value to each collection, and to the domain as a whole, which corresponds to the number of things to "talk about" in the domain. This number gives a measure of semantic complexity; that is, it will correspond to the number of objects, attributes, and relationships in the domain, but not to the level of syntactic diversity allowed when conveying these meanings. Defining a measure of semantic complexity for a dialog system domain will give an insight towards making a complexity measurement standard. With such a standard, natural language programmers can measure the feasibility of making a natural language interface, compare different language processors' ability to handle more and more complex domains, and quantify the abilities of the current state of the art in natural language processors. 
Computational linguists have traditionally sought to model language by finding underlying parameters which govern numerous examples. I describe a different approach which argues that numerous examples themselves, by virtue of their many possible arrangements, provide the only way to specify a sufficiently rich set of "parameters". Essentially I argue for a different relationship between example and parameter. With examples primary, and parameterizafions of them secondary, the real "productions". Rather than representing a redundant complexity, examples should actually be seen as a simplification, a basis for the numerous arrangements of their "parameterizations". Another way of looking at it is to say I argue arrangements of examples, rather than simply revealing underlying parameters, represent in themselves an ignored resource for the modelling of syntactic, and semantic, complexity. 
In this paper we will define this phenomenon and illustrate its impact on interpretation by examining short texts excerpted from the Tipster corpus and other online sources.  1. The Problem 
This paper treats the classification of the semantic functions performed by adnominal constituents in Japanese, where many parts of speech act as adnominal constituents. In order to establish a formal treatment of the semantic roles, the similarities and differences among adnominal constituents, i.e. adjectives and "noun + NO (in English "of + noun")" structures, which have a broad range of semantic functions, are discussed. This paper also proposes an objective method of classifying these constructs using a large amount of linguistic data. The feasibility of this was verified with a selforganizing semantic map based on a neural network model. 
Experimental studies of interactive language use have shed light on the cognitive and interpersonal processes that shape conversation; corpora are the emergent products of these processes. I will survey studies that focus on under-modelled aspects of interactive language use, including the processing of spontaneous speech and disfluencies; metalinguistic displays such as hedges; interactive processes that affect choices of referring expressions; and how communication media shape conversations. The findings suggest some agendas for computational linguistics. Introduction Language is shaped not only by grammar, but also by the cognitive processing of speakers and addressees, and by the medium in which it is used. These forces have, until recently, received little attention, having been originally consigned to "performance" by Chomsky, and considered to be of secondary importance by many others. But as anyone who has listened to a tape of herself lecturing surely knows, spoken language is formally quite different from written language. And as those who have transcribed conversation are excruciatingly aware, interactive, spontaneous speech is especially messy and disfluent. This fact is rarely acknowledged by psychological theories of comprehension and production (although see Brennan & Schober, in press; Clark, 1994, 1997; Fox Tree, 1995). In fact, experimental psycholinguists still make up most of their materials, so that much of what we know about sentence processing is based on a sanitized, ideal form of language that no one actually speaks. But the field of computational linguistics has taken an interesting turn:  Linguists and computational linguists who formerly used made-up sentences are now using naturally- and experimentally-generated corpora on which to base and test their theories. One of the most exciting developments since the early 1990s has been the focus on corpus data. Organized efforts such as LDC and ELRA have assembled large and varied corpora of speech and text, making them widely available to researchers and creators of natural language and speech recognition systems. Finally, Internet usage has generated huge corpora of interactive spontaneous text or "visible conversations" that little resemble edited texts. Of course, ethnographers and sociolinguists who practice conversation analysis (e.g., Sacks, Schegloff, & Jefferson, 1974; Goodwin, 1981) have known for a long time that spontaneous interaction is interesting in its own right, and that although conversation seems messy at first glance, it is actually orderly. Conversation analysts have demonstrated that speakers coordinate with each other such feats as achieving a joint focus of attention, producing closely timed turn exchanges, and finishing each another’s utterances. These demonstrations have been compelling enough to inspire researchers from psychology, linguistics, computer science, and human-computer interaction to turn their attention to naturalistic language data. But it is important to keep in mind that a corpus is, after all, only an artifact—a product that emerges from the processes that occur between and within speakers and addressees. Researchers who analyze the textual records of conversation are only overhearers, and there is ample evidence that overhearers experience a conversation quite differently from addressees and from side participants (Schober & Clark, 1989; Wilkes-Gibbs & Clark, 1992). With a corpus alone, there is no independent evidence of what people actually intend or understand at  different points in a conversation, or why they make the choices they do. Conversation experiments that provide partners with a task to do have much to offer, such as independent measures of communicative success as well as evidence of precisely when one partner is confused or has reached a hypothesis about the other’s beliefs or intentions. Task-oriented corpora in combination with information about how they were generated are important for discourse studies. We still don't know nearly enough about the cognitive and interpersonal processes that underlie spontaneous language use—how speaking and listening are coordinated between individuals as well as within the mind of someone who is switching speaking and listening roles in rapid succession. Hence, determining what information needs to be represented moment by moment in a dialog model, as well as how and when it should be updated and used, is still an open frontier. In this paper I start with an example and identify some distinctive features of spoken language interchanges. Then I describe several experiments aimed at understanding the processes that generate them. I conclude by proposing some desiderata for a dialog model. Two people in search of a perspective To begin, consider the following conversational interchange from a laboratory experiment on referential communication. A director and a matcher who could not see each another were trying to get identical sets of picture cards lined up in the same order. (1) D:ah boy this one ah boy all right it looks kinda likeon the right top there’s a square that looks diagonal M: uh huh D: and you have sort of another like rectangle shape, thelike a triangle, angled, and on the bottom it’s uh I don’t know what that is, glass shaped M: all right I think I got it D: it’s almost like a person kind of in a weird way M: yeah like like a monk praying or something D: right yeah good great M: all right I got it (Stellmann & Brennan, 1993) Several things are apparent from this exchange. First, it contains several disfluencies or  interruptions in fluent speech. The director restarts her first turn twice and her second turn once. She delivers a description in a series of installments, with backchannels from the matcher to confirm them. She seasons her speech with fillers like uh, pauses occasionally, and displays her commitment (or lack thereof) to what she is saying with displays like ah boy this one ah boy and I don’t know what that is. Even though she is the one who knows what the target picture is, it is the matcher who ends up proposing the description that they both end up ratifying: like a monk praying or something. Once the director has ratified this proposal, they have succeeded in establishing a conceptual pact (see Brennan & Clark, 1996). En route, both partners hedged their descriptions liberally, marking them as provisional, pending evidence of acceptance from the other. This example is typical; in fact, 24 pairs of partners who discussed this object ended up synthesizing nearly 24 different but mutually agreed-upon perspectives. Finally, the disfluencies, hedges, and turns would have been distributed quite differently if this conversation had been conducted over a different medium—through instant messaging, or if the partners had had visual contact. Next I will consider the proceses that underlie these aspects of interactive spoken communication. 
§©¨Vªz«P¬®­f¯C°t¯±l²t³°¨´V±l´Vµ¶²z·g¸©¹C°¶­z±ºV¯C°t²­¸ ¬»¨¢¼½´¾±lª&«Pµº#¬®­¿±lÀgª±l¨¢º±ºAÁpµ¶²Â«°t¨¸ ºÃÄ¬»¨P¼VÅµ¶´¯µG­z¬»ªÆ¬®µ¶¨C°¶ÃÇ­ªz²&ÈPÅlªzÈ²Æ±­ÉIÊ5«¢± ´¾±Æªz«Pµº¿¬®­jµ¶²Æ¬®±l¨}ªÆ±º¿ÁËµ¶²cÃ®±°t²&¨¢¬»¨P¼Ìªµ ¯C°t²Æ­z±j°t¨g·Ì­&±Ã®±ÅlªÆ±ºH­ Èv¹P­z±ÆªSµ¶Áeªl°t²Æ¼G±lª ­·}¨gª°¶Ålª¬®Å¾­ ªz²&È¢ÅÆªzÈ²Æ±­É¾§©ª¬®­ÃÄµrÅ°¶ÃË³·G±lª Å°t¨¤«°t¨PºÃ®±¨°¶Ã®­zµÅµ¶´¤¯µG­z¬»ª¬Äµ¶¨C°¶Ã­ª&²zÈPÅl¸ ªzÈ²Æ±­ÉfÍf°t²&ªÆ­5µ¶Á­¯±±Åz«°¶­fÎ5±Ã®Ã°¶­X±l´¤¸ ¹±ºº±ºÏ¬»¨P­ª°¨¢Å±­¤°t²Æ±'¹±¬»¨P¼SÈP­z±ºR­z¬»¸ ´ÈPÃ»ª°t¨P±µ¶ÈP­zÃ»·GÉ¾Ê5«P±¤µ¶Èvª&¯Èvª¬®­°V¯C°t²z¸ ªÆ¬°¶ÃÐ¯°t²­z±Ñ¬»¨RÎ«¢¬®Å&«Ì¬Ò¨P­ ª°t¨PÅ±­¤µ¶Áªz«P± ª°t²¼G±lªI­ª&²zÈPÅlªzÈ²±­e°t²±e´Ñ°t²&Ó}±ºÉ Ô xvi¤tbgg¦wXU¦r¤}uËgÇv Õ×Ö °t²¬Ä±Æª|·½µ¶Á­ ª°tª¬®­ª¬ÄÅt°¶ÃV´¾±lª&«¢µrº­ÏÎ5±l²Æ±Ø¯v²µ¶¸ ¯µG­&±ºÙµ Ö ±l²Úª&«P±Û²±Å±l¨gªÚ·G±°t²­ÜÁpµ¶²ÚÃ®±°t²z¨P¬»¨P¼ ªÆµ7¯²ÆµrºrÈ¢Å±Û°ÝÁÞÈ¢Ã®Ã2¯C°t²Æ­&±Úµ¶ÁÌÁn²Æ±±l¸ßª±lÀgªà­z±l¨¸ ªÆ±Æ¨¢Å±­âáË±¶Éã¼É®³'äµrºá å¥æGæGçèÆ³éc°¶¼G±Æ²z´Ñ°t¨áåtæGæGêèÆ³ ë µGÃ®Ã®¬»¨¢­á å¥æGæGì¶èl³ í¨°tªz¨C°t¯C°t²zÓ}«¢¬áå¥æGæGì¶èÆ³ °t¨Pº î ±lÓ¬»¨P±Ñáå¥æGæGï¶èzèÆÉð§©¨Ì¯C°t²l°¶Ã®ÃÄ±ÃË³°jÃ®µ¶ª¾µ¶ÁñÎ@µ¶²&ÓØ¬®­ ¹±¬»¨P¼Sºµ¶¨P±0µ¶¨2òÆóô¶õÞõÄöt÷Ìøôtùòúnûvüýá Õ ¹¨¢±Æ·G³få¥æGæråGþ ÿ ²Æ±¡±l¨¢­ ªÆ±lª&ªÆ±¶³ å¥æGæ£¢GèÆ³ÚÁpµÅÆÈ¢­&¬»¨¢¼ µ¶¨ ¯C°t²zª¬°¶Ã °t¨C°¶Ã»·­z¬®­hµ¶Á­&±l¨gª±l¨¢Å±­y°tªÌª&«P±¿ÃÄ± Ö ±Ãjµ¶ÁÃ®µrÅ°¶Ã ¯«v²l°¶­z±­E°t¨PºÑªz«P±E²Æ±Ã°tª¬®µ¶¨¢­¹±lª|Î@±±Æ¨cª&«P±l´É î «°¶Ã®Ã®µtÎ ¯°t²­z¬»¨P¼ ª°¶­ Ó­ °t²±Úµ¶Ánª±l¨ ÁËµ¶²z´È¸ Ã°tªÆ±º×°¶­ º¬ Ö ¬®º¬Ò¨P¼ ªz«P±7­z±l¨gª±l¨PÅ±#¬»¨gªÆµ#¨Pµ¶¨¸ µ Ö ±l²Ã°t¯¯¢¬»¨P¼ ­z±¥¤GÈP±l¨PÅ±­ µ¶ÁÜ­·}¨gª°¶ÅÆªÆ¬®Å9­ª&²zÈPÅl¸ ªzÈ²±­³ ° ªl°¶­Ó Åt°¶ÃÄÃ®±º ¦Æó£§û©¨úÞûvütÉ éµG­ª µ¶ÁÌª&«¢±ÚÅ&«gÈ¨Ó¬»¨¢¼ Î@µ¶²&Ó­ «° Ö ±ÚÅµ¶¨PÅ±l¨gªz²l°tªÆ±º µ¶¨à¨¢µ¶È¨¸©¯v«²°¶­&±­Há%ÍX­³ý±¶Éã¼É ë «}Èv²Åz«âáåtæGïGïèÆ³ í¨°t´V­«C°Î °t¨PºSéj°t²ÅlÈ¢­'áå¥æGæGê¶èÆ³ ë °t²ºv¬®± °t¨Pº Í¬Ä±l²Å± á å¥æGæGïGèl³X±±l¨P­ª&²°áå¥æGæGï¶è&èlÉ %ªz«P±l² Å&«gÈ¨vÓr¬Ò¨P¼¿ªl°¶­Ó­c¬Ò¨ Ö µGÃ Ö ±Ì²±ÅµG¼¶¨P¬¬»¨¢¼H­ Èv¹z±Ålª&¸ Ö ±l²z¹2á î  è°t¨Pº Ö ±l²&¹v¸|µ¶¹z±Ålª¤á è¯°¥¬Ò²­á Õ ²Æ¼g°t¸ ´Vµ¶¨ ±lª¨°¶ÃËÉ®³6å¥ææGærþvéÈv¨Pµ£±lªe°¶ÃËÉ®³å¥æGæGæGèÆÉ  Ê5«¢±¿µ¶Èªz¯Èª!µ¶Á­«C°¶Ã®Ã®µÎ ¯C°t²Æ­&±l²Æ­Â¬®­ÌÈP­z±lÁÞÈ¢Ã Î«¢±Æ²Æ± ° Åµ¶´¯PÃ®±lªÆ±ý¯C°t²Æ­z±ýªz²±±¬®­¤¨Pµ¶ª²±¥¤GÈP¬»²Æ±º áp±¶Éã¼É®³6ÅÃ°¶­&­z¬"!PÅ°tªÆ¬®µ¶¨Ç³f­È´¤´ý°t²¬#t°tªÆ¬®µ¶¨Ç³¦¹¢¬®Ã®¬»¨P¼¶È°¶Ã °¶Ã®¬Ä¼¥¨v´V±l¨gªlèlÉhÊ5«P± Åµ¶´¤¯PÃ®±lÀ¬»ª|·Ìµ¶Áeª&²°¶¬»¨P¬»¨¢¼jÁÞÈ¢Ã®Ã ¯°t²­z¬»¨P¼°¶Ã®¼Gµ¶²¬Òª&«´¾­%´ý°·ªz«P±l²Æ±lÁËµ¶²Æ±'¹±° Ö µG¬Äº±º ¬»¨Ï­ÈPÅ&«ªl°¶­Ó­É$%¨ªz«P±Vµ¶ªz«P±l²«°¨¢º³iÁnÈPÃ®Ãf¯C°t²Æ­ ¸ ¬»¨¢¼E«C°¶­fª&«P±¨°¶º Ö °t¨gªl°¶¼G±­Xµ¶Á¢¯v²µºrÈPÅ¬»¨¢¼Åµ¶´¯µG­&¬»¸ ªÆ¬®µ¶¨C°¶Ãf­ ªz²zÈPÅlª&Èv²±­³%!v¨Pº¬»¨¢¼´ÈPÃ»ªÆ¬»¯PÃ®±Ñ­ª&²zÈPÅlªzÈ²Æ±­ ­z¬»´ÈPÃ»ª°t¨P±µ¶ÈP­zÃ»·G³I°t¨PºÈP­z¬»¨P¼­ Èv¹¸|­ª&²zÈPÅlªzÈ²±­VÁpµ¶² ¬»¨vÁË±l²Æ±l¨PÅ±¨°t¹µ¥Èvª¦«P¬®¼¶«P±l²z¸|Ã®± Ö ±ÃPµ¶¨¢±­É'&h«P¬®ÃÄ±ñ­«C°¶Ã»¸ Ã®µÎÌ­·r­ª±l´¾­5ª|·g¯P¬®Å°¶Ã®Ã»·´Ñ°tÓG±ÈP­z±%µ¶Á­z¬»¨P¼GÃ®±l¸ßÎ5µ¶²Æº º¢°tªl°r³C°¤ÁnÈPÃÄÃÇ¯C°t²Æ­&±l²eÅt°t¨ ÈP­z± «¢¬®¼¶«P±l²&¸ Ã®± Ö ±Ã¦­ ªz²&È¢Ål¸ ªzÈ²Æ±­ ¬Ò¨Ì°Åµ¶´¯µG­z¬»ªÆ¬®µ¶¨C°¶Ãf´Ñ°t¨¨¢±Æ²³f±¶Éã¼É®³X°$%Í Ápµ¶²¬®ºv±Æ¨gªÆ¬»Án·r¬Ò¨P¼S°(eÍ³vµ¶²¨°0Åµ¶¨) Èv¨PÅlª¬Äµ¶¨µ¶Á0¨Í­ ¬»¨µ¶²Æº±l²Iªµ¬®º±l¨gª¬»Án· °¤Ã®µ¶¨¢¼G±Æ²1¨Í¦É Õ ¯C°t²zª¬°¶Ãv¯C°t²­z±l²¬®­fª|·}¯¢¬®Å°¶Ã®Ã»·ÑÅµ¶¨PÅ±l²z¨P±ºSµ¶¨PÃ»· ÎI¬»ªz«Ï°V­´Ñ°¶Ã®Ãi¨gÈ´¹±l²µ¶Áª°t²¼G±lªe­ ·g¨gª°¥ÅlªÆ¬®Å0¯C°tª&¸ ªÆ±l²&¨¢­³ °t¨PºÌ´Ñ°·Øªz«P±l²Æ±lÁËµ¶²Æ±j²±¥¤GÈP¬»²Æ±Ã®±­&­Sª&²°¶¬»¨¸ ¬»¨¢¼à¬»¨Ápµ¶²z´Ñ°tªÆ¬®µ¶¨ÇÉ Ê5«C°tªÌÎ@µ¶ÈPÃ®ºÛ¨Pµ¶ªÌ¹±Âªz«P± Å°¶­z±ñÎ«¢±Æ¨± Ö °¥ÃÒÈC°tª¬Ò¨P¼0°eÁnÈPÃÄÃv¯°t²­z±l²µ¶¨S­z±ÃÄ±ÅlªÆ±º ª°t²¼G±lªj¯°tª&ªÆ±l²&¨P­³¹±Å°tÈP­z±Ì¬»ªÆ­cªz²l°¶¬»¨¢¬»¨P¼Â´ý°tª±l¸ ²Æ¬°¶ÃÎ5µ¶È¢Ã®ºÑ­ ªÆ¬®Ã®ÃP¬Ò¨PÅÃ»ÈPº±IÁnÈPÃ®ÃP¯C°t²Æ­&±l¸ßª&²Æ±±­@Ã°t¹±Ã®±º ÎI¬»ªz« µ¥ªz«P±l²¯°tª&ªÆ±l²&¨P­e°¶­5Î5±Ã®ÃpÉ Ê5«¢±S°t¯¯²Æµg°¶Å&«Ï¯²Æ±­&±l¨gª±º2«¢±l²±¶³Xµ¶Á5ªz²l°¶¬»¨°t¹PÃ®± øvôtù¥2Þúßô¶õøvôtùlòÆúÞûvüt³e°tª&ªÆ±l´¤¯vª­SªµÏ²±ºrÈ¢Å± ªz«P±j¼g°t¯ ¹±lª|Î5±±l¨ ­ «°¶Ã®Ã®µtÎ °t¨PºÑÁÞÈ¢Ã®Ã¢¯°t²­z¬»¨P¼ÉX§©ªI¬®­I°t¨±lÀg¸ ªÆ±l¨P­z¬®µ¶¨hµ¶ÁE­«C°¶Ã®Ã®µÎÝ¯°t²­z¬»¨P¼ªµÎ°t²Æº­¤«°t¨PºÃ®¬Ò¨P¼ Åµ¶´¯µG­z¬»ªÆ±%°t¨¢º¤´ÈPÃ»ªÆ¬»¯PÃ®±I¯C°tªzª±l²z¨P­³rÎ«¢¬®Ã®±ñ´ý°¶¬»¨¸ ª°¶¬»¨P¬»¨¢¼'ªz«P±Ã®µÅ°¶Ã¢¨°tª&È²Æ±µ¶Áiªz«P±Eª°¶­ ÓC³C°t¨¢ºý­z¬»´¸ ¯¢Ã®¬®Å¬»ª|·Sµ¶Á¦ª&²°¶¬»¨P¬»¨¢¼'´ý°tª±l²Æ¬°¶ÃËÉ ¨¨P±°t¯v¯²Æµ}°¶Åz«Ìªµj¯C°t²zªÆ¬°¶Ã5¯C°t²Æ­&¬»¨¢¼cÎñ°¶­¤¯²Æ±l¸ ­z±l¨}ªÆ±º#¹}·Aä5ÈPÅ&«v«PµGÃ±lªe°¶ÃËÉIáå¥ææGæèÆ³ÏÎ«PµÛ±lÀg¸ ªÆ±l¨Pº±º°'­«C°¶Ã®Ã®µÎ¸©¯C°t²Æ­z¬»¨P¼'ªÆ±Å&«v¨P¬¤GÈP±ªµ0¯C°t²zª¬°¶Ã ¯°t²­z¬»¨P¼ÉðÊ5«P±µ¶Èvª&¯vÈªVµ¶Á ¨ÍÜ°t¨Pº3eÍàÅz«}Èv¨Óg¸ ¬»¨¢¼EÎ°¶­fÈP­z±ºý°¶­5°t¨¾¬»¨¯ÈvªªÆµ¼¶²l°t´´Ñ°tªÆ¬®Å°¶Ã²±Ã°t¸ ªÆ¬®µ¶¨j¬»¨vÁË±l²Æ±l¨PÅ±¶ÉÊ5«P±0¬»¨Áp±l²Æ±Æ¨¢Å±­e¯²µÅ±­&­¬Ä­EÅ°¶­ ¸ Å°¶º±º³°t¨¢ºV°EÅÃ®±°t²¬Ò´¤¯²Æµ Ö ±l´¾±l¨}ªÎñ°¶­µ¶¹vªl°¶¬»¨¢±º ¹g·Ñ¯C°¶­z­&¬Ò¨P¼¤²Æ±­ÈPÃ»ªÆ­e°¶ÅÆ²ÆµG­z­ñÅ°¶­zÅ°¶º±­É  415©687@9BA¡C DFEGEHCI6PD8Q@9 RS68CTEUDVC@7IWXD8Y`EaDVCcbdW"5©e f D8bgEHCIA¥b@A¡5P7IA¥hpirqtsruPvH7wDV5BhyxCDV5P7cbV££)I  9©A¥W"CAI7d9©6h`WbDV5AIP7IA¡5©bdW68568R7@9aDV768R de9PvHCIQ@9fg££8hRS68CiG5©hHW"5Bekjml1nobpqD8Qd9©WA¡r£A¥hfiPq A¡P7cA¡5©hGW"5©es7@9BA(RAFDV7@vGCcAtbuEUD8Q¥A(7I6vW"5©Q¥Y"vBhHAwb7@Cdv©Q¡x 7dvHC¡D8YW"5HR68CdvDV7IW685y0lzCc6Q¥A¥b@bdW"5©ehe£6A¥b0b@W"v©Y{7¡DV5©A¡x 68v©bdY"q|R68Cwbu7dC@vBQI7dvHCIA¥btDV7D8YY0YA¡r£A¥Yb¥pyRCI68}YA¡R~7w7c6 CIWe89r7F sW{5©Q¥Ah7@9©A¡CIADVCcA5B6vQD8bdQFD8hHA¥b¥pB7d9©Ab7@Cdv©Q¡x 7dvHC¡D8YYA¡r£A¥Y068R'7d9©A68vH7dEHvH7 Wb YW"W{7cA¥hiPqs7@9UDV7168R 7d9©ARAFDV7@vHCIAhbdA¡7  9©WbEaDVEA¡CEHCIAbdA¡5P7cbDV5A¡r7IA¡5©bdW685g68Rs7@9©A D8Ye£68CIW"7@9G 68R4CIePDV685A¡7D8YSg£)pg££p 9©A¡CIAFDVR~7IA¡Cg|x1s'Ip f 9BWQ@99aDV5BhHYA¥bgDV5©hA¡Px E©Y6£W{7cbvQ¥68wE6£bdW"7IW685aD8Y1bu7dC@vBQ¡7@vHCIA¥b|x1sWbvD A¡68C@qPxiUD8b@A¥hD8Ye£68CIW"7d9H7d9aDV7v©bdA¥bCD f xuh©DV7¡D bdAe8A¡5r7IbRS68CYAFDVC@5©W{5©e|Q@9PvG5Hub$7 f 68Cdub f W"7@9 l(sy7¡D8e£b¥pDV5©hkQ¥68i©W"5BA¥bhb@A¥e8A¡5r7Ib68R%rrDVCcW#6gvBb YA¡5©e87d9©bW{5v68CchGAICq7I6(hGA¥QW#hHA f 9©A¡7@9BAICEaDVCd768R7@9©A bdAI5P7IA¡5©Q¥AyDq$iAwDV5kW{5©bu7DV5©Q¥At68RDw7¡DVCIe£AI7 EaDV7dx 7IAICd5yy4mbDvA¡68C@qPxiaD8bdA¥hfD8Ye£68CIW"7d9HpW"7hH6A¥b 5©687DVi©b7@CD8Q¡76Fr£A¡C|7d9©Ah©DV7DfhvHCIW"5©ef7dC¡D8W"5BW"5©eHp iHvG7hvDVu£A¥bh7d9©A5BA¥QA¥bdbcDVCdqDVi©b7@CD8Q¡7cW#685©bhvHCIW"5©e W"5HRA¡CIAI5BQ¥Ahx%R68C1AFD8Q@9|EUDVC@7IWQ¡v©YXDVC W"5Bbu7DV5©Q¥A8 5tA¡r7IA¡5©hGW"5©eh|x1s%p f 9©WQd9WbzDaDV7'A¡7@9©6hUp f A9aDr£Au£A¡EH77d9©Abu7dC@vBQ¡7@vHCIAk68Rh7d9©A|W"5GRSA¡CIA¡5©Q¥A A¥Qd9aDV5©Wb DV5BhW"7cbkY6QFD8Y5aDV7@vGCcA8  9BWbkEaDVx EA¡CwhGA¥b@Q¡CIW"iAbt7@9BAvA¡r7IA¡5©hHA¥h3rrA¡CcbdW685W"5Db@A¥Y"Rx Q¥685P7¡D8W"5BAhyDV5H5©A¡CFp f 9©WY#A|A¥YXDVi68C¡DV7IW"5©ek6£bu7IY"q 685|7d9©AA¡P7cA¡5©bdW685©bw5©A¥A¥hHA¥h|7c69UDV5©hHYAQ¥68EU6£bdW"x 7IW685aD8YQFD8bdA¥bsA¥Q¡7cW#685¡|hHA¥bdQICIW"iA¥bw7d9©AvEaDVCd7cWXD8Y EaDVCIb@A¡CFq¢A¥bv©Y"7IbRS68C1j l¡DV5©hs£l¡DVCIAmEHCIA¥b@A¡5P7IA¥h W"5sA¥Q81¤p1R6£YY6 f A¥hiPq¥Db9©68C@7$hGWb@Q¡v©bdbdW685¦W"5 sA¥Q8©§H ¨ ©kªS«¬U­®~¯)°e±  9©A7dC¡D8W"5©W{5©e3EH9UD8b@ACcA¥Q¥A¥W"r£A¥bDYWbu768R7¡DVCIe£A¡7 7²qPEAbs68RbqP5r7D8Q¡7cW#QEUDV7@7IA¡C@5Bb7d9aDV7v5©A¥A¥h¡7c6iUA WhHA¡5P7IW"i©A¥h³A8oeHp0j l´DV5©hk£lIp%DV5©hD7dCDgW{5©W"5©e Q¥68C@EGv©bW"5 f 9©WQ@97DVCce£A¡7EaDV7d7cA¡Cd5W"5©b7¡DV5BQA¥bDVCcA vDVCdu£A¥h f W"7d9YXDViA¥YA¥h|iHCD8Q@u£A¡7Ibt7 7@9BAI5b7I68CcA¥b CD f x²hBDV7¡Dtbu7DV7cWb7cW#QbmDViU68vG77@9BAbdAhW"5Bbu7DV5©Q¥A¥bW{5|D A¡68C@qhBDV7¡Db7@Cdv©Q¡7@vGCcA8  9BAEaDVCIb@W{5©eEH9aD8bdA CIAQ¥A¥W"rrAbsDkbdA¡5r7IA¡5©Q¥A7I6|iAvEaDVCcbdA¥h¡DV5©hWhHA¡5P7cW"x i©A¥b W"5©b7¡DV5BQA¥bw68R'7@9BAh7¡DVCIe£A¡7EUDV7@7IA¡C@5©b1iaD8bdA¥h|685 7d9©AW"5GRS68CdvDV7IW685sb7c68CIA¥hvW"5y7d9©A1A¡68C@q£q5s7@9©A CIAIyD8W"5©hGAICt68Rq7@9BAEaDVEA¡C f Aw6£b7cY{qD8hH68EH7(7@9©A 7IAICdW{5©6£Y6£e8qs68R'7d9©AaDV71r£A¡CIb@W685  µU¶· ¸$¹ºF»»S¼y½¥¾g¿VÀ¾)Á¹yÂÃÄÀ©ÅÆ0»~Ç sPvHEHE6£bdA7@9BA7dCDgW{5©W"5©ewh©DV7DQ¥685r7D8W"5©b07d9©Ab@A¡5Gx 7IA¡5©Q¥A¥b$DgbvW{5³È0WeH £p1DV5Bh7d9©AkbdA¡5r7IA¡5©Q¥Ak7c6iA EUDVCcbdA¥hWb¥É  ÊrÊ ËrË ÊPÊ ÌPÍ ÎrÍGÏ ÐÑ ÒPÒ ÊPÊ Ó  ÔÕÖ×Ø  ÙÚÛÜ  DV5BhyQ¥685©bdWhHA¡C 7@9BA7¡D8bus68R'iH5BhHW"5©e£lqb Ý r£A¡Cdq(CDV5©e£A6gR f 68CIhHb0W"5t7@9©Ab@A¡5P7cA¡5©Q¥A1Wb0Q¥685Hx bdWhHA¡CIA¥h3DÞcßVàáVâãá£ß8äåhRS68C(iUA¥W"5BekDv£l0  9BAaDV7 x1sD8Ye£68CcW"7d9H 7dCcWA¥bk7I6³bvHEHE68Cd77d9aDV7k9PqPx E687d9©A¥b@W#biPq³yDF7IQ@9BW"5©e¡lhsæbuvGi©b@A¥ç£v©A¡5BQA¥b68R 7d9©AQFDV5©hGWh©DV7IAkDV5BhE6£bdb@W"iBY"qb@68Av68R1W"7cbbvHCdx CI68vH5©hGW"5©esQ¥685r7IA¡P7¡ f W"7d9buvGi©b@A¥ç£v©A¡5BQA¥b68Re£wleb 7d9aDV7DVEGEADVCIA¥hW"57d9©At7@CD8W"5©W"5BeQ¥68CdEHv©b¥  9BA¥b@A bvHi©bdA¥ç£v©A¡5©Q¥A¥bp QFD8YYA¥häâèå¡écpb9©68v©Y#hfQ¥685r7D8W"5æDV7 YAFD8b71685©A£wliU68vG5©h©DVCdqviHCD8Q@u£A¡7F 57d9©AsA¡GDVwE©Y#AkDVi6r£A8pQ¥685Bb@WhGA¡Cv7@9©AvCDV5©e£A 68R f 68CIhHbv§8xkD8byDQFDV5BhHWh©DV7IA£wl  9UDF7ylhs bdA¥ç)vBA¡5©Q¥AshH6A¥bh5©687DVEGEUAFDVCD8btDsQ68E©YA¡7IAv£l W"5¡7dC¡D8W"5BW"5©eHpiHvG7sb@68A|68RW"7Ib7cW#YA¥byhG6fDVEHEAFDVC DV5BhhQFDF5EGCc6FrW#hHAebvHEGEU68Cd7cW{5©emA¡rWhGAI5BQ¥Ag  9BA%7IWYA ê©ë ÎìÌPÍÎPÍGÏ)í EGCc6FrWhGA¥bfD³E6£b@W{7cW"rrAA¡rWhHA¡5BQA iA¥QFDVv©bdA7d9©Av685BY"qfDVEGEUAFDVCDV5©Q¥Av68Rê ÌPÍ³ÎPÍGÏ)í Wb DV7q7@9BAmiA¥e£W"5H5©W"5Be68R%Dw£wl0Gm5v7@9©Aw687@9BA¡C9aDV5©hp ê ÊPÊ ë ÎìPí EHCc6FrWhHA¥bD f AFDVu£A¡CA¡rWhHA¡5©Q¥A1iAQFDVvBb@A ê ÊPÊí DVEGEADVCIb|7 f WQ¥A3iA¡R68CcAD£wlîiHvG7k7@9GCcA¥A 7IW"A¥bvW"5æ687d9©A¡CsEU6£bdW"7IW685©b¥¦4 Q¥Q68CIhHW"5Be£Y"q£pAFD8Qd9 7IWYAh9aD8b DwE6£b@W{7cW"rrAQ¥68vH5P7~ïGðVé ÞcðVñàaä¡pbuEA¥Q¥W"RqPx W"5Be7d9©Aq5rvGiUA¡Cz68R7cW"A¥b'7@9BAel(shb@A¥ç£v©A¡5©Q¥A1DVEHx EAFDVCIAhtW"5t7@CD8W"5©W{5©ehDV7'7d9©AbIDVAE6£bdW"7IW685 f W{7@9©W{5 7d9©A£wlòD8bvW"7sDVEGEUAFDVCIb f W"7@9BW"57d9©AkQFDV5BhHWh©DV7IA8p DV5Bh3Dv5©A¥ePDV7cW{r£AvQ¥68vH5P7àå²ó ÞcðVñàaä¡p0buEA¥Q¥W"RqW"5©e 7d9©Ae5PvHiA¡C6gRB7cW"A¥b7d9©AbdA¥ç)vBA¡5©Q¥A DVEHEAFDFCIA¥hwW{5 687d9©A¡CtEU6£bdW"7IW685©b¥3È©68Cê©ë Îì³ÌPÍÎPÍHÏ£í f Av9UDr£A ôõ8ö ÷Fõø©ùúû yDV5©h ùü¥ý ÷Fõø©ùúmûÿþ p f 9©WYAR68C ê ÊPÊ ë ÎìPí ôõ8ö ÷õFø©ùyú0û wDV5©h ùü¥ý ÷Fõø©ùú0û ¤  W#YA¥byW{5³7d9©AUDF7vrrA¡CcbdW685DVCIA|Q¥68wEGCcWbdA¥h68R lhs3bdAç£vBAI5BQ¥Abs685©Y{q£  9©A|Q¥68E6£b@W"7IW685aD8Y1D8Y"x e£68CIW"7@9G`Q¥685©b@W#hHA¡CcbD8Yb@6$A¡iA¥hHhHA¥hfb7@Cdv©Q¡7dvHCcA¥b¥p 9BAI5BQ¥AmD7IWYAvD¥qDgQ¥Q¥68CIhHW"5Be)Y{qW"5BQY{v©hHAwD8Yb@6wEaDV7@x 7IA¡C@5YXDViA¥Yb7d9aDV7 b7¡DV5©hsR68CDQ68E©YA¡7IAhEaDV7d7cA¡Cd5 W"5Bbu7DV5©Q¥A7²qrEBWQFD8YY"qD(EG9HC¡D8bdAFI s68A68Re7d9©A Îì 7IWYA¥bv©bdA¥hiPqk7d9©AQ68E6£b@W"x 7IW685aD8YGD8Ye£68CcW{7@9HDVCcAEHCcA¥bdA¡5r7IA¥hvW"5È0W#eH£  WY#Ab ¥x¡DVCIA|Q¥68wE6£bdA¥h³68R(lhsf685©Y{q)pwDV5BhQDV5¡iA vBb@A¥hiPq7d9©A|aDV7vrrAICIbdW6857I66H41685©e7@9BA¥b@A 7IWYA¥b¥p  W#YA¥bkDV5©h¤EHCI6rWhHAsA¡rWhHA¡5©Q¥AsRS68Ct7d9©A CDV5©e£Aw§8x@iA¥W"5©eDy£wliA¥QDVvBb@A7I6£e£A¡7@9BAIC7d9©A¡q  ¢¤£  ¥§¦©¨¦¦¦¨¥§©¨!"!#%$&¦¥§¦¨(''¦¦¦¨¨)£  01£  ¥§¦©¨2©3%''4¦¦"¦¦"¦©¨5¥§¨6!#7¥§¦©¨8234¦¦"¦©¨©¨£  91£  ¥§¦©¨2©3%''"''4¦¦"¦©¨5¥§¨6!#7¥§¦©¨823"''4¦¦"¦¨©¨£  @BADCFEHGPIRQTS1UWVXI`YacbRdfeDIhgiGpaFAqVrAqVfCtsracgpa  ¢  ¦¦ ¥§©¨  01£  ¥§©¨!"!#  91£  !#23(''¦¦8¨  £  ¦¦8¨  1£  ¦¦8¨5£  1£  ¥§©¨!"!#  ¦¨  ¨  £  !#  ¦¨  ¨  1£  ¦¨  ¨  ufvw xHyT  0  9  ¢    ¢    0  9  0    ¢    0    9    @BADCFEHGPI©S©FbIhFgifI ¨ gADeDIsHI`GPAqdTIsefGFbgghrIhsfacg`aiAqVX@BADCHj¤Q  k ldIPGmghrIonprTeDIoG`acVfCTIqj4UhrEHbAqVrCsghrIeAtVHVrI`G uvwnpaFxsHI`gI k gPIszyHgPADeDIp{c|i}~GI`rI k gghrIAqgPdHGPIr| I`V k IenWAtgirAtVghrI k FbRdTiAqgPIovBjpADeDIX{aFeDFVrI aFeqGPI&aFs©dHGPldADsHIoadTiAtgAqdTIIPdADsIPV k IXFGgirI I`VgAqGPIvyfaFpAqg k ldIPGPWgirInprTeDIGpacVrCTIFgirI k acVrsHAsracgImnxFGspjuFgImgizacgghrItzacgRdTI`GPiADFV aFsgP~GIeqFVRpADeIW©yTFGACAtVaFeDeqhfGPFbI`VgPI`V k I ©yAqVFGPsHI`GgP k TeDeDI k grEdHdFGigPAqVrCI`d©AsHI`V k I ghacg k &dTI`GpghrIWnprTeDI~GpacVrCTIFzghrI k acVrsHADsfacgI nFGPsHjrI k FbdTiAqgPIaFeDCTFGAqghHb k FEreDsdGF| s©E k IoghrIoI`d©ADsI`V k IenWAqghrFEHggifADRhI`VgPI`V k IFyp GPIlaFeADAqVrCRghrI k FbdzThAqgPADFVaFeI`dADsHI`V k Iqj z¡ ¢¤£¥c£¦f¥q§F¨©F¥¨§qª«m£f¬­4¢¤ªT®1¬¯°¥¯¡±¬« ²a k ³iI`VgPIPV k IADoGI`dHGPIiI`VgPIs%(a6´lµP¶·¸µ`¶¹µ ºc»¼½r¾ S¿girIiVrsHIhFghrICFG`acd k FGhGI¡dFVrsg dTiAtgADFVfIPgÀnxII`VRghrIhI`VgI`V k InxFGspyacVfshgirI IsHCTI k FGiGPIrdFVrssgPeIAqghrI`GivÁhXgpaFCThFGhdacgh| gPIPGhVÂAqVr¡g`acV k IpjÃ@BADCFEGIÄ¡r&nWsahI`VgPI`V k I CFGpacdHoFGWghrIhI`YacbRdreIhCTAqdTI`VsacldIqj ÅIVflnÆsHI`ÇHVfIwghrIwgPADeDIXFadacghgI`GhV³AqVH| ¡g`acV k IAqVgI`GhbtmFgifIehI`VgPI`V k IsCFG`acdj4ÈI`| VrFgPIwÉhÊlË¤ÌÍlËghrIbmacYAqbmaFe1eDI`VrCFgh k FVrhADs©| I`GIsoÎFGpgirIdHGI k IsHAqVfCoacVrsoTeDelnWAqVrC k FVgPI`YgP FghrICTAqdTI`VAqVr¡g`acV k IFyÏacVrseDI`ghÐacVrsÑ¤IigirI ¡g`acGhgAqVfCoacVrsI`VrsHAtVrCmdTiAqgPADFVrFÏghrIiAqVr¡gpacV k I Ò IqjÓCHjDyÔacVrsÕÎFGmghrIevgAqV8@BADCFEGIsTÖ`j³×§V ghrIiIgPI`Gibpya¿gPADeDIØF¤gifIhCTAqdTI`VAqVr¡g`acV k IADWa dacghAqVwghrIiI`VgI`V k ImCFGpacdHsghacgPa&gPAD¡ÇrIgirI ÎTeDeDlnWAtVrC k FVrsAqgADFVfpS QTjØpADWI`bIsHsHIsnWAqghrAqVXghrIhGpacVrCTIFBVrsHI  ÐÙÄÉhÊlË¤ÌÍlËsgiÑhÚÉ¿ÊlËÌÍlË1ÛrghacgApyghrI gPADeDIADtI`bIsHsHIsnWAqghrAqV4gifI¤dacghgPIPGhV4AqVH| ¡g`acV k IacVrsoAqgP k FVgI`Yg&j ©jØoAqV k etErsHIacgeDI&aF¡gsFVrIFRghrIwAtVrrgpacV k I FEHVfsracGhhVrsHIÐPÜ¸ÑyTnprA k  k FGhGI¡dzFVfsgP ghrIzFEVrsracGhmHGpa kiÝ I`gP Ò IqjÓCHjDy ¥§¦¨ y ¦©¨ Ö`j ©jUWVIsHCTItFxØ~ghacg k FGhGI¡dzFVfsHgamdacgi| gPI`GiVÞAqVr¡g`acV k I Ò GpacgifI`GghacVßgPàa(vxÁhHÖ bEfrg1I¡EfeDeqI`b¿IsHsIsRnWAqghrAqVtghrIGpacVrCTI FghrIAqVr¡g`acV k IAqgPiIeq Ò VrsHI1ÐgiGFErCFÑÖPj z¡á â³ª©ã±r§lä¦¥c§¨©F¥F¨¤§cª UoADeDeqEr¡giGpacgPIpsAqV%©I k gADFVà©jDQTypgifIaqeCTFGAqghHb EfiI~ghrIgiGpaFAqVrAtVrC k FGhdHEf¡g`acgPADrgPA k pyåÊFæ ç&ÊlèrË¤Ø acVfsÄØ¡ÊqØ¡éê ç&ÊlèrË¤Ø¿ÎFGmI`dTI`GhgADeIeAqgoI`V k FEVgPI`Gj fI~btI`bFGimI`V k ©sHIghrIhIrgpacgPADrgPA k pAqVahgiGPADI sfacg`aWrghGiE k giEHGPIFSBI&a k dacgh¿fGPFb%ghrIGFgBFHghrI ghGAIgPa k I`GigpaFAqVVrsHI k FGhGPIp¡dFVrsHtgPsaXgPADeDIFy ¡gFGPAqVrChghrIpgAeDIWrgpacgAD¡gA k AqVtghrI k FGhGPIp¡dFVrsHAtVrC Vf©sHIFjrImacG k e°acIeDiaFeDFVfCghrIRdzacgiacGPIghrI vxÁhfyAqVr¡g`acV k ItgÀdIRacVfswe¡aczIeDIsHGpa kiÝ I`gF ghrI k FGiGPIrdFVrsAqVrChdacghgI`GhVj1pADeIpxF k acVfsHADsracgPI AqVfrgpacV k I k acVIRGPI`giGPADI`dTIsfGFbgifIRbtI`bFGi sEHGAtVrC~dacGPhAqVrCWÎTeDeD&nWAqVrC~ghrI k FGhGPIp¡dFVrsHAtVrC dzacgiiAqVghrIghGADIFjÏrIgiGPADIAD k GI&acgPIpsis©EGAqVfC~ghrI ghG`aFAqVfAqVrCsdHaFhIe k FVr¡giGhE k gPAqVrCwghrIiI`VgI`V k I CFGpacdHiFGÏIla k thI`VgPI`V k IacVrsCI`VfI`G`acgPAqVrChaFeeghrI gPADeDIpFGWI&a k edza&ghgPIPGhVAqVr¡g`acV k IFj  VP  NP  NP  0  
We introduce an annotation scheme for temporal expressions, and describe a method for resolving temporal expressions in print and broadcast news. The system, which is based on both hand-crafted and machine-learnt rules, achieves an 83.2% accuracy (Fmeasure) against hand-annotated data. Some initial steps towards tagging event chronologies are also described. Introduction The extraction of temporal information from news offers many interesting linguistic challenges in the coverage and representation of temporal expressions. It is also of considerable practical importance in a variety of current applications. For example, in question-answering, it is useful to be able to resolve the underlined reference in “the next year, he won the Open” in response to a question like “When did X win the U.S. Open?”. In multidocument summarization, providing finegrained chronologies of events over time (e.g., for a biography of a person, or a history of a crisis) can be very useful. In information retrieval, being able to index broadcast news stories by event times allows for powerful multimedia browsing capabilities. Our focus here, in contrast to previous work such as (MUC 1998), is on resolving time expressions, especially indexical expressions like “now”, “today”, “tomorrow”, “next Tuesday”, “two weeks ago”, “20 mins after the next hour”, etc., which designate times that are dependent on the speaker and some  “reference” time1. In this paper, we discuss a temporal annotation scheme for representing dates and times in temporal expressions. This is followed by details and performance measures for a tagger to extract this information from news sources. The tagger uses a variety of hand-crafted and machine-discovered rules, all of which rely on lexical features that are easily recognized. We also report on a preliminary effort towards constructing event chronologies from this data. 
 the large red American car  The order of prenominal adjectival modiﬁers in English is governed by complex and difﬁcult to describe constraints which straddle the boundary between competence and performance. This paper describes and compares a number of statistical and machine learning techniques for ordering sequences of adjectives in the context of a natural language generation system. 
Spoken dialogue managers have beneﬁted from using stochastic planners such as Markov Decision Processes (MDPs). However, so far, MDPs do not handle well noisy and ambiguous speech utterances. We use a Partially Observable Markov Decision Process (POMDP)-style approach to generate dialogue strategies by inverting the notion of dialogue state; the state represents the user’s intentions, rather than the system state. We demonstrate that under the same noisy conditions, a POMDP dialogue manager makes fewer mistakes than an MDP dialogue manager. Furthermore, as the quality of speech recognition degrades, the POMDP dialogue manager automatically adjusts the policy. 
Prepositional phrase attachment is a common source of ambiguity in natural language processing. We present an unsupervised corpus-based approach to prepositional phrase attachment that achieves similar performance to supervised methods. Unlike previous unsupervised approaches in which training data is obtained by heuristic extraction of unambiguous examples from a corpus, we use an iterative process to extract training data from an automatically parsed corpus. Attachment decisions are made using a linear combination of features and low frequency events are approximated using contextually similar words. Introduction Prepositional phrase attachment is a common source of ambiguity in natural language processing. The goal is to determine the attachment site of a prepositional phrase in a sentence. Consider the following examples: 1. Mary ate the salad with a fork. 2. Mary ate the salad with croutons. In both cases, the task is to decide whether the prepositional phrase headed by the preposition with attaches to the noun phrase (NP) headed by salad or the verb phrase (VP) headed by ate. In the first sentence, with attaches to the VP since Mary is using a fork to eat her salad. In sentence 2, with attaches to the NP since it is the salad that contains croutons.  Formally, prepositional phrase attachment is simplified to the following classification task. Given a 4-tuple of the form (V, N1, P, N2), where V is the head verb, N1 is the head noun of the object of V, P is a preposition, and N2 is the head noun of the prepositional complement, the goal is to classify as either adverbial attachment (attaching to V) or adjectival attachment (attaching to N1). For example, the 4-tuple (eat, salad, with, fork) has target classification V. In this paper, we present an unsupervised corpus-based approach to prepositional phrase attachment that outperforms previous unsupervised techniques and approaches the performance of supervised methods. Unlike previous unsupervised approaches in which training data is obtained by heuristic extraction of unambiguous examples from a corpus, we use an iterative process to extract training data from an automatically parsed corpus. The attachment decision for a 4-tuple (V, N1, P, N2) is made as follows. First, we replace V and N2 by their contextually similar words and compute the average adverbial attachment score. Similarly, the average adjectival attachment score is computed by replacing N1 and N2 by their contextually similar words. Attachment scores are obtained using a linear combination of features of the 4-tuple. Finally, we combine the average attachment scores with the attachment score of N2 attaching to the original V and the attachment score of N2 attaching to the original N1. The proposed classification represents the attachment site that scored highest. 
This paper presents a novel statistical model for automatic identification of English baseNP. It uses two steps: the Nbest Part-Of-Speech (POS) tagging and baseNP identification given the N-best POS-sequences. Unlike the other approaches where the two steps are separated, we integrate them into a unified statistical framework. Our model also integrates lexical information. Finally, Viterbi algorithm is applied to make global search in the entire sentence, allowing us to obtain linear complexity for the entire process. Compared with other methods using the same testing set, our approach achieves 92.3% in precision and 93.2% in recall. The result is comparable with or better than the previously reported results. 
We have developed a system that generates evaluative arguments that are tailored to the user, properly arranged and concise. We have also developed an evaluation framework in which the effectiveness of evaluative arguments can be measured with real users. This paper presents the results of a formal experiment we have performed in our framework to verify the influence of argument conciseness on argument effectiveness 
In this paper, a computational approach for resolving zero-pronouns in Spanish texts is proposed. Our approach has been evaluated with partial parsing of the text and the results obtained show that these pronouns can be resolved using similar techniques that those used for pronominal anaphora. Compared to other well-known baselines on pronominal anaphora resolution, the results obtained with our approach have been consistently better than the rest. Introduction In this paper, we focus specifically on the resolution of a linguistic problem for Spanish texts, from the computational point of view: zero-pronouns in the “subject” grammatical position. Therefore, the aim of this paper is not to present a new theory regarding zeropronouns, but to show that other algorithms, which have been previously applied to the computational resolution of other kinds of pronoun, can also be applied to resolve zeropronouns. The resolution of these pronouns is implemented in the computational system called Slot Unification Parser for Anaphora resolution (SUPAR). This system, which was presented in Ferrández et al. (1999), resolves anaphora in both English and Spanish texts. It is a modular system and currently it is being used for Machine Translation and Question Answering, in which this kind of pronoun is very important to solve due to its high frequency in Spanish texts as this paper will show. We are focussing on zero-pronouns in Spanish texts, although they also appear in other languages, such as Japanese, Italian and Chinese. In English texts, this sort of pronoun occurs far less frequently, as the use of subject  pronouns is generally compulsory in the language. While in other languages, zeropronouns may appear in either the subject´s or the object´s grammatical position, (e.g. Japanese), in Spanish texts, zero-pronouns only appear in the position of the subject. In the following section, we present a summary of the present state-of-the-art for zeropronouns resolution. This is followed by a description of the process for the detection and resolution of zero-pronouns. Finally, we present the results we have obtained with our approach. 
We present a clustering algorithm for Arabic words sharing the same root. Root based clusters can substitute dictionaries in indexing for IR. Modifying Adamson and Boreham (1974), our Two-stage algorithm applies light stemming before calculating word pair similarity coefficients using techniques sensitive to Arabic morphology. Tests show a successful treatment of infixes and accurate clustering to up to 94.06% for unedited Arabic text samples, without the use of dictionaries. Introduction Canonisation of words for indexing is an important and difficult problem for Arabic IR. Arabic is a highly inflectional language with 85% of words derived from tri-lateral roots (AlFedaghi and Al-Anzi 1989). Stems are derived from roots through the application of a set of fixed patterns. Addition of affixes to stems yields words. Words sharing a root are semantically related and root indexing is reported to outperform stem and word indexing on both recall and precision (Hmeidi et al 1997). However, Arabic morphology is excruciatingly complex (the Appendix attempts a brief introduction), and root identification on a scale useful for IR remains problematic. Research on Arabic IR tends to treat automatic indexing and stemming separately. Al-Shalabi and Evans (1998) and El-Sadany and Hashish (1989) developed stemming algorithms. Hmeidi et al (1997) developed an information retrieval system with an index, but does not explain the underlying stemming algorithm. In Al-Kharashi and Evans (1994), stemming is done manually  and the IR index is built by manual insertion of roots, stems and words. Typically, Arabic stemming algorithms operate by “trial and error”. Affixes are stripped away, and stems “undone”, according to patterns and rules, and with reference to dictionaries. Root candidates are checked against a root lexicon. If no match is found, affixes and patterns are readjusted and the new candidate is checked. The process is repeated until a root is found. Morpho-syntactic parsers offer a possible alternative to stemming algorithms. Al-Shalabi and Evans (1994), and Ubu-Salem et al (1999) develop independent analysers. Some work builds on established formalisms such a DATR (Al-Najem 1998), or KIMMO. This latter strand produced extensive deep analyses. Kiraz (1994) extended the architecture with multi-level tape, to deal with the typical interruption of root letter sequences caused by broken plural and weak root letter change. Beesley (1996) describes the re-implementation of earlier work as a single finite state transducer between surface and lexical (root and tag) strings. This was refined (Beesley 1998) to the current on-line system capable of analysing over 70 million words. So far, these approaches have limited scope for deployment in IR. Even if substantial, their morpho-syntactic coverage remains limited and processing efficiency implications are often unclear. In addition, modern written Arabic presents a unique range of orthographic problems. Short vowels are not normally written (but may be). Different regional spelling conventions may appear together in a single text and show interference with spelling errors. These systems, however, assume text to be in perfect (some even vowelised) form, forcing the need for editing prior to processing. Finally, the success of these algorithms depends critically on root, stem, pattern or affix dictionary quality,  and no sizeable and reliable electronic dictionaries exist. Beesley (1998) is the exception with a reported 4930 roots encoded with associated patterns, and an additional affix and non-root stem lexicon1. Absence of large and reliable electronic lexical resources means dictionaries would have to be updated as new words appear in the text, creating a maintenance overhead. Overall, it remains uncertain whether these approaches can be deployed and scaled up cost-effectively to provide the coverage required for full scale IR on unsanitised text. Our objective is to circumvent morphosyntactic analysis of Arabic words, by using clustering as a technique for grouping words sharing a root. In practise, since Arabic words derived from the same root are semantically related, root based clusters can substitute root dictionaries for indexing in IR and furnish alternative search terms. Clustering works without dictionaries, and the approach removes dictionary overheads completely. Clusters can be implemented as a dimension of the index, growing dynamically with text, and without specific maintenance. They will accommodate effortlessly a mixture of regional spelling conventions and even some spelling errors. 
The paper develops a constraint-based theory of prosodic phrasing and prominence, based on an HPSG framework, with an implementation in ALE. Prominence and juncture are represented by n-ary branching metrical trees. The general aim is to deﬁne prosodic structures recursively, in parallel with the deﬁnition of syntactic structures. We address a number of prima facie problems arising from the discrepancy between syntactic and prosodic structure 
An approach to automatic detection of syllable structure is presented. We demonstrate a novel application of EM-based clustering to multivariate data, exempli ed by the induction of 3- and 5-dimensional probabilistic syllable classes. The qualitative evaluation shows that the method yields phonologically meaningful syllable classes. We then propose a novel approach to grapheme-to-phoneme conversion and show that syllable structure represents valuable information for pronunciation systems. 
     ! "#     $%     #       $%    "#     & '( ()'(  #*   + ,  )-                      .  ) -           /          /         )         "      0              ,     1      /  2  33!4 )        55  )         .     / )*6   //  *     /       /     )-   7      5-)55-)52(* 3384 +              /  2  339  : 33 4      5-)5    )-       ;)  /        / ) /   ,    )    /  <  ,     / ) ; ,   )       )     * ==   - ) <>         *5-)5 /?  *   /    /-) +,  )-           )          ) /       )    2 3!4   /  /         //                   .     ,     "      <    *    /        ,  %  )>     7      ))6 0-) 6      )    , > /    )     ,  *    /   6  /  +     /       !    !    /  +     /         )          !     !         !  %-)   *    /   6 . /     !   .   !   .-   !               "  2 4       24             )     @ 2  δ 4 )           /)    δ 6 ×!     ⊆                    )/  A  )                 )       ; ,  )    )6         /     !" # )BCB C BC  /  ;  )        )  .  2  δ 4                /            /         /     +  )        )      6     ∅/ , )-               +,  )  /-     7         )    )        ')     /)  ,  )     7 ;     7   /    /) /   ,        /   *)  ,   )    )           ; ,  ) ,     B$%&'()*+,C   /         )      B$C B$ %C B$%&C B$%&'C B$%&'(CD                    "δ    ≠      ∈   "  !                                  /7   ,  )-      -  21" 33E4            /  ,         + / , /  /    )        /      )         / )     / ) ) >   )   )          /     ;,  -.// ! -./0123 )   /  ;-*      /) /24  01232 4 + -./ 012    ) -./0123  ) -.//   )      ,   )  6   / )   8 F D 0       /)             )     )  ) / 6 '    ,+   )   "  / E )  /) )     -  <         -  G       .   ,   /  0H 1  *  !  I   &> *   , ,  <   , ;- * )     - )  * *    $            - ) ;    E-     , - ) ' 0  /   E                  ,     )   /       )    -  )  .   24/))         6  A   ;     )     ,  4 24   2424@F ;        =  ,  5 24  6 24 24@!      ;- *   ;          /       /  /   , 782 4  9:;24 24@F ;          /      /        ,  < 24  9 24 = 2/)4  > 2)/4 24@!      ,          ;, 2? @4@8E <   ε     "2εε4@2""4@  2" ε4 @ 2ε "4 @ I ;             /)     24@J∞ - 6  @KL≠ε∃≠24MJ∞N  ;  - ∈    246   24@KL∈≠ε24MJ∞N  +  ∈ 24       .   )O P) OP /6                 =                                     <  +∞    ≤≤ ≤ ≤                +          +        +               ;  ,        ,                  ),            )   / /    ) 8FD+   /     ,    /) )      /  <            "      < >      -  24  ; 8 )  ,   )-    .  @2  δ 4    @2  δ 4                   /            /        / , )   F-  *2 4)  ,   )  )  ,     ,       /)   )       )   6  @ @  @ @∅        /   )   /       ; ,  )  ,     /  5$A&'()*+, 5 ) A / %  /2$842 8F4 2B8F42C8!4D2$%F42$% & ! 4 2$D 8 I4 2$E 8 I4D 2F$ 8I42G$8I4D) $ %&           /  2  33!4      /    )         / 1  ) /           /  /   / <  /    ,   )-    <       ,       /         /                          ε≤                   ε      "δ  ≠                ε≤      ∈        ε         ε            ∅          !!∈  ""∈!        !#  " !"≤ "δ" ≠        ∈   " !!"    "   !!"                                 $               % ∅  #      %     #                        A     /      )               + )      ,         /)  / 1  )   ?)) )/; ≥ <? G     /)   )    ,             / 6 ∑     ≤   ∈          )       ,     ; F )      ,      /  .  !"   ,       >      * 2   4       / ,     1  /               F8 !"       /   ;   ,      /      0Q     ) A1    )     )  2!"4        )             /)  !"      6 2!" 4@ 2!"4R  2!" 4 <)6  2!" 4@ ∏  (( ))  ∈ .  /      )         2  4    /)         Q      /                // ,       ,                    ;        * 6 +@ /, @ /  )    @ /      @ /       R                                                     !""#           !  !               7/,      @ =+RS   @ =RS@=+RS <  ,   ,  IFI    /         ) -     / 8$ /  >  3F  3!    T>3!  >3! )       I3  I  : :>        ,      /   * , ) ;! ;    )      )/   /       /   2: 33!4    /             /       )      / )       6 <       .        )  /    ; ,    BHI4J KC!BHL4JKC      , /BIC     BLC $       )             )    &$  -      <>-  <       &     #   +  ,   )-                   7      )  7      $    $    $  !"#$ %&'()*+,-./ 
 The noisy channel model has been applied  to a wide range of problems, including  spelling correction. These models consist  of two components: a source model and a  channel model. Very little research has  gone into improving the channel model  for spelling correction. This paper  describes a new channel model for  spelling correction, based on generic  string to string edits. Using this model  gives  significant  performance  improvements compared to previously  proposed models.  Introduction The noisy channel model (Shannon 1948) has been successfully applied to a wide range of problems, including spelling correction. These models consist of two components: a source model and a channel model. For many applications, people have devoted considerable energy to improving both components, with resulting improvements in overall system accuracy. However, relatively little research has gone into improving the channel model for spelling correction. This paper describes an improvement to noisy channel spelling correction via a more powerful model of spelling errors, be they typing mistakes or cognitive errors, than has previously been employed. Our model works by learning generic string to string edits, along with the probabilities of each of these edits. This more powerful model gives significant improvements in accuracy over previous  approaches to noisy channel spelling correction. 
This paper introduces a statistical model for query-relevant summarization: succinctly characterizing the relevance of a document to a query. Learning parameter values for the proposed model requires a large collection of summarized documents, which we do not have, but as a proxy, we use a collection of FAQ (frequently-asked question) documents. Taking a learning approach enables a principled, quantitative evaluation of the proposed system, and the results of some initial experiments—on a collection of Usenet FAQs and on a FAQ-like set of customer-submitted questions to several large retail companies—suggest the plausibility of learning for summarization.  not query-relevant, and thus provide very little guidance to the user in assessing relevance. Query-relevant summarization (QRS) aims to provide a more effective characterization of a document by accounting for the user’s information need when generating a summary.  (a) ¡  ¢£  Search for  ¤¥  relevant  documents  ¦§  (b) Summarize documents relative to Q  σ' ( ) 0 1 2 σ! " # $ % & σ     σ©       ¨  
Extractive summarization techniques cannot generate document summaries shorter than a single sentence, something that is often required. An ideal summarization system would understand each document and generate an appropriate summary directly from the results of that understanding. A more practical approach to this problem results in the use of an approximation: viewing summarization as a problem analogous to statistical machine translation. The issue then becomes one of generating a target document in a more concise language from a source document in a more verbose language. This paper presents results on experiments using this approach, in which statistical models of the term selection and term ordering are jointly applied to produce summaries in a style learned from a training corpus. 
This paper reports the first part of a project that aims to develop a knowledge extraction and knowledge discovery system that extracts causal knowledge from textual databases. In this initial study, we develop a method to identify and extract cause-effect information that is explicitly expressed in medical abstracts in the Medline database. A set of graphical patterns were constructed that indicate the presence of a causal relation in sentences, and which part of the sentence represents the cause and which part represents the effect. The patterns are matched with the syntactic parse trees of sentences, and the parts of the parse tree that match with the slots in the patterns are extracted as the cause or the effect. 
In terms of both speed and memory consumption, graph uniﬁcation remains the most expensive component of uniﬁcation-based grammar parsing. We present a technique to reduce the memory usage of uniﬁcation algorithms considerably, without increasing execution times. Also, the proposed algorithm is thread-safe, providing an eﬃcient algorithm for parallel processing as well. 
Dominance constraints are logical descriptions of trees that are widely used in computational linguistics. Their general satisﬁability problem is known to be NP-complete. Here we identify the natural fragment of normal dominance constraints and show that its satisﬁability problem is in deterministic polynomial time. 
The deﬁnitions of the basic concepts, rules, and constraints of centering theory involve underspeciﬁed notions such as ‘previous utterance’, ‘realization’, and ‘ranking’. We attempted to ﬁnd the best way of deﬁning each such notion among those that can be annotated reliably, and using a corpus of texts in two domains of practical interest. Our main result is that trying to reduce the number of utterances without a backwardlooking center (CB) results in an increased number of cases in which some discourse entity, but not the CB, gets pronominalized, and viceversa. 
Existing software systems for automated essay scoring can provide NLP researchers with opportunities to test certain theoretical hypotheses, including some derived from Centering Theory. In this study we employ ETS's e-rater essay scoring system to examine whether local discourse coherence, as de ned by a measure of Rough-Shift transitions, might be a signi cant contributor to the evaluation of essays. Our positive results indicate that Rough-Shifts do indeed capture a source of incoherence, one that has not been closely examined in the Centering literature. These results not only justify Rough-Shifts as a valid transition type, but they also support the original formulation of Centering as a measure of discourse continuity even in pronominal-free text. 
In this paper, we outline a theory of referential accessibility called Veins Theory (VT). We show how VT addresses the problem of "left satellites", currently a problem for stack-based models, and show that VT can be used to significantly reduce the search space for antecedents. We also show that VT provides a better model for determining domains of referential accessibility, and discuss how VT can be used to address various issues of structural ambiguity. Introduction In this paper, we outline a theory of referential accessibility called Veins Theory (VT). We compare VT to stack-based models based on Grosz and Sidner's (1986) focus spaces, and show how VT addresses the problem of "left satellites", i.e., subordinate discourse segments that appear prior to their nuclei (dominating segments) in the linear text. Left-satellites pose a problem for stack-based models, which remove subordinate segments from the stack before pushing a nuclear or dominating segment, thus rendering them inaccessible. The percentage of such cases is typically small, which may account for the fact that their treatment has been largely overlooked in the literature, but the phenomenon nonetheless persists in most texts. We also show how VT  can be used to address various issues of structural ambiguity. 
Building a bilingual dictionary for transfer in a machine translation system is conventionally done by hand and is very time-consuming. In order to overcome this bottleneck, we propose a new mechanism for lexical transfer, which is simple and suitable for learning from bilingual corpora. It exploits a vector-space model developed in information retrieval research. We present a preliminary result from our computational experiment. Introduction Many machine translation systems have been developed and commercialized. When these systems are faced with unknown domains, however, their performance degrades. Although there are several reasons behind this poor performance, in this paper, we concentrate on one of the major problems, i.e., building a bilingual dictionary for transfer. A bilingual dictionary consists of rules that map a part of the representation of a source sentence to a target representation by taking grammatical differences (such as the word order between the source and target languages) into consideration. These rules usually use case-frames as their base and accompany syntactic and/or semantic constraints on mapping from a source word to a target word. For many machine translation systems, experienced experts on individual systems compile the bilingual dictionary, because this is a complicated and difficult task. In other words, this task is knowledge-intensive and labor-intensive, and therefore, time-consuming. Typically, the developer of a machine translation system has to spend several years building a general-purpose bilingual dictionary. Unfortunately, such a general-purpose  dictionary is not almighty, in that (1) when faced with a new domain, unknown source words may emerge and/or some domain-specific usages of known words may appear and (2) the accuracy of the target word selection may be insufficient due to the handling of many target words simultaneously. Recently, to overcome these bottlenecks in knowledge building and/or tuning, the automation of lexicography has been studied by many researchers: (1) approaches using a decision tree: the ID3 learning algorithm is applied to obtain transfer rules from case-frame representations of simple sentences with a thesaurus for generalization (Akiba et. al., 1996 and Tanaka, 1995); (2) approaches using structural matching: to obtain transfer rules, several search methods have been proposed for maximal structural matching between trees obtained by parsing bilingual sentences (Kitamura and Matsumoto, 1996; Meyers et. al., 1998; and Kaji et. al.,1992). 
This paper describes a language independent method for alignment of parallel texts that makes use of homograph tokens for each pair of languages. In order to filter out tokens that may cause misalignment, we use confidence bands of linear regression lines instead of heuristics which are not theoretically supported. This method was originally inspired on work done by Pascale Fung and Kathleen McKeown, and Melamed, providing the statistical support those authors could not claim. Introduction Human compiled bilingual dictionaries do not cover every term translation, especially when it comes to technical domains. Moreover, we can no longer afford to waste human time and effort building manually these ever changing and incomplete databases or design language specific applications to solve this problem. The need for an automatic language independent task for equivalents extraction becomes clear in multilingual regions like Hong Kong, Macao, Quebec, the European Union, where texts must be translated daily into eleven languages, or even in the U.S.A. where Spanish and English speaking communities are intermingled. Parallel texts (texts that are mutual translations) are valuable sources of information for bilingual lexicography. However, they are not of much use unless a computational system may find which piece of text in one language corresponds to which piece of text in the other language. In order to achieve this, they must be aligned first, i.e. the various pieces of text must  be put into correspondence. This makes the translations extraction task easier and more reliable. Alignment is usually done by finding correspondence points – sequences of characters with the same form in both texts (homographs, e.g. numbers, proper names, punctuation marks), similar forms (cognates, like Region and Região in English and Portuguese, respectively) or even previously known translations. Pascale Fung and Kathleen McKeown (1997) present an alignment algorithm that uses term translations as correspondence points between English and Chinese. Melamed (1999) aligns texts using correspondence points taken either from orthographic cognates (Michel Simard et al., 1992) or from a seed translation lexicon. However, although the heuristics both approaches use to filter noisy points may be intuitively quite acceptable, they are not theoretically supported by Statistics. The former approach considers a candidate correspondence point reliable as long as, among some other constraints, “[...] it is not too far away from the diagonal [...]” (Pascale Fung and Kathleen McKeown, 1997, p.72) of a rectangle whose sides sizes are proportional to the lengths of the texts in each language (henceforth, ‘the golden translation diagonal’). The latter approach uses other filtering parameters: maximum point ambiguity level, point dispersion and angle deviation (Melamed, 1999, pp. 115–116). António Ribeiro et al. (2000a) propose a method to filter candidate correspondence points generated from homograph words which occur only once in parallel texts (hapaxes) using linear regressions and statistically supported noise filtering methodologies. The method avoids heuristic filters and they claim high precision alignments.  In this paper, we will extend this work by defining a linear regression line with all points generated from homographs with equal frequencies in parallel texts. We will filter out those points which lie outside statistically defined confidence bands (Thomas Wonnacott and Ronald Wonnacott, 1990). Our method will repeatedly use a standard linear regression line adjustment technique to filter unreliable points until there is no misalignment. Points resulting from this filtration are chosen as correspondence points. The following section will discuss related work. The method is described in section 2 and we will evaluate and compare the results in section 3. Finally, we present conclusions and future work. 
This paper presents a restricted version of Set-Local Multi-Component TAGs Weir, 1988 which retains the strong generative capacity of Tree-Local MultiComponent TAG i.e. produces the same derived structures but has a greater derivational generative capacity i.e. can derive those structures in more ways. This formalism is then applied as a framework for integrating dependency and constituency based linguistic representations. 
We discuss the advantages of lexicalized tree-adjoining grammar as an alternative to lexicalized PCFG for statistical parsing, describing the induction of a probabilistic LTAG model from the Penn Treebank and evaluating its parsing performance. We nd that this induction method is an improvement over the EM-based method of Hwa, 1998, and that the induced model yields results comparable to lexicalized PCFG. 
The paper proposes an information-theorybased method for feature types analysis in probabilistic evaluation modelling for statistical parsing. The basic idea is that we use entropy and conditional entropy to measure whether a feature type grasps some of the information for syntactic structure prediction. Our experiment quantitatively analyzes several feature types’ power for syntactic structure prediction and draws a series of interesting conclusions. 
We present a new approach to stochastic modeling of constraintbased grammars that is based on loglinear models and uses EM for estimation from unannotated data. The techniques are applied to an LFG grammar for German. Evaluation on an exact match task yields 86% precision for an ambiguity rate of 5.4, and 90% precision on a subcat frame match for an ambiguity rate of 25. Experimental comparison to training from a parsebank shows a 10% gain from EM training. Also, a new class-based grammar lexicalization is presented, showing a 10% gain over unlexicalized models. 
In recent years, statistical approaches on ATR (Automatic Term Recognition) have achieved good results. However, there are scopes to improve the performance in extracting terms still further. For example, domain dictionaries can improve the performance in ATR. This paper focuses on a method for extracting terms using a dictionary hierarchy. Our method produces relatively good results for this task. Introduction In recent years, statistical approaches on ATR (Automatic Term Recognition) (Bourigault, 1992; Dagan et al, 1994; Justeson and Katz, 1995; Frantzi, 1999) have achieved good results. However, there are scopes to improve the performance in extracting terms still further. For example, the additional technical dictionaries can be used for improving the accuracy in extracting terms. Although, the hardship on constructing an electronic dictionary was major obstacles for using an electronic technical dictionary in term recognition, the increasing development of tools for building electronic lexical resources makes a new chance to use them in the field of terminology. From these endeavour, a number of electronic technical dictionaries (domain dictionaries) have been acquired. Since newly produced terms are usually made out of existing terms, dictionaries can be used as a source of them. For example, ‘distributed database’ is composed of ‘distributed’ and ‘database’ that are terms in a computer science domain. Further, concepts and terms of a domain are frequently imported from related domains.  For example, the term ‘Geographical Information System (GIS)’ is used not only in a computer science domain, but also in an electronic domain. To use these properties, it is necessary to build relationships between domains. The hierarchical clustering method used in the information retrieval offers a good means for this purpose. A dictionary hierarchy can be constructed by the hierarchical clustering method. The hierarchy helps to estimate the relationships between domains. Moreover the estimated relationships between domains can be used for weighting terms in the corpus. For example, a domain of electronics may have a deep relationship to that of computer science. As a result, terms in the dictionary of electronics domain have a higher probability to be terms of computer science domain than terms in the dictionary of others do (Felber, 1984). The recent works on ATR identify the candidate terms using shallow syntactic information and score the terms using statistical measure such as frequency. The candidate terms are ranked by the score and are truncated by the thresholds. However, the statistical method solely may not give accurate performance in case of small sized corpora or very specialized domains, where the terms may not appear repeatedly in the corpora. In our approach, a dictionary hierarchy is used to avoid these limitations. In the next section, we describe the overall method description. In section 2, section 3, and section 4, we describe primary methods and its details. In section 5, we describe experiments and results 
We present a system for identifying the semantic relationships, or semantic roles, lled by constituents of a sentence within a semantic frame. Various lexical and syntactic features are derived from parse trees and used to derive statistical classi ers from hand-annotated training data. 
Writing English is a big barrier for most Chinese users. To build a computer-aided system that helps Chinese users not only on spelling checking and grammar checking but also on writing in the way of native-English is a challenging task. Although machine translation is widely used for this purpose, how to find an efficient way in which human collaborates with computers remains an open issue. In this paper, based on the comprehensive study of Chinese users requirements, we propose an approach to machine aided English writing system, which consists of two components: 1) a statistical approach to word spelling help, and 2) an information retrieval based approach to intelligent recommendation by providing suggestive example sentences. Both components work together in a unified way, and highly improve the productivity of English writing. We also developed a pilot system, namely PENS (Perfect ENglish System). Preliminary experiments show very promising results. Introduction With the rapid development of the Internet, writing English becomes daily work for computer users all over the world. However, for Chinese users who have significantly different culture and writing style, English writing is a big barrier. Therefore, building a machine-aided English writing system, which helps Chinese users not only on spelling checking and grammar checking but also on writing in the way of native-English, is a very promising task.  Statistics shows that almost all Chinese users who need to write in English1 have enough knowledge of English that they can easily tell the difference between two sentences written in Chinese-English and native-English, respectively. Thus, the machine-aided English writing system should act as a consultant that provide various kinds of help whenever necessary, and let users play the major role during writing. These helps include: 1) Spelling help: help users input hard-to-spell words, and check the usage in a certain context simultaneously; 2) Example sentence help: help users refine the writing by providing perfect example sentences. Several machine-aided approaches have been proposed recently. They basically fall into two categories, 1) automatic translation, and 2) translation memory. Both work at the sentence level. While in the former, the translation is not readable even after a lot of manually editing. The latter works like a case-based system, in that, given a sentence, the system retrieve similar sentences from translation example database, the user then translates his sentences by analogy. To build a computer-aided English writing system that helps Chinese users on writing in the way of native-English is a challenging task. Machine translation is widely used for this purpose, but how to find an efficient way in which human collaborates well with computers remains an open issue. Although the quality of fully automatic machine translation at the sentence level is by no means satisfied, it is hopeful to 
As an application of NLP to computer-assisted language learning(CALL) , we propose a diagnostic processing of Japanese being able to detect errors and inappropriateness of sentences composed by the students in the given situation and the context of the exercise texts. Using LTAG(Lexicalized Tree Adjoining Grammar) formalism, we have implemented a prototype of such a diagnostic parser as a component of a CALL system being developed. 
The main aim of this paper is to analyse the e ects of applying pronominal anaphora resolution to Question Answering QA systems. For this task a complete QA system has been implemented. System evaluation measures performance improvements obtained when information that is referenced anaphorically in documents is not ignored. 
We propose a distribution-based pruning of n-gram backoff language models. Instead of the conventional approach of pruning n-grams that are infrequent in training data, we prune n-grams that are likely to be infrequent in a new document. Our method is based on the n-gram distribution i.e. the probability that an n-gram occurs in a new document. Experimental results show that our method performed 7-9% (word perplexity reduction) better than conventional cutoff methods. 
This is a paper that describes computational linguistic activities on Philippines languages. The Philippines is an archipelago with vast numbers of islands and numerous languages. The tasks of understanding, representing and implementing these languages require enormous work. An extensive amount of work has been done on understanding at least some of the major Philippine languages, but little has been done on the computational aspect. Majority of the latter has been on the purpose of machine translation. 
There are many challenging problems for Vietnamese language processing. It will be a long time before these challenges are met. Even some apparently simple problems such as spelling correction are quite difficult and have not been approached systematically yet. In this paper, we will discuss one aspect of this type of work: designing the so-called Vietools to detect and correct spelling of Vietnamese texts by using a spelling database based on TELEX code. Vietools is also extended to serve many purposes in Vietnamese language processing. Introduction For the past two decades computational linguistics (CL) has progressed substantially in Vietnam, mainly in these basic aspects: data acquisition from the keyboard, encoding, and restitution through an output device for Vietnamese diacritic characters, updates on the fonts in Microsoft DOS/Windows, standardization for Vietnamese (James Do, Ngo Thanh Nhan), automatic translation of English documents into Vietnamese and vice versa (Phan Thi Tuoi, Dinh Dien), recognition of handwriting (Hoang Kiem, Nguyen Van Khuong), speech processing (Nguyen Thanh Phuc, Quach Tuan Ngoc), building bilingual dictionaries such as English-Vietnamese and VE, French-Vietnamese and V-F dictionaries (Lac Viet), archives of old Sino-Vietnamese documents (Ngo Trung Viet, Cong Tam), etc. Some of these works have been presented in Informatics and IT workshops organized in Vietnam. These efforts are modest and do not yet show our full potential. There are many reasons for this weakness. The major reasons that the different efforts are quite isolated and there is not enough coordination. Some coordinated workshops held from time to time would be very helpful.  At the IT Dept. DaNang University we are building a lexical database based on TELEX code for accomplishing the following tasks: - Converting Vietnamese texts from any font to any other font. - Putting texts in alphabetical order independently of the font in use. - Looking up words up in the monolingual and / or multilingual dictionary. - Building specialized monolingual dictionaries. At present, we are taking part in the GETA, CLIPS, IMAG, France, in the FEV project: for a multilingual dictionary: French-Vietnamese via English. In fact, inputting Vietnamese texts still encounters many problems, not yet solved properly. The most common mistakes in detecting and correcting spelling errors are: - wrong intonation or misspelling, - not following spelling specialization, not using syllables systematically in the same texts, etc. Winword, a commercial text processor, is not able to detect and correct spelling mistakes. The program designed by Ngo Thanh Nhan (without an associated spelling dictionary) and other software packages for Vietnamese still do not offer adequate solutions. We propose here a general solution for building the so-called Vietools for detecting and correcting spelling errors. Vietools is designed for office application such as Winword, Excel, Acess, PowerPoint, etc. in Microsoft Windows. Vietools has also been extended for converting and rearranging Vietnamese words in the dictionaries and consulting the Vietnamese dictionaries, including multilingual dictionaries. 
2. Machine Translation Four major efforts on machine translation in India are presented below. The first one is from one Indian language to another, the next three are from English to Hindi. 2.1. Anusaaraka Systems among Indian languages In the anusaaraka systems, the load between the human reader and the machine is divided as follows: language-based analysis of the text is carried out by the machine, and knowledge-based analysis or interpretation is left to the reader. The machine uses a dictionary and grammar rules, to produce the output. Most importantly, it does not use world knowledge to interpret (or disambiguate), as it is an error prone task and involves guessing or inferring based on knowledge other than the text. Anusaaraka aims for perfect "information preservation". We relax the requirement that the output be grammatical. In fact, anusaaraka output follows the grammar of the source language (where the grammar rules differ, and cannot be applied with 100 percent confidence). This requires that the reader undergo a short training to read and understand the output. Among Indian languages, which share vocabulary, grammar, pragmatics, etc. the task (and the training) is easier. For example, words in a language are ambiguous, but if the two languages are close, one is likely to find a one to one correspondence between words such that the meaning is carried across from the source language to target language. For example, for 80 percent of the Kannada words in the anusaaraka dictionary of 30,000 root words, there is a single equivalend Hindi word which covers the senses of the original Kannada word. Similarly, wherever the two languages differ in grammatical constructions, either an existing construction in the target language which expresses the same meaning is used, or a new construction is invented (or an old construction used  with some special notation). For example, adjectival participial phrases in the south Indian languages are mapped to relative clauses in Hindi with the ’*’ notation (Bharati, 2000). Similarly, existing words in the target language may be given wider or narrower meaning (Narayana, 1994). Anusaarakas are available for use as email servers (anusaaraka, URL). 2.2. Mantra System The Mantra system translates appointment letters in government from English to Hindi. It is based on synchronous Tree Adjoining Grammar and uses treetransfer for translating from English to Hindi. The system is tailored to deal with its narrow subjectdomain. The grammar is specially designed to accept analyze and generate sentential constructions in "officialese". Similarly, the lexicon is suitably restricted to deal with meanings of English words as used in its subject-domain. The system is ready for use in its domain. 
Electronically available multilingual information can be divided into two major categories: (1) alphabetic language information (English-like alphabetic languages) and (2) ideographic language information (Chinese-like ideographic languages). The information available in non-English alphabetic languages as well as in ideographic languages (especially, in Japanese and Chinese) is growing at an incredibly high rate in recent years. Due to the ideographic nature of Japanese and Chinese, complicated with the existence of several encoding standards in use, efficient processing (representation, indexing, retrieval, etc.) of such information became a tedious task. In this paper, we propose a Han Character (Kanji) oriented Interlingua model of indexing and retrieving Japanese and Chinese information. We report the results of mono- and cross- language information retrieval on a Kanji space where documents and queries are represented in terms of Kanji oriented vectors. We also employ a dimensionality reduction technique to compute a Kanji Conceptual Space (KCS) from the initial Kanji space, which can facilitate conceptual retrieval of both mono- and cross- language information for these languages. Similar indexing approaches for multiple European languages through term association (e.g., latent semantic indexing) or through conceptual mapping (using lexical ontology such as, WordNet) are being intensively explored. The Interlingua approach investigated here with Japanese and Chinese languages, and the term (or concept) association model investigated with the European languages are similar; and these approaches can be easily integrated. Therefore, the proposed Interlingua model can pave the way for handling multilingual information access and retrieval efficiently and uniformly. Keywords: Cross-language Information Retrieval; Multilingual Information Processing; Latent Semantic Indexing. *Nara Institute of Science and Technology 8916-5, Takayama, Ikoma, Nara, 630-0101 Japan E-Mail: {maruf-h, matsu}@is.aist-nara.ac.jp  60  Md. M. Hasan et al.  1. Introduction The amount of multilingual information available electronically has escalated in recent years. Lately, the information in non-English European languages and in Chinese, Japanese, Korean and Vietnamese (CJKV) is increasing at an incredibly high rate. Both Japanese and Chinese (also, Korean and Vietnamese, to some extent) are ideographic languages that use thousands of ideographic characters (also known as: Han characters, Kanji, Hanzi or Hanja) in writing. Managing the huge number of characters is no longer a problem in processing Japanese and Chinese language information. However, computer processing of these languages is complicated with the absence of word delimitation and the existence of several national and industrial encoding standards. Word delimitation (also called, segmentation) is an extra task to perform to process these languages, because in the written Japanese and Chinese texts, explicit boundaries between words are not available. Due to the existence of several encoding standards, it is also quite common to notice that most Internet search engines provide two different services to search for Chinese information in Traditional Chinese (commonly encoded in BIG-5 code) and the Simplified Chinese (GB code) form. Technically speaking, the tasks of processing and retrieval of traditional and simplified Chinese can be considered the tasks involving two distinct languages. Similarly, JIS, Shift-JIS and EUC, etc. are common Japanese encoding standards. The existence of several encoding standards complicates the information retrieval (IR) tasks for both Japanese and Chinese [24]. In this paper, we will formulate a unified framework to cope with the above-mentioned problems as well as to facilitate effective multilingual information retrieval. Electronically available multilingual information can be divided into two major categories: (1) alphabetic language information (English-like alphabetic languages) and (2) ideographic language information (Chinese-like ideographic languages). Unicode, an increasingly popular encoding standard, defines uniform codes for almost all characters (both alphabetic and ideographic) of the world languages [43]. The common CJK ideographs section defined under Unicode is a superset of all ideographic Han characters used across the CJKV languages. This offers us an opportunity to represent Japanese and Chinese documents uniformly in Unicode. By doing so, we can also take advantage of Kanji to index and retrieve information across these languages. Nonetheless, the Kanji-derived semantic units or concepts can also be easily associated with the corresponding terms (stem, word or concept, etc.) of the alphabetic languages, and therefore, a universal multilingual IR framework can also be achieved. The ubiquity of the Internet, the proliferation of electronic information and the emergence of globalization offer us the challenge of engineering sophisticated techniques to process multilingual and heterogeneous information efficiently. Therefore, in the recent years, the IR community put an exclusive focus on Cross-language Information Retrieval (CLIR) to address this new challenge [33]. CLIR investigates information indexing and retrieval issues across the languages. CLIR is a special case of Monolingual Information Retrieval (MLIR), and addresses the retrieval issues where queries and documents are given in different languages. If either the query or the document collection can be  An Interlingua Approach  61  effectively translated into the target language, the CLIR problems can be reduced to MLIR problems. The commonly used techniques for CLIR include three different approaches: (1) query translation, (2) document translation, and (3) combination of both query and document translation. However, given the fact that the quality of machine translation (MT) is still well below the desired level, CLIR often takes advantage of multilingual dictionaries, thesauri or word or sentence -aligned parallel corpora to circumvent MT. Also, there are CLIR approaches, which tend to bypass MT by making use of multilingual conceptual ontology [8] or multilingual term association [36]. Successful CLIR systems for European languages (English, French, Spanish and German, etc.) are demonstrated using conceptual mapping and term association techniques. In this paper, we investigate mono- and cross- language IR for Japanese and Chinese using Kanji mapping and semantic association of Kanji-derived concepts – a Kanji-based Interlingua CLIR for these ideographic languages. Precisely speaking, we focus on the semantic information captured in Kanji and attempt to engineer the Kanji for effective mono- and cross- language information retrieval for Japanese and Chinese information. Unlike the characters (letters) of the non-ideographic languages (e.g., English, Arabic or Sanskrit), a single Kanji is capable of capturing significant semantic information within itself. However, single Kanji is ambiguous, and therefore, we attempted to index the Kanji through their explicit and implicit semantic contents. Despite our focus on these ideographic Asian languages, we have also included a discussion towards developing an Interlingua model of multilingual information processing, which is capable of handling other (non-ideographic) languages. The organization of this paper is as follows. We briefly discuss the special issues of Japanese and Chinese information retrieval (IR) in Section 2. We include a detailed literature review of Japanese and Chinese IR in Section 3. In Section 4, we discuss several encoding standards of Japanese and Chinese texts, and their relationships with the Unicode. Our Kanji oriented retrieval experiments include four different indexing approaches: (1) single Kanji indexing, (2) single Kanji with Kanji bi-gram indexing, (3) single Kanji with correlated Kanji pair indexing (i.e., indexing the Kanji pairs that have high co-occurrence tendencies), and (4) Kanji based semantic indexing (i.e., by extracting latent Kanji concepts after applying the dimensionality reduction techniques). In Section 5, we introduce the vector space IR model in terms of Kanji vectors and Kanji-document matrix. The detail mathematical formalism of Kanji Co-occurrence Tendency (KCT) and Kanji Semantic Indexing (KSI) are outlined in Section 6. Finally, we discuss our mono- and cross- language IR experiments in Section 7, followed by a discussion and analysis in section 8. Throughout the entire article, whenever appropriate, we draw analogical comparisons between our approach and those of others to justify the formalism and the benefit of the proposed Interlingua model for multilingual information indexing and retrieval. Readers who are familiar with the Japanese and Chinese language processing and vector space IR techniques, may skim through the introductory sections (Section 2, 3 and 4) and focus more on the later sections of this paper. Introductory sections are marked with asterisks.  62  Md. M. Hasan et al.  2*. Special Issues in Japanese and Chinese Information Retrieval In the following two Subsections, we will analyze the linguistic facets of the Japanese and Chinese languages, respectively, from an IR perspective. 2.1* Japanese IR An investigation into popular Japanese full-text IR systems (NAMZU and FREYA, etc.) revealed that most Japanese IR systems mimic the IR systems for European languages [12, 28]. The indexing and retrieval usually involve with the four major steps: (1) segmentation: to locate word boundaries, (2) morphological analysis: to find the word-stems, (3) representation and indexing of the stems: e.g., to use inverted file or other data-structures to represent the document collection, and (4) query-document similarity measurement: to associate documents with queries using cosine or other similarity measures. The first two steps, segmentation and morphological analysis, are computationally expensive complex tasks, and these preprocessing steps result in a loss of syntactical cues from the documents and the queries. Given also the fact that vector space representation of documents and queries is a flat representation (known as a “bag of words”), which ignores contextual information of the original documents and the queries [37], it makes sense to represent the documents and queries only in terms of Kanji. We will investigate several ways of indexing Kanji (sometimes associated with further processing, such as Kanji mapping and correlation, etc.) straightforwardly to bypass computationally intensive segmentation and morphological analysis. It can be noted that, for Japanese-Chinese CLIR, such an approach can bring us an added advantage because the query or the document translation steps can be easily circumvented with Kanji association.  An Interlingua Approach  63  Original Japanese Text Fragment: JIS コードで保存した日本語のテキスト Segmented and Annotated Representation: JIS /コード(code)/で(PREP)/保存した(saved)/日本語(Japanese)/の(PREP)/テキスト(text) Equivalent English Text Fragment: Japanese text saved in JIS code  Script-wise Representation  Kanji  保存, 日本語  Katakana  コード, テキスト  Hiragana:  で, した, の  Roman:  JIS  Katakana vs. their Kanji Definition  Katakana  Corresponding Kanji definition  コード:  符号, 記号 (情報を表現する),  弦楽器の弦, 電気器具や電灯線  テキスト:  本文, 文書, 原文, 原典  A Semantically Equivalent Kanji-based Representation: {JIS 符号保存日本語本文}  Figure 1 A Japanese text fragment with four different scripts and its maximum likelihood mapping to corresponding Kanji Japanese texts are usually written using four different scripts: Kanji, Katakana, Hiragana and Roman alphabet [39]. The following example of a short Japanese text fragment (in Figure 1) shows the combination of all four scripts. In running Japanese texts, however, Kanji is more dominating than any other scripts. In writing, ideographic Kanji can be replaced with its Hiragana spelling. However, the Hiragana spellings are ambiguous. Kanji expresses the semantic more obviously than its Hiragana spelling; and probably because of this reason, despite a long ongoing debate on replacing Kanji completely with the Hiragana, Kanji still remains a major component in Japanese writing. Katakana is mostly used to transliterate loan words (except those borrowed from Chinese). However, the phonetic scarcity of Japanese, made the Katakana transliteration quite ambiguous. For example, the Katakana string, コード may represent different English words: code, cord and chord, etc. Moreover, Katakana transliteration is often inconsistent; the English word, “digital” can be written as デジタル or ディジタル, for instance. Most Katakana strings are content bearing terms and therefore, in Japanese IR, special care has always been taken to efficiently process the Katakana strings. In the ordinary Japanese-English dictionaries, a Katakana  64  Md. M. Hasan et al.  entry appears with its original foreign word and a Japanese definition (often using Kanji). For example, the Katakana string, テキスト is defined with the relevant Kanji strings 文書, 原文, 原典 or 本文 as well as the English word, “text”. In our Kanji based IR approach, we propose mapping the Katakana strings onto the relevant Kanji with the help of dictionary definitions. Hiragana strings are mostly shorter functional words or inflectional components. Short Hiragana strings usually play syntactical and morphological roles, and are usually ignored. However, continuous and long strings of Hiragana are potentially a replacement1 for a rare and complicated Kanji (or a Kanji string), which, to some extent of accuracy, can also be replaced with the relevant Kanji. The Roman alphabet is usually mapped to the respective ASCII characters and indexed accordingly. For an effective Kanji-based information retrieval, we need to preprocess the Japanese documents and queries and represent them in terms of equivalent Kanji. Although sophisticated algorithms can be developed for Hiragana to Kanji and Katakana to Kanji mapping, we, for simplicity, use maximum likelihood based mapping strategy in this research. In Figure 1, we explain a Kanji based representation of the example text fragment using corresponding Kanji (the acronym, JIS, stands for the Japanese Industrial Standard). It is worthy to note here that the complexity and the computational costs of such preprocessing are lesser than those of full segmentation and morphological analysis. Another justification for processing Japanese IR in this way is the usability of such indexing for CLIR without machine translation. Nonetheless, mapping a Katakana string to its original language is another feasible option for multilingual IR. Although a single Kanji captures significant semantic information within itself, such information is highly ambiguous. Therefore, we also derived useful indexing information, such as, Kanji n-grams2, correlated Kanji pairs and principal components, automatically from the initial Kanji based representation for effective indexing and retrieval. 2.2* Chinese IR In comparison to Japanese IR, Chinese information retrieval is more straightforward because Chinese texts are mostly written homogeneously using only Kanji. However, like Japanese, Chinese text is also written without explicit word delimiters and therefore, segmentation must be performed to extract words from the string of Kanji for word or phrase level indexing. Since Chinese is a non-inflectional language, morphological analysis is not important. There are three major approaches to indexing Chinese text: (1) single Kanji indexing, (2) Kanji n-gram indexing, and (3) word or phrase level indexing (after segmentation). However, most practical systems incorporate more than one of the above indexing schemes  
This paper presents the mechanisms of and criteria for compiling a new learner corpus of English, the quantitative characteristics of the corpus and a practical example of its pedagogical application. The Taiwanese Learner Corpus of English (TLCE), probably the largest annotated learner corpus of English in Taiwan so far, contains 2105 pieces of English writing (around 730,000 words) from Taiwanese college students majoring in English. It is a useful resource for scholars in Second Language Acquisition (SLA) and English Language Teaching (ELT) areas who wish to find out how people in Taiwan learn English and how to help them learn better. The quantitative information shown in the work reflects the characteristics of learner English in terms of part-of-speech distribution, lexical density, and trigram distribution. The usefulness of the corpus is demonstrated by a means of corpus-based investigation of learners’ lack of adverbial collocation knowledge. Keywords: learner corpus, Taiwanese Learner Corpus of English (TLCE), Second Language Acquisition (SLA), English as Foreign Language (EFL), quantitative analysis, lexical density, collocation. 1. Introduction A computer corpus is a body of computerized written text or transcribed speech. Computer corpora are useful for a wide variety of research purposes, in fields such as lexicography, natural language processing, and all varieties of linguistics. The first computer corpus made its appearance in the early 1960s when two scholars at Brown University compiled a one-million-word corpus, known as the Brown Corpus [Francis & Kucera, 1964]. It contains a wide range of American English texts with grammatical annotation. For decades, this pioneering work was an important source for linguistic scholars who wished to perform quantitative as well as qualitative that is crucial for a broad coverage system. Third, a static WSD model is unlikely to be robust and portable, since it is very difficult to build a single model relevant to a wide * Department of Foreign Languages and Literature, National Sun Yat-sen University, Kaohsiung, Taiwan E-mail: hsuehueh@mail.nsysu.edu.tw  88  H. H. Shih  analysis of language structure and use [Francis & Kucera, 1982]. In the early 1970s, an equivalent British collection, the Lancaster-Oslo-Bergn (LOB) Corpus, was designed and compiled to facilitate comparative studies. Quantitative information on the distribution of various linguistic features in these two corpora became available [Johansson & Norheim, 1988; Nakamura, 1993]. The two corpora and other subsequently compiled corpora are similar in structure and size, and are considered to be first generation corpora. With the fast development in technology needed for text capture, storage and analysis, the scale of computer corpora has increased considerably, and a corpus of one million words seems to be inadequate for large scale studies on lexis. In the early 1980s, the publisher Collins and Birmingham University compiled the first mega-size corpus, the Cobuild Corpus, for the production of a new English dictionary. The scale of the corpus reached 13-million words by the time the dictionary was published in 1987 [Collins, 1987]. In preparation for a new generation of language reference publications, the corpus was transformed into the Bank of English in 1991 and has been growing larger in size ever since. Another well-known mega-corpus, the British National Corpus, was compiled between 1991 and 1994 by a consortium of academics and publishing houses. This corpus consists of 100 million words of part-of-speech tagged contemporary written and spoken British English. Access to the corpus was originally restricted within Europe, and it was not until very recently that the corpus was made accessible worldwide. Due to the need for comparative studies of different English varieties as in the first generation, the International Corpus of English compilation project was launched in the 1990s [Greenbaum, 1996] to gather written and spoken forms of national varieties of English throughout the world. The project aims to collect up to 20 subcorpora, each containing one million words of English used in countries where English is the first language, and in countries such as India and Singapore where English is an additional office language. The corpus will enable researchers to use each national subcorpus independently for descriptive research and also to undertake comparative studies. For nearly fifty years, machine-readable language corpora have greatly benefited people in both linguistics and publishing houses. Linguistic scholars have been able to better understand language structure and use with the aid of quantitative data. Publishers have produced new pedagogical tools that reflect the real use of language. However, it was not until the 1990s that scholars in the EFL and SLA sectors began to recognize the theoretical as well as practical potential of corpora and to believe that with the aid of quantitative information, computer learner corpora can form an authoritative basis for obtaining further insights into the interlanguage systems of language learners. Publishing houses also realize the vital role that learner corpora play on designing EFL tools, which can be improved “with the NS (native speaker) data giving information about what is typical in English, and the NNS (non-native speaker) data highlighting what is difficult for learners in general and for specific groups of learners” [Granger, 1998a] However, it is difficult to create learner corpora on the huge scale of native corpora mainly because each collection is usually confined to classroom language.  Compiling Taiwanese Learner Corpus of English  89  In 1993 the International Corpus of Learner English (ICLE) was launched [Granger, 1993] through academic collaboration worldwide. At present, the corpus contains 14 different national varieties, some of which are subdivided regionally, and each subcorpus contains 200,000 words. A great deal of comparative research has been done based on the ICLE, providing statistics-based interpretation of the learners’ lexicon, grammar, and discourse [Granger, 1998b]. Another learner corpus, and probably the largest corpus of single group learners so far, is the Hong Kong University of Science and Technology Learner Corpus [Milton & Tong, 1991], which consists of five million words of written English from Cantonese learners. This corpus is intended to be used for the development of English teaching materials in Hong Kong. SLA scholars in Japan soon followed the trend, and several learner corpus projects were launched, such as the JEFLL corpus of around 200,000 words from Japanese EFL learners’ written data, the SST Corpus of 1 million spoken words of learners, and the CEJL Corpus of junior high school to university students. In China, Chinese Middle School Students’ Written English and Chinese Middle School Students’ Spoken English are two learner corpora forming the Corpus of Middle School English Education that was compiled at South China Normal University beginning in 1998. Apart from academic circles, publishing houses such as Longman and Cambridge University Press have also compiled their own learner corpora for the development of their own language related publications. While many countries around the world have been creating their own learner corpora, little work has been done in Taiwan. The Soochow Colber Student Corpus [Bernath, 1998], which was compiled between 1984 and 1995 at Soochow University, can be viewed as a pioneering Taiwanese corpus of learner English. It contains around 227,000 words of written text from junior and senior students of Soochow University and National Taiwan University. No other corpus of comparable size was compiled until 1999 when a one-million-word learner corpus project, the Taiwanese Learner Corpus of English, was launched at Sun Yat-sen University. This corpus is a collection of written data from college students majoring in English at the university. The data has been annotated for various linguistic features using the TOSCA-ICLE tagger/lemmatizer [Aarts, Barkema, & Oostdijk, 1997], assigning to each word its lemma and a tag of its morphological, syntactic and semantic information. With the permission of the compiler of the Soochow Colber Student Corpus to incorporate 85% of its contents, consisting of written data from students majoring in English, the scale of the TLCE has increased from its original 530,000 to 730,000 words. The corpus continues to grow in size. Currently, the TLCE is probably by far the largest annotated learner corpus of English in Taiwan. In the following sections, a complete description of the TLCE will be given, including its purpose, design criteria, method of data capture and documentation, corpus structure and grammatical annotations. The quantitative characteristics of the TLCE as well as its pedagogical application will be depicted and illustrated at the end of the paper.  90  H. H. Shih  2. Compilation of the TLCE  2.1 Purpose The history of the computer learner corpus is less than a decade old, but it has been widely considered as “a useful resource for anyone wanting to find out how people learn languages and how they can be helped to learn them better” [Leech, 1998]. Learner output is indeed hard data that SLA scholars can utilize to depict learners’ interlanguage systems. The TLCE has been compiled in the hope that it will become a useful resource for SLA scholars who want to understand the internal learning process of Taiwanese learners of English, and in the hope that with corpus-based research findings, EFL teachers will be able to tailor their teaching to students’ needs.  2.2 Corpus Design Criteria  It is important to have clear design criteria when compiling a learner corpus because of the heterogeneous nature of learners and learning situations. Clear criteria help make it possible to interpret research results correctly and help justify the results of comparative studies on different corpora.  Table 1 shows the design criteria of the TLCE. The subjects who have contributed data to the corpus are students majoring in English at the three universities, ranging from freshmen to seniors (aged 19 to 22). Their English proficiency varies from intermediate to advanced levels. The TLCE includes written production of two different genres, namely, informal writings and essay writings. Informal writings consist of daily or weekly journals, which the learners are encouraged to keep during their writing courses, and essay writings are the compositions they are asked to submit regularly for their courses. The types of compositions are mainly descriptive, narrative, expository and argumentative.  Table 1. TLCE Design Criteria  attributes  age  19-22  level  Intermediate to advanced  Mother tongue Chinese  Learning context EFL  medium  Written text  genre  journals and compositions  2.3 Data capture and documentation The data of the TLCE are in three forms: electronic files, printouts and handwritten texts. More than half of the collection has been submitted through e-mail, which is the easiest way of gathering data for the corpus. E-mail or Microsoft Word files are converted into text files. Another source of data, learners’ printouts, have been scanned and transformed into a machine readable format. Post-editing of the scanned data is  Compiling Taiwanese Learner Corpus of English  91  necessary to remove scanning errors. The most time-consuming task is the collection of handwritten texts; all the data have to be keyboarded. As the issue of spelling errors is not a concern in the project and errors would hinder part-of-speech tagging in the subsequent annotation work, all the data in the corpus are spellchecked. The documentation of each piece of writing is needed for researchers to create their own subcorpora according to selection based on pre-defined attributes, and to carry out different comparison studies. For this reason, details about attributes are recorded as an SGML file header for each text. The information includes the university where the learner is studying, the academic year in which the text is collected, the school year (proficiency level) of the learner, and the genre of the text. For instance, the header <#nsysu-891-f-DES> indicates that the text is a descriptive composition written by a freshman at Sun Yat-sen University in the first semester of the 1989 academic year.  2.4 Corpus structure  As stated in Section 2.2, journals and compositions are the two genres of writing collected in the corpus. Journals are informal writings from students, recording what concerns them the most during a day or a week. The journals are sent to their teachers through e-mail systems. Compositions are the essay writings based mainly on different writing strategies: description, narration, exposition and argumentation. The first two are often taught in the first year at universities, whereas the expository and argumentative types are practiced in the second and the third years. Table 2 illustrates the structure of the corpus, including the total numbers of texts and words, and the percentage of the corpus each genre represents.  Table 2. The Structure of the TLCE  Text Types journal composition Description/narration  Total number of texts Total number of words Proportion (%)  823  213091  29.4  435  134363  18.5  (first year)  Exposition/argumentation (second/third years)  738  others  109  333734  46.1  43156  6.0  As indicated in the table, the ratio of journals to compositions in the corpus stands at around 3 to 7. Expository and argumentative types of writings are most numerous, making up more than 46% of the whole corpus. Data classified as others came originally from the Soochow Colber Student Corpus with type labels that did not fit into the TLCE categories. For instance, they are labeled as autobiographical writings, letters, imaginative writings or creative writings.  92  H. H. Shih  2.5 Grammatical Annotation Computer corpora are either raw corpora or annotated corpora. Raw corpora simply contain plain text, whereas annotated corpora have extra encoded features obtained through part-of-speech tagging or syntactic parsing. Part-of-speech tagging is a process of attaching a category and probably other attributes to each word, whereas syntactic parsing provides the structural analysis of each sentence. The former is usually done automatically by rule-based, probabilistic or mixed taggers, and the average tagging accuracy is about 95%; the latter can be done by automatic full/partial parsers outputting one or more syntactic structures for a sentence. The text in the TLCE is currently part-of-speech tagged using the TOSCA-ICLE tagger [Aarts et al., 1997]. TOSCA-ICLE is a stochastic tagger, supplemented with a rule-based component, which tries to correct observed systematic errors of the statistical components. Each word is given its lemma, and a part-of-speech tag, which consists of a major wordclass label, followed by attributes for subclasses and for its morphological information. There are 17 major word classes in the tag set (see Appendix A) and a total of 270 different attribute combinations. 3. Quantitative Analysis A major advantage of the corpus approach lies in the usefulness for conducting quantitative analysis. The quantitative features of a corpus provide a basic but global view of the characteristics of the learners’ writings. The following findings depict the characteristics of the TLCE as a learner corpus. 3.1 Part-of-speech Distribution Figure 1 shows the part-of-speech distribution of the corpus. The graph only indicates those parts of speech individually making up at least 5% of the total corpus. As can be seen, Nouns (N) and verbs (VB) exist in similar proportions in the corpus. Pronouns (PRON) are third, followed by prepositions (PREP), adverbs (ADV), adjectives (ADJ), articles (ART) and conjunctions (CONJUNC). Note that the words in nominal form (N or PRON) make up nearly one third of the whole corpus.  Compiling Taiwanese Learner Corpus of English  93  CONJUNC 5% ART 6% ADJ 7% ADV 7% PREP 8% PUNC 12%  N 19% VB 18% PRON 14%  Figure 1. POS Distribution  3.2 Type/Token Ratio (Lexical Density) For open classes, N, VB, ADV, and ADJ, it is desirable to know their type/token ratios. The type-token ratio, also called the lexical density, is often used as a measure of the lexical complexity of a text. Here, it is used as the measure of the word versatility of an open class. It is the ratio of different words to the total number of words in the class and is calculated by the formula Lexical _ Density = number _ of _ separate _ words(type) *100 . total _ number _ of _ words(token) Although N and VB have similar distributions as shown in Figure 1,their lexical densities show great discrepancy. As can be seen in Figure 2, the lexical density of N is four times higher than that of VB. This phenomenon is also found in the pair consisting of ADJ and ADV, where ADJ has a much higher density value than ADV. In other words, although the frequency counts of VB and ADV in the learner corpus are similar to those of N and ADJ, respectively, the variety of actual words used in the categories of VB and ADV is much more limited than in the N and ADJ categories.  94  H. H. Shih  9 8 7 6 5 4 3 2 1 0 Lexical Density  N 7.122685279  VB 1.765860039  ADV 1.931505188  ADJ 8.557947727  Figure 2. Lexical Density 3.3 Part-of-speech trigrams A POS trigram is a pattern of three adjacent POSs. It reveals to a certain extent the habitual use of syntactic structures by language learners. The corpus has a total of 777,096 trigrams from 2202 different patterns. Hence, the type-token ratio of POS trigrams is as low as 2.8. Table 3 shows the distribution of the front rank trigram patterns according to frequency of use. As can be seen, the first 50 patterns make up a large proportion of use in the distribution diagram. In fact, it is calculated that the top 220 ranking patterns make up 82% of the trigrams. In other words, learners use only 10% of the POS trigram patterns in 80% of their writings. These figures demonstrate the serious lack of structural variations in learners’ writings.  Compiling Taiwanese Learner Corpus of English  95  Frequency  25000 20000 15000 10000 5000 0 1  51 101 151 201 251 301 Rank of Trigrams  Figure 3. Trigram Distribution  4. Pedagogical Application The main purpose of compiling the TLCE is to provide Taiwanese researchers in the SLA and EFL areas with a large quantity of authentic learner data, which can be used to conduct qualitative analysis based on quantitative information. With the availability of this useful resource, they can utilize advanced corpus analysis tools to systematically uncover the features of non-nativeness existing in learner English. The findings will enable EFL teachers to focus on areas where remedial work is needed. In this section, a pedagogical application of the TLCE is demonstrated through an investigation of learners’ lack of adverbial collocation knowledge from both overuse and underuse perspectives. A series of experiments were carried out based on a contrastive approach, comparing learner English (from the TLCE) with native English (from a one-million-word subset of the BNC). 4.1 Top 10 adverbs in the BNC and the TLCE A frequency list of adverbs with the “-ly” suffix was obtained from each corpus, and their top 10 adverbs were taken into consideration. The left column of Table 3 shows the top 10 adverbs used by the learners, and the right column shows those used by the native speakers. The bracketed number following an adverb indicates the adverb’s rank in the other corpus.  96  H. H. Shih  Table 3. Top 10 adverbs in the two corpora.  Top NTLCE(learner)  BNC (native)  -----------------------------------------------------------------------  
Usually, there are various non-alphabet symbols (“/”, “:”, “-”, etc.) occurring in Mandarin texts. Such symbols may be pronounced more than one oral expression with respect to its sense category. In our previous works, we proposed the multi-layer decision classifier to disambiguate the sense category of non-alphabet symbols; the elementary feature is the statistical probability of token adopting the Bayesian rule. This paper adopts more features of tokens in sentences. Three techniques are further proposed to improve the performance. Experiments show that the proposed techniques can disambiguate the sense category of target symbols quite well, even with small size of data. The precision rates for inside and outside tests are upgraded to 99.6% and 96.5% by using more features of token and techniques. Key Words: Multi-layer decision classifier, Bayesian rule, word sense disambiguation, voting scheme, pattern table. 1. Introduction Various homographs or non-alphabet symbols in the Mandarin (but not limited to) occur frequently. The patterns containing these symbols may be pronounced with respect to its semantic sense. The non-alphabet symbols are defined: the symbols which are not the Mandarin characters (字) and may be pronounced different oral expressions. We call such phenomenon oral ambiguity. The purpose of word sense disambiguation (WSD) is to identify the most possible category among candidate’s sense category. It is important to disambiguate the word sense automatically for the natural language processing (NLP). Many works [Brown etc., 1991], [Fujii and Inue,1998] and [Ide and Veronis,1998], addressed WSD problems in the past. In our previous works [Hwang, etc., 1999a; Hwang, etc., 1999b], we proposed the ψ Correspondence author. 67  multi-layer decision classifier (MLDC) to predict the sense category, in which the voting scheme is used to predict the final category. Even though the domains of sense in the paper just focus on three non-alphabet symbols, the proposed approach can be extended into other symbols in Mandarin and related ambiguity problems. The features of token and improving techniques described in this paper will be employed in the 2nd layer classifier. The main domain will focus on the improvements for the 2nd layer decision classifier. The model of our previous works is regarded as the baseline system. Comparing with the baseline model, the proposed features of token and techniques in this paper improve the performance of inside test from 97.8 to 99.6% and outside test from 93.0 to 96.6%. The paper is organized as follows: related information and previous works will be described first. Section 3 elaborates the principal techniques for 2nd layer classifier in MLDC. Section 4 focuses on the evaluation for empirical features. Some improving techniques are proposed in section 5. The conclusions are presented in last Section. 2. Description of Related Works In this Section, we first describe the applications of word sense disambiguation. The precious literatures on WSD and several methods, which are used to disambiguate the sense categories and classification problems of ambiguity, will be introduced next. Finally we will illustrate our previous approach. 2.1 Applications of Word Sense Disambiguation The applications of WSD in natural language processing include the following domains: • Content and thematic analysis Analyzing the distribution of pre-defined categories of words in text. • Information retrieval and extraction When querying information, in a standalone system or Internet environment, the system should identify the real meaning for the query; excluding unnecessary data then correctly return desirable information among heterogeneous data. • Machine translation We can first disambiguate the word sense categories, and then translate the word into correct semantic meanings associated with the target word. • Speech processing Within the text analysis phase of TTS synthesis, the sense ambiguity of non-alphabet symbols or homographs should be resolved. The patterns containing such symbols can be translated into their oral expressions. The problem dealt with in our paper is very important for the precise speech output of TTS system. 68  2.2 Related Works A lot of literatures have been published on word sense disambiguation in the past. They range from dictionary-based to corpus-based approaches. The former is dependent on the definitions of machine readable dictionary (MRD) [Veronis, etc., 1990] while the later usually rely only on the frequency of word extracted from the text corpus to construct the feature database [Schutze, etc.,1995]. Corpus-based approach adopts the co-occurrence of words which are extracted from the large text corpora to construct the feature database [Leacock, 1993] and provides the advantage of being generally applicable to new text, domains and corpus without the costly, error-prone parsing and semantic analysis. However, corpus-based approach also has some weakness: the corpus is always hard to collect and is time-consuming. The situation is so called “knowledge acquisition bottleneck” [Gale etc., 1992]. Based on the type of context in examples, the classifiers for word sense category use two contextual information: local and topical context. Hearst, etc. [1999] use local context with a narrow syntactic parse, in which the context is segmented into noun phrases, verb groups and other groups. Gale etc.[1992] developed a topical classifier, in which the Bayesian rule is used and the only information adopted is the co-occurrence of unordered word. With respect to the contextual information, lexical information is formalized form of information involved in each surrounding word. Lee etc. [1997] adopt the discrimination score, based on maximum entropy of surrounding words in a sentence, to discriminate the word sense. Its precision rate is 80 % average. Yarowsky [1994 and 1997] build a classifier using the local context cues within ± k windows for target word. A log-likelihood ratio is generated, which stands for the strength of each clue of local context. The decision will be made for matching sorted ratio sequence to decide the sense category of target word. The average performance ranges from 96% to 97% while the domain size of sense is only 2 for all ambiguous questions. 2.3 Our Previous Works In contrast to 2-gram, 3-gram and n-gram language models, our previous paper [Hwang, etc., 1999a, 1999b] proposed an approach of multi-layer decision classifiers, which can resolve the category ambiguity of oral expression for non-alphabet symbols. A two-layer classifier has been developed. The first layer decision classifier can be viewed as decision tree based on the linguistic knowledge. Some impossible categories will be excluded while the remaining categories are all the possible categories. The second classifier employs a voting scheme to predict the final category with maximum probability score. The precision rates for inside and outside testing are 97.8% and 93.0% average. 69  3. The Principal Techniques At first, the data set and sense categories for three target symbols are described. In 2nd  decision classifier, a voting scheme, derived from Bayesian rule, is used to predict the  portable sense category with maximum score.  3.1 Elementary Information of Data Set  The original data set is collected through different source, including: Academic Sinica  Balance Corpus (ASBC), text files downloaded from Internet. ASBC is composed of 316 text  files which contain 5.22M characters in Mandarin, English and other symbols totally [Huang,  1995; CKIP, 1995]. Only the sentence with such non-alphabet symbols will be extracted and  appended into the empirical data set. Examples of three non-alphabet symbols slash (/), colon  (:) and dash (-) are extracted and appended into our empirical data set. The sentence size of  three non-alphabet symbols is 1115,1282 and 1685 respectively. The ratio of training and  testing set is 4:1 appropriately. These sentences will be classified into different sense category  with respect to target symbols. The sense categories and their oral expressions are listed in  Tables 1-3. Less frequent (less than 1%) sense categories will be neglected.  Word segmentation paradigm is based on the Academia Sinica Chinese Electronic  Dictionary (ASCED), which contains about 78,000 words. The words in ASCED are  composed of one to 10 characters. Our principal rule of segmentation is first subject to  maximal length of words and then to least number of words in a segmented pattern sequence.  The priority scheme is that the segmented word sequence, which contains a word of maximal  length, will be chosen. If two sequences have same maximum length of words, we compare  further the total number of words in such sequences; then the sequence that is composed of  least number of words will be chosen. The same segmentation’s priority will be adopted  within the training phase and testing phase.  There are several categories which speech for non-alphabet symbol “/” are silence; the  duration for silence in prosodic parameter is still different to other senses. During the  synthesis processing in TTS system, the duration with respective to its category will be varied  and decided with respect to prosody needed. The numbers of token and sentence for three  target symbols in our feature database are listed in Table 4. Table 1: Seven sense categories and their related oral expressions of the target symbol “/”.  category 1. date  lexical patterns with non-alphabet  symbol “／”  oral expression in Mandarin  ３／４(March 4th)  三月四日  data dis. (%) 15.96  2. fraction  ３／４(three fourth)  四分之三  8.88  3. time(music)  ３／４(three four time)  四分之三拍  17.52  4. path, directory ／ｄｅｖ／ｎｕｌｌ  斜線ｄｅｖ斜線ｎｕｌｌ  25.69  5, computer words Ｉ／Ｏ  Silence or 斜線  2.04  6. production version ＶＡＸ／ＶＭＳ  Silence (longer pause) or 斜線  5.52  7. others  中／日／韓文(China/Japan/Korea) Silence (longer pause)  25.45  70  Table 2: Five sense categories and its related oral expressions of target symbol “:”.  Sense category １. punctuation  lexical patterns with non-alphabet symbol “：” 優點：經濟省時  oral expression in Mandarin 優點(silence)經濟省時  data dis. (%) 32.64  ２. time  ３：２０PM  下午三點二十分(three twenty PM)  11.63  ３. versus ４. telephone ５. expression  ３：２０ TEL：４２６４８５６ 教練表示：照常進行  三比二十(three versus twenty) 電話(silence)４２６４８５６ 教練表示(silence)照常進行  13.39 8.50 33.43  Table ３: Seven sense categories and its related oral expressions of target symbol “-”.  Category １. figure, address ２. interval ３. production ４. computer term ５. tel. fax ６. hyphen ７. minus  lexical patterns with non-alphabet symbol “－” 圖２－１(Figure 2-1) ６－９月份營業收入 ｐｃ－ｃｉｌｌｉｏｎ Ｅ－Ｍａｉｌ 電話：４２６－４８５６ 登記地點－圖書館前 公式：Ｘ－２＝２０  oral expression in Mandarin  data dis. (%)  圖２之１ ６至９月份營業收入 ｐｃ(silence)ｃｉｌｌｉｏｎ Ｅ(silence)Ｍａｉｌ 電話：４２６(silence)４８５６ 登記地點(silence)圖書館前 公式：Ｘ減２等於２０  7.64 21.05 17.01 5.91 21.91 24.22 2.23  Table 4: numbers of token and sentence for three target symbols.  sn  
This paper introduces a Chinese summarizier called ThemePicker. Though the system incorporates both statistical and text analysis models, the statistical model plays a major role during the automated process. In addition to word segmentation and proper names identification, phrasal chunk extraction and content density calculation are based on a semantic network pre-constructed for a chosen domain. To improve the readability of the extracted sentences as auto-generated summary, a shallow parsing algorithm is used to eliminate the semantic redundancy. 
Users' queries for Web search are usually short. For example, the average length of TREC topic description for conventional text retrieval is 15 tokens [11,12], while analyses of web search engine logs reveal that the average query length for Web search is about 2.3 tokens [6,9]. Short queries means that the information about the user’s intention provided to the search engine is very limited. To deal with the short query problem, interactive search techniques [2,7] which attempt to identify the user’s intentions and suggest more precise query terms are therefore commonly incorporated in Web search engine design. To determine more relevant query terms for each given query, the conventional 115  interactive search processes often rely on the key terms in the retrieved documents [2,7,10]. The key term set is extracted either statically from the documents during preprocessing or dynamically on-the-fly. Since the precision rates of the retrieved documents are usually not high enough, the extracted key terms are often found not relevant and not very helpful in practical Web search services. In fact, extraction of relevant terms can be carried out by analyzing users’ logs. In recent years, mining search engine logs has been obtaining more attention. Silverstein et al. [9] performed a second-order analysis on a log with a huge number of Web query terms. The results are then used to facilitate phrase recognition and query expansion [3]. In this paper, we propose a new approach based on log analysis for developing more effective interactive Web search engines. The most important feature of the proposed approach is that the suggested terms are extracted from similar query sessions, rather than from the contents of the retrieved documents. A query session is defined as a sequence of search requests issued by a user for a certain information need. The basis of the proposed approach is that two users with the same information need will issue common or related query terms. For example, in search for a subject regarding “search engine technology”, a user may submit query terms such as “search engine”, “Web search”, “Google”, “Web search and multimedia”, while another user may submit “Web search”, “Lycos”. Therefore, if similar query sessions could be identified, query terms for the same information need can be extracted and applied to improve the effectiveness of search engines. The remainder of the paper will be organized as follows. Section 2 is a brief 116  introduction to the idea of interactive search based on similar query sessions. The method proposed for segmenting query sessions from proxy logs will be described in Section 3. Then, how query sessions are clustered is addressed in Section 4. Section 5 will present some experiment results and a conclusion is given in Section 6. 2. Interactive Search Based on Similar Query Sessions Fig.1 is an abstract diagram showing our idea for interactive search. Before introducing the basic idea of the proposed approach, the concept of query session is presented and defined below: Definition of Query Session: Query session = (ID, R1,…, Rm) where ID means the identifier of a user submitting a sequence of requests to a search engine in a certain period of time. Each request Ri = (ti, qi) means user ID sends a query term q to the search engine at time t The proposed approach is assumed that the query space of users is formed by clusters of users’ query sessions, and a set of query sessions grouped in the same clusters contain similar information needs. For each input query session with a sequence of i query terms, the interactive search process is then designed to retrieve the most similar cluster of query sessions from the query space, and then extract relevant terms in the cluster as suggested terms for next search. Once the i+1th query term is selected, it forms a new query session with i+1 terms and the interactive process will perform again. 117  Query Space  Document Space  Similar Sessions Ri-1, Ri, Ri+1 Input Query Session Suggested Terms  Search Subjects Popular Visited Pages with the Subject  Fig.1 An abstract diagram showing our idea for interactive search.  Based on the above definition and idea, the problem to be dealt with is then formulated.  The Query Session Clustering Problem For a set of query sessions from a query session log, the considering problem is to cluster these query sessions into different groups based on estimated similarity between query sessions. Each cluster can be defined as {Si| f(Si, Sj) > threshold}, in which f() is the similarity estimation function between query sessions.  Overview of the Proposed Approach  The proposed approach, as shown in Fig. 2, is composed of three processing modules: query session segmentation module, query session clustering module and relevant term extraction module. In the stage of query session segmentation, each query 118  session will be segmented and extracted from a proxy log, according to the time gap between successive search requests. All of the extracted query sessions will form as a query session log. In the session clustering stage, the sessions with similar queries will be clustered and the cluster names extracted from composed high frequency terms. In the relevant term extraction stage, the relevance between the recorded query terms will be calculated and sets of relevant terms will be extracted for term suggestion applications in a search engine.  Proxy Log  The Proposed Approach Query Query Session SegmSeesnstiaotniosn Query Session Clustering  Relevant Term Extraction  Term Suggestion in Search Engine  Extracted Relevant Terms  Fig.2 An overview of the proposed approach 3. Query Session Segmentation A common proxy server might easily have thousands of clients accessing the web through it. Not only the general HTTP requests could pass through the proxy server, all of search HTTP requests are same. Compared with common search engine logs, a proxy server's log records more rigid information for users' information access and, more importantly, the recorded search requests are not limited to certain search engines. 119  However, a proxy log might record too much information and only some of them are useful in terms of search engine applications [13]. In our application it is sufficient to only use the following fields of logging information: y A timestamp that indicates when a search request was submitted. y A client address that indicates the IP address of the requesting instance. y A URL string that contains the request content.  Since the experiments are just performing, the testing log is from NTU proxy servers and is still small. Some statistics of the testing proxy log are listed in Table 1.  Logging Days No. of Total Clients No. of Total Queries No. of Distinct Queries  15 days (2000/4/22 00:00~ 2000/5/7 00:00) 12,005 341,443 51,125  Table 1. Some statistics of the testing proxy log.  It is noted that the recorded search queries in the log are limited to that for two representative search engine sites in Taiwan: www.kimo.com.tw and www.yam.com.tw.  In addition to identifying unique users, an effective query session segmentation algorithm has to determine which are the starting and ending requests for each user's information need. Most of search requests posse a property of time locality. Client ID with temporal information really provides a strong constraint in determining the query sessions. For this reason, we adopt an assumption similar to Silverstein et al. 120  that queries for a single information need come clustered in time, and then there is a gap before the user returns to the search engine. The method for query session segmentation is then proposed as follows: The Method for Query Session Segmentation: For a proxy log, it will segment the whole log L = {Ti| where Ti = (IDk, ti, qi)} into a set of query sessions {Si| Si = (IDk, R1,…, Rm), where Ri = (ti, qi), and ti – ti-1 < threshold}, where ti is the timestamp when the query qi issued. Analysis of Segmented Query Sessions To realize the performance of the above method, several experiments have been performed. Fig. 3 shows the relationship between the time thresholds and the numbers of segmented query sessions. The time thresholds determine the maximum time gap between two successive requests from the same client. The values of the time thresholds were tuned from 0 seconds to 360 seconds. In the research of Silverstein et al, 5 minutes as suggested is a proper threshold value. With the same threshold value, the number of segmented query sessions is shown in Table 2. The percentages of the segmented singleton and non-singleton query sessions are found similar to those reported by Silverstein et al. 121  No. of Sessions  30000 25000 20000 15000 10000 5000 0 0  > 1 query  50  100  150  200  250  300  350  400  Threshold (seconds)  Fig 3. The numbers of segmented query sessions (that with more than 1 queries), regarding to the change of increasing time thresholds.  
Abstract Computer learner corpora have been widely used by SLA/EFL specialists since mid 1990s to gain better insights into authentic learner language. The work presented in this paper examines the inter-language of Taiwanese learners of English from a part-of-speech sequence perspective. Two pre-tagged corpora (one learner corpus and one native corpus) are involved in this work. The experimental results indicate that there are more than one third of eligible POS trigrams that are never practiced by the Taiwanese learners in their writing and the learners have stronger preference than native speakers in using pronouns, especially right after punctuations, verbs and conjunctions. 
• ability to handle real-world (or unrestricted) text, and • either ease of adaptation to a certain domain or application or generality in order to cover a wide range of domains or applications. * University of Patras, Department of Electrical& Computer Engineering,26500Patras, Greece. E-mail: stamatatos@wcl.ee.upatras.gr. t University of Patras, Department of Electrical& Computer Engineering, 26500Patras, Greece. E-mail: fakotaki@wcl.ee.upatras.gr. University of Patras, Department of Electrical& Computer Engineering, 26500Patras, Greece. E-mail: gkokkin@wcl.ee.upatras.gr. © 2001 Association for Computational Linguistics  Computational Linguistics  Volume 26, Number 4  The two main factors that characterize a text are its content and its style, both of which can be used for categorization purposes. Nevertheless, the literature on computational stylistics is very limited in comparison to the work dealing with the propositional content of the text. This is due to the lack of a formal definition of style as well as to the inability of current NLP systems to incorporate stylistic theories that require complicated information. In contrast to traditional stylistics based on formal linguistic theories, the use of statistical methods in style processing has proved to be a reliable approach (Biber 1995). According to the stylostatisticians, a given style is defined as a set of measurable patterns, called style markers. We adopt this definition in this study. Typical classificatory tasks in computational stylistics are the following: • Text genre detection concerns the identification of the kind (or functional style) of the text (Karlgren and Cutting 1994; Michos et al. 1996; Kessler, Nunberg, and Schi.itze 1997). • Authorship attribution concerns the identification of the author of the text (Holmes and Forsyth 1995; Baayen, Van Halteren, and Tweedie 1996; Tweedie, Singh, and Holmes 1996). These tasks have so far been considered completely separate problems. A typical text categorization system utilizing stylistic analysis (i.e., either text genre or authorship identification) is usually based on the following modules: . Extraction of style markers: A set of quantifiable measures are defined and a text-processing tool is usually developed, to automatically count them. 2. Classification procedure: A disambiguation method (e.g., statistical, connectionist, etc.) is applied to classify the text in question into a predefined category (i.e., a text genre or an author). The most important computational approaches to text genre detection have focused on the use of simple measures that can be easily detected and reliably counted by a computational tool (Kessler, Nunberg, and Sch~itze 1997). To this end, various sets of style markers have been proposed (Karlgren and Cutting 1994), all of which are, in essence, subsets of the set used by Biber (1995), who ranked registers along seven dimensions by applying factor analysis to a set of lexical and syntactic style markers that had been manually counted. In general, the current text genre detection approaches try to avoid using existing text processing tools rather than taking advantage of them. Authorship attribution studies have focused on the establishment of the authorship of anonymous or doubtful literary texts, such as the Federalist Papers, 12 of which are of disputed authorship (Mosteller and Wallace 1984; Holmes and Forsyth 1995). Typical methodologies deal with a limited number of candidate authors using long text samples of several thousand words. Almost all the approaches to this task are based mainly on distributional lexical style markers. In a review paper of authorship attribution studies, Holmes (1994) claims: "yet, to date, no stylometrist has managed to establish a methodology which is better able to capture the style of a text than that based on lexical items" (p. 87). To the best of our knowledge, there is still no computational system that can distinguish the texts of a randomly chosen group of authors without requiring human assistance in the selection of both the most appropriate set of style markers and the most accurate disambiguation procedure.  472  Stamatatos, Fakotakis, and Kokkinakis  Text Categorization  In this paper we describe an approach to text categorization in terms of genre and author based on a new stylometric method that utilizes already existing NLP tools. In addition to the style markers relevant to the actual output of the NLP tool (i.e., the analyzed text), we introduce analysis-level style markers, which represent the way in which the text has been analyzed by that tool. Such measures contain useful stylistic information and are easily available without additional computational cost. To illustrate, we apply the proposed technique to text categorization tasks for Modern Greek corpora using an already existing sentence and chunk boundaries detector (SCBD) in unrestricted Modern Greek text (Stamatatos, Fakotakis, and Kokkinakis 2000). We present a set of small-scale but reasonable experiments in text genre detection, author identification, and author verification tasks and show that the performance of the proposed method is better in comparison with the most popular distributional lexical measures, i.e., functions of vocabulary richness and frequencies of occurrence of the most frequent words. Our approach is trainable and can be easily adapted to any set of stylistically homogeneous categories. We begin by discussing work relevant to text genre detection and authorship attribution focusing on the various types of style markers employed (Section 2). Next, we describe the proposed solution for extracting style markers using already existing NLP tools (Section 3) and apply our method to Modern Greek (Section 4), briefly describing the SCBD and proposing our set of style markers. The techniques used for automatic categorization of the stylistic vectors are discussed in Section 5. Section 6 deals with the application of our approach to text genre detection, and Section 7, with authorship attribution, for both author identification and author verification. In Sections 8 and 9, we discuss important performance issues of the proposed methodology and the conclusions that can be drawn from this study. 2. Current Trends in Stylometry The main feature that characterizes both text genre detection and authorship attribution studies is the selection of the most appropriate measures, namely, those that reflect the style of the writing. Various sets have been proposed in the literature. In this section, we classify the most popular of the proposed style markers, taking into account the information required for their calculation rather than the task they have been applied to. 2.1 Token-Level Measures The simplest approach considers the sample text as a set of tokens grouped in sentences. Typical measures of this category are word count, sentence count, character per word count, and punctuation marks count. Such features have been widely used in both text genre detection and authorship attribution research since they can be easily detected and computed. It is worth noting that the first pioneering works in authorship attribution, when no powerful computational systems were available, were based exclusively on these measures. For example, Morton (1965) used sentence length for testing the authorship of Greek prose, Brinegar (1963) adopted word length measures, and Brainerd's (1974) approach was based on distribution of syllables per word. Although such measures seemed to work in specific cases, they became subject to heavy criticism for their lack of generality (Smith 1983, 1985). 2.2 Syntactic Annotation The use of measures related to syntactic annotation of the text is very common in text genre detection. Such measures provide very useful information for the exploration  473  Computational Linguistics  Volume 26, Number 4  of the characteristics of style (Biber 1995). Typical paradigms are passive count, nominalization count, and counts of the frequency of various syntactic categories (e.g., part-of-speech tags). Recently, syntactic information has also been applied to authorship attribution. Specifically, Baayen, Van Halteren, and Tweedie (1996) used frequencies of occurrence of rewrite rules as they appear in a syntactically annotated corpus and proved that they perform better than word frequencies. Their calculation requires tagged or parsed text, however. Current NLP tools are not able to provide accurate calculation results for many of the previously proposed style markers. In the study of register variation conducted by Biber (1995), a subset of the measures (i.e., the simplest ones) was calculated by computational tools and the remaining were counted manually. Additionally, the automatically acquired measures were counterchecked manually. Many researchers, therefore, try to avoid the use of features related to syntactic annotation in order to avoid such problems (Kessler, Nunberg, and Sch~itze 1997). As a result, the recent advances in computational linguistics have not notably affected research in computational stylistics. 2.3 Vocabulary Richness Various measures have been proposed for capturing the richness or the diversity of the vocabulary of a text and they have been applied mainly to authorship attribution studies. The most typical measure of this category is the type-token ratio V/N, where V is the size of the vocabulary of the sample text, and N is the number of tokens of the sample text. Similar features are the hapax legomena (i.e., words occurring once in the sample text) and the dislegomena (i.e., words occurring twice in the sample text). Since text length dramatically affects these features, many researchers have proposed functions of these features that they claim are text length independent (Honor6 1979; Yule 1944; Sichel 1975). Additionally, instead of using a single measure, some researchers have used a set of such vocabulary richness functions in combination with multivariate statistical techniques to achieve better results in authorship attribution (Holmes 1992). In general, these measures are not computationally expensive. However, according to results of recent studies, the majority of the vocabulary richness functions are highly text length dependent and quite unstable for texts shorter than 1,000 words (Tweedie and Baayen 1998). 2.4 Common Word Frequencies Instead of using vocabulary distribution measures, some researchers have counted the frequency of occurrence of individual words in the sample text. Such counts are a reliable discriminating factor (Karlgreen and Cutting 1994; Kessler, Nunberg, and Schi~tze 1997) and have been applied to many works in text genre detection. Their calculation is simple, but nontrivial effort is required for the selection of the most appropriate words for a given problem. Morever, the words that best distinguish a given group of authors cannot be applied to a different group of authors with the same success (Holmes and Forsyth 1995). Oakman (1980) notes: "The lesson seems clear not only for function words but for authorship word studies in general: particular words may work for specific cases such as 'The Federalist Papers' but cannot be counted on for other analyses" (p. 28). Furthermore, the results of such studies are highly language dependent. Michos et al. (1996) introduce the idea of grouping certain words in categories, such as idiomatic expressions, scientific terminology, formal words, and so on. Although this solution is language independent, it requires the construction of a complicated computational mechanism for the automated detection of the categories in the sample text. Alternatively, the use of sets of common high-frequency words (typically 30 or 50 words) has been applied mainly to authorship attribution studies (Burrows 1987).  474  Stamatatos, Fakotakis, and Kokkinakis  Text Categorization  NCPtoo, I  A ,ys,s i measures J  Figure 1 The proposed method.  L. measures j  The application of a principal components analysis on the frequencies of occurrence of the most frequent words achieved remarkable results in plotting the texts in the space of the first two principal components, for a wide variety of authors (Burrows 1992). This approach is language independent and computationally inexpensive. Various additional restrictions to this basic method have been proposed (e.g., separation of common homographic forms, removal of proper names from the most frequent word list, etc.), aimed at improving its performance. For a fully automated system, such restrictions require robust and accurate NLP tools. 3. The Proposed Method Our method attempts to exploit already existing NLP tools for the extraction of stylistic information. To this end, we use two types of measures, as can be seen in Figure 1: • measures relevant to the actual output of the NLP tool (i.e., usually tagged or parsed text), and • measures relevant to the particular methodology by which the NLP tool analyzes the text (analysis-level measures). Thus, the set of style markers is adapted to a specific, already existing NLP tool, taking into account its particular properties. Analysis-level measures capture useful stylistic information without additional cost. The NLP tool is not considered a black box. Therefore, full access to its source code is required in order to define and measure analysis-level style markers. Moreover, tool-specific knowledge, rather than languagespecific knowledge, is required for the definition of such measures. In other words, researchers using this approach can define analysis-level measures based on their deep understanding of a particular NLP tool even if they are not familiar with the natural language to which the methodology is to be applied. To illustrate the proposed method, we apply it to Modern Greek using the SCBD, an existing NLP tool able to detect sentence and chunk boundaries in unrestricted text, as described in the next section. In addition to a set of easily computable features (i.e., token-level and syntax-level measures) provided by the actual output of the SCBD, 475  Computational Linguistics  Volume 26, Number 4  we use a set of analysis-level features, i.e., measures that represent the way in which the input text has been analyzed by the SCBD. The particular analysis-level style markers can be calculated only when this specific computational tool is utilized. However, the SCBD is a general-purpose tool and was not designed for providing stylistic information exclusively. Thus, any NLP tool (e.g., part-of-speech taggers, parsers, etc.) can provide similar measures. The appropriate analysis-level style markers have to be defined according to the methodology used by the tool in order to analyze the text. For example, some similar measures have been used in stylistic experiments in information retrieval on the basis of a robust parser built for information retrieval purposes (Strzalkowski 1994). This parser produces trees to represent the structure of the sentences that compose the text. However, it is set to "skip" or surrender attempts to parse clauses after reaching a time-out threshold. When the parser skips, it notes that in the parse tree. The measures proposed by Karlgren (1999) as indicators of clausal complexity are the average parse tree depth and the number of parser skips per sentence, which in essence are analysis-level style markers. 4. Style Markers for Modem Greek As mentioned above, the subset of style markers used for Modern Greek depends on the text analysis by the specific NLP tool, the SCBD. Thus, before describing the set of style markers we used, we briefly present the main features of the SCBD. 4.1 Description of the SCBD The SCBD is a text-processing tool able to deal with unrestricted Modern Greek text. No manual preprocessing is required. It performs the following procedures: Sentence boundary detection: The following punctuation marks are considered potential sentence boundaries: period, exclamation point, question mark, and ellipsis. A set of automatically acquired disambiguation rules (Stamatatos, Fakotakis, and Kokkinakis 1999) is applied to every potential sentence boundary in order to locate the actual sentence boundaries. These rules utilize neither lexicons with specialized information nor abbreviation lists. Chunk boundary detection: Intrasentential phrase detection is achieved through multiple-pass parsing making use of an approximately 450-keyword lexicon (i.e., closed-class words such as articles and prepositions) and a 300-suffix lexicon containing the most common suffixes of Modern Greek words. Initially, using the suffix lexicon, a set of morphological descriptions is assigned to any word of the sentence not included in the keyword lexicon. If the suffix of a word does not match any of the entries of the suffix lexicon, then no morphological description is assigned to this word. It is marked as a special word and is not ignored in subsequent analysis. Then, each parsing pass (five passes are performed) analyzes a part of the sentence, based on the results of the previous passes, and the remaining part is kept for the subsequent passes. In general, the first passes try to detect simple cases that are easily recognizable, while the last passes deal with more complicated ones. Cases that are not covered by the disambiguation rules remain unanalyzed. The detected chunks are noun phrases (NPs),  476  Stamatatos, Fakotakis, and Kokkinakis  Unrestricted Text  i,  Sentence Boundary  I:!  Detection  II  ...............  Assignment of  ~ t_.._._.__._._.l  Morphological Descriptions H  ~_ ~ = ~ .....~ . . . . . . . . . . . . I Multiple-Pass Parsing  ~,X~K~yv~r~ts I ~ [-_.._.______.J  Text Categorization  Parsed Text Figure 2 The SCBD structure.  prepositional phrases (PPs), verb phrases (VPs), and adverbial phrases (APs). In addition, two chunks are usually connected by a sequence of conjunctions (CONs). The SCBD is able to cope rapidly with any piece of text, even ill-formed text, and its performance is comparable to more sophisticated systems that require more complicated resources. Figure 2 gives an overview of the SCBD. An example of its output for a sample text, together with a rough English translation (included in parentheses), is given below (note that special words, those that do not match with any of the stored suffixes, are marked with an asterisk): VP[&eu 0gAcouoz pg{oo(I don't want to pour)] NP[A&& (oil)] PP[crrr/9~wr~& (in the fire)] CON[of&kale(but)] VP[rr~¢re4a; (I believe)] CON[drL (that)] NP[r/ err~fldpvu~rr/(the encumbrance)] PP[o-rou rrpo~rcoko7Lcr#6 (of the budget)] PP[ozrr6 rov¢ flov&evrg¢ (by the deputies)] VP[&u #rcopeg uce rrpoe#erpeirc~L (can not be measured)] #6uo (merely) PP[#e rc~ 5*&¢.*6px. rcou c~uc,Spo#~n&u (with the 5 bil. Dr. of the retroactive salaries)] troy (that) NP[rc~po~u re&evrcdc~ (they took lately)] VP[rrponc~&cburc~¢(causing)] NP[rr/(Sva~op&~ r~]g ~oLu~¢7v,&#r/~ (the discontent of the public opinion)]. It is worth noting that we did not modify the structure of the SCBD in order to calculate style markers, aside from adding simple functions for their measurement. 4.2 Stylometric Levels Our aim during the definition of the set of style markers was to take full advantage of the analysis of the text by the SCBD. To this end, we included measures relevant to the actual output of this tool as well as measures relevant to the methodology used by the SCBD to analyze the text. Specifically, the proposed set of style markers comprises three levels: • Token Level: The sample text is considered as a set of tokens grouped in sentences. This level is based on the output of the sentence boundary  477  Computational Linguistics  Volume 26, Number 4  detector: Code M01 M02 M03  Description detected sentences/words 1 punctuation marks/words detected sentences/potential sentence boundaries  Phrase Level: The sample text is considered as a set of phrases (i.e., chunks). This level is based on the output of the chunk boundary detector:  Code M04 M05 M06 M07 M08 M09 M10 Mll M12 M13  Description detected NPs/total detected chunks detected VPs/total detected chunks detected APs/total detected chunks detected PPs/total detected chunks detected CONs/total detected chunks words included in NPs/detected NPs words included in VPs/detected VPs words included in APs/detected APs words included in PPs/detected PPs words included in CONs/detected CONs  Analysis Level: Measures that represent the way in which the sample text has been analyzed by the particular methodology of the SCBD are included here:  Code M14 M15 M16 M17 M18 M19 M20 M21 M22  Description detected keywords/words special words/words assigned morphological descriptions/words chunks' morphological descriptions/total detected chunks words remaining unanalyzed after pass 1/words words remaining unanalyzed after pass 2/words words remaining unanalyzed after pass 3/words words remaining unanalyzed after pass 4/words words remaining unanalyzed after pass 5/words  It is clear that the analysis level contains extremely useful stylistic information. For example, M14 and M15 are valuable markers that indicate of the percentage of highfrequency words and the percentage of unusual words included in the sample text, respectively. M16 is a useful indicator of the morphological ambiguity of the words and M17 indicates the degree to which this ambiguity has been resolved. Moreover, markers M18 to M22 indicate the syntactic complexity of the text. Since the first parsing passes analyze the most common cases, it is easy to understand that a large part of a syntactically complicated text would not be analyzed by them (e.g., high values for M18, M19, and M20 in conjunction with low values for M21 and M22). Similarly, a syntactically simple text would be characterized by low values for M18, M19, and M20.  
Joe sneezed,  sneeze(joe) Joe sneezed.  He laughed,  laugh(joe) Joe laughed.  Bill laughed too. laugh(bill) Bill laughed.  We will assume that if the context is loosely specified enough to permit alternative realizations of the same content, then different versions of the same text could be generated or analysed: (1) Joe sneezed and laughed. Bill laughed too. Joe sneezed. Joe and Bill laughed. Joe sneezed. He laughed. So did Bill.  SomervilleCollege,OxfordOX2 6HD. E-mail: stephenpulman@linguistics-phil°l°gy'°xac'uk  Q 2001Associationfor Computational Linguistics  Computational Linguistics  Volume 26, Number 4  Secondly, we illustrate the formalism and the general approach by developing an account of contextual interpretation based on a kind of quasi-logical form and giving an account of a fragment (in the sense of Montague [1974a]) that treats several core phenomena of English contextual dependence. We also have some theoretical objectives: the particular approach illustrated here, like most computational approaches to contextual interpretation, uses an intermediate quasi-logical form representation level. Using such a level of representation incurs an obligation to say what it means ("no notation without denotation"). We try to show how the theory presented here leads to a natural semantics for these quasi-logical forms, and indeed leads to a truth theory for contextually dependent interpretation that supports a natural consequence relation, and one appropriate for cases where interpretations are not fully specified. We relate this approach both to the classical tradition of formal linguistic semantics exemplified by Davidson (1972) and Montague (1974b) and more recent literature on the use of underspecification in semantics. The structure of the paper is as follows: In the next section we give an outline of the formalism and illustrate with the small fragment of English that has been implemented within this framework. We present analyses of the contextual interpretation of pronouns, definites, ellipsis, focus, and quantifier scope. There is far more to say about each of these phenomena, of course, and the analyses here are by no means claimed to be definitive. The aim is merely to show that we can, to a first approximation, provide a reasonably fully worked out description of these phenomena in a truly bidirectional way. We then go on to compare the current approach with that of some other theories with similar aims: the "standard" version of quasi-logical form implemented in the Core Language Engine, as rationally reconstructed by Alshawi and Crouch (1992) and Crouch and Pulman (1994); underspecified Discourse Representation Theory (Reyle 1993); and the "glue language" approach of Dalrymple et al. (1996). Finally, we discuss some of the semantic and logical issues raised by the approach described here, in particular the extent to which the theory meets the desiderata for accounts of underspecification outlined by van Eijck and Jaspars (1996), and the extent to which the theory supplies a methodologically satisfactory account of truth and interpretation for sentences involving contextually dependent constructs. 2. Contextual Interpretation The major components and assumptions of the approach to contextual interpretation here are as follows: . We assume that the output of grammatical processing of a sentence is a quasi-logical form, henceforth QLF. Of course, for anything other than a trivial grammar, a given sentence will typically yield many QLFs. We will assume that syntactic and lexical disambiguation have taken place and that the only things still needed for a complete interpretation are the resolution of constructs like pronouns, definites, ellipsis, and so on. We return later to issues concerning robustness of linguistic coverage and to the interleaving of contextual disambiguation with syntactic and semantic processing. For concreteness, we are assuming here that QLFs are built using a simple unification grammar formalism of the type described in Pulman (1996), and that a chart parser and semantic head-driven generator are  498  Pulman  Bidirectional Contextual Resolution  used for the analysis of sentences to QLFs and vice versa. But little of this detail is essential to our main aims: a wide range of grammatical formalisms and interpreters would be compatible with the basic assumptions of the contextual interpretation mechanism, assuming only that the same grammatical description is used in both the analysis and generation direction. What is required is that QLFs are, as here, expressed in a typed higher-order logic, augmented with constructs representing the interpretation of context-dependent elements (pronouns, ellipsis, focus, etc.). These constructs correspond as directly as possible to properties of the linguistic structure that express them and are, to as small an extent as possible, dependent on the requirements of contextual resolution (unlike, say, the metavariables of standard QLFs [Alshawi and Crouch 1992], or the labels of UDRS [Reyle 1996], which are motivated entirely by the mechanisms that operate on them after grammatical processing). Syntactic properties relevant for binding constraints, parallelism, scope constraints, and so on, are not directly represented at QLF (again unlike standard QLFs) but are assumed to be available as components of the linguistic context.~ . The context-independent meanings of sentences, which we refer to as resolved logical forms (RLFs), are expressed in the "ordinary" subset of the QLF language. A fully resolved RLF can be directly evaluated for truth: it contains no QLF constructs. Since it is just an expression of "ordinary" logic, it could serve as a knowledge representation and reasoning language, and thus the output of some information system producing such representations could in principle feed directly into generation (modulo well-known "equivalence of logical form" problems). . "Contexts" are here modeled by sets of sentences in the RLF subset of this language, with some kind of salience ordering on them (recency, in the implementation), about which we say nothing more. These sentences may, but need not, arise from prior linguistic processing. Contexts contain information about the form as well as the content of previous utterances, as mentioned earlier. Context sentences may also reflect features of the nonlinguistic context gained by direct observation or inference. This is a very minimal theory of context. We need to be able to reason about context, hence we need it represented in a logic. We need to be able to refer to properties of the form of linguistic utterances as well as their content, hence context must contain this information too. We obviously need some nonlinguistic information. We also need some structure to reflect the fact that not all components of the context are relevant to everything, hence salience. This is all we need for the time being, although there is clearly much more to be said.  
t University of Edinburgh, ICCSand Informatics, 2, BuccleuchPlace, EH8 9LW Edinburgh UK. E-maih Massimo.Poesio@ed.ac.uk 
1. Introduction Aspectual classification maps clauses (e.g., simple sentences) to a small set of categories in order to reason about time. For example, events, such as, You calledyourfather, are distinguished from states, such as, You resembleyourfather. The ability to distinguish stative clauses from event clauses is a fundamental component of natural language understanding. These two high-level categories correspond to fundamental distinctions in many domains, including the distinctions between diagnosis and procedure in the medical domain, and between analysis and activity in the financial domain. Stativity is the first high-level distinction made when defining the aspectual class of a clause. Events are further distinguished according to completedness (sometimes called telicity), which determines whether an event reaches a culmination or completion point at which a new state is introduced. For example, I made afire is culminated, since a new state is introduced--something is made, whereas I gazed at the sunset is nonculminated.  * Computer Science Dept., 1214 Amsterdam Ave., New York NY 10027. E-mail: evs@cs.columbia.edu t Computer Science Dept., 1214 Amsterdam Ave., New York, NY 10027. E-mail: kathy@cs.columbia.edu @ 2001 Association for Computational Linguistics  Computational Linguistics  Volume 26, Number 4  Aspectual classification is necessary for interpreting temporal modifiers and assessing temporal entailments (Moens and Steedman 1988; Dorr 1992; Klavans 1994) and is therefore a required component for applications that perform certain natural language interpretation, generation, summarization, information retrieval, and machine translation tasks. Each of these applications requires the ability to reason about time. A verb's aspectual category can be predicted by co-occurrencefrequencies between the verb and linguistic phenomena such as the progressive tense and certain temporal modifiers (Klavans and Chodorow 1992). These frequency measures are called linguistic indicators. The choice of indicators is guided by linguistic insights that describe how the aspectual category of a clause is constrained by the presence of these modifiers. For example, an event can be placed in the progressive, as in, She was jogging, but many stative clauses cannot, e.g., *She was resembling herfather (Dowty 1979). One advantage of linguistic indicators is that they can be measured automatically. However, individual linguistic indicators are predictively incomplete, and are therefore insufficient when used in isolation. As demonstrated empirically in this article, individual linguistic indicators suffer from limited classification performance due to several linguistic and pragmatic factors. For example, some indicators were not motivated by specific linguistic insights. However, linguistic indicators have the potential to interact and supplement one another, so it would be beneficial to combine them systematically. In this article, we compare three supervised machine learning methods for combining multiple linguistic indicators for aspectual classification: decision trees, genetic programming, and logistic regression. A set of 14 indicators are combined, first for classification according to stativity, and then for classification according to completedness. This approach realizes the potential of linguistic indicators, improving classification performance over a baseline method for both tasks with minimal overfitting, as evaluated over an unrestricted set of verbs occurring in two corpora. This serves to demonstrate the effectiveness of these linguistic indicators and thus provides a much-needed full-scale, expandable method for automatic aspectual classification. The results of learning are linguistically viable in two respects. First, learning automatically produces models that are specialized for different aspectual distinctions; the same set of 14 indicators are combined in different ways according to which classification problem is targeted. Second, inspecting the models resulting from learning revealed linguistic insights that are relevant to aspectual classification. We also evaluate an unsupervised method for this task. This method uses cooccurrence statistics to group verbs according to meaning. Although this method groups verbs generically and is not designed to distinguish according to aspectual class in particular, we show that the results do distinguish verbs according to stativity. The next two sections of this article describe aspectual classification and linguistic indicators. Section 4 describes the three supervised learning methods employed to combine linguistic indicators for aspectual classification. Section 5 gives results in terms of classification performance and resulting linguistic insights, comparing these results, across classification tasks, to baseline methods. Section 6 describes experiments with an unsupervised approach. Finally, Sections 7, 8, and 9 survey related work, describe future work, and present conclusions. 2. Aspect in Natural Language Because, in general, the sequential order of clauses is not enough to determine the underlying chronological order, aspectual classification is required for interpreting  596  Siegel and McKeown  Improving Aspectual Classification  Table 1 Aspectual classes. This table is adapted from Moens and Steedman (1988, p. 17).  Culminated Nonculminated  EVENTS  punctual  extended  CULMINATION CULMINATED  PROCESS  recognize  build a house  POINT  PROCESS  hiccup  run, swim, walk  STATES understand  even the simplest narratives in natural language. For example, consider: (1) Sue mentioned Miami (event). Jim cringed (event). In this case, the first sentence describes an event that takes place before the event described by the second sentence. However, in (2) Sue mentioned Miami (event). Jim already knew (state). the second sentence describes a state, which begins before the event described by the first sentence. Aspectual classification is also a necessary prerequisite for interpreting certain adverbial adjuncts, as well as identifying temporal constraints between sentences in a discourse (Moens and Steedman 1988; Dorr 1992; Klavans 1994). In addition, it is crucial for lexical choice and tense selection in machine translation (Moens and Steedman 1988; Klavans and Chodorow 1992; Klavans 1994; Dorr 1992). Table 1 sun~narizes the three aspectual distinctions, which compose five aspectual categories. In addition to the two distinctions described in the previous section, atomicity distinguishes punctual events (e.g., She noticed the picture on the wall) from extended events, which have a time duration (e.g., She ran to the store). Therefore, four classes of events are derived: culmination, culminated process, process, and point. These aspectual distinctions are motivated by a series of syntactic and entailment constraints described in the first three subsections below. Further cognitive and philosophical rationales behind these semantic distinctions are surveyed by Siegel (1998b). First we describe aspectual constraints that linguistically motivate the design of several of the linguistic indicators. Next we describe an array of semantic entailments and temporal constraints that can be put to use by an understanding system once input clauses have been aspectually classified. Then we describe how aspect influences the interpretation of temporal connectives and modifiers. Aspectual transformations are described, and we introduce the concept of a clause's fundamental aspectual category. Finally, we describe several natural language applications that require an aspectual classification component. 2.1 Aspectual Markers and Constraints Certain features of a clause, such as the presence of adjuncts and tense, are constrained by and contribute to the aspectual class of the clause (Vendler 1967; Dowty 1979; Pustejovsky 1991; Passonneau 1988; Klavans 1994; Resnik 1996; Olsen and Resnik 1997). Table 2 illustrates an array of linguistic constraints, as more comprehensively 597  Computational Linguistics  Volume 26, Number 4  Table 2 Several aspectual markers and associated constraints on aspectual class.  If a clause can occur: with a temporal adverb (e.g., then) in progressive as a complement of force~persuade after "Whathappened was..." with a duration in-PP (e.g., in an hour) in the perfect tense  then it must be: Event Extended Event Event Event Culminated Event Culminated Event or State  summarized by Klavans (1994) and Siegel (1998b). Each entry in this table describes an aspectual marker and the constraints on the aspectual category of any clause that appears with that marker. For example, a clause must be extended to appear in the progressive tense, e.g., (3) He was prospering in India extended event), which contrasts with, (4) * Y o uwere noticing that I can hardly be blamed ... (atomic event)) As a second example, since the perfect tense requires that the clause it occurs in must entail a consequent state, an event must be culminated to appear in the perfect tense. For example, (5) Thrasymachus has made an attempt to get the argument into his own hands (culminated event), contrasts with, (6) *He has cowered down in a paralysis of fear (nonculminated event). 2.2 Aspectual Entailments Table 3 lists several aspectual entailments. A more comprehensive list can be found in Klavans (1994) or Siegel (1998b). Each entry in this table describes a linguistic phenomenon, a resulting entailment, and the constraints on aspectual class that apply if the resulting entailment holds. For example, the simple present reading of an event, e.g., He jogs, denotes the h a b i t u a l reading, i.e., every day, whereas the simple present reading of a state, e.g., He appears healthy, entails at the moment. These entailments serve two purposes: They further validate the three aspectual distinctions, and they illustrate an array of inferences that can be made by an understanding system. However, these inferences can only be made after identifying the aspectual category of input clauses. 
Definition al and a2 corefer if and only if Referent(a1) = Referent(a2). Putting it simply: to determine whether al and a2 corefer, first determine Referent(a1) and Referent(a2), then see if they are equal. Ideally, of course, one would like to annotate many other semantic relations that hold between parts of a text, because they are also relevant for text interpretation. One candidate is the relation of anaphora. Loosely speaking--and glossing over some difficulties regarding the precise delimitation of anaphora (Sidner 1979; Partee 1989; van * Information TechnologyResearchInstitute, University of Brighton,LewesRoad, BrightonBN2 4GJ, UK. E-mail:Kees.van.Deemter@itri.bton.ac.uk t Mathematicaland Computing Science,GoldsmithsCollege,University of London, London SE146NW, UK. E-mail:R.Kibble@gold.ac.uk (~) 2001Associationfor Computational Linguistics  Computational Linguistics  Volume 26, Number 4  Deemter 1992)--an NP O~1 is said to take an NP a2 as its anaphoric antecedent if and only if al depends on Oz2 for its interpretation (e.g., Kamp and Reyle 1993). It follows that anaphora and coreference are different things. Coreference, for example, is an equivalence relation; anaphora, by contrast, is irreflexive, nonsymmetrical, and nontransitive. Secondly, anaphora, as it has just been defined, implies context-sensitivity of interpretation, and this is not true for coreference. For example, a name (President W. I. Clinton) and a description (Hillary Rodham's husband) can corefer without either of the two depending on the other for its interpretation. Anaphoric and coreferential relations can coincide, of course, but not all coreferential relations are anaphoric, nor are all anaphoric relations coreferential. (An example of the latter is bound anaphora, see Section 2.1.) Coreference annotation has been a focus of the Sixth and Seventh Message Understanding Conferences (MUC-6, MUC-7) and various other annotation exercises (e.g., Passoneau 1997; Garside, Leech, and McEnery 1997; Davies et al. 1998; Poesio 2000). In this squib, we intend to point at some fundamental problems with many of these annotation exercises, which are caused by a failure to distinguish properly between coreference, anaphora, and other, related phenomena. Because the MUC project is the bestknown example of coreference annotation, on which much subsequent work is based, and because of the public availability of the MUC Task Definition (TD, Hirschman and Chinchor [1997]), we will focus on MUC. Four criteria are listed for the MUC TD, in order of their priority (Hirschman and Chinchor 1997): . The MUC information extraction tasks should be supported by the annotations 2. Good (defined as ca. 95%) interannotator agreement should be achievable 3. It should be possible to annotate texts quickly and cheaply 4. A corpus should be created that can be used as a tool for linguists outside MUC. The TD makes it clear that the annotation task has been simplified in a number of ways. For example, only NPs were annotated. Such eminently sensible simplifications notwithstanding, we will argue that the above-mentioned criteria have not been achieved and that a rethinking of the coreference annotation enterprise is in order before it ventures into new domains involving speech, noisy data, etc. (see for example, Bagga, Baldwin, and Shelton [1999]), or before it extends the relation of coreference to cover whole/part and class/instance relations (e.g. Popescu-Belis 1998; Hirschman and Chinchor 1997). 2. Problems with Coreference Annotation In this section, some unclarities and inconsistencies will be discussed that we found in the literature on coreference annotation, and which appear to stem from confusion about what reference and coreference are. In Section 2.1, we will explore the tendency to apply coreference annotation to nonreferring NPs and bound anaphora, and we will argue that this tendency is problematic. In Section 2.2, we will argue that existing annotation enterprises still fail to respond properly to the well-known problem of how to annotate NPs that are used intensionally. In Section 2.3, we turn to a suggestion for the improvement of the actual process of annotation that has been made in the  630  van Deemter and Kibble  On Coreferring  literature, namely to separate the task of determining the "markables" from that of establishing coreference relations between them, showing that this separation is hard to maintain. At the end of each subsection, some suggestions (Remedies) will be made on how the problems m a y be tackled. These suggestions will be elaborated in Section 3. 2.1 Annotating Nonreferring NPs and Bound Anaphora The notion of reference is common to a broad variety of semantic theories (see Gamut [1991], Chapter 1, for discussion). W h e n s p e a k e r s / w r i t e r s use an N P to refer to an object or a set of objects, they try to single out the entity uniquely. Thus, when someone utters the NP the tenant of the house, the speaker may aim to single out a unique person, say Mr. X. Even when this is the case (i.e., the NP is used referentially rather than attributively1), the notion of referring has its problems. For example, the speaker may be mistaken in her belief that Mr. X is the tenant of the house (Donnellan 1966). In such cases it is unclear who is being referred to. Such problems notwithstanding, work on coreference annotation has usually taken the notion of reference for granted, on the assumption that clear cases, where the referent of an NP is clearly defined, outnumber the problematic ones, at least in some important types of discourse. Let us, for now, buy into the assumption that reference is a straightforward notion. Then, following Bach (1987) (especially Sections 3.2 and 12.2), for example, one thing that is clear about reference is that some NPs do not refer. When someone says (1) a. No solution emerged from our discussions, or b. Whenever a solution emerged, we embraced it, the solution NPs do not refer to any single solution, nor to any definite set of solutions. Most theorists would agree that they do not have a referent. Nonreferring NPs can enter anaphoric relations. (For example, the NP a solution is the (bound) anaphoric antecedent to it in (lb).) But if they do not refer, the c0reference relation as defined in Section i (which presupposes that Referent(olD and Referent(o~2) are defined) is not applicable to them. Even so, the MUC TD asks annotators to treat them as if it was applicable. It acknowledges (page 10) that "one may argue that [a bound anaphor and its antecedent] are not coreferential in the usual sense," but falls short of explaining explicitly what types of anaphora are to be annotated and how (Hirschman and Chinchor 1997).2 The annotation of bound anaphora merits some further elaboration. Consider, for example, quantifying NPs such as Every TV network (or, even more problematic, Most computational linguists [Hirschman and Chinchor 1997], see also Section 3). If Every TV network refers at all, then presumably it refers to the set of all TV networks (relevant to a certain domain). The TD, however, asks annotators to let Every TV network corefer with its in (lc). According to the definition of coreference, this means that Referent(Every TV network) = Referent(its) so that Referent(its) is the set of all TV networks, predicting incorrectly that (lc) means (ld): (1) c. Every T V network reported its proyqts cp. Every T V network reported every T V network's proyqts,  
 Betrayal in Self-Deception Dave Striver loved the university. He loved its ivy-covered clocktowers, its ancient and sturdy brick, and its sun-splashed verdant greens and eager youth. He also loved the fact that the university is free of the stark unforgiving trials of the business world---only this isn't a fact: academia has its own tests, and some are as merciless as any in the marketplace. A prime example is the dissertation defense: to earn the Ph.D., to become a doctor, one must pass an oral examination on one's dissertation. This was a test Professor Edward Hart enjoyed giving. Dave wanted desperately to be a doctor. But he needed the signatures of three people on the first page of his dissertation, the priceless inscriptions which, together, would certify that he had passed his defense. One of the signatures had to come from Professor Hart, and Hart had often said--to others and to himself--that he was honored to help Dave secure his well-earned dream. Well before the defense, Striver gave Hart a penultimate copy of his thesis. Hart read it and told Dave that it was absolutely first-rate, and that he would gladly sign it at the defense. They even shook hands in Hart's book-lined office. Dave noticed that Hart's eyes were bright and trustful, and his bearing paternal. At the defense, Dave thought that he eloquently summarized Chapter 3 of his dissertation. There were two questions, one from Professor Rodman and one from Dr. Teer; Dave answered both, apparently to everyone's satisfaction. There were no further objections. Professor Rodman signed. He slid the tome to Teer; she too signed, and then slid it in front of Hart. Hart didn't move. "Ed?" Rodman said. Hart still sat motionless. Dave felt slightly dizzy. "Edward, are you going to sign?" Later, Hart sat alone in his office, in his big leather chair, saddened by Dave's failure. He tried to think of ways he could help Dave achieve his dream.  Figure 1 An example of a story by BRUTUS1.  642  Book Reviews of necessary and sufficient conditions (DefB 8, pp. 98-99) in terms of actions, goals, and beliefs of two agents, the betrayer and the betrayed. This illustrates the sort of thematic knowledge in which BRUTUS's stories are grounded. Thematic knowledge is part of the knowledge level, which also comprises the following: . More general domain knowledge pertaining to the kinds of things that may constitute the subject matter of stories: agents, events, beliefs, goals, actions, and reaction (pp. 175-179); . Linguistic knowledge, pertaining to morphology, syntax, paragraph, and discourse structure (pp. 167, 180-182); . Literary knowledge, incorporating principles of storytelling, including story-grammars, designed to achieve specific literary objectives such as triggering imaging in the reader, causing the reading to project personal consciousness onto the characters. Literary knowledge also includes rhetorical tropes, evaluative valences, analogies, and image associations (pp. 182-185); . A special level of literary augmented grammars (LAGs) brings the rhetorical knowledge of the literary knowledge level to bear on the syntax controlled by the linguistic level (pp. 185-189). Story generation uses the knowledge of the kinds just listed in four types of developments through time at the process level: . Thematic concept instantiation sets the "stage" for a given story, exploiting thematic knowledge for a chosen theme; . Plot generation takes the characters and characteristics identified in the "stage"-setting phase and generates a scenario, using domain and thematic knowledge, particularly of action and reactive behavior. It results in a detailed scenario that generates consequences inferred from the knowledge level by production rules, and deploys a temporal sequence of events. The production rules follow ordinary first-order logic, but extensions are promised making use of temporal logic, conditional logic, and deontic logic, as well as "logics of action, deliberate action, intending etc." (p. 100). . Story structure expansion takes place on the basis of story grammars, at each choice-point of which sentence-types are produced. . Linguistic and literary knowledge are then used for language generation, that is, the production of specific linguistic structures down to the level of sentences, phrases, and words (pp. 194-197). An example of a production rule used in the sample story is as follows (p. 179): Rule committee_membersbehavior IF Candidate is some person and Thesis is the thesis of Candidate and the committee of the Candidate includes Member and Request_To Sign is some request and 643  Computational Linguistics  Volume 26, Number 4  Member is the requestee of Request_To_Sign and The requester of Request_To_Sign is the chairman of the committee and Thesis is the document of subject of Request_ToSign and Status of Request_To_Sign is pending THEN do answer(Member, Request_To_Sign) and do sign(Member, Thesis)  The literary themes at the heart of BRUTUS's stories are chosen for intrinsic interestingness: "The central thrust of BRUTUS's approach to story generation is ... to begin with a well-established, interesting literary theme like betrayal, and then work d o w n " (p. 199). So the interestingness is canned, and the complexity of the architecture makes for a startling air of creativity. But can interestingness be formalized? And is BRUTUS really creative? Some--such as Hofstadter and the Fluid Analogies Research Group (1995)--have boldly tackled the challenge of endowing machines with creativity. Naysayers--such as Dreyfus (1992)--have claimed to show that the enterprise is impossible in principle. The former have tended to be workers in AI, the latter philosophers. Bringsjord and Ferrucci are unusual in that they belong to both camps at once. They "cheerfully operate under the belief that human (literary) creativity is beyond computation--and yet strive to craft the appearance of creativity from suitably configured computation" (p. 149). I know of no reason to dispute the authors' claim (repeated on the website devoted to machine and book1) that BRUTUS is "the world's most advanced story generator." The approach is ingenious and thorough, and the results quite impressive. By contrast, the arguments against Strong AI, despite being couched in clean deductive form, remain unconvincing. Bringsjord and Ferrucci may seem to have stacked the deck against BRUTUS (and in favor of their philosophical thesis) by insisting that all computer storytelling must proceed exclusively by means of the traditional tools of databases and algorithms. Their "approach is a thoroughly logic-based one. Neural nets and dynamical systems are nowhere to be found in this volume" (p. 26). Their "explanation" of this strategy "is based on two stories, one involving Tchaikovsky, and the other involving Sherlock Holmes." The first notes that Tchaikovsky convinced audiences that his sixth Symphony was worth listening to by renaming it Path~tique, and talking of the range of emotional experiences he intended it to express. No robot composer, the authors assert, could provide that sort of gloss. The second alleges that even if some connectionist robot descended from MIT's COG were to emulate the powers of Sherlock Holmes, it could never explain how it accomplished its feats of inference without using language and deductive logic. Take the latter "explanation" first. Most of Sherlock Holmes's "deductions" are actually inductions, or crucially rest on prior--and often dubious--inductive inferences (such as "dogs always bark at strangers," in their example drawn from the story of Silver Blaze, p. 31). But suppose the detective's reasoning were exclusively logical. Bringsjord and Ferrucci stress the fact that the nature of this reasoning couldn't be "gleaned from neural nets" any more than our own could be read off a brain scan (p. 30), Quite, but so what? That doesn't show that the logical reasoning didn't super-  
(2) The girl heard by the window {that John was coming ] was speaking too loudly.} The boldface material represents two possible continuations of the first part of each sentence, but the choice of the appropriate continuation depends on how the parser initially structures the preceding material. In (1), the parser might choose to make the postverbal noun phrase either the direct object of the matrix verb, compatible with the first continuation, or the subject of an upcoming complement sentence, compatible with the second. In (2), the ambiguity centers upon whether heard is taken as an active main verb or as a passive participle inside a relative clause (the girl who was heard). Early experiments such as those of Frazier and Rayner (1982) suggested that speakers had a clear preference for the first continuation in each case, as verified by an increase in reaction time when the disambiguating word, underlined in examples above, forced the second continuation. In a case such as (1), the initial preference seems easy to reanalyze, while in (2), the correct analysis seems unrecoverable and is commonly referred to as a garden path. These facts were initially explained by a theory that assumed a serial parser. The choice function guiding the initial analysis was independently justified either by considerations drawn from linguistics (Gibson 1991, Weinberg 2000) or theories of memory (Frazier and Rayner 1982). Importantly, these theories assumed that listeners delayed consideration of semantic, pragmatic, or frequency-based factors until after an initial analysis was constructed. The present book highlights recent extensions and challenges to these theories.  648  Book Reviews Chapters by Richard Lewis (based on SOAR [Newell 1990]), by Paola Merlo and Suzanne Stevenson (based on a symbolic connectionist architecture), and by James Henderson (based on an extension of recurrent networks using Temporal Serial Variable Binding) try to derive initial parsing preferences and to distinguish possible from impossible reanalyses by means of architectural constraints defined in their underlying (implemented) models. The common idea here is that the distinction between an easy reanalysis and a garden path should fall out as a side effect of constraints that need to be imposed on the system in order to allow it to perform analysis on "normal" unambiguous input in an efficient manner. These models suggest that restrictions on how local the initial misattachment point is to the point of disambiguation, or how many constraints need to be specified in order for attachment to occur, determine the possibility of reanalysis. The fact that these factors characterize models with such different architectural bases suggests that these notions will play an important role in the design of any parser. Chapters by Charles Clifton, Jr., by Michael Tanenhaus, Michael Spivey-Knowlton, and Joy Hanna, by Gerry Altmann, by Steffan Corley and Matthew Crocker, and by Martin Pickering and Matthew Traxler debate the correctness of the simple statement of the preference function that chooses initial analyses in terms of a set of noninteracting nonsemantic or pragmatic constraints. Evidence in favor of a radically interactive constraint-satisfaction model comes from the nonstationarity of preferences during sentence comprehension. For example, the relative frequency with which a verb appears as either an active main verb or a passive participle, the compatibility of the subject noun phrase with interpretation as the agent of a clause (as required by the verb of a main-clause analysis), and the presence of a post-verbal by phrase (normally associated with a passive participle), contribute to the availability of the reduced relative clause, as shown by the ease of interpretation in (3). (3) The evidence examined by the lawyer turned out to be unreliable. In (3), evi&nce is a bad agent, and thus would disfavor the main-verb analysis of examined. The by phrase, which is associated with the passive-participle reading, also disfavors the main-clause reading. Trueswell, Tanenhaus, and Garnsey (1994) found no significant difference in reading time at the main verb (turned) when compared with an unambiguous control (formed by replacing examined with a verb like chosen). Noninteractive theorists claim that this means that the parser initially tries the wrong main-clause analysis, on the basis of structural factors, but that this analysis is rapidly revised. Interactive theorists claim, however, that this result follows simply from al- lowing factors like "agentivity" and "presence of by" to interact in the choice of the initial analysis. The chapter by Tanenhaus, Spivey-Knowlton, and Hanna presents a constraint-satisfaction implementation that allows us to test predictions of the interactive theory. They claim that the model can explain a range of hitherto contradictory findings. For example, they claim that experiments, such as that of Ferreira and Clifton (1986), using stimuli like (3) but without the by phrase, that were interpreted as demon- strating a stage where the parser abstracted away from issues of agentivity, could actually be predicted by an interactive model. This is because Ferreira and Clifton's stimuli did not include by phrases that could further bias the model to the relative- clause reading of the ambiguous verb, because the language as a whole is biased to treat main-verb/past-particle ambiguities as main verbs, and because the stimuli used verbs that were equibiased between a relative-clause and main-clause reading. These factors conspired in their simulation to outweigh any evidence from the nonagentivity of the subject, which would favor the relative-clause reading. By contrast, since 649  Computational Linguistics  Volume 26, Number 4  the stimuli in Trueswell, Tanenhaus, and Garnsey (1994) contained more factors that biased in favor of the relative-clause reading, an interactive model would predict that relative clauses would be easier to understand in these cases. On the other hand, two major criticisms of constraint-based models are leveled by Clifton in this volume. The first is that, while there is much evidence that one can make the dispreferred analysis a more fit competitor, one doesn't seem to be able to make the analysis preferred by purely structural constraints unfit (e.g., blocking the preferred interpretation in cases where it turns out to be correct). This is predicted by a model that gives particular constraints a first crack at analysis, but is not predicted by a fully interactive model. The second criticism is that it is too easy to produce models that can handle any range of data if there is no limit to the number and type of constraints that can interact. As more sources of constraint are proposed, this becomes a real problem. For example, Altmann's and Pickering and Traxler's chapters emphasize the contribution of plausibility as a source of constraint. Corley and Crocker try to deal with this second line of attack. While it is reasonable to believe that the mental lexicon contains entries for verbs that specify the likelihood of each possible complement type, considerations of sparse data make it less feasible to suppose that every lexical choice for possible adjective-noun pairs is stored. Nonetheless, there seems to be a clear difference between the ease of understanding of cases such as (4) and (5):  (4) The warehouse fires many of its employees every summer. (5) The corporation fires many of its employees every summer.  MacDonald (1994) suggested that this was due to the greater frequency of warehouse fires as an adjective-noun pair and the relative rarity of this part-of-speech analysis for corporation fires. Corley and Crocker try to show that these data do not force word-by-word bigram conditioning, and propose an alternative where only word to parbof-speech-category (unigram) and part-of-speech-category to part-of-speech-category (bigram) parameters are computed when assigning probabilities to ambiguous lexical items. Their model is inspired by standard part-of-speech taggers. This balancing between enriching the repertoire of constraints and keeping their number tractable and learnable should also occupy the field in the near future. Topics of the rest of the book include expanding the experimental paradigms used in sentence processing (to ERP work, as discussed in Colin Brown and Pater Hagoort's chapter), expanding the cross-linguistic coverage of the field (in chapters by Barbara Hemforth, Lars Konieczny, and Christoph Scheepers and by Marica De Vincenzi), and, in an interesting final section, semantic processing (chapters by Lyn Frazier, by Linda Moxey and Anthony Sanford, and by Amit Almor). This book represents the state of the art in sentence processing, with interesting examples and opportunities for computational modeling. The themes that it presents are likely to occupy the field for some time.  
The seminal positions from which the papers generally proceed are not directly represented in the volume, primarily Levin's work on English verb classes (Levin 1993), and Pustejovsky's generative lexicon (Pustejovsky 1995). Nevertheless, the currency of these constructs forms the context for the organization of the workshop and the volume. These positions differ from each other, naturally, but each has the effect of claiming that there are generalizations that may be captured between lexical items and either forms or functions, and thus that the lexical entry itself may be simpler, underspecified for elements of reference, or resident in a smaller, nonpolysemous lexicon. Such theoretical elegance appears at first examination to benefit efficiency at implementation time, which is what attracts computational linguistics to these approaches. However, the difficulties encountered in trying to extend their coverage at the same time delimit the level of their usefulness for system development, while helping the theorists further the instantiation of the paradigms implied by their theories. By the juxtaposition of theory and implementation we become aware of the problems at the poles of the current models: the claims they make are so general that numerous exceptions arise in implementation (like the Levin classes); or they require such specificity that their limited coverage (e.g., the conversion of a count noun to a mass noun in the case that the referent is an animal fur) makes implementation unproductive.  652  Book Reviews The volume is organized into four sections: "Lexical rules and underspecification," "Breadth of semantic lexicons," "Depth of semantic lexicons," and "Lexical semantics and pragmatics." The intent of this organization is to cover the considerations in building semantic lexicons, and the goal is achieved both in the characterization of the issues of the time and the general concerns that will have to be addressed for some time to come. 2. Lexical Rules and Underspecification This section concerns the assertion and application of lexical rules as a means of reducing the static size of the lexicon--that is, to distill out all predictable properties and create lexicons that contain only idiosyncratic information. The boundaries of such rules--for example, whether they should also cover phenomena hitherto considered morphological--form the basic body of issues in this section. "Categorization of types and application of lexical rules" by Boyan Onyshkevych describes the implementation issues associated with the coverage of lexical rules. This is a good organization and reference paper describing the implications for lexicon and processing efficiency of covering particular phenomena in lexical rules, and also of applying these rules (thereby extending the lexical corpus) at particular points in natural language processing systems. This paper actually reads like two separate papers, the latter half reviewing several lexical rule implementations. Also valuable as a reference work in this section is "The lexical semantics of English count and mass nouns" by Brendan Gillon. This paper provides a synopsis of the empirical contrasts of mass and count nouns, at both the phrase and clause levels in English. This paper is worth reading for the concise presentation of the phenomena and the pertinent syntactic tests. The same bent toward empiricism compels Gillon to be very careful about characterizing the lexical rules associated with the conversion from count to mass and vice versa, and as a result he seems hesitant to commit to more general claims that would combine the animal-meat and animal-fur phenomena (where the count noun animal term is converted to a mass noun). The remaining two papers in this section show some of the variety of thinking about the extent of lexical rules. Sehitoglu and Bozahin ("Lexical rules and lexical organization") describe a model of lexical rules for highly agglutinating languages such as Turkish, in which lexical rules handle inflectional and derivational morphology as well as classic lexical-semantic phenomena. Sanfilippo ("Word disambiguation by lexical underspecification"), using the Pustejovsky notion of "qualia" to decompose lexical reference, argues that lexical underspecification coupled with mechanisms for capturing local grammatical context can account for disambiguation without lexical rules. These two papers express near-extremes with respect to the articulation of lexical rules, but share the goal of reducing overgeneration of forms and interpretations. 3. Breadth of Semantic Lexicons This section of the volume is for the most part a contrast in views of Levin's verb classes. Both Dorr and Jones ("Acquisition of semantic lexicons") and Burns and Davis ("Building and maintaining a semantically adequate lexicon using CYC') attempt to take advantage of the encyclopedic work of Beth Levin (1993) in categorizing verbs according to an association of meaning and subcategorization frames. Levin's book does not claim to prove that there is a hard-and-fast correlation between the form of verb arguments and the meaning of the verb; rather, Levin "assumes" the association. Since its publication, many researchers have been torn between attempting to employ 653  Computational Linguistics  Volume 26, Number 4  the categorizations as a handy organization of English verbs, and trying to test the hypothesis that Levin appears to assert as axiomatic. Both the papers in this section that deal with Levin classes face this conflict. Dorr and Jones attempt to match Levin's classes with the subject-area codes in the Longman Dictionary of Contemporary English to automate lexical acquisition, using syntactic cues. Burns and Davis attempt to use Levin's classes to make generalizations about large groups of verbs for the naturallanguage linkage to the CYC knowledge base. In both cases, the authors find that Levin's work is useful as a comprehensive description of subcategorization patterns for lexical implementation, but much less reliable as a predictor of semantic behavior. Also in the section on breadth is the first of two papers by Nirenburg and Raskin, "Lexical rules for deverbal adjectives," in which the authors present an ontology-based method of characterizing lexical rules for adjectives. They introduce lexical rules intended to capture the semantic side of the derivation from verb to adjective, but find numerous exceptions, gaps, and departures from an already stretched assumption about what is deverbal (e.g., international). They add to the other voices in the volume who recognize the value of lexical rules only when they are practical for implementation. 4. Depth of Semantic Lexicons As Levin was the absent guest in the section on breadth, Pustejovsky's generative lexicon is a theme in the papers on depth. Two papers use the qualia structure to explain coverage phenomena in noun phrases. In "The adjective vieux: The point of view of 'Generative Lexicon'," Pierrette Boullion makes a strong claim for the value of generative lexical processes to account for the otherwise underivable range of meanings of adjectives like the French vieux 'old'. Here, both the scalar and property-modifying senses of vieux can be distilled down to a single meaning by showing that the scope of modification is not the reference of the modificand but rather parts of the semantic decomposition of it, and further that the syntactic distribution of the adjective is consistent with this explanation. Michael Johnston and Federica Busa approach noun compounds in terms of qualia structures in "Qualia structure and the compositional interpretation of compounds." Clues about noun compounding in both English and Italian suggest the means by which aspects of noun modifiers instantiate variables in the modificand. While this approach should appear promising for machine translation and multilingual information retrieval, other issues remain, such as ambiguity or overgeneration in noun compounds of three or more nouns. The holy grail of lexical semantics (in computational linguistics, at least), has been automatic accumulation of word senses in the right granularity, and ultimately the automatic disambiguation of words in text. Machine-readable dictionaries provide the well-known relations of hyponymy and meronymy as hierarchical relations, but also can provide clues about collocational facts that will enable automatic disambiguation. Jen Chen and Jason Chang ("Integrating machine readable dictionary and thesaurus ...') investigate these principles as discoverable from the LongmanDictionaryofContemporaryEnglish merged with other collections such as Roget'sThesaurus, the LongmanLexicon of Contemporary English, and WordNet. Applying information retrieval-techniques, they show the potential of forming word-sense clusters across multiple collections to achieve senses for disambiguation that are neither too coarse-grained nor too finegrained to be useful for text handling. Burstein, Wolff, and Lu ("Using lexical semantic techniques to classify free-responses") present a very engaging application of lexical semantics to educational testing in the scoring of flee-word test responses. A comprehensive review of lexical ap-  654  Book Reviews proaches finds no one perfect model for representing paraphrases, but relying on a sublanguage-dependent model demonstrates that concept-based lexicons for scoring written responses show promise. 
* Groningen University Institute for Drug Exploration, Department of SocialPharmacy and Pharmacoepidemiology,Ant. Deusinglaan 1, 9713 AV Groningen, The Netherlands. E-mail: marc@farm.rug.nl t Faculty of Health Sciences,Department of Health Ethics and Philosophy, P.O. Box 616, 6200MD Maastricht, The Netherlands. E-mail: rein.vos@zw.unimaas.nl :~Max Planck Institute for Psycholinguistics,P.O. Box 310, 6500AH Nijmegen. E-mail:baayen@mpi.nl (~) 2000 Association for Computational Linguistics  Computational Linguistics  Volume 26, Number 3  150 - 100 o.i Z 50 0  I ]lrll,,,.,,  5  10  .......  15  20  0.15  Z  0.10  Z  0.05  5  10  15  20  Frequency Class (a)  Frequency Class (b)  Figure 1 Frequency distribution of medical expert word types. Panel (a) shows the number of side-effect-related word types as judged by a medical expert (Nexpert) as a function of the first 23 frequency classes. Panel (b) shows the proportion of expert types/total corpus types (Ntotal) for the first 23 frequency classes. The horizontal dashed line indicates the mean proportion of 0.0619.  It is common practice in information retrieval to discard the lowest-frequency words a priori as nonsignificant (Rijsbergen 1979). In Smadja's collocation algorithm Xtract, the lowest-frequency words are effectively discarded as well (Smadja 1993). Church and Hanks (1990) use mutual information to identify collocations, a method they claim is reasonably effective for words with a frequency of not less than five. A frequency threshold of five seems quite low. Unfortunately, even this lower frequency threshold of five is too high for the extraction of side-effect-related terms from our medical abstracts. To see this, consider the left panel of Figure 1, which plots the number of side-effect-related words in our corpus of abstracts as judged by a medical expert, as a function of word-frequency class. The side-effect-related words with a frequency of less than five account for 295 of a total of 432 expert words (68.3%). The right panel of Figure 1 shows that the first 23 word-frequency classes are characterized by, on average, the same proportion of side-effect-related words. The a priori assumption of Rijsbergen (1979) that the lowest-frequency words are nonsignificant is not warranted for our data, and, we suspect not for many other data sets as well. The recent literature has seen some discussion of the appropriate statistical methods for analyzing the contingency tables that contain the counts of how a word is distributed inside and outside the windows around a seed term. Dunning (1993) has called attention to the log-likelihood ratio, G2, as appropriate for the analysis of such contingency tables, especially when such contingency tables concern very low frequency words. Pedersen (1996) and Pedersen, Kayaalp, and Bruce (1996) follow up Dunning's suggestion that Fisher's exact test might be even more appropriate for such contingency tables. We have therefore investigated for the full range of word frequencies whether there is an optimal window size with respect to recall and the number of significant words extracted using both the log-likelihood ratio and Fisher's exact test. In Section 2, we will show that indeed there seems to be an optimal window size for both statistical tests. However, a recurrent pattern of local optima calls this conclusion into question. Upon closer inspection, this recurrent pattern appears at fixed ratios of the number of words inside the window to the number of words outside the window (complement). 302  Weeber, Vos, and Baayen  Extracting the Lowest-Frequency Words  In Section 3, we will relate the recurrent patterns of local optima at fixed window- complement ratios (henceforth W/C-ratios) to the distributions of the lowest-frequency words over window and complement. We will call attention to the critical effect of the choice of W/C-ratios on the significance of the lowest-frequency words. As the improvement in the extraction of side-effect terms from medical abstracts, as gauged by the F-measure, which combines recall and precision (Rijsbergen 1979), is small, we also applied the same approach to the extraction of Dutch verb-particle combinations from a newspaper corpus. In Section 4, we report substantially better results for this more lexical extraction task, which is subject to the same statistical behavior of the lowest-frequency words. In the last section, we will discuss the consequences of our findings for the optimization of word-based extraction systems and collocation research with respect to the lowest-frequency words.  2. An Optimal Window Size for Medical Abstracts? The MEDLINE bibliographic database contains a large number of abstracts of scientific journal papers discussing medical and drug-related research. Typically, abstracts discussing medical drugs mention the side effects of these drugs briefly. Information on side effects is potentially relevant for finding new applications for existing drugs (Rikken and Vos 1995). We are therefore interested in any terms related to the side effects of drugs. Before proceeding, it may be useful to clarify the way in which the present research differs from standard research on collocations. In the latter kind of research, there is no a priori knowledge of which combinations of words are true collocations. Moreover, the most salient collocations generally are found at the top of a list ranked according to measures for surprise or association, such as G2 or mutual information (Manning and Sch~itze 1999). The large numbers of word combinations with significant but low values for these measures are often of less interest. Low-frequency words are predominant among these kinds of collocations. In our research, we likewise find many low-frequency terms for side effects with low ranks in medical abstracts. The relatively well-known side effects that are mentioned frequently can be captured by examining the top ranks in the lists of extracted words. At the same time, the rarely mentioned side-effect terms are no less important, and in post marketing surveillance the extraction of such side-effect terms may be crucial for the acceptance or rejection of new medicines. Is reliable automatic extraction of both low- and high-frequency side-effect terms from MEDLINE abstracts feasible? To answer this question, we explored the efficacy of a standard collocation-based term extraction method that extracts those words that appear more frequently in the immediate neighborhood of a given seed term than might be expected under chance conditions. We compiled two corpora on the side effects of the cardiovascular drugs captopril and enalapril from MEDLINE abstracts. The first corpus contains all abstracts mention- ing captopril and the word side. The second corpus contains all abstracts mentioning captopril and at least one of the compounds side-effect, side effect, side-effects, and side effects. Thus, the second corpus is a subset of the first. The first corpus is comprised of 118,675 tokens and 7,678 types; the second corpus 103,603 tokens and 6,582 types. A medical expert marked 432 of the latter word types as side-effect-related terms. The left panel of Figure 1 summarizes the head of the frequency distribution of these terms in the larger corpus. Note that most side-effect-related terms have a frequency lower  303  Computational Linguistics  Volume 26, Number 3  Table 1 General 2x2 contingency table. A = frequency of the target in the window corpus, B = frequency of the target in the complement corpus, W = total number of words in the window, C = total number of words in the complement. Corpus size N = W + C.  frequency of target sum frequency of other words  window A W- A W  complement B C- B C  A+B W+C-A-B W+C  than five. What we need, then, is an extraction method that is sensitive enough to select such very low frequency terms. In the collocation-based method studied here, the neighborhood of a given seed term is defined in terms of a window around the seed term. We constructed windows around all seed terms in the corpus, leading to a window corpus and a complement corpus. The window corpus contains all words that appear within a given window size of the seed term. For instance, with a window size of 10, any word appearing from five words before the seed to five words after the seed as well as the seed itself is included in the window corpus. The word tokens not in the window corpus comprise the complement corpus. Any type in the window corpus is a potential side-effectrelated term. For any such target type, we tabulate its distribution in window and c o m p l e m e n t corpora in a contingency table like Table 1. Given W and C, we need to know whether the frequency of the target in the window corpus, A, is high enough to warrant extraction. Typically, given the marginal distribution of the contingency table, a target is extracted for which wA--~A> ~-B2-~, and for which the tabulated distribution is nonhomogeneous according to tests such as G2 and Fisher's exact test for a given cMevel. In this approach, the window size is a crucial variable. At small window sizes, many potentially relevant terms fail to appear in the window corpus. However, at large window sizes, many irrelevant words are found in the window corpus and may be extracted spuriously. To see to what extent window size may affect the results of the extraction procedure, consider the solid lines in panels (a) and (b) of Figure 2. The left panel shows the results for recall w h e n w e use the log-likelihood ratio, G2, the right panel the results for Fisher's exact test. We define recall as the proportion of the number of side-effect words extracted and the total number of side-effect words available in the window. For both statistical tests, recall seems to be optimal at w i n d o w size 2. However, at this window size, the number of words extracted is very small. This can be seen in panels (c) and (d). Considered jointly, panels (a) and (c) suggest an optimal window size of 24 for our larger corpus (corpus 1), as recall is still high, and the number of significant words is maximal. W h e n Fisher's exact test is used instead of G2, panels (b) and (d) suggest 42 as the optimal size. The dashed lines in panels (a) to (d) show the corresponding results for our smaller corpus (corpus 2). Unsurprisingly, the general pattern for this subcorpus is quite similar, although the drops in recall and the number of significant words, Nsig, occur at somewhat smaller window sizes. Interestingly, we can synchronize the curves for both corpora by plotting recall and the number of significant items, Nsig, against the window-complement ratio (W/C). This is shown in panels (e) and (f). These panels suggest not an optimal window size 304  Weeber, Vos, and Baayen  04f  0.3  i  ~ 0.2  ~  i  0  20  40  6(J  80 100 120  24  86  Window Size  (a)  6°°r A '~ 4°°t / ',il _ . ~ ~  ~ [  Extracting the Lowest-Frequency Words  03 i  0.2  o. h  0  20  6  3oor , ~o  }  40  60  80  100 120  42  82  Window Size  (b)  .,/  i  ......  0  20  40  60  80  100 120  24  86  Window Size  (c)  4oo[ ,  200I,  0.0  0.2  0.17  0.4  0.6  0.8  0.62  w/c  (e)  0  20  6  300I iilI  40  60  80  100 120  42  82  Window Size  (d)  ..... "1  0.0  0.2  0.4  0.6  0.8  0.05  0.29  0.58  w/c  (t)  Figure 2 Results of the word extraction procedure (a = 0.05). Solid line = corpus 1, dashed line = corpus 2. Panel (a) shows the log-likelihood, G2, recall results as a function of the window size. Panel (b) shows recall values for Fisher's exact test. Panel (c) shows the total number of significant words (Nsig) as a function of the window size for G2. Panel (d) shows the same as (c) but for Fisher's exact test. Panel (e), G2, and (f), Fisher's exact test, also show the total number of significant words, but as a function of the W/C-ratio;the ratio of the number of words in the window corpus to the number of words in the complement corpus.  but an optimal W/C-ratio (0.17 for G2 and 0.29 for Fisher's exact test). A l t h o u g h w e now seem to have shown that recall and Nsig depend on the choice of window size, the sudden drops in recall and Nsig and the reoccurrence of such drops at various W/C-ratios is a source of worry, not only for G2 results, but also for the results based on Fisher's exact test. A further source of worry is the fact that the two tests diverge considerably with respect to the optimal W/C-ratio. 3. Contingency Tables and the Lowest-Frequency Words Before w e can h a v e a n y confidence in the o p t i m a l i t y of a g i v e n W/C-ratio, w e s h o u l d understand why the saw-tooth-shaped patterns of Nsig arise. Both the log-likelihood ratio (G2) a n d Fisher's exact test c o m p u t e the significance of contingency tables similar to Table 1. So w h y is it that the left panels in Figure 2 differ f r o m the right panels? G2 has a •2-distribution as N --* cx~. This convergence is not g u a r a n t e e d for low expected frequencies and sparse tables, which renders use of G2 problematic for our lowest-frequency words in that it may suggest words to be more remarkable than they 305  Computational Linguistics  Volume 26, Number 3  Table 2 Contingency tables for hapax legomena, dis legomena, and tris legomena. W = number of words in window corpus; C = number of words in complement corpus. Total corpus size: N = W + C.  (a): 1  0  W-1 C  (b): 2  0  W-2  C  (c): 1  
* Departament de Llenguatgesi SistemesInformatics,Modul C 5 - Campus Nord, Jordi Girona Salgado 1-3, E-08034Barcelona.E-maih morrill@lsi.upc.es;http://www-lsi.upc.es/~morrill/ (~) 2000Associationfor ComputationalLinguistics  Computational Linguistics  Volume 26, Number 3  object relativizations (5) exhibit a severe deterioration in acceptability (Chomsky 1965, Chap. 1). (4) a. The dog that chased the cat barked. b. The dog that chased the cat that saw the rat barked. c. The dog that chased the cat that saw the rat that ate the cheese barked. (5) a. The cheese that the rat ate stank. b. ?The cheese that the rat that the cat saw ate stank. c. ??The cheese that the rat that the cat that the dog chased saw ate stank. Discussing such center embedding, Johnson (1998) presents the essential idea developed here, noting that processing overload of dependencies invoked in psycholinguistic literature could be rendered in terms of the maximal number of unresolved dependencies as represented by proof nets. Kimball (1973, 27) considers sentences such as (6), which are three ways ambiguous according to the attachment of the adverb. He points out that the lower the attachment of the adverb, the higher the preference (he calls this relationship Right Association). (6) Joe said that Martha believed that Ingrid fell today. Left-to-right quantifier scope preference is illustrated by: (7) a. Someone loves everyone. b. Everyone is loved by someone. Both sentences exhibit both quantifier scopings: (8) a. ~xVy(love y x) b. Vy3x(love y x) However, while the dominant reading of (7a) is (8a), that of (7b) is (8b), i.e., the preference is for the first quantifier to have wider scope. Note that the same effect is observed when the quantifiers are swapped: (9) a. Everyone loves someone. b. Someone is loved by everyone. While both sentences in (9) have both quantifier scopings, the preferred readings give the first quantifier wide scope. Finally, we will look at heavy noun phrase shift, which is the preference for complex object noun phrases to "shift" to the end of the sentence. Consider the two sentences in (10); the second, in which the "heavy" direct object follows the indirect object, is more acceptable than the first.  320  Morrill  Incremental Processing and Acceptability  (10) a. ?John gave the painting that Mary hated to Bill. b. John gave Bill the painting that Mary hated.  We argue that a simple metric of categorial processing complexity explains these and other performance phenomena.  2. Lambek Calculus  We shall assume some familiarity with Lambek categorial grammar as presented in, for example, Moortgat (1988, 1997), Morrill (1994), or Carpenter (1998), and limit ourselves here to reviewing some central technical and computational aspects. The types, or (category) formulas, of Lambek calculus are freely generated from a set of primitives by the binary infix connectives "/" (over), "V' (under) (directional divisions) and "-" (product). With respect to a semigroup algebra (L, +) (i.e., a set L closed under an associative binary operation + of adjunction), each formula A is interpreted as a subset [A] of L by residuation as follows:  (11) ~[a.B]] ~- ($1q-82]$1 E ~ a ] & s 2 E ~B~} ~AkB]] = {sirs' C [[AB,s'+s E HB]]} ~B/A] -- (sirs' E [[A~,s+s' E ~B]I}  A sequent, F ~ A, comprises a succedent formula A and an antecedent configuration F, which is a a finite sequence of formulas. 1A sequent is valid if and only if in all interpretations the ordered adjunction of elements inhabiting the antecedent formulas always yields an element inhabiting the succedent formula. The following Gentzen-style sequent presentation is sound and complete for this interpretation (Buszkowski 1986, Dogen 1992), and indeed for free semigroups (Pentus 1994): hence the Lambek calculus can make an impressive claim to be the logic of concatenation; a parenthetical notation A(F) represents a configuration containing a distinguished subconfiguration F.  (12) a. A ~ A  F~A A(A)~B  id  " A(F).::~ B  Cut  F ~ A A(B) ~ C \L b. A ( F , A \ B ) =~ C F ~ A A(B) ~ C c. &(B/A,F) ~ C /L  A,F ~ B \R F ::~ AkB F,A=~B /R r ~ B/A  F,A,B,A ~ C d. F,A.B,A ~ C •L  F ~ A A =~ B F,A =~ A.B .R  By way of example, "lifting" A ~ B/(A\B) is generated as follows:  (13) A =~ A B=~ B \ L A,A\B ~ B A ~ B/(A\B) /R  
U.S. Department of Defense  Klaus Ries Carnegie Mellon University and University of Karlsruhe Elizabeth Shriberg SRI International Daniel Jurafsky University of Colorado at Boulder Rachel Martin Johns Hopkins University Marie Meteer BBN Technologies  We describea statistical approachfor modeling dialogue acts in conversational speech, i.e., speechact-like units such as STATEMENT,QUESTION,BACKCHANNEL,AGREEMENT,DISAGREEMENT, and APOLOGY. Our model detects and predicts dialogue acts based on lexical, collocational, and prosodic cues, as well as on the discourse coherence of the dialogue act sequence. The dialogue model is based on treating the discourse structure of a conversation as a hidden Markov model and the individual dialogue acts as observations emanating from the model states. Constraints on the likely sequence of dialogue acts are modeled via a dialogue act n-gram. The statistical dialogue grammar is combined with word n-grams, decision trees, and neural networks modeling the idiosyncratic lexical and prosodic manifestations of each dialogue act. We develop a probabilistic integration of speech recognition with dialogue modeling, to improve both speech recognition and dialogue act classification accuracy. Models are trained and evaluated using a large hand-labeled database of 1,155 conversations from the Switchboard corpus of spontaneous human-to-human telephone speech. We achieved good dialogue act labeling accuracy (65% based on errorful, automatically recognized words and prosody, and 71% based on word transcripts, compared to a chance baselineaccuracy of35% and human accuracy of 84%) and a small reduction in word recognition error.  • SpeechTechnologyand ResearchLaboratory,SRIInternational,333 RavenswoodAve.,MenloPark,CA 94025, 1-650-859-2544. E-mail:stolcke@speech.sri.com. @ 2000Associationfor ComputationalLinguistics  Computational Linguistics  Volume 26, Number 3  Table 1 Fragment of a labeled conversation (from the Switchboard corpus).  Speaker  Dialogue Act  Utterance  A YEs-No-QuESTION  So do you go to college right now?  A ABANDONED  Are yo-,  B  YES- ANSWER  Yeah,  B  STATEMENT  it's my last year [laughter].  A DECLARATIVE-QUESTION You're a, so you're a senior now.  B  YEs-ANSWER  Yeah,  B  STATEMENT  I'm working on my projects trying to graduate  [laughter].  A APPRECIATION  Oh, good for you.  B  BACKCHANNEL  Yeah.  A  APPRECIATION  That's great,  A YEs-No-QUESTION  um, is, is N C University is that, uh, State,  B  STATEMENT  N C State.  A SIGNAL-NoN-UNDERSTANDING What did you say?  B  STATEMENT  N C State.  1. Introduction The ability to model and automatically detect discourse structure is an important step toward understanding spontaneous dialogue. While there is hardly consensus on exactly how discourse structure should be described, some agreement exists that a useful first level of analysis involves the identification of dialogue acts (DAs). A DA represents the meaning of an utterance at the level of illocutionary force (Austin 1962). Thus, a DA is approximately the equivalent of the speech act of Searle (1969), the conversational game move of Power (1979), or the adjacency pair part of Schegloff (1968) and Saks, Schegloff, and Jefferson (1974). Table 1 shows a sample of the kind of discourse structure in which we are interested. Each utterance is assigned a unique DA label (shown in column 2), drawn from a well-defined set (shown in Table 2). Thus, DAs can be thought of as a tag set that classifies utterances according to a combination of pragmatic, semantic, and syntactic criteria. The computational community has usually defined these DA categories so as to be relevant to a particular application, although efforts are under way to develop DA labeling systems that are domain-independent, such as the Discourse Resource Initiative's DAMSL architecture (Core and Allen 1997). While not constituting dialogue understanding in any deep sense, DA tagging seems clearly useful to a range of applications. For example, a meeting summarizer needs to keep track of who said what to whom, and a conversational agent needs to know whether it was asked a question or ordered to do something. In related work DAs are used as a first processing step to infer dialogue games (Carlson 1983; Levin and Moore 1977; Levin et al. 1999), a slightly higher level unit that comprises a small number of DAs. Interactional dominance (Linell 1990) might be measured more accurately using DA distributions than with simpler techniques, and could serve as an indicator of the type or genre of discourse at hand. In all these cases, DA labels would enrich the available input for higher-level processing of the spoken words. Another important role of DA information could be feedback to lower-level processing. For example, a speech recognizer could be constrained by expectations of likely DAs in a given context, constraining the potential recognition hypotheses so as to improve accuracy. 340  Stolcke et al.  Dialogue Act Modeling  Table 2 The 42 dialogue act labels. DA frequencies are given as percentages of the total number of utterances in the overall corpus.  Tag  Example  %  STATEMENT BACKCHANNEL/ACKNOWLEDGE OPINION ABANDONED/UNINTERPRETABLE AGREEMENT/ACCEPT APPRECIATION YEs-No-QUESTION NON-VERBAL YES ANSWERS CONVENTIONAL-CLOSING WH-QUESTION NO ANSWERS RESPONSE ACKNOWLEDGMENT HEDGE DECLARATIVE YES-No-QuESTION OTHER BACKCHANNEL-QUESTION QUOTATION SUMMARIZE/REFORMULATE AFFIRMATIVENON-YES ANSWERS ACTION-DIRECTIVE COLLABORATIVE COMPLETION REPEAT-PHRASE OPEN-QUESTION RHETORICAL-QUESTIONS HOLD BEFORE ANSWER/AGREEMENT REJECT NEGATIVE NON-NO ANSWERS SIGNAL-NON-UNDERSTANDING OTHER ANSWERS CONVENTIONAL-OPENING OR-CLAUSE DISPREFERRED ANSWERS 3RD-PARTY-TALK OFFERS, OPTIONS ~ COMMITS SELF-TALK D OWNPLAYER MAYBE/AcCEPT-PART TAG-QUESTION DECLARATIVE WH-QUESTION APOLOGY THANKING  Me, I'm in the legal department. Uh-huh. I think it's great So, -/ That's exactly it. I can imagine. Do you have to have any special training? <Laughter>, < Throat_clearing> Yes. Well, it's been nice talking to you. What did you wear to work today? No. Oh, okay. I don't know if I'm making any senseor not. So you can afford to get a house? Well give me a break, you know. Is that right? You can't be pregnant and have cats Oh, you meanyou switched schoolsfor the kids. It is. Why don't you go first Who aren't contributing. Oh, fajitas How about you ? Who would steal a newspaper? I'm drawing a blank. Well, no Uh, not a whole lot. Excuse me? I don't know How are you? or is it more of a company? Well, not so much that. M y goodness, Diane, get down from there. I'I1 have to check that out What's the word I'm lookingfor That's all right. Something like that Right? You are what kind of buff? I'm sorry. Hey thanks a lot  36% 19% 13% 6% 5% 2% 2% 2% 1% 1% 1% 1% 1% 1% 1% 1% 1% .5% .5% .4% .4% .4% .3% .3% .2% .3% .2% .1% .1% .1% .1% .1% .1% .1% .1% .1% .1% <.1% <.1% <.1% <.1% <.1%  The goal of this article is twofold: On the one hand, we aim to present a comprehensive framework for modeling and automatic classification of DAs, founded on well-known statistical methods. In doing so, we will pull together previous approaches as well as new ideas. For example, our model draws on the use of DA n-grams and the hidden Markov models of conversation present in earlier work, such as Nagata and Morimoto (1993, 1994) and Woszczyna and Waibel (1994) (see Section 7). However, our framework generalizes earlier models, giving us a clean probabilistic approach for performing DA classification from unreliable words and nonlexical evidence. For the 341  Computational Linguistics  Volume 26, Number 3  speech recognition task, our framework provides a mathematically principled way to condition the speech recognizer on conversation context through dialogue structure, as well as on nonlexical information correlated with DA identity. We will present methods in a domain-independent framework that for the most part treats DA labels as an arbitrary formal tag set. Throughout the presentation, we will highlight the simplifications and assumptions made to achieve tractable models, and point out how they might fall short of reality. Second, we present results obtained with this approach on a large, widely available corpus of spontaneous conversational speech. These results, besides validating the methods described, are of interest for several reasons. For example, unlike in most previous work on DA labeling, the corpus is not task-oriented in nature, and the amount of data used (198,000 utterances) exceeds that in previous studies by at least an order of magnitude (see Table 14). To keep the presentation interesting and concrete, we will alternate between the description of general methods and empirical results. Section 2 describes the task and our data in detail. Section 3 presents the probabilistic modeling framework; a central component of this framework, the discourse grammar, is further discussed in Section 4. In Section 5 we describe experiments for DA classification. Section 6 shows how DA models can be used to benefit speech recognition. Prior and related work is summarized in Section 7. Further issues and open problems are addressed in Section 8, followed by concluding remarks in Section 9. 2. The Dialogue Act Labeling Task The domain we chose to model is the Switchboard corpus of human-human conversational telephone speech (Godfrey, Holliman, and McDaniel 1992) distributed by the Linguistic Data Consortium. Each conversation involved two randomly selected strangers who had been charged with talking informally about one of several, selfselected general-interest topics. To train our statistical models on this corpus, we combined an extensive effort in human hand-coding of DAs for each utterance, with a variety of automatic and semiautomatic tools. Our data consisted of a substantial portion of the Switchboard waveforms and corresponding transcripts, totaling 1,155 conversations. 2.1 Utterance Segmentation Before hand-labeling each utterance in the corpus with a DA, we needed to choose an utterance segmentation, as the raw Switchboard data is not segmented in a linguistically consistent way. To expedite the DA labeling task and remain consistent with other Switchboard-based research efforts, we made use of a version of the corpus that had been hand-segmented into sentence-level units prior to our own work and independently of our DA labeling system (Meteer et al. 1995). We refer to the units of this segmentation as utterances. The relation between utterances and speaker turns is not one-to-one: a single turn can contain multiple utterances, and utterances can span more than one turn (e.g., in the case of backchanneling by the other speaker in midutterance). Each utterance unit was identified with one DA, and was annotated with a single DA label. The DA labeling system had special provisions for rare cases where utterances seemed to combine aspects of several DA types. Automatic segmentation of spontaneous speech is an open research problem in its own right (Mast et al. 1996; Stolcke and Shriberg 1996). A rough idea of the difficulty of the segmentation problem on this corpus and using the same definition of utterance units can be derived from a recent study (Shriberg et al. 2000). In an automatic labeling  342  Stolcke et al.  Dialogue Act Modeling  of word boundaries as either utterance or nonboundaries using a combination of lexical and prosodic cues, we obtained 96% accuracy based on correct word transcripts, and 78% accuracy with automatically recognized words. The fact that the segmentation and labeling tasks are interdependent (Warnke et al. 1997; Finke et al. 1998) further complicates the problem. Based on these considerations, we decided not to confound the DA classification task with the additional problems introduced by automatic segmentation and assumed the utterance-level segmentations as given. An important consequence of this decision is that we can expect utterance length and acoustic properties at utterance boundaries to be accurate, both of which turn out to be important features of DAs (Shriberg et al. 1998; see also Section 5.2.1). 2.2 Tag Set We chose to follow a recent standard for shallow discourse structure annotation, the Dialog Act Markup in Several Layers (DAMSL) tag set, which was designed by the natural language processing community under the auspices of the Discourse Resource Initiative (Core and Allen 1997). We began with the DAMSL markup system, but modified it in several ways to make it more relevant to our corpus and task. DAMSL aims to provide a domain-independent framework for dialogue annotation, as reflected by the fact that our tag set can be mapped back to DAMSL categories (Jurafsky, Shriberg, and Biasca 1997). However, our labeling effort also showed that content- and task-related distinctions will always play an important role in effective DA labeling. The Switchboard domain itself is essentially "task-free," thus giving few external constraints on the definition of DA categories. Our primary purpose in adapting the tag set was to enable computational DA modeling for conversational speech, with possible improvements to conversational speech recognition. Because of the lack of a specific task, we decided to label categories that seemed inherently interesting linguistically and that could be identified reliably. Also, the focus on conversational speech recognition led to a certain bias toward categories that were lexically or syntactically distinct (recognition accuracy is traditionally measured including all lexical elements in an utterance). While the modeling techniques described in this paper are formally independent of the corpus and the choice of tag set, their success on any particular task will of course crucially depend on these factors. For different tasks, not all the techniques used in this study might prove useful and others could be of greater importance. However, we believe that this study represents a fairly comprehensive application of technology in this area and can serve as a point of departure and reference for other work. The resulting SWBD-DAMSL tag set was multidimensional; approximately 50 ba- sic tags (e.g., QUESTION,STATEMENT)could each be combined with diacritics indicat- ing orthogonal information, for example, about whether or not the dialogue function of the utterance was related to Task-Management and Communication-Management. Approximately 220 of the many possible unique combinations of these codes were used by the coders (Jurafsky, Shriberg, and Biasca 1997). To obtain a system with somewhat higher interlabeler agreement, as well as enough data per class for statistical modeling purposes, a less fine-grained tag set was devised. This tag set distinguishes 42 mutually exclusive utterance types and was used for the experiments reported here. Table 2 shows the 42 categories with examples and relative frequencies.1 While some  
Readers unfamiliar with Chinese can gain an appreciation of the problem of multiple interpretations from Figure 1, which shows two alternative interpretations of the same Chinese character sequence. The text is a joke that relies on the ambiguity of phrasing. Once upon a time, the story goes, a man set out on a long journey. Before he could return home the rainy season began, and he had to take shelter at a friend's house. But he overstayed his welcome, and one day his friend wrote him a note: the first line in Figure 1. The intended interpretation is shown in the second line, which means "It is raining, the god would like the guest to stay. Although the god wants you to stay, I do not!" On seeing the note, the visitor took the hint and prepared to leave. As a joke he amended the note with the punctuation shown in the third line, which leaves three sentences whose meaning is totally different--"The rainy day, the staying day. Would you like me to stay? Sure!" * School of Computing and Mathematical Sciences, The Robert Gordon University, Aberdeen, Scotland t Computer Science, University of Waikato, Hamilton, New Zealand (~) 2000 Association for Computational Linguistics  Computational Linguistics A sentence in Chinese Interpretation 1 Interpretation 2 Figure 1 A Chinese sentence with ambiguity of phrasing. A sentence in Chinese Interpretation 1 Interpretation 2 Figure 2 An example that can be segmented in two different ways.  Volume 26, Number 3  physics  evidence I products  \  /  price9 N~bodY  theory  I  barber  J  ~ - - science  t - ~ reason  image  ...  understand ,..  s ch oot I J ~ -! ,~  study credit subject  student ...  physics / \ physicist  Figure 3 Example of treating each character in a query as a word. This example relies on ambiguity of phrasing, but the same kind of problem can arise with word segmentation. Figure 2 shows a more prosaic example. For the ordinary sentence of the first line, there are two different interpretations depending on the context of the sentence: "I like New Zealand flowers" and "I like fresh broccoli" respectively. The fact that machine-readable Chinese text is invariably stored in unsegmented form causes difficulty in applications that use the word as the basic unit. For example, search engines index documents by storing a list of the words they contain, and allow the user to retrieve all documents that contain a specified combination of query terms. This presupposes that the documents are segmented into words. Failure to do so, and treating every character as a word in itself, greatly decreases the precision of retrieval since large numbers of extraneous documents are returned that contain characters, but not words, from the query. Figure 3 illustrates what happens when each character in a query is treated as a single-character word. The intended query is "physics" or "physicist." The first character returns documents about such things as "evidence," "products," "body,.... image," "prices"; while the second returns documents about "theory,.... barber," and so on. Thus many documents that are completely irrelevant to the query will be returned, causing the precision of information retrieval to decrease greatly. Similar problems occur in word-based compression, speech recognition, and so on. 376  Teahan, Wen, McNab, and Witten  Chinese Word Segmentation  It is true that most search engines allow the user to search for multiword phrases by enclosing them in quotation marks, and this facility could be used to search for multicharacter words in Chinese. This, however, runs the risk of retrieving irrelevant documents in which the same characters occur in sequence but with a different intended segmentation. More importantly, it imposes on the user an artificial requirement to perform manual segmentation on each full-text query. Word segmentation is an important prerequisite for such applications. However, it is a difficult and ill-defined task. According to Sproat et al. (1996) and Wu and Fung (1994), experiments show that only about 75% agreement between native speakers is to be expected on the "correct" segmentation, and the figure reduces as more people become involved. This paper describes a general scheme for segmenting text by inferring the position of word boundaries, thus supplying a necessary preprocessing step for applications like those mentioned above. Unlike other approaches, which involve a dictionary of legal words and are therefore language-specific, it works by using a corpus of alreadysegmented text for training and thus can easily be retargeted for any language for which a suitable corpus of segmented material is available. To infer word boundaries, a general adaptive text compression technique is used that predicts upcoming characters on the basis of their preceding context. Spaces are inserted into positions where their presence enables the text to be compressed more effectively. This approach means that we can capitalize on existing research in text compression to create good models for word segmentation. To build a segmenter for a new language, the only resource required is a corpus of segmented text to train the compression model. The structure of this paper is as follows: The next section reviews previous work on the Chinese segmentation problem. Then we explain the operation of the adaptive text compression technique that will be used to predict word boundaries. Next we show how space insertion can be viewed as a problem of hidden Markov modeling, and how higher-order models, such as the ones used in text compression, can be employed in this way. The following section describes several experiments designed to evaluate the success of the new word segmenter. Finally we discuss the application of language segmentation in digital libraries. Our system for segmenting Chinese text is available on the World Wide Web at http://www.nzdl.org/cgi-bin/congb. It takes GB-encoded input text, which can be cut from a Chinese document and pasted into the input window.1Once the segmenter has been invoked, the result is rewritten into the same window. 2. Previous Methods for Segmenting Chinese The problem of segmenting Chinese text has been studied by researchers for many years; see Wu and Tseng (1993) for a detailed survey. Several different algorithms have been proposed, which, generally speaking, can be classified into dictionary-based and statistical-based methods, although other techniques that involve more linguistic information, such as syntactic and semantic knowledge, have been reported in the natural language processing literature. Cheng, Young, and Wong (1999) describe a dictionary-based method. Given a dictionary of frequently used Chinese words, an input string is compared with words in the dictionary to find the one that matches the greatest number of characters of the  
 Computational Linguistics EVIDENCE  Volume 26, Number 3  With its distanl orbit ( - 50 pcrccnt l~nhc~ from [hc sun Ihan F:~th -} o., . . . . . blanket  M,u's experiences frigid weather conditions. .......  .  .. . . . . . . . . . . .  typically ~vcragc about  ( (-70 degrees Fahrenheit })  at the equat~  0 ......... degrees C n~ poles  2. . . . . . . . . . . . . . . .  the  at tropical latitudcs  is w~m enough to  thaw ice on ~ t ' "on.  /  ON-VOLmON CAUSE If------------.  but ~y liquid water  bccaus~ of the  for~d in Ihis way would cvnporate a~l inst~tly  low ntmosph~ic pressure.  Figure 1 A rhetorical structure representation of the first paragraph in text (1).  span. The internal nodes are labeled with the names of the rhetorical relations that hold between the textual spans that are subsumed by their child nodes. Each relation between two nodes is represented graphically by means of a combination of straight lines and arcs. The material subsumed by the text span that corresponds to the starting point of an arc is subsidiary to the material subsumed by the text span that corresponds to the end point of an arc. A relation represented only by straight lines corresponds to cases in which the subsumed text spans are equally important. Text spans that subsume subsidiary information, i.e., text spans that correspond to starting points of arcs, are called satellites. All other text spans are called nuclei. Text fragments surrounded by curly brackets denote parenthetical units: their deletion does not affect the understanding of the textual unit to which they belong. For example, the textual unit Mars experiencesfrigid weather conditions is at the end of an arc that originates from the textual unit With its distant orbit--50 percentfartherfrom the sun than Earth---and slim atmospheric blanket because the former represents something that is more essential to the writer's purpose than the latter and because the former can be understood even if the subsidiary span is deleted, but not vice versa. The satellite information JUSTIFIES in this case the writer's right to present the information in the nucleus. The text spans Only the midday sun at tropical latitudes is warm enough to thaw iceon occasion, and but any liquid waterformed in this way would evaporate almost instantly because of the low atmospheric pressure are connected by straight lines because they are equally important with respect to the writer's purpose; they correspond to the elements of a C O N T R A S T relation. The text fragment--50 percent farther from the sun than Earth--is surrounded by curly brackets because it is parenthetical. Traditionally, it has been assumed that rhetorical structures of the kind shown in Figure 1 can be built only if one understands fully the semantics of the text and the intentions of the writer. To understand, for example, that the information given in the last two sentences of the first paragraph of text (1) is EVIDENCE to the information given in the first sentence, one needs to understand that the last two sentences may increase the reader's belief of the first sentence. And to understand that it was the low atmospheric pressure that caused the liquid water to evaporate, one needs to understand that without the information presented in the satellite, the reader may not know the particular CAUSEof the situation presented in the nucleus. In spite of the large number of discourse-related theories that have been proposed so far, there have emerged no algorithms capable of deriving the discourse structure of free, unrestricted texts. On one hand, the theories developed in the traditional, truth-based semantic perspective (Kamp 1981; Lascarides and Asher 1993; Asher 1993; Hobbs et al. 1993; Kamp and Reyle 1993; Asher and Lascarides 1994; Kameyama 396  Marcu  Rhetorical Parsing of Unrestricted Texts  1994; Polanyi and van den Berg 1996; van den Berg 1996; Gardent 1997; Schilder 1997; Cristea and Webber 1997; Webber et al. 1999) take the position that discourse structures can be built only in conjunction with fully specified clause and sentence syntactic structures. These theories have a grammar as their backbone and rely on sophisticated logics of belief and default logics in order to intertwine and characterize the sentence- and discourse-based linguistic phenomena. Despite their formal elegance, implementations of these theories cannot yet handle naturally occurring texts, such as that shown in (1). On the other hand, the theories aimed at characterizing the constraints that pertain to the structure of unrestricted texts and the computational mechanisms that would enable the derivation of these structures (van Dijk 1972; Zock 1985; Grosz and Sidner 1986; Mann and Thompson 1988; Polanyi 1988, 1996; Hobbs 1990) are either too informal or incompletely specified to support a fully automatic approach to discourse analysis. In this paper, I explore the ground found at the intersection of these two lines of research. More precisely, I explore the extent to which rhetorical structures of the kind shown in Figure 1 can be built automatically by relying only on cohesion and connectives, i.e., phrases such as for example, and, although, and however that are used "to link linguistic units at any level" (Crystal 1991, 74).1 The results show that although cohesion and connectives are ambiguous indicators of rhetorical structure, when used in conjunction with a well-constrained mathematical model of valid rhetorical structures, they enable the implementation of surprisingly accurate rhetorical parsers. 2. Foundation The hypothesis that underlies this work is that connectives, cohesion, shallow processing, and a well-constrained mathematical model of valid rhetorical structure trees (RS-trees) can be used to implement algorithms that determine • the elementary units of a text, i.e., the units that constitute the leaves of the RS-tree of that text; • the rhetorical relations that hold between elementary units and between spans of text; • the relative importance (nucleus or satellite) and the size of the spans subsumed by these rhetorical relations. In what follows, I examine each facet of this hypothesis intuitively and explain how it contributes to the derivation of a rhetorical parsing algorithm, i.e., an algorithm that takes as input free, unrestricted text and that determines its valid RS-trees. For each facet, I consider first the arguments that support the hypothesis and then discuss potential difficulties. 2.1 Determining the Elementary Units Using Connectives and Shallow Processing 2.1.1 Pro Arguments. Recent developments in the linguistics of punctuation (Nunberg 1990; Briscoe 1996; Pascual and Virbel 1996; Say and Akman 1996; Shiuan and Ann 1996) have emphasized the role that punctuation can have in solving a variety of natural language processing tasks ranging from syntactic parsing to information  
 Volume 26, Number 3  the corresponding general concept (type of objects). Language perception--the words we hear--is no exception: instead of showing a triangle to the robot, one can describe such a triangle verbally, which results in the same effect--the robot constructs and stores a specific instance of the concept triangle. Thus, language is viewed as one of the means of affecting the hearer's cognitive state, or, for a speaker, as one of the means of action in the external world. This idea is elaborated in the last part of the book. The second part, "Theory of Grammar," develops a universal computational formalism that is then applied to all language analysis and logical reasoning tasks throughout the rest of the book. The formalism, called LA-grammar (for left associative), is similar to good old augmented transition networks (ATNs). Like any generative grammar, it describes a language by means of the rules of an algorithm that reads the input string symbol by symbol and at some moment either accepts it as grammatical or rejects it as ungrammatical. The analysis algorithm maintains a record of some internal state--say, a tape with special symbols being written and erased, starting with an empty tape. After a symbol is read from the input, a rule is sought that allows it to be accepted given the current internal state (the whole contents of the tape). If no such rule exists, the string is rejected. If more than one rule is found, all alternatives are continued in parallel. Each rule provides an instruction for changing the current internal state (the contents of the tape); it also enables some subset of the rules and disables the others. Only enabled rules are used at each step of analysis. The manner in which the rules decide whether or not the new symbol is compatible with the current state is not specified by the definition of the LA-formalism, the only requirement being that the corresponding Boolean function be recursive, i.e., computable in principle. The same holds for the procedure that changes the internal state. This gives great freedom in implementing parsers with "memory" to handle phenomena such as long-distance dependencies and discontinuous constituents, but it raises the problem of development of formalisms for specifying these functions and procedures. Depending on what restrictions are placed on these Boolean functions, procedures, and the sets of rules that can be enabled simultaneously, subclasses of LA-grammars called C3 (less restricted), C2, and C1 are defined, with C1 having linear complexity. These subclasses are orthogonal to (that is, independent of) Chomsky's hierarchy of context-sensitive (CSG), context-free (CFG), and regular grammars: some CS languages are C1 languages and thus can be parsed in linear time by a suitable C1 grammar. The author argues for the hypothesis that all natural languages--even those with non-context-free phenomena (if they exist)--belong to the C1 class and thus have linear complexity, which is the author's main argument in favor of using his LA-grammars in language analysis and against using traditional phrase-structure (PS) grammars and formalisms mathematically equivalent to them (in which he includes, for example, HPSG). The third part, "Morphology and Syntax," introduces the basic concepts of morphology and syntax and shows how to write C1 grammars that in a uniform manner build, element b y element, morphs out of letters: I + o + v, wordforms out of morphs: lov + es, and sentences out of wordforms: loves + Mary. Such a linear, automaton-type order of processing of the elements is the third cornerstone of the theory (the L in SLIM): the author argues that we produce and perceive utterances letter by letter, word by word, and this order is to be directly modeled in a functional model of language and--because of the author's requirement of direct application of the rules by the parser--in the linguistic description. This is an idea that I find highly arguable, as the linguistic competence engaged in the processing is hardly of linear character.  450  Book Reviews The final cornerstone is surface compositionality (the S in SLIM): no zero elements are allowed. In the area of morphology, such surface-compositional, linear processing leads to rejection of the ideas underlying, say, the KIMMO model that builds a graphical representation of the word on the fly on the basis of interdependencies between its parts: lady + es = ladies, put + ed = put. Instead, all possible graphical variants of a m o r p h - allomorphs: {lady-, ladi-}, {-s, -es}--are predefined in the dictionary (or generated by a stand-alone algorithm), and compatibility rules decide what combinations are possible: ladi + es, ,lady + s. Such a "morphist" approach to morphology is analogous to well-known lexicalist approaches to syntax. I believe this is a good (even if not very new) idea. For syntax, the same LA-grammar mechanism is used. Thus, the very notion of a tree-like syntactic structure is absent from the theory (LA-syntactic structure is always linear, as per the L in SLIM). A sentence is either accepted by the automaton after it has read the final punctuation or it is rejected at some step; the semantic operations that augment the transition rules directly assemble the semantic representation of the sentence in the same linear order. At the end, consistency (grammaticality) of the sentence is checked for valency and agreement: all valencies should be filled and all agreement conditions satisfied. At any moment, the next word can prove to be incompatible with the already-read part of the sentence; then the analysis fails. Examples of small grammars, for both morphology and syntax, are given for English and German, including an example of the treatment of a supposedly non-context-free construction in German. The author's main point here is that C1 grammar can be parsed in linear time with the suggested syntactic parser, while traditional CFGs have polynomial (almost cubic) complexity. How then is the ambiguity problem handled? For example, one of the most challenging problems of parsing leading to high complexity is ambiguity of prepositional phrase attachment: Put the block in the box on the table; what is on the table--box, block, or put? What Hausser suggests (page 236) is simply to leave the prepositional phrases unattached, it being the pragmatic module's job to incorporate them into the semantic network. As far as I understand, he would treat the example above roughly as if it were Put the block. This is in the box. This is on the table, where it is the business of semantics, not syntax, to identify what this refers to. Unfortunately, I did not find any further explanation of how the pragmatic module would deal with such fragments. As for the syntactic parser, unattached phrases do not present any problems to it, since no syntactic structure at all is considered in the SLIM theory. The last part, "Semantics and Pragmatics," further develops the idea of the crucial role of pragmatics in natural language--a good idea that in my opinion is surprisingly underestimated by the computational linguistics mainstream. The difference is the following: semantics deals with the meaning that is stored in the dictionary entry for the word once and for all. Pragmatics deals with the meaning that the word has in a specific act of communication (occurring in a specific place, time, and circumstances). Look, a mushroom! says a traveler to his companion, having noted a rock formation with a flat wide top and thin base. To identify the object referred to, the hearer tries to find an object in their common view that most plausibly (or least implausibly) matches the standard dictionary definition of a mushroom; in this act of communication, the referent of the word mushroom proves to be a rock formation. Distinguishing the dictionary meaning from the context meaning allows the author to give an elegant solution to the problem of vagueness--which is perhaps the most valuable (though not completely new) idea of the book. How is it that the word 451  Computational Linguistics  Volume 26, Number 3  mushroom is so vague as to be applicable even to rock formations? To what else, then, is it applicable? Should its dictionary entry describe this continuum of meanings? Which of these meanings are more direct than others? Hausser's answer is this: neither the dictionary entry of the word mushroom nor its referring to the rock formation is vague; what is tensile is the matching of the dictionary definition against the least implausible candidate available in the specific communication situation. Clearly, there are other approaches to semantics, such as invariant definitions. Hausser's idea is similar to that of prototypes, which in its turn has received much criticism--see Wierzbicka (1989). Thus, the final representation of the analyzed utterance proves to be pragmatic rather than semantic in its nature, according to the distinction made above though functionally it corresponds to what is called semantic representation in the flameworks that do not make this fine distinction. It is quite similar to the familiar old semantic network (though the author carefully avoids this term). All knowledge that the cognitive agent has is represented as such a semantic network, whose nodes and relations are built by the agent either during parsing and interpretation of linguistic input, or by interpretation of otherwise-perceived images such as visual forms, or by logical inference---thinking. The last chapter of the book describes logical inference implemented as a simple LA-grammar that can take two facts and produce as output a new fact--their logical combination such as or or and. Possibly, the author has more to say about how logical inference is supposed to be done in SLIM theory but there was not enough space in the book to say it; however, in the way it is presented (pages 494-495), such uncontrolled, purposeless adding of trivial relations to the network does not seem to be a good substitute for classical inference methods; rather it looks like a bad illustration of the universal applicability of LA-grammars. The same approach is described for text generation: "The most general form of navigation is the accidental, aimless motion of the focus point through the contents of word bank [i.e., semantic network--A.G.] analogous to free association in humans" (page 477). A simple LA-grammar is used for such "aimless" navigation through the network, verbalizing the nodes that are passed by. Though no real-world examples are given, I expect that the utterances generated would resemble a delirium rather than a reasonable reaction of the system; again, possibly, the author has better ideas about how to make such generation more purposeful but did not have the space to explain them. The only thing explained is how the system can answer simple yes-no and wh- questions, interpreting them as patterns for search in the network. Surprisingly, in direct contradiction with the author's desiderata (page 181), the grammar used for parsing is not used for text generation. Instead, quite another grammar--actually another mechanism stuffed with ad hoc solutions and additions to the general LAgrammar formalism--is suggested for this purpose. In general, in spite of its wide coverage, solid introduction, and quite a few good ideas, in its new proposals the book impressed me as yet another manual on building toy systems, especially in its treatment of semantics, reasoning, and text generation. It is good news and bad news: people who need to build a simple question-answering system or talking robot could find the suggested approach useful. On the other hand, the book does not provide any deep linguistic discussion, considering mostly the John loves Mary kind of artificial examples. One of the main innovations introduced in the book is the LA-grammar formalism that--unlike traditional PS grammars--satisfies the author's eight desiderata for a generative grammar (page 180). Unfortunately, three very important requirements are absent from the author's list.  452  Book Reviews First, a grammar for a talking robot should be robust enough to understand incomplete or slightly ungrammatical sentences. The algorithm presented by the author, however, just rejects the sentence and aborts the analysis process when the next word read is unexpected. The author defines a generative grammar as a device "to formally decide for any arbitrary expression whether or not it is grammatically well-formed" (page 130) without any option of s o m e h o w processing (understanding) an input ex= pression that it would not generate. Given this definition, it is strange that the author has chosen a generative g r a m m a r as the basis for a functional model of language. In communication, we do not worry much about whether or not the utterance we hear is grammatical but instead about what it means, and it is not a human-like behavior for a talking robot to fail to understand a whole sentence only because of a split infinitive or misplaced comma. Are there any alternatives to generative grammars that are more appropriate for talking robots? For example, Mel'~uk's Meaning ~=~Text theory (Steele 1990) directly describes the translation of texts into meanings and vice versa. Second, grammar formalisms should allow for the easy maintenance and extension of large grammars. The author argues that LA-grammars are easy to debug since the LA-parser executes the grammar rules directly, whereas traditional PS parsers translate the grammar rules into internal tables, which makes it difficult to track what actions of the parser correspond to what rules. This is as true as the statement that Assembler programs are easier to debug than Prolog ones since at each moment you know exactly w h a t line of y o u r code is being executed. However, is it easier to maintain a large program in Assembler? An LA-grammar resembles the data structure used internally by the Earley algorithm: a list of all possible continuations in each possible state--with the exception that in this case you write this data structure manually. On pages 335336, the author illustrates how easy it is to add a new rule to a toy four-line grammar. If this is considered easy, then I guess that a realistic-size LA-grammar would turn into a maintenance nightmare. Unfortunately (and probably not by accident) the author does not give any clear data on whether such realistic-size LA-grammars exist for any language, and if so, what the number of rules in such a grammar is and what its coverage of a real text corpus is.1 Third, grammar formalisms should directly support linguistic intuition, allowing the linguist to write down his or her ideas more or less directly. The good news about LA-grammar is that it is based on notions well known in general linguistics, valency and agreement, while the basic idea underlying PS grammars takes these phenomena as rather marginal (taking them seriously required the drastic changes that resulted in the emergence of HPSG). Actually, Hausser's syntax has a lot in common with the dependency approach (which he does not even mention), and this is the reason for its applicability to free-word-order languages. Is it then the long-awaited efficient computational formalism for dependency grammar? Possibly it is a good step towards such formalism. However, LA-grammars for natural languages presented in the book are rather counterintuitive linguistically. While the notion of constituent is lost, the notions of dependency used in the book do not agree with linguistic tradition (Mel'~uk 1988). Often I found it hard to follow why a certain combination of rules happened 
There are many other articles of immediate interest, including several on grammar models popular in CL (Mark Steedman on Categorial Grammar, Georgia Green on Head-Driven Phrase Structure Grammar, and Mary Dalrymple on Lexical Functional Grammar) and several that treat computational simulations of psycholinguistic phenomena. Dennis Norris's general article on computational psycholinguistics focuses nicely on interdisciplinary issues; it motivates why a range of computational models remain interesting within cognitive science when language is the subject of investigation. 2. Cognitive Science or Cognitive Sciences? It is noteworthy that the editors do not attempt a general overview article on cognitive science, and that MITECS promises information on the cognitive sciences (plural). This suggests a fragmented view of the field, which, coming from its greatest authorities, must be taken seriously. It is also reflected in the relatively little attention paid to specifically cognitive issues in many of the articles (for example, the articles on Infor-  
Volume 26, Number 2  pated in advance. Furthermore, handling the interactions often requires violating the modularity of the system, because detecting when the special case has occurred still requires both planning and linguistic knowledge. The IGEN generator solves this problem. IGEN handles interactions between the text planning and linguistic components without having to sacrifice any of the generator's modularity. The key to IGEN's approach is the use of annotations that the linguistic component attaches to each linguistic expression it constructs. These annotations abstract away from the details of the linguistic expressions, describing only those properties of the expressions that are potentially relevant to the planner. The planner can then evaluate the choices made by the linguistic component and determine how those choices interact with the text plan independently of the linguistic component's processes and data structures. As a result, IGEN can make decisions involving interactions between the components while retaining complete modularity. In fact, replacing IGEN's normal linguistic component with one for a different language involves no change in the planner despite the fact that the two languages have d~- 
Consider Figure 1 (adapted from Rist [1996]) in which a message is expressed through two different modalities, namely text and graphics. The figure illustrates a kind of reasoning required to understand multimodal presentations: in order to make sense of the message, the interpreter must realize what individuals are referred to by the pronouns he and it in the text. For the sake of argument, it is assumed that the graphical symbols in the figure are understood directly in terms of a graphical lexicon, in the same way that the words he, it, and washed are understood in terms of the textual • Departmentof ComputerScience,Institutefor AppliedMathematicsand Systems(IIMAS),National AutonomousUniversityof Mexico(UNAM),Mexico.E-mail:luis@leibniz.iimas.unam.mx. @ 2000Associationfor ComputationalLinguistics  Computational Linguistics I©c  Volume 26, Number 2  "He washed it" Figure 1 Instance of linguistic anaphor with pictorial antecedent. "Saarbrficken lies at the intersection between the border between France and Germany and a line from Paris to FrankJhrt. "  Figure 2 Instance of a pictorial anaphor with linguistic antecedent. lexicon. It can easily be seen that given the graphical context, he should resolve to the man, and it should resolve to the car. However, this inference is not valid since the information inferred is not contained in the overt graphical context and the meaning of the words involved. One way to look at this problem is as a case of anaphoric inference. Consider that the information provided by graphical means can also be expressed through the following piece of discourse: There is a man, a car, and a bucket. He washed it. With Kamp's discourse representation theory (DRT) (Kamp 1981; Kamp and Reyle 1993) a discourse representation structure (DRS) in which the reference to the pronoun he is constrained to be the man can be built. However, the pronoun it has two possible antecedents, and conceptual knowledge is required to select the appropriate one. In particular, the knowledge that a man can wash objects with water, and that water is carried in buckets, must be employed. If these concepts are included in the interpretation context like DRT conditions (which should be retrieved from memory rather than from the normal flow of discourse), the anaphora can be solved. By analogy, situations like the one illustrated in Figure 1 have been considered problems of anaphors with pictorial antecedents in which the interpretation context is built not from a preceding text but from a graphical representation that is introduced with the text (Andr6 and Rist 1994). Consider now the converse situation shown in Figure 2 (adapted from Rist [1996]), in which a drawing is interpreted as a map in the context of the preceding text. The dots and lines in the drawing, and their properties, do not have an interpretation and the picture in itself is meaningless. However, given the context introduced by the text, and also considering the common knowledge that Paris is a city in France, and Frankfurt a city in Germany, and that Germany lies to the east of France (to the right), 140  Pineda and Garza  Multimodal Reference Resolution  it is possible to infer that the denotations of the dots to the left, middle, and right in the picture are Paris, Saarbr~.icken, and Frankfurt, respectively, and that the dotted lines denote borders between countries, and in particular, the lower segment denotes the border between France and Germany. In this example, graphical symbols can be thought of as "variables" of the graphical representation or "graphical pronouns" that can be resolved in terms of the textual antecedent. Here again, the inference is not valid, as the graphical symbols could be given other interpretations or none at all. The situation in Figure 2 has been characterized as an instance of a pictorial anaphor with linguistic antecedent, and further related examples can be found in Andr6 and Rist (1994). This situation, however, cannot be modeled very easily in terms of Kamp's DRT because the "pronouns" are not linguistic objects, and lacking a proper formalization of the graphical information, there is no straightforward way to express in a discourse representation structure that a dot representing "a variable" in the graphical domain has the same denotation as a natural language name or description introduced from text in a DRS. Furthermore, the situation in Figure 1 can be thought of as anaphoric only if we ignore the modality of the graphics, as was done above; but if the notion of modality is to be considered at all in the analysis, then the situation in Figure 1 poses the same kinds of problems as the one in Figure 2. In general, graphical objects, functioning as constant terms or as variables, introduced as antecedents or as pronouns, cannot be expressed in a DRS, since the rules constructing these structures are triggered by specific syntactic configurations of the natural language in which the information is expressed. However, this limitation can be overcome if graphical information can be expressed in a language with well-defined syntax and semantics. An alternative is to look at these kinds of problems in terms of the traditional linguistic notion of deixis (Lyons 1968). Deixis has to do with the orientational features of language, which are relative to the spatio-temporal situation of an utterance. Under this view, and in connection with the notion of graphical anaphora discussed above, it is possible to mention the deictic category of demonstrative pronouns: words like this and that permit us to make reference to extralinguistic objects. In Figure 1, for instance, the pronouns he and it can be supported by overt pointing acts at the time the expression he washed it is uttered. Note that the purpose of the pointing act is to provide the referents for the pronouns directly, greatly simplifying the resolution process. However, the deictic use of a pronoun does not necessarily have to be supported by a physical gesture, because deictic use is characterized, more generally, by the identification of the referent in a metalinguistic context. Ambiguity in such words is not unusual, as they can also function as anaphors if they are preceded by a linguistic context, and even as determiners with a deictic component (e.g., this car). Additionally, not only demonstratives and pronouns but also proper names, definite descriptions, and even indefinites can be used deictically. As a great variety of contextual factors are conceivably involved in the interpretation of a deictic expression, gestures, although prominent, should be thought of only as one particular kind of contextual factor. In summary, the denotation of a deictic term is the individual that is picked out by the human interpreter in relation to the interpretation context.1 Consider that in the same way that an anaphoric inference is required for identifying the antecedent of an anaphoric term, an inference process is required for interpreting a term used deictically. We refer to this process as a deictic inference. The inference by  
t Image,Speechand IntelligentSystems(ISIS)ResearchGroup,Departmentof Electronicsand Computer Science,Universityof Southampton,SouthamptonSO171BJ,UK. E-mail:rid@ecs.soton.ac.uk @ 2000Associationfor ComputationalLinguistics  Computational Linguistics  Volume 26, Number 2  that (literate) humans are able to read aloud, so that systems that can pronounce print serve as models of human cognitive performance. Modern text-to-speech (TTS) systems use lookup in a large dictionary or lexicon (we use the terms interchangeably) as the primary strategy to determine the pronunciation of input words. However, it is not possible to list exhaustively all the words of a language, so a secondary or backup strategy is required for the automatic phonemization of words not in the system dictionary. The latter are mostly (but not exclusively) proper names, acronyms, and neologisms. At this stage of our work, we concentrate on English and assume that any such missing words are dictionary-like with respect to their spelling and pronunciation, as will probably be the case for many neologisms. Even if the missing words are dictionary-like, automatic determination of pronunciation is a hard problem for languages like English and French (van den Bosch et al. 1994). In fact, English is notorious for the lack of regularity in its spelling-to-sound correspondence. That is, it has a deep orthography (Coltheart 1978; Liberman et al. 1980; Sampson 1985) as opposed to the shallow orthography of, for example, Serbo-Croatian (Turvey, Feldman, and Lukatela 1984). To a large extent, this reflects the many complex historical influences on the spelling system (Venezky 1965; Scragg 1975; Carney 1994). Indeed, Abercrombie (1981, 209) describes English orthography as "one of the least successful applications of the Roman alphabet." We use 26 letters in English orthography yet about 45-55 phonemes in specifying pronunciation. It follows that the relation between letters and phonemes cannot be simply one-to-one. For instance, the letter c is p r o n o u n c e d / s / i n cider b u t / k / i n cat. On the other hand, t h e / k / s o u n d of kitten is written with a letter k. Nor is this lack of invariance between letters and phonemes the only problem. There is no strict correspondence between the number of letters and the number of phonemes in English words. Letter combinations (ch,gh, II, ea) fre- quently act as a functional spelling unit (Coltheart 1984)--or grapheme--signaling a single phoneme. Thus, the combination ough is pronounced / A f / in enough, while ph is pronounced as the single p h o n e m e / f / i n phase. However, ph in uphill is pronounced as two phonemes,/ph/. Usually, there are fewer phonemes than letters but there are exceptions, e.g., (six,/sIks/). Pronunciation can depend upon word class (e.g., convict, subject). English also has noncontiguous markings (Wijk 1966; Venezky 1970) as, for instance, when the letter e is added to (mad,/mad/) to make (made,/meId/), also spelled maid! The final e is not sounded; rather it indicates that the vowel is length- ened or dipthongized. Such markings can be quite complex, or long-range, as when the suffix y is added to photograph or telegraph to yield photography or telegraphy, re- spectively. As a final comment, although not considered further here, English contains many proper nouns (place names, surnames) that display idiosyncratic pronunciations, and loan words from other languages that conform to a different set of (partial) regularities. These further complicate the problem. This paper is concerned with an analogical approach to letter-to-sound conversion and related string rewriting problems. Specifically, we aim to improve the performance of pronunciation by analogy (PbA) by information fusion, an approach to automated reasoning that seeks to utilize multiple sources of information in reaching a decision-in this case, a decision about the pronunciation of a word. The remainder of this paper is organized as follows: In the next section, we contrast traditional rule-based and more modern data-driven approaches (e.g., analogical reasoning) to language processing tasks, such as text-to-phoneme conversion. In Section 3, we describe the original (PRONOUNCE) PbA system of Dedina and Nusbaum (1986) in some detail as this forms the basis for the later work. Section 4 reviews our own work in this area. Next, in Section 5, we make some motivating remarks about information fusion and its use in computational linguistics in general. In Section 6, we present in some detail the  196  Marchand and Damper  Improving Pronunciation by Analogy  multistrategy (or fusion) approach to PbA that enables us to obtain clear performance improvements, as described in Section 7. Finally, conclusions are drawn and directions for future studies are proposed in Section 8.  2. Rule-based and Data-driven Conversion  Given the problems described above, how is it possible to perform automatic phonemization at all? It is generally believed that the problem is largely soluble provided sufficient context is available. For example, the substring ough is pronounced /o~5/ when its left context is th in the word although,/u/when its left context is thr in the word through, and / A f / when its left context is en in the word enough: in each case, the right context is the word delimiter symbol. In view of this, context-dependent rewrite rules have been a popular formalism for the backup pronunciation strategy in TTS systems. The form of the rules, strongly inspired by concepts from generative phonology (Chomsky and Halle 1968, 14), is:  A[BJC --+ D  (1)  which states that the letter substring B with left context A and right context C receives the pronunciation (i.e., phoneme substring) D. Such rules can also be straightforwardly cast in the IF... THEN form commonly featured in high-level programming languages and employed in expert, knowledge-based systems technology. They also constitute a formal model of universal computation (Post 1943). Conventionally, these rules are specified by an expert linguist, conversant with the sound and spelling systems of the language of concern. Typical letter-to-sound rule sets are those described by Ainsworth (1973), McIlroy (1973), Elovitz et al. (1976), Hunnicutt (1976), and Divay and Vitale (1997). Because of the complexities of English spelling-to-sound correspondence detailed in the previous section, more than one rule generally applies at each stage of transcription. The potential conflicts that arise are resolved by maintaining the rules in a set of sublists, grouped by (initial) letter and with each sublist ordered by specificity. Typically, the most specific rule is at the top and most general at the bottom. In the Elovitz et al. rules, for instance, transcription is a one-pass, left-to-right process. For the particular target letter (i.e., the initial letter of the substring currently under consideration), the appropriate sublist is searched from top to bottom until a match is found. This rule is then fired (i.e., the corresponding D substring is right-concatenated to the evolving output string), the linear search terminated, and the next untranscribed letter taken as the target. The last rule in each sublist is a context-independent default for the target letter, which is fired in the case that no other, more specific rule applies. We refer to the mapping between B and D in Equation (1) as a correspondence. Given a set of correspondences, we can align text with its pronunciation. For example, consider the word (make,/meIk/). A possible alignment (which uses the null phoneme--see below) is:  m  a  k  e  m  eI  k  197  Computational Linguistics  Volume 26, Number 2  It should be obvious, however, that it is all but impossible to specify a canonical set of correspondences for words of English on which all experts could agree. For instance, why should we use the single-letter correspondences a-~ /eI/, k--* /k/, and e--+ / - / as above, rather than the composite ake ~ / e I k / , which captures the noncontiguous marking by the final e? Of course, alignment and correspondences are at the heart of the rule-based methodology. So, although the context-dependent rule formalism has been vastly influential-from both theoretical linguistic and practical system implementation perspectives--it does have its problems. From a practical point of view, the task of manually writing such a set of rules, deciding the rule order so as to resolve conflicts appropriately, maintaining the rules as mispronunciations are discovered, etc., is very considerable and requires an expert depth of knowledge of the specific language. For these reasons, and especially to ease the problem of creating a TTS system for a new language, more recent attention has focused on the application of automatic techniques based on machine learning from large corpora--see Damper (1995) for a comprehensive review, van den Bosch (1997) for more recent discussion, and Dietterich (1997) for an accessible review of the underpinning machine learning methodologies. The rule-based approach has also been challenged from a more theoretical point of view. For instance, Jones (1996, 1) describes the goal of his book Analogical Natural Language Processing as: to challenge the currently predominant assumption in the field of natural language processing that the representation of language should be done within the rule-based paradigm alone. For historical reasons, this traditional position is largely the result of the influence of Chomsky and his efforts to define language in terms of formal mathematics.... The contrasting view, taken here, is that language is not something that can be described in a neat and tidy way. This is also the perspective adopted here. It is also conceivable that data-driven techniques can actually outperform tradi- tional rules. However, this possibility is not usually given much credence. For instance, Divay and Vitale (1997) recently wrote: "To our knowledge, learning algorithms, although promising, have not (yet) reached the level of rule sets developed by humans" (p. 520). Dutoit (1997) takes this further, stating "such training-based strategies are often assumed to exhibit much more intelligence than they do in practice, as revealed by their poor transcription scores" (p. 115, note 14). Pronunciation by analogy (PbA) is a data-driven technique for the automatic phonemization of text, originally proposed as a model of reading, e.g., by Glushko (1979) and Kay and Marcel (1981). It was first proposed for TTS applications over a decade ago by Dedina and Nusbaum (1986, 1991). See also the work of Byrd and Chodorow (1985), which considers computer-based pronunciation by analogy but does not mention the possible application to text-to-speech synthesis. As detailed by Damper (1995) and Damper and Eastmond (1997), PbA shares many similarities with the artificial intelligence paradigms variously called case-based, memory-based, or instance-based reasoning as applied to letter-to-phoneme conversion (Stanfill and Waltz 1986; Lehnert 1987; Stanfill 1987, 1988; Golding 1991; Golding and Rosenbloom 1991; van den Bosch and Daelemans 1993). PbA exploits the phonological knowledge implicitly contained in a dictionary of words and their corresponding pronunciations. The underlying idea is that a pronunciation for an unknown word is derived by matching substrings of the input to  198  Marchand and Damper  Improving Pronunciation by Analogy  substrings of known, lexical words, hypothesizing a partial pronunciation for each matched substring from the phonological knowledge, and assembling the partial prontmciations. Although initially it attracted little attention from workers in speech synthesis, several groups around the world are now trying to develop the approach as a backup to dictionary matching. In spite of opinions to the contrary expressed in the literature (see above), there is accumulating evidence that PbA easily outperforms linguistic rewrite rules (Damper and Eastmond 1996, 1997; Yvon 1996; Bagshaw 1998). More recently, Damper et al. (1999) conducted a careful performance evaluation of four techniques for letter-tosound conversion: rules, backpropagation neural networks (Sejnowski and Rosenberg 1987; McCulloch, Bedworth, and Bridle 1987), PbA and the IBI-IG method based on information gain weighting (Daelemans, van den Bosch, and Weijters 1997; van den Bosch 1997). Results showed PbA to be the best of the techniques evaluated by a significant margin. Although one obviously cannot say from a limited evaluation with just three competitors that PbA is the best method available, it is clearly worthy of serious consideration and further development. This paper marks a stage of that development. As a psychological (or theoretical) model, PbA is "seriously underspecified" so that the implementor wishing to use analogy within the pronunciation component of a TTS system "faces detailed choices which can only be resolved by trial and error" (Damper and Eastmond 1997, 1). The impact of implementational choices on performance has been studied by Sullivan and Damper (1993), Damper and Eastmond (1996, 1997), and Yvon (1996, 1997). One important dimension on which implementations vary is the strategy used to score the candidate pronunciations that PbA produces. By and large, these investigators have sought the single best pronunciation strategy. However, the range of choices is wide, so that some rather different implementations with different performance can be produced, yet these are all (in some sense) pronunciation by analogy. This raises the possibility, which forms the main focus of this paper, that different multiple PbA strategies--whose outputs are combined to give the final pronunciation--might be used to good effect. If the different strategies make different errors, then it is conceivable that such a multistrategy approach can produce a lower error rate than even the best single strategy. Indeed, it may be that a strategy with a poor performance by itself can make a positive overall contribution to high-accuracy pronunciation derivation when used in concert with other strategies. It can be viewed as a "specialist" within a committee of experts (e.g., Jacobs et al. 1991; Dietterich 1997): most often its opinion is invalid, but occasionally--for inputs within its sphere of expertise--it produces the correct answer when the other "generalist" strategies do not. There is currently much interest in the topic of information fusion in a wide variety of application settings. This work can be seen as a specific instance of information fusion. 3. Dedina and Nusbaum's System The results reported here were obtained using an extended and improved version of PRONOUNCE, the Dedina and Nusbaum (D&N) system, which we now describe. 3.1 Principles The basic PRONOUNCE system consists of four components: the lexical database; the matcher, which compares the target input to all the words in the database; the pronunciation lattice (a data structure representing possible pronunciations); and the decision function, which selects the "best" pronunciation among the set of possible ones. Reflecting PbA's origins as an empirical, psychological model, this selection is heuristic  199  Computational Linguistics A N @  E C  D 0  Volume 26, Number 2 T E ot-  Figure 1 Simplified pronunciation lattice for the word anecdote. For clarity, only a subset of the arcs is shown. Full pattern matching is used as described in Section 3.2. Phoneme symbols are those employed by Sejnowski and Rosenberg. rather than being based (like certain other approaches to automatic pronunciation) on any statistical model. 3.1.1 Pattern Matching. The input word is first compared to words listed in the lexicon (Webster's Pocket Dictionary) and substrings common to both are identified. For a given dictionary entry, the process starts with the input string and the dictionary entry leftaligned. Substrings sharing contiguous, common letters in matching positions in the two strings are then found. Information about these matching letter substrings--and their corresponding phoneme substrings in the dictionary entry under consideration-is entered into the input string's pronunciation lattice as detailed below. (Note that this requires the letters and phonemes of each word in the lexicon to have been previously aligned in one-to-one fashion.) The shorter of the two strings is then shifted right by one letter and the matching process repeated. This continues until the two strings are right-aligned, i.e., the number of right shifts is equal to the difference in length between the two strings. This process can be alternatively seen as a matching between substrings of the incoming word "segmented in all possible ways" (Kay and Marcel 1981, 401) and the entries in the lexicon. 3.1.2 Pronunciation Lattice. Matched substrings, together with their corresponding phonemic mappings as found in the lexicon, are used to build the pronunciation lattice for the input string. A node of the lattice represents a matched letter, Li, at some position, i, in the input. The node is labeled with its position index i and with the phoneme that corresponds to Li in the matched substring, Pim say, for the mth matched substring. An arc is placed from node i to node j if there is a matched substring starting with L i and ending with Lj. The arc is labeled with the phonemes intermediate between Pim and Pjm in the phoneme part of the matched substring. Additionally, arcs are labeled with a "frequency" count (see below), which is incremented by one each time that substring (with that pronunciation) is matched during the pass through the lexicon. Figure 1 shows an example pronunciation lattice for the word anecdote. For clarity, the lattice has been simplified to show only a subset of the arcs. This word suffers from the so-called silence problem whereby PbA fails to produce any pronunciation, because there is no complete path through the lattice (see next page). In the case illustrated, there is no cd ~ / k d / mapping in the dictionary other than in the word anecdote itself. Hence, in view of the leave-one-out testing strategy (see next page), there will never be an arc between nodes ( / k / , 4 ) and ( / d / , 5). 200  Marchand and Damper  Improving Pronunciation by Analogy  3.1.3 Decision Function. A possible pronunciation for the input string then corresponds to a complete path through its lattice, with the output string assembled by concatenating the phoneme labels on the nodes/arcs in the order that they are traversed. (Different paths can, of course, correspond to the same pronunciation.) Scoring of candidate pronunciation uses two heuristics in PRONOUNCE. If there is a unique shortest path, then the pronunciation corresponding to this path is taken as the output. If there are tied shortest paths, then the pronunciation corresponding to the best-scoring of these is taken as the output. In D&N's original work, the score used is the sum of arc "frequencies" (Dedina and Nusbaum's term, and nothing to do with frequency of usage in written or spoken communication) obtained by counting the number of times the corresponding substring matches between the input and the entire lexicon. The scoring heuristics are one obvious dimension on which different versions of PbA can vary. In the following, when we refer to a multistrategy approach to PbA, it is principally the use of multiple scoring strategies which is at issue. 3.2 Appraisal PRONOUNCE was evaluated on just 70 monosyllabic pseudowords--a subset of those previously used in reading studies by Glushko (1979). Such a test is largely irrelevant to TTS applications: the test set is not representative of general English, either in the small number of words used or their length. Also, D&N's claimed results on this pseudoword test set have proved impossible to replicate (Damper and Eastmond 1996, 1997; Yvon 1996; Bagshaw 1998). In addition, no consideration is given to the case where no complete path through the lattice exists (the silence problem mentioned earlier). D&N's pattern matching (when building the pronunciation lattice) is a "partial" one. That is, as explained in section 3.1.1, the process starts with the leftmost letter of the input string and of the current dictionary entry aligned and continues until the two are right-aligned. They give (on page 59) the example of the input word blopematching to the lexical entry sloping. At the first iteration, the initial b of blope aligns with the initial s of sloping, and the common substring lop is extracted. The process terminates at the third iteration, when the final e of blopealigns with the final g of sloping: there are no common substrings in this case. There seems to be no essential reason for starting and discontinuing matching at these points. That is, we could shift and match over the range of all possible overlaps--starting with the final e of blope aligned with the initial s of sloping, and terminating with the initial b of the former aligned with the final g of the latter. We call this "full" as opposed to "partial" matching. (Note that the simplified pronunciation lattice depicted in Figure 1 was obtained using full pattern matching.) One conceivable objection to partial pattern matching is that some morphemes can act both as prefix and suffix (e.g., someBODY and BODYguard). From this point of view, full matching seems worth consideration. A linguistic justification for the full method is that affixation is often implicated in the creation of new words. 4. Previous Work and Extensions In this section, we briefly review our previous work on a single-strategy approach to PbA (Damper and Eastmond 1996, 1997). The basic purpose of the earlier work was to reimplement D&N's system but to improve the scoring heuristic used to find the best path through the pronunciation lattice. To have a more realistic and relevant evaluation on a large corpus of real words, as opposed to a small set of pseudowords, we adopted the methodology of removing each word in turn from the lexicon and  201  Computational Linguistics  Volume 26, Number 2  deriving a pronunciation by analogy with the remaining words. In the terminology of machine learning, this is called leave-one-out or n-fold cross validation (Daelemans, van den Bosch, and Weijters 1997; van den Bosch 1997) where n is here the size of the dictionary. PbA has been used in this work to solve three string-mapping problems of importance in speech technology: letter-to-phoneme translation, phoneme-to-letter translation, and letter-to-stress conversion.  4.1 Lexical Database The lexical database on which the analogy process is based is the 20,009 words of Webster's Pocket Dictionary (1974 edition), manually aligned by Sejnowski and Rosenberg (S&R) (1987) for training their NETtalk neural network. The database is publicly available via the World Wide Web from URL: ftp://svr-ftp.eng.cam.ac.uk/pub/comp.speech /dictionaries/. It has data arranged in columns:  aardvark aback abacus abaft  a-rdvark xb@k@bxkxs xb@ft  
 Volume 26, Number 2  languages and language pairs benefit from the best of both the empiricist and rationalist traditions. This article presents three such models, along with methods for efficiently estimating their parameters. Each new method is designed to account for an additional universal property of translational equivalence in bitexts: . Most word tokens translate to only one word token. I approximate this tendency with a one-to-one assumption. . Most text segments are not translated word-for-word. I build an explicit noise model. . Different linguistic objects have statistically different behavior in translation. I show a way to condition translation models on different word classes to help account for the variety. Quantitative evaluation with respect to independent human judgments has shown that each of these three estimation biases significantly improves translation model accuracy over a baseline knowledge-free model. However, these biases will not produce the best possible translation models by themselves. Anyone attempting to build an optimal translation model should infuse it with all available knowledge sources, including syntactic, dictionary, and cognate information. My goal here is only to demonstrate the value of some previously unused kinds of information that are always available for translation modeling, and to show how these information sources can be integrated with others. A review of some previously published translation models follows an introduction to translation model taxonomy. The core of the article is a presentation of the model estimation biases described above. The last section reports the results of experiments designed to evaluate these innovations. Throughout this article, I shall use CA££/~GT4A/~C letters to denote entire text corpora and other sets of sets, CAPITAL letters to denote collections, including sequences and bags, and italics for scalar variables. I shall also distinguish between types and tokens by using bold font for the former and plain font for the latter. 2. Translation Model Decomposition There are two kinds of applications of translation models: those where word order plays a crucial role and those where it doesn't. Empirically estimated models of translational equivalence among word types can play a central role in both kinds of applications. Applications where word order is not essential include • cross-language information retrieval (e.g., McCarley 1999), • multilingual document filtering (e.g., Oard 1997), • computer-assisted language learning (e.g., Nerbonne et al. 1997), • certain machine-assisted translation tools (e.g., Macklovitch 1994; Melamed 1996a), • concordancing for bilingual lexicography (e.g., Catizone, Russell, and Warwick 1989; Gale and Church 1991),  222  Melamed  Models of Translational Equivalence  • corpus linguistics (e.g., Svartvik 1992), • "crummy" machine translation (e.g., Church and Hovy 1992; Resnik 1997).  For these applications, empirically estimated models have a number of advantages over handcrafted models such as on-line versions of bilingual dictionaries. Two of the advantages are the possibility of better coverage and the possibility of frequent updates by nonexpert users to keep up with rapidly evolving vocabularies. A third advantage is that statistical models can provide more accurate information about the relative importance of different translations. Such information is crucial for applications such as cross-language information retrieval (CLIR). In the vector space approach to CLIR, the query vector Q' is in a different language (a different vector space) from the document vectors D. A word-to-word translation model T can map QI into a vector Q in the vector space of D. In order for the mapping to be accurate, T must be able to encode many levels of relative importance among the possible translations of each element of QI. A typical bilingual dictionary says only what the possible translations are, which is equivalent to positing a uniform translational distribution. The performance of cross-language information retrieval with a uniform T is likely to be limited in the same way as the performance of conventional information retrieval without term-frequency information, i.e., where the system knows which terms occur in which documents, but not how often (Buckley 1993). Applications where word order is crucial include speech transcription for translation (Brousseau et al. 1995), bootstrapping of OCR systems for new languages (Philip Resnik and Tapas Kanungo, personal communication), interactive translation (Foster, Isabelle, and Plamondon 1996), and fully automatic high-quality machine translation (e.g., A1-Onaizan et al. 1999). In such applications, a word-to-word translation model can serve as an independent module in a more complex sequence-tosequence translation model.2 The independence of such a module is desirable for two reasons, one practical and one philosophical. The practical reason is illustrated in this article: Order-independent translation models can be accurately estimated more efficiently in isolation. The philosophical reason is that words are an important epistemological category in our naive mental representations of language. We have many intuitions (and even some testable theories) about what words are and how they behave. We can bring these intuitions to bear on our translation models without being distracted by other facets of language, such as phrase structure. For example, the translation models presented in the last two chapters of Melamed (to appear) capture the intuitions that words can have multiple senses and that spaces in text do not necessarily delimit words. The independence of a word-to-word translation module in a sequence-to-sequence translation model can be effected by a two-stage decomposition. The first stage is based on the observation that every sequence L is just an ordered bag, and that the bag B can be modeled independently of its order O. For example, the sequence (abcI consists of the bag {c,a, b} and the ordering relation {(b,2), (a, 1), (c,3)}. If we represent each sequence L as a pair (B, O), then  Pr(L) - Pr(B,O)  (1)  -- Pr(B)-Pr(OIB ).  (2)  2 "Sentence-to-sentence"mightbe a moretransparentterm than "sequence-to-sequence,"but all the modelsthat I'm awareof apply equallywell to sequencesof words that are not sentences.  223  Computational Linguistics  Volume 26, Number 2  Now, let L1 and L2 be two sequences and let A be a one-to-one mapping between the elements of L1 and the elements of L2. Borrowing a term from the operations research literature, I shall refer to such mappings as assignments. 3 Let .4 be the set of all possible assignments between L1 and L2. Using assignments, we can decompose conditional and joint probabilities over sequences:  Pr(LIIL2) = ~ Pr(L1,A[L2)  (3)  AG.4  Pr(L,,L2) = ~ Pr(L1, A, L2)  (4)  ACA  where  Pr(L,,A]L2) - Pr(B1,01,AIL2)  (5)  = Pr(B1,AIL2) •Pr(OI[B1, A, L2)  (6)  Pr(L1,A, L2) ~ Pr(B,, O1, A, B2, 02)  (7)  = Pr(B1, A, B2). Pr(O1, O2IB1,A, B2)  (8)  Summing bag pair probabilities over all possible assignments, we obtain a bag-to-bag  translation model:  Pr(B1, B2) = ~ Pr(B,, A, B2)  (9)  AEA  The second stage of decomposition takes us from bags of words to the words that they contain. The following bag pair generation process illustrates how a wordto-word translation model can be embedded in a bag-to-bag translation model for languages £1 and £2:  . Generate a bag size /.4 1 is also the assignment size. 2. Generate l language-independent concepts C1,..., C1. 3. From each concept Ci, 1 < i < I, generate a pair of word sequences (ffi,rTi) from £~ x £~, according to the distribution trans(G ~), to lexicalize the concept in the two languages. 5 Some concepts are not lexicalized in some languages, so one of ffi and rTi may be empty.  A pair of bags containing m and n nonempty word sequences can be generated by a process where l is anywhere between 1 and m + n. For notational convenience, the elements of the two bags can be labeled so that B1 - {u~,...,t~} and B2 ~ {V~. . . . . ~ } , where some of the 1/'s and "?'s may be empty. The elements of an assignment, then, are pairs of bag element labels: A -{(h,jl) . . . . . (h, jl)}, where each i ranges o v e r {IJ1. . . . . 11l}, eachj ranges over {v~. . . . . x~},  3 Assignments are different from Brown, Della Pietra, Della Pietra, and Mercer's (1993) alignments in that assignments can range over pairs of arbitrary labels, not necessarily sequence position indexes. Also, unlike alignments, assignments must be one-to-one. 4 The exact nature of the bag size distribution is immaterial for the present purposes. 5 Since they are put into bags, ffi and r7i could just as well be bags instead of sequences. I make them sequences only to be consistent with more sophisticated models that account for noncompositional compounds (e.g. Melamed, to appear, Chapter 8).  224  Melamed  Models of Translational Equivalence  each i is distinct, and each j is distinct. The label pairs in a given assignment can be generated in any order, so there are I! ways to generate an assignment of size I.6 It follows that the probability of generating a pair of bags (B1,B2) with a particular assignment A of size l is  Pr(B1,A, B2]I,C,trans) : Pr(1). I! n E Pr(C)trans('fi'vilC)"  (lO)  (i,j) ff A CCC  The above equation holds regardless of how we represent concepts. There are many plausible representations, such as pairs of trees from synchronous tree adjoining grammars (Abeill6 et al. 1990; Shieber 1994; Candito 1998), lexical conceptual structures (Dorr 1992) and WordNet synsets (Fellbaum 1998; Vossen 1998). Of course, for a representation to be used, a method must exist for estimating its distribution in data. A useful representation will reduce the entropy of the trans distribution, which is con- ditioned on the concept distribution as shown in Equation 10. This topic is beyond the scope of this article, however. I mention it only to show how the models presented here may be used as building blocks for models that are more psycholinguistically sophisticated. To make the translation model estimation methods presented here as general as possible, I shall assume a totally uninformative concept representation--the trans dis- tribution itself. In other words, I shall assume that each different pair of word sequence types is deterministically generated from a different concept, so that trans(.1i,~i]C) is zero for all concepts except one. Now, a bag-to-bag translation model can be fully specified by the distributions of l and trans.  Pr(B1,A, B2]I,trans) = Pr(l). I! H trans(~,~j)  (11)  (i,j) CA  The probability distribution trans (.1,~) is a word-to-word translation model. Unlike 
 Computational Linguistics  Volume 26, Number 2  2. Pipelines in NLG For the past 20 years, the NLG community has generally agreed that modularizing NLG systems is sensible. This has become even more true in recent years, because of a growing trend to incorporate existing modules (especially realization systems such as FUF/SURGE [Elhadad and Robin 1997]) into new systems. While different systems use different numbers of modules, all recent systems that I am aware of are divided into modules. This leads to the question of how modules should interact. In particular, is it acceptable to arrange modules in a simple pipeline, where a later module cannot affect an earlier module? Or is it necessary to allow revision or feedback, where a later module can request that an earlier module modify its results? If a pipeline is used, should modules pass a single solution down the line, or should they pass multiple solutions and let subsequent modules choose between these? Many authors have argued that pipelines cannot optimally handle certain linguistic phenomena. For example, Danlos and Namer (1988) point out that in French, whether a pronoun unambiguously refers to an entity depends on word ordering. This is because the pronouns le or la (which convey gender information) are abbreviated to 1' (which does not contain gender information) when the word following the pronoun starts with a vowel. But in a pipelined NLG system, pronominalization decisions are typically made earlier than word-ordering decisions; for example in the three-stage pipelined architecture presented by Reiter and Dale (2000), pronominalization decisions are made in the second stage (microplanning), but word ordering is chosen during the third stage (realization). This means that the microplanner will not be able to make optimal pronominalization decisions in cases where le or la are unambiguous, but I' is not, since it does not know word order and hence whether the pronoun will be abbreviated. Many other such cases are described in Danlos's book (Danlos 1987). The common theme behind many of these examples is that pipelines have difficulties satisfying linguistic constraints (such as unambiguous reference) or performing linguistic optimizations (such as using pronouns instead of longer referring expressions whenever possible) in cases where the constraints or optimizations depend on decisions made in multiple modules. This is largely due to the fact that pipelined systems cannot perform general search over a decision space that includes decisions made in more than one module. Despite these arguments, most applied NLG systems use a pipelined architecture; indeed, a pipeline was used in every one of the systems surveyed by Reiter (1994) and Paiva (1998). This may be because pipelines have many engineering advantages, and in practice the sort of problems pointed out by Danlos and other pipeline critics do not seem to be a major problem in current applied NLG systems (Mittal et al. 1998). 3. STOP The STOP system (Reiter, Robertson, and Osman 1999) generates personalized smokingcessation leaflets, based on responses to a questionnaire about smoking likes and dislikes, previous attempts to quit, and so forth. The output of the system is a four-page leaflet; each page is size A5. An example of the two "inside" pages of a leaflet produced by STOP is shown in Figure 1. A STOP leaflet also contains a front page that is only partially generated (the rest is logos and fixed text) and a back page that is selected from a collection of 16 possible back pages, but is not otherwise personalized; these are not shown here due to space restrictions.  252  Reiter  Pipelines and Size Constraints  iiiiill~i~ii;!;?! iii!iiiiii!!ii~ ~Tx :" ir :N,. N N W r sill'  6_ 0  o~-  XD  m •  g  ~ ,-~o .9o 0 - ~ o_  ~ .~  o~o  ~.~m o-  ~ ~  o~  o- O "O C ~o~  o.% d ~ mE 0 0 E >,  =o~ o ~: ."='0=9-~,0~_-~~~ ~o0 o: ~= -O.-- -~- ~ -0  c~xE  o'~U~  (~  ~=~ o ~ ~,~, - ~ o ~ -°~~_=.~=_  ~  m E (0~ >, =~oo.o. ~. -oE_(~~  O J~ 0 0 0 ~ err~-  "~o~ 00.  ~a o C~. c o  m~,~ .~ ~~c ~~. ,0~ ,...  ° oo ~ o = ~: :o~  ._ ~:._= ~:- ~~0,>=~=oo~. ~  •  o~o  :: o~'o E  C ~  °d <'o  6_  o,O o .  ~•- - ~~- ~ C ~0  )oo_~  O  •-0a o0= ~ ~~1  ~gE~ :I~ C tO  >,  > > O. 0 0  >->-  _~_  ~:~ 0  0o  ~ooa ~  i "~ = "~ ._~ -~  ,', E  ~ (~'~'~  •~~ E~0 m>.,=~_  O  r~ c n  r,  N~eg  <  7 ~ i i imsm' iiii!i!iiiiiiii~;i!~  .=_ o o ,,,  ~o~  7  o- = -  .~= oo ,~-~  ~ ~ -  "-- m ' O li o .C~  ~ ~ : >-I~ m ~ m ~ '- m - >"  ~  0 •  Z . . , a~ xo ~ m ~ . . ~m • c  : of.~ ._: ~o_. ~ 00 ~.~o  ..........  C O.~ mm = 4 "o~o .C~o0 .~0 0 ~  >~tU  ~.-= .o~-×-~~>~-.c=-~~ -~m~  X: ~iiiiiiiiiiiiiiiiiiiiiiii!i  _~ ~ 0 C  ~ oC~°'o  • 6 o= = . ..c:: = "~ E.~_ -o ~ ~  ~"= ~  -~,  ~  o~ °-  $= E  a  =o®  o  0E..~ ~ E ~  0 ~  ~0 o0 - ~  ~l_l  ~ o o ~ "~o ~_~ ~ "-- a. Q .o0~ >m~ CO "~0--.  g o O • • "O O N'~ O~ >0 ! .~o  o.~ .~  ~0= 0 C  .-~ ~0 >" ' ~ 0 0.~  o .  =~-~ ~ o~.~-  -~  Q; 0~ 0~ × Q; C ~D r~ ~a ~a  253  Computational Linguistics  Volume 26, Number 2  A STOP leaflet must fit on four A5 pages; this is a hard constraint. Furthermore, it is important to communicate as much information as possible subject to the size constraint; this is a characteristic that the system tries to optimize. However, it is even more important that leaflets be easy to read, and size optimization should not be at the expense of readability. For example, replacing an itemized list (such as the one at the top of the second page in Figure 1) by a complex multiclause sentence can reduce size but often makes leaflets harder to read, especially for poor readers; hence we do not do this. The original version of STOP used a three-stage pipelined architecture (with each pipeline module producing only one solution) similar to the one presented by Reiter and Dale (2000). An initial document-planning stage produced a document plan data structure, which specified the content of the document in terms of messages. In STOP, messages were represented as strings (or lists of strings) that specified word forms and word order, but not punctuation, capitalization, and intertoken white space. The document plan also specified how messages were grouped into higher-level structures (such as paragraphs); discourse relations between messages or groups of messages; and the importance of each message and message group. Once an initial document plan had been produced, the document trimmer component of the document planner attempted to ensure that the document produced by the document plan did not exceed four A5 pages. It did this using a heuristic function that estimated the size of the final document from the document plan. If the heuristic size estimator indicated that the document was too large, the trimmer identified the least important message in the document plan, deleted this message, and recalculated the document's estimated size.1 This process continued until the document fitted on four A5 pages according to the size estimator. At this point the document plan was passed on to the other stages of the system, microplanning and realization. These performed tasks such as deciding when discourse relations should be expressed via cue phrases, and adding appropriate punctuation, capitalization, and white space to the text (both of which tasks, incidentally, are affected by trimming and hence must take place after it). The realizer produced an RTF file, which was printed using Microsoft Word; in a sense Word could be considered to be a fourth pipeline stage. The main difficulty in this approach was estimating the size of the final document. Since messages were represented as strings, we initially thought it would be easy to build an accurate size estimator. But in fact this proved to be a difficult task, because the size of a document is highly dependent on its exact surface form, including cue phrases, punctuation and capitalisation, and even typographic features such as bold face. For example, consider the leaflet extract shown in Figure 1. This fits on two A5 pages, as desired. However, if "bad for your health" in the paragraph just below the graphic were changed from italic face to bold face, then this paragraph would require four lines instead of three lines. Our layout style does not allow a section to start on a page unless both the section header and two lines of section text can fit on the page. Therefore, increasing the size of this paragraph to four lines causes Word to start the section headed "You could do it ..." on the next page; this makes the leaflet overflow onto an additional page, and thus violate the overall size constraint. Thus, a very small change in a document (such as changing a few words from italics to bold) can cause significant changes in a document's size. The fact that a  
2. The engine drained in 20 seconds. 3. Within 20 seconds, Tom drained the engine. 4. Tom drained the engine of the oil in 20 seconds. In contrast, a state-of-the-art natural language generation (NLG) system would likely be able to produce only one of them, modulo variations introduced by, for example, passivization, topicalization, or pronominalization. The problem is not simply to choose among paraphrases, but to be able to produce them to start with. In this book, an extended version of his dissertation, Stede provides the theoretical and practical means of endowing a text generator with such capability. He concen- trates on tactical planning, namely, on choosing an appropriate verbalization for some content that another component of the NLG system (the text planner) has assembled in response to communicative goals. Stede's approach promotes lexicalization as the central step in this process and the lexicon itself as the link between the languageindependent domain model and the language-specific resources necessary to generate the surface sentence. Stede's ultimate goal is ambitious: he seeks to provide an architecture that can be used as is to perform generation of the same content in different languages--even lexical entries are to be reused when possible. In his view, multilingual generation, including the problem of language divergences, can be seen as a mere extension of the monolingual paraphrase task. Whereas the methods proposed are very compelling with respect to monolingual generation, it is not clear whether the multilingual goal has been fully achieved. I will come back to this point at the end of this review. The book is structured in three main parts: • Chapters 1 through 3 provide the introduction and background on NLG, on lexicalization, and on lexical variation. Further background is  270  Book Reviews sprinkled throughout the book, in particular regarding verbal aspect (Chapter 4) and lexical semantics (Chapter 6). Chapters 4 through 8 constitute the core of the book; I will discuss them below. Finally, Chapter 9 showcases the paraphrasing capabilities of the generator through a number of examples. Chapter 10 is an interesting although occasionally weak description of how the approach can be extended to generate paragraphs, not just sentences. Chapter 11 summarizes the relation between the author's and others' approaches, and speculates on a few directions for future research. At first I found Chapters 4 through 8 slightly overwhelming, as they introduce several levels of representation, each with its own terminology and acronyms. However, at a second, more-careful, reading, everything falls into place. The resulting approach has at its center a lexicon that partly implements current theories of lexical semantics such as Jackendoff's (1990) and Levin's (1993). The lexicon is used to mediate and map between a language-independent domain model and a language-dependent ontology widely used in NLG, the Upper Model (Bateman 1990). Although the idea of a two-level representation accommodating language-neutral and language-specific requirements is not new (see for example Nirenburg and Levin [1992], Dorr and Voss [1993], and Di Eugenio [1998]), Stede is among the few who make effective use of those two levels in a complex system. Chapter 4 presents the domain model built by means of the description logic system LOOM (MacGregor and Burstein 1991). Stede is specifically interested in verbalizations of situations, to use his own neutral term. Thus, the domain model contains a representation for categories such as states, activities, and events that Stede nicely synthesizes from linguistic work on aspect. 1 Chapter 5 discusses the different levels of representation used by the generator: the language-neutral level of situation specifications (SitSpecs), built as instantiations of portions of the domain model; and the language-specific level of semantic sentence specifications (SemSpecs), written in terms of Upper Model concepts and relations. Importantly, SemSpecs are lexicalized. A SemSpec for a sentence is built by collecting all possible verbalization options that cover a subset of the meaning expressed by the SitSpec to be verbalized, and combining them appropriately. In the same way that the domain model provides the foundation for the well-formedness of SitSpecs, the Upper Model guarantees that a SemSpec can actually be correctly converted to linguistic output. Chapter 6 describes the lexicon. Among other information, each lexical entry provides a denotation, which is a small excerpt of domain-model concepts and relations and represents the meaning associated with the lexical entry, and a partial SemSpec (PSemSpec), which describes the contribution of this lexical entry to the SemSpec representing the sentence. A PSemSpec can point to lexical entries in different languages if they are equivalent in denotation (e.g., rise in English and steigen in German). The correspondence between denotation and PSemSpec is maintained by coindexed variables. Chapter 7 presents the lexical resources to generate verbal alternations. Stede is interested in alternations that change verbal meanings, such as the resultative-causative 
system, more flexible in terms of the primitive relations or concepts that it can provide resort to, and less wedded to the tacit, not at all unproblematic, assumption that the right way to view any language, including natural language, is as a set of strings. This book is a collection of recent papers that pursue the connection between logical and natural languages further, mainly with regard to monadic second-order logic (MSO). Their relevance is apparent in the light of three earlier results. The first is the considerable amount of previous work in formal language theory on "tree languages." Given the extent to which trees and concepts based on tree position are used to express linguistic regularities, it is natural to think of using trees rather than strings as the basic elements of a formal language. It is also known that the tree languages definable in MSO are precisely those whose yields are the context-free string languages. The second is Rogers's landmark thesis (Rogers 1994), which demonstrated that significant portions of Government-and-Binding Theory (GB) can be very transparently expressed in MSO. The third is a collection of evidence from Dutch and Swiss German (Huybregts 1976, 1984; Shieber 1985) for the non-context-freeness of at least some natural languages. Thus, while MSO may be a very elegant choice, it does not quite appear to be a sufficient choice. While no clear answer to this dilemma presents itself here, several of the chapters in this volume do address this issue directly. Rogers shows that dropping the usual requirement of bounded branching on trees leads to the same expressive power as infinite context-free grammars that are finitely presentable as regular expressions of context-free rules. These can be used in certain accounts of coordination. Morawietz shows how MSO can be thought of as a constraint language and thereby incorporated into a constraint logic programming language, which can then generate any  274  Book Reviews computable language. M6nnich demonstrates how to use a kind of "macro grammar" to define tree languages in MSO for the non-context-free phenomena that one can find in natural languages by replacing the normal substitution process that rewriting systems assume in their notion of derivation by a higher-order variant in which operations such as composition can be explicitly represented as objects in the macro language. Other chapters approach tree description from different perspectives. Kallmeyer presents a tree description language that allows for true underspecification of dominance relations rather than just their path lengths. Kracht generalizes trees to handle some of the thorny formal issues surrounding the use of "categories," complexes of possibly several tree nodes that play a significant role in Chomsky's Barriers theory. There are also two chapters (by Kolb and Cornell) that discuss the application of declarative approaches to tree description to Chomsky's much more derivationbased Minimalist Program. In overall content, the book offers a very pleasing balance between formal rigor and linguistic application and a very well-informed set of references. To say the least, this is not a book for the mathematically faint of heart. It must be said, however, that the editors and authors have managed to organize the book so that it is remarkably self-contained on the formal side. The only notable exception is the paper by Palm on a first-order alternative for formulating linguistic principles, which relies too heavily on a report published elsewhere to address the numerous counterexamples and exceptions to the near-theorems it presents. Otherwise, the presumed familiarity with the Minimalist Program in the later chapters may actually pose a greater barrier to comprehension by likely readers who fall outside the book's intended audience, i.e., researchers and graduate students in linguistics with a strong background in logic. The book also suffers from a rather large number of grammatical and typographical errors, including several in formal definitions, which makes the reading difficult in places. Another slight problem, which is really a more systemic one in mathematical linguistics, is that no direct attempt is made to address other trends in generative linguistics that have a bearing on the subject under formal consideration. The use of parallel, yet very different levels of syntactic representation, such as in LexicalFunctional Grammar (LFG), or of more than one kind of constituent, as in the Prague school of linguistics, leaves open the possibility of using MSO to formulate constraints on multiple representations that, when intersected or otherwise combined, may produce very elegant analyses of non-context-free phenomena. There is also an established precedent in computer science of using MSO on richer graph-based languages than tree languages (Courcelle 1988; Engelfriet 1991), which might make MSO a realistic option for formalizing principles over feature-structure-based theories of language in a more constrained fashion. A courteous attempt is, in fact, made to relate the importance of this research to schools of linguistic thought other than GB/Minimalism, e.g., to LFG or Head-driven Phrase Structure Grammar, but for the most part these are throw-away remarks or outright misrepresentations. The Mathematics of Syntactic Structure is, nevertheless, a faithful representation of the state of the art in this area, and can be recommended as a useful primer for computational linguists on descriptive complexity theory and its applications to linguistic theory. 275  Computational Linguistics  Volume 26, Number 2  References Courcelle, Bruno. 1988. Some applications of logic, of universal algebra, and of category theory to the theory of graph transformations. EATCS Bulletin, 36:161-218. Engelfriet, Joost. 1991. A regular characterization of graph languages definable in monadic second-order logic. Theoretical Computer Science, 88(1):139-150. Huybregts, M. A. C. 1976. Overlapping dependencies in Dutch. Technical Report, Utrecht Working Papers in Linguistics.  
Indeed, Christopher Manning and Hinrich Schtitze's new, by-no-means slim textbook on statistical NLP--strangely, the first since Charniak'sl--begins "The need for a thorough textbook for Statistical Natural Language Processing hardly needs to be argued for." Indubitably so; the question is, is this it? Foundationsof StatisticalNatural LanguageProcessing(henceforth FSNLP) is certainly ambitious in scope. True to its name, it contains a great deal of preparatory material, including: gentle introductions to probability and information theory; a chapter on linguistic concepts; and (a most welcome addition) discussion of the nitty-gritty of doing empirical work, ranging from lists of available corpora to in-depth discussion of the critical issue of smoothing. Scattered throughout are also topics fundamental to doing good experimental work in general, such as hypothesis testing, cross-validation, and baselines. Along with these preliminaries, FSNLP covers traditional tools of the trade: Markov models, probabilistic grammars, supervised and unsupervised classification, and the vector-space model. Finally, several chapters are devoted to specific problems, among them lexicon acquisition, word sense disambiguation, parsing, machine translation, and information retrieval.2 (The companion website contains further useful material, including links to programs and a list of errata.) In short, this is a Big Book3, and this fact alone confers some benefits. For the researcher, FSNLP offers the convenience of one-stop shopping: at present, there is no other NLP reference in which standard empirical techniques, statistical tables, definitions of linguistic terms, and elements of information retrieval appear together; furthermore, the text also summarizes and critiques many individual research papers. Similarly, someone teaching a course on statistical NLP will appreciate the large number of topics FSNLP covers, allowing the tailoring of a syllabus to individual in- 
The rest of the papers are grouped into six sections, each of which is prefaced with two or three well-written pages from the editors. These introductions contain valuable commentary on the coming papers--even pointing out a possible flaw in the evaluation part of one. The opening section holds three papers on so-called classical approaches. Here one finds the oft-cited papers of Luhn, Edmundson, and Pollock and Zamora. As a package, these papers provide a novice with a good idea of how basic summarization works. My only quibble was in their reproduction. In Luhn's paper, an article from Scientific American is summarized and it would have been beneficial to have this included in the book as well. Some of the figures in another paper contained very small fonts and were hard to read; fixing this for a future print run is probably worth thinking about. The next section holds papers on corpus-based approaches to summarization, starting with Kupiec et al.'s paper about a summarizer trained on an existing corpus of manually abstracted documents. Two new papers building upon the Kupiec et al. work follow this. Exploiting the discourse structure of a document is the topic of the next section. Of the five papers here, I thought Daniel Marcu's was the best, nicely describing summarization work so far and then clearly explaining his system, which is based on Rhetorical Structure Theory. The following section on knowledge-rich approaches to summarization covers such things as Wendy Lehnert's work on breaking 
A doctor (whom a doctor)m (hired)n hired another nurse. As Gazdar and Mellish point out in their textbook, from which this example is taken, this is a legal English sentence only if m and n are equal (Gazdar and Mellish 1989). Strings of the form anbn are not regular expressions and therefore cannot be represented by finite-state machines. They require a context-free grammar definition and a machine with an unbounded memory, such as the stack of a pushdown automaton. Natural language constructions can require even more complex forms such as anbncn, which in turn must be defined using indexed grammars and parsed with a machine that has the equivalent of multiple stacks of memory. However "suppose that we had access to hardware that would handle FSTNs [finite-state transition networks] ... ultrafast and observed that in actual occurrences of anbn constructions, the value of n never exceeded 3; then we might decide to compile our RTN [recursive transition network] ... descriptions down into FSTNs subject to an n = 3 upper bound (such a compilation is possible for any given finite upper bound on the value of n)" (Gazdar and Mellish 1989). Kornai points out that such finite upper bounds are indeed what is observed in parsing natural languages. Kornai argues in his introduction that while the surge of thought and development surrounding transformational models in the early 1960s threatened to remove all credibility from finite-state approaches to NLP, the "extraordinary impact" of Thomp-  
In OT, the processes that try but fail to disrupt a robust generalization are described not as rules (cf. Paradis 1988), but as lower-ranked generalizations. Such a generalization may fail in contexts where it is overruled by a higher-ranked requirement of the language (or of the underlying form). As Kager emphasizes, this interaction of violable constraints can yield complex surface patterns. OT therefore holds out the promise of simplifying grammars, by factoring all complex phenomena into simple surface-level constraints that partially mask one another.' Whether this is always possible under an appropriate definition of "simple constraints" (e.g., Eisner 1997b) is of course an empirical question. 2. Relevance Before looking at Kager's textbook in detail, it is worth pausing to ask what broader implications Optimality Theory might have for computational linguistics. If you are an academic phonologist, you already know OT by now. If you are not, should you take the time to learn? So far, OT has served CL mainly as a source of interesting new problems--both theoretical and (assuming a lucrative market for phonology workbench utilities) prac-  
291  Computational Linguistics  Volume 26, Number 2  
Q 2000Associationfor ComputationalLinguistics  Computational Linguistics  Volume 26, Number 1  
Dictionaries can be constructed in various ways--see Watson (1993a, 1995) for a taxonomy of (general) finite-state automata construction algorithms. A word is simply a finite sequence of symbols over some alphabet and we do not associate it with a meaning in this paper. A necessary and sufficient condition for any deterministic automaton to be acyclic is that it recognizes a finite set of words. The algorithms described here construct automata from such finite sets. The Myhill-Nerode theorem (see Hopcroft and Ullman [1979]) states that among the many deterministic automata that accept a given language, there is a unique automaton (excluding isomorphisms) that has a minimal number of states. This is called the minimal deterministic automaton of the language. The generalized algorithm presented in this paper has been independently developed by Jan Daciuk of the Technical University of Gdafisk, and by Richard Watson • Departmentof AppliedInformatics,TechnicalUniversityof Gdafisk,U1.G. Narutowicza11/12, PL80-952Gdafisk,Poland.E-mail:jandac@pg.gda.pl LinguisticModellingLaboratory,LPDP--BulgarianAcademyof Sciences,Bulgaria.E-mail: stoyan@lml.bas.bg :~Departmentof ComputerScience,Universityof Pretoria,Pretoria0002,SouthAfrica.E-mail: watson@cs.up.ac.za § E-mail:watson@OpenFIRE.org (~)2000Associationfor ComputationalLinguistics  Computational Linguistics  Volume 26, Number 1  and Bruce Watson (then of the IST Technologies Research Group) at Ribbit Software Systems Inc. The specialized (to sorted input data) algorithm was independently developed by Jan Daciuk and by Stoyan Mihov of the Bulgarian Academy of Sciences. Jan Daciuk has made his C++ implementations of the algorithms freely available for research purposes at www.pg.gda.pl/~jandac/fsa.html. 1Stoyan Mihov has implemented the (sorted input) algorithm in a Java package for minimal acyclic finite-state automata. This package forms the foundation of the Grammatical Web Server for Bulgarian (at origin2000.bas.bg) and implements operations on acyclic finite automata, such as union, intersection, and difference, as well as constructions for perfect hashing. Commercial C++ and Java implementations are available via www.OpenFIRE.org. The commercial implementations include several additional features such as a method to remove words from the dictionary (while maintaining minimality). The algorithms have been used for constructing dictionaries and transducers for spell-checking, morphological analysis, two-level morphology, restoration of diacritics, perfect hashing, and document indexing. The algorithms have also proven useful in numerous problems outside the field of NLP, such as DNA sequence matching and computer virus recognition. An earlier version of this paper, authored by Daciuk, Watson, and Watson, appeared at the International Workshop on Finite-state Methods in Natural Language Processing in 1998--see Daciuk, Watson, and Watson (1998).  2. Mathematical Preliminaries  We define a deterministic finite-state automaton to be a 5-tuple M = (Q, ~, 6, q0, F), where Q is a finite set of states, q0 E Q is the start state, F C Q is a set of final states, is a finite set of symbols called the alphabet, and 6 is a partial mapping 6: Q x G ~Q denoting transitions. When 6(q,a) is undefined, we write ~(q,a) = _L. We can extend the 6 mapping to partial mapping 6*: Q x ~* ~Q as follows (where a E Y,, x E ~*):  = q 6*(q, ax) = {6*(6(q,a),x)  ifotherwise6(~q,a)J_  Let DAFSA be the set of all deterministic finite-state automata in which the transition function 6 is acyclic--there is no string w and state q such that 6" (q, w) = q. We define £(M) to be the language accepted by automaton M: £(M) = {xE I 6*(q0,x) The size of the automaton, IMI, is equal to the number of states, IQ[. ~(G*) is the set of all languages over G. Define the function 2: Q ~7~(G*) to map a state q to the set of all strings on a path from q to any final state in M. More precisely,  Z (q) = {x 16"(q,x) c F} £ (q) is called the right language of q. Note that £(M) = £ (q0). The right language of  
 Computational Linguistics  Volume 26, Number 1  The structure of this paper is as follows: In Section 2 we recall some standard definitions from language theory. Section 3 investigates a sufficient condition for a context-free grammar to generate a regular language. We also present the construction of a finite automaton from such a grammar. In Section 4, we discuss several methods to approximate the language generated by a grammar if the sufficient condition mentioned above is not satisfied. These methods can be enhanced by a grammar transformation presented in Section 5. Section 6 compares the respective methods, which leads to conclusions in Section 7.  2. Preliminaries Throughout this paper we use standard formal language notation (see, for example, Harrison [1978]). In this section we review some basic definitions. A context-free grammar G is a 4-tuple (G,N,P,S), where G and N are two finite disjoint sets of terminals and nonterminals, respectively, S E N is the start symbol, and P is a finite set of rules. Each rule has the form A ~ ~ with A E N and ~ E V*, where V denotes N U ~. The relation ~ on N x V* is extended to a relation on V* x V* as usual. The transitive and reflexive closure of 4 is denoted by 4*. The language generated b y G is given b y the set {w E G* I S 4 " w}. By definition, such a set is a context-free language. By reduction of a grammar we mean the elimination from P of all rules A 4 , 7 such that S --+* c~Afl --* a"/fl 7 " w does not hold for any~,flEV*andwEG*. We generally use symbols A, B, C. . . . to range over N, symbols a, b, c , . . . to range over ~, symbols X, Y, Z to range over V, symbols a, fl,"7. . . . to range over V* and symbols v, w, x.... to range over G* We write ¢ to denote the empty string. A rule of the form A --, B is called a unit rule. A (nondeterministic) finite automaton .T is a 5-tuple (K, G, A, s, F), where K is a finite set of states, of which s is the initial state and those in F c K are the final states, is the input alphabet, and the transition relation A is a finite subset of K x Z* x K. We define a configuration to be an element of K x G*. We define the binary relation t- b e t w e e n configurations as: (q, vw) F- (q', w) if and only if (q, v, q') E A. The transitive and reflexive closure of ~- is d e n o t e d b y F-*. Some input v is r e c o g n i z e d if (s, v) t-* (q, c), for some q E F. The language accepted by .T is defined to be the set of all strings v that are recognized. By definition, a language accepted by a finite automaton is called a regular language.  3. Finite Automata in the Absence of Self-Embedding We define a spine in a parse tree to be a path that runs from the root down to some leaf. Our main interest in spines lies in the sequences of grammar symbols at nodes bordering on spines. A simple example is the set of parse trees such as the one in Figure 1, for a grammar of palindromes. It is intuitively clear that the language is not regular: the grammar symbols to the left of the spine from the root to E"communicate" with those to the right of the spine. More precisely, the prefix of the input up to the point where it meets the final node c of the spine determines the suffix after that point, in such a way that an unbounded quantity of symbols from the prefix need to be taken into account. A formal explanation for why the grammar may not generate a regular language relies on the following definition (Chomsky 1959b):  18  Nederhof S--',a S a S-->b S b S---~ ~  S a Sa Y\ b Sb  Experiments with Regular Approximation  Figure 1 Grammar of palindromes, and a parse tree.  Definition A g r a m m a r is s e l f - e m b e d d i n g if there is some A E N such that A --+* c~Afl, for some a¢eandfl¢e.  If a grammar is not self-embedding, this means that when a section of a spine in a parse tree repeats itself, then either no grammar symbols occur to the left of that section of the spine, or no grammar symbols occur to the right. This prevents the "unbounded communication" between the two sides of the spine exemplified by the palindrome grammar. We now prove that grammars that are not self-embedding generate regular languages. For an arbitrary grammar, we define the set of reeursive nonterminals as:  B  N = {A E N I  Ag]}  m We determine the partition N" of N consisting of subsets N1, N2. . . . , Nk, for some k > 0, of mutually recursive nonterminals:  H = {N1,N2 .... ,Nk} NIUN2U...UNk=N Vi[Ni 7L O] Vi, j[i • j =~ Ni N Nj = 0]  and for all A, B E N:  3i[A E Ni A B @Nil - ~oQ, fll, O~2,fl2[a ---~* alBfll A B ---+* c¢2Afl2],  We now define the function recursive from N" to the set {left, right, self, cyclic}. For l<iKk:  recursive(Ni)  -- left, if ~LeftGenerating(Ni) = right, if LeftGenerating(Ni) -- self, if LeftGenerating(Ni) = cyclic, if -,LeftGenerating(Ni)  /x RightGenerating(Ni) /x ~RightGenerating(Ni) /x RightGenerating(Ni) /x ~RightGenerating( Ni )  where  LeftGenerating(Ni) = 3(A --* aBfl) E P[A E Ni A B E Ni /X ~ 7~ e] RightGenerating(Ni) = 3(A --* aBfl) E P[A E Ni /x B E Ni /~ fl • ¢]  19  Computational Linguistics  Volume 26, Number 1  When recursive(Ni) = left, Ni consists of only left-recursive nonterminals, which does not mean it cannot also contain right-recursive nonterminals, but in that case right recursion amounts to application of unit rules. When recursive(Ni) = cyclic, it is only such unit rules that take part in the recursion. That recursive(Ni) = self, for some i, is a sufficient and necessary condition for the grammar to be self-embedding. Therefore, we have to prove that if recursive(Ni) E {left, right, cyclic}, for all i, then the grammar generates a regular language. Our proof differs from an existing proof (Chomsky 1959a) in that it is fully constructive: Figure 2 presents an algorithm for creating a finite automaton that accepts the language generated by the grammar. The process is initiated at the start symbol, and from there the process descends the grammar in all ways until terminals are encountered, and then transitions are created labeled with those terminals. Descending the grammar is straightforward in the case of rules of which the left-hand side is not a recursive nonterminal: the subautomata found recursively for members in the right-hand side will be connected. In the case of recursive nonterminals, the process depends on whether the nonterminals in the corresponding set from H are mutually left-recursive or right-recursive; if they are both, which means they are cyclic, then either subprocess can be ap- plied; in the code in Figure 2 cyclic and right-recursive subsets Ni are treated uni- formly. We discuss the case in which the nonterminals are left-recursive. One new state is created for each nonterminal in the set. The transitions that are created for terminals and nonterminals not in Ni are connected in a way that is reminiscent of the con- struction of left-corner parsers (Rosenkrantz and Lewis 1970), and specifically of one construction that focuses on sets of mutually recursive nonterminals (Nederhof 1994, Section 5.8). An example is given in Figure 3. Four states have been labeled according to the names they are given in procedure make~fa. There are two states that are labeled qB. This can be explained by the fact that nonterminal B can be reached by descending the grammar from S in two essentially distinct ways. The code in Figure 2 differs from the actual implementation in that sometimes, for a nonterminal, a separate finite automaton is constructed, namely, for those nonterminals that occur as A in the code. A transition in such a subautomaton may be labeled by another nonterminal B, which then represents the subautomaton corresponding to B. The resulting representation is similar to extended context-free grammars (Purdom and Brown 1981), with the exception that in our case recursion cannot occur, by virtue of the construction. The representation for the running example is indicated by Figure 4, which shows two subautomata, labeled S and B. The one labeled S is the automaton on the top level, and contains two transitions labeled B, which refer to the other subautomaton. Note that this representation is more compact than that of Figure 3, since the transitions that are involved in representing the sublanguage of strings generated by nonterminal B are included only once. The compact representation consisting of subautomata can be turned into a single finite automaton by substituting subautomata A for transitions labeled A in other automata. This comes down to regular substitution in the sense of Berstel (1979). The advantage of this way of obtaining a finite automaton over a direct construction of a nondeterministic automaton is that subautomata may be determinized and minimized before they are substituted into larger subautomata. Since in many cases determinized and minimized automata are much smaller, this process avoids much of the combina-  20  Nederhof  Experiments with Regular Approximation  let K = O, A = O, s = fresh_state, f = fresh_state, F = {f}; make_fa( s, S, f). procedure makeffa(qo, a, ql): if a= e then let A = A U {(q0,e, ql)} e l s e i f a = a, s o m e a E ,U then let A = A U {(q0, a, ql)} e l s e i f a = Xfl, s o m e X E V, fl C V* s u c h t h a t IflI > 0 then let q = fresh_state; makeffa(qo, X, q); makeffa( q, t , ql ) else let A = a; (* a must consist of a single nonterminal *) if there exists i such that A C Ni t h e n for e a c h B E Ni d o let qB = fresh_state end; if recursive(Ni) = left then for each (C-+ XI'.'Xm) E P such that CENi AX1,...,Xm~Ni do make_fa(qo, XI " . Xm, qc ) end; f o r e a c h (C --+ DX1 ... X,~) C P s u c h t h a t C,D ~ Ni A X1,...,Xm ~ Ni d o make ffa( qD , X I "" X,~ , qc ) end; let A = A U {(qA, e, ql)} else (* recursive(g,) C {right, cyclic} *) for each (C-+ X1...Xm) E P such that CENi A X1,...,Xm~Ni d o make_fa(qc, X 1 . . . Xm, ql) end; f o r e a c h (C --~ X I ".. X m D ) E P s u c h t h a t C, D E Ni AXI,...,Xm ~ Ni d o makc_fa(qc, XI ".. Xm, qD) end; let A = A U {(qo, e, qa)} end else for e a c h (A -+ fl) C P do make_fa(qo,fl, ql) e n d (* A is not recursive *) end end end. procedure fresh_state(): create some object q such that q ~ K; let K=KU{q}; return q end. Figure 2 Transformation from a grammar G = (E, N,P, S) that is not self-embedding into an equivalent finite automaton 3v = (K, E, A, s, F).  21  Computational Linguistics  Volume 26, Number 1  S --* Aa A --* SB A ~ Bb B --* Bc B ---* d c  N = {S,A,B} ]kf : {N1, N2} N1 = {S,A} recursive(N1) = left N2 -- {B} recursive(N2) = left  __qA a  d  qB  Figure 3 Application of the code from Figure 2 on a small grammar.  S  B  c . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
1. Introduction We will define a dependency transduction model in terms of a collection of weighted head transducers. Each head transducer is a finite-state machine that differs from "standard" finite-state transducers in that, instead of consuming the input string left to right, it consumes it "middle out" from a symbol in the string. Similarly, the output of a head transducer is built up middle out at positions relative to a symbol in the output string. The resulting finite-state machines are more expressive than standard left-to-right transducers. In particular, they allow long-distance movement with fewer states than a traditional finite-state transducer, a useful property for the translation task to which we apply them in this paper. (In fact, finite-state head transducers are capable of unbounded movement with a finite number of states.) In Section 2, we introduce head transducers and explain how input-output positions on state transitions result in middle-out transduction. When applied to the problem of translation, the head transducers forming the dependency transduction model operate on input and output strings that are sequences of dependents of corresponding headwords in the source and target languages. The dependency transduction model produces synchronized dependency trees in which each local tree is produced by a head transducer. In other words, the dependency * 180 Park Avenue,FlorhamPark,NJ 07932 t 180 ParkAvenue,FlorhamPark,NJ 07932 180 ParkAvenue,FlorhamPark,NJ 07932 @ 2000Associationfor ComputationalLinguistics  Computational Linguistics  Volume 26, Number 1  model applies the head transducers recursively, imposing a recursive decomposition of the source and target strings. A dynamic programming search algorithm finds optimal (lowest total weight) derivations of target strings from input strings or word lattices produced by a speech recognizer. Section 3 defines dependency transduction models and describes the search algorithm. We construct the dependency transduction models for translation automatically from a set of unannotated examples, each example comprising a source string and a corresponding target string. The recursive decomposition of the training examples results from an algorithm for computing hierarchical alignments of the examples, described in Section 4.2. This alignment algorithm uses dynamic programming search guided by source-target word correlation statistics as described in Section 4.1. Having constructed a hierarchical alignment for the training examples, a set of head transducer transitions are constructed from each example as described in Section 4.3. Finally, the dependency transduction model is constructed by aggregating the resulting head transducers and assigning transition weights, which are log probabilities computed from the training counts by simple maximum likelihood estimation. We have applied this method of training statistical dependency transduction models in experiments on English-to-Spanish and English-to-Japanese translations of transcribed spoken utterances. The results of these experiments are described in Section 5; our concluding remarks are in Section 6. 2. Head Transducers 2.1 Weighted Finite-State Head Transducers In this section we describe the basic structure and operation of a weighted head transducer. In some respects, this description is simpler than earlier presentations (e.g., Alshawi 1996); for example, here final states are simply a subset of the transducer states whereas in other work we have described the more general case in which final states are specified by a probability distribution. The simplified description is adequate for the purposes of this paper. Formally, a weighted head transducer is a 5-tuple: an alphabet W of input symbols; an alphabet V of output symbols; a finite set Q of states q0. . . . . qs; a set of final states F c Q; and a finite set T of state transitions. A transition from state q to state q' has the form (q,q',w,v,o~,fl, cl where w is a member of W or is the empty string c; v is a member of V or ¢; the integer o~is the input position; the integer fl is the output position; and the real number c is the weight or cost of the transition. A transition in which oz = 0 and fl = 0 is called a head transition. The interpretation of q, q', w, and v in transitions is similar to left-to-right transducers, i.e., in transitioning from state q to state qt, the transducer "reads" input symbol w and "writes" output symbol v, and as usual if w (or v) is e then no read (respectively write) takes place for the transition. The difference lies in the interpretation of the read position c~ a n d the write position ft. To interpret the transition positions as transducer actions, we consider notional input and output tapes divided into squares. On such a tape, one square is numbered 0, and the other squares are numbered 1, 2. . . . rightwards from square 0, and - 1 , - 2 . . . . leftwards from square 0 (Figure 1). A transition with input position ~ and output position fl is interpreted as reading w from square c~ on the input tape and writing v to square fl of the output tape; if square fl is already occupied, then v is written to the next empty square to the left of  46  Alshawi, Bangalore, and Douglas <q, q ' , w, v, a, fl,, c>  Learning Dependency Translation Models  @ o p -@ C I l w l w0 I I -4 -3a,=--2-1 0 1 2 3 4 -4 -3 -2 -1 0 1 2 ,0=3 4 Figure 1 Transition symbols and positions. fl if fl < 0, or to the right of fl if fl > 0, and similarly, if input was already read from position a, w is taken from the next unread square to the left of a if a < 0 or to the right of c~ if a ~ 0. The operation of a head transducer is nondeterministic. It starts by taking a head transition {q, q', w0, v0, 0, 0, c} where w0 is one of the symbols (not necessarily the leftmost) in the input string. (The valid initial states are therefore implicitly defined as those with an outgoing head transition.) w0 is considered to be at square 0 of the input tape and v0 is o u t p u t at square 0 of the output tape. Further state transitions may then be taken until a final state in F is reached. For a derivation to be valid, it must read each symbol in the input string exactly once. At the end of a derivation, the output string is formed by taking the sequence of symbols on the target tape, ignoring any empty squares on this tape. The cost of a derivation of an input string to an output string by a weighted head transducer is the sum of the costs of transitions taken in the derivation. We can now define the string-to-string transduction function for a head transducer to be the function that maps an input string to the output string produced by the lowest-cost valid derivation taken over all initial states and initial symbols. (Formally, the function is partial in that it is not defined on an input when there are no derivations or when there are multiple outputs with the same minimal cost.) In the transducers produced by the training method described in this paper, the source and target positions are in the set {-1,0,1}, though we have also used handcoded transducers (Alshawi and Xia 1997) and automatically trained transducers (A1shawl and Douglas 2000) with a larger range of positions. 2.2 Relationship to Standard FSTs The operation of a traditional left-to-right transducer can be simulated by a head transducer by starting at the leftmost input symbol and setting the positions of the first transition taken to a = 0 and fl = 0, and the positions for subsequent transitions to o~= 1 and fl = 1. However, we can illustrate the fact that head transducers are more 47  Computational Linguistics  a:a  a:a ~  b:b  Volume 26, Number 1  0:0 Figure 2 Head transducer to reverse an input string of arbitrary length in the alphabet {a, b}. expressive than left-to-right transducers by the case of a finite-state head transducer that reverses a string of arbitrary length. (This cannot be performed by a traditional transducer with a finite number of states.) For example, the head transducer described below (and shown in Figure 2) with input alphabet {a, b} will reverse an input string of arbitrary length in that alphabet. The states of the example transducer are Q = {ql, q2} and F = {q2}, and it has the following transitions (costs are ignored here): {ql, q2,a,a,O,O} <ql, q2, b, b, 0, 0> <q2,q2,a,a,-1,1} (q2,q2, b, b, -1,1} The only possible complete derivations of the transducer read the input string right to left, but write it left to right, thus reversing the string. Another similar example is using a finite-state head transducer to convert a palindrome of arbitrary length into one of its component halves. This clearly requires the use of an empty string on some of the output transitions. 3. Dependency Transduction Models 3.1 Dependency Transduction using Head Transducers In this section we describe dependency transduction models, which can be used for machine translation and other transduction tasks. These models consist of a collection of head transducers that are applied hierarchically. Applying the machines hierarchically means that a nonhead transition is interpreted not simply as reading an inputoutput pair (w, v), but instead as reading and writing a pair of strings headed by (w, v) according to the derivation of a subnetwork. For example, the head transducer shown in Figure 3 can be applied recursively in order to convert an arithmetic expression from infix to prefix (Polish) notation (as noted by Lewis and Stearns [1968], this transduction cannot be performed by a pushdown transducer). In the case of machine translation, the transducers derive pairs of dependency trees, a source language dependency tree and a target dependency tree. A dependency tree for a sentence, in the sense of dependency grammar (for example Hays [1964] and Hudson [1984]), is a tree in which the words of the sentence appear as nodes (we do not have terminal symbols of the kind used in phrase structure grammar). In such a tree, the parent of a node is its head and the child of a node is the node's dependent. The source and target dependency trees derived by a dependency transduction model are ordered, i.e., there is an ordering on the nodes of each local tree. This 48  Alshawi, Bangalore, and Douglas  b b ~C~  b:b  Learning Dependency Translation Models b:b  Figure 3 Dependency transduction network mapping bracketed arithmetic expressions from infix to prefix notation.  I I want to make a  collect call I • ,  [ quiero  hac~ una llamada de cobr~ I  Figure 4 Synchronized dependency trees derived for transducing I want to make a collect call into quiero hacer una llamada de cobrar.  means, in particular, that the target sentence can be constructed directly by a simple recursive traversal of the target dependency tree. Each pair of source and target trees generated is synchronized in the sense to be formalized in Section 4.2. An example is given in Figure 4. Head transducers and dependency transduction models are thus related as follows: Each pair of local trees produced by a dependency transduction derivation is the result of a head transducer derivation. Specifically, the input to such a head transducer is the string corresponding to the flattened local source dependency tree. Similarly, the output of the head transducer derivation is the string corresponding to the flattened local target dependency tree. In other words, the head transducer is used to convert a sequence consisting of a headword w and its left and right dependent words to a sequence consisting of a target word v and its left and right dependent words (Figure 5). Since the empty string may appear in a transition in place of a source or target symbol, the number of source and target dependents can be different. The cost of a derivation produced by a dependency transduction model is the sum of all the weights of the head transducer derivations involved. When applying a dependency transduction model to language translation, we choose the target string obtained by flattening the target tree of the lowest-cost dependency derivation that also generates the source string. We have not yet indicated what weights to use for head transducer transitions. The definition of head transducers as such does not constrain these. However, for a dependency transduction model to be a statistical model for generating pairs of strings, we assign transition weights that are derived from conditional probabilities. Several  49  Computational Linguistics  Volume 26, Number 1  Iw1 ..- wk.ll  ~÷1 ..-'~nl  Iv,  Figure 5 Head transducer converts the sequences of left and right dependents (wl ... wk-l/ and (wk+i •••w,) of w into left and right dependents (vl... vj-1) and {Vj+I... Vp) of v.  probabilistic parameterizations can be used for this purpose including the following for a transition with headwords w and v and dependent words w' and v': P(q', w', v', fllw, v, q). Here q and q' are the from-state and to-state for the transition and a and fl are the source and target positions, as before. We also need parameters P(q0, ql]w, v) for the probability of choosing a head transition (qo,ql, w,v,O,O) given this pair of headwords. To start the derivation, we need parameters P(roots(wo, vo)) for the probability of choosing w0,v0 as the root nodes of the two trees. These model parameters can be used to generate pairs of synchronized depen- dency trees starting with the topmost nodes of the two trees and proceeding recursively to the leaves. The probability of such a derivation can be expressed as: P( oots(wo, vo))P(Dwo,vo) where P(Dw,v) is the probability of a subderivation headed by w and v, that is P(Dw,v) = P(qo, qllw, v) H P(qi+l, Wi,Vi,~i, fli]w,v, qi)P(Dwi,vl) 1Kiln for a derivation in which the dependents of w and v are generated by n transitions. 3.2 Transduction Algorithm To carry out translation with a dependency transduction model, we apply a dynamic programming search to find the optimal derivation. This algorithm can take as input either word strings, or word lattices produced by a speech recognizer. The algorithm is similar to those for context-free parsing such as chart parsing (Earley 1970) and the CKY algorithm (Younger 1967). Since w o r d string input is a special case of word lattice input, we need only describe the case of lattices. We now present a sketch of the transduction algorithm. The algorithm works bottom-up, maintaining a set of configurations. A configuration has the form In1, n2, w, v, q, c, t] corresponding to a bottom-up partial derivation currently in state q covering an input sequence between nodes nl and n2 of the input lattice, w and v are the topmost 50  Alshawi, Bangalore, and Douglas 
 Computational Linguistics  Volume 26, Number 1  1.2 Finite-State Approximation and c-Moves In experimenting with finite-state approximation techniques for context-free and more powerful grammatical formalisms (such as the techniques presented in Black [1989], Pereira and Wright [1991, 1997], Rood [1996], Grimley-Evans [1997], Nederhof [1997, 1998], and Johnson [1998]), we have found that the resulting automata often are extremely large. Moreover, the automata contain many e-moves (jumps). And finally, if such automata are determinized then the resulting automata are often smaller. It turns out that a straightforward implementation of the subset construction determinization algorithm performs badly for such inputs. In this paper we consider a number of variants of the subset construction algorithm that differ in their treatment of c-moves. Although we have observed that finite-state approximation techniques typically yield automata with large numbers of c-moves, this is obviously not a necessity. Instead of trying to improve upon determinization techniques for such automata, it might be more fruitful to try to improve these approximation techniques in such a way that more compact automata are produced. 1 However, because research into finite-state approximation is still of an exploratory and experimental nature, it can be argued that more robust determinization algorithms do still have a role to play: it can be expected that approximation techniques are much easier to define and implement if the resulting automaton is allowed to be nondeterministic and to contain c-moves. Note furthermore that even if our primary motivation is in finite-state approximation, the problem of determinizing finite-state automata with c-moves may be relevant in other areas of language research as well. 1.3 Subset Construction and c-Moves The experiments were performed using the FSA Utilities. The FSA Utilities toolbox (van Noord 1997, 1999; Gerdemann and van Noord 1999; van Noord and Gerdemann 1999) is a collection of tools to manipulate regular expressions, finite-state automata, and finite-state transducers. Manipulations include determinization, minimization, composition, complementation, intersection, Kleene closure, etc. Various visualization tools are available to browse finite-state automata. The toolbox is implemented in SICStus Prolog, and is available free of charge under Gnu General Public License via anonymous ftp at ftp://ftp.let.rug.nl/pub/vannoord/Fsa/, and via the web at http://www.let.rug.nl/~vannoord/Fsa/. At the time of our initial experiments with finite-state approximation, an old version of the toolbox was used, which ran into memory problems for some of these automata. For this reason, the subset construction algorithm has been reimplemented, paying special attention to the treatment of E-moves. Three variants of the subset construction algorithm are identified, which differ in the way c-moves are treated:  per graph The most obvious and straightforward approach is sequential in the following sense: Firstly, an equivalent automaton without c-moves is constructed for the input. To do this, the transitive closure of the graph consisting of all c-moves is computed. Secondly, the resulting automaton is then treated by a subset construction algorithm for c-free automata. Different variants of per graph can be identified, depending on the implementation of the c-removal step.  
Computational Linguistics  Volume 26, Number 1  The rest of this section discusses the importance of nonlinear morphology and outlines the research objectives of this work. 1.1 Nonlinear Morphology Early generative morphophonological theory was mainly based on the linear segmental approach of The Sound Pattern of English (Chomsky and Halle 1968). In the mid seventies, however, linguists had departed from this linear framework to a nonlinear one. Goldsmith (1976), working on the phonology of African tone languages, proposed autosegmental phonology with multitiered representations. Goldsmith made use of two tiers to describe tone languages: one to represent sequences of vowels and consonants, and another to describe tone segments. McCarthy (1979) applied autosegmental phonology to Semitic root-and-pattern morphology resulting in what is now known as the theory of nonconcatenative (or nonlinear) morphology, as opposed to concatenative morphology. McCarthy's findings have become ubiquitous. In fact, "every aspect of the theory of morphology and morphophonology," remarks Spencer (1991, 134), "has had to be reappraised in one way or another in the wake of [McCarthy's] analysis of Semitic and other languages." Spencer's statement could well apply to the theory of computational morphology. Two-level morphology (Koskenniemi 1983), as well as its predecessor in the work of Kay and Kaplan (1983), is also deeply rooted in the linear concatenative tradition. Indeed, "if Koskenniemi had been interested in Arabic or Warlpiri rather than Finnish," notes Sproat (1992, 206), "his system might have taken on a rather different character from the start." It would prove difficult, if not impossible, to implement Semitic languages using linguistically motivated theoretical models such as those of McCarthy and others in the field with traditional two-level morphology. Kay (1987) was the first computational linguist to make use of McCarthy's findings. He proposed that a four-tape finite-state machine, as opposed to the traditional two-tape machines of two-level morphology, be used to describe the autonomous morphemes of Arabic. Kay devised a system for manipulating the multitape machine, albeit using an ad hoc procedure to control the movements of the machine's head(s). We shall build upon Kay's work by providing higher-level lexical and rule formalisms, and algorithms for compiling the formalisms into multitape machines, eliminating the need for the ad hoc procedure that controls head movements. We shall revisit Kay's approach in Section 6.1. Previous work to implement Semitic languages, namely, Akkadian (Kataja and Koskenniemi 1988), Arabic (Beesley, Buckwalter, and N e w t o n 1989; Beesley 1990, 1991, 1996, 1998a, 1998b, 1998c, forthcoming) and Hebrew (Lavie, Itai, and Ornan 1990), employed traditional two-level morphology with some augmentation to handle the nonlinearity of stems, but did not make any use of the then-available theory of nonconcatenative morphology. The challenge here lies in the fact that two-level morphology assumes the lexical representation of a surface form to be the concatenation of the corresponding lexical morphemes in question. To resolve the problem, these authors (with the exception of Beesley's work from 1996 on) provided for a simultaneous search of various root and affix lexica, the result of which served as the lexical tape of the two-level system. We shall revisit these approaches in Sections 6.2 and 6.3. Other publications dealing with Semitic computational morphology are confined to proposals for compiling autosegmental descriptions into automata (Kornai 1991; Wiebe 1992; Bird and Ellison 1992). They revolve around encoding autosegmental representations (by various encoding mechanisms) and providing ways for compiling such encodings into finite machines. None provide for lexicon and rule formalisms that can be compiled into their respective encodings or directly into automata. No  78  Kiraz  Multitiered Nonlinear Morphology  Semitic language, to the best of the author's knowledge, has been implemented with these proposals. We shall revisit these approaches in Section 6.4. 1.2 Research Objectives The purpose of this work is to provide a theoretical computational framework under which nonlinear morphology can be handled in a linguistically and computationally motivated manner with the following objectives in mind: 1. The framework is to present a general multitiered computational morphology model that allows for both linear and nonlinear morphophonological descriptions. 2. The formalism of the framework is to handle various linguistic theories and models including McCarthy's initial findings as well as later models for Hebrew (Bat-E1 1989), moraic theory (McCarthy and Prince 1990a, 1995), the affixational approach to handling templates in Arabic (McCarthy 1993), and others. That is, a flexible formalism that leaves the grammar writer with ample room to choose the appropriate linguistic theory for an application. 3. Multitiered lexica and grammars written in the formalism are to be compiled into finite-state machines (multitape automata in this case). The multitape machines are created by a compiler that employs a finite-state engine with an algebraic interface to n-way regular expressions. 4. The multitape machines are to be as close as possible in spirit to two-level morphology in that surface forms map to lexical morphemes. Our lexical level is a multitiered representation. This paper provides an overall description of the theoretical framework, compilation algorithms, and illustrations. Additionally, it discusses other related topics crucial to developing Semitic grammars. Results that emerged earlier from this work appear elsewhere (Kiraz 1994b, 1996, 1997a, 1997b, 1997c, in press), but have been thoroughly enhanced and reworked since. New contributions include enhancing the theoretical framework (Section 3), compiling lexica and rules into multitape finite-state machines (Section 4), and evaluating the current model with respect to previous ones (Section 6). 2. Problems in Semitic Morphophonology The challenges in implementing Semitic morphophonological grammars are numerous. Here, we shall concentrate only on nonlinearity that poses computational difficulties to traditional finite-state models. 2.1 The Nonlinear Stem The main characteristic of Semitic derivational morphology is that of the "root and pattern." The "root" represents a morphemic abstraction, usually consisting of consonants, e.g., {ktb} 'notion of writing'. Stems are derived from the root by the superimposition of patterns. A "pattern" (or "template") is a sequence of segments containing Cs that represent the root consonants, e.g., ClaC2C2aC3 and maC1C2aC3 (the indices refer to root consonants). Root consonants are slotted in, in place of the Cs, to derive stems, e.g., Arabic/kattab/'wrote' and/maktab/'office' from the root {ktb} and the above two patterns, respectively.  79  Computational Linguistics  Volume 26, Number 1  Table 1 Syriac verbal stems with the root {ktb}. The data provides stems in underlying morphological forms; surface forms are parenthesized. The passive is marked by the reflexive prefix {?et}.  Measure Active  Passive ({?et}+)  P%l (1) katab (ktab)  kateb (?etkteb)  Pa~%l (2) katteb  kattab  ?afqel (3) ?akateb (?akteb) ?akatab (?ettaktab)  3 C V C V (7: I I k t b  a  e  I I C V C, C V C,  IV I  k t  b  a  e  V C, V C V C I I I k t b  (~) /katab/  (b) /katteb/  (c)/?akat eb/  Figure 1 Autosegmental representations of Syriac Measures 1-3. Each morpheme sits on its own autonomous tier.  2.2 Various Linguistic Models There are various linguistic models for representing patterns. In traditional accounts, the "vocalism" (i.e., vocalic elements) is collapsed with the pattern (Harris, 1941), e.g., maC1C2aC3 above. Since the late 1970s, however, more sophisticated descriptions have emerged, most notably by McCarthy (1981, 1986, 1993) and McCarthy and Prince (1990a, 1990b, 1995). Consider the Syriac verbal data in Table 1 containing verbs classified according to various measures. 1 Glancing over the table horizontally, one notes the same pattern of consonants and vowels per measure; the only difference is that the vowels are invariably {ae} in active stems, but {aa} in passive stems (apart from Measure 1 whose vocalism is idiosyncratic, a general Semitic phenomenon). Surface forms that result after phonological rules are applied appear in parentheses. The vowel after [k] in the Measure 1 and 3 forms, for example, is deleted because it is a short vowel in an open syllable, a general phenomenon of Aramaic (of which Syriac is a dialect). Further, [7] in the passive of Measure 3 is assimilated into the preceding [t] of the reflexive prefix. Both rules are employed in */?et?akatab/~ /?ettaktab/. Under McCarthy's autosegmental analysis, a stem is represented by three autonomous morphemes: a consonantal root, a vocalism, and a pattern that consists of Cs and Vs. For example, the analysis of /katteb/ (Measure 2) produces three morphemes: the root {ktb} 'notion of writing', the vocalism {ae} 'PERF ACT' and the pattern {CVCCVC} 'Measure 2'. Some stems include affix morphemes, e.g. the prefix {?a} of Measure 3; these sit on their own tier. The segments are linked together by association lines as in Figure 1. Note the spreading of [a] i n / k a t a b / a n d the gemination of [t] i n / k a t t e b / . 
We present a formal semantics for an objectoriented formalism which allows for the representation of plural objects (such as ‘Three N’, ‘Most of the N’, ‘Some N’,...). The semantics is given in terms of a mapping to a variant of Discourse Representation Theory. It is motivated by its suitability for natural language generation and interactive editing of the representations. 
We propose a dynamic programming algorithm for calculating the similarity between two segments of words of the same language. The similarity is considered as a vector whose coordinates refer to the levels of analysis of the segments. This algorithm is extremely efficient for retrieving the best example in Translation Memory systems. The calculus being constructive, it also gives the correspondences between the words of the two segments. This allows the extension of Translation Memory systems towards Example-based Machine Translation. Introduction In Translation Memory (TM) or Example-Based Machine Translation (EBMT) systems, one of the decisive tasks is to retrieve from the database, the example that best approaches the input sentence. In Planas (1999) we proposed a twostep retrieval procedure, where a rapid and rough index-based search gives a short list of example candidates, and a refined matching selects the best candidates from this list. This procedure drastically improves the reusability rate of selected examples to 97% at worst, for our English-Japanese TM prototype; with the classical TM strategy, this rate would constantly decline with the number of non matched words. It also allows a better recall rate when searching for very similar examples. We describe here the Multi-level Similar Segment Matching (MSSM) algorithm on which is based the second step of the above retrieval procedure. This algorithm does not only give the distance between the input and the example source segments, but also indicates which words would match together. It uses F different levels  of data (surface words, lemmas, parts of speech (POS), etc.) in a combined and uniform way. The computation of the worst case requires F*m*(n-m+2) operations, where m and n are respectively the lengths of the input and the candidate (m<=n). This leads to a linear behavior when m and n have similar lengths, which is often the case for TM segments1. Furthermore, because this algorithm gives the exact matching links (along with the level of match) between all of the words of the input and the candidate sentence, it prepares the transfer stage of an evolution of TM that we call Shallow Translation. This involves substituting in the corresponding translated candidate (stored in the memory), the translation of the substituted words, provided that the input and the candidate are "similar enough".  
Most of the studies in the framework of Lambek calculus have considered the parsing process and ignored the generation process. This paper wants to rely on the close link between Lambek calculus and linear logic to present a method for the generation process with semantic proof nets. We express the process as a proof search procedure based on a graph calculus and the solutions appear as a matrix computation preserving the decidability properties, and we characterize a polynomial time case. 
In this article, we present a statistical approach to machine translation that is based on Data-Oriented Parsing: Data-Oriented Translation (DOT). In DOT, we use linked subtree pairs for creating a derivation of a source sentence. Each linked subtree pair has a certain probability, and consists of two trees: one in the source language and one in the target language. When a derivation has been formed with these subtree pairs, we can create a translation from this derivation. Since there are typically many different derivations of the same sentence in the source language, there can be as many different translations for it. The probability of a translation can be calculated as the total probability of all the derivations that form this translation. We give the computational aspects for this model, show that we can convert each subtree pair into a productive rewrite rule, and that the most probable translation can be computed by means of Monte Carlo disambiguation. Finally, we discuss some pilot experiments with the Verbmobil corpus. 
A method is described by which a rhetoricalstructure tree can be realized by a text structure made up of sections, paragraphs, sentences, vertical lists, and other textual patterns, with discourse connectives added (in the correct positions) to mark rhetorical relations. We show that text-structuring can be formulated as a Constraint Satisfaction Problem, so that all solutions respecting constraints on text-structure formation and structural compatibility can be e ciently generated. Of the many solutions generated by this method, some are stylistically preferable to others we show how further constraints can be applied in order to select the best versions. Finally, we discuss some extensions such as the generation of indented text structures. 
This paper presents the use of probabilistic class-based lexica for disambiguation in targetword selection. Our method employs minimal but precise contextual information for disambiguation. That is, only information provided by the target-verb, enriched by the condensed information of a probabilistic class-based lexicon, is used. Induction of classes and ne-tuning to verbal arguments is done in an unsupervised manner by EM-based clustering techniques. The method shows promising results in an evaluation on real-world translations. 
Sumo is a formalism for universal segmentation of text. Its purpose is to provide a framework for the creation of segmentation applications. It is called universal as the formalism itself is independent of the language of the documents to process and independent of the levels of segmentation e.g. words, sentences, paragraphs, morphemes... considered by the target application. This framework relies on a layered structure representing the possible segmentations of the document. This structure and the tools to manipulate it are described, followed by detailed examples highlighting some features of Sumo. Introduction Tokenization, or word segmentation, is a fundamental task of almost all NLP systems. In languages that use word separators in their writing, tokenization seems easy: every sequence of characters between two whitespaces or punctuation marks is a word. This works reasonably well, but exceptions are handled in a cumbersome way. On the other hand, there are languages that do not use word separators. A much more complicated processing is needed, closer to morphological analysis or part-of-speech tagging. Tokenizers designed for those languages are generally very tied to a given system and language. However, the gap becomes smaller when we look at sentence segmentation: a simplistic approach would not be su cient because of the ambiguity of punctuation signs. And if we consider the segmentation of a document into higher-level units such as paragraphs, sections, and so on, we can notice that language becomes less relevant. These observations lead to the de nition of our formalism for segmentation not just tok-  enization that considers the process independently from the language. By describing a segmentation system formally, a clean distinction can be made between the processing itself and the linguistic data it uses. This entails the ability to develop a truly multilingual system by using a common segmentation engine for the various languages of the system; conversely, one can imagine evaluating several segmentation methods by using the same set of data with di erent strategies. Sumo is the name of the proposed formalism, evolving from initial work by Quint, 1999; Quint, 2000. Some theoretical works from the literature also support this approach: Guo, 1997 shows that some segmentation techniques can be generalized to any language, regardless of their writing system. The sentence segmenter of Palmer and Hearst, 1997 and the issues raised by Habert et al., 1998 prove that even in English or French, segmentation is not so trivial. Lastly, A t-Mokhtar, 1997 handles all kinds of presyntactic processing in one step, arguing that there are strong interactions between segmentation and morphology. 
We propose a treatment of `extraposition' which allows items to be assimilated directly even when they appear far from their canonical positions. This treatment supports analyses of a number of phenomena which are otherwise hard to describe. The approach requires a generalisation of standard chart parsing techniques. 
Systems now exist which are able to compile uni cation grammars into language models that can be included in a speech recognizer, but it is so far unclear whether non-trivial linguistically principled grammars can be used for this purpose. We describe a series of experiments which investigate the question empirically, by incrementally constructing a grammar and discovering what problems emerge when successively larger versions are compiled into nite state graph representations and used as language models for a medium-vocabulary recognition task. 
 construction module and processed by the other  This paper describes a hybrid approach to modules of the deep translation branch as shown in spontaneous speech parsing. The implemented figure 1.  parser uses an extended probabilistic LR parsing 2 Spontaneous Speech Parsing  model with rich context and its output is post-  processed by a symbolic tree transformation routine The Integrated Processing unit uses the acoustic  that tries to eliminate systematic errors of the scores of the word hypotheses in the word graph  parser. The parser has been trained for three and a statistical trigram model to guide all  different languages and was successfully integrated connected parsers through the lattice using an A*-  in the Verbmobil speech-to-speech translation search algorithm. This is similar to the work  system. The parser achieves more than 90%/90% presented by (Schmid, 1994) and (Kompe et al.,  labeled precision/recall on parsed Verbmobil 1997). This A*-search algorithm is used by the  utterances while 3% of German and 5% of all probabilistic shift-reduce parser (see section 3) to  English input cannot be parsed.  find the best scored path through the word graph according to acoustic and language model  
A generative statistical model of dependency syntax is proposed based on Tesniere's classical theory. It provides a stochastic formalization of the original model of syntactic structure and augments it with a model of the string realization process, the latter which is lacking in Tesniere's original work. The resulting theory models crossing dependency links, discontinuous nuclei and string merging, and it has been given an e cient computational rendering. 
We present some novel machine learning techniques for the identiﬁcation of subcategorization information for verbs in Czech. We compare three different statistical techniques applied to this problem. We show how the learning algorithm can be used to discover previously unknown subcategorization frames from the Czech Prague Dependency Treebank. The algorithm can then be used to label dependents of a verb in the Czech treebank as either arguments or adjuncts. Using our techniques, we are able to achieve 88% precision on unseen parsed text. 
This paper describes experiments in the automatic construction of lexicons that would be useful in searching large document collections for text fragments that address a speciﬁc information need, such as an answer to a question. 
We present a noun chunker for German which is based on a head-lexicalised probabilistic contextfree grammar. A manually developed grammar was semi-automatically extended with robustness rules in order to allow parsing of unrestricted text. The model parameters were learned from unlabelled training data by a probabilistic context-free parser. For extracting noun chunks, the parser generates all possible noun chunk analyses, scores them with a novel algorithm which maximizes the best chunk sequence criterion, and chooses the most probable chunk sequence. An evaluation of the chunker on 2,140 hand-annotated noun chunks yielded 92 recall and 93 precision. 
One way to begin a negotiation subdialogue is to express doubt at a proposition. However, expressions of doubt occur in a variety of forms, each of which conveys information about the nature of the doubt that is important for the subsequent resolution of the con ict. This paper presents our work on realizing expressions of doubt appropriately in natural language dialogues. 
Verbs were clustered semantically on the basis of their alternation behaviour, as characterised by their syntactic subcategorisation frames extracted from maximum probability parses of a robust statistical parser, and completed by assigning WordNet classes as selectional preferences to the frame arguments. The clustering was achieved a iteratively by measuring the relative entropy between the verbs' probability distributions over the frame types, and b by utilising a latent class analysis based on the joint frequencies of verbs and frame types. 
Backward beam search for dependency analysis of Japanese is proposed. As dependencies normally go from left to right in Japanese, it is eﬀective to analyze sentences backwards (from right to left). The analysis is based on a statistical method and employs a beam search strategy. Based on experiments varying the beam search width, we found that the accuracy is not sensitive to the beam width and even the analysis with a beam width of 1 gets almost the same dependency accuracy as the best accuracy using a wider beam width. This suggested a deterministic algorithm for backwards Japanese dependency analysis, although still the beam search is eﬀective as the N-best sentence accuracy is quite high. The time of analysis is observed to be quadratic in the sentence length. 
A deterministic ﬁnite state transducer is a fast device for analyzing strings. It takes O(n) time to analyze a string of length n. In this paper, an application of this technique to Japanese dependency analysis will be described. We achieved the speed at a small cost in accuracy. It takes about 0.17 millisecond to analyze one sentence (average length is 10 bunsetsu, based on PentiumIII 650MHz PC, Linux) and we actually observed the analysis time to be proportional to the sentence length. The accuracy is about 81% even though very little lexical information is used. This is about 17% and 9% better than the default and a simple system, respectively. We believe the gap between our performance and the best current performance on the same task, about 7%, can be ﬁlled by introducing lexical or semantic information. 
After 3 years of specifying the UNL (Universal Networking Language) language and prototyping deconverters1 from more than 12 languages and enconverters for about 4, the UNL project has opened to the community by publishing the specifications (v2.0) of the UNL language, intended to encode the meaning of NL utterances as semantic hypergraphs and to be used as a "pivot" representation in multilingual information and communication systems. A UNL document is an html document with special tags to delimit the utterances and their rendering in UNL and in all natural languages currently handled. UNL can be viewed as the future "html of the linguistic content". It is only an interface format, leading as well to the reuse of existing NLP components as to the development of original tools in a variety of possible applications, from automatic rough enconversion for information retrieval and information gathering translation to partially interactive enconversion or deconversion for higher quality. We illustrate these points by describing an UNL-French deconverter organized as a specific "localizer" followed by a classical MT transfer and an existing generator.  Keywords UNL, interlingua, pivot, deconversion, UNL-French localization, transfer, generation.  Introduction The UNL project of network-oriented multilingual communication has proposed a standard for encoding the meaning of natural language utterances as semantic hypergraphs intended to be used as pivots in multilingual information and communication systems. In the first phase (1997-1999), more than 16 partners representing 14 languages have worked to build deconverters transforming an (interlingual) UNL hypergraph into a natural language utterance. In this project, the strategy used to achieve this initial objective is free. The UNL-French deconverter under development first performs a "localization" operation within the UNL format, and then classical transfer and generation steps, using the Ariane-G5 environment and some UNL-specific tools. The use of classical transfer and generation steps in the context of an interlingual project may sound surprising. But it reflects many interesting issues about the status of the UNL  language, designed as an interlingua, but diversely used as a linguistic pivot (disambiguated abstract English), or as a purely semantic pivot. After introducing the UNL language, we present the architecture of the UNL-French deconverter, which "generates" from the UNL interlingua by first "localizing" the UNL form for French, within UNL, and then applying slightly adapted but classical transfer and generation techniques, implemented in the Ariane-G5 environment, supplemented by some UNL-specific tools. Then, we discuss the use of the UNL language as a linguistic or semantic pivot for highly multilingual information systems. 
Computational approaches to reference resolution, like Centering Theory, are best at resolving referring expressions which denote familiar referents. We demonstrate how, by taking a proof-theoretic approach to reference resolution within a Centering-type framework, we are able to make sense of referring expressions for unfamiliar referents. These include, in addition to bridging descriptions, de nite descriptions like the rst man" and the rst snowdrops of Spring". We claim that the rst of these denotes a unique subset of a plural discourse antecedent. While the second has no discourse antecedent, we similarly treat it as denoting a unique subset of a familiar referent. 
This paper describes a method for retrieving patterns of words and expressions frequently used in a specic domain and building a dictionary for machine translation(MT). The method uses an untagged text corpus in retrieving word sequences and simplied part-of-speech templates in identifying their syntactic categories. The paper presents experimental results for applying the words and expressions to a patternbased machine translation system. 
We present a new statistical language model based on a combination of individual word language models. Each word model is built from an individual corpus which is formed by extracting those subsets of the entire training corpus which contain that significant word. We also present a novel way of combining language models called the “union model”, based on a logical union of intersections, and use this to combine the language models obtained for the significant words from a cache. The initial results with the new model provide a 20% reduction in language model perplexity over the standard 3-gram approach. Introduction Statistical language models are based on information obtained from the analysis of large samples of a target language. Such models estimate the conditional probability of a word given a sequence of preceding words. The conditional probability can be further used to determine the likelihood of a sentence through the product of the individual word probabilities. A popular type of statistical language model is the dynamic language model, which dynamically modifies conditional probabilities depending on the recent word history. For example the cachedbased natural language models (Kuhn R. & De Mori R., 1990) incorporates a cache component into the model, which estimates the probability  of a word depending upon its recent usage. Trigger based models go a step further by triggering associated words to each content word in a cache giving each associated word a higher probability (Lau et al., 1993). Our statistical language model, based upon individual word domains, extends these ideas by creating a new language model for each significant word in the cache. A significant word is hard to define; it is any word that significantly contributes to the content of the text. We define it as any word which is not a stop word, i. e. articles, prepositions and some of the most frequently used words in the language such as “will”, “now”, “very”, etc. Our model combines individual word language models with a standard global n-gram language model. A training corpus for each significant word is formed from the amalgamation of the text fragments taken from the global training corpus in which that word appears. As such these corpora are smaller and closely constrained; hence the individual language models are more precise than the global language model and thereby should offer performance gains. One aspect of the performance of this joint model is how the global language model is to be combined with the individual word language models. This is explored later. This paper is organised as follows. Section 1 explains the basis for this model. The mathematical background and how the models are combined are explained in section 2. In the third section, a novel method of combining the word models, the probabilistic-union model is explained. Finally, results and conclusion are drawn.  
Through the alignment of definitions from two or more different sources, it is possible to retrieve pairs of words that can be used indistinguishably in the same sentence without changing the meaning of the concept. As lexicographic work exploits common defining schemes, such as genus and differentia, a concept is similarly defined by different dictionaries. The difference in words used between two lexicographic sources lets us extend the lexical knowledge base, so that clustering is available through merging two or more dictionaries into a single database and then using an appropriate alignment technique. Since alignment starts from the same entry of two dictionaries, clustering is faster than any other technique. The algorithm introduced here is analogybased, and starts from calculating the Levenshtein distance, which is a variation of the edit distance, and allows us to align the definitions. As a measure of similarity, the concept of longest collocation couple is introduced, which is the basis of clustering similar words. The process iterates, replacing similar pairs of words in the definitions until no new clusters are found. Introduction Clustering methods to identify semantically similar words are usually divided in relationbased and distribution-based approaches [Hirawaka, Xu and Haase 1996]. Relation-based clustering methods rely on the relations in a semantic network or ontology to judge the similarity between two concepts, either by measuring the shortest length that connects two  concepts in the hierarchical net [Agirre and Rigau 1996], or by comparing the information content shared by the members under the same cluster [Morris and Hirst 1991, Resnik 1997]. However, even although these ontologies describe a huge number of members for a cluster, few words of a category may be interchangeable in the same context and then used as members of the same cluster. This means that not all words in a category are necessary. Conversely, distribution-based clustering methods depend on pure statistical analysis of the lexical occurrences in running texts. A major drawback is that distribution-based methods require us to process a large amount of data in order to get more reliable results. Moreover, the use of large corpora is not always practical, due to economic, time or capabilities factors. Gao [1997] states that the problem for statistical alignment algorithms, such as those based on the facts described by Gale and Church [1991], is the low frequency of words that occur in parallel corpora. The consequences for lacking large corpora include results based on low-frequency words, which are quite unrepresentative for clustering. From a methodological point of view, there is, in addition to the above two approaches, a little known approach called the analogy-based approach. This employs an inferential process and is used in computational linguistics and artificial intelligence as an alternative to current rule-based linguistic models. 
 verb occurs in the dative prepositional phrase al-  We automatically classify verbs into lexical semantic classes, based on distributions of indicators of verb alternations, extracted from a very large annotated corpus. We address a problem which is particularly di cult because the verb classes, although semantically di erent, show similar surface syntactic behavior. Five grammatical features are su cient to reduce error rate by more than 50 over chance: we achieve almost 70  ternation in English. One diagnostic for diathesis alternations is the subcategorization alternatives of a verb. However, some classes exhibit the same subcategorization possibilities but di er in their argument structures, i.e. the content of the thematic roles assigned to the arguments of the verb. This type of situation constitutes a particularly di cult case for corpus-based classi cation methods.  accuracy in a task whose baseline performance is In this paper, we apply corpus-based lexical  34, and whose expert-based upper bound we cal- acquisition methodology to distinguish classes of  culated at 86.5. We conclude that corpus-driven verbs which allow the same subcategorizations,  extraction of grammatical features is a promising but di er in thematic roles. We rst assume that  methodology for ne-grained verb classi cation. one can automatically restrict the choice of classes  
We describe a method of word segmentation in Japanese in which a broad-coverage parser selects the best word sequence while producing a syntactic analysis. This technique is substantially different from traditional statistics- or heuristics-based models which attempt to select the best word sequence before handing it to the syntactic component. By breaking up the task of finding the best word sequence into the identification of words (in the word-breaking component) and the selection of the best sequence (a by-product of parsing), we have been able to simplify the task of each component and achieve high accuracy over a wide variety of data. Word-breaking accuracy of our system is currently around 97~98%. 1. Introduction Word-breaking is an unavoidable and crucial first step toward sentence analysis in Japanese. In a sequential model of word-breaking and syntactic analysis without a feedback loop, the syntactic analyzer assumes that the results of word-breaking are correct, so for the parse to be successful, the input from the word-breaking component must include all words needed for a desired syntactic analysis. Previous approaches to Japanese word segmentation have relied on heuristics- or statistics-based models to find the single most likely sequence of words for a given string, which can then be passed to the syntactic component for further processing. The most common heuristics-based approach utilizes a connectivity matrix between parts-of-speech and word probabilities. The most likely analysis can be obtained by searching for the path with the minimum connective cost (Hisamitsu and Nitta 1990), often supplemented by additional heuristic devices such as the longest-string-match or the least-number-of-bunsetsu (phrase). Despite its popularity, the connective cost method has a major disadvantage in that hand-tuning is not only labor-intensive but also unsafe, since adjusting the  cost for one string may cause another to break. Various heuristic (e.g. Kurohashi and Nagao 1998) and statistical (e.g. Takeuchi and Matsumoto 1997) augmentations of the minimum connective cost method have been proposed, bringing segmentation accuracy up to around 98-99% (e.g. Kurohashi and Nagao 1998, Fuchi and Takagi 1998). Fully stochastic language models (e.g. Nagata 1994), on the other hand, do not allow such manual cost manipulation and precisely for that reason, improvements in segmentation accuracy are harder to achieve. Attaining a high accuracy using fully stochastic methods is particularly difficult for Japanese due to the prevalence of orthographic variants (a word can be spelled in many different ways by combining different character sets), which exacerbates the sparse data problem. As a result, the performance of stochastic models is usually not as good as the heuristics-based language models. The best accuracy reported for statistical methods to date is around 95% (e.g. Nagata 1994). Our approach contrasts with the previous approaches in that the word-breaking component itself does not perform the selection of the best segmentation analysis at all. Instead, the word-breaker returns all possible words that span the given string in a word lattice, and the best word sequence is determined by applying the syntactic rules for building parse trees. In other words, there is no task of selecting the best segmentation per se; the best word-breaking analysis is merely a concomitant of the best syntactic parse. We demonstrate that a robust, broad-coverage parser can be implemented directly on a word lattice input and can be used to resolve word-breaking ambiguities effectively without adverse performance effects. A similar model of word-breaking is reported for the problem of Chinese word segmentation (Wu and Jiang 1998), but the amount of ambiguity that exists in the word  lattice is much larger in Japanese, which requires a different treatment. In the following, we first describe the word-breaker and the parser in more detail (Section 2); we then report the results of segmentation accuracy (Section 3) and the results of related experiments assessing the effects of the segmentation ambiguities in the word lattice to parsing (Section 4). In Conclusion, we discuss implications for future research. 2. Using a broad-coverage parser for word-breaking The word-breaking and syntactic components discussed in the current study are implemented within a broad-coverage, multi-purpose natural language understanding system being developed at Microsoft Research, whose ultimate goal is to achieve deep semantic understanding of natural language1. A detailed description of the system is found in Heidorn (in press). Though we focus on the word-breaking and syntactic components in this paper, the syntactic analysis is by no means the final goal of the system; rather, a parse tree is considered to be an approximate first step toward a more useful meaning representation. We also aim at being truly broad-coverage, i.e., returning useful analyses irrespective of the genre or the subject matter of the input text, be it a newspaper article or a piece of e-mail. For the proposed model of word-breaking to work well, the following properties of the parser are particularly important. The bottom-up chart parser creates syntactic analyses by building incrementally larger phrases from individual words and phrases (Jensen et al. 1993). The analyses that span the entire input string are the complete analyses, and the words used in that analysis constitutes the word-breaking analysis for the string. Incorrect words returned by the word-breaker are filtered out by the syntactic rules, and will not make it into the final complete parse. All the grammar rules, written in the formalism of Augmented Phrase Structure Grammar (Heidorn 1975), are binary, a feature crucial for dealing with free word-order and 
We use seven machine learning algorithms for one task: identifying base noun phrases. The results have been processed by di erent system combination methods and all of these outperformed the best individual result. We have applied the seven learners with the best combinator, a majority vote of the top ve systems, to a standard data set and managed to improve the best published result for this data set. 
We have developed a summarization method that creates a summary suitable for the process of sifting information retrieval results. Unlike conventional methods that extract important sentences, this method constructs short phrases to reduce the burden of reading long sentences. We have developed a prototype summarization system for Japanese. Through a rather large-scale taskbased experiment, the summary this system creates proved to be effective to sift IR results. This summarization method is also applicable to other languages such as English.  Introduction Summaries are used to select relevant information from information retrieval results. The goal of summarization for such “indicative” use is to provide fast and accurate judgement. Most automatic summarization systems adopt the “sentence selection” method, which gives a score to every sentence on the basis of its characteristics, such as word frequency, the position in which it appears, etc. and selects sentences with high scores. The sentences collected in such a way tend to be so long and complex that the reader must reconstruct the structure while reading them. Reading such sentences involves some annoyance. Our aim is to reduce this burden by providing an “at-a-glance” summary. Phrase-representation summarization is a method to create the “at-a-glance” summary for the Japanese language. Here we present the concept, the algorithm, and evaluation of the efficacy of the summary produced by a prototype based on this method. Extension to English is also discussed. 
This paper describes a statistical approach to the interpretation of metonymy. A metonymy is received as an input, then its possible interpretations are ranked by applying a statistical measure. The method has been tested experimentally. It correctly interpreted 53 out of 75 metonymies in Japanese.  
We present an implemented system for processing deﬁnite descriptions. The system is based on the results of a corpus analysis previously reported, which showed how common discourse-new descriptions are in newspaper corpora, and identiﬁed several problems to be dealt with when developing computational methods for interpreting bridging descriptions. The annotated corpus produced in this earlier work was used to extensively evaluate the proposed techniques for matching deﬁnite descriptions with their antecedents, discourse segmentation, recognizing discourse-new descriptions, and suggesting anchors for bridging descriptions. 
In this paper, we describe a system and methods for nding structural correspondences from the paired dependency structures of a source sentence and its translation in a target language. The system we have developed nds word correspondences rst, then nds phrasal correspondences based on word correspondences. We have also developed a GUI system with which a user can check and correct the correspondences retrieved by the system. These structural correspondences will be used as raw translation patterns in a corpus-based translation system. 
This paper describes an algorithm for accelerating the CFG-parsing process by using dependency or modi er-modi ee relationship information given by, for instance, dependency estimation programs such as stochastic parsers, user's indication in an interactive application, and linguistic annotations added in a source text. This is a method for enhancing existing grammar-based CFG-parsing system by using dependency information. 
In this paper, we investigate the acoustic prosodic marking of demonstrative and personal pronouns in taskoriented dialog. Although it has been hypothesized that acoustic marking affects pronoun resolution, we ﬁnd that the prosodic information extracted from the data is not sufﬁcient to predict antecedent type reliably. Interspeaker variation accounts for much of the prosodic variation that we ﬁnd in our data. We conclude that prosodic cues should be handled with care in robust, speakerindependent dialog systems. 
Statistical signiﬁcance testing of diﬀerences in values of metrics like recall, precision and balanced F-score is a necessary part of empirical natural language processing. Unfortunately, we ﬁnd in a set of experiments that many commonly used tests often underestimate the significance and so are less likely to detect diﬀerences that exist between diﬀerent techniques. This underestimation comes from an independence assumption that is often violated. We point out some useful tests that do not make this assumption, including computationally-intensive randomization tests. 
This paper introduces a new type of grammar learning algorithm, inspired by string edit distance Wagner and Fischer, 1974. The algorithm takes a corpus of at sentences as input and returns a corpus of labelled, bracketed sentences. The method works on pairs of unstructured sentences that have one or more words in common. When two sentences are divided into parts that are the same in both sentences and parts that are di erent, this information is used to nd parts that are interchangeable. These parts are taken as possible constituents of the same type. After this alignment learning step, the selection learning step selects the most probable constituents from all possible constituents. This method was used to bootstrap structure on the ATIS corpus Marcus et al., 1993 and on the OVIS1 corpus Bonnema et al., 1997. While the results are encouraging we obtained up to 89.25  non-crossing brackets precision, this paper will point out some of the shortcomings of our approach and will suggest possible solutions. 
In this paper, we present a summarization system for spontaneous dialogues which consists of a novel multi-stage architecture. It is speci cally aimed at addressing issues related to the nature of the texts being spoken vs. written and being dialogical vs. monological. The system is embedded in a graphical user interface and was developed and tested on transcripts of recorded telephone conversations in English and Spanish Callhome. 
1. Introduction Research on syntactic parsing has been a focus in natural language processing for a long time. As the development of corpus linguistics, many statistics-based parsers were proposed, such as Magerman(1995)’s statistical decision tree parser, Collins(1996)’s bigram dependency model parser, Ratnaparkhi(1997)’s maximum entropy model parser. All of them tried to get the complete parse trees of the input sentences, based on the statistical data extracted from an annotated corpus. The best parsing accuracy of these parsers was about 87%. Realizing the difficulties of complete parsing, many researches turned to explore the partial parsing techniques. Church(1988) proposed a simple stochastic technique for recognizing the non-recursive base noun phrases in English. Voutilaimen(1993) designed an English noun phrase recognition tool --- NPTool. Abney(1997) applied both rule-based and statistics-based approaches for parsing chunks in English. Due to the advantages of simplicity and robustness, these systems can be acted as good preprocessors for the further complete parsing. In this paper, we will introduce our partial parsing approach for the Chinese language. We first proposed a shallow syntactic knowledge description: constituent boundary representation.  It simplified the complex constituent levels in parse trees and only kept the boundary information of every word in different constituents. Then, we developed a simple and efficient constituent boundary prediction algorithm, based on different local context templates learned from the annotated corpus. An open test on 2780 Chinese real text sentences showed the satisfying results: 94%(92%) precision for the words with multiple (single) boundary tag output. 2. Constituent boundary description The constituent boundary representation comes from the simplification of the complete parse trees of the sentences. It omits the constituent1 levels in parse trees and only keeps the boundary information of every word in different constituents, i.e. it is at the left boundary, right boundary or middle position of a constituent. Evidently, if the input sentence has only one parse tree, i.e. without syntactic ambiguity, the constituent boundary position of every word in the sentence is clear and definite. In the sense, the constituent boundary tag indicates the basic syntactic structure information in the sentence. Separating them from the constituent structure tree and assigning them to every word in the sentence, we can form a special syntactic unit: word boundary block (WBB). Definition: A word boundary block is the combination of the word(including part-of-speech information) and its constituent boundary tag, i.e. wbbi=<wi, bi>, where wi is the ith word in the sentence, bi can value 0,1,2, which means wi is at 
We present a system for manipulating a wide class of linguistic diagrams, which is conﬁgurable and extensible, and allows deployment as a web-delivered system. A major theme of this work is the transfer of the devices of formal grammar into the analysis and construction of diagrams. 
In this paper I elaborate a model of competence for corpus-based machine translation CBMT along the lines of the representations used in the translation system. Representations in CBMT-systems can be rich or austere, molecular or holistic and they can be ne-grained or coarse-grained. The paper shows that di erent CBMT architectures are required dependent on whether a better translation quality or a broader coverage is preferred according to Boitet 1999's formula: Coverage * Quality = K". 
The Korean Combinatory Categorial Grammar KCCG formalism can uniformly handle word order variation among arguments and adjuncts within a clause as well as in complex clauses and across clause boundaries, i.e., long distance scrambling. In this paper, incremental parsing technique of a morpheme graph is developed using the KCCG. We present techniques for choosing the most plausible parse tree using lexical information such as category merge probability, head-head co-occurrence heuristic, and the heuristic based on the coverage of subtrees. The performance results for various models for choosing the most plausible parse tree are compared. 
Work on the production of texts in English describing instances of a particular event type from multiple news sources will be described. A system has been developed which extracts events , such as meetings, from texts in English, Russian, Spanish, and Japanese. The extraction is currently carried out using only ontological information. The results of a set of such extractions were combined to produce a table of event instances, date stamped, with links back to the original documents. The original documents can then be summarized and translated by the system on demand. By using techniques from information retrieval, information extraction, summarization, and machine translation, in a multi-lingual environment, new documents can be produced which provide "at a glance" access to news on events from multiple sources. The paper concludes with a discussion of the key resources which need to be developed to enhance the accuracy and coverage of the techniques used in our experiment. 
We present a class-based approach to building a verb lexicon that makes explicit the close association between syntax and semantics for Levin classes. We have used Lexicalized Tree Adjoining Grammars to capture the syntax associated with each verb class and have augmented the trees to include selectional restrictions. In addition, semantic predicates are associated with each tree, which allow for a compositional interpretation. 
We propose an algorithm for the transfer of packed linguistic structures, that is, ﬁnite collections of labelled graphs which share certain subparts. A labelled graph is seen as a word over a vocabulary of description elements (nodes, arcs, labels), and a collection of graphs as a set of such words, that is, as a language over description elements. A packed representation for the collection of graphs is then viewed as a context-free grammar which generates such a language. We present an algorithm that uses a conventional set of transfer rules but is capable of rewriting the CFG representing the source packed structure into a CFG representing the target packed structure that preserves the compaction properties of the source CFG. 
This paper describes algorithms and software developed to characterise and detect generic intelligent language-like features in an input signal, using Natural Language Learning techniques: looking for characteristic statistical "language-signatures" in test corpora. As a first step towards such species-independent language-detection, we present a suite of programs to analyse digital representations of a range of data, and use the results to extrapolate whether or not there are language-like structures which distinguish this data from other sources, such as music, images, and white noise. We assume that generic speciesindependent communication can be detected by concentrating on localised patterns and rhythms, identifying segments at the level of characters, words and phrases, without necessarily having to "understand" the content. We assume that a language-like signal will be encoded symbolically, i.e. some kind of character-stream. Our language-detection algorithm for symbolic input uses a number of statistical clues: data compression ratio, "chunking" to find character bit-length and boundaries, and matching against a Zipfian type-token distribution for "letters" and "words". We do not claim extensive (let alone exhaustive) empirical evidence that our languagedetection clues are "correct"; the only real test will come when the Search for ExtraTerrestrial Intelligence finds true alien signals. If and when true SETI signals are found, the first step to interpretation is to identify the language-like features, using techniques like the above. Our current research goal is to apply Natural Language Learning techniques to the identification of "higher-level" grammatical and semantic structure in a linguistic signal.  Introduction A useful thought experiment is to imagine eavesdropping on a signal from outer space. How can you decide that it is a message between intelligent life forms, without dialogue with the source? What is special about the language signal that separates it from non-language? What special ’zone’ in the signal universe does language occupy? Is it, indeed, separable from other semi-structured sources, such as DNA and music (fig 1). Solving this problem might not only be useful in the event of detecting such signals from space, but also, by deliberately ignoring preconceptions based on human texts, may provide us with some better understanding of what language really is.  The Signal Universe Very Random e.g White Noise  Long chain Polymers  Structured Signals  Very predictable e.g Pulsars  All Language (only?)  Music D.N.A  Figure 1 However, we need to start somewhere, and our initial investigations - which this paper summarises - make some basic assumptions (which we would hope to relax in later research). Namely, that identifiable script will be a serial string, possessing a hierarchy of elements broadly equivalent to ‘characters,’ ‘words’, and  ‘spaces’, and possess something akin to human grammar.  Identifying the ‘Character Set’ In ‘real’ decoding of unknown scripts it is accepted that identifying the correct set of discrete symbols is no mean feat (Chadwick 1967). To make life simple for ourselves we assume a digital signal with a fixed number of bits per character. Very different techniques are required to deal with audio or analogue equivalent waveforms (Elliott & Atwell 99, 00). We have reason to believe that the following method can be modified to relax this constraint, but this needs to be tested further. The task then reduces to trying to identify the number of bits per character. Suppose the probability of a bit is PI. Then the message entropy of a string of length N will be given by:  E = SUM [PI ln Pi]; i =1,N  If the signal contains merely a set of random digits, the expected value of this function will rise monotonically as N increases. However, if the string contains a set of symbols of fixed length representing a character set used for communication, it is likely to show some decrease in entropy when analysed in blocks of this length, because the signal is ‘less random’ when thus blocked. Of course, we need to analyse blocks that begin and end at character boundaries. We simply carry out the measurements in sliding windows along the data. In figure 2 below, we see what happens when we apply this to samples of 8-bit ASCII text:  Entropy  Figure 2  Language  3 4  5 4  6 5  76 8 7 9 8  Bit Length  Entropy profile as an indicator of character bit-length  We notice a clear drop, as predicted, for a bit length of 8.Modest progress though it may be, it is not unreasonable to assume that the first piece of evidence for the presence of language-like structure, would be the identification of a lowentropy, character set within the signal.  Identifying ‘Words’ Again, work by crytopaleologists suggests that, once the character set has been found, the separation into word-like units, is not trivial and again we cheat, slightly: we assume that the language possesses something akin to a ‘space’ character. Taking our entropy measurement described above as a way of separating characters, we now try to identify the one, which represents ‘space’. It is not unreasonable to believe that, in a wordbased language, it is likely to be one of the most frequently used characters. Using a number of texts in a variety of languages, we first identified the top three most used characters. For each of these we hypothesised in turn that it represented ‘space’. This then allowed us to segment the signal into words-like units (‘words’ for simplicity). We could then compute the frequency distribution of words as a function of word length, for each of the three candidate ‘space’ characters (fig 3).  Frequency 1 4 7 10 13 16 19 22 25 28 31  Candidate w ord patterns using the three Figuremo3s:t fCreaqnuednitdpaattteerwnsoarsds-elpeenragttohrsdistributions  450  using the 3 most frequent characters.  400  350  300  250  200  150  100  50  0  W ord length  It can be seen that one ‘separator’ candidate (unsurprisingly, in fact, the most frequent character of all) results in a very varied distribution of word lengths. This is an interesting distribution, which, on the right hand side of the peak, approximately follows the well-known ‘law’ according to Zipf (Zipf, 1949), which predicts this  Frequency % 1 4 7 10 13 16 19 22 25  behaviour on the grounds of minimum effort in a communication act. To ascertain whether the word-length frequency distribution holds for language in general, multiple samples from 20 different languages from Indo-European, Bantu, Semitic, Finno-Ugrian and MalayoPolynesian groups were analysed (fig 4).  Word MleunltigplUtehgsraidamnip,slatenrsidfbrMouamtlaiIoynodn-oPs-oEiluynnroempseiaaunnl,ltaBinpagnluetaug,seSaegmmroiptuicpl,seFsinfnroo- m Indo-European, Semitic, Finno-Ugrian, and MalayoPo2ly5n.e0s0ian language groups 20.00 15.00 10.00 5.00 0.00  Figure 4  Word length  Frequency % 1 4 7 10 13 16 19 22 25  Using statistical measures of significance, it was found that most groups fell well within 5% limits – only two individual languages were near exceeding these limits – of the proposed Human language word-length profile shown in fig 5. Figure 5:HHuummaann lLaanngguaugaegwe oPrrdo-flielnegth fr2e0q.u00ency distribution profile 15.00 10.00 5.00 0.00 Word length Zipf’s law is a strong indication of language-like behaviour. It can be used to segment the signal provided a ‘space’ character exists. However, we should not assume Zipf to be an infallible language detector. Other natural phenomena such as molecular distribution in yeast DNA possess characteristics of power laws. Analyses of protein length distributions also display Poisson distributions where the number of proteins is plotted against the lengths of amino acids (Jenson 1998).  Identifying ‘Phrases’ Although alien brains may be more or less powerful than ours (Norris 1999), it is reasonable to assume that all intelligent problem solvers are subject to the same ultimate constraints of computational power and storage and their symbol systems will reflect this. Thus, language must use small sets of rules to generate a vast world of implications and consequences. Perhaps its most important single device is the use of embedded clauses and phrases (Minsky 1984), with which to represent an expression or description, however complex, as a single component of another description. In serial languages, this appears to be achieved by clustering words into ‘chunks’ (phrases, sentences) of information, which are more-or-less consistent and selfcontained elements of thought. Furthermore, in human language at least, these ‘chunks’ tend to consist of content terms, which describe what the chunk is ‘about’ and functional terms, which attribute references and context by which the content terms convey their information unambiguously. ‘King’ is usually a content term; ‘of’ and ‘the’ are functional. We use ‘term’ rather than word, because many languages make far less use of full words for functional operations than does English: in Latin the transformation ‘rex’ (‘king’) to ‘regis’ (of the king) is one such example. Functional terms in a language tend to be short, probably attributable to the principle of least effort, as they are used frequently. A further distinguishing characteristic of functional and content terms is that different texts will often vary in their content but tend to share a common linguistic structure and therefore make similar use of functional terms. That is, the probability distribution of content terms will vary from text to text, but the distribution of function terms will not. Using English text, which had been enciphered using a simple substitution cipher (to avoid cheating), we identified  across a variety of texts, the most common words, with least inter-text variation. These we call ‘candidate function words’. Now, suppose these words occurred at random in the signal: we would expect to see the spacing between them to be merely a function of their individual probabilities of occurrence. Analysing this statistically (as a Poisson distribution) or simply simulate it practically, we find that there are a non-insignificant number of cases wherein there are very large gaps (of the order of several tens of words) between successive occurrences. Compare this with the results from our analysis (fig 6).  Frequency  Figure 6 Function Word separation in English.  
This paper describes a machine translation architecture that integrates the use of examples for flexible, idiomatic translations with the use of linguistic rules for broad coverage and grammatical accuracy. We have implemented a prototype for English-to-Japanese translation, and our evaluation shows that the system has good translation quality, and only requires reasonable computational resources.  
We have implemented an interactive, Web-based, chat-style machine translation system, supporting speech recognition and synthesis, local- or thirdparty correction of speech recognition and machine translation output, and online learning. The underlying client-server architecture, implemented in JavaTM, provides remote, distributed computation for the translation and speech subsystems. We further describe our Web-based user interfaces, which can easily produce di erent useful con gurations. 
In this paper, we present a new phrase break prediction architecture that integrates probabilistic approach with decision-tree based error correction. The probabilistic method alone usuatollyinhsuereenrts dfraotamsppaerrsfeonrmesasnpcreobdleegmrasdaantdioint odnulye covers a limited range of contextual information. Moreover, the module can not utilize the selective morpheme tag and relative distance to the other phrase breaks. The decision-tree based error correction was tightly integrated to overcome these limitations. Tquheenicneitisiaclloyrrpehcrtaedsewbirtehatkhteaegrgreodr mcoorrrpechteimngedsee-cision tree which was induced by C4.5 from the correctly tagged corpus with the output of the probabilistic predictor. The decision tree-based post error correction provided improved results even with the phrase break predictor that has pcaonorbineitiaexl ipbelryfotrumnaendcet.o Mnoewreocvoerrp, uths ewsiytshtoeumt massive retraining. 
This paper introduces a description language for syntactically annotated corpora which allows for encoding both the syntactic annotation to a corpus and the queries to a syntactically annotated corpus. In terms of descriptive adequacy and computational e ciency, the description language is a compromise between script-like corpus query languages and high-level, typed uni cation-based grammar formalisms. 
Our principle objective was to reduce the error rate of speech recognition systems used by professional translators. Our work concentrated on Spanish-to-English translation. In a baseline study we estimated the error rate of an off-the-shelf recognizer to be 9.98%. In this paper we describe two independent methods of improving speech recognizers: a machine translation (MT) method and a topic-based one. An evaluation of the MT method suggests that the vocabulary used for recognition cannot be completely restricted to the set of translations produced by the MT system and a more sophisticated constraint system must be used. An evaluation of the topic-based method showed significant error rate reduction, to 5.07%. Introduction Our goal is to improve the throughput of professional translators by using speech recognition. The problem with using current offthe-shelf speech recognition systems is that these systems have high error rates for similar tasks. If the task is simply to recognize the speech of a person reading out loud, the error rate is relatively low; the error rate of large vocabulary research systems (20,000-60,000 word vocabularies) performing such a task is, at best, around 10% (see, for example, Robinson and Christie 1998, Renals and Hochberg 1996, Hochberg et al. 1995 and Siegler and Stern 1995). The popular press has reported slightly lower results for commercial systems. For example, PC Magazine (Poor 1998) compared Dragon’s NaturallySpeaking and IBM’s  ViaVoice (both continuous speech recognition systems with approximately 20,000 word vocabularies). They evaluated these systems by having five speakers read a 350 word text at a slow pace (1.2 words/second) after completing a half hour training session with each system. The average recognition error rate was 11.5% (about 40 errors in the 350 word text). An evaluation of the same two systems without training resulted in a recognition error rate of 34% (Keizer 1998). If the task is more difficult than recognizing the speech of a person reading, the error rate increases dramatically. For example, Ringger (1995) reports an average error rate of 30% for recognizing careful, spontaneous speech on a specific topic. However, the error rate of paced speech can be as low as 5% if the vocabulary is severely limited or if the text is highly predictable and the system is tuned to that particular genre. Unfortunately, the speech of expert translators producing spoken translations does not fall into any of the “easy to recognize” categories. In many translation tasks the source document is in electronic form and the obvious question to ask is if an analysis of the source document could lead to a reduction of the speech recognition error rate. For example, suppose we have a robust machine translation system and use it to generate all the possible translations of a given source text. We could then use this set of translations to help predict what the translator is saying. We describe this approach in §1 below. A simpler approach is to identify the topic of the source text and use that topic to aid in speech recognition. Such as approach is described in §2. Both methods were tested in a Spanish-to-English translation task.  This research rests on two crucial ideas. The first is that lexical and translation knowledge extracted from source documents by automated natural language processing can be utilized in a large-vocabulary, continuous speech recognizer to achieve low word-error rates. The second idea is that the translator should be able to dictate a translation and correct the resulting transcription in much less time than if they had to type the translation themselves or rely on a transcriber/typist. 1. Using machine translation The difference between a typical speech dictation system and the situation described above, is that the translator is viewing the source text on a computer—that is, the text is available online. This source text can be analyzed using a machine translation (MT) component. Hopefully, this analysis will cut down on the recognition perplexity by having the recognizer make choices only from the set of possible renderings in the target language of the words in the source language. In this section we describe the MT subsystem in detail. The function of this subsystem is to take Spanish sentences as input and produce a set of English words that are likely to occur in translations of these sentences. For example, if the Spanish text is 1. Butros Ghali propone vía diplomática para solucionar crisis haitiana we would expect the translation set to include the words (among others): {Boutros, Ghali, proposes, diplomatic, route, to, settle, Haitian, crisis} Hopefully, this translation set will be a good predictor of what the translator actually said. 1.1 The MT subsystem The MT subsystem consists of 4 components: the Spanish morphological analyzer, the dictionary lookup component, the lexical transfer component, and the English morphological generator. These components are briefly described in this section.  1.1.1 Spanish morphological analyzer The morphology analyzer takes Spanish words as input and outputs a set of possible morphological analyses for those words. Each analysis consists of the root word and a set of feature structures representing the information obtained from inflectional morphology. Examples are given below.  Word cafés pequeña podría  Feature structure ((root café) (cat n) (number plural)) ((root pequeño)(cat adj)(gender f)) ((root podrir) (cat v) (tense imperfect indicative) (person 3)(number singular))  1.1.2 Dictionaries and dictionary lookup The dictionary lookup component takes a feature structure produced by the morphological analyzer, looks up the root-word/part-of-speech pair in the dictionary, and adds information to the existing feature structure. The words in the dictionary were derived from doing a corpus analysis of a set of 20 Spanish test documents. All the unique words in this corpus, including proper nouns, were included in the dictionary (approximately 1,500 words). A few examples are shown below.  actividad comenzar cuestion  ((root actividad) (cat n) (trans  activity energy) (gender f))  ((root comenzar)(cat v)(trans begin  start) (verbtype irregular 129))  ((root cuestion) (cat n) (trans question  dispute  problem  issue)  (gender f))  1.1.3 The lexical transfer component At the end of the dictionary lookup phase, for each word in the Spanish sentence we have a feature structure containing the information in the dictionary entry along with the parameter values that were gained from morphological analysis. One feature, trans, contains the possible English translations of that Spanish word. The lexical transfer component converts this Spanish feature structure to one or more English feature structures; one feature structure is created for each value in the trans field. For example, the feature structure associated with an instance of actividad encountered in some text  will be ‘transferred’ to two English feature structures: one for activity and one for energy. Similarly, encountering a cuestion in some text, will result in the creation of four feature structures; those representing the English words question, dispute, problem, and issue. In addition, the transfer component converts other features in the Spanish feature structure to features recognizable to the English morphological generator.  1.1.4 The English morphological generator We used an English Morphological generator developed at the Computing Research Laboratory at New Mexico State University by Steve Beale. The morphological generator takes feature structures as input and produces correctly inflected English words. Examples of the feature structures used as input and their associated output are illustrated below:  ((root run) (cat v) (num plural)(form progressive)) ((root run) (cat v) (tense future) (form progressive)) ((root man) (cat n) (number plural))  are running will be running men  1.2 Evaluation Suppose we wish to have a user dictate an English translation of a Spanish sentence that appears on a computer screen. This Spanish sentence is input to the MT system and the output is a set of English words. In the ideal case, the words in the English sentence the translator dictates are contained in this set. If one could offer a sort of guarantee that the words of any reasonable translation of the Spanish sentence are contained within this set, then incorporating the MT subsystem into a speech recognition system would be relatively straight forward; the vocabulary at any given moment would be restricted to this word set. If, on the other hand, such a guarantee cannot be made then this approach will not work. The evaluation of the natural language subsystem is designed to test whether reasonable translations are contained within this set of words.  The test material consisted of 10 Spanish newspaper articles. The articles were translated into English by two independent translators. The following table shows that roughly 1/3 of the words in the translations the professional translators produced are not in the set of words produced by the natural language subsystem (T1 and T2 are the two different English translations):  Table 1 : % of words in translation not in word set  Document  T 1  T 2  number  
This paper proposes a method for extracting bilingual text pairs from a comparable corpus. The basic idea of the method is to apply bootstrapping to an existing corpusbased cross-language information retrieval (CLIR) approach. We conducted preliminary tests with English and Japanese bilingual corpora. The bootstrapping method led to much better results for the task of extracting translation pairs compared with a corpus-based CLIR method without bootstrapping, and the extracted translation pairs could be useful training data for improving results of the corpus-based CLIR method. 
In this paper, we rst experimentally investigated the factors that make extracts hard to read. We did this by having human subjects try to revise extracts to produce more readable ones. We then classied the factors into ve, most of which are related to cohesion, after which we devised revision rules for each factor, and partially implemented a system that revises extracts. 
Recent years have shown a surge in interest in temporal database systems, which allow users to store time-dependent information. We present a novel controlled natural language interface to temporal databases, based on translating natural language questions into SQL Temporal, a temporal database query language. The syntactic analysis is done using the Type-Logical Grammar framework, highlighting its utility not only as a theoretical framework but also as a practical tool. The semantic analysis is done using a novel theory of the semantics of temporal questions, focusing on the role of temporal preposition phrases rather than the more traditional focus on tense and aspect. Our translation method is considerably simpler than previous attempts in this direction. We present a prototype software implementation. 
We will report on one of the two tasks in the IREX (Information Retrieval and Extraction Exercise) project, an evaluation-based project for Information Retrieval and Information Extraction in Japanese (Sekine and Isahara, 2000) (IREX Committee, 1999). The project started in 1998 and concluded in September 1999 with many participants and collaborators (45 groups in total from Japan and the US). In this paper, the Named Entity (NE) task is reported. It is a task to extract NE’s, such as names of organizations, persons, locations and artifacts, time expressions and numeric expressions from newspaper articles. First, we will explain the task and the deﬁnition, as well as the data we created and the results. Second, the analyses of the results will be described, which include analysis of task diﬃculty across the NE types and system types, analysis of domain dependency and comparison to human performance. 
This paper describes an approach to actively acquire a language computational model. The purpose of this acquisition is rapid development of NLP systems. The model is created with the syntax module of the Boas knowledge elicitation system for a quick ramp up of a standard transfer-based machine transla tion system from L into English. Introduction Resource acquisition for NLP systems is a wellknown bottleneck in language engineering. It would be a clear advantage to have a methodology that could provide a much cheaper way of NLP resources acquisition. The methodology should be universal in the sense that it could be applied to any language and require no skilled labour of professionals. Our approach attempts just that. We describe it on the example of the syntax module of the Boas knowledge elicitation system for a quick ramp up of a standard transfer-based machine translation system from any language into English (Nirenburg 1998). This work is a part of an ongoing project devoted to the creation of resources for NLP by eliciting knowledge from informants. 
Speech repairs occur often in spontaneous spoken dialogues. The ability to detect and correct those repairs is necessary for any spoken language system. We present a framework to detect and correct speech repairs where all relevant levels of information, i.e., acoustics, lexis, syntax and semantics can be integrated. The basic idea is to reduce the search space for repairs as soon as possible by cascading lters that involve more and more features. At rst an acoustic module generates hypotheses about the existence of a repair. Second a stochastic model suggests a correction for every hypothesis. Well scored corrections are inserted as new paths in the word lattice. Finally a lattice parser decides on accepting the repair. 
A Cantonese Chinese transcription system to automatically convert stenograph code to Chinese characters is reported. The major challenge in developing such a system is the critical homocode problem because of homonymy. The statistical N-gram model is used to compute the best combination of characters. Supplemented with a 0.85 million character corpus of domain-specific training data and enhancement measures, the bigram and trigram implementations achieve 95% and 96% accuracy respectively, as compared with 78% accuracy in the baseline model. The system performance is comparable with other advanced Chinese Speech-to-Text input applications under development. The system meets an urgent need of the Judiciary of post1997 Hong Kong. 
This paper describes a framework for multilingual inheritance-based lexical representation which allows sharing of information across languages at all levels of linguistic description. The paper focuses on phonology. It explores the possibility of establishing a phoneme inventory for a group of languages in which language-speciﬁc phonemes function as “allophones” of newly deﬁned metaphonemes. Dutch, English, and German were taken as a test bed and their vowel phoneme inventories were studied. The results of the cross-linguistic analysis are presented in this paper. The paper concludes by showing how these metaphonemes can be incorporated in a multilingual lexicon. 
The recently completed MLIS DicoPro project addressed the need for a uniform, platformindependent interface for accessing multiple dictionaries and other lexical resources via the Internet intranets. Lexical data supplied by dictionary publishers for the project was in a variety of SGML formats. In order to transform this data to a convenient standard format HTML, a high level transformation language was developed. This language is simple to use, yet powerful enough to perform complex transformations not possible with similar transformation tools. XMLTrans provides rooted recursive transductions, similar to transducers used for natural language translation. The tool is written in standard Java and is available to the general public. 
Grammatical relationships (GRs) form an important level of natural language processing, but diﬀerent sets of GRs are useful for diﬀerent purposes. Therefore, one may often only have time to obtain a small training corpus with the desired GR annotations. On such a small training corpus, we compare two systems. They use diﬀerent learning techniques, but we ﬁnd that this diﬀerence by itself only has a minor eﬀect. A larger factor is that in English, a diﬀerent GR length measure appears better suited for ﬁnding simple argument GRs than for ﬁnding modiﬁer GRs. We also ﬁnd that partitioning the data may help memory-based learning. 
Agglutinative languages present rich morphology and for some applications they need deep analysis at word level. The work here presented proposes a model for designing a full morphological analyzer. The model integrates the two-level formalism and a unification-based formalism. In contrast to other works, we propose to separate the treatment of sequential and non-sequential morphotactic constraints. Sequential constraints are applied in the segmentation phase, and non-sequential ones in the final feature-combination phase. Early application of sequential morphotactic constraints during the segmentation process makes feasible an efficient implementation of the full morphological analyzer. The result of this research has been the design and implementation of a full morphosyntactic analysis procedure for each word in unrestricted Basque texts. Introduction Morphological analysis of words is a basic tool for automatic language processing, and indispensable when dealing with highly agglutinative languages like Basque (Aduriz et al., 98b). In this context, some applications, like spelling correction, do not need more than the segmentation of each word into its different component morphemes along with their morphological information. However, there are other applications such as lemmatization, tagging, phrase recognition, and determination of clause boundaries (Aduriz et  al., 95), which need an additional global morphological parsing1 of the whole word. Such a complete morphological analyzer has to consider three main aspects (Ritchie et al., 92; Sproat, 92): 1) Morphographemics (also called morpho- phonology). This term covers orthographic variations that occur when linking morphemes. 2) Morphotactics. Specification of which morphemes can or cannot combine with each other to form valid words. 3) Feature-combination. Specification of how these morphemes can be grouped and how their morphosyntactic features can be combined. The system here presented adopts, on the one hand, the two-level formalism to deal with morphographemics and sequential morphotactics (Alegria et al., 96) and, on the other hand, a unification-based word-grammar2 to combine the grammatical information defined in morphemes and to tackle complex morphotactics. This design allowed us to develop a full coverage analyzer that processes efficiently unrestricted texts in Basque. The remainder of this paper is organized as follows. After a brief description of Basque morphology, section 2 describes the architecture for morphological processing, where the morphosyntactic component is included. Section 3 specifies the phenomena covered by the analyzer, explains its design criteria, and presents implementation and evaluation details. Section 4 compares the 
The paper illustrates a linguistic knowledge acquisition model making use of data types, innite memory, and an inferential mechanism for inducing new information from known data. The model is compared with standard stochastic methods applied to data tokens, and tested on a task of lexico semantic classi cation. 
Sharing portions of grammars across languages greatly reduces the costs of multilingual grammar engineering. Related languages share a much wider range of linguistic information than typically assumed in standard multilingual grammar architectures. Taking grammatical relatedness seriously, we are particularly interested in designing linguistically motivated grammatical resources for Slavic languages to be used in applied and theoretical computational linguistics. In order to gain the perspective of a language-family oriented grammar design, we consider an array of systematic relations that can hold between syntactical units. While the categorisation of primitive linguistic entities tends to be language-specific or even construction-specific, the relations holding between them allow various degrees of abstraction. On the basis of Slavic data, we show how a domain ontology conceptualising morphosyntactic "building blocks" can serve as a basis of a shared grammar of Slavic. Introduction In applied computational linguistics, the need for developing and utilising operational notions of shared grammars stems from multilingual grammar engineering. If considerable portions of existing grammars can be reused for the specification of new grammars, development efforts can be greatly reduced. A shared grammar also facilitates the difficult task of maintaining consistency within and across the individual parallel grammars. In machine translation, the specification of a shared grammar can furthermore be exploited for simplifying the transfer process. Without much ado, computational linguists engaged in multilingual grammar development have always tried to reduce their labour by importing existing grammar components in a simple "copy-pastemodify" fashion. But there were also a number of systematic attempts to create and describe shared grammars that are convincingly documented in publications. [Kam88] demonstrates the concept for a relatively restricted domain, the grammatical description of simple nominal expressions in five languages. [BOP88] were able to exploit the grammatical overlap of two Slavic languages, for the design of a lean transfer process in Russian to Czech machine translation. In multilingual application development within Microsoft research, grammar sharing has extensively been exploited ± [Pin96], [GLPR97]. However, all these approaches are rather opportunistic in the sense that existing grammatical descriptions based on existing grammar models were explored. We went a step further and started grammar design with a notion of a shared grammar for a family of related languages. Pursuing the goal of designing linguistically motivated grammatical  Hans USZKOREIT Language Technology Lab, DFKI Computational Linguistics, Saarland University Saarbrücken, Germany, D-66041 uszkoreit@dfki.de resources for Slavic languages to be used in computational linguistics, one is inevitably confronted with primary problems stemming from the fact that different linguistic theories cut up grammars in quite different ways, and grammar formalisms differ in their degree of granularity. It cannot be expected, therefore, that the minimal differences between two languages or their shared elements form easily identifiable units in the available language-specific grammars. Therefore, an ontology conceptualising morphosyntactic "building blocks" would offer a solid basis for a shared grammar of Slavic in the sense of [ASU99]. Our use of the term ontology is fairly pragmatic, namely, as representing a formal shared conceptualisation of a particular domain of interest. It describes concepts relevant for the domain, their relationships, as well as "axioms" about these concepts and relationships. Note that such a pragmatic approach does not presuppose any general all-encompassing ontology of language but rather "mini-ontologies" conceptualising the selected domain from various perspectives in a consistent way. The domain of interest in this project is the grammatical knowledge on Slavic morphosyntax contained in linguistic theories and linguistic descriptions. While the categorisation of primitive linguistic entities tends to be language-specific or even construction-specific, the relations holding between them allow various degrees of abstraction. In order to gain the perspective of language-family oriented grammar design, we will consider the array of systematic relations that can hold between syntactically significant items. Systematic relations Systematic relations motivate shared patterns of variation cross-linguistically as well as across constructions. In a constraint-based theory like HPSG, where the grammatical properties of linguistic entities are typically revealed in complex taxonomies, nothing in the formal apparatus would actually exclude the possibility to organise also the relations holding in syntactic constructions in a type hierarchy. So, the type subsumption could be interpreted as modelling a continuum from general – and presumably universal – systematic relations to more and still more specific instances of these relations resulting from admissible cross-classifications.1 In 
Previous stochastic approaches to generation do not include a tree-based representation of syntax. While this may be adequate or even advantageous for some applications, other applications pro t from using as much syntactic knowledge as is available, leaving to a stochastic model only those issues that are not determined by the grammar. We present initial results showing that a tree-based model derived from a tree-annotated corpus improves on a tree model derived from an unannotated corpus, and that a tree-based stochastic model with a handcrafted grammar outperforms both. 
We present an approach to the incremental accrual of lexical information for unknown words that is constraint-based and compatible with standard uniﬁcation-based grammars. Although the techniques are language-independent and can be applied to all kinds of information, in this paper we concentrate on the domain of German noun inﬂection. We show how morphological information, especially inﬂectional class, is successfully acquired using a typebased HPSG-like analysis. Furthermore, we sketch an alternative strategy which makes use of ﬁnitestate transducers. 
Abstract This paper presents an empirical assessment of the LFGDOP model introduced by Bod & Kaplan (1998). The parser we describe uses fragments from LFG-annotated sentences to parse new sentences and Monte Carlo techniques to compute the most probable parse. While our main goal is to test Bod & Kaplan's model, we will also test a version of LFG-DOP which treats generalized fragments as previously unseen events. Experiments with the Verbmobil and Homecentre corpora show that our version of LFG-DOP outperforms Bod & Kaplan's model, and that LFG's functional information improves the parse accuracy of tree structures. 
Common wisdom has it that the bias of stochastic grammars in favor of shorter derivations of a sentence is harmful and should be redressed. We show that the common wisdom is wrong for stochastic grammars that use elementary trees instead of context-free rules, such as Stochastic Tree-Substitution Grammars used by Data-Oriented Parsing models. For such grammars a non-probabilistic metric based on the shortest derivation outperforms a probabilistic metric on the ATIS and OVIS corpora, while it obtains competitive results on the Wall Street Journal (WSJ) corpus. This paper also contains the first published experiments with DOP on the WSJ. 1. Introduction A well-known property of stochastic grammars is their propensity to assign higher probabilities to shorter derivations of a sentence (cf. Chitrao & Grishman 1990; Magerman & Marcus 1991; Briscoe & Carroll 1993; Charniak 1996). This propensity is due to the probability of a derivation being computed as the product of the rule probabilities, and thus shorter derivations involving fewer rules tend to have higher probabilities, almost regardless of the training data. While this bias may seem interesting in the light of the principle of cognitive economy, shorter derivations generate smaller parse trees (consisting of fewer nodes) which are not warranted by the correct parses of sentences. Most systems therefore redress this bias, for instance by normalizing the derivation probability (see Caraballo & Charniak 1998). However, for stochastic grammars that use elementary trees instead of context-free rules, the propensity to assign higher probabilities to shorter derivations does not necessarily lead to a bias in favor of smaller parse trees, because elementary trees may differ in size and lexicalization. For Stochastic Tree-Substitution Grammars (STSG) used by DataOriented Parsing (DOP) models, it has been observed that the shortest derivation of a sentence consists of the largest subtrees seen in a treebank that generate  that sentence (cf. Bod 1992, 98). We may therefore wonder whether for STSG the bias in favor of shorter derivations is perhaps beneficial rather than harmful. To investigate this question we created a new STSG-DOP model which uses this bias as a feature. This non-probabilistic DOP model parses each sentence by returning its shortest derivation (consisting of the fewest subtrees seen in the corpus). Only if there is more than one shortest derivation the model backs off to a frequency ordering of the corpussubtrees and chooses the shortest derivation with most highest ranked subtrees. We compared this nonprobabilistic DOP model against the probabilistic DOP model (which estimates the most probable parse for each sentence) on three different domains: the Penn ATIS treebank (Marcus et al. 1993), the Dutch OVIS treebank (Bonnema et al. 1997) and the Penn Wall Street Journal (WSJ) treebank (Marcus et al. 1993). Surprisingly, the non-probabilistic DOP model outperforms the probabilistic DOP model on both the ATIS and OVIS treebanks, while it obtains competitive results on the WSJ treebank. We conjecture that any stochastic grammar which uses units of flexible size can be turned into an accurate non-probabilistic version. The rest of this paper is organized as follows: we first explain both the probabilistic and nonprobabilistic DOP model. Next, we go into the computational aspects of these models, and finally we compare the performance of the models on the three treebanks. 2. Probabilistic vs. Non-Probabilistic Data-Oriented Parsing Both probabilistic and non-probabilistic DOP are based on the DOP model in Bod (1992) which extracts a Stochastic Tree-Substitution Grammar from a treebank ("STSG-DOP").1 STSG-DOP uses subtrees 
We argue that in general, the analysis of lexical cohesion factors in a document can drive a summarizer, as well as enable other content characterization tasks. More narrowly, this paper focuses on how one particular cohesion factor—simple lexical repetition—can enhance an existing sentence extraction summarizer, by enabling strategies for overcoming some particularly jarring enduser effects in the summaries, typically due to coherence degradation, readability deterioration, and topical under-representation. Lexical repetition is instrumental to, among other things, the topical make-up of a text, and in our framework a lexical repetition-based model of discourse segmentation, capable of detecting topic shifts, is integrated with a linguistically-aware summarizer utilizing notions of salience and dynamically-adjustable summary size. We show that even by leveraging lexical repetition alone, summaries are of comparable, and under certain conditions better, quality than the ones delivered by a state-of-the-art summarizer. This is encouraging for a broad research platform focusing on the recognition and use of cohesive devices in text for a range of content characterisation and document management tasks. 
A multifunctional NLP environment, ETAP-3, is presented. The environment has several NLP applications, including a machine translation system, a natural language interface to SQL type databases, synonymous paraphrasing of sentences, syntactic error correction module, and a computer-assisted language learning tool. Emphasis is laid on a new module of the processor responsible for the interface with the Universal Networking Language, a recent product by the UN University intended for the facilitation of multilanguage, multiethnic access to communication networks such as WWW. The UNL module of ETAP-3 naturally combines the two major approaches accepted in machine translation: the transfer-based approach and the interlingua approach. 1. Introductory Remarks ETAP-3 is a multipurpose NLP environment that was conceived in the 1980s and has been worked out in the Institute for Information Transmission Problems, Russian Academy of Sciences (Apresjan et al. 1992, Boguslavsky 1995). The theoretical foundation of ETAP-3 is the Meaning ⇔ Text linguistic theory by Igor' Mel'ÿuk and the Integral Theory of Language by Jurij Apresjan. ETAP-3 is a non-commercial environment primarily oriented at linguistic research rather than creating a marketable software product. The main focus of the research carried out with ETAP-3 is computational modelling of natural languages. This attitude explains our effort to  develop the models in a way as linguistically sound as possible. We strive to incorporate into the system much linguistic knowledge irrespective of whether this knowledge is essential for better text processing (e.g. machine translation) or not. In particular, we want our parser to produce what we consider a correct syntactic representation of the sentence – first of all because we believe that this interpretation is a true fact about the natural language. We have had many occasions to see that in the long run the theoretical soundness and completeness of linguistic knowledge incorporated in an NLP application will pay. All NLP applications in ETAP-3 are largely based on an original system of three-value logic and use an original formal language of linguistic descriptions, FORET. 2. ETAP-3: Modules, Features, Design, Implementation 2.1 ETAP-3 Modules The major NLP modules of ETAP-3 are as follows: • High Quality Machine Translation System • Natural Language Interface to SQL Type Databases • System of Synonymous Paraphrasing of Sentences • Syntactic Error Correction Tool • Computer-Aided Language Learning Tool • Tree Bank Workbench Another module, a new UNL converter responsible for the interface with the Universal Networking Language, a recent product designed  
In this paper, we present a solution to the problem of generating Japanese numeral classiﬁers using semantic classes from an ontology. Most nouns must take a numeral classiﬁer when they are quantiﬁed in languages such as Chinese, Japanese, Korean, Malay and Thai. In order to select an appropriate classiﬁer, we propose an algorithm which associates classiﬁers with semantic classes and uses inheritance to list only those classiﬁers which have to be listed. It generates sortal classiﬁers with an accuracy of 81%. We reuse the ontology provided by Goi-Taikei — a Japanese lexicon, and show that it is a reasonable choice for this task, requiring information to be entered for less than 6% of individual nouns. 
While language-independent sentence alignment programs typically achieve a recall in the 90 percent range, the same cannot be said about word alignment systems, where normal recall ﬁgures tend to fall somewhere between 20 and 40 percent, in the language-independent case. As words (and phrases) for various reasons are more interesting to align than sentences, we need methods to increase word alignment recall, preferably without sacriﬁcing precision. This paper reports on a series of experiments with pivot alignment, which is the use of one or more additional languages to improve bilingual word aligment. The conclusion is that in a multilingual parallel corpus, pivot alignment is a safe way to increase word alignment recall without lowering the precision. 
Binding constraints have resisted to be fully integrated into the course of grammatical processing despite its practical relevance and cross-linguistic generality. The ultimate root for this is to be found in the exponential "overgenerate & filter" procedure of the mainstream rationale for their satisfaction. In this paper we design an alternative approach based on the view that nominals are binding machines. Introduction Binding constraints are an important set of filters in the process of anaphor resolution1. As they delimit the relative positioning of anaphors and their possible antecedents in the grammatical geometry, these constraints are of crucial importance for restricting the search space for antecedent candidates and enhancing the performance of resolvers. From an empirical perspective, they stem from quite robust generalizations and exhibit a universal character, given their parameterized validity across natural languages. From a conceptual point of view, in turn, the relations among binding constraints involve non-trivial symmetry, which lends them a modular nature. Accordingly, they have typically been taken as one of the most intriguing and robust grammar modules. 
This paper explores the usefulness of a technique from software engineering, code instrumentation, for the development of large-scale natural language grammars. Information about the usage of grammar rules in test and corpus sentences is used to improve grammar and testsuite, as well as adapting a grammar to a speci c genre. Results show that less than half of a large-coverage grammar for German is actually tested by two large testsuites, and that 10 30 of testing time is redundant. This methodology applied can be seen as a re-use of grammar writing knowledge for testsuite compilation. The construction of genre-speci c grammars results in performance gains of a factor of four. 
Previous work has shown that adding genearnaelixzaamtiopnle-obfastehdemeaxcahminpeletsrainnslatthieoncoErpBuMs Tof styrasntesmlatceadnexreadmupcleettheextrebqyuairsemd uacmhoausnatnoforpdreerof magnitude for Spanish-English and FrenchEnglish EBMT. Using word clustering to autpormovaidtiecatllhye gmenaejroarliitzye otfhetheixsaimmpplreovcoemrpeunst cfaonr French-English with no manual intervention; the prior work required a large bilingual dictionary tagged with parts of speech and the mtchraeeanctuleuadsltceinrrefinoartgmiownaittoihofnag,srmaemvaelmnl aabmrerotutuelnerts.opfeBrmyfoarsnmeueaadnlilncygecan be achieved. This paper describes a method wfcdoluuhrcmseitrneeegdrbiyntuhgbseitinsleiigcnzhegsnuotiaaqflnutdhewsaeo,rerdaxdnamdcmloiuptnssltoeeelcirnoiengrcgputiuavclseanrdneeoqsbcsuueiamrtpeederner.t-- 
This paper presents a robust client/server implementation of a word sense disambiguator for English. This system associates a word with its meaning in a given context using dictionaries as tagged corpora in order to extract semantic disambiguation rules. Semantic rules are used as input of a semantic application program which encodes a linguistic strategy in order to select the best disambiguation rule for the word to be disambiguated. The semantic disambiguation rule application program is part of the client/server architecture enabling the processing of large corpora. 
Acquiring and updating terminological resources are di cult and tedious tasks, especially when semantic information should be provided. This paper deals with Term Semantic Categorization. The goal of this process is to assign semantic categories to unknown technical terms. We propose two approaches to the problem that rely on di erent knowledge sources. The exogeneous approach exploits contextual information extracted from corpora. The endogeneous approach relies on a lexical analysis of the technical terms. After describing the two implemented methods, we present the experiments that we conducted on signi cant test sets. The results demonstrate that term categorization can provide a reliable help in the terminology acquisition processes. 
Huge multilingual news articles are reported and disseminated on the Internet. How to extract the key information and save the reading time is a crucial issue. This paper proposes architecture of multilingual news summarizer, including monolingual and multilingual clustering, similarity measure among meaningful units, and presentation of summarization results. Translation among news stories, idiosyncrasy among languages, implicit information, and user preference are addressed. Introduction Today many web sites on the Internet provide online news services. Multilingual news articles are reported periodically, and across geographic barrier to disseminate to readers. Readers can access the news stories conveniently, but it takes much time for people to read all the news. This paper will present a personal news secretariat that helps on-line readers absorb news information from multiple sources in different languages. Such a news secretariat eliminates the redundant information in the news articles, reorganizes the news for readers, and helps them resolve the language barriers. Reorganization of news is some sort of document summarization, which creates a short version of original document. Recently, many papers touch on single document summarization (Hovy and Marcu, 1998a). Only a few touch on multiple document summarization (Chen and Huang, 1999; Mani and Bloedorn, 1997; Radev and McKeown, 1998) and multilingual document  summarization (Hovy and Marcu, 1998b). For multilingual multiple news summarization, several issues have to be addressed: (1) Translation among news stories in different languages The basic idea in multiple document summarizations is to identify which parts of news articles present similar reports. Because the news stories are in different languages, some kind of translation is required, e.g., term translation. Besides the problem of translation ambiguity, different news sites often use different names to refer the same entity. The translation of named entities, which are usually unknown words, is another problem. (2) Idiosyncrasy among languages Different languages have their own specific features. For example, a Chinese sentence is composed of characters without word boundary. Word segmentation is indispensable for Chinese. Besides, Chinese writers often assign punctuation marks at random, how to determine a meaningful unit for similarity checking is a crucial issue. Thus some tasks may be done for specific languages during summarization. 
Table is a very common presentation scheme, but few papers touch on table extraction in text data mining. This paper focuses on mining tables from large-scale HTML texts. Table filtering, recognition, interpretation, and presentation are discussed. Heuristic rules and cell similarities are employed to identify tables. The F-measure of table recognition is 86.50%. We also propose an algorithm to capture attribute-value relationships among table cells. Finally, more structured data is extracted and presented.  Introduction  Tables, which are simple and easy to use, are very common presentation scheme for writers to describe schedules, organize statistical data, summarize experimental results, and so on, in texts of different domains. Because tables provide rich information, table acquisition is useful for many applications such as document understanding, question-and-answering, text retrieval, etc. However, most of previous approaches on text data mining focus on text parts, and only few touch on tabular ones (Appelt and Israel, 1997; Gaizauskas and Wilks, 1998; Hurst, 1999a). Of the papers on table extractions (Douglas, Hurst and Quinn, 1995; Douglas and Hurst 1996; Hurst and Douglas, 1997; Ng, Lim and Koo, 1999), plain texts are their targets.  In plain text, writers often use special symbols, e.g., tabs, blanks, dashes, etc., to make tables. The following shows an example. It depicts book titles, authors, and prices.  title  author  Statistical Language Learning  E.Charniak  Cross-Language Information Retrieval G. Grefenstette  Natural Language Information Retrieval T.Strzalkowski  price $30 $115 $144  When detecting if there is a table in free text, we should disambiguate the uses of the special symbols. That is, the special symbol may be a separator or content of cells. Previous papers employ grammars (Green and Krishnamoorthy, 1995), string-based cohesion measures (Hurst and Douglas, 1997), and learning methods (Ng, Lim and Koo, 1999) to deal with table recognition. Because of the simplicity of table construction methods in free text, the expressive capability is limited. Comparatively, the markup languages like HTML provide very flexible constructs for writers to design tables. The flexibility also shows that table extraction in HTML texts is harder than that in plain text. Because the HTML texts are huge on the web, and they are important sources of knowledge, it is indispensable to deal with table mining on HTML texts. Hurst (1999b) is the first attempt to collect a corpus from HTML files, LATEX files and a small number of ASCII files for table extraction. This paper focuses on HTML texts. We will discuss not only how to recognize tables from HTML texts, but also how to identify the roles of each cell (attribute and/or value), and how to utilize the extracted tables. 
The paper describes a similarity-based model to present the morphological rules for Chinese compound nouns. This representation model serves functions of 1) as the morphological rules of the compounds, 2) as a mean to evaluate the properness of a compound construction, and 3) as a mean to disambiguate the semantic ambiguity of the morphological head of a compound noun. An automatic semantic classification system for Chinese unknown compounds is thus implemented based on the model. Experiments and error analyses are also presented. 1. Introduction The occurrences of unknown words cause difficulties in natural language processing. The word set of a natural language is open-ended. There is no way of collecting every words of a language, since new words will be created for expressing new concepts, new inventions. Therefore how to identify new words in a text will be the most challenging task for natural language processing. It is especially true for Chinese. Each Chinese morpheme (usually a single character) carries meanings and most are polysemous. New words are easily constructed by combining morphemes and their meanings are the semantic composition of morpheme components. Of course there are exceptions of semantically noncompositional compounds. In Chinese text, there is no blank to mark word boundaries and no inflectional markers nor capitalization markers to denote the syntactic or semantic types of new words. Hence the unknown word identification for Chinese became one of the most difficult and demanding research topic. The syntactic and semantic categories of unknown words in principle can be determined by their content and contextual information. However many difficult problems have to be solved. First of all it is not possible to find a uniform representational schema and categorization algorithm to handle different types of unknown words, since each type of unknown words has very much different morpho-syntactic structures. Second, the clues for  identifying different type of unknown words are also different. For instance, identification of names of Chinese is very much relied on the surnames, which is a limited set of characters. The statistical methods are commonly used for identifying proper names (Chang et al. 1994, Sun et al. 1994). The identification of general compounds is more relied on the morphemes and the semantic relations between morphemes. There are co-occurrence restrictions between morphemes of compounds, but their relations are irregular and mostly due to common sense knowledge. The third difficulty is the problems of ambiguities, such as structure ambiguities, syntactic ambiguities and semantic ambiguities. For instances, usually a morpheme character/word has multiple meaning and syntactic categories. Therefore the ambiguity resolution became one of the major tasks. Compound nouns are the most frequently occurred unknown words in Chinese text. According to an inspection on the Sinica corpus (Chen etc. 1996), 3.51% of the word tokens in the corpus are unknown, i.e. they are not listed in the CKIP lexicon, which contains about 80,000 entries. Among them, about 51% of the word types are compound nouns, 34% are compound verbs and 15% are proper names. In this paper we focus our attention on the identification of the compound nouns. We propose a representation model, which will be facilitated to identify, to disambiguate and to evaluate the structure of a compound noun. In fact this model can be extended to handle compound verbs also. 1.1 General properties of compounds and their identification strategy The semantic category and syntactic category are closely related. For coarse-grained analysis, syntactic categorization and semantic categorization are close related. For instances, nouns denote entities; active verbs denote events and stative verbs denote states. For fine-grained analysis, syntactic and semantic classifications take different classification criterion. In our model the coarse-grained analysis is processed first. The syntactic categories of an unknown  word are predicted first and the possible semantic syntactic behaviors, it is hard to derive a set of  categories will be identified according to its top morphological rules to generate the set of  ranked syntactic categories. Different syntactic Chinese compounds without over-generation or  categories require different representational models under-generation. The set of general compounds  and different fine-grained semantic classification is an open-class. The strategy for automatic  methods.  identification will be relied not only on the  The presupposition of automatic se- morpho-syntactic structures but also morpho-  mantic classification for compounds is that the semantic relations. In general, certain  meaning of a compound is the semantic com- interpretable semantic relationships between  position of its morphemic components and the morphemic must be held. However there is no  head morpheme determines the major semantic absolute means to judge whether the semantic  class of this compound. There are many poly- relations between morphemic components are  syllabic words of which the property of se- acceptable, i,e. the acceptability of such type of  mantic composition does not hold, for in- compounds is not simply ‘yes’ or ‘no’. The  stances the transliteration words, those words degree of properness of a compound should  should be listed in the lexicon. Since for the depend on the logical relation between  majority of compounds the presupposition hold, morphemic components and their logicalness  the design of our semantic classification algo- should be judged by common sense knowledge.  rithm will be based upon this presupposition. It is almost impossible to implement a system  Therefore the process of identifying semantic with common sense knowledge. Chen & Chen  class of a compound boils down to find and to (1998) proposed an example-based measurement  determine the semantic class of its head mor- to evaluate the properness of a newly coined  pheme. However ambiguous morphological compound instead. They postulate that for a  structures cause the difficulties in finding head newly coined compound, if the semantic relation  morpheme. For instances, the compound in 1a) of its morphemic components is similar to the  has two possible morphological structures, but existing compounds, then it is more likely that  only 1b) is the right interpretation.  ­! 1a)  ‘American’  ­ ! b)  ‘America’ ‘people’ ,  ­ ! c)  ‘beautiful’ ‘country-man’  Once the morphological head is determined, the  semantic resolution for the head morpheme is the  next difficulty to be solved. About 51.5% of the  200 most productive morphemes are polysemous  and according to the Collocation Dictionary of  Noun and Measure Words (CDNM), in average  each ambiguous morpheme carries 3.5 different  senses (Huang et al. 1997).  this newly coined compound is proper.  2.1 Example-based similarity measure  Supposed that a compound has the structure of  XY where X and Y are morphemes and sup-  >a² posed without loss of generality Y is the head.  For instance,  ‘learn-word-machine’ is a  noun compound and the head morphemeY is  ² >a ’machine’ and the modifier X is  ‘learn-  ² word’. In fact the morpheme has four differ-  ent meanings. They are ‘machine’, ‘airplane’,  ‘secret’ and ‘opportunity’. How do computers  2. Representation Models  judge which one is the right meaning and how is the compound construction well-formed or logi-  Compounds are very productive types of unknown words. Nominal and verbal compounds are easily  ² cally meaningful? First of all, the examples with the head morpheme are extracted from cor-  coined by combining two/many words/characters. pora and dictionaries. The examples are classi-  Since there are more than 5000 commonly used fied according to their meaning as shown in the  Chinese characters and each with idiosyncratic Table 1.  =============================================================  Senses  semantic category  examples  >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ²t ±Q² C² J² U² <û² )2* nbdijof = ? Cp122  ² ² è² Ü² 1² nf² 4Ù² )3* bjsqmbof = ? Cp334  ² r² ´² È² ² Ü² c² )4* pqqpsuvojuz = ? Db153  ²/ ø² c² .² ý² ² )5* tfdsfu = ?  Eb122  ² >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> Table 1. Four senses of the morpheme ‘ ’ and their respective samples  >a² The meaning of  is then determined by  comparing the similarity between it and each class  of examples. The meaning of the input unknown  word will be assigned with the meaning of the class  with the most similar morpho-semantic structures  with this unknown word. The similarity measure is  based on the following formula.  Supposed that each class of examples forms the  following semantic relation rules. The rules show  the possible semantic relations between prefix and  suffix Y and their weight in term of the frequency  distribution of each semantic category of the pre-  fixes in the class.  Rules: Sem1 + Y Freq1 Sem2 + Y Freq2 : : : : Semk + Y Freqk ( freqi: the number of the words of the form Semi + Y ) ² Take suffix with meaning of ‘machine’ as ² example. For the morpheme ‘machine’, the ² extracted compounds of the form X+ ‘machine’ and the semantic categories of the modifiers are shown in Table 2 and the morphological rule derived from them is in Table 3. The semantic types and their hierarchical structure are adopted from the Chilin (Mei et al. 1984). The similarity is measured between the semantic class of the prefix X of the unknown compound and the prefix semantic types shown in the rule. One of the measurements proposed is: 2 SIMILAR (Sem,Rule) = { i=1,k InformationS Load(Sem Semi) * Freqi } / Max-value Where Sem is the semantic class of X. Max-value  2 is the maximal value of { Information Load(S SSemi) * Freqi } for all semantic classes S. The max-value normalizes the SIMILAR value to S 0~1. S Semi denotes the least common ancesS tor of S and Semi. For instance, (Hh03 Hb06) = H. The Information-Load(S) of a semantic class S is defined as Entropy(semantic system) – Entropy(S). Simply speaking it is the 2 amount of reduced entropy after S is seen. En- tropy(S) = i=1,k -P(Semi|S) * Log P(Semi|S), where {Sem1, Sem2,…,Semk} is the set of the bottom level semantic classes contained in S.  ===============================  ±Q ²  ² ( ) Fa211 Hh031  ( ) Bg033  + ² ¼½ ² ( ) Hh031  ( ) Ih033  J ² A ² ( ) Ae173 Hh031  ( ) Bo171  7 ² þ ² ( ) Ae173 Hh032  ( )If083 Ih063  <û ² , ² ( ) Hc013 He032  ( ) Hi141 Hj345  Q ² ã ² ( ) He032  ( ) Fa221  Þï ² w¯ ² ( ) Eb342  ( ) Hc071  =ï ² ËU ² ( ) Eb342  ( ) Ih031  ì ² Û ² ( ) Ae162 Hg162  ( ) Bg051   ² ÷Ó ² ( ) Dk162 Hg191  ( ) Ih042  ¤É ² Ty ² ( ) Bo041  ( ) Hg192  Dã ² Ï ² ( ) Hd121  ( ) Ih061  ´` ² R¡ ² ( ) Ca032 Ca041  ( ) Fb012  Çï ² r ² ( ) Hc122  ( ) Hc231  à* ² È ² ( ) Br121  ( ) Fa212 Jd102  ================================  ² Table 2. The semantic categories of modifiers of the compounds of X-“ nbdijof”  > a ² Take the word  ‘learning-word-  machine’ as example. In the table 3, the results  show the estimated similarity between the  ² X- Semi freqi  ===============  Hh031  3  Ae173  
Repetition is very common. Adaptive language models, which allow probabilities to change or adapt after seeing just a few words of a text, were introduced in speech recognition to account for text cohesion. Suppose a document mentions Noriega once. What is the chance that he will be mentioned again? If the ﬁrst instance has probability p, then under standard (bag-of-words) independence assumptions, two instances ought to have probability p 2, but we ﬁnd the probability is actually closer to p/2. The ﬁrst mention of a word obviously depends on frequency, but surprisingly, the second does not. Adaptation depends more on lexical content than frequency; there is more adaptation for content words (proper nouns, technical terminology and good keywords for information retrieval), and less adaptation for function words, cliches and ordinary ﬁrst names.  1. Introduction  Adaptive language models were introduced in the Speech Recognition literature to model repetition. Jelinek (1997, p. 254) describes cache-based models which combine two estimates of word (ngram) probabilities, Pr L, a local estimate based on a relatively small cache of recently seen words, and Pr G, a global estimate based on a large training corpus.  1. Additive: Pr A (w) = λPr L (w) + (1 − λ) Pr G (w)  2. Case-based:  Pr  C  (  w  )  =  λλ  
 of' relation. If c0 is a kind of c, then c is a hy-  Knowledge of which words are able to ll particular argument slots of a predicate can be used for structural disambiguation. This paper describes a proposal for acquiring such knowledge, and in line with much of the recent work in this area, a probabilistic approach is taken. We develop a novel way of using a semantic hierarchy to estimate the probabilities, and demonstrate the general approach using a prepositional phrase attachment experiment. 
We compare the potential of two classes of linear and hierarchical models of discourse to determine co-reference links and resolve anaphors. The comparison uses a corpus of thirty texts, which were manually annotated for co-reference and discourse structure. 
Typical approaches to XML authoring view a XML document as a mixture of structure (the tags) and surface (text between the tags). We advocate a radical approach where the surface disappears from the XML document altogether to be handled exclusively by rendering mechanisms. This move is based on the view that the author’s choices when authoring XML documents are best seen as language-neutral semantic decisions, that the structure can then be viewed as interlingual content, and that the textual output should be derived from this content by language-speciﬁc realization mechanisms, thus assimilating XML authoring to Multilingual Document Authoring. However, standard XML tools have important limitations when used for such a purpose: (1) they are weak at propagating semantic dependencies between different parts of the structure, and, (2) current XML rendering tools are ill-suited for handling the grammatical combination of textual units. We present two related proposals for overcoming these limitations: one (GF) originating in the tradition of mathematical proof editors and constructive type theory, the other (IG), a specialization of Deﬁnite Clause Grammars strongly inspired by GF. 
Subjectivity is a pragmatic, sentence-level feature that has important implications for text processing applications such as information extraction and information retrieval. We study the effects of dynamic adjectives, semantically oriented adjectives, and gradable adjectives on a simple subjectivity classiﬁer, and establish that they are strong predictors of subjectivity. A novel trainable method that statistically combines two indicators of gradability is presented and evaluated, complementing existing automatic techniques for assigning orientation labels. 
Pronominalization has been related to the idea of a local focus – a set of discourse entities in the speaker’s centre of attention, for example in Gundel et al. (1993)’s givenness hierarchy or in centering theory. Both accounts say that the determination of the focus depends on syntactic as well as pragmatic factors, but have not been able to pin those factors down. In this paper, we uncover the major factors which determine the focus set in descriptive texts. This new focus deﬁnition has been evaluated with respect to two corpora: museum exhibit labels, and newspaper articles. It provides an operationalizable basis for pronoun production, and has been implemented as the reusable module gnome-np. The algorithm behind gnome-np is compared with the most recent pronoun generation algorithm of McCoy and Strube (1999). 
 This paper proposes a new unsupervised learning method for obtaining English part-ofspeech(POS) disambiguation rules which would improve the accuracy of a POS tagger. This method has been implemented in the experimental system APRAS (Automatic POS Rule Acquisition System), which extracts POS disambiguation rules from plain text corpora by utilizing different types of coded linguistic knowledge, i.e., POS tagging rules and syntactic parsing rules, which are already stored in a fully implemented MT system. In our experiment, the obtained rules were applied to 1.7% of the sentences in a non-training corpus. For this group of sentences, 78.4% of the changes made in tagging results were an improvement. We also saw a 15.5 % improvement in tagging and parsing speed and an 8.0 % increase of parsable sentences.  
 ~{Y¨f26­f@R@~Yw²~{¨f6³fQ&&¨f¥w62R~f¬ä6~w~w26@~f© $®dªh f@d1~ &¦f~Qfv@f¥¨fv~wf1¥©¬R¥2Q¨f8f2¨d¦¬R@¨fd¿Üv¨H¨öw2wvf¥luQ»w~wQ£{f@x¢ ux@fÀRAf~wQxv% wf~w¨f{ ~wf§~t@~wvQ{ ¤  P ¨1!$£w°@¦Q(f¦2@¨1¥wQAvi`Q¥wvbV©§!$¨f¦2!~~{7{5¦©@t2¨&~ u´ä9ª6¦wdA¨#¤v¦f2~{d8x{@~©  ~wf¢@ff¥v~C¨ ® ~Q¦d~Qf~Q¬R¬R¥£f{{Á6ÛÑu~@¨v¨v¨v¨v~²¨v@QÌf~wØ©QsC)²fd2Q¨f¨f(d~{~{¥u@ufQ¯¦§@~ÑQ´±h±h2¡ov¢¥@v¬~2QäYÖv{{f¥w@âö¨f¨ffÕQ~ff¨f¨fvuff@¢¢Ëhv@@vY¦v~w¨²¥wQ)àY£22@fw@R)¦w%¥~@¨¦f2Ñfx×ww2~{wf~wv~w2¥¤fvxx~w©¯{w¦ffÑfÔ{~f¡¦vQ¥wAv¢{AQ6{¤¥~Q&~§Q@v~{Ú©f£dfxffvËw9{¯2f1fÁ¤{©ä~h22QhQ®6Y²~Q¥¢Qì¨f2¥YuÍvsff6~~YQÕffv~§@©ox1~QÏ¨dw¥w@~Q~w2Àl&f}{Ït§¥f¦w¦fQfUÐh~%Qd1$%Q¨ff2Q¤Ù°±h¬Ru~¨fffÛf©f¥f¨f~w©©)22~wwxwf@v¥wf@~f¦w¨f¨f~Qfff¦v~°ÖslÑ~»f~¨v~vvª³vf£df~wÕf±h~f©fØ³{¥¨f~Q2¢{~©¨f{Ï¨vv¬RQw~f¨Åx¨f¡f(¨²xQ¿ì²xhÎ¦f¢¤@QQ2vQ~Q~U~9Q¢fÕw®f@f¨vvÔv~dv¢f¢vl@Qwxf¨f¦d£×¢2@2ì£~w~w~¨fuhfÕ36¢Q¢dif©wYÌ{dwQQ²2vf$w£wf~w@@¨fv£d2f~v©~©{)©­2ÎY¾¨fw©ä~w¨f¨Uvf%@~Q®C2²v~¦wu@¦v~w©¨vvÀö{w¦RYf¯¦w°uw®s{@¨f¬Rf~æ~®d¨vÛ²f2³2¬RQ°Rg£ö¥2~f2¦f@¦vf¬­f~QQ2R¨fY®R@2ì6{¨v~Q¬R~wâQ¨f{²lö(xv¬R®¢2A¤f¥w£fÎ²)f¦fff@vf¨v{vf@Y~wx~¤)á6¢v¨fwYh²d~A²Qu~{A¥wf¢fQf2@¬Ôhj²v¿@£fh2{Qv2¬RÀö¾Q@vl6¥)¥~wuÍff)~w~©²¢Q¬­¢®t¨f£fQâfodo¨ä~¥~v¨oh¨f2¥~Qf~§~w@¬RÎQ~w²Qwvvæ¶v¤Ù­v&~w~©¦v2~w¦~f£wQQ¨f~2)¥¨f@A¨f¦¨¢fA¢A¬¤¢v@¯x¨vj@¥6Ë¶~{­2~w~wÜ~QÑ¨vff¤d¨vu§s{f@¥Àj&~Qwfh­h°@&2²@@£fQ2x6¥ä(Q{v8~ÑS£wvQ¢w2ì~d¨wv~f6¨f~{6¨f)~©~¢f@¨f©v@QQ®¨f£wvf{d£Q6~w~{wf¦)~w6Î6x¦wf¾Q¦fRª°~Aw2~~@Y{f®swfâ{¢±hv~¦¢vv6{~wffw2×vÏt)@­262v@d~96f£QQff¿ÍYÑQ¦f¨fö~Q6¬­¢³¤Ñ4Q¨f¨ffªfvv2{f@Ú@d@£w¨fh{~Ah{©2¦@{{Qd¬RQ¥¥@¥Y)¦f~YQ~w~{~2Î@h~Ö¦vv¬R²d6¨fvÎfA{2@v¢~d~~Q@@ÙEf2f2Ëv¥Y@2vª¦dffv¨v22fÌf¥dd¥{2fwfw{2$~Q2¨¨©¥¥ÎÎ¤¤¤¤¤¤®© Lõöõöñnøí¾ø³5ò÷êeñëìø67ÍéuíÜëøkë¾ê¤ñ2ò FëìLfó÷ñêêòô¶úFeëCýCësD õGéuíìõGè$F{ê"êF$éuí¾ë¾#ë¾ê¢éuêséxñbêjê¤ñïsðuïìë¾ë¾ðf@ðféuñññí¾ûuê¤ôò÷íìó÷êò÷ê¢ê¤íìø#ø6ø6ê¢eRëëò÷î¤ñïñô¢ëìññløuò÷õöò÷õöøxóúlø4Dù6úuêîRéfêíìñísëìø)ñïò÷ôõGí¾õöøxòÄñF$ñø)õhó÷ëìóëìü¦ò÷éuëìó÷õöéuò÷ê¤ø#øõöêjeí¾¨wë ò÷Dí¾ñë¾êòûuò÷éuø6eöïÜó÷ê­é6ëë¾ê¤ò÷ñëCô(ô¢ëìò÷õGõöêïjôúFvóõöDéuëìéuîRõöóÄéuêñíìê(ï¾øûQòò÷¢ê¢õöë¾õöõGñóøø6Fûu¤ë¾ë¾ó÷%ñêê¶Wó÷ÿ@%óò÷üëïm  ¡ a1udº| y£¢¥¤ ¹jtvº¶cfe§¦ ~Qv¦~f¦f¬­~f2¦f@¦±h{±hQ±hQ}~f°°w{~¨v¨v¨v¨v¨v226fsf¦©¤@{v~w~Q~¨f¨f¨f¥v~¬R¥@uu{6QY2¤¥«¤±h¢¢@@©ww2~w©¦ff@w~f6@v¨ªsfR¦®%£w~¨f¥w{¨f@6~f~Qf)6¥¢~w¬R@©2f~@¨f¦@¥¬~6xv¬R~¬­i2f¥w2~w¬­Ð©Q¦f~¦~w¡wf~w¨vx¤­6AAvf2o~Q@v¨ff¢w~¼¢6vgv~2¨fQv£¬­@f¥¦¦d2@@@¦v2¦f¨v@@Q~³f&22of²(¨f©¬­6söff2Q¨f~d2¨f¦f6¦f®Q¯¥2~w¥Í@2Q{{~Q2vf~¦@@²~@®(o8³@©¬dR¥s(2{~{~{@f~w¨f¨f@vg~Q²£&{Avf¨v¨v¦Qf66w@@¬­Qv2~@v¨f®{2ÕQR@2ffQxQ@¨fxw@²@wfd¬RQ¦~~wÏQ©6{Rå¯v¨v~Q2¢¢~df¨8v)d6Yª¨f~°f¢Ù©ª2w°fQ{¦d~¢2¬Rf2@~~QYQ@@w²¥wwff~)f©~w~6¨x¯1Q2¦vhvÍ@¦¦¨f¢¦vfu©#fwvf£w¨wff(­~wªQv~@¬R@2@¯w22Y¥{{x@¦v@v¥@Qf~df1¨f©¥w»Y@¨fwvf~©»2@$ä2å¨f²f$±h¥¤~@~w~w~{¥sf{22²1w@@v¨ff¨v¦f$¨vvv~®i¢@(±hw6¨f¨f~w~¬­vxª¥U6v@¨f&¨A~f©Ðv¤~i©ö³~w22@¨f~d¨f¨f~©xQ2¨f¦vQ³&@{®$f~Q}¢wQhQ{6f~³Q~¯Y¨f2f§(~f²¬f(­¨v²~~2vv2{u~w¥{f@@¬R¥2~w¥w@¯vö@¬R~w®%6U§v2ff~w@~¥¯¨f$v~¢~~w¨f~Q&x³2Qvv¢voQ222§v¨f¦Q6{¨v®f¢~w2¨fvj{xf¡wwf¨f{~wQ¥w~f~Q@¤~wx²2Y¢Q¨v¥w$f~w¥¦w®$¬¤R¦h{xvf¬­QRP¨¯~@Y¬R~2U~wgQ@ff~w¤Q¯v9f~$~2¨ävÍ¨ff¬R~Qvi~{¨f$®f¥v©~dQfs~26~¦~{²ÜU2f@@Qwvf®¨f©¢¥wf°¨f@u¢g¨f{2~wf¨f~©QY@f¨f6vv¬­æ¶f6³U³®­2fC¥§~w~Q¨f~UY~ww¢2{¨ì~{Y¢fÜQl%~¨f¦~©(~w¬­H{~6Q~w2Q¢6°¦ff2A¢@~f³­v1v6±h»©U¨v¨v)¨f~~~Q£©~2f¦fxd~Y¨f¦¤@u¬R¨v¦v~d¨f¨fd~¦£wA6~¨v¦w¥vo2¦¦~Q&¬Rf¦f2¥x{¤~¢²fg¥w~w¦¥¥w2Q­6¢¦f¢2@{¬R¥¨f~f)v¨fQ¥~w6iö£w~f~Qff¨fu~¬R@2ª¥d¨ª~w6¦d~¦x¨v¬¨vu@f22x~©6t@ww22@xª¨¨¨~~¤¤¤¤¤¤¤¤®©©©å®¨®® Ç©R{@xö¤~Q2Qi ~w¤Q ¦v¥wfªw©  ©R¦d@~Yvf{f¥&d¨ff~f¬R¢@x­{~¥C)w~Q~1¢w¦ff³Q6~wiu~ww{f2© ~²w~w  ©R{v²Q u¦v~w¥w{vRfdf~fv¬R¢fx¥U{¥Uw© @~¦¦fwf~~hQv¥v¥ vff~¨v¥x £w{~©fYx³¥w6v§f¢~@xv{~¦wöQ{~uQ62{ ¨fQ2®t@f~sq2fw®vw¨fvÀ6®@w©(¥wf¥2~Á»¨f³¨2®d6o®$ª~wQ¨f¨f¨f¬­2fl~{¥wf¥wv®%{f~w¨fQ~w&¥wx~w¢~RR¦f¿¨f¬Rl~w~{w22¨f@{2§~A2³6¦fwd¨C6 ¬R~wä~¯®td&wuv¦f¨@9f@d~d~²v@¨f~wftQ~2@{Y¥w©²¢xv~Q¦¥$~³f¬­l«l©w¦w~~§Q~uu~ufd£@ ¤¤¤ ®  v¥±h6©¨v@~w~wwwQd¨fdfU}­@@v¢f~Q¤f#A{¨v¦fv²¡w~{w~v6~{~w6¢ff~wf§¨f¦wQY~@©¦~vv~@f~fww2£f²äw@Y³¨v¥~f¦w~d¦wf¥w¬RfflQvRfs~¨ff fª~f2fwf£d~w°Q~w~w¨Ð~wf®$fw©w¯w¨f2@~~f¿ÍYR¢¬³uR¨fxQ¥w2~¢2v@fv¬R~v2¨fÀöf®$w¨f©16ª~Q¨f@~@ æ¶Q¨f@2H~¬R¢²fÐx&¨f~2{©@¨f¦~i¤°¨f³©Av© @~¢2l~fvw±hx¨v~³£¼@6@¨v22¬­~w29¨f¡v¢~©®ffv¥wfv@¥Q@~¢¥~²6ff~fvQYox{26Q~U¡~w2vvwf£6(QRf~~w22Qff¢2 ¤¤¤ ®© ¬R~f~Q¬RbV{«¤)QQ6¿Ü~f2¨v¨v2@~w(w­À~{x~Q¨f~f¬R¢~2!g@v¦Y6v2¦¬­¨v2{­¨Y¦©2¢vv2Q©fl6Q!¢@QQ6x©fx±h~¦¥&¦w£w7fw(ö¬­f@¨fh¬­@¦2Q~f2{R¨fQ~~³6@x¨1~{£wv¨f@¨xwfl¦w@~®u1l©Q¦v2¢2©­¬~wR£wx@~9~QwÀh¦¥Qf~&@QlChQQ~wwÁ­v¦2¬­)wdQ¥~{¤&¦v)vp¥dfjRQ~~wv@@w@!~w¥wv~¨v¦wQ~~6¦fxf³f¨f¢@f¬ts~fw2%x9@Qª¨ff¥£@~¦¤~~f~Qftx~8vv2³2f2£w¬µ1öwtQ¬2¦©hªwf@Q~©x¥wul!$¬Rh~w­¦v¥vQ²¨fw¦vu{2~Q)6h~Q@Àö~df6)~¦©vl{¬R@%@©$@Cxd2¨¨wsf@~~dQ6¨v(~¦wæ¶26¬Rj@Q~åj~wA2¨vvf¤³2§vQ¦f{f¨vv6ÇuQ~w~w¨U6fvªtÀ©jÐ$v~dvx~w1$³v­f)¬R{¥A¢2vP©j¯ft2f¯@22@¥w6{¨vu~{¢iR%©%¡±hv~l2~D@§~{¿¬fwÁ&6&²~w¨f¨f~w~&@v¨f~ª26d2­v¢2i(f¯¦fw­v2&~³@~w){¨fw¬­c¥~{¬­¢6)@~R¦~{µ¬R6¦vR%l¨fw¨~xRs¬R¬­2fRY¨vff@~{~f¨fu2Q¢¥~~§vPw²Y{~Qf¦wÀ6¨§¢¨Y¨v¨6h~wfv~{¦x®~w¬­@{¨f&@%6&¨fff¦w6x¨v¦v2¨f¨fsj)~w¬R2¬R¢v~w~Q¬©ll¤¨fQwfiff~w¨f¨f~w±h6~whf@v2@f@d¦~wvfx¨v¨vfd~Q~Q$~Q2ª¥¤¤¤¤¤¤©®®¨ ¬­¬R±hQfwu¨v¨vÀ%~~¨f¨f¥A³Á&@~wf6¨A~~¢{~)~w25¥@©ä~³~Ql¢W¨f@¨f¨fw~f¦2¤~)±hQ¨f¨16w2¨d¤w~w66ÐjvR~wtÍ~)@w¦ff¥~fw~¦2~Q£¨¬­¥p¥²&%2"dw§f~Q{!²¨vf~w¨f~¢¨wQ@v¥À(~w36u2~¦f2v£!$±hf©~@¨v­A¥¦xf@UQ¨ffvfh°Q~!¢Q2wlx~w¦ww©©(~f¤¢¦f~{©1£j±h¬­{~l¥~¬¨fª2~§f¦¤{~w³v¦26vi~Q@v¨1¦¨f@¢~©­³%@~öRx~¢±h²vu(~ww¨ff6d¦©våj~{2d~{fv¨f¨Q@f(Çx³f¤)À%2&¯f±hf¬R~w2@ R¨fw©f9x~d¨f¨ff@Rf¬R@@2@f¨¨fv~~¨o)³@@fd¦2dx~Q¨ 0¤¨ ~@¨f~QbV¡¢±h¡±h¨v£wifffx¨f¨ff¨U@j{0°xxY(¢xA®(¦w¢Ydx­)26¦d@¨vc ©¥w~w@³d¦ffx¨vC§Rwf¡¥¢~w¨f¢~fö{w¤­( ¦f2Q~Q~w¦2d2@¦v¨1vYwfsA~¨ffY U~Q~¦@¥&tl&w¦¦@¬­(Q6@~@¬R¥p©~{fu¨f¬­@¨vs{{~Q³~QQ@­(2dQ21@{v»¦~{6f2f{~w¢¨fQ¦fw¥&2vvuw2w~w¥w¥~QvQQ¦Ðvf1~Y{¥¦@ff&R~wf¨v§¦ffl¥~w ¥)Q¥©¨ff~{Q~w@fQ{¢ä~¦wf22~f¥w°ìu2j¨v®dv~wY³~f¤6~QfQ¨f~¦2$@¥w6¥x~)Ðwdwiv@¢fQ@©¦d2{2¥2¢~d«&~{6@~{¯Uj~@{~ff22¦df¬h2¨v2Q$@@ì¥¤¤¤¤ ©©  d¦d~QvQ ¯&u¨v~w¨vRd­ffQ~w~d¥w©©Q¥wv¬Rf6~§f¢~f~wÀ¤¨v~ffw¨ful¦f2~¯@lw&w~w~fQ~w~wh~j@~Q2¨¯dY6{³Q2Àö@&©®wwRx¡f©©­~l26f¢~ff¨f²QÁ¤¨f~w~1~{~v~Q2~2sA~¦d~w{@)x©Ud6Q°~@6@iv6@@w£~w¾¦d©Rf¦f©£¨f¢w¡¼~~Q¨ff6­æ~dQ³¡¦(ª~w2¨U¦wQ¬­dvw)©l®wwxvfw~w~w@Á#f³¥±hQ2v~{~w¨vsv¥~¢~2{Q~w¨ffQ¤%w@UC$~@v~Q&®dv³tA)@®v¨v¥ÁQ¢~w~~16w©~v~w¦fjv~w©h¨vd2{¦~w¢vQvvwQ)vÁf2&f¦f¨fQ)¥w9Q¦v¦v¨fA@&¦f¦w¦f~w¤¨v~Q¥w¦fR¥wvwf¢6222~f&@~w2j~wª°¢l~©fuRf@wf³v±h©¦~f®w~Q6w@~wYQx~w~¨f³~{{i%ö@f¤¢@¦©l~Q26¨vx¥w¨v@x§(~ffQvÁ&f@@dY6ª­wQ6¥¦dvv2~wv¥w2¦¢RYQ6A²w@2~d6¦¿ì¨f{~v¢¿f~~x¨v¨v~w~w2ffQ~Q~Q¨¤¤¤¤ ¦¨§© e@#%¦2!C¥02¨1A¥0 ¦f¦±hQ±h62Q¨v~¨v¨v¨v~w6~Q~~¨f¨f%~{@R@@vj©¥Ð$¨Ð%~¦w(d¤)d®%~wQ°{~wQYw{Qw2d~w2f¦R~{x¡¨wu¦w²~Qv~f³²6¨ffQ³xf@£ªffi¤2~Q¥w¦¢6v¬w©sR@x©£wQ22¨fQæ¶@®@f2@@w~²uv~$Q(¥¥w¨fR©¢~wv{¦~ä~w62¦Qfd@u6¦hfd¥³Æd¥w~lg{v¥f2¬­¨v­~x¥~{¨Yv6f¨föf@2fv~@2h¨f¢³xf2f~w~wÉv¡~wY~U~¿2åvf¯f¦v¨fvQf6@³QU~2~¯~$f¥w­¦vY¨¢~~Q1{~l2xQd6@@@v¡wv2~®1YQ@~Q@w6@2f¥£¦fYwd6Qwv­x¨ffv~Vöh2A~w2³@vQ¦¥¢slÀö¦v~w~wv§~d~¯©Q$wvff©f@v¢u2æ@­©w©q@Cv¡­~«&2¨f®fd~~ff62C~QÆd²~f2°fQlvff¦¬ww(¦v@Q¢f66x~w22¤62¨f~~¥6£2f{{hQ¥wf@@fw~Q¬R2xf¢@$f¢®t2dd~¨f¦¤~1vv@@2@2f¦¨vvdQQQ$~Q~Q2¥¤¤¤®®© æ¶~Q¨f6¦¨¿~§¨v¨v@x@§~vd2@Áx~{Qf®­{{&­~{2¢"{{x~{@¿ì!$e@i2@~{jf6¨f2#&A©Af6©%#%f%'6v~5E±h~w±h@~)lÆh¨f(1@f!¨v¨fv­v@2¬03Ph¦f³xUv¤}¨2f2Qf¥(À6­¦¬h¬­%~d©¬­~Qd{2~{!CQ@w~A¢¨v~{w{Âf2±h²)fvs)@)¦2¨vf66©f(Ff¦w(22~~w@¦2¥~§v~Q~{¨1@¬hAt2{¢@Rvöl~¨f¨2fCx2~dv2wf©~QªÐ@ö4f6)2Q{1f@122v62vR%Á¨ff{Q~@f2P@v{fv@2~~w¥¯dQ~wuf¢26¢fÀ%Q¨f¥wQ22l~Q¦v¦f2fi§2~Q$2QÆ¬R2²®$j¥ww©v­2~w~w¬h)¨fQQ®j¦}©w2Á¨8w®d²¯2¨Y¢¢¥wvQ¨fx6dÐ¨fv@ª6~wff~h~{¨fA£ff2f¬­¬­¢{fvQv¥wxw£wwQ6¨¤¤¤ FëìúuD õöéuêð²ítêjðQ568óÄû@êô¢èC7ñ¢øuüYõöü6éxø­úuíìõ ðò÷éuêDï%ò÷ø@DñîRë$ë%ûQøuï ðxðQú©êõöí¾õöeöè{ølõöíÜï(ëìûuõhëõëìññRó÷éuêë¾ø6ò÷êjéxîRúuúuëvê¢êêóÄðuïtïìfñø6í¾êø#ò÷øuë¾õ@ïòeòêôFªëìëìDøuüYê¢ò÷éuõöñGïìúxêïwøeöðfïêj¨fñò÷ò÷õGúuøsíëìîRFêñGéuë¾ø@eöêêéxõ@ëìí¾CEúuêCòñ9ADfêðuô@%ñô¢óíìéYó÷ñêñóë¾ü³ñøuë¾ò÷ëìúuõöú)ê¶ó÷ò÷ølÿ@õ@ò÷õöþöò÷ëôøhîRêõGD óFõGîRüðuFwDè$ó÷êêtñ%ï¾éuø@îRñýCóÄëìêûuñïê¢ï¾éuó÷ø#ø6ê)ê%ò÷ýieë¾ô óÄñëìDé³êñê¶ë¾íìñGø#ÿ2ò÷êsýieöõöeë&êøõî¤D DûuîRñGñó÷ó÷eöú)õ@íìõ@ê1þöôúuûêþ@ò÷êøúêïó  ¥f¦ff¦fF{¿Ü{¨v¨v22~Ð¨vQ26fvqq~~¥w@v@f6¥lj2Á&Á¤ª~ffwfª@@~Àöf¨f6Qªf@ª©Q{f@xªs©Px¦vv¥Y~{v¤~vf¤vf2Q¢¯¦fw@QªfIl@v±hfl­f§¢QQx~f6¢v¨ful&R¥wªwlx¬¬R¢¨f¨f@A£ff°22fw®{~fj~{~w~i~fQj­f¿¢¨ff¦fYfUfx2ÇC¬6~¥26w¨f2BE2~U)Q¦{f¥{x~xv@@6~Q~f¨§vD¦f~Q¦fw²ö@~~wfYs¬­¨fÉfÍ¯Q&6Qf~©Y)@{¦¦¦d@@~f@v6f©~22¦§@¦~¦¢2v§Æhv2¨v~2U§¨f®¤¾f¿Üævhl{2äffv¨vw¥wxf¦~@iöd¤ª¦j¢Q}»Qv2vf~QÀ§dfv~w{@©vw¨d¥ªª@£©£j¦f¥w~¤~wfQ@¯¦vfiff£f¨v¦2f²@²¨f@¥~f~w¢d®f°¨f¨vQ¥wx©@v~¬§2£Q&¦¨vvQ¡wAf¨f2ª¥w¥@2¨f¥wjRfYvQ2v¥¬h¦¢¤¤@fQ6Â§²Qf~wiQxf~Q2v¨vÇ@2ªC~~2f2¥w{2¨f6~@¤@ä@¨f~{©22QYfwÐ@vÇ&Çx@~wx¨v@®¤@x¦v~{@2&ªÀö¥³©s~Qx@¢~l{©Q{h~f@vQ´°¨f2@v@f¦©~wUo$xf¾vR¦fQv¦©d@@~¦¨f®vv¨f¢©®CwxQfY~~Q¤¥~fUf~w@£¬h¨f¨fH{~w¨w¨f6w~Q¤¬v@Bfl¥©G§©~f@~{{~Qf&Qfª¨fÉS¬­©vf@~~w~@2U@Rv~@RÂ¤¯)fQª¬­xf£wRQ¨f~Q¦2²f¨fRf@¨vf¤~Q62j22¨f6~dx~f¥A2$¦fRv¨fvfw@~QQ®t¬fff2~Y~¬­¤{f@6g¬­¦~{{vf~~wv@³@¢¨v¨v@@{{~{@6@2¦¦fv¨v¨vx2{{QQxf~Q66ö¤¤¤¤¤¤¤¤¤¤ T U ¹sx&| ¢ r&udº¹x&u f¦¦f¦¦fvf±hQ±h{{ääQR¨v¨v~¤@~{~{f~6~¦~¨f¨f¥~{~{vY&~{2æ¶f66²h~ww¬Rw{¬­v²6¨fvvdª{f@2~A~¨f~Q­@~¦~{v2$§@¨f~£w{vA6¨v622w~wQ³&~~~R¨f¨o~{fwÐ{¢2¨fxF©²62x22@~©){~{)f¨f©U~Y²¬R°v@¨f@$²2Á&°{2vv¨v£Q¨v}w{@v6¨f~)@@R¨vv6~{w~@Qw)~df)f~w@f¨¢vö26v®wxf~dQª¡w%xvx¥®u~¬­2v~w@6¥w2%~wf~wv{f£w~@Ðv&f2¡¨fQ²¥Av~vl¥w2fff¥22h)¥ªf~@w2~¢f@w~2CQ§w¤Q~§t@Q22l²f©oQ2¨f@¨v³¥²)f6¢~Qiw)¥h@Rww~t¥²±h¥¨fw³ª~d~wfwöxA®x~Y26QQf¨fo¦v©2¨vfÍ¥2@f¨v{~w~~{&¬R&@@f)6hu6fs{¥w¬Rx6³©äQfQ2v@~ªffQf~vfw~f~Q6©f¦vff@³¨¢¦f­vf2°¦(~{{2¥wv²u³f~{Ð~Qä6~{vv~Q¥w@2~~®{2R¨v6@vQ2v~¨fj¨f({¨v{f~~w©#¥w¤¢¨f{~§oiY~f¨f~{~w4¦fvRv¨fv@fg26sQ±h{@2~2f2w¢uuf@©¨vw¨föwwwv¨fu¦f~{f2Q¦­x~~wöQ~°@f@{Qf1¢@Y{f¢q@{2Q²@vQ£~~2~ww¢x¢v¢¨f@fQ©³¦ff~Q~~QQ¥@jv³26¤Rsovv@@¬R~~{v@¨v@6&wv6ª@{dª~¦¨v¨v¨v¨vu¥fxQ~Q@¢@2¨~¤¤¤¤© ýiêí¾ê1ëìéuêø D ïìêú Fõöí$ëìéuê(ôí¾ê¢ñëìò÷õöø³õGFvñ­ïìò÷îRðuó÷ê%ûuò eöí¾ñî8îRõ@úuêó   og¢¡g$tfgCxl|tgCu  £¥¤§¦©¨¨!"$#&%'$#()0214356678¤@90A2ABC¤D%FE$"EG6IH8P)Q2H  R P)S47TPU2V7WYXQ2(`a6IP)¨b1dcC1dP)Se¤gfhDiqpdr0st)tIu2vxwy2brh5Yr0s  fi( Qj`atwSk©VG'wG0¤ ")vx!2)t  ¥adeqf¥2a"QCe'ga2E2he  ga"ml!Q"I)¨W1n"$#poiU"S4bqi¨)1dP0¤n90A2AC92¤©rhs¨X0"7gX6Q R 61I¨WQ2  X6QS4EtCP)0#uVcYP R 61)2tU"7)7W"P)¨Q21g21g24¨$#C¨WX"P)Q2gQ2vGP R   qi}~1dPI)Dt£X6P)Q2tt)¥fiQ2vweP)6 s(R P0t¤©xy@r giQz$ {2rv8¨r2Sew$eqG2|wvW#y  gvx6)1Iv81Is6¨WX)¤rh¨'£  ¦©QQC¤Y90AA2AC¤irh")¨P)Qm)XQ2¨uPU2V71¥¨WDv)uP)6s(P0¤  fles6nv2jiqIv8cCpdr2r07w5"sU$tUtI#Tr2u2pYevw(5xyy2r ¥%nrdyze g{t"t§Cv8r2¤jwGY2'|wwvxw w( y  22t)tvxw(y©rdgt~¥)6r2  vx)v8s6)e(E261~ja2(e  3©"¨7Weit1Y2#¦5)¨1dP)CtS4S46)1¤n90AA"¤n¥1I¨e} R ¨P)  1IE2X'vxQ"q2tCP)QSmjP)0#m#QXtS4Pg1dPI)tX6P)t)¨¤¢£6X R ¨X027  'EGQ"IPy£§¥A"2H)9a(e(~Q2)67W7¥¨W`6)1I¨Phce"35E"IP)S4PQ"v  ~Q2S4EtCP)6iCX¨Xegt7c¤   ¥wfÇwåsvjfQf)~~§~Q1~ff{R ~Âf~~¢{¢2  For example, a paragraph occur. Applying this of text is grammatical observation to the wherever the line breaks segmentation of a double column of text will indicate where the line breaks occur. For example, a paragraph occur. Applying this of text is grammatical observation to the wherever the line breaks segmentation of a double column of text will indicate where the line breaks occur.  Sometimes sentences may conspire to form false positives of rivers of white space which appear to separate blocks. Sometimes sentences may conspire to form false positives of rivers of white space which appear to separate blocks.  Number Of  Date Of   ¥wf  å  ~{@QDofgs¥ ~QCa2tsQHiorÁ¤s¦es¦f¥wf©QNwame}B~wirtvh{Afd¥©dr}es~sdvQ~ ~w  p(X|A)=0.8  fVGv'u6sCVE¨2)U21IS1I0#S42Q(1§#n7$EC¨)1qQtV$1I"0V#¨e(7¨PPhcR '2E1§)¦©Q2¨V$¡ "¤§V¨7¡¨Pc4g"P $R #jP§2}§1I1IQ"¨U#¢0¡ #  ¨1§vxQ77Q0}§0# V(ck}§Q2U#£¡¥¤TSmc Y`227tiVG6Ph}~u"$#e9¤yf{v  PVGR ' E)"Q2$V$#m"VP ¨R 7¨5P)¨XQ21jP))¨W(¥tP "R P)Q2¨Q1I¥e1 EGR Q2Qj¨}(Pqv¨Q2P)QYP}§R Q2t)7W¨#! R VGP§¥P uR ¤ 4P R ¥X6QP)¨Ct"P)¨Q4vQ2'%&}§Q2t7W#  A p(Y|A)=0.4  X Y   ¥f  åsÆd~w¢f¥6~wwf~{2w2vfR~w²¨fw~2w$u~f­~¨f{~{6f2w  p(X|B)=0.5 B p(Y|B)=0.3    g fg         e          d                nlm  klm  s jq  rlq  jq  k nrq  tq  lj knqq  l  tkt r  ij   & $&      # #  % %  0  & &   ' d )  % & a 3 e  & f   && 22   '!  d %    %  f  # h    a   $  ' (  )   & % &  $  #% $  $3  # h30    (3    $  ( #$  2 %  0  2 (  2 &  # w    $   3  # &  #% & #$  &q $&  2 $ % &  2  &  %&  2 '  ( 3  %  &       &   &$$    # ( $ (  2  2   $    &  $  $&     # P  $   % & %  $  & e X  p  e  k tq  nrqu   ttr lj  m  %  $  #     #% $ (  h3 2  2   $ %  2 %      $    3  % "  ((3     &&  Y  b  #   $&  % 3 d 1  2a  $ % $  # #'  (   $  3 'h  !   iiiiiiiiiiiiiiiiiiiiiii  $ e  && 22  2! $   & P   $  d 1  # #'  (   $  'h  a    iiiii iiiiiiiiiiiiiiiiiii  z  zy y  ju  r  l  nttm   n  
This paper explores two directions for the next step beyond the state of the art of statistical parsing: probabilistic partial parsing and committee-based decision making. Probabilistic partial parsing is a probabilistic extension of the existing notion of partial parsing, which enables ne-grained arbitrary choice on the tradeo between accuracy and coverage. Committeebased decision making is to combine the outputs from dierent systems to make a better decision. While various committee-based techniques for NLP have recently been investigated, they would need to be further extended so as to be applicable to probabilistic partial parsing. Aiming at this coupling, this paper gives a general framework to committee-based decision making, which consists of a set of weighting functions and a combination function, and discusses how it can be coupled with probabilistic partial parsing. Our experiments have so far been producing promising results. 
The left-corner transform removes left-recursion from probabilistic context-free grammars and unication grammars, permitting simple top-down parsing techniques to be used. Unfortunately the grammars produced by the standard left-corner transform are usually much larger than the original. The selective left-corner transform described in this paper produces a transformed grammar which simulates left-corner recognition of a user-speci ed set of the original productions, and top-down recognition of the others. Combined with two factorizations, it produces non-left-recursive grammars that are not much larger than the original. 
In order to realize their full potential, multimodal interfaces need to support not just input from multiple modes, but single commands optimally distributed across the available input modes. A multimodal language processing architecture is needed to integrate semantic content from the different modes. Johnston 1998a proposes a modular approach to multimodal language processing in which spoken language parsing is completed before multimodal parsing. In this paper, I will demonstrate the difficulties this approach faces as the spoken language parsing component is expanded to provide a compositional analysis of deictic expressions. I propose an alternative architecture in which spoken and multimodal parsing are tightly interleaved. This architecture greatly simplifies the spoken language parsing grammar and enables predictive information from spoken language parsing to drive the application of multimodal parsing and gesture combination rules. I also propose a treatment of deictic numeral expressions that supports the broad range of pen gesture combinations that can be used to refer to collections of objects in the interface. Introduction Multimodal interfaces allow content to be conveyed between humans and machines over multiple different channels such speech, graphics, pen, and hand gesture. This enables more natural and efficient interaction since different kinds of content are best suited to particular modes. For example, spatial information is effectively conveyed using gesture for input (Oviatt 1997) and 2d or 3d graphics for output (Towns et al 1998). Multimodal interfaces also stand to play a critical role in the ongoing migration of interaction onto wireless portable computing devices, such as PDAs and next generation phones, which have limited screen real estate and no keyboard. For such devices, complex graphical user interfaces are not feasible and speech and pen will be the  primary input modes. I focus here on multimodal  interfaces which support speech and pen input.  Pen input consists of gestures and drawings which  are made in electronic ink on the computer display  and processed by a gesture recognizer. Speech  input is transcribed using a speech recognizer.  This paper is concerned with the  relationship between spoken language parsing and  multimodal parsing, specifically whether they  should be separate modular components, and the  related issue of determining the appropriate level  of constituent structure at which multimodal  integration should apply. Johnston 1998a  proposes a modular approach in which the  individual modes are parsed and assigned typed  feature structures representing their combinatory  properties and semantic content.  A  multidimensional chart parser then combines these  structures in accordance with a unification-based  multimodal grammar. This approach is outlined in  Section 1. Section 2 addresses the compositional  analysis of deictic expressions and their interaction  with conjunction and other aspects of the  grammar. In Section 3, a new architecture is  presented in which spoken and multimodal parsing  are interleaved. Section 4 presents an analysis of  deictic numeral expressions, and Section 5  discusses certain constructions in which  multimodal integration applies at higher levels of  constituent structure than a simple deictic noun  phrase. I will draw examples from a multimodal  directory and messaging application, specifically a  multimodal variant of VPQ (Buntschuh et al  1998).  
Multimodal interfaces require effective parsing and understanding of utterances whose content is distributed across multiple input modes. Johnston 1998 presents an approach in which strategies for multimodal integration are stated declaratively using a uniﬁcation-based grammar that is used by a multidimensional chart parser to compose inputs. This approach is highly expressive and supports a broad class of interfaces, but offers only limited potential for mutual compensation among the input modes, is subject to signiﬁcant concerns in terms of computational complexity, and complicates selection among alternative multimodal interpretations of the input. In this paper, we present an alternative approach in which multimodal parsing and understanding are achieved using a weighted ﬁnite-state device which takes speech and gesture streams as inputs and outputs their joint interpretation. This approach is signiﬁcantly more efﬁcient, enables tight-coupling of multimodal understanding with speech recognition, and provides a general probabilistic framework for multimodal ambiguity resolution. 
We describe how we constructed an automatic scoring function for machine translation quality; this function makes use of arbitrarily many pieces of natural language processing software that has been designed to process English language text. By machine-learning values of functions available inside the software and by constructing functions that yield values based upon the software output, we are able to achieve preliminary, positive results in machine-learning the difference between human-produced English and machine-translation English. We suggest how the scoring function may be used for MT system development.  Introduction to the MT Plateau We believe it is fair to say that the field of machine translation has been on a plateau for at least the past decade.2 Traditional, hand-built MT systems held up very well in the ARPA MT evaluation (White and O'Connell 1994). These systems are relatively expensive to build and generally require a trained staff working for several years to produce a mature system. This is the current commercial state of the art: hand-building specialized lexicons and translation rules. A completely different type of system was competitive in this evaluation, namely, the purely statistical CANDIDE system built at IBM. It was generally felt that this system had also reached a plateau in that more data and more training was not likely to improve the quality of the output. Low Density Machine Translation However, in the case of "Low Density Machine Translation" (see Nirenburg and Raskin 1998, Jones and Havrilla 1998) commercial market forces are not likely to provide significant incentives for machine translation systems for Low Density (Non-Major) languages any time soon. Two noteworthy efforts to break past the data and labor bottlenecks for high-quality machine translation development are the following. The NSF Summer Workshop on 
Automatic transliteration problem is to transcribe foreign words in one’s own alphabet. Machine generated transliteration can be useful in various applications such as indexing in an information retrieval system and pronunciation synthesis in a text-to-speech system. In this paper we present a model for statistical Englishto-Korean transliteration that generates transliteration candidates with probability. The model is designed to utilize various information sources by extending a conventional Markov window. Also, an efficient and accurate method for alignment and syllabification of pronunciation units is described. The experimental results show a recall of 0.939 for trained words and 0.875 for untrained words when the best 10 candidates are considered. Introduction As the amount of international communication increases, more foreign words are flooding into the Korean language. Especially in the area of computer and information science, it has been reported that 29.4% of index terms are transliterated from or directly written in English in the case of a balanced corpus, KT-SET [18]. The transliteration of foreign words is indispensable in Korean language processing. In information retrieval, a simple method of processing foreign words is via query term translation based on a synonym dictionary of foreign words and their target transliteration. It is  necessary to automate the construction process of a synonym dictionary since its maintenance requires continuous efforts for ever-incoming foreign words. Another area to which transliteration can be applied is a text-to-speech system where orthographic words are transcribed into phonetic symbols. In such applications, maximum likelihood [15], decision tree [1], neural network [10] or weighted finited-state acceptor [19] has been used for finding the best fit. English-to-Korean transliteration problem is that of generating an appropriate Korean word given an English word. In general, there can be various possible transliterations in Korean which correspond to a single English word. It is common that the newly imported foreign word is transliterated into several possible candidate words based on pronunciation, out of which only a few survive in competition over a period of time. In this respect, a statistical approach makes sense where multiple transliteration variations exist for one word, generating candidates in probable order. In this paper, we present a statistical method to transliterate English words in Korean alphabet to generate various candidates. In the next section, we describe a phonetic mapping table construction. In Section 2, we describe alignment and syllabification methods, and in Section 3, mathematical formulation for a statistical model is presented. Section 4 provides experimental results, and finally, we state our conclusions.  * Present address: Sevice Engineering Team, Chollian Service Development Division, DACOM Corporation, Seoul, Korea (E-mail : syrup913@chollian.net)  
We describe a segmentation component that utilizes minimal syntactic knowledge to produce a lattice of word candidates for a broad coverage Japanese NL parser. The segmenter is a finite state morphological analyzer and text normalizer designed to handle the orthographic variations characteristic of written Japanese, including alternate spellings, script variation, vowel extensions and word-internal parenthetical material. This architecture differs from conventional Japanese wordbreakers in that it does not attempt to simultaneously attack the problems of identifying segmentation candidates and choosing the most probable analysis. To minimize duplication of effort between components and to give the segmenter greater freedom to address orthography issues, the task of choosing the best analysis is handled by the parser, which has access to a much richer set of linguistic information. By maximizing recall in the segmenter and allowing a precision of 34.7%, our parser currently achieves a breaking accuracy of ~97% over a wide variety of corpora. Introduction The task of segmenting Japanese text into word units (or other units such as bunsetsu (§phrases)) has been discussed at great length in Japanese NL literature ([Kurohashi98], [Fuchi98], [Nagata94], et al.). Japanese does not typically have spaces between words, which means that a parser must first have the input string broken into usable units before it can analyze a sentence. Moreover, a variety of issues complicate this operation, most notably that potential word candidate records may overlap (causing ambiguities for the parser) or there may be gaps where no suitable record is found (causing a broken span). These difficulties are commonly addressed using either heuristics or statistical methods to create a model for identifying the best (or n-best) sequence  of records for a given input string. This is typically done using a connective-cost model ([Hisamitsu90]), which is either maintained laboriously by hand, or trained on large corpora. Both of these approaches suffer from problems. Handcrafted heuristics may become a maintenance quagmire, and as [Kurohashi98] suggests in his discussion of the JUMAN segmenter, statistical models may become increasingly fragile as the system grows and eventually reach a point where side effects rule out further improvements. The sparse data problem commonly encountered in statistical methods is exacerbated in Japanese by widespread orthographic variation (see §3). Our system addresses these pitfalls by assigning completely separate roles to the segmenter and the parser to allow each to delve deeper into the complexities inherent in its tasks. Other NL systems ([Kitani93], [Kurohashi98]) have separated the segmentation and parsing components. However, these dual-level systems are prone to duplication of effort since many segmentation ambiguities cannot be resolved without invoking higher-level syntactic or semantic knowledge. Our system avoids this duplication by relaxing the requirement that the segmenter identify the best path (or even n-best paths) through the lattice of possible records. The segmenter is responsible only for ensuring that a correct set of records is present in its output. It is the function of the parsing component to select the best analysis from this lattice. With this model, our system achieves roughly 97% recall/precision (see [Suzuki00] for more details). 
 ! "$  % &'  %  #  #%  ( 1 2 %) ,  %!  %  %  & ( %& ) ' * +  . 1 1 %' & ' * +  3  4 5 ! $ ! $! $ +  6 5 - $ ! $! $ +  22 :  7!  -  #  -  - ! % !!  .  9  ++ +  -  %  !  !  -  7! +  +%  +!  -  0$  %  !  - 7%  !  ! % !! !  -  !+  !  #.  !$  .  -  %  #  !  -! +  .!  !+  -  7!  ++ $  %  !  -- ! 0  - 7+  -  $  %  0  0  0 -!  !  !  00.  % ++ $  22 ; < -  2 2 :$ . 0  + -%  !-  #!  %  $ 8!  !  %  !!  -+  -  %  !  !  ! #+ #  -  !+  ! $=  !  #  ! % !! ! - %  0 # ++  #9 !  2 2 ' :$  . -!  !  -  !  $  !  -  7%  %+  -  0$  +0  -  0 -- ! 0  #  !  #  +  #  # - + + 9*  - 221; ,!  >  2 2 1 :$ = + +  !%  + %+  !  !%  0$  +  -  0  $ 8+ !  %  + ! -!  +0  -- ! 0 %  --  0$ .0 .  !-  .  -- ! + #  -.  !  !  %+ ! -!  %  $ # - 0$  -+ - 7!+ ++ !  ! ! -  %  ++  -  7!  !  !  %$  ! % !!  !%  !  7%  . ?$  !-  7  %+  !  $  #  0  !  7!  -  7  !-  -  !+  !  7 !%  -+  #  Term extraction Corpus Co-occurrence data extraction  Terms and their frequencies  Correlation analysis  Co-occurring term pairs and their frequencies  Association Thesaurus (Pairs of associated terms and their correlation values)  !+  9  ? $$  !  -  -  !+ $  7!  ..  0  7$  !  -  ! +%  %+ ! - ! ! ! + $  -  +  +  ! !+  + !+  -@  !!  !+$  - . 7 !#  +  !+  .  !! !  - @ ! 7!  +  $  =  - +.  !-@  !!  .  $  +  - #+  !  + %-%+ ! @ %  !+ $  +  + ! - !$ - .  + - *+  .. $ !!  .  !!  .  +.  .  .  .  #  ! # + !-  #%  ++  ..  -.  !$  ..  #  0 . + -! +  $  ' (' .  7!  -!  # ++ + 0 $  .-  + -.  !%  !+  $ 8- !  !  ! %!! !  !  $ .0  #! ! +  $  !+  A  B > 6 B C AD 3 3 > E ? 8F 4  ,C ? ? 8 F 4 3 > E ? 8 F 4 B C  +#  7 ! !+  .  @! !  . !!  -  +  .  $= -  -%  +  #  BC G G #0 + % +- -  !  #.  -  9!  -  #.  - # -. -  A  9:  9:  89 :  . 2 2 ': $  +.  -  -  -! %  -9 : -9 : -9 : -9 :  +  $, +.  -  -  -! +  !  -  9 $$ .  -9 :  !! ! - @ ! -  9 #0 %  ::  9:  ! %!! ! - @ ! -  9 $$ 9 ! : : $ , + .  -  %  $  7  # - !%  -! +  !  H+ !  -!  +  .  9 $$ 9  ::  9 $$  -  -  9+ ! ::$  !  !#  %  +  +# .  !-  !  -  -  $  .  7!  !  #  -  00  +#  -! +  $?  ++ .  0  - . %- @ !  7 ! %7  !+  .  9/  2 2 I :$  -.  7  !+  $  %.  .  !  #  7  !+  !!  %  .-  +-  +-  !+  +  $  7  !+  !!  +-  !+  $=  #  !  -! +  ! ! 7 ! %7  !%  +  $=  0 0+  !B  #  !# # .  #  0  -  ! -!+  !  -  -. !  ! # $$  $ !+  = = =I  . +#  ! A = 9= = I:  B ++ !7 !! ! !!  ! !+ ! ! $ ,. . .$ .  !  9= = := I$ =  .  !! ! - @ !  7 ! !% +  - . A 8-  0 + -+  = = I !!  ..  7  !+  ! -7 7 -@ ==  # !% !%  #  < #%  9 :E7 +  7 !+  9 +!  9/ !  !  !  :  9" !  +!  ! %#  #  0 $ !%  !%#  #  $  ? @$  < #%  !%#  " !%  !%#  ,!  ? @$  ,!  ? @$  1J& 9  :  1J& 9  :  I)&  :  9  :  '  21  9  :  21 9  :  9  1'  :  (1  (1  9  (1 9  :  (1  :  :  9# : ,  -  !! 8! ! ! 9A #  # < -.  #% ()( . J )&& . ' (I . !!  ! %# 9) $' L : 9I & $' L : 9 ''L : ! %# $:  ! = 9= = I: + - $ B  -  7  !+  !% ==  !!  -@  7  !%  + +-  = =I $  ! 9= = : = I  #  - . A 8- ! +  !  .  !+  !  .!  !+ # .  !  7%  !+  !!  -@  7  !+  !-  !  +-  !-  !  $  =  0.  0  .  .!  !! ! - @ ! - 7 %  !+  $B  #%  !  .!  -@ !  !  .  !+  #  !+  !+ $  !%  !. !  -@ !  !  !!  !+  #  !+  !+  !$  =0  #  #  #  #  !  - &'' +  . !+ 0 7  E0  -  7  !+  #%  ! %#  !%  ! %#  I $J % K ! +  !  $ # 9:  +-  $  ''  %- @ !  !  -  .  %  " !% 12 . ( II . ' (I .  ! %# 9J I $J L : 9 ) $I L : 9 ''L :  #  9# :$  +0  !%  ! %#  #  +  #%  ! %#  %  #  $  !%  !%#  %  #  .  #%  !.  !%  !.  0 #$  +!  -!  !%  !.  0 # . ($ + ! $  9#  $ 221: + +  %  #  !!  -  !!  +  ! -! +  .  #  & I L !! ! $  !! ! .  !!  - !+  !  %  #  !+  $$  !  -  . . $ 8- . ! !  - !%  +  !  .  .  #  -  $  .  ! !!  !%  ! %#  !+ @ . .  +%  ! +0  $  !"  #  $  =  +  +  +  .!  !%  ! %#  #  !  $C  !  - I& 22(  K !+ !  -  !  - !+  - *+  +$8  7+  !- @ ! -  +.  '  ..  !! !  7!  +.  .  -! +  .  -  )%  1& ''' %  . +%  - !! %  7!  !%  ($  #0  ($1  > 2 '''  '' .  $  +  #  - + ! ! + -0 .$ =  !# +  --!  !#  $K ! . !  - . +9 7 ! -  7 ! -! %!! ! : !!  0  -  .  +  0  .!  #  -  ! %!! !  !!  $  !%  Thesaurus Zoom-in overview Generic term ? $ $ !+ 0  Zoom-in Specific term $  !"  # +# .  M-  %  0  #  ! !@  @  .  .7 !  .  -  7 +!  !#  -  $  +#  ! 0 !#  #.  !+ . # +  0  $  0  +# . ++ !+%  +  !%  0  %  #  --!  7+  -  !+ $  C M@  -%  .A  %.  +0  -  !-  $  %  .  ..  +!  #%  !  !+ $  %.  .  ++ +  !#  0  -  $  #0 @  +%  +  0  0 -!  !!  -  -  00.  %  #%  -  $ ! !+  -  0  -!  .  ? $$  +! -  %  7+  +!  - .$  #  +  0 0 . -!+ %+  !  -  +! -  !+ $  00.  -  - !+ $8 !  -!  -!  -  %  . +! #%  !  !+ $"  0%  0.  !  !  0  !  !  -#  + !-!  C !+  !  !  + !-! +!$  !&  ! 9 :$ -! %+ +  -. !. 0. !! 0 0 .$ --!  =  +  7+  0  0!  !  %  !%  +%  +%0 %  9E %  !=  2 &2 : $  +%0 %  #  $ .0 0  +  !  . .+  +  +  # -!  .#  $  !!  .  +%0 %  .  ++  -! %  $  !  "  B  -  00.  !  - % 7!  !%  $  % 7!  .! !  #-  #!  ! # # .$ , I $$ -  !%  $  0 0. -  !  -!  !  ! + $ .%  0.  0 -!  -%  ! $, . ! !  -  ! + - .$  #-  # #. .  I''  +%  +$  :/  !! !  -!  !$  !  .  -  -  !  !!  -% - 9 - %  @ !% 0  !  - @ !: - %  $  !-  9:  !  - - !!  .  9:  # -! ! !  -  !  'L -  #-  !  $8  %  # . ( ('$  :, !  !+ $  ,! -  !  - - @ ! -# !  !! !  $  #  B  -  !  -%  $  %  !# # . $ ,  $  7+ 7+ I$ $ -  ! !% % !  % + !-  7+  - .$ .  :,  0  D3  4  !  -  I''  +  +$  -  $  : ' N NO  D  ;  ' N NO  D  ;  0  %  !  .  ;  ;  ;  .  # 0 % ! # + !%  +  %  !  0  !  .  %  !  .  + !- ! $  %  %- @ !  !  . %- @ !  + !- ! . 7  #  -  #  !-  0  !! ! - @ %  !-  -!  !$  !  -  +  +  #  #!  .  -  7+  !!  !!  .  $  -  + !- !0 . $  !0 + - !  $#  .  $I $  0  !! ! - @ %  !-  -  00.  +!  !! " #  $  =  0+  +  +  !H0  %  $  0  0#  = = = #. $ 8  -!  7% 0  .!  !  -  @$  .  .  7+  %  $I $  +  -  %+  0  . # & !$  !! + # 0  !-  +0  #  !  0 .$  +  +  -  -  !+  #!  #-  #!  .!  !# I $ $  I $ $I $  7+ -  0  .  ? $I$ 8  -  - !+ % +  0  -%- 7  0$  - - !0  -  0  -%  .$  % 8 + 0 !! #  7  0  A  C  !  @  +  !#  -  $  !-  +%  !$  7%  0  !! # 0 -  0%  0  -  -%  .  $  %8 + 0  0  - - ! !A  -  M!  +!  $  !!  + !-  !  -  !$  -  ! -- !  0  -  + !$  # /#  -  #  !-  $  /  !-  '  I  
This paper describes a hybrid parsing method for Japanese which uses both a hand-crafted grammar and a statistical technique. The key feature of our system is that in order to estimate likelihood for a parse tree, the system uses information taken from alternative partial parse trees generated by the grammar. This utilization of alternative trees enables us to construct a new statistical model called Triplet/Quadruplet Model. We show that this model can capture a certain tendency in Japanese syntactic structures and this point contributes to improvement of parsing accuracy on a shallow level. We report that, with an underspeciﬁed HPSG-based grammar and a maximum entropy estimation, our parser achieved high accuracy: 88.6% accuracy in dependency analysis of the EDR annotated corpus, and that it outperformed other purely statistical parsing methods on the same corpus. This result suggests that proper treatment of hand-crafted grammars can contribute to parsing accuracy on a shallow level. 
We present in this paper the method of English-to-KoreanE-K transliteration and back-transliteration. In Korean technical documents, many English words are transliterated into Korean words in various forms in diverse ways. As English words and Korean transliterations are usually technical terms and proper nouns, it is hard to nd a transliteration and its variations in a dictionary. Therefore an automatic transliteration system is needed to nd the transliterations of English words without manual intervention. To explain E-K transliteration phenomena, we use phoneme chunks that do not have a length limit. By applying phoneme chunks, we combine di erent length information with easy. The E-K transliteration method has three steps. In the rst, we make a phoneme network that shows all possible transliterations of the given word. In the second step, we apply phoneme chunks, extracted from training data, to calculate the reliability of each possible transliteration. Then we obtain probable transliterations of the given English word. 
 This paper examines the generation problem for a certain linguistically relevant subclass of LFG grammars. Our main result is that the set of strings that such a grammar relates to a particular f-structure is a context-free language. This result obviously extends to other context-free based grammatical formalisms, such as PATR, and also to formalisms that permit a context-free skeleton to be extracted perhaps some variants of HPSG. The proof is constructive: from the given f-structure a particular contextfree grammar is created whose yield is the desired set of strings. Many existing generation strategies top-down, bottom-up, head-driven can be understood as alternative ways of avoiding the creation of useless context-free productions. Our result can be established for the more general class of LFG grammars, but that is beyond the scope of the present paper.  
In Japanese, case structure analysis is very important to handle several troublesome characteristics of Japanese such as scrambling, omission of case components, and disappearance of case markers. However, for lack of a widecoverage case frame dictionary, it has been difﬁcult to perform case structure analysis accurately. Although several methods to construct a case frame dictionary from analyzed corpora have been proposed, they cannot avoid data sparseness problem. This paper proposes an unsupervised method of constructing a case frame dictionary from an enormous raw corpus by using a robust and accurate parser. It also provides a case structure analysis method based on the constructed dictionary. 
Srinivas (97) enriches traditional morpho-syntactic POS tagging with syntactic information by introducing Supertags. Unfortunately, words are assigned on average a much higher number of Supertags than traditional POS. In this paper, we develop the notion of Hypertag, first introduced in Kinyon (00a) and in Kinyon (00b), which allows to factor the information contained in several Supertags into a single structure and to encode functional information in a systematic manner. We show why other possible solutions based on mathematical properties of trees are unsatisfactory and also discuss the practical usefulness of this approach. Introduction As a first step prior to parsing, traditional Part of Speech (POS) tagging assigns limited morpho-syntactic information to lexical items. These labels can be more or less fine-grained depending on the tagset , but syntactic information is often absent or limited. Also, most lexical items are assigned several POS. Although lexical ambiguities are dealt with by POS taggers, either in a rule-based or in probabilistic manner, it is useful to delay this decision at a further parsing step (e.g. Giguet (98) shows that knowing constituent boundaries is crucial for solving lexical ambiguity correctly). In order to do so, it would help to be able to encode several POS into one compact representation. In order to assign richer syntactic information to lexical items Joshi & Srinivas (94) and Srinivas (97) introduce the notion of Supertags, developed within the framework of Tree Adjoining Grammars (TAG). The idea behind Supertags is to assign to each word in a sentence, instead of a traditional POS, an "elementary tree", which constitutes a primitive syntactic structure within the TAG framework. A supertagged text can then be inputed to a parser or shallow parser, thus alleviating the task of the parser. Several problems remain though:  • Even when no lexical ambiguity occurs, each word can anchor several trees (several hundreds for some verbs)1. On average for English a word is associated with 1.5 POS and with 9 supertags (Joshi (99)). One common solution to the problem is to only retain the "best" supertag for each word, or eventually the 3 best supertags for each word, but then early decision has an adverse effect on the quality of parsing if the wrong supertag(s) have been kept : one typically obtains between 75% and 92% accuracy when supertagging, depending on the type of text being supertagged and on the technique used) (cf Srinivas (97), Chen & al (99), Srinivas & Joshi (99)). This means that it may be the case that every word in 4 will be assigned the wrong supertag, whereas typical POS taggers usually achieve an accuracy above 95%. • Supertagged texts rely heavily on the TAG framework and therefore may be difficult to exploit without being familiar with this formalism. • Supertagged texts are difficult to read and thus difficult to annotate manually. • Some structural information contained in Supertags is redundant • Some information is missing, especially with respect to syntactic functions2. So our idea is to investigate how supertags can be underspecified so that instead of associating a set of supertags to each word, one could associate one single structure, which we call hypertag, and which contains the same information as a set of supertags as well as functional information Our practical goal is fourfolds : a) delaying decision for parsing b) obtaining a compact and readable representation, which can be manually annotated 
The goal of text categorization is to classify documents into a certain number of predefined categories. The previous works in this area have used a large number of labeled training documents for supervised learning. One problem is that it is difficult to create the labeled training documents. While it is easy to collect the unlabeled documents, it is not so easy to manually categorize them for creating training documents. In this paper, we propose an unsupervised learning method to overcome these difficulties. The proposed method divides the documents into sentences, and categorizes each sentence using keyword lists of each category and sentence similarity measure. And then, it uses the categorized sentences for training. The proposed method shows a similar degree of performance, compared with the traditional supervised learning methods. Therefore, this method can be used in areas where low-cost text categorization is needed. It also can be used for creating training documents. Introduction With the rapid growth of the internet, the availability of on-line text information has been considerably increased. As a result, text categorization has become one of the key techniques for handling and organizing text data. Automatic text categorization in the previous works is a supervised learning task, defined as assigning category labels (pre-defined) to text documents based on the likelihood suggested by a training set of labeled documents. However, the previous learning algorithms have some  problems. One of them is that they require a large, often prohibitive, number of labeled training documents for the accurate learning. Since the application area of automatic text categorization has diversified from newswire articles and web pages to electronic mails and newsgroup postings, it is a difficult task to create training data for each application area (Nigam K. et al., 1998). In this paper, we propose a new automatic text categorization method based on unsupervised learning. Without creating training documents by hand, it automatically creates training sentence sets using keyword lists of each category. And then, it uses them for training and classifies text documents. The proposed method can provide basic data for creating training documents from collected documents, and can be used in an application area to classify text documents in low cost. We use the χ2 statistic (Yang Y. et al., 1998) as a feature selection method and the naive Bayes classifier (McCallum A. et al., 1998) as a statistical text classifier. The naive Bayes classifier is one of the statistical text classifiers that use word frequencies as features. Other examples include k-nearest-neighbor (Yang Y. et al., 1994), TFIDF/Roccio (Lewis D.D. et al., 1996), support vector machines (Joachims T. et al., 1998) and decision tree (Lewis D.D. et al., 1994). 
We present a method to realize exible mixedinitiative dialogue, in which the system can make e ective con rmation and guidance using concept-level con dence measures CMs derived from speech recognizer output in order to handle speech recognition errors. We de ne two concept-level CMs, which are on contentwords and on semantic-attributes, using 10-best outputs of the speech recognizer and parsing with phrase-level grammars. Content-word CM is useful for selecting plausible interpretations. Less con dent interpretations are given to conrmation process. The strategy improved the interpretation accuracy by 11.5. Moreover, the semantic-attribute CM is used to estimate user's intention and generates system-initiative guidances even when successful interpretation is not obtained. 
This paper describes a multilingual text generation system in the domain of CAD CAM software instructions for Bulgarian, Czech and Russian. Starting from a language-independent semantic representation, the system drafts natural, continuous text as typically found in software manuals. The core modules for strategic and tactical generation are implemented using the KPML platform for linguistic resource development and generation. Prominent characteristics of the approach implemented are a treatment of multilinguality that makes maximal use of the commonalities between languages while also accounting for their di erences and a common representational strategy for both text planning and sentence generation. 
Designing the dialogue strategy of a spoken dialogue system involves many nontrivial choices. This paper presents a reinforcement learning approach for automatically optimizing a dialogue strategy that addresses the technical challenges in applying reinforcement learning to a working dialogue system with human users. We then show that our approach measurably improves performance in an experimental system. 
A hybrid system for tagging part of speech is described that consists of a neuro tagger and a rule-based corrector. The neuro tagger is an initial-state annotator that uses dierent lengths of contexts based on longest context priority. Its inputs are weighted by information gains that are obtained by information maximization. The rule-based corrector is constructed by a set of transformation rules to make up for the shortcomings of the neuro tagger. Computer experiments show that almost 20% of the errors made by the neuro tagger are corrected by these transformation rules, so that the hybrid system can reach an accuracy of 95.5% counting only the ambiguous words and 99.1% counting all words when a small Thai corpus with 22,311 ambiguous words is used for training. This accuracy is far higher than that using an HMM and is also higher than that using a rule-based model. 
In the last decade, members of the computational linguistics community have adopted a perspective on discourse based primarily on either Rhetorical Structure Theory or Grosz and Sidner’s Theory. However, only recently, researchers have started to investigate the relationship between the two perspectives. In this paper, we use Moser and Moore’s (1996) work as a departure point for extending Marcu’s formalization of RST (1996). The result is a ﬁrst-order axiomatization of the mathematical properties of text structures and of the relationship between the structure of text and intentions. The axiomatization enables one to use intentions for reducing the ambiguity of discourse and the structure of discourse for deriving intentional inferences. 
Due to grammatical similarities, even a one-to-one mapping between Korean and Japanese words (or morphemes) can usually result in a high quality Korean-to-Japanese machine translation. However, multi-word translation units (MWTU) such as idioms, compound words, etc., need an n-to-m mapping, and their component words often do not appear adjacently, resulting in a discontinuous MWTU. During translation, the MWTU should be treated as one lexical item rather than a phrase. In this paper, we define the types of MWTUs and propose their representation and recognition method depending on their characteristics in Korean-to-Japanese MT system. In an experimental evaluation, the proposed method turned out to be very effective in handling MWTUs, showing an average recognition accuracy of 98.4% and a fast recognition time. 
In this paper, parsing-as-deduction and constraint programming are brought together to outline a procedure for the speciﬁcation of constraint-based chart parsers. Following the proposal in Shieber et al. (1995), we show how to directly realize the inference rules for deductive parsers as Constraint Handling Rules (Fru¨hwirth, 1998) by viewing the items of a chart parser as constraints and the constraint base as a chart. This allows the direct use of constraint resolution to parse sentences. 
This paper describes two new bunsetsu identication methods using supervised learning. Since Japanese syntactic analysis is usually done after bunsetsu identication, bunsetsu identication is important for analyzing Japanese sentences. In experiments comparing the four previously available machinelearning methods (decision tree, maximum-entropy method, example-based approach and decision list) and two new methods using category-exclusive rules, the new method using the category-exclusive rules with the highest similarity performed best. 
While recent advancements in virtual reality technology have created a rich communication interface linking humans and computers, there has been little work on building dialogue systems for 3D virtual worlds. This paper proposes a method for altering the instruction dialogue to match the user's view in a virtual environment. We illustrate the method with the system MID-3D, which interactively instructs the user on dismantling some parts of a car. First, in order to change the content of the instruction dialogue to match the user's view, we extend the re nement-driven planning algorithm by using the user's view as a plan constraint. Second, to manage the dialogue smoothly, the system keeps track of the user's viewpoint as part of the dialogue state and uses this information for coping with interruptive subdialogues. These mechanisms enable MID-3D to set instruction dialogues in an incremental way it takes account of the user's view even when it changes frequently. 
In natural language generation, different generation tasks often interact with each other in a complex way, which is hard to capture in the pipeline architecture described by Reiter (Reiter, 1994). This paper focuses on the interaction between a specific type of aggregation and text planning, in particular, maintaining local coherence, and tries to explore what preferences exist among the factors related to the two tasks. The evaluation result shows that it is these preferences that decide the quality of the generated text and capturing them properly in a generation system could lead to coherent text. 
W e present an efficientmulti-levelchart parser that was designed for syntacticanalysisof closed captions (subtitles) in a real-time Machine Translation (MT) system. In order to achieve high parsing speed, we divided an existing English grammar into multiple levels. The parser proceeds in stages. At each stage, rules corresponding to only one level are used. A constituent pruning step is added between levels to insure that constituents not likely to be part of the final parse are removed. This results in a significant parse time and ambiguity reduction. Since the domain is unrestricted, out-of-coveragesentences are to be expected and the parser might not produce a single analysis spanning the whole input. Despite the incomplete parsing strategy and the radical pruning, the initial evaluation results show that the loss of parsing accuracy is acceptable. The parsing time favorable compares with a Tomita parser and a chart parser parsing time when run on the same grammar and lexicon. 
The process of documenting designs is tedious and often error-prone. We discuss a system that automatically generates documentation for the single step transition behavior of Statecharts with particular focus on the correctness of the result in the sense that the document will present all and only the facts corresponding to the design being documented. Our approach is to translate the Statechart into a propositional formula, then translate this formula into a natural language report. In the later translation pragmatic effects arise due to the way the information is presented. Whereas such effects can be difficult to quantify, we account for them within an abstract framework by applying a series of transformations on the structure on the report while preserving soundness and completeness of the logical content. The result is an automatically generated hypertext report that is both logically correct and, to a relatively high degree of confidence, free of misleading implicatures. 
This paper proposes a grammar-based approach to semantic annotation which combines the notions of robust parsing and fuzzy grammars. We present an overview of a preliminary research aimed to generalize some results from a recent project on interaction through speech with information systems where techniques based on the above notions have been successfully applied. The goal of the article is to give a development environment to linguists• 
This paper presents a parsing system for the detection of syntactic errors. It combines a robust partial parser which obtains the main sentence components and a finite-state parser used for the description of syntactic error patterns. The system has been tested on a corpus of real texts, containing both correct and incorrect sentences, with promising results. Introduction The problem of syntactic error detection and correction has been addressed since the early years of natural language processing. Different techniques have been proposed for the treatment of the significant portion of errors (typographic, phonetic, cognitive and grammatical) that result m valid words (Weischedel and Sondheimer 1983; Heidorn et al. 1982). However, although most currently used word-processors actually provide a grammar checking module, little work has been done on the evaluation of results. There are several reasons for this: • Incomplete coverage. Some of the best parsers at the moment can analyze only a subset of the sentences in real texts. Compared to syntactic valid structures, the set of syntactically incorrect sentences can be considered almost infinite. When a sentence cannot be parsed it is difficult to determine whether it corresponds to a syntactic error or to an uncovered syntactic construction. In the literature, syntactic errors have been defined mostly with respect to their corresponding correct constructions. The use of unrestricted corpora confronts us with the problem of flagging a correct structure as erroneous (false alarms). These facts widen the scope of the problem, as not only incorrect structures but also correct ones must be taken into account. On the other hand, robust parsing systems (e.g., statistical ones) are often unable to distinguish ungrammatical structures from correct ones.  • The need for big corpora. Each kind of syntactic error occurs with very low frequency and, therefore, big corpora are needed for testing. Even if such corpora were available, the task of recognizing error instances for evaluation is a hard task, as there are no syntactically annotated treebanks with error marks for the purposes of evaluation and testing. Thus, to obtain naturally occurring test data, hundreds of texts must be automatically and manually examined and marked. The aim of the present work is to examine the feasibility of corpus-based syntactic error detection, with methods that are sensitive enough to obtain high correction rates and discriminating enough to maintain low false alarm rates. The system will be applied to Basque, an agglutinative language with relative free order among sentence components. Its recent standardization makes it necessary to develop a syntactic checking tool. The remainder of this paper is organized as follows. After commenting on the literature on syntactic error detection in section 2, section 3 presents a description of the linguistic resources we have used. Section 4 describes the error types we have treated, while section 5 gives the evaluation results. 
Many corpora which are prime candidates for automatic error correction, such as the output of OCR software, and electronic texts incorporating markup tags, include information on which portions of the text are most likely to contain errors. This paper describes how the error markup tag <?> is being incorporated in the spell-checking of an electronic version of Diderot's Encyclopddie, and evaluates whether the presence of this tag has significantly aided in correcting the errors which it marks. Although the usefulness of error tagging may vary from project to project, even as the precise way in which the tagging is done varies, error tagging does not necessarily confer any benefit in attempting to correct a given word. It may, of course, nevertheless be useful in marking errors to be fixed manually at a later stage of processing the text. 
 sense disambiguation (WSD) algorithm aimed at  improving the precision of our cross-language  We have developed a word sense retrieval system.  disambiguation algorithm, following Cheng and  Wilensky (1997), to disambiguate among WordNet synsets. This algorithm is to be used in a cross-language information retrieval system, CINDOR, which indexes queries and documents in a language-neutral concept representation based on WordNet synsets. Our goal is to improve retrieval precision through word sense disambiguation. An evaluation against human disambiguation judgements suggests promise for our approach.  2 Related Work To determine the sense of a word, a WSD algorithm typically uses the context of the ambiguous word, external resources such as machine-readable dictionaries, or a combination of both. Although dictionaries provide useful word sense information and thesauri provide additional information about relatiomhips between words, they lack pragmatic information as can be found in corpora. Corpora contain  examples of words that enable the development of  
Most studies on discourse markers implicitly assume that only one marker or discourse relation will occur in a sentence. In reality, more than one relation may hold between text spans and may be cued by multiple discourse markers. We describe here a method for hierarchically organising discourse markers. The hierarchies are intended for use by a generation system to enable the selection and placement of more than one marker in a single text span. 
This paper describes an implementation of some key aspects of a theory of dialogue processing whose main concerns are to provide models of GROUNDING and of the role of DISCOURSE OBLIGATIONSin an agent's deliberation processes. Our system uses the TrindiKit dialogue move engine toolkit, which assumes a model of dialogue in which a participan. t's knowledge is characterised in terms of INFORMATION STATES which are subject to various kinds of updating mechanisms. 
We empirically show that there are significant differences between the discourse structure of Japanese texts and the discourse structure of their corresponding English translations. To improve translation quality, we propose a computational model for rewriting discourse structures. When we train our model on a parallel corpus of manually built Japanese and English discourse structure trees, we learn to rewrite Japanese trees as trees that are closer to the natural English rendering than the original ones. 
Our aim in this paper is to identify genreindependent factors that influence the decision to pronominalize. Results based on the annotation of twelve texts from four genres show that only a few factors have a strong influence on pronominaliza- tion across genres, i.e. distance from last mention, agreement, and form of the antecedent. Finally, we describe a probabilistic model of pronominalization derived from our data. 
This paper describes a method for linear text segmentation which is twice as accurate and over seven times as fast as the state-of-the-art (Reynar, 1998). Inter-sentence similarity is replaced by rank in the local context. Boundary locations are discovered by divisive clustering. 
Bagging and boosting, two effective machine learning techniques, are applied to natural language parsing. Experiments using these techniques with a trainable statistical parser are described. The best resulting system provides roughly as large of a gain in F-measure as doubling the corpus size. Error analysis of the result of the boosting technique reveals some inconsistent annotations in the Penn Treebank, suggesting a semi-automatic method for finding inconsistent treebank annotations. 
The goal of this paper is to describe how the EuroWordNet framework for representing lexical meaning is being modified within an Italian National Project in order to include information on adjectives. The focus is on the 'new' semantic relations being encoded and on the revisions we have made to the EuroWordNet Top Ontology structure. We also briefly discuss the utility of the information which is being encoded for computational applications. Introduction The Princeton WordNet (henceforth WN) is a lexical semantic network in which the meanings of words are represented in terms of their conceptual and lexical relations to other words. The basic notion around which it is developed is that of a synset (synonyms set), i.e. a set of words with the same Part-of-Speech (PoS) that can be interchanged in a certain context. Various conceptual and lexical relations are then encoded between synsets of the same PoS: e.g., hyponymy, antonymy, meronymy, etc. (Miller et al. 1990; Fellbaum 1998b).  Within the EuroWordNet (henceforth EWN) projectI a similar (multilingual) lexical resource was developed, retaining the basic underlying design of WN, but enriching the set of lexicalsemantic relations to be encoded for nouns and verbs in various ways2, in order to obtain a maximally re-usable resource for computational applications. Thus, a) cross-PoS (xPos) relations were added so that different surface realizations of similar concepts within and across languages could be matched (e.g., the noun research and the verb to research could be linked as 
The performance of machine learning algorithms can be improved by combining the output of different systems. In this paper we apply this idea to the recognition of noun phrases. We generate different classifiers by using different representations of the data. By combining the results with voting techniques described in (Van Halteren et al., 1998) we manage to improve the best reported performances on standard data sets for base noun phrases and arbitrary noun phrases. 
This paper presents the syntactic and semantic tags used to annotate predicate-argument structure in the Berkeley FrameNet Project. It briefly explains the theory of frame semantics on which semantic annotation is based, discusses possible applications of FrameNet annotation, and compares FrameNet to other prominent iexical resources. Introduction This paper presents the tagset used to annotate the predicate-argument structures of English verbs, adjectives, and nouns in the Berkeley FrameNet Project (NSF IR]-9618838, "Tools for Lexicon Building"), a corpus-based computational lexicography project based on the theory of frame semantics (see Fillmore 1982). It briefly explains the theoretical background and shows how frame-semantic annotation creates lexicographic generalizations that are not possible with more traditional linguistic approaches to argument structure based on thematic roles. 
This paper presents a corpus-based approach to word sense disambiguation that builds an ensemble of Naive Bayesian classifiers, each of which is based on lexical features that represent co-occurring words in varying sized windows of context. Despite the simplicity of this approach, empirical results disambiguating the widely studied nouns line and interest show that such an ensemble achieves accuracy rivaling the best previously published results. 
Many corpus-based machine translation systems require parallel corpora. In this paper, we present a word-for-word glossing algorithm that requires only a source language corpus. To gloss a word, we first identify its similar words that occurred in the same context in a large corpus. We then determine the gloss by maximizing the similarity between the set of contextually similar words and the different translations of the word in a bilingual thesaurus. 1. Introduction Word-for-word glossing is the process of directly translating each word or term in a document without considering the word order. Automating this process would benefit many NLP applications. For example, in crosslanguage information retrieval, glossing a document often provides a sufficient translation for humans to comprehend the key concepts. Furthermore, a glossing algorithm can be used for lexical selection in a full-fledged machine translation (MT) system. Many corpus-based MT systems require parallel corpora (Brown et al., 1990; Brown et al., 1991; Gale and Church, 1991; Resnik, 1999). Kikui (1999) used a word sense disambiguation algorithm and a non-paralM bilingual corpus to resolve translation ambiguity. In this paper, we present a word-for-word glossing algorithm that requires only a source language corpus. The intuitive idea behind our algorithm is the following. Suppose w is a word to be translated. We first identify a set of words similar to w that occurred in the same context as w in a large corpus. We then use this set (called  the contextually similar words of w) to select a translation for w. For example, the contextually similar words of duty in fiduciary duty include responsibility, obligation, role, ... This list is then used to select a translation for duty. In the next section, we describe the resources required by our algorithm. In Section 3, we present an algorithm for constructing the contextually similar words of a word in a context. Section 4 presents the word-for-word glossing algorithm and Section 5 describes the group similarity metric used in our algorithm. In Section 6, we present some experimental results and finally, in Section 7, we conclude with a discussion of future work. 2. Resources The input to our algorithm includes a collocation database (Lin, 1998b) and a corpus-based thesaurus (Lin, 1998a), which are both available on the Interne0. In addition, we require a bilingual thesaurus. Below, we briefly describe these resources. 2.1. Collocation database Given a word w in a dependency relationship (such as subject or object), the collocation database can be used to retrieve the words that occurred in that relationship with w, in a large corpus, along with their frequencies2. Figure 1 shows excerpts of the entries in the collocation database for the words corporate, duty, and fiduciary. The database contains a total of 11 million unique dependency relationships. I Available at www.cs.umanitoba.ca/-lindek/depdb.htm and www.cs.umanitoba.ca/-lindek/simdb.htm 2 We use the term collocation to refer to a pair of words that occur in a dependency relationship (rather than the linear proximity of a pair of words).  78  Table 1. Clustered similar words o f duty as given by (Lin, 1998a).  CLUSTER  CLUSTERED SIMILARWORDSOFDUTY (WITH SIMILARITYSCORE)  responsibility 0.16, obligation 0.109, task 0.101, function 0.098, role 0.091, post 0.087, position 0.086, job 0.084, chore 0.08, mission 0.08, assignment 0.079, liability 0.077 .... tariff0.091, restriction 0.089, tax 0.086, regulation 0.085, requirement 0.081, procedure 0.079, penalty 0.079, quota 0.074, rule 0.07, levy 0.061 .... fee 0.085, salary 0.081, pay 0.064, fine 0.058 personnel 0.073, staff0.073 training 0.072, work 0.064, exercise 0.061 privilege 0.069, right 0.057, license 0.056  2.2. Corpus-based thesaurus Using the collocation database, Lin used an unsupervised method to construct a corpusbased thesaurus (Lin, 1998a) consisting of 11839 nouns, 3639 verbs and 5658 adjectives/adverbs. Given a word w, the thesaurus returns a clustered list of similar words of w along with their similarity to w. For example, the clustered similar words of duty are shown in Table 1. 2.3. Bilingual thesaurus Using the corpus-based thesaurus and a bilingual dictionary, we manually constructed a bilingual thesaurus. The entry for a source language word w is constructed by manually associating one or more clusters of similar words of w to each candidate translation of w. We refer to the assigned clusters as Words Associated with a Translation (WAT). For example, Figure 2 shows an excerpt of our English~French bilingual thesaurus for the words account and duty. Although the WAT assignment is a manual process, it is a considerably easier task than providing lexicographic definitions. Also, we only require entries for source language words that have multiple translations. In Section 7, we  corporate: modifier-of: duty: objeet-of: subject-of: adj-modifier: fiduciary: modifier-of:  client 196, debt 236, development 179, fee 6, function 16, headquarter 316, IOU 128, levy 3, liability 14, manager 203, market 195, obligation 1, personnel 7, profit 595, responsibility 27, rule 7, staff 113, tax 201, training 2, vice president 231 . . . . assume 177, breach 111, carry out 71, do 114, have 257, impose 114, perform 151.... affect 4, apply 6, include 42, involve 8, keep 5, officer 22, protect 8, require 13, ... active 202, additional 46, administrative 44, fiduciary 317, official 66, other 83, ... act 2, behavior I, breach 2, claim I, company 2, duty 317, irresponsibility 2, obligation 32, requirement 1, responsibility 89, role 2, ...  Figure 1. Excepts o f entries in the collocation database for the words corporate, duty, andfiduciary.  account: 1. compte:  investment, transaction, payment, saving, i money, contract, Budget, reserve, security,! contribution, debt, property holding  W A T for ii~::::=........................  2. rapport: duty: 1. devoir: 2. taxe:  report, statement, testimony, card, story, record, document, data, information, view, cheek, figure, article, description, estimate, assessment, number, statistic, comment, letter, picture, note, ... responsibility, obligation, task, function, role, post, position, job, chore, mission, assignment, liability.... tariff, restriction, tax, regulation, requirement, procedure, penalty, quota, rule, levy, ...  Figure 2. Bilingual thesaurus entries for account and duty.  discuss a method for automatically assigning the WATs.  3. Contextually Similar Words The contextually similar words of a word w are words similar to the intended meaning of w in its context. Figure 3 gives the data flow diagram for our algorithm for identifying the contextually similar words of w. Data are represented by ovals, external resources by double ovals and processes by rectangles. By parsing a sentence with Minipar3, we extract the dependency relationships involving w. For each dependency relationship, we retrieve  3 Available at www.cs.umanitoba.ca/-lindek/minipar.htm  79  Input  
Arabic inflectional morphology requires infixation, prefixation and suffixation, giving rise to a large space of morphological variation. In this paper we describe an approach to reducing the complexity of Arabic morphology generation using discrimination trees and transformational rules. By decoupling the problem of stem changes from that of prefixes and suffixes, we gain a significant reduction in the number of rules required, as much as a factor of three for certain verb types. We focus on hollow verbs but discuss the wider applicability of the approach. Introduction Morphologically, Arabic is a non-concatenative language. The basic problem with generating Arabic verbal morphology is the large number of variants that must be generated. Verbal stems are based on triliteral or quadriliteral roots (3- or 4-radicals). Stems are formed by a derivational combination of a root morpheme and a vowel melody; the two are arranged according to canonical patterns. Roots are said to interdigitate with patterns to form stems. For example, the Arabic stem katab (he wrote) is composed of the morpheme ktb (notion of writing) and the vowel melody morpheme 'a-a'. The two are coordinated according to the pattern CVCVC (C=consonant, V=vowel). There are 15 triliteral patterns, of which at least 9 are in common use, and 4 much rarer quadriliteral patterns. All these patterns undergo some stem changes with respect to voweling in  the 2 tenses (perfect and imperfect), the 2 voices (active and passive), and the 5 moods (indicative, subjunctive, jussive, imperative and energetic).~ The stem used in the conjugation of the verb may differ depending on the person, number, gender, tense, mood, and the presence of certain root consonants. Stem changes combine with suffixes in the perfect indicative (e.g., katab-naa 'we wrote', kutib-a 'it was written') and the imperative (e.g. uktub-uu 'write', plural), and with both prefixes and suffixes for the imperfect tense in the indicative, subjunctive, and jussive moods (e.g. ya-ktub-na 'they write, feminine plural') and in the energetic mood (e.g. ya-ktub-unna or ya-ktub-un 'he certainly writes'). There are a total of 13 person-number-gender combinations. Distinct prefixes are used in the active and passive voices in the imperfect, although in most cases this results in a change in the written form only if diacritic marks are used.2 Most previous computational treatments of Arabic morphology are based on linguistic models that describe Arabic in a nonconcatenative way and focus primarily on analysis. Beesley (1991) describes a system that analyzes Arabic words based on Koskenniemi's 
Part of Speech tagging for English seems to have reached the the human levels of error, but full morphological tagging for inflectionally rich languages, such as Romanian, Czech, or Hungarian, is stillan open problem, and the results are far from being satisfactory. This paper presents results obtained by using a universalized exponential feature-based model for fivesuch languages. It focuses on the data sparseness issue, which is especiallysevere for such languages (the more so that there are no extensive annotated data for those languages). In conclusion, we argue strongly that the use of an independent morphological dictionary is the preferred choice to more annotated data under such circumstances. 
This paper investigates the impact of Constraint Dependency Grammars (CDG) on the accuracy of an integrated speech recognition and CDG parsing system. We compare a conventional CDG with CDGs that are induced from annotated sentences and template-expanded sentences. The grammars are evaluated on parsing speed, precision/coverage, and improvement of word and sentence accuracy of the integrated system. Sentence-derived CDGs significantly improve recognition accuracy over the conventional CDG but are less general. Expanding the sentences with templates provides us with a mechanism for increasing the coverage of the grammar with only minor reductions in recognition accuracy. 
This paper proposes a statistical method for learning dependency preference of Japanese subordinate clauses, in which scope embedding preference of subordinate clauses is exploited as a useful information source for disambiguating dependencies between subordinate clauses. Estimated dependencies of subordinate clauses successfully increase the precision of an existing statistical dependency analyzer. 
This paper demonstrates that machine learning is a suitable approach for rapid parser development. From 1000 newly treebanked Korean sentences we generate a deterministic shift-reduce parser. The quality of the treebank, particularly crucial given its small size, is supported by a consistency checker. 
The eventual goal of a language model is to accurately predict the value of a missing word given its context. We present an approach to word prediction that is based on learning a representation for each word as a function of words and linguistics predicates in its context. This approach raises a few new questions that we address. First, in order to learn good word representations it is necessary to use an expressive representation of the context. We present a way that uses external knowledge to generate expressive context representations, along with a learning method capable of handling the large number of features generated this way that can, potentially, contribute to each prediction. Second, since the number of words "competing" for each prediction is large, there is a need to "focus the attention" on a smaller subset of these. We exhibit the contribution of a "focus of attention" mechanism to the performance of the word predictor. Finally, we describe a large scale experimental study in which the approach presented is shown to yield significant improvements in word prediction tasks. 
We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trMned and tested on the previously established [5,9,10,15,17] "standard" sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus [9]. The major technical innovation is tire use of a "ma~ximum-entropy-inspired" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head. 
We present an unsupervised method for detecting grammatical errors by inferring negative evidence from edited textual corpora. The system was developed and tested using essay-length responses to prompts on the Test of English as a Foreign Language (TOEFL). The errorrecognition system, ALEK, performs with about 80% precision and 20% recall. Introduction A good indicator of whether a person knows the meaning of a word is the ability to use it appropriately in a sentence (Miller and Gildea, 1987). Much information about usage can be obtained from quite a limited context: Choueka and Lusignan (1985) found that people can typically recognize the intended sense of a polysemous word by looking at a narrow window of one or two words around it. Statistically-based computer programs have been able to do the same with a high level of accuracy (Kilgarriff and Palmer, 2000). The goal of our work is to automatically identify inappropriate usage of specific vocabulary words in essays by looking at the local contextual cues around a target word. We have developed a statistical system, ALEK (Assessing Le____xicalKnowledge), that uses statistical analysis for this purpose. A major objective of this research is to avoid the laborious and costly process of collecting errors (or negative evidence) for each word that we wish to evaluate. Instead, we train ALEK on a general corpus of English and on edited text containing example uses of the target word. The system identifies inappropriate usage based on differences between the word's local context cues in an essay and the models of context it has derived from the corpora of well-formed sentences.  A requirement for ALEK has been that all steps in the process be automated, beyond choosing the words to be tested and assessing the results. Once a target word is chosen, preprocessing, building a model of the word's appropriate usage, and identifying usage errors in essays is performed without manual intervention. ALEK has been developed using the Test of English as a Foreign Language (TOEFL) administered by the Educational Testing Service. TOEFL is taken by foreign students who are applying to US undergraduate and graduate-level programs. 
We present a method for automatically detecting errors in a manually marked corpus using anomaly detection. Anomaly detection is a method for determining which elements of a large data set do not conform to the whole. This method fits a probability distribution over the data and applies a statistical test to detect anomalous elements. In the corpus error detection problem, anomalous elements are typically marking errors. We present the results of applying this method to the tagged portion of the Penn Treebank corpus. 
This paper describes a method for estimating conditional probability distributions over the parses of "unification-based" grammars which can utilize auxiliary distributions that are estimated by other means. We show how this can be used to incorporate information about lexical selectional preferences gathered from other sources into Stochastic "Unificationbased" Grammars (SUBGs). While we apply this estimator to a Stochastic LexicalFunctional Grammar, the method is general, and should be applicable to stochastic versions of HPSGs, categorial grammars and transformational grammars. 
We describe a novel approach to 'packing' of local ambiguity in parsing with a wide-coverage HPSG grammar, and provide an empirical assessment of the interaction between various packing and parsing strategies. We present a linear-time, bidirectional subsumption test for typed feature structures and demonstrate that (a) subsumption- and equivalence-based packing is applicable to large HPSGgrammars and (b) average parse complexity can be greatly reduced in bottom-up chart parsing with comprehensive HPSGimplementations. 
This paper presents a new approach to statistical sentence generation in which Mternative phrases are represented as packed sets of trees, or forests, and then ranked statistically to choose the best one. This representation offers advantages in compactness and in the ability to represent syntactic information. It also facilitates more efficient statistical ranking than a previous approach to statistical generation. An efficient ranking algorithm is described, together with experimental results showing significant improvements over simple enumeration or a lattice-based approach. 
We present a cut and paste based text summarizer, which uses operations derived from an analysis of human written abstracts. The summarizer edits extracted sentences, using reduction to remove inessential phrases and combination to merge resuiting phrases together as coherent sentences. Our work includes a statistically based sentence decomposition program that identifies where the phrases of a summary originate in the original document, producing an aligned corpus of summaries and articles which we used to develop the summarizer. 
Automatic generation of text summaries for spoken language faces the problem of containing incorrect words and passages due to speech recognition errors. This paper describes comparative experiments where passages with higher speech recognizer confidence scores are favored in the ranking process. Results show that a relative word error rate reduction of over 10% can be achieved while at the same time the accuracy of the summary improves markedly. 
We present three systems for surface natural language generation that are trainable from annotated corpora. The first two systems, called NLG1 and NLG2, require a corpus marked only with domainspecific semantic attributes, while the last system, called NLG3, requires a corpus marked with both semantic attributes and syntactic dependency information. All systems attempt to produce a grammatical natural language phrase from a domain-specific semantic representation. NLG1 serves a baseline system and uses phrase frequencies to generate a whole phrase in one step, while NLG2 and NLG3 use maximum entropy probability models to individually generate each word in the phrase. The systems NLG2 and NLG3 learn to determine both the word choice and the word order of the phrase. We present experiments in which we generate phrases to describe flights in the air travel domain. 
In this paper, we describe an empirical evaluation of an adaptive mixed initiative spoken dialogue system. We conducted two sets of experiments to evaluate the mixed initiative and automatic adaptation aspects of the system, and analyzed the resulting dialogues along three dimensions: performance factors, discourse features, and initiative distribution. Our results show that 1) both the mixed initiative and automatic adaptation aspects led to better system performance in terms of user satisfaction and dialogue efficiency, and 2) the system's adaptation behavior better matched user expectations, more efficiently resolved dialogue anomalies, and resulted in higher overall dialogue quality. 
Current spoken dialogue systems are deficient in their strategies for preventing, identifying and repairing problems that arise in the conversation. This paper reports results on learning to automatically identify and predict problematic human-computer dialogues in a corpus of 4774 dialogues collected with the How May I Help You spoken dialogue system. Our expectation is that the ability to predict problematic dialogues will allow the system's dialogue manager to modify its behavior to repair problems, and even perhaps, to prevent them. We train a problematic dialogue classifier using automaticallyobtainable features that can identify problematic dialogues significantly better (23%) than the baseline. A classifier trained with only automatic features from the first exchange in the dialogue can predict problematic dialogues 7% more accurately than the baseline, and one trained with automatic features from the first two exchanges can perform 14% better than the baseline. 
In spoken dialogue systems, it is important for a system to know how likely a speech recognition hypothesis is to be correct, so it can reprompt for fresh input, or, in cases where many errors have occurred, change its interaction strategy or switch the caller to a human attendant. We have discovered prosodic features which more accurately predict when a recognition hypothesis contains a word error than the acoustic confidence score thresholds traditionally used in automatic speech recognition. We present analytic results indicating that there are significant prosodic differences between correctly and incorrectly recognized turns in the TOOT train information corpus. We then present machine learning results showing how the use of prosodic features to automatically predict correct versus incorrectly recognized turns improves over the use of acoustic confidence scores alone. 
Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard. In this paper we report adapting a lexicalized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations. 
It is generally recognized that the common nonterminal labels for syntactic constituents (NP, VP, etc.) do not exhaust the syntactic and semantic information one would like about parts of a syntactic tree. For example, the Penn Treebank gives each constituent zero or more 'function tags' indicating semantic roles and other related information not easily encapsulated in the simple constituent labels. We present a statistical algorithm for assigning these function tags that, on text already parsed to a simplelabel level, achieves an F-measure of 87%, which rises to 99% when considering 'no tag' as a valid choice. 
Given the lack of word delimiters in written Japanese, word segmentation is generally considered a crucial first step in processing Japanese texts. Typical Japanese segmentation algorithms rely either on a lexicon and grammar or on pre-segmented data. In contrast, we introduce a novel statistical method utilizing unsegmented training data, with performance on kanji sequences comparable to and sometimes surpassing that of morphological analyzers over a variety of error metrics. I Introduction Because Japanese is written without delimiters be- tween words) accurate word segmentation to re- cover the lexical items is a key step in Japanese text processing. Proposed applications of segmentation technology include extracting new technical terms, indexing documents for information retrieval, and correcting optical character recognition (OCR) errors (Wu and Tseng, 1993; Nagao and Mori, 1994; Nagata, 1996a; Nagata, 1996b; Sproat et al., 1996; Fung, 1998). Typically, Japanese word segmentation is performed by morphological analysis based on lexical and grammatical knowledge. This analysis is aided by the fact that there are three types of Japanese characters, kanji, hiragana, and katakana: changes in character type often indicate word boundaries, although using this heuristic alone achieves less than 60% accuracy (Nagata, 1997). Character sequences consisting solely of kanji pose a challenge to morphologically-based segreenters for several reasons. First and most importantly, kanji sequences often contain domain terms and proper nouns: Fung (1998) notes that 50-85% of the terms in various technical dictio- ~Theanalogoussituationin Englishwouldbe if wordswere written withoutspaces betweenthem.  Sequence length 1 - 3 kanji 4 - 6 kanji more than 6 kanji Total  # of characters 20,405,486 12,743,177 3,966,408 37,115,071  % of corpus 25.6 16.1 5.1 46.8  Figure 1: Statistics from 1993 Japanese newswire (NIKKEI), 79,326,406 characters total. naries are composed at least partly of kanji. Such words tend to be missing from general-purpose lexicons, causing an unknown word problem for morphological analyzers; yet, these terms are quite important for information retrieval, information extraction, and text summarization, making correct segmentation of these terms critical. Second, kanji sequences often consist of compound nouns, so grammatical constraints are not applicable. For instance, the sequence sha-chohlkenlgyoh-mulbuchoh (presidentlandlbusinesslgeneral manager = "a president as well as a general manager of business") could be incorrectly segmented as: sha- chohlken-gyohlmulbu-choh (presidentl subsidiary business[Tsutomu [a name][general manager); since both alternatives are four-noun sequences, they cannot be distinguished by part-of-speech information alone. Finally, heuristics based on changes in character type obviously do not apply to kanji-only sequences. Although kanji sequences are difficult to segment, they can comprise a significant portion of Japanese text, as shown in Figure 1. Since sequences of more than 3 kanji generally consist of more than one word, at least 21.2% of 1993 Nikkei newswire consists of kanji sequences requiring segmentation. Thus, accuracy on kanji sequences is an important aspect of the total segmentation process. As an alternative to lexico-grammatical and supervised approaches, we propose a simple, effi-  241  cient segmentation method which learns mostly from very large amounts of unsegmented training data, thus avoiding the costs of building a lexicon or grammar or hand-segmenting large amounts of training data. Some key advantages of this method are: • No Japanese-specific rules are employed, enhancing portability to other languages. • A very small number of pre-segmented training examples (as few as 5 in our experiments) are needed for good performance, as long as large amounts of unsegmented data are available. • For long kanji strings, the method produces results rivalling those produced by Juman 3.61 (Kurohashi and Nagao, 1998) and Chasen 1.0 (Matsumoto et al., 1997), two morphological analyzers in widespread use. For instance, we achieve 5% higher word precision and 6% better morpheme recall. 2 Algorithm Our algorithm employs counts of character n-grams in an unsegmented corpus to make segmentation decisions. We illustrate its use with an example (see Figure 2). Let "A B C D W X Y Z" represent an eight-kanji sequence. To decide whether there should be a word boundary between D and W, we check whether ngrams that are adjacent to the proposed boundary, such as the 4-grams sl = " A B C D" and 82 = " W X Y Z", tend to be more frequent than n-grams that straddle it, such as the 4-gram tl ----"B C D W". If so, we have evidence of a word boundary between D and W, since there seems to be relatively little cohesion between the characters on opposite sides of this gap. The n-gram orders used as evidence in the segmentation decision are specified by the set N. For instance, if N = {4} in our example, then we pose the six questions of the form, "Is #(s~) > #(tj)?", where #(x) denotes the number of occurrences of x in the (unsegmented) training corpus. If N = {2,4}, then two more questions (Is "#(C D) > #(D W)?" and "Is #(W X) > #(O W)?") are added. More formally, let s~ and 8~ be the non- straddling n-grams just to the left and right of location k, respectively, and let t~ be the straddling n-gram with j characters to the right of location k.  s, ?  I ABCb{i WXYZ  t,  %  •  /,  Figure 2: Collecting evidence for a word boundary - are the non-straddling n-grams 81 and 82 more frequent than the straddling n-grams tl, t2, and t3?  Let I> (y, z) be an indicator function that is 1 when y > z, and 0 otherwise,2 In order to compensate for the fact that there are more n-gram questions than (n - 1)-gram questions, we calculate the fraction of affirmative answers separately for each n in N:  
A long-standing issue regarding algorithms that manipulate context-free grammars (CFGs) in a "topdown" left-to-right fashion is that left recursion can lead to nontermination. An algorithm is known that transforms any CFG into an equivalent nonleft-recursive CFG, but the resulting grammars are often too large for practical use. We present a new method for removing left recursion from CFGs that is both theoretically superior to the standard algorithm, and produces very compact non-left-recursive CFGs in practice. 
We propose a method for identifying diathesis alternations where a particular argument type is seen in slots which have different grammatical roles in the alternating forms. The method uses selectional preferences acquired as probability distributions over WordNet. Preferences for the target slots are compared using a measure of distributional similarity. The method is evaluated on the causative and conative alternations, but is generally applicable and does not require a priori knowledge specific to the alternation. 
In this paper we tackle sentence boundary disambiguation through a part-of-speech (POS) tagging framework. We describe necessary changes in text tokenization and the implementation of a POS tagger and provide results of an evaluation of this system on two corpora. We also describe an extension of the traditional POS tagging by combining it with the document-centered approach to proper name identification and abbreviation handling. This made the resulting system robust to domain and topic shifts. 
We compare the asymptotic time complexity of left-to-right and bidirectional parsing techniques for bilexical context-free grammars, a grammar formalism that is an abstraction of language models used in several state-of-the-art real-world parsers. We provide evidence that left-to-right parsing cannot be realised within acceptable time-bounds if the so called correct-prefix property is to be ensured. Our evidence is based on complexity results for the representation of regular languages. 
Acknowledgments are relatively rare in humancomputer interaction. Are people unwilling to use this human convention when talking to a machine, or is their scarcity due to the way that spoken-language interfaces are designed? We found that, given a simple spoken-language interface that provided opportunities for and responded to acknowledgments, about half of our subjects used acknowledgments at least once and nearly 30% used them extensively during the interaction. 
Alignment of phonetic sequences is a necessary step in many applications in computational phonology. After discussing various approaches to phonetic alignment, I present a new algorithm that combines a number of techniques developed for sequence comparison with a scoring scheme for computing phonetic similarity on the basis of multivalued features. The algorithm performs better on cognate alignment, in terms of accuracy and efficiency, than other algorithms reported in the literature. 
Reduplication, a central instance of prosodic morphology, is particularly challenging for state-ofthe-art computational morphology, since it involves copying of some part of a phonological string. In this paper I advocate a finite-state method that combines enriched lexical representations via intersection to implement the copying. The proposal includes a resource-conscious variant of automata and can benefit from the existence of lazy algorithms. Finally, the implementation of a complex case from Koasati is presented. 
A finite-state method, based on leftmost longestmatch replacement, is presented for segmenting words into graphemes, and for converting graphemes into phonemes. A small set of hand-crafted conversion rules for Dutch achieves a phoneme accuracy of over 93%. The accuracy of the system is further improved by using transformation-based learning. The phoneme accuracy of the best system (using a large rule and a 'lazy' variant of Brill's algoritm), trained on only 40K words, reaches 99%. 
This paper describes AUTOSEM, a robust semantic interpretation framework that can operate both at parse time and repair time. The evaluation demonstrates that AUTOSEM achieves a high level of robustness efficiently and without requiring any hand coded knowledge dedicated to repair. 
Although natural language is ambiguous, various linguistic and extra-linguistic factors often help determine a preferred reading. In this paper, we show that model generation can be used to model this process in the case of reciprocal statements. The proposed analysis builds on insights from Dalrymple et al. 98 and is shown to provide an integrated, computational account of the interplay between model theoretic interpretation, knowledge-based reasoning and preferences that characterises the interpretation of reciprocals. 
We introduce a framework for semantic interpretation in which dependency structures are mapped to conceptual representations based on a parsimonious set of interpretation schemata. Our focus is on the empirical evaluation of this approach to semantic interpretation, i.e., its quality in terms of recall and precision. Measurements are taken with respect to two real-world domains, viz. information technology test reports and medical finding reports. 
The paper describes a natural language based expert system route advisor for the public bus transport in Trondheim, Norway. The system is available on the Internet,and has been intstalled at the bus company's web server since the beginning of 1999. The system is bilingual, relying on an internal language independent logic representation. 
 contain strings of keywords. Typical queries are, as in most Web search applications, two  Simple measures can achieve high-accuracy cross-language retrieval in carefully chosen applications. Image retrieval is one of those applications, with results ranging from 68% of human translator performance for German, to 100% for French.  to three words in length. At this point, all of the captions are in English. eMotion hosts a large database of images for sale and for licensing, PictureQuest. At least 10% of PictureQuest's user base is outside the United States. The tests were performed on the PictureQuest database of approximately  
A major obstacle to the construction of a probabilistic translation model is the lack of large parallel corpora. In this paper we first describe a parallel text mining system that finds parallel texts automatically on the Web. The generated Chinese-English parallel corpus is used to train a probabilistic translation model which translates queries for Chinese-English cross-language information retrieval (CLIR). We will discuss some problems in translation model training and show the preliminary CUR results. 
This paper describes a system that provides customer service by allowing users to retrieve identification numbers of parts for medical systems using spoken natural language dialogue. The paper also presents an evaluation of the system which shows that the system successfully retrieves the identification numbers of approximately 80% of the parts. 
This paper proposes a way to improve the translation quality by using information on dialogue participants that is easily obtained from outside the translation component. We incorporated information on participants' social roles and genders into transfer rules and dictionary entries. An experiment with 23 unseen dialogues demonstrated a recall of 65% and a precision of 86%. These results showed that our simple and easy-to-implement method is effective, and is a key technology enabling smooth conversation with a dialogue translation system. 
We report on a method for utilising corpora collected in natural settings. It is based on distilling (re-writing) natural dialogues to elicit the type of dialogue that would occur if one the dialogue participants was a computer instead of a human. The method is a complement to other means such as Wizard of Oz-studies and un-distilled natural dialogues. We present the distilling method and guidelines for distillation. We also illustrate how the method affects a corpus of dialogues and discuss the pros and cons of three approaches in different phases of dialogue systems development. 
This paper describes an application of APE (the Atlas Planning Engine), an integrated planning and execution system at the heart of the Atlas dialogue management system. APE controls a mixedinitiative dialogue between a human user and a host system, where turns in the 'conversation' may include graphical actions and/or written text. APE has full unification and can handle arbitrarily nested discourse constructs, making it more powerful than dialogue managers based on finitestate machines. We illustrate this work by describing Atlas-Andes, an intelligent tutoring system built using APE with the Andes physics tutor as the host. 
Ithaca, NY USA, 14850 richard @cogentex.com  Tanya Korelsky CoGenTex, Inc. 840 Hanshaw Road Ithaca, NY USA, 14850 t a n y a @c o g e n t e x . c o m  Owen Rambow * ATT Labs-Research, B233 180 Park Ave, PO Box 971 Florham Park, NJ USA, 07932 rambow @research.att.com  Abstract In this paper we describe an implemented framework for developing monolingual or multilingual natural language generation (NLG) applications and machine translation (MT) applications. The framework demonstrates a uniform approach to generation and transfer based on declarative lexico-structural transformations of dependency structures of syntactic or conceptual levels ("uniform lexico-structural processing"). We describe how this framework has been used in practical NLG and MT applications, and report the lessons learned. 
We describe Talk'n'Travel, a spoken dialogue language system for making air travel plans over the telephone. Talk'n'Travel is a fully conversational, mixed-initiative system that allows the user to specify the constraints on his travel plan in arbitrary order, ask questions, etc., in general spoken English. The system operates according to a plan-based agenda mechanism, rather than a finite state network, and attempts to negotiate with the user when not all of his constraints can be met. Introduction This paper describes Talk'n'Travel, a spoken language dialogue system for making complex air travel plans over the telephone. Talk'n'Travel is a research prototype system sponsored under the DARPA Communicator program (MITRE, 1999). Some other systems in the program are Ward and Pellom (1999), Seneff and Polifroni (2000) and Rudnicky et al (1999). The common task of this program is a mixed-initiative dialogue over the telephone, in which the user plans a multi-city trip by air, including all flights, hotels, and rental cars, all in conversational English over the telephone. The Communicator common task presents special challenges. It is a complex task with many subtasks, including the booking of each flight, hotel, and car reservation. Because the number of legs of the trip may be arbitrary, the number of such subtasks is not known in advance. Furthermore, the user has complete  freedom to say anything at any time. His utterances can affect just the current subtask, or multiple subtasks at once ("I want to go from Denver to Chicago and then to San Diego"). He can go back and change the specifications for completed subtasks. And there are important constraints, such as temporal relationships between flights, that must be maintained for the solution to the whole task to be coherent. In order to meet this challenge, we have sought to develop dialogue techniques for Talk'n'Travel that go beyond the rigid systemdirected style of familiar IVR systems. Talk'n'Travel is instead a mixed initiative system that allows the user to specify constraints on his travel plan in arbitrary order. At any point in the dialogue, the user can supply information other than what the system is currently prompting for, change his mind about information he has previously given and even ask questions himself. The system also tries to be helpful, eliciting constraints from the user when necessary. Furthermore, if at any point the constraints the user has specified cannot all be met, the system steps in and offers a relaxation of them in an attempt to negotiate a partial solution with the user. The next section gives a brief overview of the system. Relevant components are discussed in subsequent sections. I System Overview The system consists of the following modules: speech recognizer, language understander, dialogue manager, state manager, language generator, and speech synthesizer. The modules 
This paper reports on a large-scale, end-toend relation and event extraction system. At present, the system extracts a total of 100 types of relations and events, which represents a much wider coverage than is typical of extraction systems. The system consists of three specialized pattem-based tagging modules, a high-precision coreference resolution module, and a configurable template generation module. We report quantitative evaluation results, analyze the results in detail, and discuss future directions. Introduction One major goal of information extraction (IE) technology is to help users quickly identify a variety of relations and events and their key players in a large volume of documents. In contrast with this goal, state-of-the-art information extraction systems, as shown in the various Message Understanding Conferences (MUCs), extract a small number of relations and events. For instance, the most recent MUC, MUC-7, called for the extraction of 3 relations (person-employer, maker-product, and organization-location) and 1 event (spacecraft launches). Our goal is to develop an IE system which scales up to extract as many types of relations and events as possible with a minimum amount of porting effort combined with high accuracy. Currently, REES handles 100 types of relations and events, and it does so in a modular, configurable, and scalable manner. Below, Section 1 presents the ontologies of relations and events that we have developed.  Section 2 describes REES' system architecture. Section 3 evaluates the system's performance, and offers a qualitative analysis of system errors. Section 4 discusses future directions.  
This paper explores the problem of identifying sentence boundaries in the transcriptions produced by automatic speech recognition systems. An experiment which determines the level of human performance for this task is described as well as a memorybased computational approach to the problem. 
This paper describes and evaluates a detector of presuppositions (DP) for survey questions. Incorrect presuppositions can make it difficult to answer a question correctly. Since they can be difficult to detect, DP is a useful tool for questionnaire designer. DP performs well using local characteristics of presuppositions. It reports the presupposition to the survey methodologist who can determine whether the presupposition is valid. Introduction Presuppositions are propositions that take some information as given, or as "the logical assumptions underlying utterances" (Dijkstra & de Smedt, 1996, p. 255; for a general overview, see McCawley, 1981). Presupposed information includes state of affairs, such as being married; events., such as a graduation; possessions, such as a house, children, knowledge about something; and others. For example, the question, "when did you graduate from college", presupposes the event that the respondent did in fact graduate from college. The answer options may be ranges of years, such as "between 1970 and 1980". Someone who has never attended college can either not respond at all, or give a random (and false) reply. Thus, incorrect presuppositions cause two problems. First, the question is difficult to answer. Second, assuming that people feel obliged to answer them anyway, their  answers present false information. This biases survey statistics, or, in an extreme case, makes them useless. The detector for presuppositions (DP) is part of the computer tool QUAID (Graesser, WiemerHastings, Kreuz, Wiemer-Hastings & Marquis, in press), which helps survey methodologists design questions that are easy to process. DP detects a presupposition and reports it to the survey methodologist, who can examine if the presupposition is correct. QUAID is a computerized QUEST questionnaire evaluation aid. It is based on QUEST (Graesser & Franklin, 1990), a computational model of the cognitive processes underlying human question answering. QUAID critiques questions with respect to unfamiliar technical terms, vague terms, working memory overload, complex syntax, incorrect presuppositions, and unclear question purpose or category. These problems are a subset of potential problems that have been identified by Graesser, Bommareddy, Swamer, and Golding (1996; see also Graesser, Kennedy, Wiemer-Hastings & Ottati, 1999). QUAID performs reliably on the first five problem categories. In comparison to these five problems, presupposition detection is even more challenging. For unfamiliar technical terms, for example, QUAID reports words with frequencies below a certain threshold. Such an elegant solution is impossible for presuppositions. Their forms vary widely across presupposition types. Therefore, their detection requires a complex set of rules, carefully tuned to identify a variety of presupposition problems. DP prints out the  90  presuppositions of a question, and relies on the survey methodologist to make the final decision whether the presuppositions are valid. 
This paper describes MIMIC, an adaptive mixed initiative spoken dialogue system that provides movie showtime information. MIMIC improves upon previous dialogue systems in two respects. First, it employs initiative-oriented strategy adaptation to automatically adapt response generation strategies based on the cumulative effect of information dynamically extracted from user utterances during the dialogue. Second, MIMIC's dialogue management architecture decouples its initiative module from the goal and response strategy selection processes, providing a general framework for developing spoken dialogue systems with different adaptation behavior. 
JAVOX provides a mechanism for the development of spoken-language systems from existing desktop applications. We present an architecture that allows existing Java1 programs to be speech-enabled with no source-code modification, through the use of reflection and automatic modification to the application's compiled code. The grammars used in JAvox are based on the Java Speech Grammar Format (JSGF); JAVOX grammars have an additional semantic component based on our JAVOX Scripting Language (JSL). JAVOX has been successfully demonstrated on real-world applications. 
We describe an architecture for spoken dialogue interfaces to semi-autonomous systems that transforms speech signals through successive representations of linguistic, dialogue, and domain knowledge. Each step produces an output, and a meta-output describing the transformation, with an executable program in a simple scripting language as the final result. The output/meta-output distinction permits perspicuous treatment of diverse tasks such as resolving pronouns, correcting user misconceptions, and optimizing scripts. 
This paper introduces an approach to representing the kinds of information that components in a natural language generation (NLG) system will need to communicate to one another. This information may be partial, may involve more than one level of analysis and may need to include information about the history of a derivation. We present a general representation scheme capable of handling these cases. In addition, we make a proposal for organising intermodule communication in an NLG system by having a central server for this information. We have validated the approach by a reanalysis of an existing NLG system and through a full implementation of a runnable specification. 
Over the past decade or so, a lot of work in computational linguistics has been directed at finding ways to exploit the ever increasing volume of electronic bilingual corpora. These efforts have allowed for substantial expansion of the computational toolbox. We describe a system, TransCheck, which makes intensive use of these new tools in order to detect potential translation errors in preliminary or non-revised translations. Introduction 
This work is in the context of TRANSTYPE, a system that observes its user as he or she types a trans- lation and repeatedly suggests completions for the text already entered. The user may either accept, modify, or ignore these suggestions. We describe the design, implementation, and performance of a prototype which suggests completions of units of texts that are longer than one word. 
In this paper we present a new, multilingual data-driven method for coreference resolution as implemented in the SWIZZLE system. The results obtained after training this system on a bilingual corpus of English and Romanian tagged texts, outperformed coreference resolution in each of the individual languages. 
In this paper, we describe a system to rank suspected answers to natural language questions. We process both corpus and query using a new technique, predictive annotation, which augments phrases in texts with labels anticipating their being targets of certain kinds of questions. Given a natural language question, our IR system returns a set of matching passages, which we then rank using a linear function of seven predictor variables. We provide an evaluation of the techniques based on results from the TREC Q&A evaluation in which our system participated. 
Customer care in technical domains is increasingly based on e-mail communication, allowing for the reproduction of approved solutions. Identifying the customer's problem is often time-consuming, as the problem space changes if new products are launched. This paper describes a new approach to the classification of e-mail requests based on shallow text processing and machine learning techniques. It is implemented within an assistance system for call center agents that is used in a commercial setting. 
This paper discusses an information extraction (IE) system, Textract, in natural language (NL) question answering (QA) and examines the role of IE in QA application. It shows: (i) Named Entity tagging is an important component for QA, (ii) an NL shallow parser provides a structural basis for questions, and (iii) high-level domain independent IE can result in a QA breakthrough. Introduction With the explosion of information in Internet, Natural language QA is recognized as a capability with great potential. Traditionally, QA has attracted many AI researchers, but most QA systems developed are toy systems or games confined to lab and a very restricted domain. More recently, Text Retrieval Conference (TREC-8) designed a QA track to stimulate the research for real world application. Due to little linguistic support from text analysis, conventional IR systems or search engines do not really perform the task of information retrieval; they in fact aim at only document retrieval. The following quote from the QA Track Specifications (www.research.att.com/ -singhal/qa-track-spec.txt) in the TREC community illustrates this point. Current information retrieval systems allow us to locate documents that might contain the pertinent information, but most of them leave it to the user to extract the useful information from a ranked list. This leaves the (often  unwilling) user with a relatively large amount of text to consume. There is an urgent need for tools that would reduce the amount of text one might have to read in order to obtain the desired information. This track aims at doing exactly that for a special (and popular) class of information seeking behavior: QUESTION ANSWERING. People have questions and they need answers, not documents. Automatic question answering will definitely be a significant advance in the state-of-art information retrieval technology. Kupiec (1993) presented a QA system MURAX using an on-line encyclopedia. This system used the technology of robust shallow parsing but suffered from the lack of basic information extraction support. In fact, the most siginifcant IE advance, namely the NE (Named Entity) technology, occured after Kupiec (1993), thanks to the MUC program (MUC-7 1998). High-level IE technology beyond NE has not been in the stage of possible application until recently. AskJeeves launched a QA portal (www.askjeeves.com). It is equipped with a fairly sophisticated natural language question parser, but it does not provide direct answers to the asked questions. Instead, it directs the user to the relevant web pages, just as the traditional search engine does. In this sense, AskJeeves has only done half of the job for QA. We believe that QA is an ideal test bed for demonstrating the power of IE. There is a natural co-operation between IE and IR; we regard QA as one major intelligence which IE can offer IR.  * This work was supported in part by the SBIR grants F30602-98-C-0043 and F30602-99-C-0102 from Air Force Research Laboratory (AFRL)/IFED.  166  An important question then is, what type of IE can support IR in QA and how well does it support it? This forms the major topic of this paper. We structure the remaining part of the paper as follows. In Section 1, we first give an overview of the underlying IE technology which our organization has been developing. Section 2 discusses the QA system. Section 3 describes the limitation of the current system. Finally, in Section 4, we propose a more sophisticated QA system supported by three levels of IE. 
This paper introduces a system for categorizing unknown words. The system is based on a multicomponent architecture where each component is responsible for identifying one class of unknown words. The focus of this paper is the components that identify names and spelling errors. Each component uses a decision tree architecture to combine multiple types of evidence about the unknown word. The system is evaluated using data from live closed captions - a genre replete with a wide variety of unknown words. 
We describe and evaluate an implemented system for general-knowledge question answering. The system combines techniques for standard ad-hoc information retrieval (IR), query-dependent text summarization, and shallow syntactic and semantic sentence analysis. In a series of experiments we examine the role of each statistical and linguistic knowledge source in the question-answering system. In contrast to previous results, we find first that statistical knowledge of word co-occurrences as computed by IR vector space methods can be used to quickly and accurately locate the relevant documents for each question. The use of query-dependent text summarization techniques, however, provides only small increases in performance and severely limits recall levels when inaccurate. Nevertheless, it is the text summarization component that allows subsequent linguistic filters to focus on relevant passages. We find that even very weak linguistic knowledge can offer substantial improvements over purely IRbased techniques for question answering, especially when smoothly integrated with statistical preferences computed by the IR subsystems. 
ARBITER is a Prolog program that extracts assertions about macromolecular binding relationships from biomedical text. We describe the domain knowledge and the underspecified linguistic analyses that support the identification of these predications. After discussing a formal evaluation of ARBITER, we report on its application to 491,000 MEDLINE~ abstracts, during which almost 25,000 binding relationships suitable for entry into a database of macromolecular function were extracted. Introduction Far more scientific information exists in the literature than in any structured database. Convenient access to this information could significantly benefit research activities in various fields. The emerging technology of information extraction (Appelt and Israel 1997, Hearst 1999) provides a means of gaining access to this information. In this paper we report on a project to extract biomolecular data from biomedical text. We concentrate on molecular binding affinity, which provides a strong indication of macromolecular function and is a core phenomenon in molecular biology. Our ultimate goal is to automatically construct a database of binding relationships asserted in MEDLINE citations. The National Library of Medicine's MEDLINE textual database is an online repository of more than 10 million citations from the biomedical literature. All citations contain the title of the corresponding article along with other bibliographic information. In addition, a large number of citations contain author-supplied abstracts. Initial studies indicate that there are ap-  proximately 500,000 MEDLINE citations relevant to molecular binding affinity. Our decision to apply information extraction technology to binding relationships was guided not only by the biological importance of this phenomenon but also by the relatively straightforward syntactic cuing of binding predications in text. The inflectional forms of a single verb, bind, indicate this relationship in the vast majority of cases, and our initial work is limited to these instances. For example, our goal in this project is to extract the binding predications in (2) from the text in (1). (1) CC chemokine receptor 1 (CCR1) is expressed in neutrophils, monocytes, lymphocytes, and eosinophils, and binds the leukocyte chemoattractant and hematopoiesis regulator macrophage inflammatory protein (MIP)- 1alpha, as well as several related CC chemokines. (2) <CC chemokine receptor 1> BINDS <leukocyte chemoattractant> <CC chemokine receptor 1> BINDS <hematopoiesis regulator macrophage inflammatory protein- 1alpha> <CC chemokine receptor 1> BINDS <related CC chemokine> Considerable interest in information extraction has concentrated on identifying named entities in text pertaining to current events (for example, Wacholder et al. 1997, Voorhees and Harman 1998, and MUC-7); however, several recent efforts have been directed at biomolecular data (Blaschke et al. 1999, Craven and Kumlien 1999, and Rindflesch et al. 2000, for example). The overall goal is to transform the information  188  encoded in text into a more readily accessible tbrmat, typically a template with slots named for the participants in the scenario of interest. The template for molecular binding can be thought of as a simple predication with predicate "bind" and two arguments which participate (symmetrically) in the relationship: BINDS(<X>, <Y>). Various strategies, both linguistic and statistical, have been used in information extraction efforts. We introduce a Prolog program called ARBITER (Assess and Retrieve Binding Terminology) that takes advantage of an existing domain knowledge source and relies on syntactic cues provided by a partial parser in order to identify and extract binding relations from text. We discuss the syntactic processing used and then report on a formal evaluation of ARBITER against a test collection of 116 MEDLINE citations in which the binding relations were marked by hand. Finally, we provide a brief overview of the results of applying ARBITER to the 500,000 MEDLINE citations discussing molecular binding affinity. 
Compound noun analysis is one of the crucial problems in Korean language processing because a series of nouns in Korean may appear without white space in real texts, which makes it difficult to identify the morphological constituents. This paper presents an effective method of Korean compound noun segmentation based on lexical data extracted from corpus. The segmentation is done by two steps: First, it is based on manually constructed built-in dictionary for segmentation whose data were extracted from 30 million word corpus. Second, a segmentation algorithm using statistical data is proposed, where simple nouns and their frequencies are also extracted from corpus. The analysis is executed based on CYK tabular parsing and min-max operation. By experiments, its accuracy is about 97.29%, which turns out to be very effective. 
Sophisticated grammar formalisms, such as LFG, allow concisely capturing complex linguistic phenomena. The powerful operators provided by such formalisms can however introduce spurious ambiguity, making parsing inefficient. A simple form of corpus-based grammar pruning is evaluated experimentally on two wide-coverage grammars, one Engiish and one French. Speedups of up to a factor 6 were obtained, at a cost in grammatical coverage of about 13%. A two-stage architecture allows achieving significant speedups without introducing additional parse failures. 
We present an algorithm and a tool for automatically revising grammars for natural language processing (NLP) systems to disallow specifically identified sentences or sets of sentences. We also outline an approach for automatically revising attribute value grammars using counter-examples. Developing grammars for NLP systems that are both general enough to accept most sentences about a domain, but constrained enough to disallow other sentences is very tedious. Our approach of revising grammars automatically using counter-examples greatly simplifies the development and revision of tightly constrained grammars. We have successfully used our tool to constrain over-generalizing grammars of speech understanding systems and obtained higher recognition accuracy. 
This paper describes an approach to providing lexical information for natural language processing in unrestricted domains. A system of approximately 1200 morphological rules is used to extend a core lexicon of 39,000 words to provide lexical coverage that exceeds that of a lexicon of 80,000 words or 150,000 word forms. The morphological system is described, and lexical coverage is evaluated for random words chosen from a previously unanalyzed corpus. 
Trigrams'n'Tags (TnT) is an efficient statistical part-of-speech tagger. Contrary to claims found elsewhere in the literature, we argue that a tagger based on Markov models performs at least as well as other current approaches, including the Maximum Entropy framework. A recent comparison has even shown that TnT performs significantly better for the tested corpora. We describe the basic model of TnT, the techniques used for smoothing and for handling unknown words. Furthermore, we present evaluations on two corpora. 
This paper proposes a framework of language independent morphological analysis and mainly concentrate on tokenization, the first process of morphological analysis. Although tokenization is usually not regarded as a difficult task in most segmented languages such as English, there are a number of problems in achieving precise treatment of lexical entries. We first introduce the concept of morpho-fragments, which are intermediate units between characters and lexical entries. We describe our approach to resolve problems arising in tokenization so as to attain a language independent morphological analyzer. 
We present a divide-and-conquer strategy based on finite state technology for shallow parsing of realworld German texts. In a first phase only the topological structure of a sentence (i.e., verb groups, subclauses) are determined. In a second phase the phrasal grammars are applied to the contents of the different fields of the main and sub-clauses. Shallow parsing is supported by suitably configured preprocessing, including: morphological and on-line compound analysis, efficient POS-filtering, and named entity recognition. The whole approach proved to be very useful for processing of free word order languages like German. Especially for the divide-andconquer parsing strategy we obtained an f-measure of 87.14% on unseen data. 
This paper presents a hybrid approach for named entity (NE) tagging which combines Maximum Entropy Model (MaxEnt), Hidden Markov Model (HMM) and handcrafted grammatical rules. Each has innate strengths and weaknesses; the combination results in a very high precision tagger. MaxEnt includes external gazetteers in the system. Sub-category generation is also discussed. Introduction Named entity (NE) tagging is a task in which location names, person names, organization names, monetary amounts, time and percentage expressions are recognized and classified in unformatted text documents. This task provides important semantic information, and is a critical first step in any information extraction system. Intense research has been focused on improving NE tagging accuracy using several different techniques. These include rule-based systems [Krupka 1998], Hidden Markov Models (HMM) [Bikel et al. 1997] and Maximum Entropy Models (MaxEnt) [Borthwick 1998]. A system based on manual rules may provide the best performance; however these require painstaking intense skilled labor.. Furthermore, shifting domains involves significant effort and may result in performance degradation. The strength of HMM models lie in their capacity for modeling local contextual information. HMMs  have been widely used in continuous speech recognition, part-of-speech tagging, OCR, etc., and are generally regarded as the most successful statistical modelling paradigm in these domains. MaxEnt is a powerful tool to be used in situations where several ambiguous information sources need to be combined. Since statistical techniques such as HMM are only as good as the data they are trained on, they are required to use back-off models to compensate for unreliable statistics. I n contrast to empirical back-off models used in HMMs, MaxEnt provides a systematic method by which a statistical model consistent with all obtained knowledge can be trained. [Borthwick et al. 1998] discuss a technique for combining the output of several NE taggers in a black box fashion by using MaxEnt. They demonstrate the superior performance of this system; however, the system is computationally inefficient since many taggers need to be run. In this paper we propose a hybrid method for NE tagging which combines all the modelling techniques mentioned above. NE tagging is a complex task and high-performance systems are required in order to be practically usable. Furthermore, the task demonstrates characteristics that can be exploited by all three techniques. For example, time and monetary expressions are fairly predictable and hence processed most efficiently with handcrafted grammar rules. Name, location and organization entities are highly variable and thus lend themselves to statistical training algorithms such as HMMs. Finally, many conflicting pieces of information regarding the class of a tag are  * This work was supported in part by the SBIR grant F30602-98-C-0043 from Air Force Research Laboratory (AFRL)/IFED. 247  frequently present. This includes information from less than perfect gazetteers. For this, a MaxEnt approach works well in utilizing diverse sources of information in determining the final tag. The structure of our system is shown in Figure 1.  I  I  I to=~mI,~mm  I  I  I o , , = - . l. , I  I MJE3;,=G,,:;,IE  
This paper reports on work carried out to develop a spelling and grammar corrector for Danish, addressing in particular the issue of how a form of shallow parsing is combined with error detection and correction for the treatment of context-dependent spelling errors. The syntactic grammar for Danish used by the system has been developed with the aim of dealing with the most frequent error types found in a parallel corpus of unedited and proofread texts specifically collected by the project's end users. By focussing on certain grammatical constructions and certain error types, it has been possible to exploit the linguistic 'intelligence' provided by syntactic parsing and yet keep the system robust and efficient. The system described is thus superior to other existing spelling checkers for Danish in its ability to deal with contextdependent errors. 
This paper describes the results of some experiments using a new approach to information access that combines techniques from natural language processing and knowledge representation with a penaltybased technique for relevance estimation and passage retrieval. Unlike many attempts to combine natural language processing with information retrieval, these results show substantial benefit from using linguistic knowledge. 
In many knowledge intensive applications, it is necessary to have extensive domain-specific knowledge in addition to general-purpose knowledge bases. This paper presents a methodology for discovering domain-specific concepts and relationships in an attempt to extend WordNet. The method was tested on five seed concepts selected from the financial domain: interest rate, stock market, inflation, economic growth, and employment. 
A large-scale controlled vocabulary indexing system is described. The system currently covers almost 70,000 named entity topics, and applies to documents from thousands of news publications. Topic definitions are built through substantially automated knowledge engineering. 
Information Extraction (IE) systems are commonly based on pattern matching. Adapting an IE system to a new scenario entails the construction of a new pattern base---a timeconsuming and expensive process. We have implemented a system for finding patterns automatically from un-annotated text. Starting with a small initial set of seed patterns proposed by the user, the system applies an incremental discovery procedure to identify new patterns. We present experiments with evaluations which show that the resulting patterns exhibit high precision and recall. 0 Introduction The task of Information Extraction (I-E) is the selective extraction of meaning from free natural language text. I "Meaning" is understood here in terms of a fixed set of semantic objects--entities, relationships among entities, and events in which entities participate. The semantic objects belong to a small number of types, all having fixed regular structure, within a fixed and closely circumscribed subject domain. The extracted objects are then stored in a relational database. In this paper, we use the nomenclature accepted in current IE literature; the term subject domain denotes a class of textual documents to be processed, e.g., "business news," and scenario denotes the specific topic of interest within the domain, i.e., the set of facts to be extracted. One example of a scenario is "management succession," the topic of MUC-6 (the Sixth Message Understanding Conference); in this scenario the system seeks to identify events in which corporate managers left 1For general references on IE, cf., e.g., (Pazienza, 1997; muc, 1995; muc, 1993).  their posts or assumed new ones. We will consider this scenario in detail in a later section describing experiments. IE systems today are commonly based on pattern matching. The patterns are regular expressions, stored in a "pattern base" containing a general-purpose component and a substantial domain- and scenario-specific component. Portability and performance are two major problem areas which are recognized as impeding widespread use of IE. This paper presents a novel approach, which addresses both of these problems by automatically discovering good patterns for a new scenario. The viability of our approach is tested and evaluated with an actual IE system. In the next section we describe the problem in more detail in the context of our IE system; sections 2 and 3 describe our algorithm for pattern discovery; section 4 describes our experimental results, followed by comparison with prior work and discussion, in section 5. 
This paper describes experiments to establish the performance of a named entity recognition system which builds categorized lists of names from manually annotated training data. Names in text are then identified using only these lists. This approach does not perform as well as state-of-the-art named entity recognition systems. However, we then show that by using simple filtering techniques for improving the automatically acquired lists, substantial performance benefits can be achieved, with resulting Fmeasure scores of 87% on a standard test set. These results provide a baseline against which the contribution of more sophisticated supervised learning techniques for NE recognition should be measured. 
Information retrieval systems have typically concentrated on retrieving a set of documents which are relevant to a user's query. This paper describes a system that attempts to retrieve a much smaller section of text, namely, a direct answer to a user's question. The SMART IR system is used to extract a ranked set of passages that are relevant to the query. Entities are extracted from these passages as potential answers to the question, and ranked for plausibility according to how well their type matches the query, and according to their frequency and position in the passages. The system was evaluated at the TREC-8 question answering track: we give results and error analysis on these queries. 
We present a novel sentence reduction system for automatically removing extraneous phrases from sentences that are extracted from a document for summarization purpose. The system uses multiple sources of knowledge to decide which phrases in an extracted sentence can be removed, including syntactic knowledge, context information, and statistics computed from a corpus which consists of examples written by human professionals. Reduction can significantly improve the conciseness of automatic summaries. 
In this paper, we analyze the performance of name finding in the context of a variety of automatic speech recognition (ASR) systems and in the context of one optical character recognition (OCR) system. We explore the effects of word error rate from ASR and OCR, performance as a function of the amount of training data, and for speech, the effect of out-of-vocabulary errors and the loss of punctuation and mixed case I Introduction 
This paper explores the usefulness of a technique from software engineering, namely code instrumentation, for the development of large-scale natural language grammars. Information about the usage of grammar rules in test sentences is used to detect untested rules, redundant test sentences, and likely causes of overgeneration. Results show that less than half of a large-coverage grammar for German is actually tested by two large testsuites, and that i030% of testing time is redundant. The methodology applied can be seen as a re-use of grammar writing knowledge for testsuite compilation. 
This paper compares the efficiency of using a standard direct-manipulation graphical user interface (GUI) with that of using the QuickSet pen/voice multimodal interface for supporting a military task. In this task, a user places military units and control measures (e.g., various types of lines, obstacles, objectives) on a map. Four military personnel designed and entered their own simulation scenarios via both interfaces. Analyses revealed that the multimodal interface led to an average 3.5-fold speed improvement in the average entity creation time, including all error handling. The mean time to repair errors also was 4.3 times faster when interacting multimodally. Finally, all subjects reported a strong preference for multimodal interaction. These results indicate a substantial efficiency advantage for multimodal over GUI-based interaction during map-based tasks. Introduction Nearly two decades ago at ACL'80, Professor Ben Shneiderman challenged the field of natural language processing as follows: In constructing computer systems which mimic rather than serve people, the developer may miss opportunities for applying the unique and powerful features of a computer: extreme speed, capacity to repeat tedious operations accurately, virtually unlimited storage for data, and distinctive input/output devices. Although the slow rate of human speech makes menu selection impractical, high-speed computer displays make menu selection an appealing alternative. Joysticks, light pens or the "mouse"  are extremely rapid and accurate ways of selecting and moving graphic symbols or text on a display screen. Taking advantage of these and other computer-specific techniques will enable designers to create powerful tools without natural language commands. [20, p. 139] He also challenged us to go beyond mere claims, but to demonstrate the benefits of natural language processing technologies empirically. Since then, not only has there been a long period of unprecedented innovation in hardware, software architectures, speech processing, and natural language processing, but NLP research has also embraced empirical methods as one of its foundations. Still, we have yet to defend claims empirically that technologies for processing natural human communication are more efficient, effective, and/or preferred, than interfaces that are best viewed as "tools," especially interfaces involving a direct manipulation style of interaction. The present research attempts to take a small step in this direction. In fact, it has often been claimed that spoken language-based human-computer interaction will not only be more natural but also more efficient than keyboard-based interaction. Many of these claims derive from early modality comparison studies [1], which found a 2-3 fold speedup in task performance when people communicated with each other by telephone vs. by keyboard. Studies of the use of some of the initial commercial speech recognition systems have reported efficiency gains of approximately 20% - 40% on a variety  331  of interactive hands-busy tasks [10] compared with keyboard input. Although these results were promising, once the time needed for error correction was included, the speed advantage of speech often evaporated [18] ~. A recent study of speech-based dictation systems [9] reported that dictation resulted in a slower and more errorful method of text creation than typing. From such results, it is often concluded that the age of spoken human-computer interaction is not yet upon us. Most of these studies have compared speech with typing, However, in order to affect mainstream computing, spoken interaction would at a minimum need to be found to be superior to graphical user interfaces (GUIs) for a variety of tasks. In an early study of one component of GUIs, Rudnicky [18] compared spoken interaction with use of a scroll bar, finding that error correction wiped out the speed advantages of speech, but users still preferred to speak. Pausch and Leatherby [17] examined the use of simple speaker-dependent discrete speech commands with a graphical editor, as compared with the standard menu-based interface. With a 19-word vocabulary, subjects were found to create drawings 21% faster using speech and mouse than with the menu-based system. They conjectured that reduction in mouse-movement was the source of the advantage. In general, more research comparing speech and spokenlanguage-based interfaces with graphical user interfaces still is needed. We hypothesize that one reason for the equivocal nature of these results is that speech is often being asked to perform an unnatural act the interface design requires people to speak when other modalities of communication would be more appropriate. In the past, strengths and weaknesses of various communication modalities have been described [2, 6, 13], and a strategy of developing multimodal user interfaces has been developed using the strengths of one mode to overcome weaknesses in another, Interface simulation studies I See also [6, 10] for a survey of results.  Figure 1. The ExInit GUI  comparing multimodal (speech/pen) interaction with speech-only have found a 35% reduction in user errors, a 30% reduction in spoken dysfluencies (which lead to recognition errors), a 10% increase in speed, and a 100% user preference for multimodal interaction over speech-only in a map-based task [14]. These results suggest that multimodal interaction may well offer advantages over GUI's for map-based tasks, and may also offer advantages for supporting error correction during dictation [16, 19].  In order to investigate these issues, we undertook a study comparing a multimodal and a graphical user interface that were built for the same map-based task~.  
We present an ongoing project for the creation of a single central terminology database for all the institutions, agencies and other bodies of the European Union. The background, objectives, benefits and main features of the system are briefly introduced, followed by a presentation of the solutions proposed to resolve the complex validation issues addressed by a project which involves interaction between many institutions with different internal validation processes as well as access from the general public. Background to the IATE Project The aim of the IATE project is to create a single central terminology database for all the institutions, agencies and other bodies of the European Union which will provide translators and terminologists with a centralised source of EU terminology data. The project, which is funded by IDA (the EU programme supporting the Interchange of Data between Administrations), started in January 2000 and the first phase - the detailed design and specification of the system - was completed in July 2000. The implementation of the prototype system is currently in progress and is scheduled to be installed for user testing by the end of November 2000. Three expert groups comprising representatives of all institutions as well as some external experts from the member states worked on the technical specifications relating to the data structure, validation process, and workflow integration requirements of the new interinstitutional terminology database, the results of which were summarised in the project's first major deliverable, the System Analysis and Design document. The work of these groups having been completed, an implementation support group was set up to provide guidance and feedback to the contractors throughout the development phase of the project. This group will be particularly concerned with user interface design, evaluation of the results of the data uploading process and further issues relating to the content of the database.  Data will be imported from existing institutional databases, principally Eurodicautom (European Commission), Euterpe (European Parliament), and TIS (Council of the European Union), as well as from some smaller databases developed at a number of other institutions and agencies including the Court of Justice, the Court of Auditors, the European Social Committee/Committee of the Regions, the European Investment Bank and the Translation Centre of the Organs of the European Union. This data uploading process will necessitate the development of tools to detect duplicate entries and to assist in the process of merging overlapping entries. Once the final import of data into the new database has been made, the other databases will be discontinued and the efforts of the institutions will be combined to maintain and develop the single, centralised IATE database. The validation process has been designed to be flexible enough to handle the widely differing processes existing at present, as well as the proposed inter-institutional validation aspect anticipated in the future. The database will be integrated into the principal components of the translation workflow of the various institutions, particularly word processing (MS Word). In September 2000, extended funding was granted by IDA to support further consolidation of the data and integration of the system with additional tools used by the institutions, including especially Trados Translator's Work Bench and Word Perfect, as well as the Glossary Production System (currently used at the Council), the Thesaurus (currently used at the Court of Justice), the One-Stop Shop (currently under development at the Commission's Translation Service) and the Trademark Translation Workflow System (integrating Translation Memory, MT and workflow, currently being developed by the Translation Centre). Further funding may be requested at a later stage to integrate LATE with even more systems, such as the Commission's Systran MT system and Euramis, the translation memory and document retrieval system. The next steps in the project are to implement the prototype (by November/December 2000) and test it in two pilot phases with increasing numbers of users at the various institutions. The system is scheduled to be completed by July 2001. Validation and Quality Control Issues The objectives of the validation part of the terminology database design and development have been to set up formal acceptance rules that during interactive input control automatically whether an entry shall be accepted or refused, to design content-related access schemes in connection with the definition of access rights for validation staff, and to work out administrative procedures to ensure that participating institutions or bodies can cooperate in the validation process. The original proposal in the Call for Tender was for a two-stage validation workflow. The first stage would be an internal review whereby new data would be first routed to other members of the same organisation for checking before being distributed for central validation according to domain and language combination to a pool of domain experts selected from the staff of all the participating organisation and possibly also other organisations. However, it became clear in the course of the analysis phase that a number of participating organisations wished to maintain independent control of their own data and were not currently in a position to handle validation by external experts. It therefore proved necessary to design a flexible system capable of handling both types of validation which would permit these institutions initially to continue to use their current internal validation process whilst at the  same time providing the means allowing them to move gradually towards a standardised, interinstitutional process in due course. Validation Workflow Cycles In order to cater for the differing validation workflows that exist in the different institutions participating in the project, it has been necessary to design a flexible and dynamic workflow model which can easily be adapted to the particular (and changing) processes of each organisation, whilst at the same time providing the structures necessary for gradual interinstitutional cooperation. Institutions must define the point at which they wish to release their data to public view and define the number, type and sequence of internal validation stages they require. Also, it is necessary to define the different validation cycle for different types of users (e.g. translators, terminologists, language/domain experts, system administrators, etc.). It is felt that this approach offers a clearer and easier gradual integration of the validation cycles of the participating institutions and agencies than an alternative approach which was considered (according to which each institution maintains one validation cycle consisting of a fixed sequence of stages and which users join at the stage specified for their role). Such a process requires the specification of the validation status of each stage in the cycle, i.e. the visibility of the term, how 'fixed' the term is, the user role required to perform this stage, and whether specific language/domain knowledge and/or institution membership is necessary for the stage. Users in each institution are grouped into different roles which are defined and maintained by the institution's administrator. Each role will be associated with different access rights (e.g. read, insert, update, delete, merge, export, import, change validation status, user and role maintenance tool, insert/delete marks, etc.). Information on individual users (e.g. name, password, source language(s), target language(s), domain expertise, role, institution, division, contact coordinates, start/end date of role, etc.) will also be maintained, together with a profile of the user's preferences (e.g. search language, search result sorting criteria, list of display languages for search results, etc.). Data Entry Validation The process of validation of an entry starts from data entry and continues through to final validation. The system has been designed to support users during data entry with easily accessible displays of the rules that are applicable to a given entry. Where possible automatic checks to verify data entry are carried out which have been derived by comparing sets of rules used in the different institutions. A complete audit trail showing all changes to any entry in the database is available off-line to the system administrator. Other features of the entry such as context information to provide either an example of the occurrence of that term or the authority/reliability of that term or confidentiality (rarely of the term as a whole, more usually of specific fields such as source and references) are provided for in the database and checked in the validation process. The ability to enter/modify reliability codes will only be assigned to validators. The range of reliability values that will be available to the system at language level and at term level will be as follows:  Code 0 
Faced with large and steadily increasing work volumes, the Patent Cooperation Treaty Translation Section at the World Intellectual Property Organization, Geneva, is looking for ways to improve the efficiency of its translation process. A terminology problem has been identified, and attention has turned to automatic bilingual terminology extraction as a possible means of solving that problem. A project has been defined and evaluation tests implemented with the aims of automatically capturing bilingual terminology from existing technical texts and their translations, validating the candidate term pairs generated, defining an appropriate database structure and generating terminological records in an automatic or semi-automatic manner. Benefits of this approach are becoming apparent and, as work progresses, the potential for extending the scope of the project to other related applications offers interesting prospects for the future. 1. Introduction At the World Intellectual Property Organization (WIPO) in Geneva, Switzerland, a specialised agency of the United Nations, the Office of the Patent Cooperation Treaty (OPCT) is responsible for processing international patent applications received from all Contracting States. Within the OPCT, translation is an important aspect of operations, helping to ensure the widest possible and easiest access to information contained in documents describing and evaluating new inventions. Given the rapid increase in the number of patent applications filed under the Patent Cooperation Treaty (PCT), and the ensuing increase in the translation work load, it is important to ensure a continued improvement in the efficiency and cost effectiveness of the translation process. This paper will look at development work undertaken in PCT Translation with a view to facilitating the translation operation. We shall first briefly examine operations in PCT Translation and identify the problems which have to be faced. Subsequently, the solution adopted for terminology development and the associated terminology extraction and database creation project will be examined in detail. Problems encountered in the software evaluation and term validation phases of the project will be indicated, methodologies discussed, and a brief outline of future activities given. A short section has also been included to give an insight into the underlying principles of automatic term extraction.  2. Translation in the Office of the PCT 2.1 Translation Work In the course of the PCT international patent application procedure, the Office of the PCT is required to translate two main types of documents: a) Abstracts of international patent applications. These are half-page summaries of descriptions of inventions and are written in dense technical language, often with poor syntax as in the example. Vehicle window and method of making the same A vehicle window comprising a relatively thin sheet of clear plastic material having opposed surfaces, an electrically operable defrosting grid adhered to one surface of the relatively thin sheet, and a relatively thick substrate layer of clear plastic material having opposed surfaces curved into a vehicle window configuration. The relatively thick substrate layer is adhered to the one surface of the relatively thin sheet and the electrically operable defrosting grid adhered thereto while in contact therewith in a molten state under heat and pressure within a cavity defined by two generally parallel curved die surfaces of cooperating injection moulding dies so that upon solidification the surfaces of the relatively thin sheet are retained in a curved configuration in generally parallel coextensive relation to corresponding curved surfaces of the relatively thick substrate layer, and a method of making the window wherein the electrically operable defrosting is formed by silk screening onto one surface of the relatively thin sheet while in a substantially planar condition a curable electrically conductive ink in the form of a defrosting grid and then curing the curable electrically conductive ink on the one surface of said relatively thin sheet so that the defrosting grid is stably adhered thereto. (PCT Gazette 20/1996, WIPO, Geneva) b) International Preliminary Examination Reports These are technical reports drawn up by engineers qualified in the field of the invention concerned. They give an opinion on the technical content of the invention with respect to novelty and inventiveness. The majority of texts received for translation are written in English, French, German or Japanese, with some in Russian and Spanish, and translated from these languages into English and/or French. All translations are done internally at WIPO, with the exception of Japanese abstracts and all Chinese texts. 2.2 Development Work Our activities are not limited to translation. Development work has been carried out on an ongoing basis in PCT Translation for many years, but it is only recently that the need for this work has been officially recognised and a mandate given to develop translation tools and computer assisted translation systems. Various lines of approach are being followed: □ Internet search facilities □ Terminology database □ Electronic dictionaries □ Translation memory □ Voice recognition systems. The most positive results to date have been achieved with the Internet search facilities, which have been implemented in the form of our TermLinks targeted search interface (Gomez, J. ITI Bulletin, August 2000). Terminology database creation has required extensive groundwork, but is  advancing well. We will see below how we now hope to move ahead rapidly. Electronic dictionaries on CD-ROM or loaded from CD-ROM onto our local area network have been introduced, and others are available on-line over the Internet. Translation memory is a slow starter since we do not believe our texts to be adequately repetitive for it to be effective, although as we shall see, the use of a workbench environment will necessitate its development. Little has been done with voice recognition, although progress may be more apparent once a collection of specialised technical terms has been created. 
The term used to refer to the development is "workflow", within the context of the translation and localisation industry. This paper proposes that workflow is an element of every translation production process, and that automating the workflow is not the all-embracing solution in itself. There are multiple workflows within any translation production process, and automation of the workflows must be preceded by an articulation and definition of the translation production processes currently employed and those which are emerging. Above all, this paper proposes that until we recognise and support those professionals involved in the translation production process who are not translating, any attempt to introduce workflow technology will ultimately contribute to the problems rather than resolving the issues. Once we achieve the definition of the translation production process, identify the workflows and recognise that translation production requires more than the art and science of translation, then we can proceed to manage the workflows in a fashion that will give our industry scalability and visibility, reducing costs whilst retaining quality. I'm going to introduce you to the workflow development by first giving you some background as to why we have such a development, followed by an articulation of the problems inherent in the translation and localisation industries. I'll outline some of the solutions which are currently proposed to resolve the problems and assist us to grow to service the requirements of today and the anticipated requirements of tomorrow. Finally, I will outline how TRADOS will support the anticipated explosive growth in our industry and assist it to expand into the globalisation arena with technology which is truly scalable.  The Background I had a title for this paper and for the presentation of our new generation of technology, which was "Localisation - It's All About The Exception". This section of the paper brings us rapidly through the background of the industry and explains why exception and change management are inherent in what we do. TRADOS Background TRADOS has been developing and marketing translation technology for over fifteen years now. Our initial product offerings - and those which are at the core of our business still today were terminology management and translation memory database systems. The interfaces that made TRADOS as popular as it is today were chosen because they were the industry standard tools for translation (Microsoft Word and WordPerfect). TRADOS provided an interface between them and the translation memory database. We continued with that same theme by providing technology to extract text from the industry standard applications used for developing and building complex reference material (FrameMaker and Interleaf) and providing tools to assist with niche formats, such as QuarkXPress and PageMaker. We have developed tools to create a translation memory from previously translated material, and provided project management and database administration utilities within the translation memory system. Translation production has never been a simple process. Any process involving multiple roles, distributed teams, complex content and formats and constant change management will present significant challenges. Management and automation of the translation production process is one which has not yet been fully addressed by any technology in an independent fashion. We have all been busy trying to reduce the total cost of translation by reducing the amount of repetitive work involved in translation and to increase the speed at which we can produce. At TRADOS we've tried to achieve this aim by getting the translator to accept the new technology as a useful tool to assist them in their creative process. We've also focussed very strongly on the introduction and acceptance of translation memory technology as being essential to the translation and localisation industry. We are continuing to incrementally improve our translation memory system, offering it as a common technology solution in a forthcoming release. Our translation interfaces have grown to incorporate our HTML/XML/SGML and our Microsoft PowerPoint interfaces. We are about to introduce a common tag format (TRADOStag, an XML based format) for the interfaces into professional publishing packages along with utilities to make the process more effective. As pioneers of translation technology, we carry a weighty responsibility - delivering on our promise to support the professional translator with best of breed technology. Translation Memory Technology Being a pioneer is a most challenging role - identifying a need in a marketplace which has not yet been fully articulated and providing a technology solution which anticipates the requirements which have not yet been specified is a supreme challenge. Not least because it involves an element of quite some risk. TRADOS tools and technology are accepted by now as the industry standard for the professional translator. Translation memory technology has delivered significant cost, quality and time to market benefits to translators and translation managers. As the global market expands, and the projections for the number of words and type of material to be translated reach vast proportions, the requirement for the translation production process to be merged with other production and business systems within all organisations is becoming imperative. To service the needs of the "Global Marketplace" the industry must expand substantially. How will it do so and still retain a semblance of quality? How do we connect our processes with the rest of the processes used within our organisation whilst still retaining the versatility that we are required to show? Most fundamental is the problem we have of doing more in a shorter timeline at a lower cost.  Translation memory technology has contributed to the scalability of the industry allowing us to grow and embrace new paradigms as rapidly as they have been presented to us. The TRADOS workflow development consolidates our technology and provides a common platform to support the complex process of producing translated material within a distributed environment with seemingly impossible deadlines. Translation Production Background Technology has moved so fast over the past 10 to 15 years it is almost difficult now to envision how translations were ever produced without the personal computer. Add the Internet into the mix of the multiple software tools now available for developing and publishing content, think about how the content to be translated has changed during the recent past from being primarily reference manuals for products with a world-wide distribution to the marketplace it is today, and gasp at the perceived diversity of process which would be necessary to deliver. In the early days of the translation memory technology systems, a translation production process was almost simple. We had the content - it was originated by the documentation group, the legal group, the marketing group. It was often originated in a word processing application such as Microsoft Word, but if it was a technical reference manual with complex illustrations a desktop publishing product such as Interleaf was used. The translation production process in the early days involved origination of the material from which text for translation was extracted (usually manual). The text was translated, the documents re-assembled, and then published. Translation was a costly business, and really only worthwhile when large volumes of text were to be translated. Early translation technology pioneers recognised that within these large volumes of text much of the content was repetitive, translation memory databases were developed to address this issue. 
Anna Sågvall Hein Department of Linguistics Uppsala University Uppsala Sweden anna@ling.uu.se  Introduction In this paper we present Scania Checker, a web-based language checker for Swedish automotive service literature. It has been developed by the Department of Linguistics at Uppsala University in co-operation with the After Sales department Technical Information at Scania, Sodertalje, Sweden. In order to ensure translation consistency and quality, technical writers at Technical Information use it to check grammar and vocabulary in the source document before it is being translated. We will show what kind of responses Scania Checker offers the writer, and demonstrate some examples of the rules behind the responses. We will also show, what the writer is expected to do in order to integrate the use of Scania Checker into a documentation and translation work-flow. The Scania Checker consists of a word checker and a grammar checker. A fundamental part of the checker is the Scania Checker Lexical Database. The contents and characteristics of the database will be presented, and how it is used by the author, as well as by the database administrator. The error coverage of the grammar checker will be out-lined and error type frequencies resulting from the training of the checker will be given. Some technical aspects of the checker will also be presented including the basic operation of the grammar checker. Finally, some possibilities of future developments will be sketched. Background Most information products concerning repair and maintenance of Scania trucks and buses are produced at the After Sales department Technical Information in Sodertalje Sweden. Today the media ranges from fiche, printed matters, software to Internet publication. All information is produced in Swedish by approx. 20 in-house technical writers and from time to time by consultants. Information products are then translated into English and from English into the remaining eight target languages.  In mid 1990, it became obvious, that it was necessary to provide technical writers with source language support in order to ensure translation consistency and quality. As a result, a prototype of a Swedish language checker was developed and installed at Scania for evaluation in 1997 (Sågvall Hein et al. 1997). The basis for the controlled vocabulary of the checker was a corpus of some 200,000 words. It was found, however, that the vocabulary was too small, and work on a major extension of it was initiated. The basis for the extension was a corpus of some 1,8 million words (Tiedemann 1998.). As a result, a new version of the checker, Scania Checker, was developed. It was introduced at Scania in August 2000 as a tool for Swedish grammar and vocabulary checking. In connection with the extension of the vocabulary, a major development of the language checking technology was carried out, and the checker was made available on the web. Experiences and achievements made in the SCARRIE1 project played an important role in the development of Scania Checker. Documentation and Translation at Scania The documentation process at After Sales Technical Information is through the Product Development process closely interlinked with the activities of Design Departments and Repair Method Department. The PD process starts by stating a market demand of a product or vehicle function. Once pre-studies are finalised, a development phase starts, which, via several decision points, where the future of the project continuously is evaluated, leads to an implementation stage. At the implementation stage, all information necessary to produce After Sales documentation should be available. After Sales departments are, however, involved already early in the development phase to provide designers with the maintainability perspective of a new component. Technical documentation at Scania at large is written in Swedish as well as English. Internally, Swedish is often used, but English is used to communicate within Scania worldwide. Furthermore, suppliers' documentation is often in English, sometimes even translated from German to English. However, technical writers at After Sales Technical Information write the repair and maintenance documentation in Swedish, which has been considered the most efficient way in the long run, since Swedish is their mother tongue. As shown in Figure 1, the workflow for the writing and translation process at After Sales department Technical Information is the following. After finalising a text, the writer runs it through Scania Checker, which marks grammatical and vocabulary mistakes, and words not found in the lexical database, i.e. candidates for new words. The writer then corrects the source document according to the suggestions made by the checker. New words are sent to a log file to be attended to by the database administrator. 
Manchester M60 1QD Harold.Somers@umist.ac.uk Summary This paper describes the use of the "cloze procedure" to make a comparative evaluation of some currently available MT systems. This technique, which involves masking some words in a text, and then asking subjects to guess the missing words, was used in the 1960s and 1970s as an evaluation method, but no more recent use has been reported, even though the methodology is simple to implement and provides an objective result. We report here two experiments in which we tested three MT systems against a human translation, with texts from three different genres, to see whether the procedure can be used to rank MT systems against each other. The paper discusses some details of the procedure which provide important variables to the test, notably what percentage of words are masked, and whether the scoring procedure should be right-wrong, or should differentiate between different degrees of wrong answer (crediting close synonyms and other plausible near-misses). We discuss other aspects of the procedure which may affect the test's usability. Especially of interest is the fact that there seems to be a lower quality threshold below which the procedure is less discriminatory: the translation is so bad that the subjects cannot make reasonable guesses at all. All trademarks are hereby acknowledged. Introduction Amongst the early attempts to evaluate Machine Translation (MT) output was the often-cited work of Sinaiko & Clare (1972, 1973) who used Wilson Taylor's (1953) "cloze procedure" to evaluate the readability of English-Vietnamese MT output. Although the technique is very simple, the technique has not reportedly been used since then, even though MT quality in general has improved hugely. In this paper we report on experiments using the technique to make a comparative evaluation of some currently available MT systems. It has been said that "machine translation evaluation is a better founded subject than machine translation" (Wilks, 1994, p.l). Much of what has been written in recent years discusses general issues in MT evaluation design (for example, Arnold et al. 1993; Vasconcellos, 1994; Sparck Jones & Galliers, 1995; White, forthcoming). For details of actual evaluations we often have to delve deeper. MT evaluation techniques are often classified along various parameters including who the evaluation is for (researcher, developer, purchaser, end-user), whether it makes use of information about how the system works (black-box vs. glass-box) and, above all, what aspect of the functionality of a system is evaluated (in the case of MT, cost, speed, usability, portability, readability, fidelity, and so on). The cloze procedure The cloze procedure was originally developed by Taylor (1953) as a measure of readability (and hence, comprehensibility) of human-written text, along the lines of  the well-known Flesch scale, and others similar, which grade texts to indicate their suitability for readers of different age ranges. The cloze procedure involves taking the text to be evaluated, masking some words in the text (every 5th word, say), and then asking subjects to guess the missing words. The name "cloze" comes from "closure", this being the term that ... gestalt psychology applies to the human tendency to complete a familiar but not-quitefinished pattern—to "see" a broken circle as a whole one, for example, by mentally closing up the gaps. (Taylor, 1953, p. 415) The readability of the text is directly computed on the basis of the ability of the subjects to guess the missing words correctly. It compares favourably with other measures of readability used in MT evaluation such as rating scales (subjective) or error counts (difficult to implement), and with other more general measures of readability which involve computations based on average word- and sentence-length, such as the Flesch scale (Flesch, 1948) (familiar through its use in Microsoft Word), the Dale-Chall formula (Dale & Chall, 1948), Gunning's (1952) FOG Index, and so on. We will not discuss here what exactly "readability" is in general, nor whether the cloze procedure accurately measures it. There is ample evidence that whatever it is measuring, the cloze test does so consistently. Results correlate highly with readability indices such as those mentioned in the previous paragraph. The cloze procedure has also been used as a tool in testing foreign-language learners (see Oiler, 1979; Hinofotis, 1987; Brown et al., 1999). Brown et al. stress the difference between using the cloze procedure for "norm-referenced purposes" such as admissions or placement testing and "criterion-referenced purposes" such as diagnostic, progress, or achievement testing, with the implication that the results are less delicate and therefore more reliable in the former case. For our application (MT evaluation) the term "readability" is used with a quite specific meaning. In general usage, "readability" is supposed to correlate with ease of reading for a reader of a given age, whereas in MT evaluation, the term is used as a quasi-synonym for "intelligibility", which in turn is just one aspect of the generally vague notion of translation "quality". 
In this paper we discuss the motivations for the development of Amikai's web-based translated chat room application. Like other successful machine translation (MT) systems, Amikai's translated chat attempts to reconcile overly-optimistic user expectations with the limited capabilities of current MT technology by adjusting user expectations and limiting the scope and domain of the translation task. We explain why chat is a natural application for MT technology and briefly describe aspects of the user experience within the Amikai system. Introduction: Making Useful Machine Translation Products The history of natural language processing in general, and machine translation research in particular, has been marked by cycles of heightened interest and expectations within the general public, followed by periods of disillusionment when people's grand dreams for the technology are not realized. One of the most dramatic of these was probably the ALPAC report in 1966[1], which effectively killed MT research in the US for years to come. It is undoubtedly this process of repeated unfulfilled expectations which has caused many people - linguists, AI researchers, and laypersons alike - to have a jaundiced view of machine translation's potential usability in everyday life. However, MT does in reality have some successful and commercially sustainable applications, most notably as translation helpers for human translators, such as the TRADOS system [2] or IBM's TranslationManager [4], and in the context of highly constrained subject matter and language usage, such as the Météo system for weather reports [3] or NTT's ALTFLASH business wire translator [7]. These types of applications share a common trait: they represent tasks where users' expectations have been modified to match the specific strengths and limitations of the MT technology, and the MT systems themselves have been limited in scope, specifically tailored to a narrow set of user needs. The mismatch between exaggerated expectations and limited technology is resolved, and useful systems result. Amikai Inc. has built upon this idea of focusing the technology and adjusting user expectations to produce a multilingual, translated chat application. The system  provides a translation back-end to a classic internet chat room, allowing speakers of different languages to converse freely in real time over the web.1 Through initial user tests, the system has proven highly usable, enabling satisfying, productive interactions between people around the globe, in spite of often inexact or completely incorrect translations. The success of the translated chat application can be attributed to three factors, all of which involve bridging the gap between user expectations and the state of current MT technology. First, Amikai's system concentrates on chat, a forum in which content is cheaply produced and has a very short shelf life. In this context, exact and complete translations are not required; slight mistranslations or awkward phrasing are easily excused in the course of the dialogue. As long as meaning sufficient to maintain the flow of conversation is communicated, the MT system has successfully done its job. In addition, the Amikai MT system is tuned for the specific domain of chat communication, taking advantage of the large amount of repetition and the highly stereotyped interactions of chat language to improve the usefulness of translation in the chat rooms. Second, the system trains users to understand better the strengths and limitations of the MT engines. The training mechanisms include a tutorial on translation-friendly language, as well as an interactive tutorial daemon (currently in development) which critiques a user's input. In addition, the chat interface to the MT engines introduces users to machine translation in a playful, hands-on environment where experimentation and exploration are encouraged. By enabling users to gain experience rapidly with an MT system, Amikai's translated chat quickly gives users a more concrete sense of the usefulness of the machine translation technology. Finally, Amikai's translated chat improves the level of translation through user-touser feedback in the form of a "Huh?" button, which a user can click when he or she doesn't understand another user's translated output. This immediately lets a user know when a translation was not of sufficient quality, prompting the user to try to find a better phrasing which will be more easily translated by the engine. This feedback loop is especially important for monolingual users who otherwise would have no way to measure the success of the translations of their own statements. In this way, the MT technology is made less mysterious and more personal, and users are better equipped to find the best ways to use the technology for their own communication needs. 
This paper surveys three research directions in parsing. First, we look at methods for both automatically generating a set of diverse parsers and combining the outputs of different parsers into a single parse. Next, we will discuss a parsing method known as transformation-based parsing. This method, though less accurate than the best current corpus-derived parsers, is able to parse quite accurately while learning only a small set of easily understood rules, as opposed to the many-megabyte parameter files learned by other techniques. Finally, we review a recent study exploring how people and machines compare at the task of creating a program to automatically annotate noun phrases.
If chart parsing is taken to include the process of reading out solutions one by one, then it has exponential complexity. The stratagem of separating read-out from chart construction can also be applied to other kinds of parser, in particular, to left-comer parsers that use early composition. When a limit is placed on the size of the stack in such a parser, it becomes context-free equivalent. However, it is not practical to profit directly from this observation because of the large state sets that are involved in otherwise ordinary situations. It may be possible to overcome these problems by means of a guide constructed from a weakened version of the initial grammar.
This paper presents a robust parsing system for unrestricted Basque texts. It analyzes a sentence in two stages: a unification-based parser builds basic syntactic units such as NPs, PPs, and sentential complements, while a finite-state parser performs syntactic disambiguation and filtering of the results. The system has been applied to the acquisition of verbal subcategorization information, obtaining 66{\%} recall and 87{\%} precision in the determination of verb subcategorization instances. This information will be later incorporated to the parser, in order to improve its performance.
We develop a set of new tabular parsing algorithms for Linear Indexed Grammars, including bottom-up algorithms and Earley-like algorithms with and without the valid prefix property, creating a continuum in which one algorithm can in turn be derived from another. The output of these algorithms is a shared forest in the form of a context-free grammar that encodes all possible derivations for a given input string.
Different NLP applications have different efficiency constraints (i.e. quality of the results and throughput) that reflect on each core linguistic component. Syntactic processors are basic modules in some NLP application. A customization that permits the performance control of these components enables their reuse in different application scenarios. Throughput has been commonly improved using partial syntactic processors. On the other hand, specialized lexicons are generally employed to improve the quality of the syntactic material produced by specific parsing (sub)process (e.g. verb argument detection or PP attachment disambiguation) . Building upon the idea of grammar stratification, in this paper a method to push modularity and lexical sensitivity, in parsing, in view of customizable syntactic analysers is presented. A framework for modular parser design is proposed and its main properties are discussed. Parsers (i.e. different parsing module chains) are then presented and their performances are analyzed in an application-driven scenarios.
In this paper we present Range Concatenation Grammars, a syntactic formalism which possesses many attractive features among which we underline here, power and closure properties. For example, Range Concatenation Grammars are more powerful than Linear Context-Free Rewriting Systems though this power is not reached to the detriment of efficiency since its sentences can always be parsed in polynomial time. Range Concatenation Languages are closed both under intersection and complementation and these closure properties may allow to consider novel ways to describe some linguistic processings. We also present a parsing algorithm which is the basis of our current prototype implementation.
The accuracy of statistical parsing models can be improved with the use of lexical information. Statistical parsing using Lexicalized tree adjoining grammar (LTAG), a kind of lexicalized grammar, has remained relatively unexplored. We believe that is largely in part due to the absence of large corpora accurately bracketed in terms of a perspicuous yet broad coverage LTAG. Our work attempts to alleviate this difficulty. We extract different LTAGs from the Penn Treebank. We show that certain strategies yield an improved extracted LTAG in terms of compactness, broad coverage, and supertagging accuracy. Furthermore, we perform a preliminary investigation in smoothing these grammars by means of an external linguistic resource, namely, the tree families of an XTAG grammar, a hand built grammar of English.
This article describes the architecture of the Survey Parser and discusses two major components related to the analogy-based parsing of unrestricted English. Firstly, it discusses the automatic generation of a large declarative formal grammar from a corpus that has been syntactically analysed. Secondly, it describes analogy-based parsing that employs both the automatically learned rules and the database of cases to determine the syntactic structure of the input string. Statistics are presented to characterise the performance of the parsing system.
A transformation-based approach to robust parsing is presented, which achieves a strictly monotonic improvement of its current best hypothesis by repeatedly applying local repair steps to a complex multi-level representation. The transformation process is guided by scores derived from weighted constraints. Besides being interruptible, the procedure exhibits a performance profile typical for anytime procedures and holds great promise for the implementation of time-adaptive behaviour.
This paper describes the key features of SOUP, a stochastic, chart-based, top-down parser, especially engineered for real-time analysis of spoken language with very large, multi-domain semantic grammars. SOUP achieves flexibility by encoding context-free grammars, specified for example in the Java Speech Grammar Format, as probabilistic recursive transition networks, and robustness by allowing skipping of input words at any position and producing ranked interpretations that may consist of multiple parse trees. Moreover, SOUP is very efficient, which allows for practically instantaneous backend response.
Minimalist Grammars are a rigorous formalization of the sort of grammars proposed in the linguistic framework of Chomsky{'}s Minimalist Program. One notable property of Minimalist Grammars is that they allow constituents to move during the derivation of a sentence, thus creating discontinuous constituents. In this paper we will present a bottom-up parsing method for Minimalist Grammars, prove its correctness, and discuss its complexity.
Previous work has demonstrated the viability of a particular neural network architecture, Simple Synchrony Networks, for syntactic parsing. Here we present additional results on the performance of this type of parser, including direct comparisons on the same dataset with a standard statistical parsing method, Probabilistic Context Free Grammars. We focus these experiments on demonstrating one of the main advantages of the SSN parser over the PCFG, handling sparse data. We use smaller datasets than are typically used with statistical methods, resulting in the PCFG finding parses for under half of the test sentences, while the SSN finds parses for all sentences. Even on the PCFG {`}s parsed half, the SSN performs better than the PCFG, as measure by recall and precision on both constituents and a dependency-like measure.
We present a context-free approximation of unification-based grammars, such as HPSG or PATR-II. The theoretical underpinning is established through a least fixpoint construction over a certain monotonic function. In order to reach a finite fixpoint, the concrete implementation can be parameterized in several ways , either by specifying a finite iteration depth, by using different restrictors, or by making the symbols of the CFG more complex adding annotations a la GPSG. We also present several methods that speed up the approximation process and help to limit the size of the resulting CF grammar.
Ambiguity packing is a well known technique for enhancing the efficiency of context-free parsers. However, in the case of unification-augmented context-free parsers where parsing is interleaved with feature unification, the propagation of feature structures imposes difficulties on the ability of the parser to effectively perform ambiguity packing. We demonstrate that a clever heuristic for prioritizing the execution order of grammar rules and parsing actions can achieve a high level of ambiguity packing that is provably optimal. We present empirical evaluations of the proposed technique, performed with both a Generalized LR parser and a chart parser, that demonstrate its effectiveness.
Existing parsing algorithms for Lexicalized Tree Grammars (LTG) formalisms (LTAG, TIG, DTG, ... ) are adaptations of algorithms initially dedicated to Context Free Grammars (CFG). They do not really take into account the fact that we do not use context free rules but partial parsing trees that we try to combine. Moreover the lexicalization raises up the important problem of multiplication of structures, a problem which does not exist in CFG. This paper presents parsing techniques for LTG taking into account these two fundamental features. Our approach focuses on robust and pratical purposes. Our parsing algorithm results in more extended partial parsing when the global parsing fails and in an interesting average complexity compared with others bottom-up algorithms.
We develop an improved form of left-corner chart parsing for large context-free grammars, introducing improvements that result in significant speed-ups more compared to previously-known variants of left corner parsing. We also compare our method to several other major parsing approaches, and find that our improved left-corner parsing method outperforms each of these across a range of grammars. Finally, we also describe a new technique for minimizing the extra information needed to efficiently recover parses from the data structures built in the course of parsing.
Over the past few years significant progress was accomplished in efficient processing with wide-coverage HPSG grammars. HPSG-based parsing systems are now available that can process medium-complexity sentences (of ten to twenty words, say) in average parse times equivalent to real (i.e. human reading) time. A large number of engineering improvements in current HPSG systems were achieved through collaboration of multiple research centers and mutual exchange of experience, encoding techniques, algorithms, and even pieces of software. This article presents an approach to grammar and system engineering, termed competence {\&} performance profiling, that makes systematic experimentation and the precise empirical study of system properties a focal point in development. Adapting the profiling metaphor familiar from software engineering to constraint-based grammars and parsers, enables developers to maintain an accurate record of system evolution, identify grammar and system deficiencies quickly, and compare to earlier versions or between different systems. We discuss a number of exemplary problems that motivate the experimental approach, and apply the empirical methodology in a fairly detailed discussion of what was achieved during a development period of three years. Given the collaborative nature in setup, the empirical results we present involve research and achievements of a large group of people.
This paper presents a probabilistic extension of Discontinuous Phrase Structure Grammar (DPSG), a formalism designed to describe discontinuous constituency phenomena adequately and perspicuously by means of trees with crossing branches. We outline an implementation of an agenda-based chart parsing algorithm that is capable of computing the Most Probable Parse for a given input sentence for probabilistic versions of both DPSG and Context-Free Grammar. Experiments were conducted with both types of grammars extracted from the NEGRA corpus. In spite of the much greater complexity of DPSG parsing in terms of the number of (partial) analyses that can be constructed for an input sentence, accuracy results from both experiments are comparable. We also briefly hint at future lines of research aimed at more efficient ways of probabilistic parsing with discontinuous constituents.
The first published LR algorithm for Tree Adjoining Grammars (TAGs [Joshi and Schabes, 1996]) was due to Schabes and Vijay-Shanker [1990] . Nederhof [1998] showed that it was incorrect (after [Kinyon, 1997]), and proposed a new one. Experimenting with his new algorithm over the XTAG English Grammar [XTAG Research Group, 1998] he concluded that LR parsing was inadequate for use with reasonably sized grammars because the size of the generated table was unmanageable. Also the degree of conflicts is too high. In this paper we discuss issues involved with LR parsing for TAGs and propose a new version of the algorithm that, by maintaining the degree of prediction while deferring the {``}subtree reduction{''}, dramatically reduces both the average number of conflicts per state and the size of the parser.
In this work we introduce the notion of path set for parsing free word order languages. The parsing system uses this notion to parse examples of sentences with scrambling. We show that by using path set, the performance constraints on scrambling such as Resource Limitation Principle (RLP) can be represented easily. Our work contrasts with models based on the notion of immediate dominance rule and binary precedence relations. In our work the precedence relations and word order constraints are defined locally for each clause. Our binary precedence relations are examples of fuzzy relations with weights attached to them. As a result, the word order principles in our approach can be violated and each violation contributes to a lowering of the overall acceptability and grammaticality. The work suggests a robust principle-based approach to parsing ambiguous sentences in verb final languages.
In this paper, we describe some concepts of language models beyond the usually used standard trigram and use such language models for statistical machine translation. In statistical machine translation the language model is the a-priori knowledge source of the system about the target language. One important requirement for the language model is the correct word order, given a certain choice of words, and to score the translations generated by the translation model $\textrm{Pr}(f_1^J/e^I_1)$, in view of the syntactic context. In addition to standard $m$-grams with long histories, we examine the use of Part-of-Speech based models as well as linguistically motivated grammars with stochastic parsing as a special type of language model. Translation results are given on the VERBMOBIL task, where translation is performed from German to English, with vocabulary sizes of 6500 and 4000 words, respectively.
We propose an algebraic method for the design of tabular parsing algorithms which uses parsing schemata [7]. The parsing strategy is expressed in a tree algebra. A parsing schema is derived from the tree algebra by means of algebraic operations such as homomorphic images, direct products, subalgebras and quotient algebras. The latter yields a tabular interpretation of the parsing strategy. The proposed method allows simpler and more elegant correctness proofs by using general theorems and is not limited to left-right parsing strategies, unlike current automaton-based approaches. Furthermore, it allows to derive parsing schemata for linear indexed grammars (LIG) from parsing schemata for context-free grammars by means of a correctness preserving algebraic transformation. A new bottom-up head corner parsing schema for LIG is constructed to demonstrate the method.
An implementation of a Spanish POS tagger is described in this paper. This implementation combines three basic approaches: a single word tagger based on decision trees, a POS tagger based on variable memory Markov models, and a feature structures set of tags. Using decision trees for single word tagging allows the tagger to work without a lexicon that lists only possible tags. Moreover, it decreases the error rate because there are no unknown words. The feature structure set of tags is advantageous when the available training corpus is small and the tag set large, which can be the case with morphologically rich languages like Spanish. Finally, variable memory Markov models training is more efficient than traditional full-order Markov models and achieves better accuracy. In this implementation, 98.58{\%} of tokens are correctly classified.
Efficiency, memory, ambiguity, robustness and scalability are the central issues in natural language parsing. Because of the complexity of natural language, different parsers may be suited only to certain subgrammars. In addition, grammar maintenance and updating may have adverse effects on tuned parsers. Motivated by these concerns, [25] proposed a grammar partitioning and top-down parser composition mechanism for loosely restricted Context-Free Grammars (CFGs). In this paper, we report on significant progress, i.e., (1) developing guidelines for the grammar partition through a set of heuristics, (2) devising a new mix-strategy composition algorithms for any rule-based grammar partition in a lattice framework, and 3) initial but encouraging parsing results for Chinese and English queries from an Air Travel Information System (ATIS) corpus.
We present an implementation of the notion of modularity and composition applied to unification based grammars. Monolithic unification grammars can be decomposed into sub-grammars with well defined interfaces. Sub-grammars are applied in a sequential manner at runtime, allowing incremental development and testing of large coverage grammars. The modular approach to grammar development leads us away from the traditional view of parsing a string of input symbols as the recognition of some start symbol, and towards a richer and more flexible view where inputs and outputs share the same structural properties.
We introduce Recursive Matrix Systems (RMS) which encompass mildly context-sensitive formalisms and present efficient parsing algorithms for linear and context-free variants of RMS. The time complexities are $\mathcal{O}(n^{2h + 1})$, and $\mathcal{O}(n^{3h})$ respectively, where $h$ is the height of the matrix. It is possible to represent Tree Adjoining Grammars (TAG [1], MC-TAG [2], and R-TAG [3]) as RMS uniformly.
We show how to augment a finite-state grammar with annotations which allow dependency structures to be extracted. There are some difficulties in determinising the grammar, which is an essential step for computational efficiency, but they can be overcome. The parser also allows syntactically ambiguous structures to be packed into a single representation.
Mathematical equations in LaTeX are composed with tags that express formatting as opposed to structure. For conversion from LaTeX to other word-processing systems, the structure of each equation must be inferred. We show how a form of least cost parsing used with a very general and ambiguous grammar may be used to select an appropriate structure for a LaTeX equation. MathML provides another application for the same technology; it has two alternative tagging schemes - presentation tags to specify formatting and content tags to specify structure. While conversion from content tagging to presentation tagging is straightforward, the converse is not. Our implementation of least cost parsing is based on Earley{'}s algorithm.
Because of the nature of the parsing problem, unification-based parsers are hard to parallelize. We present a parallelization technique designed to cope with these difficulties.
This paper describes a rule based method for partial parsing, particularly for noun phrase recognition, which has been used in the development of a noun phrase recognizer for Modern Greek. This technique is based on a cascade of finite state machines, adding to them a characteristic very crucial in the parsing of words with free word order: the simultaneous examination of part of speech and grammatical feature information, which are deemed equally important during the parsing procedure, in contrast with other methodologies.
313  gether as a single set, the accepting capacity of the systems equals the accepting power of deterministic pushdown automata. In other words, the systems collapse to RC-uniquely parsable grammars. When local level is considered, where conditions apply tQ all rules of each component independently, for some more restrictive classes, one gets more computational power keeping the parsability without backtracking. We give a simple recognition algorithm for these systems. Our algorithm has two distinct phases: one in which we find the unique component which is to become active and the other one in which a t-leftmost reduction is performed by this component. The former phase is based on the algorithm proposed in [1] for solving the multiple pattern matching problem. The only difference is that the searching process is stopped as soon as a matching pattern has been found in the text. The text is the sentential form while the dictionary of matching patterns is formed by the words in the lefthand side of all rules of the given uniquely parsable grammar system. For the latter phase we define a procedure which provides the word obtained by a t-leftmost re­ duction of the current sentential form in the component found in the first phase. Then, this process is resumed. A criterion to detect infinite loops in the derivation process is also presented. When no reduction is possible anymore, we check the sentential form and decide whether or not the input word is accepted. The exact complexity of this algorithm is briefly discussed. Acknowledgements The work done by the second author has been supported by the Direcci6n General de Ensefianza Superior e Investigaci6n Cientifica, SB 97-00110508. References [1] A. V. Aho and M. J. Corasick. 1975. Efficient string matching: an aid to bibliographic search. Commun. ACM 18:333-340. 
Table 1: Parsing systems as CHR programs: Earley's algorithm  Items  Axiom  Goal  edge (A , Alpha , Bet a , I , J) edge ( sprime , [] , [s] , 0 , 0) edge ( sprime , [s] , [] , O , Len)  
We investigate a method of improving the memory efficiency of a chart parser. Specifically, we propose a technique to reduce the number of active arcs created in the process of parsing. We sketch the differences in the chart algorithm, and provide empirical results that demonstrate the effectiveness of this technique.
We describe a new model for dependency structure analysis. This model learns the relationship between two phrasal units called bunsetsus as three categories; {`}between{'}, {`}dependent{'}, and {`}beyond{'}, and estimates the dependency likelihood by considering not only the relationship between two bunsetsus but also the relationship between the left bunsetsu and all of the bunsetsus to its right. We implemented this model based on the maximum entropy model. When using the Kyoto University corpus, the dependency accuracy of our model was 88{\%}, which is about 1{\%} higher than that of the conventional model using exactly the same features.
In an information system indexing can be accomplished by creating a citation based on context-free parses, and matching becomes a natural mechanism to extract patterns. However, the language intended to represent the document can often only be approximately defined, and indices can become shared forests. Queries could also vary from indices and an approximate matching strategy becomes also necessary. We present a proposal intended to prove the applicability of tabulation techniques in this context.
 d e n t7 2 0 \p n h a n g { \p n tx tb  (} { \p n tx ta  )} }  { \* \p n s e c lv l8 \p n lc ltr\p n s ta rt1 \p n in d e n  t7 2 0 \p n h a n g { \p n tx tb  (} { \p n tx ta  )} }  { \* \p n s e c lv l9 \p n lc rm \p n s ta r t1 \p n in d e n  t7 2 0 \p n h a n g { \p n tx tb  (} { \p n tx ta )} } \p a rd \p la in \w id c tlp a r  \f4 \la n g 1 0 3 6  T h is  s e n te n c e  u s e s { \b  b o ld } , { \i  ita lic s } , a n d c o m b in e d { \b \i ita lic s a n d  b o ld }  c h a ra c te rs  p lu s  an  in d e x  m a rk { \p a r d \p la in  \w id c tlp a r  \v \f4 \la n g 1 0 3 6 { \x e { in d e x m a rk } } } .  \p a r } ....  Figure 1: An example of RTF document  1.1.b. A suitable internal structure for Translation Memories A first problem that this observation arises is how to represent all this data in a convenient way in Translation Memories. We have proposed to use a structure of linked lattices that we call TELA in [Planas 1998] and [Planas 1999]. We invite the readers to refer to these documents for more information, since we are not going to discuss this problem here. Just bear in mind that the TELA structure allows the TM system to separate the document data according to its nature, while keeping the link between the different elements of the document. Next Figure illustrates this:  
2. The MULTEXT-East dataset The MULTEXT-East project (Multilingual Text Tools and Corpora for Eastern and Central European Languages) was a spin-off of the EU MULTEXT project. The project ran 1995-97 and developed language resources for six languages: Bulgarian, Czech, Estonian, Hungarian, Romanian, and Slovene, and, additionally, for English as the "hub" language of the project. It also adapted existing tools and standards to these languages. The main results of the project [6] were an annotated multilingual corpus and morpholexical resources for the languages in question. The development of the morpholexical resources proceeded in three stages. First, harmonised morphosyntactic descriptions (MSDs) were developed for the languages of the project. These are used to describe a wordform as being, say, a proper inanimate masculine noun in singular accusative. The second stage was in building the actual lexica, which cover the lexical stock of the corpus collected in the project. Finally, the developed lexica were used to annotate a portion of the MULTEXT-East corpus with context-disambiguated MSDs and lemmas. One of the objectives of MULTEXT-East has been to make its resources freely available for research purposes. In the scope of the EU TELRI (Trans European Language Resources Infrastructure) concerted action, the complete results of TELRI and MULTEXT-East have been released on a CD-ROM [7]. Since the release of the CD-ROM, the resources have been used in a number of experiments. In the course of this work, errors and inconsistencies were discovered in the specifications and in the data, which were subsequently corrected. This lead the partners' to consider releasing a new version of the corrected resources, which had been recently completed. The rest of this section explains the contents and structure of this new release; more information is available on the home page of the project, at http://nl.ijs.si/ME/. 2.1 The Morphosyntactic Specifications The syntax and semantics of the MULTEXT-East MSDs are given in the morphosyntactic specifications of the project. These specifications have been developed in the formalism and on the basis of specifications for six Western European languages of the EU MULTEXT project and in cooperation with EAGLES, the Expert Advisory Group on Language Engineering Standards. The MULTEXT-East morphosyntactic specifications contain, along with introductory matter, also: - the list of defined categories (parts-of-speech) - common tables of attribute-values - language particular tables Of the MULTEXT-East categories, Slovene uses Noun (N), Verb (V), Adjective (A), Pronoun (P), Adverb (R), Adposition (S) (these include prepositions and postpositions; Slovene uses only prepositions), Conjunction (C), Numeral (M), Interjection (I), Residual (X) (for unknown words), Abbreviation (Y), and Particle (Q). The common tables of the specification give for each category a table defining the attributes appropriate for the category and the values defined for these attributes. They also define which attributes/values are appropriate for each of the MULTEXT-East languages; the tabular structure facilitates the addition of new languages, and the new release adds Croatian (HR) to the specifications. The common tables have a strictly defined format, which enables the automatic expansion and validation of MSDs. The format of these tables is exemplified by the start of the Noun table, given below:  = ============== ============== = EN RO SL CS BG ET HU HR  P ATT  VAL  Cx x x x x x x x  = ============== ============== =  
2.1. Basic terms It is perhaps to some extent unfortunate that we seem to be saddled with the term »machine translation«. It is seen by many as an old-fashioned term, redolent of a pre-electronic and precomputer age. More particularly, however, this term implies only purely automatic systems not involving human participation at any stage. It excludes, almost by definition, all kinds of computer aids for translation. What is wanted is an easily understood term that covers computerbased systems that provide translations which can be used as such (without revision) or which can be used as the basis for higher quality human translation, and which covers also many kinds of translation support tools. The term chosen for the »Compendium« was »translation software«. This is, perhaps, reasonable for covering commercial products (both fully automatic systenms and support tools), but it is not adequate as a term for research and development activity. An additional area of confusion is the use of the term »computer-aided« or »computerassisted« translation (CAT). Sometimes it refers to the use of automatic translation systems with facilities for human involvement or intervention, before or after text processing (i.e. pre-editing, controlled input, post-editing). In other words, CAT is used by many vendors in preference to MT. On other occasions CAT refers to computer support tools for translation such as dictionaries, translation memories, etc. There is further confusion from the availability on the market of software combining both types of system, e.g. MT and translation memories. 2.2. Basic distinctions. The first distinction therefore for the general public has to be between: a): wholly automatic systems, i.e. systems that (attempt to) translate texts and sentences as wholes and (b): computer-based translation aids, i.e. systems that provide linguistic aids for translation. In the latter case, it is easier (and probably clearer) to list the aids, since many of them will already be familiar (even if not in electronic form): 1. Dictionaries: both bilingual and multilingual, with and without grammatical information, with and without guidance on usage (appropriateness) 2. Language aids providing grammatical information (morphology, noun/verb paradigms) 3. Spelling checkers 4. Style checkers 5. Terminology aids, such as glossaries of ‘authorized’ terminology for a particular scientific, technical or commercial field 6. Specialised glossaries, e.g. for a translator’s special subject areas, for particular clients, agencies and customers Other computer-based aids will not be known by the general public, and would therefore require detailed explanations (see below, section 4). These include tools for pre-editing and controlled language, tools for the creation of corpora of ‘approved’ translations (translation memories and alignment tools), and management support tools for, e.g. budgeting and cost controls, workflow and personnel management, etc.  3. Automatic translation (MT) systems 3.1. Minimal basic features Firstly the general public needs to know what distinguishes a ‘true’ MT system from a ‘dictionary translation’. The need for this stems from the existence on the market and on the Internet of systems that are described as translation systems but which are in effect no more than bilingual dictionaries. As a minimal definition, we can say that a translation program should be more than simply substitute words of the source text by words of the target language. It should provide: (a) minimally correct morphology. For example, the endings of adjectives should agree with the nouns they modify, the grammatical cases of nouns should agree with the selected verb forms, etc. (b) some minimal syntactic processing. For example, the order of adjectives and nouns should be inverted when translating between English and French, the position of the verb should be correct when translating between German and English. (c): some semantic processing. For example, there should be some selection among alternative ‘equivalents’ according to context or subject field. 
Language technology (LT) plays an increasingly important role in the field of multilingual information content production and knowledge management. Companies are more and more faced with the problem of the multilingual bottleneck, and are looking for new technologies which bring in productivity gains in the localisation and globalisation process. On the other hand, the increasing number of LT products, especially machine translation systems (MT) calls for useful and practical assessment procedures for potential users in order to prove if a given system fits a user's needs. Static evaluations which test a system's fitness as-is, may be good for coarse-grained system comparisons, e.g. for best buy reports, or to make a system rating as the first step of more detailed system tests. Dynamic evaluations which consider also a system's adaptability and extensibility in a certain use case are more time-consuming, but they are the most realistic and constructive approaches to evaluation: they ideally combine both the assessment of the state-of-art of a system with the assessment of its extensibility and adaptability potential. The paper investigates the requirements for a useful evaluation of MT and presents practical guidelines to trigger the successful deployment of LT in an industrial context. The scenario is multilingual technical documentation in the automotive industry. 
Not much use has been made of the Translation Memory tool (TWB) in the first year of activity. However, the situation is now rapidly changing, with intensive work being invested in compiling aligned English-Slovenian translation segments. All documents are to be converted to TM, and future translations are to be done directly in TWB. So far, about 10,000 segments have been aligned. 
the work of management in translation companies, and in which way? 
The internet is no longer English only. The data is voluminous and the number of proficient linguists cannot match the day to day needs of several government agencies. Handling foreign languages is not limited to translating documents but goes beyond the journalistic written formats. Military, diplomatic and official interactions in the US and abroad require more than one or two foreign language skills. The CHALLENGE is both managing the user{'}s expectations and stimulating new areas for MT research and development.
The application of MT on the Internet has certainly attracted much attention in recent years, and many observers see its future mostly in this arena of real-time raw translation. However, the need for high-volume, fast turn-around translation of publication quality has not abated. This paper will take stock of that particular use of MT and venture predictions as to its future.
his paper is concerned with the technology of using the PARS English-Russian bi- directional machine translation systems in teaching English as a foreign language. This technology has no connection with the old form of computer-assisted language learning which uses «drill-and-practice» computer exercises and provides a sort of surrogate «electronic teacher». The main objective of the educational implication of PARS is to help the learner become familiar with the words in their normal contexts. The introduction of a machine translation system into teaching foreign languages is intended to get the most fruitful pedagogical results from the use of personal computers and expose the learners to the up-to-date information technologies.
ENGSPAN, a machine translation program (English-Spanish), has been used by the Translation Services unit of the Pan American Health Organization since 1985. In 1999, a total of 2,106,178 words were translated in that language combination, 86{\%} of which were done with the help of ENGSPAN; the cost per word was 8.75 cents, that is, 31{\%} below the normal rate. These positive results are explained by a combination of factors: the use of an MT program especially designed to meet the needs of the institution; the close collaboration of translators and computational linguists in the improvement of the program; the application of a pragmatic, flexible, and selective approach with regard to the quality of the end product; and in particular the support of competent translators who do the postediting work.
Our project Wired for Peace: Virtual Diplomacy in Northeast Asia (Http://www- neacd.ucsd.edu/) has as its main aim to provide policymakers and researchers of the U.S., China, Russia, Japan, and Korea with Internet based tools to allow for continuous communication on issues of the regional security and cooperation. Since the very beginning of the project, we have understood that Web-based translation between English and Asian languages would be one of the most necessary tools for successful development of the project. With this understanding, we have partnered with Systran (www.systransoft.com), one of the leaders in MT field, in order to develop Internet-based tools for both synchronous and asynchronous translation of texts and discussions. This submission is a report on a work in progress.
The Internet is a wonderful medium that frees its users from the confines of geographic boundaries. While the acceptance of the Internet is pervasive, the language barrier is somewhat tougher to overcome. Several options exists on the market to deliver multilingual content, few solutions can stand up to the dynamic demand of a modern website. Language context, translation turnaround times, and various business models are all barriers to creating a total solution for globalization and localization of websites. We will examine the difficulties in localizing a dynamic website and discuss the challenges we have overcome to create a dynamic translation platform.
As is known, the majority of the actual textual content on the Internet is in English language. This represents an obstacle to those non-English speaking users willing to access the Internet. The idea behind this MT-based application is to allow any Arabic user to search and navigate through the Internet using Arabic language without the need to have prior knowledge of English language. The infrastructure of TARJIM.COM relies on 3 basic core components : 1- The Bi-directional English-Arabic Machine translation Engine, 2- The intelligent Web page layout preserving component and 3-The Search Engine query interceptor.
In this paper we describe the KANTOO machine translation environment, a set of software services and tools for multilingual document production. KANTOO includes modules for source language analysis, target language generation, source terminology management, target terminology management, and knowledge source development. The KANTOOsystem represents a complete re-design and re-implementation of the KANT machine translation system.
ARL{'}s FALCon system has proven its integrated OCR and MT technology to be a valuable asset to soldiers in the field in both Bosnia and Haiti. Now it is being extended to include six more SYSTRAN language pairs in response to the military{'}s need for automatic translation capabilities as they pursue US national objectives in East Asia. The Pacific Rim Portable Translator will provide robust automatic translation bidirectionally for English, Chinese, Japanese, and Korean, which will allow not only rapid assimilation of foreign information, but two-way communication as well for both the public and private sectors.
The LabelTool/TrTool system is designed to administer text strings that are shown in devices with a very limited display area and translated into a very large number of foreign languages. Automation of character set handling and file naming and storage together with real{--}time simulation of text string input are the main features of this application.
The LogoVista ES translation system translates English text to Spanish. It is a member of LEC{'}s family of translation tools and uses the same engine as LogoVista EJ. This engine, which has been under development for ten years, is heavily linguistic and rule-based. It includes a very large, highly annotated English dictionary that contains detailed syntactic, semantic and domain information; a binary parser that produces multiple parses for each sentence; a 12,000+-rule, context-free English grammar; and a synthesis file of rules that convert each parsed English structure into a Spanish structure. The main tasks involved in developing a new language pair include the addition of target-language translations to the dictionary and the addition of rules to the synthesis file. The system{'}s modular design allows the work to be carried out by linguists, independent of engineers.
One of the most important components of any machine translation system is the translation lexicon. The size and quality of the lexicon, as well as the coverage of the lexicon for a particular use, greatly influence the applicability of machine translation for a user. The high cost of lexicon development limits the extent to which even mature machine translation vendors can expand and specialize their lexicons, and frequently prevents users from building extensive lexicons at all. To address the high cost of lexicography for machine translation, L{\&}H is building a Lexicography Toolkit that includes tools that can significantly improve the process of creating custom lexicons. The toolkit is based on the concept of using automatic methods of data acquisition, using text corpora, to generate lexicon entries. Of course, lexicon entries must be accurate, so the work of the toolkit must be checked by human experts at several stages. However, this checking mostly consists of removing erroneous results, rather than adding data and entire entries. This article will explore how the Lexicography Toolkit would be used to create a lexicon that is specific to the user{'}s domain.
This paper describes some of the features of the new 32-bit Windows version of PAHO{'}s English-Spanish (ENGSPAN®) and Spanish-English (SPANAM®) machine translation software. The new dictionary update interface is designed to help users add their own terminology to the lexicon and encourage them to write context-sensitive rules to improve the quality of the output. Expanded search capabilities provide instant access to related source and target entries, expressions, and rules. A live system demonstration will accompany this presentation.
This paper discusses an informal methodology for evaluating Machine Translation software documentation with reference to a case study, in which a number of currently available MT packages are evaluated. Different types of documentation style are discussed, as well as different user profiles. It is found that documentation is often inadequate in identifying the level of linguistic background and knowledge necessary to use translation software, and in explaining technical (linguistic) terms needed to use the software effectively. In particular, the level of knowledge and training needed to use the software is often incompatible with the user profile implied by the documentation. Also, guidance on how to perform more complex tasks, which may be especially idiosyncratic, is often inadequate or missing altogether.
{``}Embedded{''} machine translation (MT) refers to an end-to-end computational process of which MT is one of the components. Integrating these components and evaluating the whole has proved to be problematic. As an example of embedded MT, we describe a prototype system called Falcon, which permits paper documents to be scanned and translated into English. MT is thus embedded in the preprocessing of hardcopy pages and subject to its noise. Because Falcon is intended for use by people in the military who are trying to screen foreign documents, and not to understand them in detail, its application makes low demands on translation quality. We report on a series of user trials that speak to the utility of embedded MT in army tasks.
We present four kinds of machine translation system in this description: E-K (English to Korean), K-E (Korean to English), J-K (Japanese to Korean), K-J (Korean to Japanese). Among these, E-K and K-J translation systems are published commercially, and the other systems have finished their development. This paper describes the structure and function of each system with figures and translation results.
This paper addresses the problem of building conceptual resources for multilingual applications. We describe new techniques for large-scale construction of a Chinese-English lexicon for verbs, using thematic-role information to create links between Chinese and English conceptual information. We then present an approach to compensating for gaps in the existing resources. The resulting lexicon is used for multilingual applications such as machine translation and cross-language information retrieval.
Cross-language information retrieval (CLIR), where queries and documents are in different languages, needs a translation of queries and/or documents, so as to standardize both of them into a common representation. For this purpose, the use of machine translation is an effective approach. However, computational cost is prohibitive in translating large-scale document collections. To resolve this problem, we propose a two-stage CLIR method. First, we translate a given query into the document language, and retrieve a limited number of foreign documents. Second, we machine translate only those documents into the user language, and re-rank them based on the translation result. We also show the effectiveness of our method by way of experiments using Japanese queries and English technical documents.
A mixed-initiative system is one which allows more interactivity between the system and user, as the system is reasoning. We present some observations on the task of translating Web pages for users and suggest that a more interactive approach to this problem may be desirable. The aim is to interact with the user who is requesting the translation and the challenge is to determine the circumstances under which the user should be able to take the initiative to direct the processing or the system should be able to take the initiative to solicit further input from the user. In fact, we envision a need to support interactive translation of Web pages as the World Wide Web becomes more accessible to people with varying needs and abilities throughout the world.
This paper describes a language independent method for alignment of parallel texts that re-uses acquired knowledge. The system extracts word translation equivalents and re-uses them as correspondence points in order to enhance the alignment of parallel texts. Points that may cause misalignment are filtered using confidence bands of linear regression analysis instead of heuristics, which are not theoretically reliable. Homographs bootstrap the alignment process so as to build the primary word translation lexicon. At each step, the previously acquired lexicon is re-used so as to repeatedly make finer-grained alignments and produce more reliable translation lexicons.
This paper describes an approach for handling structural divergences and recovering dropped arguments in an implemented Korean to English machine translation system. The approach relies on canonical predicate-argument structures (or dependency structures), which provide a suitable pivot representation for the handling of structural divergences and the recovery of dropped arguments. It can also be converted to and from the interface representations of many off-the-shelf parsers and generators.
Research in computational linguistics, computer graphics and autonomous agents has led to the development of increasingly sophisticated communicative agents over the past few years, bringing new perspective to machine translation research. The engineering of language- based smooth, expressive, natural-looking human gestures can give us useful insights into the design principles that have evolved in natural communication between people. In this paper we prototype a machine translation system from English to American Sign Language (ASL), taking into account not only linguistic but also visual and spatial information associated with ASL signs.
This paper describes a language independent linearization engine, oxyGen. This system compiles target language grammars into programs that take feature graphs as inputs and generate word lattices that can be passed along to the statistical extraction module of the generation system Nitrogen. The grammars are written using a flexible and powerful language, oxyL, that has the power of a programming language but focuses on natural language realization. This engine has been used successfully in creating an English linearization program that is currently employed as part of a Chinese-English machine translation system.
This paper presents the implementation part of my doctoral research at the University of Cambridge. It provides a description of the Information Structure Transfer (IST), a machine translation prototype designed within the framework of the Spoken Language Translator (SLT by SRI, Cambridge/Palo Alto) and based on the Core Language Engine ([1]). The IST includes two discourse-processing modules: the pre-transfer Information Structure Activator (ISA) and the post-transfer Information Structure Generator (ISG). The IST prototype calculates and processes vital features of information structure explored in context of structural differences between positional and nonpositional languages. It offers algorithmic solutions and an implementation framework for local discourse processing in machine translation. Under scrutiny is a web of interrelated factors such as pronominalization, anaphora resolution, zero anaphors, definiteness and constituent order.
Translations produced by an MT system can automatically be assigned a number that reflects the MT system{'}s confidence in their quality. We describe the design of such a confidence index, with focus on the contribution of source analysis, which plays a crucial role in many MT systems, including ours. Various problematic areas of source analysis are identified, and their impact on the overall confidence index is given. We will describe two methods of training the confidence index, one by hand-tuning of the heuristics, the other by linear regression analysis.
Researchers, developers, translators and information consumers all share the problem that there is no accepted standard for machine translation. The problem is much further confounded by the fact that MT evaluations properly done require a considerable commitment of time and resources, an anachronism in this day of cross-lingual information processing when new MT systems may developed in weeks instead of years. This paper surveys the needs addressed by several of the classic {``}types{''} of MT, and speculates on ways that each of these types might be automated to create relevant, near-instantaneous evaluation of approaches and systems.
Machine Translation evaluation has been more magic and opinion than science. The history of MT evaluation is long and checkered - the search for objective, measurable, resource-reduced methods of evaluation continues. A recent trend towards task-based evaluation inspires the question - can we use methods of evaluation of language competence in language learners and apply them reasonably to MT evaluation? This paper is the first in a series of steps to look at this question. In this paper, we will present the theoretical framework for our ideas, the notions we ultimately aim towards and some very preliminary results of a small experiment along these lines.
Parallel corpora enriched with descriptive annotations facilitate multilingual authoring development. Departing from an annotated bitext we show how SGML markup can be recycled to produce complementary language resources. On the one hand, several translation memory databases together with glossaries of proper nouns have been produced. On the other, DTDs for source and target documents have been derived and put into correspondence. This paper discusses how these resources have been automatically generated and applied to an interactive bilingual authoring system. This tool is capable of handling a substantial proportion of text both in the composition and translation of structured documents.
This paper presents an approach to extract invertible trans- lation examples from pre-aligned reference translations. The set of in- vertible translation examples is used in the Example-Based Machine Translation (EBMT) system EDGAR for translation. Invertible bilin- gual grammars eliminate translation ambiguities such that each source language parse tree maps into only one target language string. The trans- lation results of EDGAR are compared and combined with those of a translation memory (TM). It is shown that i) best translation results are achieved for the EBMT system when using a bilingual lexicon to sup- port the alignment process ii) TMs and EBMT-systems can be linked in a dynamical sequential manner and iii) the combined translation of TMs and EBMT is in any case better than each of the single system.
Although undeniably useful for the translation of certain types of repetitive document, current translation memory technology is limited by the rudimentary techniques employed for approximate matching. Such systems, moreover, incorporate no real notion of a document, since the databases that underlie them are essentially composed of isolated sentence strings. As a result, current TM products can only exploit a small portion of the knowledge residing in translators{'} past production. This paper examines some of the changes that will have to be implemented if the technology is to be made more widely applicable.
The paper deals with the question whether representations of verb semantics formulated on the basis of a lexically and syntactically restricted domain (weather forecasts) can apply to other, less restricted textual domains. An analysis of a group of Polish polysemous verbs of motion, existence and appearance inspired by cognitive semantics, especially the metaphor theory, is presented, and the usefulness of the conceptual representations of the Polish motion/appearance/existence verbs for automatic translation of texts belonging to less restricted domains is evaluated and discussed.
Machine translation has proved itself to be easier between languages that are closely related, such as German and English, while far apart languages, such as Chinese and English, encounter much more problems. The present study focuses upon Swedish and Norwegian; two languages so closely related that they would be referred to as dialects if it were not for the fact that they had a Royal house and an army connected to each of them. Despite their similarity though, some differences make the translation phase much less straight-forward than what could be expected. Taking the outset in sentence aligned parallel texts, this study aims at highlighting some of the differences, and to formalise the results. In order to do so, the texts have been aligned on smaller units, by a simple cognate alignment method. Not at all surprising, the longer words were easier to align, while shorter and often high-frequent words became a problem. Also when trying to align to a specific word sense in a dictionary, content words rendered better results. Therefore, we abandoned the use of single-word units, and searched for multi-word units whenever possible. This study reinforces the view that Machine Translation should rest upon methods based on multiword unit searches.
We describe our experience in adapting an existing high- quality, interlingual, unidirectional machine translation system to a new domain and bidirectional translation for a new language pair (English and Italian). We focus on the interlingua design changes which were necessary to achieve high quality output in view of the language mismatches between English and Italian. The representation we propose contains features that are interpreted differently, depending on the translation direction. This decision simplified the process of creating the interlingua for individual sentences, and allows the system to defer mapping of language-specific features (such as tense and aspect), which are realized when the target syntactic feature structure is created. We also describe a set of problems we encountered in translating modal verbs, and discuss the representation of modality in our interlingua.
In this paper we propose a representation for what we have called an interpretation of a text. We base this representation on TMR (Text Meaning Representation), an interlingual representation developed for Machine Translation purposes. A TMR consists of a complex feature-value structure, with the feature names and filler values drawn from an ontology, in this case, ONTOS, developed concurrently with TMR. We suggest on the basis of previous work, that a representation of an interpretation of a text must build on a TMR structure for the text in several ways: (1) by the inclusion of additional required features and feature values (which may themselves be complex feature structures); (2) by pragmatically filling in empty slots in the TMR structure itself; and (3) by supporting the connections between feature values by including, as part of the TMR itself, the chains of inferencing that link various parts of the structure.
Sam m endrag The paper describes a natural language based expert system route adviser for the public bus transport in Trondheim, Norway. The sy­ stem is available on the Internet, and has been installed at the bus company’s web server since the beginning of 1999. The system is bilin­ gual, relying on an internal language independent logic representation. 
Word alignment of parallel texts is typically carried out using many kinds of knowledge, or information sources, in concert, i.e., it is profitably viewed as a kind of cooperative process, where e.g. distribution, string similarity, cooccurrence statistics, and other in­ formation sources are used together. We investigate a novel such information source in this paper, namely the use of a third language as a ‘pivot’ to increase alignment recall, hence the name pivot alignment. The results of the preliminary experiments reported here indicate that pivot alignment increases word alignment recall, without sacrificing preci­ sion. We conclude that the method is well worth exploring further, by examining more languages and language combinations. 
This article describes how Granska - a surface-oriented system for checking Swedish grammar - is constructed. With the use of special error detection rules, the system can detect and suggest corrections for a number of grammatical errors in Swedish texts. Specifically, we focus on how erroneously split compounds and noun phrase agreement are handled in the rules. The system combines probabilistic and rule-based methods to achieve high efficiency and robustness. This is a necessary prerequisite for a grammar checker that will be used in real lime in direct interaction with users. We hope to show that the Granska system with higher efficiency can achieve the same or better results than systems that use rule-based parsing alone. 1. Introduction Grammar checking is one of the most widely used tools within language technology. Spelling, grammar and style checking for English has been an integrated part of common word processors for some years now. For smaller languages, such as Swedish, advanced tools have been lacking. Recently, however, a grammar checker for Swedish has been launched in Word 2000 and also as a stand-alone system called Grammatifix (Arppe 2000, this volume; Bim 2000, this volume). There are many reasons for further research and development of grammar checking for Swedish. First, the need for writing aids has increased, both concerning the need for more efficiency and quality in writing. Secondly, the linguistic analysis in grammar checking needs further development, especially in dealing with special features in Swedish grammar and its grammatical deviations. This is a development that most NLP-systems will benefit from, since they often lack necessary methods for handling ungrammatical input. Thirdly, Proceedings of NODALIDA 1999, pages 49-56  50 there is need for more sophisticated methods for evaluating the functionality and usability of grammar checkers and their effect on writing and writing ability. There are two research projects that focus on grammar checking for Swedish. These projects have resulted in two prototype systems: Scarrie (Sagvall-Hein 1998; Scarrie 2000) and Granska (Domeij, Eklundh, Knutsson, Larsson & Rex 1998). In this article we describe how the Granska system is constructed and how grammatical errors are handled by its error rule component. The focus will be on the treatment of agreement and split compound errors, two types of errors that frequently occur in Swedish texts. 2. The Granska system Granska is a hybrid system that uses surface grammar rules to check grammatical constructions in Swedish. The system combines probabilistic and rule-based methods to achieve high efficiency and robustness. This is a necessary prerequisite for a grammar checker that runs in real time in direct interaction with users (e.g. Kukich 1992). Using special error rules, the system can detect a number of Swedish grammar problems and suggest corrections for them. In figure 1 the modular structure of the system is presented. First, in the tokenizer, potential words and special characters are recognized as such. In the next step, a tagger is used to assign part of speech and inflectional form information to each word. The tagged text is then sent to the error rule component where error rules are matched with the text in order to search for specified grammatical problems. The error rule component also generates error corrections and instructional information about detected problems that are presented to the user in a graphical interface. Furthermore, the system contains a spelling detection and correction module which can handle Swedish compounds ( Kann, Domeij, Hollman & Tillenius 1998). The spelling detection module can be used from the error rules for checking split compound errors. Text Figure 1. An overview o f the Granska system. Proceedings of NODALIDA 1999  51 The system is implemented in C++ under Unix and there is also a web site where it can be tested from a simple web interface (see www.nada.kth.seAheory/projects/granska/demo.html). There is ongoing work for designing a graphical interface for PC which can be used interactively during writing. The PC system will be used as a research tool for studying usability aspects with real users. 3. Tagging and lexicon The Granska system uses a hidden Markov model (Carlberger & Kann 1999) to tag and disambiguate all words in the input text. Every word is given a tag that describes its part of speech and morphological features. The tagging is done on the basis of a lexicon with 160 000 word forms constructed from SUC, a hand tagged corpus of one million words (Ejerhed, Källgren, Wennstedt & Åström 1992). The lexicon has been further complemented with words from SAOL, the Swedish Academy’s wordlist (Svenska akademien 1986). The Markov model is based on statistics from SUC about the occurrence of words and tags in context. From this information the tagger can choose the most probable tag for every word in the text if it is listed in the lexicon. Unknown words are tagged on the basis of probabilistic analysis of word endings. 4. Error rules The error rule component uses special error rules to process the tagged text in search for grammatical errors. Since the Markov model also disambiguates and tags morphosyntactically deviant words with only one tag, there is normally no need for further disambiguation in the error rules in order to detect an error. An example of an agreement error is ett röd bil (a red car), where en (a) does not agree with röd (red) and bil (car) in gender. The strategy differs from most rule-based systems which often use a complete grammar in combination with relaxation techniques to detect morphosyntactical deviations (e.g. Sågvall-Hein 1998). An error rule in Granska that can detect the agreement error in ett röd bil is shown in rule 1 below. Rule 1: kong22@inkongruens 
This paper presents work on adapting the Proteus Information Extraction sys­ tem to Swedish. It turned out that the cross-lingual adaptation as such was fairly straight-forward; however, the Proteus system design did not render itself that well to reconfiguration at such a low level as needed. To evaluate the adaptation, the system was tested on a Swedish version of the MUC-6 Scenario Template Task. The Swedish version performed excellently on a training corpus, but quite discouragingly on an unseen test corpus. As a consequence of that work, a new Information Extraction system is being designed and the layout of that system is described. 
The tagger used for the Oslo Corpus of Tagged Norwegian Texts has very good statistical results. In spite of this, it makes mistakes. In this paper we take a closer look at some of them. Although some mistakes are of a kind that would disappear if we improved the tagger, many are impossible or very difficult to do anything about. They are due to errors in the corpus (spelling errors, foreign words, non-standard spellings), to elliptic sentences, such as headlines, and to structural ambiguity, which abounds to a surprising extent. Proofreading the corpus would have removed the first kind of problems, but the other two types cannot be resolved in any obvious way. 1. Introduction The first version of the first ever comprehensive tagger for Norwegian is ready. Both the nynorsk and the bokmål (the two Norwegian language varieties) versions have been used to tag a large number of texts (= the Oslo Corpus of Tagged Norwegian Texts). The corpus has an advanced web-based user interface, which often gives nice results, but it also makes it easy to discover mistakes and shortcomings of the tagger. The present paper will focus on these. 
One prospective way to improve information retrieval is to use several indexing methods to retrieve different sets of documents, and then to merge (or combine) these results into one single result. The merging should be done in a way th at produces a final result that is more accurate than the output of any of the individual classifiers. A merging algorithm called SE Q U E L has been applied for this task to data in the field of information retrieval. This article describes the results of these experiments, as well as conceivable future directions. 
An indexing tool was built to provide for one of several information seeking tasks. In ac­ cordance with the basic principles of work held by the HUMLE laboratory at SICS, a so­ lution regarding indexing would be a semi-automatic tool. This approach is also relevant as the continuation of the indexing project is conducted in co-operation with the Swedish Parliament, where a staff of professional indexers currently is investigating the utility of automatic and semi-automatic indexing tools to raise productivity. 
We work with two tasks in mind: query expansion and text concept indexing. We outline some arguments showing why onto-matching is useful and how it can be implemented. Also, we conducted some experiments with query expansion for AltaVista. 
LPs are expressions that belong to the lexicon and consist of more than one word. Included are semantic idioms the meaning of which is not built up compositionally from the individual meanings of the words in the idiom. (An English example is kick the bucket meaning 'die', a Swedish equivalent ta ner skylten (lit. take the sign down) also meaning 'die'.) They should be lexically listed because of semantic reasons. Another group consists of syntactic idioms containing "ungrammatical" or non-standard combinations of words, in syntactic terms (inte så värst (lit. not so worst) meaning 'not particularly'; English example: by and large). The syntactic idioms do not make up regular grammatical structures and must therefore be listed as wholes in a parsing system. Of course, since syntactic idioms are syntactically irregular, no general function of semantic interpretation can apply over them and therefore a syntactic idiom is also a semantic one. 
In this study, we describe a method for parsing part-of-speech tagged unrestricted texts in Swedish using finite-state networks. We use the Xerox Finite-Slate Tool because of its expressiveness and power for writing and compiling regular expressions and relations. The parser is divided into four modules: i) contiguous phrase structure marker, ii) phrasal head marker, iii) syntactic function tagger, and iv) non­ contiguous group boundary recognizer. The aim is to develop a parser that can be used as a light/shallow parser for marking phrase structure and, when needed, to label syntactic functions. We believe that modularity is necessary since different NLP tasks require various levels of analysis. The parser for Swedish is under development, but present-day results are promising. 1. Introduction In several Natural Language Processing (NLP) tasks, such as information retrieval, information extraction, speech technology, machine translation, etc., full or partial information about phrasal and/or syntactic structures is needed. The main interest in these tasks lies in detecting the constituent structures and sometimes their syntactic functions in a robust and fast way. In this study, our aim is to develop a parser for Swedish part-of-speech tagged texts, based on finite-state techniques using the Xerox Finite-State Tool (Karttunen et al, 1997). 
In this paper we show that some of the syntactic patterns in an NLP lexicon can be used to identify semantically ”similar” adjectives and verbs. We define semantic similarity on the basis of parameters used in the literature to classify adjectives and verbs semantically. The semantic clusters obtained from the syntactic encodings in the lexicon are evaluated by comparing them with semantic groups in existing tax­ onomies. The relation between adjectival syntactic patterns and their meaning is particularly interesting, because it has not been explored in the literature as much as it is the case for the relation between verbal complements and tu-guments. The identification of semantic groups on the basis of the syntactic encodings in the con­ sidered NLP lexicon can also be extended to other word classes and, maybe, to other languages for which the same type of lexicon exists. 
This article addresses the issue of selection restrictions for noun phrase specifiers. Danish data is presented which shows that definiteness plays an important role in this respect. It is pointed out that an analysis is required in which the specifier, when present, leaves a mark on the projected phrase. This is achieved by assuming that specifiers are syntactic heads of noun phrase constructions. Further an elaborate classification of specifiers is also needed in terms of which selection restrictions may be formulated, rJong with a cross-categorial definiteness feature. These properties are part of the analysis proposed in this analysis. Introduction When investigating empirical data it becomes clear that noun phrases often have multiple specifiers appearing before the noun. An important goal of noun phrase analysis is the specification of selection restrictions for noun phrase specifiers and pre-nominals in general to account for combinations of specifiers. It is this goal that is pursued in this article. In section 1 a set of Danish noun phrases are presented which form the basis of a discussion of what properties determine the restrictions on combinations of pre-nominals. In section 2 a number of previous HPSG analyses of noun phrases and pre-nominals are discussed. In section 3 the proposed analysis is introduced and sample analyses are shown. The proposed analysis has been implemented in the LKB system (Copestake 1999). A test suite consisting of the data in section 1 has been run and the results are presented in section 4. The article is concluded in section 5. 
It is only during the last few years that attention has started to shift from pure textbased retrieval towards other media. Information retrieval from spoken documents is analogous to text-based retrieval; however, accessing audio documents causes some extra problems, in particular with respect to document segmentation, choice of in­ dexing features, and robustness. In addition, retrieval of documents in Swedish, like most non-English languages, adds the extra dimension of morphology; also, when analysing spoken Swedish data, prosodic patterns have to be taken into cu:count. In this paper we introduce SIREN, the Swedish Information Retrieval Engine, a very flexible, modular IR system which has been designed with a specific eye towards these issues.  
This report describes two experiments in finding errors in optically scanned Swedish without lexicon. First, statistics were used to find unexpectedly frequent trigrams and correction rules were created. Second, Bengt Sigurds model of Swedish phonotax was used to detect words with phonotactically illegal beginning or end. The phonotax did not perform as well as the statictic rules did on their training material, but outscored them by far on new text. A correction tool was created with the phonotax as means of error detection. The tool displays every occurrence of an error string at the same time and gives the user the possibility to give different corrections to each occurrence. This work shows that it is possible to find errors in optically scanned text without relying on a lexicon, and that word structure can provide useful information to the correction process. 1. Introduction Optical character recognition (OCR) is a technique for moving text resources from paper medium to electronic form, something that is often needed in our computerised society. Companies and authorities want to make old material machine readable or searchable. Unfortunately, it does not get us all the way. With good paper originals, OCR can achieve 99% of the characters correctly recognised but the result will still contain in average one error word per 20 words which means 5% incorrect words or about one error per sentence (Kukich, 1992). Depending on the application of the optically scanned text, large post processing efforts can be necessary. Since OCR is often used to move large amounts of text to electronic form, the proofreading is a task both demanding and dull. This makes the need for good tools of spell checking and correction large and urgent. Most spell checkers and OCR post processing systems are lexicon based. A lexicon of reasonable size is used to match against the text, and any word token not in the text is presented as a possible error. Probability scores or similarity measures are then used to generate correction suggestions. Proceedings of NODALIDA 1999, pages 174-181  175 I will concentrate on the error finding process and not try to generate correction suggestions. I want to find ways of proofreading text without relying on a lexicon. Instead I will try to define rules that identify character sequences that are unlikely to be correct word tokens. I made two experiments: using statistical methods to find unexpectedly frequent character sequences, and using phono- or graphotactical rules to find unlikely character combinations. Obviously these results can be generalised for all kinds of proofreading tasks: e.g. handwriting recognition or dictation tasks. The work described in this report has been done within a Master thesis at the Language Engineering Programme at Uppsala University. The work has been carried out at SICS and was funded by the Digital Library project. 1.1 OCR errors Many recognition errors are caused by graphical similarity : a r g u i n e n t (argument), t e a t u r e (feature), m e a n (mean), s e m a n t i c s (semantics), s y s t e m e t (systemet), t e x t f ö r s t å e l s e (textförståelse), d i s a m b i q u e r a s (disambigueras). Proofreading by hand is difficult. The graphical errors are by definition difficult to detect by ocular scanning through the text: the visual difference between bodv and body is very small. Other problems are print quality, font and the age of the original that sometimes produce eirors that make it impossible to guess the original word like a p p T i j d o - t i L - s (approaches.) or Umt (that). Another group of errors that occurs in optical scanning of text is split errors; spaces are inserted in a word and produces a number of strings, many of them incorrect: p r o n u n c i a t i o n (pronunciation), i n t e (inte), ö r e I i g g e r (/ore/igger).However, many or even most of these errors still produce string tokens that are unlikely or impossible words in the language under consideration. 1.2 Approaches to Error Correction Most approaches to correction of scanning errors are lexicon based. A lexicon of reasonable size is used to match against the text, and any word token not in the lexicon is defined as incorrect. This leads to many false alarms, since a lexicon never can cover everything. Many correct words and proper names will be presented as errors by the system. To find real word errors — i.e. errors that result in another correct word — sequences of parts of speech are evaluated for likelihood of occurrence, and unlikely sequences are marked as possible errors.(Meknavin et al., 1998; Tong & Evans 1996; Huismann, 1999). Proceedings of NODALIDA 1999 
In this paper we will present a system that is able to perform cooperative information retrieval actions over a text knowledge base. The knowledge base is composed by four levels: Interaction, Domain, Information Retrieval and Text. The interaction level is responsible for the dialogue management, including the inference of attitudes. The domain level is composed by rules encoding knowledge about the text domain. The information retrieval level includes knowledge about IR actions over sets of documents. The text level has knowledge about the words in each text. Cooperation is achieved through two main strategies: 1) clustering the answer sets of documents accordingly with the domain and IR-level knowledge; 2) keeping the context of the interaction and inferring the user intentions. 1. Introduction In this paper we present a cooperative information retrieval system in the law domain. The information retrieval system is composed by a text knowledge base built from the Portuguese Attorney General documents. As an example, the following dialogue demonstrates the major features of our system (the example will be explained in more detail in the last section). User - Ul: Documents about pensions for relevant services? System - U2; Pensions that were given or refused? User - U3: Both. System - U4; Pensions for militaries or civilians? User - U5: Civilians System - U6 : There are 129 documents, listed in the right frame. User - U7: where there was a drowning? System - U8 : Since there are 45 documents of the 129 above that match the concept. 
In this paper we describe the evaluation of a language-dependent aligner. We begin by introducing the alignment program, explaining why it would be interesting to evaluate it with particular emphasis on the language pair English-Portuguese. A short presentation of the corpus used to test the aligner is also given. We then describe three experiments that were performed in the evaluation process, presenting the results and di.scussing the methodology. The paper ends with a discussion of more general conclusions relative to an evaluation of this kind. 1. Introduction Two criteria that are often employed in the evaluation of NLP programs are performance and usability. Another criterion, less frequently mentioned, is the adequaey of handling particular languages. The present study describes a set of experiments devised to perform such an evaluation. Although researchers concerned with parallel corpus building and exploration will generally be happy to use a system available for their languages without evaluating it thoroughly, especially when the system is freely distributed - as is the case of the present system, the kind of work reported originates from two relevant concerns. The first one is about methodological aspects related to the development of NLP systems. The second concern is evaluation and comparison of products. In fact, there is a blatant lack of serious evaluation work of products and systems concerning the Portuguese language, which is a situation we have been trying to change in the project Computational Processing of Portuguese at SINTEF.' The Translation Corpus Aligner (TCA) was developed in connection with the English-Norwegian Parallel Corpus (ENPC) project with the aim of automatically aligning English and Norwegian texts (see e.g. Hofland 1996, Hofland & Johansson 1998). Although the program was originally written for the language pair EnglishNorwegian, it has been further developed to handle other language pairs, including English-Portuguese. It includes a language-dependent component in the form of an anchor word list. In the present paper we set out to evaluate the TCA for the language pair English-Portuguese. In particular, we want to • investigate the effect of the anchor word list; • compare the results of the program with and without the anchor word list; • find out how much a proofreader has to check manually after alignment In order to perform the evaluation, we used the English-Portuguese part of the ENPC, which currently includes 16 English texts, about 12,000 words each, that have translations into Portuguese.^ Proceedings of NODALIDA 1999, pages 191-205  192  2. A short description of the TCA The alignment program automatically matches original sentences with their translations. In the process of linking corresponding sentences, the program makes use of an anchor word list that contains word pairs of the languages in question. The one used here was not originally made for Portuguese and English, but was adapted from the EnglishNorwegian anchor word list. In the alignment process, a value is given to the combination of sentences based on matches in the word list. The program goes through the texts and reads chunks of sentences in each language, resulting in matrices in which the program seeks the highest values for a match between sentences. In addition to the anchor word list, these values depend on the number of characters within the sunits/sentences in each language, on special characters (such as ?!;, etc.), on proper names, and on cognates. One of the strengths of the program is that it does not require any preprocessing in the form of "hard regions", e.g. paragraph alignment, and therefore can get back on track after an alignment error (or in spite of a translation discrepancy). To take an example, we could imagine the following chunks of text to be aligned:  English original (cxlracl from Doris Lessing's Portuguese translation by Bcrnardctte Pinto Lcitc  The Good Terrorist)  <s>Shc faced him, undefianl but confident, and < p x s> E la cncarou-o, sem de.safio mas confiantc, c  said, "I wonder if they will accept us','"</s> pcrguntou:</.sx/p>  <s>And, as she had known he would, he .said, "It <pxs>& m dash; Achas que nos accitam'?</s> </p>  is a question of whether we will accept <pxs>& m dash; E, conforme sabia que Jasper  them."</s>c/p>  responderia, este rctorquiu;</sx/p>  < p x s> S h c had withstood the test on her, that <pxs>&mdaslv, E tudo uma questäo dc nös os  bony pain, and he let her wrist go and went on to accitarmos a clcs.</sx/p>  the door.</s>  < pxs> A licc resistira ao teste sobre a sua pcssoa, ä  dor össca, c cle largou-lhc o pulso c dirigiu-sc para a  porta.</s>  Figure I. Texts to be aligned  We would expect that some words in the English extract would match some of the Portuguese words in the anchor word list; these matches would in turn enhance the possibility of the program linking the correct sentences. For the second English sentence in the extract above, for instance, we will get the following matches in the word lisl:^  and c is, 's / é, csld question* / pergunt* quest* wc nös accept* accit* them / Ihes, os, as  The TCA assigns a unique identification to each s-unit with its corresponding sunit(s) in the translation. When the original sentence has only one corresponding sentence in the translation, we get a 1:1 correspondence. When the original sentence has been translated into two sentences, we get a 1:2 correspondence:  <s id=DL2.1.sl6 corresp='DL2TP.1.sl7 DL2TP.1.sl8'>And, as she had known he would, he said, "It is a question of whether we will accept them."</s> <s id=DL2TP.1.sl7 corresp=DL2.1.sl6>tmdash; E, conforme sabia que Jasper responderia, este retorquiu:</s>  Proceedings of NODALIDA 1999  193 <s id=DL2TP.1.sl8 corresp =DL2.1. sl6>&mda’ sh; E tudo uma questao de nos os aceitarmos a eles.</s> Each text has a unique code, normally starting with the authors’ initials, e.g. a text by Doris Lessing has a code DLx. All texts will be referred to by their code. 3. Human intervention required The program handles 1:1, 1:2, and 1:0 correspondences, i.e. one s-unit matching one, two or zero s-units in the translation; the latter two have to be checked manually. The remaining matrices, containing 1:1 correspondences only, are assumed to be correct and are not systematically checked. Hence, the file that is proofread only includes matrices that do not contain 1:1 correspondences throughout. Table 1 shows the percentage of matrices that the proofreader has to check, an average of 48.8%, for the 16 English-Portuguese texts. This apparently discouraging percentage needs some explanation since it does not reflect the actual manual intervention that takes place. The picture becomes skewed simply because we immediately associate half of the matrices with half of the sentences in a text. This is not the case, however. Each matrix contains approximately 10-12 sentence pairs, and as will be shown in the matrix below (Figure 2), the proofreader will only have to investigate the s-units that are not 1:1 correspondences. This is to say that although the proofreader has to investigate almost half of the matrices, he will not have to investigate half of the texl/s-units, but merely a small percentage. Table I. Number of matrices to check Text Matrices in Matrices % output nic'' to check  ABRI AHI ATI BCl DLl DL2 FFl JBIP JBIPP JHl MAI NGl PDJ3 RDOl STl WBI Total  120  78  65  131  98  74  
In this paper we present some techniques, experiences and results from the SCARRIE project, which has aimed at developing improved proofreading tools for the Scandinavian languages. The focus is on methods which were used for spelling and grammar checking and particularly some novel analyses and treatments dealing with the extensive lexical and grammar variation in Norwegian Bokmål. The major findings are that (1) since in Bokmål, lexical variants may differ with respect to grammatical features, stylistic replacement at the word level causes a need for grammar checking, and (2) the different systems for gender agreement in Bokmål can be handled in an economical way by a single grammar and lexicon if the features in the lexicon are interpreted dynamically depending on the subnorm or style preferred by the author. 1. Introduction Among language technology applications, proofreading can be equally challenging as, for instance, machine translation. In a fair number of cases, errors in texts cannot be adequately corrected without understanding the intention of the author in the given context. In practice, however, automatic proofreading systems excel not by their understanding of the text but by their consistency and tirelessness in processing high volumes without becoming 'blind' to relatively simple errors as humans tend to become. But even with limited expectations, the user may may find a proofreading system unacceptable if the number of false alarms is higher than the number of actual errors spotted, or if many suggestions for correction are inappropriate. It is therefore useful to invest in research aimed at improving the coverage of the system as well as the system's ability to propose corrections that are appropriate in the given context, whether grammatical or stylistic. The SCARRIE project is a language technology project aimed at building high-quality proofreading tools for the Scandinavian languages (Danish, Swedish and Norwegian). The project was sponsored by the European Commission through the Telematics programme. The project ran from December 1996 through February 1999. The coordinator was WordFinder Software AB (Växjö, Sweden). The other main partners in the project were the HIT-programme at Universitetet i Bergen, Institutionen för lingvistik at Uppsala Universitet, Center for Sprogteknologi (København) and Svenska Dagbladet (Stockholm). Although the projeet aimed at eventual commercial exploitation, it did involve a great deal of linguistic and computational research. Proceedings of NODALIDA 1999, pages 206-215  207 At the end of the project, prototypes and evaluation reports were delivered for these languages. The prototypes correct simple misspellings and mistypings by means of advanced spelling and sound based matching criteria. They also have good coverage in their recognition of new compounds and derivations. Furthermore, they can detect repeated sequences, correct diacritical marks, correct words in the context of idioms and multi-word expressions, correct words based on different styles or norms, and perform limited grammar correction. We will in the remainder of this paper only report on the Norwegian part of the project. Earlier publications (Rosén & De Smedt 1998, De Smedt & Rosén 1999) have highlighted different aspects of the linguistic and computational methodologies which are at the basis of SCARRIE for Norwegian. In this paper, we concentrate on the problems of proofreading for a language which shows rich variation not only in the lexicon but also in grammar. The specific problems related to grammar correction and style which are discussed below have to our knowledge never before been thoroughly researched with natural language processing methods. 2. Lexical and inflectional variants in Bokmål Designing a system for automatic proofreading is difficult for any language, but Norwegian Bokmål presents a special challenge. Bokmål allows rich variation in the form of stems as well as inflectional endings. As we will see, this variation has grammatical consequences. First, we observe that many word stems in Bokmål have variants, as shown in for instance (1) and (2). (1) inelk / mjølk (milk) (2) gress/gras (grass) There is also variation in inflection, as exemplified in (3) and (4). (3) bok+en / bok+ci (bookn-DEF) (4) arbeid+et / arbeid+a / arbeid+de (work-ned) When computing the possible combinations of different stems and endings, we observe that the situation becomes more complex and the number of allowed variants increases, as demonstrated in (5). (5) melk+en / melk+a / mj0lk+en / mjølk+a (milk+DEF) When compounding also enters the picture, word forms can easily have a dozen or more variants. At sentence level it is obvious that even more possible combinations may be found. Consider sentence (6) containing thirteen words; this sentence as a whole has no less than 165,888 possible spellings when all combinations of variants are enumerated. (6) De lavtlønte sykelijemsansatte ble helt utmattet og slukket tørsten med den surnete fløtemelken. (The low-paid hospital employees became totally exhausted and quenched their thirst with the soured cream milk.) Proceedings of NODALIDA 1999  208 Not all combinations of variants are equally acceptable in all contexts, because variation is not free, but bound to more or less established subnorms within Bokmål. In other words, for almost all words that have variants, it is the case that the choice between them is not neutral, but depends on the author’s style. Although the situation is vastly complex, we have in SCARRIE for Norwegian distinguished between three basic styles: radical, conservative and neutral. The stem melk, for instance, is conservative or neutral, whereas mjølk is radical; the ending +en is conservative or neutral, while +a is radical or neutral. Example (6) has only neutral variants; entirely conservative or radical variants of this sentence, as well as a great number of inconsistent combinations, can easily be constructed. As a final remark on basic styles, we mention that SCARRIE for Norwegian also handles a school book norm (læreboknormalen) in Bokmål, but this is another, quite complicated story which we will not go into here. The fact that lexical items are associated with a norm or style value has a number of consequences. First, the user of a proofreading system should be able to state a preferred style. The system should be sensitive to that style so that whenever it makes a suggestion for a correction of a spelling error, it proposes a form that fits with the author's style. Second, we can observe that some forms are rarely or never used because they are infelicitous combinations of different styles, such as mjølken in (5), which combines a radical stem with a non-radical ending. Even though such forms may be allowed in Bokmål, they will need to be replaced under all major styles (conservative, neutral and radical) if consistency is to be achieved. Third, variants may have different grammatical features; this final complication is an important theme of this paper. 3. Lexicon SCARRIE uses full-form lexicons which contain all inflectional forms of words except genitives (which are very regular). In order to restrict the system's suggestions for correction to those word forms that occur in the author's chosen style, it would be possible to construct separate lexicons for each subnorm. However, since there is considerable overlap between subnorms, this would be a wasteful and inflexible solution. Moreover, separate lexicons would not allow straightforward correction of word forms belonging to other styles than the author's stated preference. Therefore, one integrated lexicon was constructed with replacements depending on style. Table 1 presents a simple example, consisting of the lexical entries belonging to the lemma bok (book). Proceedings of NODALIDA 1999  209  Table 1. Lemma f o r ( w i t h o u t frequency information)  wordform style code compound codes replacement grammar code  bok  N  N,sg,indef  N_f_sg_indef  boka  C2  N,sg  boken  N_f_sg_def  boken  C3  N,sg  boka  N_fm_sg_def  bøkene N  N,pl  N_f_pl_def  bøker  N  N,pl,indef  N_f_pl_indef  The entries for the indefinite singular hok (book), plural definite bøkene (the books) and plural indefinite bøker (books) all have a style code N which means they are normal forms and do not need to be replaced under any styles. The entry for the singular definite boka (the book) specifies that under style code C2 (conservative), it should be replaced by boken. Conversely, the entry for boken specifies that under style code C3 (radical), it should be replaced by boka. In other subnorms, both word forms are acceptable and therefore never replaced. For forms with more variants, the coding in the lexicon can be quite complex; for more examples from the lexicon, we refer the reader to Rosén & De Smedt (forthcoming). We focus now on grammar checking, which obviously relies on grammatical information associated with lexical entries. The last column in Table 1 contains grammar codes that are used by a parser which can for instance detect lack of agreement in the NP, as in (7). (7) * Den lille bøkene (the little+SG+DEF books+PL+DEF) Before discussing the grammar codes in the lexicon in more detail, the grammar correction mechanism itself will first be sketched. 4. Grammar correction in SCARRIE Various approaches to grammar correction have been tried out for the various languages covered in the SCARRIE project. The system for Norwegian is based on the CORRie platform, which has a built-in LR parser based on augmented context free grammar (Vosse 1992, 1994). Grammar rules for Norwegian were written for use with this parser. The following kinds of grammatical errors can be automatically corrected by the Norwegian SCARRIE grammar; 1. Lack of gender, number and/or definiteness agreement between (a) determiner, adjective phrase and noun in NP, (b) subject or object and nominal or adjectival complement in S, and (c) noun and postposed possessive in NP. 
In this paper the current stage o f the Uppsala Word Aligner (UWA) is described. The system is developed within the project on parallel texts, PLUG, which has its focus on the analysis o f bi-lingual text collections with Swedish either as the source or the target language. UWA comprises a set o f knowledgelite approaches' for word alignment and lexicon extraction. A distinctivefeature is its modularity. In the article, the main principles o f the alignment software are introduced, different configurations and approaches are described, and examples o falignment results are presented. 1. Introduction }Vord alignment aims at the identification of translation equivalents between linguistic units below the sentence level within parallel text (Merkel 1999), mainly bilingual text ibitext). Those units include single-word units (SlVUs) and multi-word units {MWUs) and will be referred to as link units further on. The basic terminology for describing parallel text and word alignment in this paper follows Ahrenberg et al (1999) and Ahrenberg et al (forthcoming). In particular, each word correspondence in the bitext describes a link instance, or simply a link. A pair of link units that is instantiated in the bitext will be referred to as link type. Word alignment systems usually assume segmented bitext {sentence aligned bitext). Common bitext segments are sentence fragments, sentences, and sequences of sentences that have corresponding units in the translation. Dep>ending on its purpose, a word alignment system attempts to maximize the number of discovered links (-> word instance alignment) (e.g. Ahrenberg et al 1998, Melamed 1999) or the number of extracted link types (-> bilingual lexicon extraction) (e.g Melamed 1995, Resnik and Melamed 1997, Tiedemann 1998a). Lexicon extraction aims at providing correct translations whereas word alignment has to deal with insertions, deletions, and other modifications within the bitext as well. Furthermore, word alignment systems may focus on specific types of link units, e.g. terms (Dagan and Church 1994, van der Eijk 1993) and collocations (Smadja et al 1996). The task of word alignment is not trivial especially because it goes beyond simple oneto-one word correspondences in many cases. Multi-word units (MWUs) have to be handled due to the use of non-compositional compounds, associated idiomatic expressions, multi-word names and so on. The difference in compounding between different languages increases the difficulties with the identification of appropriate Proceedings of NODALIDA 1999, pages 216-227  217 correspondences further. In addition, the text type is decisive for the word alignment process. Technical text tends to include specific terms and simple structures that are translated directly while e.g. literary texts include many language-specific idioms. Concurrently, Martin Kay’s proposal for approaching machine translation can be applied to word alignment as well: “The keynote will be modesty. At each stage, we will do only what we know we can do reliably. Little steps for little feet!” (M.Kay 1980) The alignment of MWUs can be approached in different ways. Smadja et al (1996) propose the compilation of source language collocations using statistical co-occurrence measures {static segmentation). The appropriate correspondent is found by iterative extension of the link unit in the target language segment {dynamic segmentation). Another approach applies collocation lists for both languages, which have been compiled in advance from the bitext (Ahrenberg et al 1998, Tiedemann 1998). MWUs are then handled like single tokens for both languages. A third possibility is to expand link units iteratively for both languages in order to find the most appropriate link. Melamed (1997) uses iterative processing in order to optimize the underlying translation model. The iteration is alternated in order to cover MWUs in both languages. The word alignment system, which is introduced in this paper, supports all the three approaches to text segmentation as far as contiguous phrases are concerned. The approach to dynamic segmentation differs from Melamed in the usage of ranked candidate lists instead of translation models. Furthermore, classified stop word lists are used for improving the result and reducing the search space. 2. The Uppsala Word Aligner (UWA) The Uppsala Word Aligner is developed within the co-operative project on parallel texts, PLUG^ (Sågvall Hein, forthcoming). The goal of PLUG is to develop, apply, and evaluate software for the alignment and generation of translation data. Word alignment is one of the major issues at hand. UWA is based on earlier studies on bilingual lexicon extraction (Tiedemann 1997, 1998a). It combines several knowledge-lite approaches to word alignment. The system is integrated into the Uplug toolbox (Tiedemann forthcoming), which provides a convenient environment for the work with modular corpus tools. As mentioned earlier, word alignment walks with small feet. Therefore, the proposal is to combine different approaches, to collect available knowledge sources, and to reach the goal by little steps. Proceedings of NODALIDA 1999  218 The principles of baby-steps 1. Prepare carefully before taking the first step. 2. Use all available tools that can help. 3. Check alternatives before taking the next step. 4. Take safe steps first; try risky steps later. 5. Remove everything that is in the way. 6. Improve walking by learning from previous steps. 7. Reach the goal with many steps rather than one big one. 8. Continue trying until you cannot get closer. Based on these general principles, UWA was designed as a modular and iterative (rule 6+8) system. The bitext runs through initial pre-processing steps before the alignment starts (rule 1). Alignment candidates are collected from any appropriate source (rule 2). Candidates are ranked by their reliability, e.g. association scores (rule 3). The most reliable candidate is aligned first (rule 4). The alignment process is split into a sequence of separated steps (rule 7). Aligned link units are removed from the search space (5). In the following the three main phases of the UWA are described: text segmentation, candidate collection, and alignment of link units. 2.1 Text Segmentation UWA assumes sentence-aligned bitexts. However, an initial sentence alignment step can be added. UWA provides modules for the work with static and dynamic text segmentation. In the case of dynamic segmentation, the text will be simply tokenized, i.e. segmented into SWUs and punctuation marks. In case of static segmentation, this phase accounts for a subsequence segmentation of the bitext into link units. It includes tokenization, the recognition of MWUs, and the actual segmentation of the text into link units. The recognition of MWUs can be automated. UWA applies iterative processing for the compilation of word collocations. The association between word units and their subsequent words is measured in terms of mutual information scores. As proposed in Ahrenberg et al (1998), classified lists of functional words are used to reduce search space and to exclude ungrammatical constructions. Consider the small example of classified stop words for English phrase generation, which is illustrated in figure 1. Figure 1: Classified stop word lists (lower case). skip token = skip at = '(or,and,but,not)' skip before = '(i,you,he,she,it,we,they)' skip after = '(mine,yours,his,hers,its,ours,theirs) non-phrase-starter = '(my,your,his,her,our,their)' non-phrase-ender = '(the,a,an)' skip at string type = '(numeric)' Proceedings of NODALIDA 1999  219  Stop words are divided into 6 types. ‘Skip token' items are not considered at all in any segmentation. Furthermore, the segmentation will stop at 'skip at’ tokens. They are considered to be single word units and the segmentation process continues with the subsequent token. ‘Skip before' defines link unit breaks in front of each instance of each word that is specified. Similarly, ‘skip after' defines breaks after each instance of words in the list. ‘Non-phrase-starter’ and ‘non-phrase-ender’ are not allowed in the beginning or at the end of any phrase, respectively. However, those words may appear within phrasal constructions as e.g. in ‘in my mind’ or ‘in a row’. Note, that the definite article is allowed in the beginning of a phrase. In this way correspondences between definite forms of English and Swedish nouns can be recognized^. Furthermore, each category may include all tokens of a certain string type. In the example above, all numeric tokens will be added to the ‘skip at’ list. In cases of overlapping definitions the stronger restriction is chosen. In the current stage of the system only four of the classes above are used: ‘skip token’, ‘skip at’, ‘non-starter’, and ‘non-ender’. In figure 2 a sample of an automatically generated list of English collocations is presented. It is based on an English subtext from the PLUG corpus (Tiedemann 1998b) with about 66,000 words. The minimal frequency was set to 4.  Figure 2: Generated phrases with frequency>4 (case-folded).  MI 10.039 10.039 10.039 9.717 9.717 9.717 9.454 9.454 9.454 9.454 9.395 9.231 9.231 9.231 9.231  f req 4 4 4 5 4 4 e 6 4 4 4 7 7 5 5  collocation the yom kippur raymond aron danny kaye yom kippur the golan heights golan heights world war ii the mishkenot sha lyndon Johnson american public opinion justice cohn tel aviv mishkenot sha the ottoman empire ottoman empire  In the current stage, the system provides contiguous phrases only. Static text segmentation applies a simple left-to-right process. It starts with the left-most token in the bitext and looks for the longest valid link unit. The segmentation continues to the right of the last validated link unit until the complete bitext is processed. Here, single word units always represent valid link units and sentence breaks always mark the end of the current link unit.  Proceedings of NODALIDA 1999  220  2.2 Identification and Collection of Candidate Pairs  In this part the system compiles and collects translation equivalents. Sources are pre­ compiled collections and generated lists of candidate pairs. In the current implementation, UWA applies the following sources:  • pairs of associated word units (applying co-occurrence measures) • cognate lists (applying string similarity measures) • single word bitext segments • pairs of low frequency units • machine readable bilingual dictionaries (MRBDs) • previously aligned word pairs (iteration)  Collections of candidate pairs are compiled by investigations on the association between link units. UWA applies co-occurrence measures and string similarity scores in order to find alignment candidates. The number of possible candidates is reduced by some general restrictions in order to improve the performance:  link distance', link units have to occur within a certain distance between their positions in the bitext segment string length', each link unit has to exceed a minimal length length difference ratio (LDR)'. the ratio between the length of the shorter link unit and the length of the longer link unit has to pass a certain threshold value string type', each link unit has to present a certain string type (e.g. ‘contains at least one alphabetic character’) frequency', the number of occurrences of each link unit has to exceed a certain value (for co-occurrence measures only) co-occurrence frequency: each pair of link units has to co-occur at least a certain number of times (for co-occurrence measures only)  The value of each of the parameters above can be adjusted to the type of investigation in progress. Certainly, string length and LDR should be restricted for investigations on string similarity, whereas frequency thresholds are important for co-occurrence measures.  UWA supports three word association scores: the Dice coefficient, mutual information,  and t-score. The current investigations were focused on the application of the Dice  coefficient.  Dice = - 2prob{S, T) prob{S) + prob{T)  In our case S and T represent the link units in the source and the target language under consideration. The probabilities of S and T to occur in the text, and the probability of both units to co-occur in the same bitext segment (sentence alignment) can be estimated by appropriate frequency counts. Simple stemming functions are used in order to reduce the inflectional variety of words in different languages and to improve the statistical calculations.  Proceedings of NODALIDA 1999  221  String similarity can be measured by different metrics (Melamed 1995, Borin 1998). UWA uses the Longest Common Subsequence Ratio (LCSR). UWA applies dynamic programming for computing the length of the longest common subsequence (LCS) of two strings (Stephen 1992). This value, divided by the length of the longer string, provides a measure for string similarity between them. In figure 2, the LCSR calculation is illustrated. In the figure, the application of the algorithm with MWUs is demonstrated as well.  Figure 2: The longest common subsequence ratio of ‘see example’ and ‘se exempel’.  s e e e Xa m P1 e  s  L1 1 1 1 1 1 1 1 1  e 1 2 22222 2 22  
1. Introduction The module of morphological analysis and/or synthesis is unavoidable in any language engineering tool for Estonian because of its rich morphology. For example, in information retrieval systems, it is usually desirable to make queries using semantic entities, not using special morphological forms of a word. As the word stem often has several shapes in Estonian, the morphological component should belong to any information retrieval system. Example 1. To make a query for all occurrences of the word “rida” (“row”), without a morphological synthesiser, three different queries are needed (we assume the possibility to add star (*) to the end of the stem instead of inflectional suffices): Proceedings of NODALIDA 1999, pages 228-242  229 rida* Cstem in strong grade) rea* Cstein in weak grade) ritta (singular additive (“to the row”), quite often used with this word) The development of EETwoLM is not the first attempt to computerise the morphological analysis and synthesis of Estonian. Ulle Viks (1994) has done important research in the field of morphological classification of Estonian on the basis of pattern recognition theory starting from the 1970s. Viks (1992) has compiled the first morphological dictionary for Estonian as a practical output of the investigations. Further, E. Kuusik and U. Viks (1998) have implemented the rule-based morphological analysis and synthesis for Estonian. Heiki-Jaan Kaalep (1996) has developed the speller for Estonian using the results of Viks (1992). Nevertheless, the growing popularity of the two-level model encourages us to consider its suitability to Estonian morphology. From the practical point of view, the appropriate description of Estonian morphology in the form of lexicons and two-level rules makes the significant move towards the application of Xerox language engineering tools to Estonian language (www.xerox.com/xrce/mltt).  2. The Brief Overview of Estonian Morphology  Estonian language is a member of Finno-Ugric family and is a close relative to Finnish. Estonian morphology is complex - inflected word-forms are built using both agglutination and stem flexion. Nouns have 14-15 cases. Plural forms often have parallel forms. Table 1. Noun paradigm (word “fcasj” (“hand”))-  Case Nominative Genitive  Abbrev-  Singular  iation Word-form Meaning  N  kdsi  (the) hand  G  kde  of the hand  Plural Form Meaning kded hands kdte of the hands  Proceedings of NODALIDA 1999  Partitive Illative Inessive Elative Allative Adessive Ablative Translative Terminative Essive Abessive Comitative Additive  P 111 In El All Ad Abl Trl Ter Es Ab Kom Adt  kätt käesse käes käest käele käel käelt käeks käeni käena käeta käega kätte  230  hand (partial object) into the hand in the hand out of the hand (on)to the hand (up)on, at the hand from the hand for, as the hand up to, until the hand as the hand without the hand with the hand (in)to the hand  käsi etc. kätesse or käsisse kätes or käsis kätest or käsist kätele or käsile kätel or käsil kätelt or käsilt käteks or käsiks käteni kätena käteta kätega -  Verbs have the following categories in Estonian; person (singular 1st to plural 3rd), voice (personal, impersonal), tense (present, imperfect, perfect, past perfect), mood (indicative, conditional, imperative, quotative).  Table 2. P art of the verb paradigm - infinite verb forms and indicative mood of the finite verb forms (word “muutma" (“to change”)).  1. Infinite (declined) forms  Morphological meaning  Supine  (illative)  Supine  inessive  Supine  elative  Supine  translative  Supine  abessive  Supine  impersonal  Infinitive  Gerund  Abbreviation Sup Sup In Sup El Sup Tr Sup Ab Sup Ips Inf Ger  Example muutma muutmas muutmast muutmaks muutmata muudetama muuta muutes  Proceedings of NODALIDA 1999  231  Participles (Pts):  Present (Pr) Personal (Ps)  participle  Impersonal (Ips)  Past  (Pt) Personal  participle  Impersonal  Pts Pr Ps Pts Pr Ips Pts Pt Ps Pts Pt Ips  muutev muudetav muutnud muudetud  2. Finite (conjugated) forms  2.1. Indicative mood (Ind)  Personal voice (Ps)  Tense  Number Person  Present  Affirm- Sg ation  1.  Ind Pr Ps Sgl muudan  2.  Ind Pr Ps Sg2 muudad  3.  Ind Pr Ps Sg3 muudab  PI  1.  IndP rP sP ll muudame  2.  Ind Pr Ps P12 muudate  3.  Ind Pr Ps P13 muudavad  Negation (Neg)  Ind Pr Ps Neg ei muuda  Imperfect Affirm- Sg ation PI  Negation  Present Affirmation  perfect Negation  Past  Affirmation  perfect Negation  Impersonal voice (Ips)  Present Affirmation  
The Data-Oriented Parsing Model (DOP, [1]; [2]) has been presented as a promising paradigm for NLP. It has also been used as a basis for Machine Translation (MT) — Data-Oriented TVanslation (DOT, [9]). Lexical Functional Grammar (LFG, [5]) has also been used for MT ([6]). LFG has recently been allied to DOP to produce a new LFG-DOP model ([3]) which improves the robustness of LFG. We summarize the DOT model of translation as well as the DOP model on which it is based. We demonstrate that DOT is not guaranteed to produce the correct translation, despite provably deriving the most probable translation. Finally, we propose a novel hybrid model for MT based on LFG-DOP which promises to improve upon DOT, as well as the pure LFG-based translation model. 
